<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 96]
- [cs.LG](#cs.LG) [Total: 144]
- [cs.AI](#cs.AI) [Total: 65]
- [cs.HC](#cs.HC) [Total: 46]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction](https://arxiv.org/abs/2508.06495)
*Juliana Resplande Sant'anna Gomes,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 论文提出了一种半自动事实核查（SAFC）系统，针对葡萄牙语新闻数据集（Fake.Br、COVID19.BR、MuMiN-PT）进行外部证据的补充，以弥补现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 由于虚假信息传播速度快于人工核查能力，且葡萄牙语缺乏整合外部证据的公开数据集，亟需开发SAFC系统。

Method: 采用大型语言模型（Gemini 1.5 Flash）提取文本主要主张，并通过搜索引擎API（Google Search API、Google FactCheck Claims Search API）检索外部证据。同时引入数据验证和预处理框架（如近重复检测）提升数据质量。

Result: 论文通过该方法成功丰富了葡萄牙语新闻数据集，为开发更强大的自动事实核查系统提供了支持。

Conclusion: 该方法有效填补了葡萄牙语SAFC系统的数据空白，为未来研究提供了实用工具和框架。

Abstract: The accelerated dissemination of disinformation often outpaces the capacity
for manual fact-checking, highlighting the urgent need for Semi-Automated
Fact-Checking (SAFC) systems. Within the Portuguese language context, there is
a noted scarcity of publicly available datasets that integrate external
evidence, an essential component for developing robust AFC systems, as many
existing resources focus solely on classification based on intrinsic text
features. This dissertation addresses this gap by developing, applying, and
analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,
MuMiN-PT) with external evidence. The approach simulates a user's verification
process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)
to extract the main claim from texts and search engine APIs (Google Search API,
Google FactCheck Claims Search API) to retrieve relevant external documents
(evidence). Additionally, a data validation and preprocessing framework,
including near-duplicate detection, is introduced to enhance the quality of the
base corpora.

</details>


### [2] [Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models](https://arxiv.org/abs/2508.06504)
*Yao Ge,Sudeshna Das,Yuting Guo,Abeed Sarker*

Main category: cs.CL

TL;DR: 本文研究了动态提示策略在少样本生物医学命名实体识别（NER）中的应用，通过检索增强生成（RAG）优化性能，显著提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在少样本生物医学NER任务中表现有限，需要改进提示策略以提升性能。

Method: 采用动态提示策略，基于输入文本相似性选择上下文学习示例，并优化静态和动态提示技术。

Result: 动态提示显著提升性能，TF-IDF和SBERT检索方法在5-shot和10-shot设置中分别提高F1分数7.3%和5.6%。

Conclusion: 上下文自适应的动态提示策略通过RAG显著提升了生物医学NER任务的性能。

Abstract: Biomedical named entity recognition (NER) is a high-utility natural language
processing (NLP) task, and large language models (LLMs) show promise
particularly in few-shot settings (i.e., limited training data). In this
article, we address the performance challenges of LLMs for few-shot biomedical
NER by investigating a dynamic prompting strategy involving retrieval-augmented
generation (RAG). In our approach, the annotated in-context learning examples
are selected based on their similarities with the input texts, and the prompt
is dynamically updated for each instance during inference. We implemented and
optimized static and dynamic prompt engineering techniques and evaluated them
on five biomedical NER datasets. Static prompting with structured components
increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA
3-70B, relative to basic static prompting. Dynamic prompting further improved
performance, with TF-IDF and SBERT retrieval methods yielding the best results,
improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,
respectively. These findings highlight the utility of contextually adaptive
prompts via RAG for biomedical NER.

</details>


### [3] [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)
*Lei Jiang,Fan Chen*

Main category: cs.CL

TL;DR: 《CarbonScaling》提出了一种分析框架，将神经扩展定律与碳排放结合，量化模型准确性与碳足迹的关系。


<details>
  <summary>Details</summary>
Motivation: 现有神经扩展定律忽视了LLM规模扩大带来的碳排放问题，需将其纳入考量以实现可持续性。

Method: 通过整合神经扩展模型、GPU硬件演进、并行优化和碳排放估算，构建CarbonScaling框架。

Result: 结果显示准确性与碳排放呈幂律关系，但实际效率问题增加了扩展因子；硬件技术对中小型模型有效，但对超大型LLM效果有限。

Conclusion: CarbonScaling为训练更可持续和碳高效的LLM提供了关键见解，优化如批量大小可缓解效率问题。

Abstract: Neural scaling laws have driven the development of increasingly large
language models (LLMs) by linking accuracy improvements to growth in parameter
count, dataset size, and compute. However, these laws overlook the carbon
emissions that scale exponentially with LLM size. This paper presents
\textit{CarbonScaling}, an analytical framework that extends neural scaling
laws to incorporate both operational and embodied carbon in LLM training. By
integrating models for neural scaling, GPU hardware evolution, parallelism
optimization, and carbon estimation, \textit{CarbonScaling} quantitatively
connects model accuracy to carbon footprint. Results show that while a
power-law relationship between accuracy and carbon holds, real-world
inefficiencies significantly increase the scaling factor. Hardware technology
scaling reduces carbon emissions for small to mid-sized models, but offers
diminishing returns for extremely large LLMs due to communication overhead and
underutilized GPUs. Training optimizations-especially aggressive critical batch
size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers
key insights for training more sustainable and carbon-efficient LLMs.

</details>


### [4] [The Art of Breaking Words: Rethinking Multilingual Tokenizer Design](https://arxiv.org/abs/2508.06533)
*Aamod Thakur,Ajay Nagpal,Atharva Savarkar,Kundeshwar Pundalik,Siddhesh Dosi,Piyush Sawarkar,Viraj Thakur,Rohit Saluja,Maunendra Sankar Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文研究了多语言环境下LLM的tokenization问题，提出了一种新的数据组合算法，显著降低了token-to-word比例，并提升了模型性能和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有tokenizer在多语言环境中效率低下，token-to-word比例高，影响了模型性能和推理速度。

Method: 通过系统研究词汇量、预tokenization规则和训练语料组成，提出了一种平衡多语言数据的数据组合算法。

Result: 新算法将平均token-to-word比例降低了约6%，并在多语言Indic模型中实现了40%以上的改进。

Conclusion: tokenization是构建高效、可扩展多语言LLM的关键因素之一。

Abstract: While model architecture and training objectives are well-studied,
tokenization, particularly in multilingual contexts, remains a relatively
neglected aspect of Large Language Model (LLM) development. Existing tokenizers
often exhibit high token-to-word ratios, inefficient use of context length, and
slower inference. We present a systematic study that links vocabulary size,
pre-tokenization rules, and training-corpus composition to both token-to-word
efficiency and model quality. To ground our analysis in a linguistically
diverse context, we conduct extensive experiments on Indic scripts, which
present unique challenges due to their high script diversity and orthographic
complexity. Drawing on the insights from these analyses, we propose a novel
algorithm for data composition that balances multilingual data for tokenizer
training. Our observations on pretokenization strategies significantly improve
model performance, and our data composition algorithm reduces the average
token-to-word ratio by approximately 6% with respect to the conventional data
randomization approach. Our tokenizer achieves more than 40% improvement on
average token-to-word ratio against stateof-the-art multilingual Indic models.
This improvement yields measurable gains in both model performance and
inference speed. This highlights tokenization alongside architecture and
training objectives as a critical lever for building efficient, scalable
multilingual LLMs

</details>


### [5] [Factor Augmented Supervised Learning with Text Embeddings](https://arxiv.org/abs/2508.06548)
*Zhanye Luo,Yuefeng Han,Xiufan Yu*

Main category: cs.CL

TL;DR: 论文提出了一种名为AEALT的监督式降维框架，通过结合增强自编码器，从预训练的大语言模型中提取低维任务相关嵌入，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 高维文本嵌入在效率上存在问题，增加了计算成本，需要一种方法在保持语义的同时降低维度。

Method: AEALT框架首先提取文本嵌入，然后通过监督增强自编码器学习低维任务相关潜在因子。

Result: AEALT在分类、异常检测和预测任务中表现优于传统深度学习方法和其他降维方法。

Conclusion: AEALT通过结合降维和预训练模型，显著提升了嵌入的效率和任务性能。

Abstract: Large language models (LLMs) generate text embeddings from text data,
producing vector representations that capture the semantic meaning and
contextual relationships of words. However, the high dimensionality of these
embeddings often impedes efficiency and drives up computational cost in
downstream tasks. To address this, we propose AutoEncoder-Augmented Learning
with Text (AEALT), a supervised, factor-augmented framework that incorporates
dimension reduction directly into pre-trained LLM workflows. First, we extract
embeddings from text documents; next, we pass them through a supervised
augmented autoencoder to learn low-dimensional, task-relevant latent factors.
By modeling the nonlinear structure of complex embeddings, AEALT outperforms
conventional deep-learning approaches that rely on raw embeddings. We validate
its broad applicability with extensive experiments on classification, anomaly
detection, and prediction tasks using multiple real-world public datasets.
Numerical results demonstrate that AEALT yields substantial gains over both
vanilla embeddings and several standard dimension reduction methods.

</details>


### [6] [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)
*Ying Liu,Can Li,Ting Zhang,Mei Wang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）在自适应教学指导中的能力，提出了GuideEval基准，发现现有LLMs在适应学习者认知状态方面表现不足，并通过行为引导的微调策略提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs的苏格拉底式提问能力，但忽视了其基于学习者认知状态的自适应指导能力。

Method: 提出GuideEval基准，通过三个阶段（感知、编排、激发）评估LLMs的教学指导能力，并采用行为引导的微调策略优化模型。

Result: 实证发现现有LLMs在适应学习者需求方面表现不佳，但行为引导的微调策略显著提升了指导性能。

Conclusion: 研究提倡从孤立内容评估转向以学习者为中心的交互范式，为评估苏格拉底式LLMs提供了新视角。

Abstract: The conversational capabilities of large language models hold significant
promise for enabling scalable and interactive tutoring. While prior research
has primarily examined their capacity for Socratic questioning, it often
overlooks a critical dimension: adaptively guiding learners based on their
cognitive states. This study shifts focus from mere question generation to the
broader instructional guidance capability. We ask: Can LLMs emulate expert
tutors who dynamically adjust strategies in response to learners'
understanding? To investigate this, we propose GuideEval, a benchmark grounded
in authentic educational dialogues that evaluates pedagogical guidance through
a three-phase behavioral framework: (1) Perception, inferring learner states;
(2) Orchestration, adapting instructional strategies; and (3) Elicitation,
stimulating proper reflections. Empirical findings reveal that existing LLMs
frequently fail to provide effective adaptive scaffolding when learners exhibit
confusion or require redirection. Furthermore, we introduce a behavior-guided
finetuning strategy that leverages behavior-prompted instructional dialogues,
significantly enhancing guidance performance. By shifting the focus from
isolated content evaluation to learner-centered interaction, our work advocates
a more dialogic paradigm for evaluating Socratic LLMs.

</details>


### [7] [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)
*Xiaoyuan Zhu,Muru Zhang,Ollie Liu,Robin Jia,Willie Neiswanger*

Main category: cs.CL

TL;DR: 论文提出了一种自动化生成高质量遗忘数据集的方法，用于语言模型的特定知识遗忘，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型可能包含敏感、有害或受版权保护的知识，需要一种无需完全重新训练即可遗忘特定知识的方法。

Method: 通过结构化提示管道自动生成教科书风格的数据，仅需输入领域名称即可生成遗忘数据集。

Result: 实验表明，该方法生成的合成数据集优于基线方法，且与专家手工整理的数据集效果相当。

Conclusion: 合成数据集为广泛领域的实用、可扩展遗忘提供了一条有前景的路径。

Abstract: Modern large language models often encode sensitive, harmful, or copyrighted
knowledge, raising the need for post-hoc unlearning-the ability to remove
specific domains of knowledge from a model without full retraining. A major
bottleneck in current unlearning pipelines is constructing effective forget
sets-datasets that approximate the target domain and guide the model to forget
it. In this work, we introduce a scalable, automated approach to generate
high-quality forget sets using language models themselves. Our method
synthesizes textbook-style data through a structured prompting pipeline,
requiring only a domain name as input. Through experiments on unlearning
biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic
datasets consistently outperform the baseline synthetic alternatives and are
comparable to the expert-curated ones. Additionally, ablation studies reveal
that the multi-step generation pipeline significantly boosts data diversity,
which in turn improves unlearning utility. Overall, our findings suggest that
synthetic datasets offer a promising path toward practical, scalable unlearning
for a wide range of emerging domains without the need for manual intervention.
We release our code and dataset at
https://github.com/xyzhu123/Synthetic_Textbook.

</details>


### [8] [BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent](https://arxiv.org/abs/2508.06600)
*Zijian Chen,Xueguang Ma,Shengyao Zhuang,Ping Nie,Kai Zou,Andrew Liu,Joshua Green,Kshama Patel,Ruoxi Meng,Mingyi Su,Sahel Sharifymoghaddam,Yanxi Li,Haoran Hong,Xinyu Shi,Xuye Liu,Nandan Thakur,Crystina Zhang,Luyu Gao,Wenhu Chen,Jimmy Lin*

Main category: cs.CL

TL;DR: 论文提出了BrowseComp-Plus基准，以解决现有评估方法在公平性和透明度上的不足，通过固定语料库实现可控实验，并展示了其在区分深度研究系统性能上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（如BrowseComp）依赖动态且不透明的网络搜索API，导致公平性和透明度问题，无法有效评估深度研究LLMs的能力。

Method: 引入BrowseComp-Plus基准，使用固定且精心筛选的语料库，每个查询包含人工验证的支持文档和挑战性负样本，支持可控实验。

Result: 实验显示，BrowseComp-Plus能有效区分系统性能，例如GPT-5的准确率从55.9%提升至70.1%（结合Qwen3-Embedding-8B检索器）。

Conclusion: BrowseComp-Plus为深度研究系统和检索方法的评估与分析提供了全面且可控的框架，有助于理解检索效果和上下文工程。

Abstract: Deep-Research agents, which integrate large language models (LLMs) with
search tools, have shown success in improving the effectiveness of handling
complex queries that require iterative search planning and reasoning over
search results. Evaluations on current benchmarks like BrowseComp relies on
black-box live web search APIs, have notable limitations in (1) fairness:
dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep
research methods; (2) transparency: lack of control over the document corpus
makes it difficult to isolate retriever contributions. In other words, the
current evaluations may compare a complete deep research system at a given
time, but they do not foster well-controlled experiments to provide insights
into the capability of underlying deep research LLMs. To address these
challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,
employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus
includes human-verified supporting documents and mined challenging negatives,
enabling controlled experimentation. The benchmark is shown to be effective in
distinguishing the performance of deep research systems. For instance, the
open-source model Search-R1, when paired with the BM25 retriever, achieves
3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with
the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with
fewer search calls. This benchmark allows comprehensive evaluation and
disentangled analysis of deep research agents and retrieval methods, fostering
insights into retrieval effectiveness, citation accuracy, and context
engineering in Deep-Research system.

</details>


### [9] [Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models](https://arxiv.org/abs/2508.06621)
*Tomohiro Sawada,Kartik Goyal*

Main category: cs.CL

TL;DR: 研究探讨了不依赖合并列表的BPE推断算法对语言模型性能的影响，发现非目标性算法对性能影响较小。


<details>
  <summary>Details</summary>
Motivation: BPE合并列表可能泄露训练数据信息，研究旨在探索不依赖合并列表的推断算法的下游影响。

Method: 比较两类BPE推断方案：目标性偏离合并列表（如随机合并顺序、删除/截断）和非目标性算法（不依赖合并列表的贪婪或精确压缩）。

Result: 目标性偏离显著降低性能，而非目标性算法对性能影响极小。

Conclusion: 研究为更简单且隐私保护的tokenization方案提供了可能。

Abstract: Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a
learned token vocabulary with a detailed merge list. Recent work has shown that
this merge list exposes a potential attack surface for extracting information
about language model's training data. In this paper, we explore the downstream
impact of BPE inference algorithms that do not rely on this merge list at all,
and hence differ from the encoding process during BPE training. To address this
question, we investigate two broad classes of BPE inference schemes that differ
from BPE application during training: a) targeted deviation from merge-lists
including random merge orders, and various corruptions of merge list involving
deletion/truncation, and b) non-targeted BPE inference algorithms that do not
depend on the merge list but focus on compressing the text either greedily or
exactly. Extensive experiments across diverse language modeling tasks like
accuracy-based QA benchmarks, machine translation, and open-ended generation
reveal that while targeted deviation from the merge lists exhibits significant
degradation in language model performance, the non-targeted merge-list-free
inference algorithms result in minimal impact on downstream performance that is
often much smaller than expected. These findings pave way for simpler and
potentially more privacy-preserving tokenization schemes that do not
catastrophically compromise model performance.

</details>


### [10] [Measuring Stereotype and Deviation Biases in Large Language Models](https://arxiv.org/abs/2508.06649)
*Daniel Wang,Eli Brignac,Minjia Mao,Xiao Fang*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）存在刻板印象偏差和偏离偏差，可能对多群体造成潜在危害。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在生成内容时可能表现出的两种偏差，以揭示其潜在风险。

Method: 通过要求四种先进LLMs生成个人资料，分析不同人口群体与政治倾向、宗教等属性的关联。

Result: 所有测试的LLMs均对多群体表现出显著的刻板印象偏差和偏离偏差。

Conclusion: 研究揭示了LLMs在推断用户属性时的偏差问题，警示其生成内容的潜在危害。

Abstract: Large language models (LLMs) are widely applied across diverse domains,
raising concerns about their limitations and potential risks. In this study, we
investigate two types of bias that LLMs may display: stereotype bias and
deviation bias. Stereotype bias refers to when LLMs consistently associate
specific traits with a particular demographic group. Deviation bias reflects
the disparity between the demographic distributions extracted from
LLM-generated content and real-world demographic distributions. By asking four
advanced LLMs to generate profiles of individuals, we examine the associations
between each demographic group and attributes such as political affiliation,
religion, and sexual orientation. Our experimental results show that all
examined LLMs exhibit both significant stereotype bias and deviation bias
towards multiple groups. Our findings uncover the biases that occur when LLMs
infer user attributes and shed light on the potential harms of LLM-generated
outputs.

</details>


### [11] [Testing the Limits of Machine Translation from One Book](https://arxiv.org/abs/2508.06665)
*Jonathan Shaw,Dillon Mee,Timothy Khouw,Zackary Leech,Daniel Wilson*

Main category: cs.CL

TL;DR: 论文研究了如何利用语言资源（如语法、词典和平行句子）提升LLM在低资源语言（如Kanuri）翻译中的表现，发现平行句子是最有效的数据源，而语法单独使用效果有限。


<details>
  <summary>Details</summary>
Motivation: Kanuri语言虽使用人口众多，但数字资源匮乏，研究旨在探索如何利用有限资源提升LLM的翻译质量。

Method: 设计两个数据集（健康和通用术语），通过提供不同语言资源组合（语法、词典、平行句子），对比LLM翻译与人工翻译的效果。

Result: 平行句子表现最佳，语法单独使用效果有限；LLM在准确性上优于流畅性。

Conclusion: LLM翻译评估需多维指标，仅依赖语法无法满足领域翻译需求。

Abstract: Current state-of-the-art models demonstrate capacity to leverage in-context
learning to translate into previously unseen language contexts. Tanzer et al.
[2024] utilize language materials (e.g. a grammar) to improve translation
quality for Kalamang using large language models (LLMs). We focus on Kanuri, a
language that, despite having substantial speaker population, has minimal
digital resources. We design two datasets for evaluation: one focused on health
and humanitarian terms, and another containing generalized terminology,
investigating how domain-specific tasks impact LLM translation quality.
  By providing different combinations of language resources (grammar,
dictionary, and parallel sentences), we measure LLM translation effectiveness,
comparing results to native speaker translations and human linguist
performance. We evaluate using both automatic metrics and native speaker
assessments of fluency and accuracy.
  Results demonstrate that parallel sentences remain the most effective data
source, outperforming other methods in human evaluations and automatic metrics.
While incorporating grammar improves over zero-shot translation, it fails as an
effective standalone data source. Human evaluations reveal that LLMs achieve
accuracy (meaning) more effectively than fluency (grammaticality).
  These findings suggest LLM translation evaluation benefits from
multidimensional assessment beyond simple accuracy metrics, and that grammar
alone, without parallel sentences, does not provide sufficient context for
effective domain-specific translation.

</details>


### [12] [Do Biased Models Have Biased Thoughts?](https://arxiv.org/abs/2508.06671)
*Swati Rajwal,Shivank Garg,Reem Abdel-Salam,Abdelrahman Zayed*

Main category: cs.CL

TL;DR: 研究探讨了语言模型中的偏见问题，发现模型的思考步骤与输出偏见的关联性较低。


<details>
  <summary>Details</summary>
Motivation: 语言模型存在性别、种族等多方面的偏见，影响其实际应用。本文旨在通过链式思考提示方法研究模型的思考步骤是否也存在偏见。

Method: 使用公平性指标对5种流行的大型语言模型进行实验，量化11种不同偏见在模型思考步骤和输出中的表现。

Result: 结果显示，模型思考步骤中的偏见与输出偏见的关联性较低（相关系数小于0.6，p值小于0.001）。

Conclusion: 与人类不同，测试的模型即使有偏见决策，其思考步骤未必总是存在偏见。

Abstract: The impressive performance of language models is undeniable. However, the
presence of biases based on gender, race, socio-economic status, physical
appearance, and sexual orientation makes the deployment of language models
challenging. This paper studies the effect of chain-of-thought prompting, a
recent approach that studies the steps followed by the model before it
responds, on fairness. More specifically, we ask the following question:
\textit{Do biased models have biased thoughts}? To answer our question, we
conduct experiments on $5$ popular large language models using fairness metrics
to quantify $11$ different biases in the model's thoughts and output. Our
results show that the bias in the thinking steps is not highly correlated with
the output bias (less than $0.6$ correlation with a $p$-value smaller than
$0.001$ in most cases). In other words, unlike human beings, the tested models
with biased decisions do not always possess biased thoughts.

</details>


### [13] [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)
*Evangelia Spiliopoulou,Riccardo Fogliato,Hanna Burnsky,Tamer Soliman,Jie Ma,Graham Horwood,Miguel Ballesteros*

Main category: cs.CL

TL;DR: 该论文提出了一种统计框架，用于识别和量化大语言模型（LLM）作为评委时的自我偏见（self-bias），并通过实验验证了某些模型存在自我偏见和家族偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LLM作为评委时可能存在的自我偏见问题，这种偏见会扭曲对模型真实性能的评估。

Method: 方法是通过统计框架建模LLM评委对自己和其他模型输出的评分分布差异，同时考虑独立第三方评委（如人类）的评分质量。

Result: 实验结果表明，某些模型（如GPT-4o和Claude 3.5 Sonnet）会系统性地给自己的输出更高评分，并存在家族偏见。

Conclusion: 结论强调了使用LLM评委时的潜在问题，并提供了减少偏见的实用建议。

Abstract: Large language models (LLMs) can serve as judges that offer rapid and
reliable assessments of other LLM outputs. However, models may systematically
assign overly favorable ratings to their own outputs, a phenomenon known as
self-bias, which can distort evaluations of true model performance. Previous
studies often conflate genuine differences in model quality with bias or
incorrectly assume that evaluations from LLMs and humans follow the same rating
distributions. In this work, we present a statistical framework that explicitly
formalizes assumptions under which self-bias can be identified and estimated.
Our method models the difference in the scoring distribution that
LLM-as-a-judge assigns to its own completions compared to other models, while
accounting for the underlying quality of the completions provided by an
independent, third-party judge (e.g., humans). Our method reliably isolates and
quantifies self-bias, even when models vary in ability, ensuring that genuine
performance differences are not mistaken for self-bias. We conduct an empirical
analysis of self-bias on a large dataset (>5000 prompt-completion pairs)
consisting of expert human annotations and judgments from nine different LLM
judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,
systematically assign higher scores to their own outputs. These models also
display family-bias; systematically assigning higher ratings to outputs
produced by other models of the same family. Our findings highlight potential
pitfalls of using LLM judges and offer practical guidance to mitigate biases
when interpreting automated evaluations.

</details>


### [14] [Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis](https://arxiv.org/abs/2508.06729)
*Komala Subramanyam Cherukuri,Pranav Abishai Moses,Aisa Sakata,Jiangping Chen,Haihua Chen*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的框架，利用大型语言模型（LLM）自动对日裔美国人监禁口述历史进行语义和情感标注，展示了LLM在敏感历史档案分析中的潜力。


<details>
  <summary>Details</summary>
Motivation: 口述历史是记录生活经验的重要资源，但其大规模分析因非结构化格式、情感复杂性和高标注成本而受限。

Method: 结合专家标注、提示设计和LLM评估（ChatGPT、Llama、Qwen），采用零样本、少样本和RAG策略进行语义和情感分类。

Result: ChatGPT在语义分类中表现最佳（F1 88.71%），Llama在情感分析中略优（82.66%）。最佳提示配置标注了92,191句。

Conclusion: LLM在精心设计的提示下可有效分析大规模口述历史，为数字人文和集体记忆保护提供了可复用的标注流程和伦理指导。

Abstract: Oral histories are vital records of lived experience, particularly within
communities affected by systemic injustice and historical erasure. Effective
and efficient analysis of their oral history archives can promote access and
understanding of the oral histories. However, Large-scale analysis of these
archives remains limited due to their unstructured format, emotional
complexity, and high annotation costs. This paper presents a scalable framework
to automate semantic and sentiment annotation for Japanese American
Incarceration Oral History. Using LLMs, we construct a high-quality dataset,
evaluate multiple models, and test prompt engineering strategies in
historically sensitive contexts. Our multiphase approach combines expert
annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We
labeled 558 sentences from 15 narrators for sentiment and semantic
classification, then evaluated zero-shot, few-shot, and RAG strategies. For
semantic classification, ChatGPT achieved the highest F1 score (88.71%),
followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama
slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models
showing comparable results. The best prompt configurations were used to
annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our
findings show that LLMs can effectively perform semantic and sentiment
annotation across large oral history collections when guided by well-designed
prompts. This study provides a reusable annotation pipeline and practical
guidance for applying LLMs in culturally sensitive archival analysis. By
bridging archival ethics with scalable NLP techniques, this work lays the
groundwork for responsible use of artificial intelligence in digital humanities
and preservation of collective memory. GitHub:
https://github.com/kc6699c/LLM4OralHistoryAnalysis.

</details>


### [15] [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755)
*Xianjun Yang,Liqiang Xiao,Shiyang Li,Faisal Ladhak,Hyokun Yun,Linda Ruth Petzold,Yi Xu,William Yang Wang*

Main category: cs.CL

TL;DR: 本文提出多轮越狱（multi-turn jailbreaking）的概念，指出当前越狱研究仅关注单轮攻击，而忽略了多轮对话中的潜在威胁。作者构建了MTJ-Bench基准测试，并呼吁社区关注LLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究仅针对单轮攻击，而LLMs具备多轮对话能力，可能引发更严重的安全威胁。

Method: 提出多轮越狱概念，构建MTJ-Bench基准测试，评估开源和闭源模型的多轮越狱能力。

Result: 揭示了多轮越狱的新漏洞，为LLM安全性研究提供了新视角。

Conclusion: 呼吁社区关注多轮越狱威胁，推动更安全的LLM开发。

Abstract: Current jailbreaking work on large language models (LLMs) aims to elicit
unsafe outputs from given prompts. However, it only focuses on single-turn
jailbreaking targeting one specific query. On the contrary, the advanced LLMs
are designed to handle extremely long contexts and can thus conduct multi-turn
conversations. So, we propose exploring multi-turn jailbreaking, in which the
jailbroken LLMs are continuously tested on more than the first-turn
conversation or a single target query. This is an even more serious threat
because 1) it is common for users to continue asking relevant follow-up
questions to clarify certain jailbroken details, and 2) it is also possible
that the initial round of jailbreaking causes the LLMs to respond to additional
irrelevant questions consistently. As the first step (First draft done at June
2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak
Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and
closed-source models and provide novel insights into this new safety threat. By
revealing this new vulnerability, we aim to call for community efforts to build
safer LLMs and pave the way for a more in-depth understanding of jailbreaking
LLMs.

</details>


### [16] [SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection](https://arxiv.org/abs/2508.06803)
*Ziqi Liu,Yangbin Chen,Ziyang Zhou,Yilin Li,Mingxuan Hu,Yushan Pan,Zhijie Xu*

Main category: cs.CL

TL;DR: SEVADE是一个自演化的多代理分析框架，用于抗幻觉的讽刺检测，通过动态代理推理引擎（DARE）和多角度文本解构，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在讽刺检测中存在单视角分析、静态推理路径和易受幻觉影响的局限性，影响了准确性和可靠性。

Method: 提出SEVADE框架，结合动态代理推理引擎（DARE）和轻量级理性裁决器（RA），将复杂推理与最终判断分离。

Result: 在四个基准数据集上，SEVADE实现了平均准确率提升6.75%，Macro-F1分数提升6.29%。

Conclusion: SEVADE通过多代理协作和分离推理与分类，有效提升了讽刺检测的性能和抗幻觉能力。

Abstract: Sarcasm detection is a crucial yet challenging Natural Language Processing
task. Existing Large Language Model methods are often limited by
single-perspective analysis, static reasoning pathways, and a susceptibility to
hallucination when processing complex ironic rhetoric, which impacts their
accuracy and reliability. To address these challenges, we propose **SEVADE**, a
novel **S**elf-**Ev**olving multi-agent **A**nalysis framework with
**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The
core of our framework is a Dynamic Agentive Reasoning Engine (DARE), which
utilizes a team of specialized agents grounded in linguistic theory to perform
a multifaceted deconstruction of the text and generate a structured reasoning
chain. Subsequently, a separate lightweight rationale adjudicator (RA) performs
the final classification based solely on this reasoning chain. This decoupled
architecture is designed to mitigate the risk of hallucination by separating
complex reasoning from the final judgment. Extensive experiments on four
benchmark datasets demonstrate that our framework achieves state-of-the-art
performance, with average improvements of **6.75%** in Accuracy and **6.29%**
in Macro-F1 score.

</details>


### [17] [Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems](https://arxiv.org/abs/2508.06810)
*Steven Coyne,Diana Galvan-Sosa,Ryan Spring,Camélia Guerraoui,Michael Zock,Keisuke Sakaguchi,Kentaro Inui*

Main category: cs.CL

TL;DR: 论文提出了一种针对语言学习的自动写作评估系统改进框架，通过分类错误类型和可推广性，生成更有效的反馈（如提示而非直接修正）。


<details>
  <summary>Details</summary>
Motivation: 现有自动写作评估系统虽能修正语法错误，但未针对语言学习优化，缺乏对学习者知识缺口的针对性反馈。

Method: 引入错误类型和可推广性标注框架，收集标注数据集，评估基于大语言模型的反馈生成方法（关键词引导、无关键词、模板引导）。

Result: 通过教师评估，比较了不同反馈生成方法的性能，包括相关性、准确性和可理解性。

Conclusion: 该框架和数据集为语言学习提供了更有效的自动反馈生成方法，未来可进一步优化。

Abstract: Recent advances in natural language processing (NLP) have contributed to the
development of automated writing evaluation (AWE) systems that can correct
grammatical errors. However, while these systems are effective at improving
text, they are not optimally designed for language learning. They favor direct
revisions, often with a click-to-fix functionality that can be applied without
considering the reason for the correction. Meanwhile, depending on the error
type, learners may benefit most from simple explanations and strategically
indirect hints, especially on generalizable grammatical rules. To support the
generation of such feedback, we introduce an annotation framework that models
each error's error type and generalizability. For error type classification, we
introduce a typology focused on inferring learners' knowledge gaps by
connecting their errors to specific grammatical patterns. Following this
framework, we collect a dataset of annotated learner errors and corresponding
human-written feedback comments, each labeled as a direct correction or hint.
With this data, we evaluate keyword-guided, keyword-free, and template-guided
methods of generating feedback using large language models (LLMs). Human
teachers examined each system's outputs, assessing them on grounds including
relevance, factuality, and comprehensibility. We report on the development of
the dataset and the comparative performance of the systems investigated.

</details>


### [18] [Text to Speech System for Meitei Mayek Script](https://arxiv.org/abs/2508.06870)
*Gangular Singh Irengbam,Nirvash Singh Wahengbam,Lanthoiba Meitei Khumanthem,Paikhomba Oinam*

Main category: cs.CL

TL;DR: 开发了一个基于Meitei Mayek文字的曼尼普尔语TTS系统，结合Tacotron 2和HiFi-GAN，支持音调语音和低资源环境。


<details>
  <summary>Details</summary>
Motivation: 为曼尼普尔语提供语音合成技术，促进语言保存和技术包容。

Method: 使用Tacotron 2和HiFi-GAN架构，开发Meitei Mayek到ARPAbet的音素映射，并构建单说话人数据集。

Result: 系统生成了清晰自然的语音，通过主客观指标验证。

Conclusion: 该系统为曼尼普尔语的语言保存和技术应用奠定了基础。

Abstract: This paper presents the development of a Text-to-Speech (TTS) system for the
Manipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and
HiFi-GAN, we introduce a neural TTS architecture adapted to support tonal
phonology and under-resourced linguistic environments. We develop a phoneme
mapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and
demonstrate intelligible and natural speech synthesis, validated through
subjective and objective metrics. This system lays the groundwork for
linguistic preservation and technological inclusion of Manipuri.

</details>


### [19] [ESNERA: Empirical and semantic named entity alignment for named entity dataset merging](https://arxiv.org/abs/2508.06877)
*Xiaobo Zhang,Congqing He,Ying He,Jian Peng,Dajie Fu,Tien-Ping Tan*

Main category: cs.CL

TL;DR: 提出了一种基于标签相似性的自动标签对齐方法，用于合并多源NER数据集，提升低资源领域的性能。


<details>
  <summary>Details</summary>
Motivation: 现有NER方法依赖大规模标注数据，但构建成本高；当前数据集合并方法缺乏可解释性和可扩展性。

Method: 结合经验和语义相似性，采用贪心成对合并策略统一标签空间。

Result: 成功合并三个NER数据集，并在金融领域低资源场景下提升性能。

Conclusion: 该方法为多源NER语料库整合提供了高效、可解释且可扩展的解决方案。

Abstract: Named Entity Recognition (NER) is a fundamental task in natural language
processing. It remains a research hotspot due to its wide applicability across
domains. Although recent advances in deep learning have significantly improved
NER performance, they rely heavily on large, high-quality annotated datasets.
However, building these datasets is expensive and time-consuming, posing a
major bottleneck for further research. Current dataset merging approaches
mainly focus on strategies like manual label mapping or constructing label
graphs, which lack interpretability and scalability. To address this, we
propose an automatic label alignment method based on label similarity. The
method combines empirical and semantic similarities, using a greedy pairwise
merging strategy to unify label spaces across different datasets. Experiments
are conducted in two stages: first, merging three existing NER datasets into a
unified corpus with minimal impact on NER performance; second, integrating this
corpus with a small-scale, self-built dataset in the financial domain. The
results show that our method enables effective dataset merging and enhances NER
performance in the low-resource financial domain. This study presents an
efficient, interpretable, and scalable solution for integrating multi-source
NER corpora.

</details>


### [20] [The ReQAP System for Question Answering over Personal Information](https://arxiv.org/abs/2508.06880)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TL;DR: ReQAP系统通过递归分解问题和增量构建操作树，支持对异构数据源的复杂查询，利用轻量级语言模型实现高效解答。


<details>
  <summary>Details</summary>
Motivation: 用户设备上存在大量结构化和非结构化数据，但缺乏高效工具支持复杂查询。

Method: 递归分解问题并构建操作树，结合轻量级语言模型进行智能解析和执行。

Result: 系统能高效解答复杂问题，并提供答案的详细追踪功能。

Conclusion: ReQAP通过透明化答案生成过程，提升用户对系统的理解和信任。

Abstract: Personal information is abundant on users' devices, from structured data in
calendar, shopping records or fitness tools, to unstructured contents in mail
and social media posts. This works presents the ReQAP system that supports
users with answers for complex questions that involve filters, joins and
aggregation over heterogeneous sources. The unique trait of ReQAP is that it
recursively decomposes questions and incrementally builds an operator tree for
execution. Both the question interpretation and the individual operators make
smart use of light-weight language models, with judicious fine-tuning. The demo
showcases the rich functionality for advanced user questions, and also offers
detailed tracking of how the answers are computed by the operators in the
execution tree. Being able to trace answers back to the underlying sources is
vital for human comprehensibility and user trust in the system.

</details>


### [21] [Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores](https://arxiv.org/abs/2508.06886)
*Arpita Saggar,Jonathan C. Darling,Vania Dimitrova,Duygu Sarikaya,David C. Hogg*

Main category: cs.CL

TL;DR: 提出了一种名为SBS的新框架，通过将对话生成与质量评分统一为一步，显著提升了基于角色的对话生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有对话数据多样性有限，导致大型语言模型在角色一致性对话生成中表现不佳。

Method: SBS框架在训练中结合增强响应和质量评分，利用名词替换和语义相似度评分作为质量代理。

Result: 在PERSONA-CHAT和ConvAI2数据集上，SBS显著提升了角色一致性对话的生成效果。

Conclusion: SBS框架通过评分条件训练，为角色对话生成提供了更优的解决方案。

Abstract: Persona-based dialogue generation is an important milestone towards building
conversational artificial intelligence. Despite the ever-improving capabilities
of large language models (LLMs), effectively integrating persona fidelity in
conversations remains challenging due to the limited diversity in existing
dialogue data. We propose a novel framework SBS (Score-Before-Speaking), which
outperforms previous methods and yields improvements for both million and
billion-parameter models. Unlike previous methods, SBS unifies the learning of
responses and their relative quality into a single step. The key innovation is
to train a dialogue model to correlate augmented responses with a quality score
during training and then leverage this knowledge at inference. We use
noun-based substitution for augmentation and semantic similarity-based scores
as a proxy for response quality. Through extensive experiments with benchmark
datasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training
allows existing models to better capture a spectrum of persona-consistent
dialogues. Our ablation studies also demonstrate that including scores in the
input prompt during training is superior to conventional training setups. Code
and further details are available at
https://arpita2512.github.io/score_before_you_speak

</details>


### [22] [Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection](https://arxiv.org/abs/2508.06913)
*Siyuan Li,Xi Lin,Guangyan Li,Zehao Liu,Aodu Wulianghai,Li Ding,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: SentiDetect是一种模型无关的框架，通过分析情感分布稳定性差异来检测LLM生成的文本，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法泛化性差且易受干扰，而LLM生成文本情感一致性较高，人类文本情感变化更大。

Method: 定义两个互补指标（情感分布一致性和情感分布保持性），量化情感稳定性和语义保留下的稳定性。

Result: 在多个数据集和LLM上表现优异，F1分数提升显著，且对干扰更具鲁棒性。

Conclusion: SentiDetect在检测LLM生成文本方面高效且鲁棒，优于现有方法。

Abstract: The rapid advancement of large language models (LLMs) has resulted in
increasingly sophisticated AI-generated content, posing significant challenges
in distinguishing LLM-generated text from human-written language. Existing
detection methods, primarily based on lexical heuristics or fine-tuned
classifiers, often suffer from limited generalizability and are vulnerable to
paraphrasing, adversarial perturbations, and cross-domain shifts. In this work,
we propose SentiDetect, a model-agnostic framework for detecting LLM-generated
text by analyzing the divergence in sentiment distribution stability. Our
method is motivated by the empirical observation that LLM outputs tend to
exhibit emotionally consistent patterns, whereas human-written texts display
greater emotional variability. To capture this phenomenon, we define two
complementary metrics: sentiment distribution consistency and sentiment
distribution preservation, which quantify stability under sentiment-altering
and semantic-preserving transformations. We evaluate SentiDetect on five
diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,
Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its
superiority over state-of-the-art baselines, with over 16% and 11% F1 score
improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,
SentiDetect also shows greater robustness to paraphrasing, adversarial attacks,
and text length variations, outperforming existing detectors in challenging
scenarios.

</details>


### [23] [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Khaled Shaban,Hozaifa Kassab*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的两阶段框架，结合了模型集成和指令调优的大语言模型，显著提升了《古兰经》问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 《古兰经》问答任务因古典阿拉伯语的复杂性和宗教文本的语义丰富性而具有独特挑战，需要一种高效的方法来解决低资源环境下的问题。

Method: 采用两阶段框架：1) 通过集成微调的阿拉伯语语言模型实现段落检索；2) 使用指令调优的大语言模型结合少量示例提示进行答案提取。

Result: 在Quran QA 2023共享任务中取得了最佳性能，检索阶段的MAP@10为0.3128，MRR@10为0.5763，提取阶段的pAP@10为0.669。

Conclusion: 模型集成和指令调优语言模型的结合能有效解决专业领域低资源问答的挑战。

Abstract: Quranic Question Answering presents unique challenges due to the linguistic
complexity of Classical Arabic and the semantic richness of religious texts. In
this paper, we propose a novel two-stage framework that addresses both passage
retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned
Arabic language models to achieve superior ranking performance. For answer
extraction, we employ instruction-tuned large language models with few-shot
prompting to overcome the limitations of fine-tuning on small datasets. Our
approach achieves state-of-the-art results on the Quran QA 2023 Shared Task,
with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of
0.669 for extraction, substantially outperforming previous methods. These
results demonstrate that combining model ensembling and instruction-tuned
language models effectively addresses the challenges of low-resource question
answering in specialized domains.

</details>


### [24] [Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models](https://arxiv.org/abs/2508.06974)
*Zhijun Tu,Hanting Chen,Siqi Liu,Chuanjian Liu,Jian Li,Jie Hu,Yunhe Wang*

Main category: cs.CL

TL;DR: 本文提出了一种渐进式训练方法，将预训练模型的浮点权重平滑转换为1-bit量化表示，显著降低了训练成本并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有1-bit量化方法通常从头训练模型，无法充分利用预训练模型，导致高训练成本和精度下降。

Method: 采用一致的渐进式训练方法，结合二进制感知初始化和双尺度补偿，平滑转换浮点权重为1-bit表示。

Result: 实验表明，该方法在不同规模的LLM上优于现有方法，实现了高性能1-bit量化。

Conclusion: 通过预训练模型实现高性能1-bit量化，避免了从头训练的高成本。

Abstract: 1-bit LLM quantization offers significant advantages in reducing storage and
computational costs. However, existing methods typically train 1-bit LLMs from
scratch, failing to fully leverage pre-trained models. This results in high
training costs and notable accuracy degradation. We identify that the large gap
between full precision and 1-bit representations makes direct adaptation
difficult. In this paper, we introduce a consistent progressive training for
both forward and backward, smoothly converting the floating-point weights into
the binarized ones. Additionally, we incorporate binary-aware initialization
and dual-scaling compensation to reduce the difficulty of progressive training
and improve the performance. Experimental results on LLMs of various sizes
demonstrate that our method outperforms existing approaches. Our results show
that high-performance 1-bit LLMs can be achieved using pre-trained models,
eliminating the need for expensive training from scratch.

</details>


### [25] [Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings](https://arxiv.org/abs/2508.07017)
*Mao Li,Fred Conrad,Johann Gagnon-Bartsch*

Main category: cs.CL

TL;DR: Vec2Summ是一种新颖的抽象摘要方法，通过语义压缩任务实现，利用均值向量表示文档集合并通过嵌入反转生成摘要。


<details>
  <summary>Details</summary>
Motivation: 解决基于LLM的摘要方法在上下文长度限制、可控性和扩展性方面的不足。

Method: 使用语义嵌入空间的均值向量表示文档，通过生成语言模型进行嵌入反转，并引入高斯分布采样以提升多样性。

Result: Vec2Summ能生成连贯的摘要，性能与直接LLM摘要相当，但细节较少，适用于需要扩展性和语义控制的场景。

Conclusion: Vec2Summ在扩展性、语义控制和语料库级抽象方面具有潜力。

Abstract: We propose Vec2Summ, a novel method for abstractive summarization that frames
the task as semantic compression. Vec2Summ represents a document collection
using a single mean vector in the semantic embedding space, capturing the
central meaning of the corpus. To reconstruct fluent summaries, we perform
embedding inversion -- decoding this mean vector into natural language using a
generative language model. To improve reconstruction quality and capture some
degree of topical variability, we introduce stochasticity by sampling from a
Gaussian distribution centered on the mean. This approach is loosely analogous
to bagging in ensemble learning, where controlled randomness encourages more
robust and varied outputs. Vec2Summ addresses key limitations of LLM-based
summarization methods. It avoids context-length constraints, enables
interpretable and controllable generation via semantic parameters, and scales
efficiently with corpus size -- requiring only $O(d + d^2)$ parameters.
Empirical results show that Vec2Summ produces coherent summaries for topically
focused, order-invariant corpora, with performance comparable to direct LLM
summarization in terms of thematic coverage and efficiency, albeit with less
fine-grained detail. These results underscore Vec2Summ's potential in settings
where scalability, semantic control, and corpus-level abstraction are
prioritized.

</details>


### [26] [SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages](https://arxiv.org/abs/2508.07069)
*Muhammad Dehan Al Kautsar,Aswin Candra,Muhammad Alif Al Hakim,Maxalmina Satria Kahfi,Fajri Koto,Alham Fikri Aji,Peerat Limkonchotiwat,Ekapol Chuangsuwanich,Genta Indra Winata*

Main category: cs.CL

TL;DR: SEADialogues是一个针对东南亚文化的多语言对话数据集，填补了现有闲聊数据集中文化多样性不足的空白。


<details>
  <summary>Details</summary>
Motivation: 现有闲聊数据集缺乏文化多样性，尤其是东南亚地区的文化背景未被充分体现。

Method: 开发了包含八种语言、六个东南亚国家的对话数据集，每段对话附带人物属性和两个文化相关主题。

Result: SEADialogues提供了丰富的文化背景和多语言支持，适用于研究文化敏感的人机对话模型。

Conclusion: 该数据集为开发更具文化意识和个性化的对话系统提供了重要资源。

Abstract: Although numerous datasets have been developed to support dialogue systems,
most existing chit-chat datasets overlook the cultural nuances inherent in
natural human conversations. To address this gap, we introduce SEADialogues, a
culturally grounded dialogue dataset centered on Southeast Asia, a region with
over 700 million people and immense cultural diversity. Our dataset features
dialogues in eight languages from six Southeast Asian countries, many of which
are low-resource despite having sizable speaker populations. To enhance
cultural relevance and personalization, each dialogue includes persona
attributes and two culturally grounded topics that reflect everyday life in the
respective communities. Furthermore, we release a multi-turn dialogue dataset
to advance research on culturally aware and human-centric large language
models, including conversational dialogue agents.

</details>


### [27] [BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context](https://arxiv.org/abs/2508.07090)
*Aditya Tomar,Nihar Ranjan Sahoo,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 论文介绍了BharatBBQ，一个针对印度多语言和文化背景的偏见评估基准，填补了现有西方中心化基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有偏见评估基准（如BBQ）主要关注西方语境，无法有效评估印度多语言和文化背景下的偏见。

Method: 开发了BharatBBQ，涵盖8种印度语言和13个社会类别，通过翻译和验证扩展数据集至392,864个示例。

Result: 评估发现多语言模型在印度语言中表现出比英语更严重的偏见，凸显了文化适应性基准的必要性。

Conclusion: 研究强调了针对特定文化和语言的偏见评估基准的重要性，以更全面地识别和减少AI系统中的偏见。

Abstract: Evaluating social biases in language models (LMs) is crucial for ensuring
fairness and minimizing the reinforcement of harmful stereotypes in AI systems.
Existing benchmarks, such as the Bias Benchmark for Question Answering (BBQ),
primarily focus on Western contexts, limiting their applicability to the Indian
context. To address this gap, we introduce BharatBBQ, a culturally adapted
benchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,
Telugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3
intersectional groups, reflecting prevalent biases in the Indian sociocultural
landscape. Our dataset contains 49,108 examples in one language that are
expanded using translation and verification to 392,864 examples in eight
different languages. We evaluate five multilingual LM families across zero and
few-shot settings, analyzing their bias and stereotypical bias scores. Our
findings highlight persistent biases across languages and social categories and
often amplified biases in Indian languages compared to English, demonstrating
the necessity of linguistically and culturally grounded benchmarks for bias
evaluation.

</details>


### [28] [Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning](https://arxiv.org/abs/2508.07101)
*Lijie Yang,Zhihao Zhang,Arti Jain,Shijie Cao,Baihong Yuan,Yiwei Chen,Zhihao Jia,Ravi Netravali*

Main category: cs.CL

TL;DR: LessIsMore是一种无需训练的稀疏注意力机制，通过全局注意力模式提升推理任务的效率和准确性，减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决现有稀疏注意力机制在长生成推理中因累积误差导致的准确性下降问题，同时避免高令牌保留率或昂贵的重新训练需求。

Method: 利用全局注意力模式，聚合局部注意力头的令牌选择与最近上下文信息，实现跨头令牌统一排名。

Result: 在多种推理任务中保持甚至提升准确性，平均解码速度提升1.1倍，令牌关注数量减少2倍，端到端速度提升1.13倍。

Conclusion: LessIsMore在减少计算开销的同时保持准确性，为推理任务提供了一种高效的稀疏注意力解决方案。

Abstract: Large reasoning models achieve strong performance through test-time scaling
but incur substantial computational overhead, particularly from excessive token
generation when processing short input prompts. While sparse attention
mechanisms can reduce latency and memory usage, existing approaches suffer from
significant accuracy degradation due to accumulated errors during
long-generation reasoning. These methods generally require either high token
retention rates or expensive retraining. We introduce LessIsMore, a
training-free sparse attention mechanism for reasoning tasks, which leverages
global attention patterns rather than relying on traditional head-specific
local optimizations. LessIsMore aggregates token selections from local
attention heads with recent contextual information, enabling unified cross-head
token ranking for future decoding layers. This unified selection improves
generalization and efficiency by avoiding the need to maintain separate token
subsets per head. Evaluation across diverse reasoning tasks and benchmarks
shows that LessIsMore preserves -- and in some cases improves -- accuracy while
achieving a $1.1\times$ average decoding speed-up compared to full attention.
Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss,
achieving a $1.13\times$ end-to-end speed-up compared to existing sparse
attention methods.

</details>


### [29] [Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution](https://arxiv.org/abs/2508.07111)
*Falaah Arif Khan,Nivedha Sivakumar,Yinong Oliver Wang,Katherine Metcalf,Cezanne Camacho,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在交叉性偏见中的表现，通过新基准WinoIdentity评估了50种偏见模式，发现模型对双重弱势身份的置信度较低，表明其性能更多依赖记忆而非逻辑推理。


<details>
  <summary>Details</summary>
Motivation: AI系统可能反映并加剧社会偏见，尤其在关键社会决策中引发身份伤害。现有研究多关注单轴偏见评估，而交叉性偏见的影响尚未充分探索。

Method: 通过扩展WinoBias数据集，创建WinoIdentity基准，包含245,700个提示，评估50种交叉性偏见模式，提出核心置信度差异指标。

Result: 评估五个LLMs发现，置信度差异高达40%，模型对双重弱势身份（如反刻板印象场景）最不确定，甚至对特权标记的置信度也下降。

Conclusion: LLMs的性能更多依赖记忆而非逻辑推理，存在价值对齐和有效性的双重失败，可能加剧社会伤害。

Abstract: Large language models (LLMs) have achieved impressive performance, leading to
their widespread adoption as decision-support tools in resource-constrained
contexts like hiring and admissions. There is, however, scientific consensus
that AI systems can reflect and exacerbate societal biases, raising concerns
about identity-based harm when used in critical social contexts. Prior work has
laid a solid foundation for assessing bias in LLMs by evaluating demographic
disparities in different language reasoning tasks. In this work, we extend
single-axis fairness evaluations to examine intersectional bias, recognizing
that when multiple axes of discrimination intersect, they create distinct
patterns of disadvantage. We create a new benchmark called WinoIdentity by
augmenting the WinoBias dataset with 25 demographic markers across 10
attributes, including age, nationality, and race, intersected with binary
gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.
Focusing on harms of omission due to underrepresentation, we investigate bias
through the lens of uncertainty and propose a group (un)fairness metric called
Coreference Confidence Disparity which measures whether models are more or less
confident for some intersectional identities than others. We evaluate five
recently published LLMs and find confidence disparities as high as 40% along
various demographic attributes including body type, sexual orientation and
socio-economic status, with models being most uncertain about
doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,
coreference confidence decreases even for hegemonic or privileged markers,
indicating that the recent impressive performance of LLMs is more likely due to
memorization than logical reasoning. Notably, these are two independent
failures in value alignment and validity that can compound to cause social
harm.

</details>


### [30] [Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens](https://arxiv.org/abs/2508.07143)
*Anna Seo Gyeong Choi,Hoon Choi*

Main category: cs.CL

TL;DR: 论文探讨了自动语音识别（ASR）系统中的偏见问题，指出其对非标准方言的误识别不仅是技术问题，还涉及道德不公，并提出三个独特的伦理维度。


<details>
  <summary>Details</summary>
Motivation: 研究ASR系统对非标准方言的误识别如何从技术问题演变为道德不公，揭示其对社会边缘化语言群体的不尊重。

Method: 通过哲学视角区分道德中性分类（discriminate1）和有害歧视（discriminate2），分析ASR系统如何将前者转化为后者。

Result: 识别了ASR偏见的三个伦理维度：时间负担、对话流中断及语言与身份的联系，指出现有技术公平指标未能捕捉这些不对称权力关系。

Conclusion: 解决ASR偏见需超越技术干预，承认多样语言形式的合法性，开发尊重语言多样性和说话者自主性的系统。

Abstract: Automatic Speech Recognition (ASR) systems now mediate countless
human-technology interactions, yet research on their fairness implications
remains surprisingly limited. This paper examines ASR bias through a
philosophical lens, arguing that systematic misrecognition of certain speech
varieties constitutes more than a technical limitation -- it represents a form
of disrespect that compounds historical injustices against marginalized
linguistic communities. We distinguish between morally neutral classification
(discriminate1) and harmful discrimination (discriminate2), demonstrating how
ASR systems can inadvertently transform the former into the latter when they
consistently misrecognize non-standard dialects. We identify three unique
ethical dimensions of speech technologies that differentiate ASR bias from
other algorithmic fairness concerns: the temporal burden placed on speakers of
non-standard varieties ("temporal taxation"), the disruption of conversational
flow when systems misrecognize speech, and the fundamental connection between
speech patterns and personal/cultural identity. These factors create asymmetric
power relationships that existing technical fairness metrics fail to capture.
The paper analyzes the tension between linguistic standardization and pluralism
in ASR development, arguing that current approaches often embed and reinforce
problematic language ideologies. We conclude that addressing ASR bias requires
more than technical interventions; it demands recognition of diverse speech
varieties as legitimate forms of expression worthy of technological
accommodation. This philosophical reframing offers new pathways for developing
ASR systems that respect linguistic diversity and speaker autonomy.

</details>


### [31] [Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172)
*Biao Yi,Jiahao Li,Baolei Zhang,Lihai Nie,Tong Li,Tiansheng Huang,Zheli Liu*

Main category: cs.CL

TL;DR: 论文提出了一种名为SafeGrad的新方法，通过梯度手术解决恶意样本在微调过程中破坏大语言模型安全对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在有害样本比例增加时防御性能急剧下降，原因是用户任务更新与安全目标之间存在冲突梯度。

Method: SafeGrad通过检测冲突并将用户任务梯度的有害部分投影到对齐梯度的正交平面上，同时使用KL散度对齐损失增强鲁棒性和数据效率。

Result: 实验表明，SafeGrad在各种大语言模型和数据集上提供了最先进的防御，即使在高有害比例下也能保持安全性和任务性能。

Conclusion: SafeGrad有效解决了安全微调中的冲突梯度问题，实现了用户任务性能与安全对齐的平衡。

Abstract: Fine-tuning-as-a-Service introduces a critical vulnerability where a few
malicious examples mixed into the user's fine-tuning dataset can compromise the
safety alignment of Large Language Models (LLMs). While a recognized paradigm
frames safe fine-tuning as a multi-objective optimization problem balancing
user task performance with safety alignment, we find existing solutions are
critically sensitive to the harmful ratio, with defenses degrading sharply as
harmful ratio increases. We diagnose that this failure stems from conflicting
gradients, where the user-task update directly undermines the safety objective.
To resolve this, we propose SafeGrad, a novel method that employs gradient
surgery. When a conflict is detected, SafeGrad nullifies the harmful component
of the user-task gradient by projecting it onto the orthogonal plane of the
alignment gradient, allowing the model to learn the user's task without
sacrificing safety. To further enhance robustness and data efficiency, we
employ a KL-divergence alignment loss that learns the rich, distributional
safety profile of the well-aligned foundation model. Extensive experiments show
that SafeGrad provides state-of-the-art defense across various LLMs and
datasets, maintaining robust safety even at high harmful ratios without
compromising task fidelity.

</details>


### [32] [Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models](https://arxiv.org/abs/2508.07173)
*Leyi Pan,Zheyu Fu,Yunpeng Zhai,Shuchang Tao,Sheng Guan,Shiyu Huang,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Felix Henry,Lijie Wen,Aiwei Liu*

Main category: cs.CL

TL;DR: Omni-SafetyBench是首个针对全模态大语言模型（OLLMs）安全评估的综合基准，填补了现有评估工具的空白，揭示了OLLMs在复杂多模态输入下的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法评估OLLMs在音频-视觉联合输入或跨模态安全一致性方面的表现，亟需专用工具。

Method: 提出Omni-SafetyBench基准，包含24种模态组合和972个样本，并设计Safety-score和CMSC-score两种指标。

Result: 评估显示OLLMs在安全性和一致性上表现不佳，复杂输入下防御能力下降，部分模态得分极低。

Conclusion: 研究强调了提升OLLM安全性的紧迫性，为未来改进提供了基础。

Abstract: The rise of Omni-modal Large Language Models (OLLMs), which integrate visual
and auditory processing with text, necessitates robust safety evaluations to
mitigate harmful outputs. However, no dedicated benchmarks currently exist for
OLLMs, and prior benchmarks designed for other LLMs lack the ability to assess
safety performance under audio-visual joint inputs or cross-modal safety
consistency. To fill this gap, we introduce Omni-SafetyBench, the first
comprehensive parallel benchmark for OLLM safety evaluation, featuring 24
modality combinations and variations with 972 samples each, including dedicated
audio-visual harm cases. Considering OLLMs' comprehension challenges with
complex omni-modal inputs and the need for cross-modal consistency evaluation,
we propose tailored metrics: a Safety-score based on conditional Attack Success
Rate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and
a Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency
across modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals
critical vulnerabilities: (1) no model excels in both overall safety and
consistency, with only 3 models achieving over 0.6 in both metrics and top
performer scoring around 0.8; (2) safety defenses weaken with complex inputs,
especially audio-visual joints; (3) severe weaknesses persist, with some models
scoring as low as 0.14 on specific modalities. Our benchmark and metrics
highlight urgent needs for enhanced OLLM safety, providing a foundation for
future improvements.

</details>


### [33] [Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback](https://arxiv.org/abs/2508.07178)
*Kejin Liu,Junhong Lian,Xiang Ao,Ningtao Wang,Xing Fu,Yu Cheng,Weiqiang Wang,Xinyu Liu*

Main category: cs.CL

TL;DR: 论文提出了一种通过去噪隐式反馈中的虚假兴趣的个性化标题生成框架（PHG-DIF），解决了现有方法忽视点击噪声的问题，显著提升了标题质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效处理历史点击流中的个性化无关噪声，导致生成的标题偏离用户真实兴趣。

Method: PHG-DIF采用双阶段过滤去除噪声（如短停留时间和异常点击爆发），并通过多级时间融合动态建模用户兴趣。

Result: 实验表明PHG-DIF显著减轻了点击噪声的负面影响，在DT-PENS数据集上达到SOTA效果。

Conclusion: PHG-DIF通过去噪和动态兴趣建模，显著提升了个性化标题生成的准确性和质量。

Abstract: Accurate personalized headline generation hinges on precisely capturing user
interests from historical behaviors. However, existing methods neglect
personalized-irrelevant click noise in entire historical clickstreams, which
may lead to hallucinated headlines that deviate from genuine user preferences.
In this paper, we reveal the detrimental impact of click noise on personalized
generation quality through rigorous analysis in both user and news dimensions.
Based on these insights, we propose a novel Personalized Headline Generation
framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).
PHG-DIF first employs dual-stage filtering to effectively remove clickstream
noise, identified by short dwell times and abnormal click bursts, and then
leverages multi-level temporal fusion to dynamically model users' evolving and
multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a
new benchmark dataset comprising the click behavior of 1,000 carefully curated
users and nearly 10,000 annotated personalized headlines with historical dwell
time annotations. Extensive experiments demonstrate that PHG-DIF substantially
mitigates the adverse effects of click noise and significantly improves
headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our
framework implementation and dataset are available at
https://github.com/liukejin-up/PHG-DIF.

</details>


### [34] [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)
*Jiaqi Yin,Yi-Wei Chen,Meng-Lung Lee,Xiya Liu*

Main category: cs.CL

TL;DR: 提出了一种自动化提取多语言企业数据管道脚本中细粒度模式谱系的新框架，解决了语义漂移问题，并通过实验验证了模型规模和提示技术对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 企业数据管道中的语义漂移问题影响了数据可重现性和治理，降低了检索增强生成（RAG）和文本到SQL系统的实用性。

Method: 提出了一种框架，从多语言脚本中提取源模式、源表、转换逻辑和聚合操作，并引入SLiCE指标评估谱系质量。

Result: 实验表明，模式谱系提取性能随模型规模和提示技术的提升而提高，32B开源模型在标准提示下性能接近GPT系列。

Conclusion: 该方法为实际应用中部署模式感知代理提供了一种可扩展且经济的解决方案。

Abstract: Enterprise data pipelines, characterized by complex transformations across
multiple programming languages, often cause a semantic disconnect between
original metadata and downstream data. This "semantic drift" compromises data
reproducibility and governance, and impairs the utility of services like
retrieval-augmented generation (RAG) and text-to-SQL systems. To address this,
a novel framework is proposed for the automated extraction of fine-grained
schema lineage from multilingual enterprise pipeline scripts. This method
identifies four key components: source schemas, source tables, transformation
logic, and aggregation operations, creating a standardized representation of
data transformations. For the rigorous evaluation of lineage quality, this
paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that
assesses both structural correctness and semantic fidelity. A new benchmark is
also presented, comprising 1,700 manually annotated lineages from real-world
industrial scripts. Experiments were conducted with 12 language models, from
1.3B to 32B small language models (SLMs) to large language models (LLMs) like
GPT-4o and GPT-4.1. The results demonstrate that the performance of schema
lineage extraction scales with model size and the sophistication of prompting
techniques. Specially, a 32B open-source model, using a single reasoning trace,
can achieve performance comparable to the GPT series under standard prompting.
This finding suggests a scalable and economical approach for deploying
schema-aware agents in practical applications.

</details>


### [35] [DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention](https://arxiv.org/abs/2508.07185)
*Kabir Khan,Priya Sharma,Arjun Mehta,Neha Gupta,Ravi Narayanan*

Main category: cs.CL

TL;DR: DySK-Attn框架通过动态知识图谱和稀疏注意力机制，使大语言模型能够高效整合实时知识，显著提升事实准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型知识静态化、更新成本高的问题，同时避免现有知识编辑技术的副作用。

Method: 结合动态知识图谱和稀疏知识注意力机制，实现粗到细的搜索，高效聚焦相关子集。

Result: 在时效性问答任务中，DySK-Attn在事实准确性和计算效率上显著优于基线方法。

Conclusion: DySK-Attn为构建实时更新的大语言模型提供了可扩展且高效的解决方案。

Abstract: Large Language Models (LLMs) suffer from a critical limitation: their
knowledge is static and quickly becomes outdated. Retraining these massive
models is computationally prohibitive, while existing knowledge editing
techniques can be slow and may introduce unforeseen side effects. To address
this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently
integrate real-time knowledge from a dynamic external source. Our approach
synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated
instantaneously. The core of our framework is a sparse knowledge attention
mechanism, which allows the LLM to perform a coarse-to-fine grained search,
efficiently identifying and focusing on a small, highly relevant subset of
facts from the vast KG. This mechanism avoids the high computational cost of
dense attention over the entire knowledge base and mitigates noise from
irrelevant information. We demonstrate through extensive experiments on
time-sensitive question-answering tasks that DySK-Attn significantly
outperforms strong baselines, including standard Retrieval-Augmented Generation
(RAG) and model editing techniques, in both factual accuracy for updated
knowledge and computational efficiency. Our framework offers a scalable and
effective solution for building LLMs that can stay current with the
ever-changing world.

</details>


### [36] [Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment](https://arxiv.org/abs/2508.07195)
*Yanru Sun,Emadeldeen Eldele,Zongxia Xie,Yucheng Wang,Wenzhe Niu,Qinghua Hu,Chee Keong Kwoh,Min Wu*

Main category: cs.CL

TL;DR: TALON框架通过建模时间异质性和语义对齐，提升LLM在时间序列预测中的表现，平均MSE提升11%。


<details>
  <summary>Details</summary>
Motivation: 解决LLM直接应用于时间序列预测时的异质性问题和模态差距。

Method: 设计异构时间编码器和语义对齐模块，将时间序列特征与LLM兼容表示对齐。

Result: 在七个真实世界基准测试中表现优异，平均MSE提升11%。

Conclusion: TALON通过模式感知和语义感知设计，有效适应LLM于时间序列预测。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in natural language processing due to their strong generalization
and sequence modeling capabilities. However, their direct application to time
series forecasting remains challenging due to two fundamental issues: the
inherent heterogeneity of temporal patterns and the modality gap between
continuous numerical signals and discrete language representations. In this
work, we propose TALON, a unified framework that enhances LLM-based forecasting
by modeling temporal heterogeneity and enforcing semantic alignment.
Specifically, we design a Heterogeneous Temporal Encoder that partitions
multivariate time series into structurally coherent segments, enabling
localized expert modeling across diverse temporal patterns. To bridge the
modality gap, we introduce a Semantic Alignment Module that aligns temporal
features with LLM-compatible representations, enabling effective integration of
time series into language-based models while eliminating the need for
handcrafted prompts during inference. Extensive experiments on seven real-world
benchmarks demonstrate that TALON achieves superior performance across all
datasets, with average MSE improvements of up to 11\% over recent
state-of-the-art methods. These results underscore the effectiveness of
incorporating both pattern-aware and semantic-aware designs when adapting LLMs
for time series forecasting. The code is available at:
https://github.com/syrGitHub/TALON.

</details>


### [37] [Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model](https://arxiv.org/abs/2508.07209)
*Chaoqun Cui,Siyuan Li,Kunkun Ma,Caiyan Jia*

Main category: cs.CL

TL;DR: 论文提出了一种名为PEP的继续预训练策略，通过预测帖子之间的关系来改进PLMs在社交媒体谣言检测中的表现，并发布了相关数据集和模型SoLM。


<details>
  <summary>Details</summary>
Motivation: PLMs在社交媒体任务（如谣言检测）中表现不佳，原因包括预训练语料与社交媒体文本不匹配、社交符号处理不足以及预训练任务不适合建模传播结构中的用户互动。

Method: 提出PEP策略，通过预测帖子间的关系（如根、分支、父级）来捕捉立场和情感的交互，同时发布了TwitterCorpus和两个未标注的传播结构数据集。

Result: PEP显著提升了PLMs在谣言检测中的性能，基准模型准确率提高了1.0-3.7%，甚至在某些数据集上超越了现有最优方法。模型SoLM也取得了竞争性结果。

Conclusion: PEP策略有效提升了PLMs在社交媒体任务中的表现，尤其是在学习判别性帖子交互特征方面。

Abstract: Pretrained Language Models (PLMs) have excelled in various Natural Language
Processing tasks, benefiting from large-scale pretraining and self-attention
mechanism's ability to capture long-range dependencies. However, their
performance on social media application tasks like rumor detection remains
suboptimal. We attribute this to mismatches between pretraining corpora and
social texts, inadequate handling of unique social symbols, and pretraining
tasks ill-suited for modeling user engagements implicit in propagation
structures. To address these issues, we propose a continue pretraining strategy
called Post Engagement Prediction (PEP) to infuse information from propagation
structures into PLMs. PEP makes models to predict root, branch, and parent
relations between posts, capturing interactions of stance and sentiment crucial
for rumor detection. We also curate and release large-scale Twitter corpus:
TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with
propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP
strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments
demonstrate PEP significantly boosts rumor detection performance across
universal and social media PLMs, even in few-shot scenarios. On benchmark
datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it
to outperform current state-of-the-art methods on multiple datasets. SoLM
alone, without high-level modules, also achieves competitive results,
highlighting the strategy's effectiveness in learning discriminative post
interaction features.

</details>


### [38] [How Does a Deep Neural Network Look at Lexical Stress?](https://arxiv.org/abs/2508.07229)
*Itai Allouche,Itay Asael,Rotem Rousso,Vered Dassa,Ann Bradlow,Seung-Eun Kim,Matthew Goldrick,Joseph Keshet*

Main category: cs.CL

TL;DR: 该论文研究了神经网络在词汇重音预测中的决策依据，通过CNN和LRP技术揭示了重音音节的光谱特性对预测的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络在语音处理中的决策依据，特别是在词汇重音预测中的可解释性问题。

Method: 使用自动构建的英语双音节词数据集，训练多个CNN架构预测重音位置，并应用LRP技术分析模型决策。

Result: 模型在测试数据上达到92%的准确率，LRP显示重音音节的频谱特性（如第一和第二共振峰）对预测影响最大。

Conclusion: 深度学习能够从自然数据中学习分布式的重音线索，扩展了传统基于严格控制刺激的语音学研究。

Abstract: Despite their success in speech processing, neural networks often operate as
black boxes, prompting the question: what informs their decisions, and how can
we interpret them? This work examines this issue in the context of lexical
stress. A dataset of English disyllabic words was automatically constructed
from read and spontaneous speech. Several Convolutional Neural Network (CNN)
architectures were trained to predict stress position from a spectrographic
representation of disyllabic words lacking minimal stress pairs (e.g., initial
stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out
test data. Layerwise Relevance Propagation (LRP), a technique for CNN
interpretability analysis, revealed that predictions for held-out minimal pairs
(PROtest vs. proTEST ) were most strongly influenced by information in stressed
versus unstressed syllables, particularly the spectral properties of stressed
vowels. However, the classifiers also attended to information throughout the
word. A feature-specific relevance analysis is proposed, and its results
suggest that our best-performing classifier is strongly influenced by the
stressed vowel's first and second formants, with some evidence that its pitch
and third formant also contribute. These results reveal deep learning's ability
to acquire distributed cues to stress from naturally occurring data, extending
traditional phonetic work based around highly controlled stimuli.

</details>


### [39] [Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition](https://arxiv.org/abs/2508.07248)
*Zhe Ren*

Main category: cs.CL

TL;DR: 论文提出了一种基于提示调优和记忆演示模板的方法，解决了少样本持续学习命名实体识别任务中的蒸馏困境。


<details>
  <summary>Details</summary>
Motivation: 少样本持续学习命名实体识别任务中，新类别实体稀缺导致模型泛化能力差，且旧类别信息不足阻碍知识蒸馏，形成少样本蒸馏困境。

Method: 设计了可扩展的锚词导向提示调优（APT）范式，结合记忆演示模板（MDT）提供历史任务样本，增强少样本场景性能。

Result: 实验表明，该方法在少样本持续学习命名实体识别任务中表现优异。

Conclusion: 通过提示调优和记忆演示模板策略，有效解决了少样本蒸馏困境，提升了模型性能。

Abstract: Knowledge distillation has been successfully applied to Continual Learning
Named Entity Recognition (CLNER) tasks, by using a teacher model trained on
old-class data to distill old-class entities present in new-class data as a
form of regularization, thereby avoiding catastrophic forgetting. However, in
Few-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it
difficult for the trained model to generalize during inference. More
critically, the lack of old-class entity information hinders the distillation
of old knowledge, causing the model to fall into what we refer to as the
Few-Shot Distillation Dilemma. In this work, we address the above challenges
through a prompt tuning paradigm and memory demonstration template strategy.
Specifically, we designed an expandable Anchor words-oriented Prompt Tuning
(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby
enhancing performance in few-shot scenarios. Additionally, we incorporated
Memory Demonstration Templates (MDT) into each training instance to provide
replay samples from previous tasks, which not only avoids the Few-Shot
Distillation Dilemma but also promotes in-context learning. Experiments show
that our approach achieves competitive performances on FS-CLNER.

</details>


### [40] [The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation](https://arxiv.org/abs/2508.07262)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: 扩展了二维动态发音模型DYNARTmo，通过集成三维腭穹内部表示，从中矢舌轮廓估计舌-腭接触区域。


<details>
  <summary>Details</summary>
Motivation: 改进发音模型以更准确地模拟舌-腭接触，支持语音科学教育和言语治疗。

Method: 实现两种腭穹几何模型（半椭圆和余弦曲线），计算侧向接触点，生成类似电子腭图的2D+可视化。

Result: 增强模型支持三种同步视图（矢状、声门和腭部），适用于静态和动态发音展示。

Conclusion: 未来工作包括添加面部（唇）视图和实现发音-声学合成，以定量评估模型真实性。

Abstract: This paper describes an extension of the two-dimensional dynamic articulatory
model DYNARTmo by integrating an internal three-dimensional representation of
the palatal dome to estimate tongue-palate contact areas from midsagittal
tongue contours. Two alternative dome geometries - a half-ellipse and a cosine
based profile - are implemented to model lateral curvature in the coronal
plane. Using these geometries, lateral contact points are analytically computed
for each anterior-posterior position, enabling the generation of
electropalatography-like visualizations within the 2D+ framework. The enhanced
model supports three synchronized views (sagittal, glottal, and palatal) for
static and dynamic (animated) articulation displays, suitable for speech
science education and speech therapy. Future work includes adding a facial
(lip) view and implementing articulatory-to-acoustic synthesis to
quantitatively evaluate model realism.

</details>


### [41] [Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models](https://arxiv.org/abs/2508.07273)
*Qiongqiong Wang,Hardik B. Sailor,Jeremy H. M. Wong,Tianchi Liu,Shuo Sun,Wenyu Zhang,Muhammad Huzaifah,Nancy Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: 论文提出两种方法（显式和隐式）来改进语音语言模型在共情推理中的表现，通过结合上下文和副语言信息，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型在共情推理方面表现不足，主要因缺乏结合上下文和副语言信息的训练数据。

Method: 1. 显式方法：直接提供副语言元数据（如情感标注）给模型；2. 隐式方法：自动生成结合情感标注和语音转录的训练问答对。

Result: 隐式方法使性能提升38.41%，显隐结合后达46.02%，验证了LLM评判的可靠性。

Conclusion: 结合上下文和副语言信息能有效提升语音语言模型的共情推理能力。

Abstract: Current large speech language models (Speech-LLMs) often exhibit limitations
in empathetic reasoning, primarily due to the absence of training datasets that
integrate both contextual content and paralinguistic cues. In this work, we
propose two approaches to incorporate contextual paralinguistic information
into model training: (1) an explicit method that provides paralinguistic
metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit
method that automatically generates novel training question-answer (QA) pairs
using both categorical and dimensional emotion annotations alongside speech
transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41%
on a human-annotated QA benchmark, reaching 46.02% when combined with the
explicit approach, showing effectiveness in contextual paralinguistic
understanding. We also validate the LLM judge by demonstrating its correlation
with classification metrics, providing support for its reliability.

</details>


### [42] [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)
*Vasudha Varadarajan,Hui Xu,Rebecca Astrid Boehme,Mariam Marlan Mirstrom,Sverker Sikstrom,H. Andrew Schwartz*

Main category: cs.CL

TL;DR: MAQuA是一种自适应提问框架，通过结合多结果建模、项目反应理论和因子分析，优化心理健康筛查问题选择，显著减少问题数量并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在心理健康评估中因频繁提问导致的用户负担和效率低下问题。

Method: 结合多结果建模、项目反应理论（IRT）和因子分析，动态选择最具信息量的问题。

Result: MAQuA将评估问题数量减少50-87%，在抑郁和饮食障碍等领域表现稳健。

Conclusion: MAQuA是一种高效、可扩展的心理健康筛查工具，适合临床工作流。

Abstract: Recent advances in large language models (LLMs) offer new opportunities for
scalable, interactive mental health assessment, but excessive querying by LLMs
burdens users and is inefficient for real-world screening across
transdiagnostic symptom profiles. We introduce MAQuA, an adaptive
question-asking framework for simultaneous, multidimensional mental health
screening. Combining multi-outcome modeling on language responses with item
response theory (IRT) and factor analysis, MAQuA selects the questions with
most informative responses across multiple dimensions at each turn to optimize
diagnostic information, improving accuracy and potentially reducing response
burden. Empirical results on a novel dataset reveal that MAQuA reduces the
number of assessment questions required for score stabilization by 50-87%
compared to random ordering (e.g., achieving stable depression scores with 71%
fewer questions and eating disorder scores with 85% fewer questions). MAQuA
demonstrates robust performance across both internalizing (depression, anxiety)
and externalizing (substance use, eating disorder) domains, with early stopping
strategies further reducing patient time and burden. These findings position
MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive
mental health screening, advancing the integration of LLM-based agents into
real-world clinical workflows.

</details>


### [43] ["Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas](https://arxiv.org/abs/2508.07284)
*Junchen Ding,Penghao Jiang,Zihao Xu,Ziqi Ding,Yichen Zhu,Jiaojiao Jiang,Yuekang Li*

Main category: cs.CL

TL;DR: 本研究对14种大型语言模型（LLM）在27种电车问题场景中的道德推理进行了评估，发现模型在不同道德哲学框架下表现差异显著，尤其是在利他主义、公平和美德伦理框架下表现较好。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地参与道德敏感决策，理解其道德推理过程变得至关重要。

Method: 采用因子提示协议，收集了3,780个二元决策和自然语言解释，分析了决策果断性、解释一致性、公众道德对齐及对无关伦理线索的敏感性。

Result: 发现增强推理模型更果断且解释更结构化，但未必更符合人类共识；在利他主义等框架下表现较好，而在亲属关系或自我利益框架下表现不佳。

Conclusion: 建议将道德推理作为LLM对齐的主要维度，并呼吁建立标准化基准以评估其决策过程和原因。

Abstract: As large language models (LLMs) increasingly mediate ethically sensitive
decisions, understanding their moral reasoning processes becomes imperative.
This study presents a comprehensive empirical evaluation of 14 leading LLMs,
both reasoning enabled and general purpose, across 27 diverse trolley problem
scenarios, framed by ten moral philosophies, including utilitarianism,
deontology, and altruism. Using a factorial prompting protocol, we elicited
3,780 binary decisions and natural language justifications, enabling analysis
along axes of decisional assertiveness, explanation answer consistency, public
moral alignment, and sensitivity to ethically irrelevant cues. Our findings
reveal significant variability across ethical frames and model types: reasoning
enhanced models demonstrate greater decisiveness and structured justifications,
yet do not always align better with human consensus. Notably, "sweet zones"
emerge in altruistic, fairness, and virtue ethics framings, where models
achieve a balance of high intervention rates, low explanation conflict, and
minimal divergence from aggregated human judgments. However, models diverge
under frames emphasizing kinship, legality, or self interest, often producing
ethically controversial outcomes. These patterns suggest that moral prompting
is not only a behavioral modifier but also a diagnostic tool for uncovering
latent alignment philosophies across providers. We advocate for moral reasoning
to become a primary axis in LLM alignment, calling for standardized benchmarks
that evaluate not just what LLMs decide, but how and why.

</details>


### [44] [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)
*Jian Chen,Jinbao Tian,Yankui Li,Zhou Li*

Main category: cs.CL

TL;DR: ARCE提出了一种通过LLM生成简单解释（Cote）来增强RoBERTa模型的方法，显著提升了AEC领域NER任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决AEC领域NER任务中预训练模型因领域术语和复杂关系而性能受限的问题，避免高成本的人工标注。

Method: 利用LLM生成简单解释（Cote），用于增量预训练RoBERTa模型，再进行下游任务微调。

Result: 在AEC基准数据集上达到77.20%的Macro-F1分数，表现优于复杂角色推理。

Conclusion: 简单解释性知识比复杂推理更有效，ARCE为领域适应提供了一种高效方法。

Abstract: Accurate information extraction from specialized texts is a critical
challenge, particularly for named entity recognition (NER) in the architecture,
engineering, and construction (AEC) domain to support automated rule checking
(ARC). The performance of standard pre-trained models is often constrained by
the domain gap, as they struggle to interpret the specialized terminology and
complex relational contexts inherent in AEC texts. Although this issue can be
mitigated by further pre-training on large, human-curated domain corpora, as
exemplified by methods like ARCBERT, this approach is both labor-intensive and
cost-prohibitive. Consequently, leveraging large language models (LLMs) for
automated knowledge generation has emerged as a promising alternative. However,
the optimal strategy for generating knowledge that can genuinely enhance
smaller, efficient models remains an open question. To address this, we propose
ARCE (augmented RoBERTa with contextualized elucidations), a novel approach
that systematically explores and optimizes this generation process. ARCE
employs an LLM to first generate a corpus of simple, direct explanations, which
we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa
model prior to its fine-tuning on the downstream task. Our extensive
experiments show that ARCE establishes a new state-of-the-art on a benchmark
AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a
key finding: simple, explanation-based knowledge proves surprisingly more
effective than complex, role-based rationales for this task. The code is
publicly available at:https://github.com/nxcc-lab/ARCE.

</details>


### [45] [CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation](https://arxiv.org/abs/2508.07295)
*Yexing Du,Kaiyuan Liu,Youcheng Pan,Zheng Chu,Bo Yang,Xiaocheng Feng,Yang Xiang,Ming Liu*

Main category: cs.CL

TL;DR: 提出了一个跨语言和跨模态的事实性基准（CCFQA），用于评估多模态大语言模型（MLLMs）在多语言语音输入中的表现。实验表明当前模型仍有挑战，并提出了一种少样本迁移学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注英语文本或视觉模态，缺乏对多语言语音输入的事实性评估，因此需要填补这一空白。

Method: 设计了包含8种语言的平行语音-文本事实问题基准（CCFQA），并提出少样本迁移学习策略，将英语问答能力迁移到多语言语音问答任务。

Result: 实验显示当前MLLMs在CCFQA上表现不佳，但提出的迁移学习方法仅用5样本训练即达到与GPT-4o-mini-Audio竞争的性能。

Conclusion: CCFQA为促进MLLMs在多语言语音理解能力的发展提供了基础资源，代码和数据集已开源。

Abstract: As Large Language Models (LLMs) are increasingly popularized in the
multilingual world, ensuring hallucination-free factuality becomes markedly
crucial. However, existing benchmarks for evaluating the reliability of
Multimodal Large Language Models (MLLMs) predominantly focus on textual or
visual modalities with a primary emphasis on English, which creates a gap in
evaluation when processing multilingual input, especially in speech. To bridge
this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal
\textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA
benchmark contains parallel speech-text factual questions across 8 languages,
designed to systematically evaluate MLLMs' cross-lingual and cross-modal
factuality capabilities. Our experimental results demonstrate that current
MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we
propose a few-shot transfer learning strategy that effectively transfers the
Question Answering (QA) capabilities of LLMs in English to multilingual Spoken
Question Answering (SQA) tasks, achieving competitive performance with
GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a
foundational research resource to promote the development of MLLMs with more
robust and reliable speech understanding capabilities. Our code and dataset are
available at https://github.com/yxduir/ccfqa.

</details>


### [46] [HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways](https://arxiv.org/abs/2508.07308)
*Cristian Cosentino,Annamaria Defilippo,Marco Dossena,Christopher Irwin,Sara Joubbi,Pietro Liò*

Main category: cs.CL

TL;DR: HealthBranches是一个用于医疗问答（Q&A）的新基准数据集，旨在评估大型语言模型（LLMs）的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 为开发更可信、可解释且临床可靠的LLMs提供基础，同时作为教育资源。

Method: 通过半自动化流程将医学来源的决策路径转化为真实患者案例，包含问题和答案。

Result: 覆盖17个医疗主题的4,063个案例，支持开放式和多项选择题形式，并提供完整的推理路径。

Conclusion: HealthBranches为高风险领域中LLMs的多步推理能力评估提供了结构化支持。

Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering
(Q&A), specifically designed to evaluate complex reasoning in Large Language
Models (LLMs). This dataset is generated through a semi-automated pipeline that
transforms explicit decision pathways from medical source into realistic
patient cases with associated questions and answers. Covering 4,063 case
studies across 17 healthcare topics, each data point is based on clinically
validated reasoning chains. HealthBranches supports both open-ended and
multiple-choice question formats and uniquely includes the full reasoning path
for each Q&A. Its structured design enables robust evaluation of LLMs'
multi-step inference capabilities, including their performance in structured
Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a
foundation for the development of more trustworthy, interpretable, and
clinically reliable LLMs in high-stakes domains while also serving as a
valuable resource for educational purposes.

</details>


### [47] [ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering](https://arxiv.org/abs/2508.07321)
*Shubhra Ghosh,Abhilekh Borah,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 论文提出了一种名为ObfusQAte的新技术，并基于此构建了ObfusQA框架，用于评估大语言模型（LLMs）在面对混淆问题时的鲁棒性。研究发现LLMs在复杂混淆问题下容易失败或产生幻觉回答。


<details>
  <summary>Details</summary>
Motivation: 现有研究未测试LLMs在面对混淆问题时的表现，因此需要系统评估其局限性。

Method: 提出ObfusQAte技术，并构建ObfusQA框架，通过三个维度（命名实体间接、干扰项间接、上下文过载）评估LLMs。

Result: LLMs在面对复杂混淆问题时表现不佳，容易失败或产生幻觉回答。

Conclusion: ObfusQA为评估LLM鲁棒性提供了全面基准，并公开了ObfusQAte以促进相关研究。

Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly
contributed to the development of equitable AI systems capable of factual
question-answering (QA). However, no known study tests the LLMs' robustness
when presented with obfuscated versions of questions. To systematically
evaluate these limitations, we propose a novel technique, ObfusQAte and,
leveraging the same, introduce ObfusQA, a comprehensive, first of its kind,
framework with multi-tiered obfuscation levels designed to examine LLM
capabilities across three distinct dimensions: (i) Named-Entity Indirection,
(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these
fine-grained distinctions in language, ObfusQA provides a comprehensive
benchmark for evaluating LLM robustness and adaptability. Our study observes
that LLMs exhibit a tendency to fail or generate hallucinated responses when
confronted with these increasingly nuanced variations. To foster research in
this direction, we make ObfusQAte publicly available.

</details>


### [48] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)
*Joseph T. Colonel,Baihan Lin*

Main category: cs.CL

TL;DR: ThemeClouds是一种基于大型语言模型（LLM）的可视化工具，用于生成主题词云，解决了传统频率词云在对话语境中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统频率词云在对话语境中效果不佳，无法捕捉语义关联和忽略填充词，限制了其在早期分析中的实用性。

Method: ThemeClouds利用LLM识别语料库中的概念级主题，并统计每个主题被多少独特参与者提及，生成基于提及广度的词云。

Result: 在用户研究中，ThemeClouds比频率词云和主题建模基线（如LDA、BERTopic）更能揭示可操作的设备问题。

Conclusion: ThemeClouds为定性工作流提供了透明度和控制，同时探讨了LLM辅助的设计权衡和交互分析的机会。

Abstract: Word clouds are a common way to summarize qualitative interviews, yet
traditional frequency-based methods often fail in conversational contexts: they
surface filler words, ignore paraphrase, and fragment semantically related
ideas. This limits their usefulness in early-stage analysis, when researchers
need fast, interpretable overviews of what participant actually said. We
introduce ThemeClouds, an open-source visualization tool that uses large
language models (LLMs) to generate thematic, participant-weighted word clouds
from dialogue transcripts. The system prompts an LLM to identify concept-level
themes across a corpus and then counts how many unique participants mention
each topic, yielding a visualization grounded in breadth of mention rather than
raw term frequency. Researchers can customize prompts and visualization
parameters, providing transparency and control. Using interviews from a user
study comparing five recording-device configurations (31 participants; 155
transcripts, Whisper ASR), our approach surfaces more actionable device
concerns than frequency clouds and topic-modeling baselines (e.g., LDA,
BERTopic). We discuss design trade-offs for integrating LLM assistance into
qualitative workflows, implications for interpretability and researcher agency,
and opportunities for interactive analyses such as per-condition contrasts
(``diff clouds'').

</details>


### [49] [Strategies of Code-switching in Human-Machine Dialogs](https://arxiv.org/abs/2508.07325)
*Dean Geckt,Melinda Fricke,Shuly Wintner*

Main category: cs.CL

TL;DR: 研究探讨了多语言者在代码切换中的行为特征，通过开发能完成地图任务的聊天机器人，验证了代码切换策略对任务完成的影响。


<details>
  <summary>Details</summary>
Motivation: 理解多语言者在代码切换中的行为特征，并探索相关技术在双语研究中的应用潜力。

Method: 开发了一个能完成地图任务的聊天机器人，采用不同的代码切换策略进行实验。

Result: 参与者喜欢可预测的代码切换行为，随机或不规范的切换会降低任务完成度和体验。

Conclusion: 未充分开发的多语言技术可能带来负面影响，但也展示了其在双语研究中的潜力。

Abstract: Most people are multilingual, and most multilinguals code-switch, yet the
characteristics of code-switched language are not fully understood. We
developed a chatbot capable of completing a Map Task with human participants
using code-switched Spanish and English. In two experiments, we prompted the
bot to code-switch according to different strategies, examining (1) the
feasibility of such experiments for investigating bilingual language use, and
(2) whether participants would be sensitive to variations in discourse and
grammatical patterns. Participants generally enjoyed code-switching with our
bot as long as it produced predictable code-switching behavior; when
code-switching was random or ungrammatical (as when producing unattested
incongruent mixed-language noun phrases, such as `la fork'), participants
enjoyed the task less and were less successful at completing it. These results
underscore the potential downsides of deploying insufficiently developed
multilingual language technology, while also illustrating the promise of such
technology for conducting research on bilingual language use.

</details>


### [50] [Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance](https://arxiv.org/abs/2508.07375)
*Wenqian Cui,Lei Zhu,Xiaohui Li,Zhihan Guo,Haoli Bai,Lu Hou,Irwin King*

Main category: cs.CL

TL;DR: FD-SLMs旨在实现实时自然语音交互，但面临语音序列长和数据质量低的挑战。TurnGuide通过动态分段和文本引导解决了这些问题。


<details>
  <summary>Details</summary>
Motivation: 解决FD-SLMs在语音交互中因长序列和低质量数据导致的对话能力下降问题。

Method: 提出TurnGuide，模仿人类对话规划，动态分段语音并生成文本引导。

Result: 实验表明TurnGuide显著提升FD-SLMs的对话能力，生成语义连贯且自然的语音。

Conclusion: TurnGuide有效解决了FD-SLMs的挑战，提升了语音交互的自然性和连贯性。

Abstract: Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation
models designed to enable natural, real-time spoken interactions by modeling
complex conversational dynamics such as interruptions, backchannels, and
overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world
double-channel conversational data to capture nuanced two-speaker dialogue
patterns for human-like interactions. However, they face a critical challenge
-- their conversational abilities often degrade compared to pure-text
conversation due to prolonged speech sequences and limited high-quality spoken
dialogue data. While text-guided speech generation could mitigate these issues,
it suffers from timing and length issues when integrating textual guidance into
double-channel audio streams, disrupting the precise time alignment essential
for natural interactions. To address these challenges, we propose TurnGuide, a
novel planning-inspired approach that mimics human conversational planning by
dynamically segmenting assistant speech into dialogue turns and generating
turn-level text guidance before speech output, which effectively resolves both
insertion timing and length challenges. Extensive experiments demonstrate our
approach significantly improves e2e FD-SLMs' conversational abilities, enabling
them to generate semantically meaningful and coherent speech while maintaining
natural conversational flow. Demos are available at
https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at
https://github.com/dreamtheater123/TurnGuide.

</details>


### [51] [Grounding Multilingual Multimodal LLMs With Cultural Knowledge](https://arxiv.org/abs/2508.07414)
*Jean de Dieu Nyandwi,Yueqi Song,Simran Khanuja,Graham Neubig*

Main category: cs.CL

TL;DR: 提出了一种数据驱动的方法，通过文化知识图谱和多语言视觉问答数据，提升多模态大语言模型在低资源语言和长尾文化实体上的表现。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在高资源环境下表现良好，但在低资源语言和长尾文化实体上表现不佳的问题。

Method: 利用Wikidata知识图谱收集文化相关图像，生成合成多语言视觉问答数据，构建数据集CulturalGround，并训练开源模型CulturalPangea。

Result: CulturalPangea在文化聚焦的多语言多模态基准测试中表现最佳，平均提升5.0分，且不影响主流视觉语言任务。

Conclusion: 通过文化知识嵌入，显著缩小了多模态大语言模型的文化差距，为全球包容性多模态系统提供了可行路径。

Abstract: Multimodal Large Language Models excel in high-resource settings, but often
misinterpret long-tail cultural entities and underperform in low-resource
languages. To address this gap, we propose a data-centric approach that
directly grounds MLLMs in cultural knowledge. Leveraging a large scale
knowledge graph from Wikidata, we collect images that represent culturally
significant entities, and generate synthetic multilingual visual question
answering data. The resulting dataset, CulturalGround, comprises 22 million
high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.
We train an open-source MLLM CulturalPangea on CulturalGround, interleaving
standard multilingual instruction-tuning data to preserve general abilities.
CulturalPangea achieves state-of-the-art performance among open models on
various culture-focused multilingual multimodal benchmarks, outperforming prior
models by an average of 5.0 without degrading results on mainstream
vision-language tasks. Our findings show that our targeted, culturally grounded
approach could substantially narrow the cultural gap in MLLMs and offer a
practical path towards globally inclusive multimodal systems.

</details>


### [52] [Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs](https://arxiv.org/abs/2508.07434)
*Zhiyi Lyu,Jianguo Huang,Yanchen Deng,Steven Hoi,Bo An*

Main category: cs.CL

TL;DR: ReLoc是一个统一的局部搜索框架，通过逐步代码修订提高代码生成效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在代码生成中面临效率和可扩展性问题，构造树搜索方法存在树规模快速膨胀、高令牌消耗等问题，而改进方法则受限于无信息奖励信号和低效搜索策略。

Method: ReLoc通过四个关键组件（初始代码起草、邻域代码生成、候选评估和当前代码更新）实现逐步修订，并结合修订奖励模型指导搜索。

Result: 实验表明，ReLoc在多样代码生成任务中表现优异，显著优于构造树搜索和现有改进方法。

Conclusion: ReLoc通过局部搜索和修订奖励模型，有效解决了代码生成的效率和性能问题。

Abstract: Large Language Models (LLMs) with inference-time scaling techniques show
promise for code generation, yet face notable efficiency and scalability
challenges. Construction-based tree-search methods suffer from rapid growth in
tree size, high token consumption, and lack of anytime property. In contrast,
improvement-based methods offer better performance but often struggle with
uninformative reward signals and inefficient search strategies. In this work,
we propose \textbf{ReLoc}, a unified local search framework which effectively
performs step-by-step code revision. Specifically, ReLoc explores a series of
local revisions through four key algorithmic components: initial code drafting,
neighborhood code generation, candidate evaluation, and incumbent code
updating, each of which can be instantiated with specific decision rules to
realize different local search algorithms such as Hill Climbing (HC) or Genetic
Algorithm (GA). Furthermore, we develop a specialized revision reward model
that evaluates code quality based on revision distance to produce fine-grained
preferences that guide the local search toward more promising candidates.
Finally, our extensive experimental results demonstrate that our approach
achieves superior performance across diverse code generation tasks,
significantly outperforming both construction-based tree search as well as the
state-of-the-art improvement-based code generation methods.

</details>


### [53] [Positional Biases Shift as Inputs Approach Context Window Limits](https://arxiv.org/abs/2508.07479)
*Blerta Veseli,Julian Chibane,Mariya Toneva,Alexander Koller*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLMs）在长输入中存在位置偏差，如“迷失在中间”（LiM）效应，但该效应在输入占模型上下文窗口50%以下时最强。超过后，首因偏差减弱，而近因偏差保持稳定，最终表现为距离偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在长输入中的位置偏差（如LiM效应）的强度和表现条件，以改进长上下文任务和评估方法。

Method: 采用相对输入长度（相对于模型上下文窗口）进行综合分析。

Result: LiM效应在输入占窗口50%以下时最强；超过后，首因偏差减弱，近因偏差稳定，表现为距离偏差。检索是推理的前提，位置偏差主要源于检索。

Conclusion: 研究结果对长上下文任务、LLM基准设计和评估方法有重要启示。

Abstract: Large Language Models (LLMs) often struggle to use information across long
inputs effectively. Prior work has identified positional biases, such as the
Lost in the Middle (LiM) effect, where models perform better when information
appears at the beginning (primacy bias) or end (recency bias) of the input,
rather than in the middle. However, long-context studies have not consistently
replicated these effects, raising questions about their intensity and the
conditions under which they manifest. To address this, we conducted a
comprehensive analysis using relative rather than absolute input lengths,
defined with respect to each model's context window. Our findings reveal that
the LiM effect is strongest when inputs occupy up to 50% of a model's context
window. Beyond that, the primacy bias weakens, while recency bias remains
relatively stable. This effectively eliminates the LiM effect; instead, we
observe a distance-based bias, where model performance is better when relevant
information is closer to the end of the input. Furthermore, our results suggest
that successful retrieval is a prerequisite for reasoning in LLMs, and that the
observed positional biases in reasoning are largely inherited from retrieval.
These insights have implications for long-context tasks, the design of future
LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.

</details>


### [54] [ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models](https://arxiv.org/abs/2508.07484)
*Archchana Sindhujan,Shenbin Qian,Chan Chi Chun Matthew,Constantin Orasan,Diptesh Kanojia*

Main category: cs.CL

TL;DR: ALOPE框架通过层优化和动态加权策略提升LLM在机器翻译质量评估中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在跨语言质量评估任务中表现不佳，因其预训练目标与回归任务不匹配，且低资源语言数据不足。

Method: ALOPE结合低秩适配器和回归任务头，动态加权多层表示，并采用多头回归策略。

Result: ALOPE优于现有LLM-based QE方法，中间层表示更适合跨语言任务。

Conclusion: ALOPE为LLM-based QE提供了有效改进，并公开模型和代码以促进研究。

Abstract: Large Language Models (LLMs) have shown remarkable performance across a wide
range of natural language processing tasks. Quality Estimation (QE) for Machine
Translation (MT), which assesses the quality of a source-target pair without
relying on reference translations, remains a challenging cross-lingual task for
LLMs. The challenges stem from the inherent limitations of existing LLM-based
QE systems, which are pre-trained for causal language modelling rather than
regression-specific tasks, further elevated by the presence of low-resource
languages given pre-training data distribution. This paper introduces ALOPE, an
adaptive layer-optimization framework designed to enhance LLM-based QE by
restructuring Transformer representations through layer-wise adaptation for
improved regression-based prediction. Our framework integrates low-rank
adapters (LoRA) with regression task heads, leveraging selected pre-trained
Transformer layers for improved cross-lingual alignment. In addition to the
layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,
which adaptively combines representations from multiple layers, and multi-head
regression, which aggregates regression losses from multiple heads for QE. Our
framework shows improvements over various existing LLM-based QE approaches.
Empirical evidence suggests that intermediate Transformer layers in LLMs
provide contextual representations that are more aligned with the cross-lingual
nature of the QE task. We make resultant models and framework code publicly
available for further research, also allowing existing LLM-based MT frameworks
to be scaled with QE capabilities.

</details>


### [55] [Augmenting Bias Detection in LLMs Using Topological Data Analysis](https://arxiv.org/abs/2508.07516)
*Keshav Varadarajan,Tananun Songdechakraiwut*

Main category: cs.CL

TL;DR: 本文提出了一种基于拓扑数据分析的方法，用于识别GPT-2中导致对特定身份群体偏见的注意力头。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽能检测大型语言模型的偏见水平，但缺乏识别具体偏见来源的工具。

Method: 使用拓扑数据分析技术，定位GPT-2中与StereoSet数据集中身份群体偏见相关的注意力头。

Result: 发现偏见（如性别或职业）集中在某些热点注意力头中，并提出了一种度量方法。

Conclusion: 该方法可用于识别特定偏见来源，未来或有助于去偏见化大型语言模型。

Abstract: Recently, many bias detection methods have been proposed to determine the
level of bias a large language model captures. However, tests to identify which
parts of a large language model are responsible for bias towards specific
groups remain underdeveloped. In this study, we present a method using
topological data analysis to identify which heads in GPT-2 contribute to the
misrepresentation of identity groups present in the StereoSet dataset. We find
that biases for particular categories, such as gender or profession, are
concentrated in attention heads that act as hot spots. The metric we propose
can also be used to determine which heads capture bias for a specific group
within a bias category, and future work could extend this method to help
de-bias large language models.

</details>


### [56] [From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR](https://arxiv.org/abs/2508.07534)
*Jia Deng,Jie Chen,Zhipeng Chen,Daixuan Cheng,Fei Bai,Beichen Zhang,Yinqian Min,Yanzipeng Gao,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文系统研究了RLVR中的探索能力，包括探索空间塑造、熵-性能交换和RL性能优化，旨在为RLVR系统提供基础框架。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在增强LLMs推理能力时缺乏对探索行为的深入研究，RLVR利用规则反馈指导LLMs生成复杂推理链，但其探索机制尚未充分探索。

Method: 研究分为三部分：(1)探索空间塑造，量化LLMs能力边界；(2)熵-性能交换分析，涵盖训练阶段、实例和标记级别；(3)RL性能优化，将探索收益转化为改进。

Result: 通过统一已有见解与新实证证据，提出了RLVR探索能力的系统性分析框架。

Conclusion: 本研究为RLVR系统的进一步发展提供了理论基础和实证支持。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of large language
models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based
feedback to guide LLMs in generating and refining complex reasoning chains -- a
process critically dependent on effective exploration strategies. While prior
work has demonstrated RLVR's empirical success, the fundamental mechanisms
governing LLMs' exploration behaviors remain underexplored. This technical
report presents a systematic investigation of exploration capacities in RLVR,
covering four main aspects: (1) exploration space shaping, where we develop
quantitative metrics to characterize LLMs' capability boundaries; (2)
entropy-performance exchange, analyzed across training stages, individual
instances, and token-level patterns; and (3) RL performance optimization,
examining methods to effectively translate exploration gains into measurable
improvements. By unifying previously identified insights with new empirical
evidence, this work aims to provide a foundational framework for advancing RLVR
systems.

</details>


### [57] [IBPS: Indian Bail Prediction System](https://arxiv.org/abs/2508.07592)
*Puspesh Kumar Srivastava,Uddeshya Raj,Praveen Patel,/Shubham Kumar Nigam,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: IBPS是一个AI驱动的印度保释预测系统，旨在通过预测结果和生成法律依据来辅助保释决策，减少主观性和延迟。


<details>
  <summary>Details</summary>
Motivation: 印度法院保释决策存在主观性、延迟和不一致问题，导致大量未审判囚犯和社会不公，需数据驱动的解决方案。

Method: 利用150,430份高等法院保释判决数据集，通过参数高效技术微调大语言模型，结合法律条文和RAG方法。

Result: 结合法律知识的模型显著优于基线，表现出高准确性和解释质量，并能泛化到专家标注的测试集。

Conclusion: IBPS为印度司法系统提供了透明、可扩展的解决方案，支持数据驱动的法律辅助，促进程序公平。

Abstract: Bail decisions are among the most frequently adjudicated matters in Indian
courts, yet they remain plagued by subjectivity, delays, and inconsistencies.
With over 75% of India's prison population comprising undertrial prisoners,
many from socioeconomically disadvantaged backgrounds, the lack of timely and
fair bail adjudication exacerbates human rights concerns and contributes to
systemic judicial backlog. In this paper, we present the Indian Bail Prediction
System (IBPS), an AI-powered framework designed to assist in bail
decision-making by predicting outcomes and generating legally sound rationales
based solely on factual case attributes and statutory provisions. We curate and
release a large-scale dataset of 150,430 High Court bail judgments, enriched
with structured annotations such as age, health, criminal history, crime
category, custody duration, statutes, and judicial reasoning. We fine-tune a
large language model using parameter-efficient techniques and evaluate its
performance across multiple configurations, with and without statutory context,
and with RAG. Our results demonstrate that models fine-tuned with statutory
knowledge significantly outperform baselines, achieving strong accuracy and
explanation quality, and generalize well to a test set independently annotated
by legal experts. IBPS offers a transparent, scalable, and reproducible
solution to support data-driven legal assistance, reduce bail delays, and
promote procedural fairness in the Indian judicial system.

</details>


### [58] [Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements](https://arxiv.org/abs/2508.07598)
*Ziheng Li,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: 论文提出KeyCP++方法，通过关键词为中心的思维链提示，解决LLM在事件检测中的过解释问题，显著提升单次学习效果。


<details>
  <summary>Details</summary>
Motivation: LLM在事件检测中因缺乏对事件触发器的准确理解而表现不佳，传统上下文学习无法有效纠正过解释问题。

Method: 提出KeyCP++，通过自动标注输入文本与检测结果间的逻辑差距，构建触发器判别提示模板，结合关键词生成深度推理。

Result: 实验证明KeyCP++在单次事件检测中取得显著进步。

Conclusion: KeyCP++通过关键词引导的推理，有效减少LLM对关键词的依赖，提升事件检测能力。

Abstract: Although the LLM-based in-context learning (ICL) paradigm has demonstrated
considerable success across various natural language processing tasks, it
encounters challenges in event detection. This is because LLMs lack an accurate
understanding of event triggers and tend to make over-interpretation, which
cannot be effectively corrected through in-context examples alone. In this
paper, we focus on the most challenging one-shot setting and propose KeyCP++, a
keyword-centric chain-of-thought prompting approach. KeyCP++ addresses the
weaknesses of conventional ICL by automatically annotating the logical gaps
between input text and detection results for the demonstrations. Specifically,
to generate in-depth and meaningful rationale, KeyCP++ constructs a trigger
discrimination prompting template. It incorporates the exemplary triggers
(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let
LLM propose candidate triggers, and justify each candidate. These
propose-and-judge rationales help LLMs mitigate over-reliance on the keywords
and promote detection rule learning. Extensive experiments demonstrate the
effectiveness of our approach, showcasing significant advancements in one-shot
event detection.

</details>


### [59] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
*Anirudh Iyengar Kaniyar Narayana Iyengar,Srija Mukhopadhyay,Adnan Qidwai,Shubhankar Singh,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: InterChart是一个评估视觉语言模型在多图表推理能力的诊断基准，分为三个难度层级，揭示模型在复杂图表任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注单一图表，而实际应用需要跨多图表推理，InterChart填补了这一空白。

Method: 通过三个难度层级（单图表事实推理、合成对齐图表集整合分析、复杂真实图表对语义推理）评估模型。

Result: 模型在图表复杂度增加时准确率显著下降，分解多实体图表可提升性能。

Conclusion: InterChart为复杂多视觉环境中的多模态推理提供了严格框架。

Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well
vision-language models (VLMs) reason across multiple related charts, a task
central to real-world applications such as scientific reporting, financial
analysis, and public policy dashboards. Unlike prior benchmarks focusing on
isolated, visually uniform charts, InterChart challenges models with diverse
question types ranging from entity inference and trend correlation to numerical
estimation and abstract multi-step reasoning grounded in 2-3 thematically or
structurally related charts. We organize the benchmark into three tiers of
increasing difficulty: (1) factual reasoning over individual charts, (2)
integrative analysis across synthetically aligned chart sets, and (3) semantic
inference over visually complex, real-world chart pairs. Our evaluation of
state-of-the-art open and closed-source VLMs reveals consistent and steep
accuracy declines as chart complexity increases. We find that models perform
better when we decompose multi-entity charts into simpler visual units,
underscoring their struggles with cross-chart integration. By exposing these
systematic limitations, InterChart provides a rigorous framework for advancing
multimodal reasoning in complex, multi-visual environments.

</details>


### [60] [LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval](https://arxiv.org/abs/2508.07690)
*Luyao Zhuang,Qinggang Zhang,Huachi Zhou,Juhua Liu,Qing Li,Xiao Huang*

Main category: cs.CL

TL;DR: 论文提出了一种名为LoSemB的逻辑引导语义桥接框架，用于解决大语言模型在工具检索中遇到的未见工具问题，通过逻辑信息挖掘和转移提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的工具库不断更新，现有方法在训练时未见的工具上表现不佳，存在分布偏移和相似性检索脆弱性问题。

Method: LoSemB框架包含逻辑嵌入对齐模块和关系增强检索机制，旨在挖掘和转移逻辑信息以提升未见工具的检索效果。

Result: 实验表明，LoSemB在归纳设置下表现优异，同时在转导设置下保持良好效果。

Conclusion: LoSemB通过逻辑引导有效解决了工具检索中的未见工具问题，为实际应用提供了新思路。

Abstract: Tool learning has emerged as a promising paradigm for large language models
(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository
rapidly expanding, it is impractical to contain all tools within the limited
input length of LLMs. To alleviate these issues, researchers have explored
incorporating a tool retrieval module to select the most relevant tools or
represent tools as unique tokens within LLM parameters. However, most
state-of-the-art methods are under transductive settings, assuming all tools
have been observed during training. Such a setting deviates from reality as the
real-world tool repository is evolving and incorporates new tools frequently.
When dealing with these unseen tools, which refer to tools not encountered
during the training phase, these methods are limited by two key issues,
including the large distribution shift and the vulnerability of
similarity-based retrieval. To this end, inspired by human cognitive processes
of mastering unseen tools through discovering and applying the logical
information from prior experience, we introduce a novel Logic-Guided Semantic
Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to
mine and transfer latent logical information for inductive tool retrieval
without costly retraining. Specifically, LoSemB contains a logic-based
embedding alignment module to mitigate distribution shifts and implements a
relational augmented retrieval mechanism to reduce the vulnerability of
similarity-based retrieval. Extensive experiments demonstrate that LoSemB
achieves advanced performance in inductive settings while maintaining desirable
effectiveness in the transductive setting.

</details>


### [61] [What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction](https://arxiv.org/abs/2508.07702)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: 论文研究了基于Transformer的模型在预测长上下文（如完整句子）时的局限性，提出了一种新的评估方法（MSP）来测试模型在三个领域的表现。


<details>
  <summary>Details</summary>
Motivation: NTP（Next Token Prediction）方法虽然能保证局部流畅性，但缺乏对全局一致性的激励，尤其是在长文本或结构化文档中。

Method: 通过Masked Sentence Prediction（MSP）任务，评估了GPT-4o、Claude 3.5 Sonnet和Gemini 2.0 Flash在三个领域（叙事、程序和说明性文本）的表现。

Result: 商业LLMs在低结构化领域中预测掩码句子的表现较差，揭示了当前模型的局限性。

Conclusion: 研究强调了NTP方法的不足，并提出了对模型在长上下文和全局一致性方面改进的需求。

Abstract: Transformer-based models primarily rely on Next Token Prediction (NTP), which
predicts the next token in a sequence based on the preceding context. However,
NTP's focus on single-token prediction often limits a model's ability to plan
ahead or maintain long-range coherence, raising questions about how well LLMs
can predict longer contexts, such as full sentences within structured
documents. While NTP encourages local fluency, it provides no explicit
incentive to ensure global coherence across sentence boundaries-an essential
skill for reconstructive or discursive tasks. To investigate this, we evaluate
three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on
Masked Sentence Prediction (MSP) - the task of infilling a randomly removed
sentence - from three domains: ROCStories (narrative), Recipe1M (procedural),
and Wikipedia (expository). We assess both fidelity (similarity to the original
sentence) and cohesiveness (fit within the surrounding context). Our key
finding reveals that commercial LLMs, despite their superlative performance in
other tasks, are poor at predicting masked sentences in low-structured domains,
highlighting a gap in current model capabilities.

</details>


### [62] [Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models](https://arxiv.org/abs/2508.07753)
*Zhenliang Zhang,Junzhe Zhang,Xinyu Hu,HuiXuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 研究探讨社会偏见是否导致大语言模型（LLMs）的忠实性幻觉，使用结构因果模型（SCM）验证因果关系，并开发偏见干预数据集（BID）进行实验。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多项任务中表现优异，但其输出可能与输入不一致（忠实性幻觉），社会偏见是否导致这一问题尚未被研究。

Method: 采用结构因果模型（SCM）验证因果关系，设计偏见干预控制混杂因素，并开发BID数据集。

Result: 实验表明偏见是忠实性幻觉的重要诱因，且不同偏见状态的影响方向各异。

Conclusion: 偏见对幻觉生成具有微妙但显著的因果影响，尤其是社会偏见针对的不公平幻觉。

Abstract: Large language models (LLMs) have achieved remarkable success in various
tasks, yet they remain vulnerable to faithfulness hallucinations, where the
output does not align with the input. In this study, we investigate whether
social bias contributes to these hallucinations, a causal relationship that has
not been explored. A key challenge is controlling confounders within the
context, which complicates the isolation of causality between bias states and
hallucinations. To address this, we utilize the Structural Causal Model (SCM)
to establish and validate the causality and design bias interventions to
control confounders. In addition, we develop the Bias Intervention Dataset
(BID), which includes various social biases, enabling precise measurement of
causal effects. Experiments on mainstream LLMs reveal that biases are
significant causes of faithfulness hallucinations, and the effect of each bias
state differs in direction. We further analyze the scope of these causal
effects across various models, specifically focusing on unfairness
hallucinations, which are primarily targeted by social bias, revealing the
subtle yet significant causal effect of bias on hallucination generation.

</details>


### [63] [SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation](https://arxiv.org/abs/2508.07781)
*Zeyu Yang,Lai Wei,Roman Koshkin,Xi Chen,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 提出了一种基于语法的分块策略，结合依赖关系和标点特征分割输入流为语义完整单元，并在此基础上构建了SASST框架，显著提升了多语言翻译质量。


<details>
  <summary>Details</summary>
Motivation: 解决语义分块和翻译时序优化问题，提升同步语音翻译的质量。

Method: 使用语法分块策略和SASST框架，整合Whisper编码器和仅解码器LLM，动态输出翻译标记或等待符号。

Result: 在CoVoST2多语言语料库上验证了翻译质量的显著提升。

Conclusion: 语法结构在LLM驱动的同步语音翻译系统中具有显著效果。

Abstract: This work proposes a grammar-based chunking strategy that segments input
streams into semantically complete units by parsing dependency relations (e.g.,
noun phrase boundaries, verb-object structures) and punctuation features. The
method ensures chunk coherence and minimizes semantic fragmentation. Building
on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech
Translation), an end-to-end framework integrating frozen Whisper encoder and
decoder-only LLM. The unified architecture dynamically outputs translation
tokens or <WAIT> symbols to jointly optimize translation timing and content,
with target-side reordering addressing word-order divergence. Experiments on
CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation
quality improvements across languages and validate the effectiveness of
syntactic structures in LLM-driven SimulST systems.

</details>


### [64] [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785)
*Haoyuan Wu,Haoxing Chen,Xiaodong Chen,Zhanchao Zhou,Tieyuan Chen,Yihong Zhuang,Guoshan Lu,Zenan Huang,Junbo Zhao,Lin Liu,Zhenzhong Lan,Bei Yu,Jianguo Li*

Main category: cs.CL

TL;DR: Grove MoE是一种新型的MoE架构，通过引入不同大小的专家和动态激活机制，提高了计算效率，同时保持了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构使用同质化专家，激活固定数量的参数，限制了计算效率。Grove MoE旨在通过异质化专家和动态激活解决这一问题。

Method: 引入Grove MoE架构，包含不同大小的专家和动态激活机制。基于Qwen3-30B-A3B-Base模型，开发了GroveMoE-Base和GroveMoE-Inst。

Result: GroveMoE模型动态激活3.14-3.28B参数，性能与类似或更大规模的SOTA开源模型相当。

Conclusion: Grove MoE通过异质化专家和动态激活，显著提升了计算效率和模型性能。

Abstract: The Mixture of Experts (MoE) architecture is a cornerstone of modern
state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate
scalability by enabling sparse parameter activation. However, traditional MoE
architecture uses homogeneous experts of a uniform size, activating a fixed
number of parameters irrespective of input complexity and thus limiting
computational efficiency. To overcome this limitation, we introduce Grove MoE,
a novel architecture incorporating experts of varying sizes, inspired by the
heterogeneous big.LITTLE CPU architecture. This architecture features novel
adjugate experts with a dynamic activation mechanism, enabling model capacity
expansion while maintaining manageable computational overhead. Building on this
architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs
developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model
during mid-training and post-training. GroveMoE models dynamically activate
3.14-3.28B parameters based on token complexity and achieve performance
comparable to SOTA open-source models of similar or even larger size.

</details>


### [65] [Can You Trick the Grader? Adversarial Persuasion of LLM Judges](https://arxiv.org/abs/2508.07805)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Yongil Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: 研究发现，通过策略性嵌入说服性语言可以偏袒LLM评委对数学推理任务的评分，即使答案错误也能获得更高分数。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM评委是否容易被说服性语言影响评分，尤其是在数学任务中，正确性本应与风格无关。

Method: 基于亚里士多德的修辞原则，定义了七种说服技巧，并将其嵌入相同答案中，测试LLM评委的反应。

Result: 说服性语言导致LLM评委对错误答案评分平均提高8%，其中“一致性”技巧影响最大。模型规模增加无法显著缓解此问题。

Conclusion: LLM评委易受说服性语言影响，需开发更鲁棒的防御机制以应对此类攻击。

Abstract: As large language models take on growing roles as automated evaluators in
practical settings, a critical question arises: Can individuals persuade an LLM
judge to assign unfairly high scores? This study is the first to reveal that
strategically embedded persuasive language can bias LLM judges when scoring
mathematical reasoning tasks, where correctness should be independent of
stylistic variation. Grounded in Aristotle's rhetorical principles, we
formalize seven persuasion techniques (Majority, Consistency, Flattery,
Reciprocity, Pity, Authority, Identity) and embed them into otherwise identical
responses. Across six math benchmarks, we find that persuasive language leads
LLM judges to assign inflated scores to incorrect solutions, by up to 8% on
average, with Consistency causing the most severe distortion. Notably,
increasing model size does not substantially mitigate this vulnerability.
Further analysis demonstrates that combining multiple persuasion techniques
amplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,
the persuasive effect persists under counter prompting strategies, highlighting
a critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need
for robust defenses against persuasion-based attacks.

</details>


### [66] [Evaluating Compositional Approaches for Focus and Sentiment Analysis](https://arxiv.org/abs/2508.07810)
*Olga Kellert,Muhammad Imran,Nicholas Hill Matlis,Mahmud Uz Zaman,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 该论文评估了语言学中的焦点分析（FA）和自然语言处理（NLP）中的情感分析（SA）的组合方法，填补了FA领域缺乏定量评估的空白。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于填补语言学中焦点分析（FA）缺乏定量评估的空白，并论证情感分析（SA）的组合规则同样适用于FA，因为SA是FA的一部分。

Method: 采用基于通用依赖（UDs）的组合方法，利用修饰、协调和否定等基本句法规则，对SA进行分析，并与非组合方法VADER进行比较。

Result: 研究结果表明，组合方法在SA中具有更高的可解释性和准确性，且适用于FA。

Conclusion: 组合方法不仅适用于SA，还可推广到FA，为相关领域提供了新的研究视角。

Abstract: This paper summarizes the results of evaluating a compositional approach for
Focus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural
Language Processing (NLP). While quantitative evaluations of compositional and
non-compositional approaches in SA exist in NLP, similar quantitative
evaluations are very rare in FA in Linguistics that deal with linguistic
expressions representing focus or emphasis such as "it was John who left". We
fill this gap in research by arguing that compositional rules in SA also apply
to FA because FA and SA are closely related meaning that SA is part of FA. Our
compositional approach in SA exploits basic syntactic rules such as rules of
modification, coordination, and negation represented in the formalism of
Universal Dependencies (UDs) in English and applied to words representing
sentiments from sentiment dictionaries. Some of the advantages of our
compositional analysis method for SA in contrast to non-compositional analysis
methods are interpretability and explainability. We test the accuracy of our
compositional approach and compare it with a non-compositional approach VADER
that uses simple heuristic rules to deal with negation, coordination and
modification. In contrast to previous related work that evaluates
compositionality in SA on long reviews, this study uses more appropriate
datasets to evaluate compositionality. In addition, we generalize the results
of compositional approaches in SA to compositional approaches in FA.

</details>


### [67] [Evaluating Large Language Models as Expert Annotators](https://arxiv.org/abs/2508.07827)
*Yu-Min Tseng,Wei-Lin Chen,Chung-Chi Chen,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在需要专业知识的领域（如金融、生物医学和法律）中作为人类专家标注替代品的有效性，并提出了多代理讨论框架。研究发现，个体LLMs在推理技术下表现有限，推理模型未显著优于非推理模型，且多代理环境中某些模型行为表现出固执性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在专业领域标注任务中的表现，填补现有文献中关于LLMs在专家知识领域应用的研究空白。

Method: 通过评估个体LLMs和多代理方法，结合推理模型（如o3-mini），在多代理讨论框架中模拟人类标注者群体行为。

Result: 个体LLMs在推理技术下表现提升有限；推理模型未显著优于非推理模型；多代理环境中某些模型行为固执。

Conclusion: LLMs在专业领域标注任务中的表现有限，多代理框架虽能模拟群体行为，但模型固执性可能影响最终标注质量。

Abstract: Textual data annotation, the process of labeling or tagging text with
relevant information, is typically costly, time-consuming, and labor-intensive.
While large language models (LLMs) have demonstrated their potential as direct
alternatives to human annotators for general domains natural language
processing (NLP) tasks, their effectiveness on annotation tasks in domains
requiring expert knowledge remains underexplored. In this paper, we
investigate: whether top-performing LLMs, which might be perceived as having
expert-level proficiency in academic and professional benchmarks, can serve as
direct alternatives to human expert annotators? To this end, we evaluate both
individual LLMs and multi-agent approaches across three highly specialized
domains: finance, biomedicine, and law. Specifically, we propose a multi-agent
discussion framework to simulate a group of human annotators, where LLMs are
tasked to engage in discussions by considering others' annotations and
justifications before finalizing their labels. Additionally, we incorporate
reasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our
empirical results reveal that: (1) Individual LLMs equipped with inference-time
techniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal
or even negative performance gains, contrary to prior literature suggesting
their broad effectiveness. (2) Overall, reasoning models do not demonstrate
statistically significant improvements over non-reasoning models in most
settings. This suggests that extended long CoT provides relatively limited
benefits for data annotation in specialized domains. (3) Certain model
behaviors emerge in the multi-agent discussion environment. For instance,
Claude 3.7 Sonnet with thinking rarely changes its initial annotations, even
when other agents provide correct annotations or valid reasoning.

</details>


### [68] [LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding](https://arxiv.org/abs/2508.07849)
*Amrita Singh,H. Suhan Karaca,Aditya Joshi,Hye-young Paik,Jiaojiao Jiang*

Main category: cs.CL

TL;DR: 本文评估了10个法律专用LLM和7个通用LLM在合同理解任务上的表现，发现法律专用模型表现更优，尤其是需要法律细节的任务。


<details>
  <summary>Details</summary>
Motivation: 填补法律NLP领域中对合同分类任务缺乏全面评估的空白。

Method: 评估10个法律专用LLM和7个通用LLM在三个英语合同理解任务上的表现。

Result: 法律专用LLM表现优于通用LLM，Legal-BERT和Contracts-BERT在两个任务上达到SOTA。

Conclusion: 结果为法律专用LLM提供了全面评估，有助于开发更准确的合同理解系统。

Abstract: Despite advances in legal NLP, no comprehensive evaluation covering multiple
legal-specific LLMs currently exists for contract classification tasks in
contract understanding. To address this gap, we present an evaluation of 10
legal-specific LLMs on three English language contract understanding tasks and
compare them with 7 general-purpose LLMs. The results show that legal-specific
LLMs consistently outperform general-purpose models, especially on tasks
requiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish
new SOTAs on two of the three tasks, despite having 69% fewer parameters than
the best-performing general-purpose LLM. We also identify CaseLaw-BERT and
LexLM as strong additional baselines for contract understanding. Our results
provide a holistic evaluation of legal-specific LLMs and will facilitate the
development of more accurate contract understanding systems.

</details>


### [69] [Large Language Models for Czech Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.07860)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文评估了19种不同规模和架构的大语言模型（LLM）在捷克语ABSA任务中的表现，发现领域特定的小模型在微调后优于通用LLM的零样本和少样本表现，而微调后的LLM达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在捷克语ABSA任务中的表现，填补相关研究的空白。

Method: 对19种LLM在零样本、少样本和微调场景下进行综合评估，分析多语言性、模型规模和时效性等因素的影响。

Result: 领域特定小模型在微调后表现最佳，微调后的LLM达到最先进水平。

Conclusion: LLM在捷克语ABSA任务中具有潜力，但需进一步研究以解决关键挑战。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to identify sentiment toward specific aspects of an entity.
While large language models (LLMs) have shown strong performance in various
natural language processing (NLP) tasks, their capabilities for Czech ABSA
remain largely unexplored. In this work, we conduct a comprehensive evaluation
of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their
performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show
that small domain-specific models fine-tuned for ABSA outperform
general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs
achieve state-of-the-art results. We analyze how factors such as
multilingualism, model size, and recency influence performance and present an
error analysis highlighting key challenges, particularly in aspect term
prediction. Our findings provide insights into the suitability of LLMs for
Czech ABSA and offer guidance for future research in this area.

</details>


### [70] [Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models](https://arxiv.org/abs/2508.07866)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文研究了在低资源语言中，通过加入少量目标语言示例对跨语言方面情感分析（ABSA）性能的提升效果。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中ABSA任务因标注数据稀缺而面临的挑战，探索加入少量目标语言示例的潜力。

Method: 在四个ABSA任务、六种目标语言和两种序列到序列模型上，评估加入少量目标语言示例对训练集的影响。

Result: 加入十个目标语言示例显著优于零样本设置，且效果接近约束解码；结合1000个目标语言示例和英语数据可超越单语基线。

Conclusion: 少量高质量标注示例对提升跨语言ABSA性能既可行又高效，为低资源和领域特定场景提供了实用指导。

Abstract: Aspect-based sentiment analysis (ABSA) has received substantial attention in
English, yet challenges remain for low-resource languages due to the scarcity
of labelled data. Current cross-lingual ABSA approaches often rely on external
translation tools and overlook the potential benefits of incorporating a small
number of target language examples into training. In this paper, we evaluate
the effect of adding few-shot target language examples to the training set
across four ABSA tasks, six target languages, and two sequence-to-sequence
models. We show that adding as few as ten target language examples
significantly improves performance over zero-shot settings and achieves a
similar effect to constrained decoding in reducing prediction errors.
Furthermore, we demonstrate that combining 1,000 target language examples with
English data can even surpass monolingual baselines. These findings offer
practical insights for improving cross-lingual ABSA in low-resource and
domain-specific settings, as obtaining ten high-quality annotated examples is
both feasible and highly effective.

</details>


### [71] [Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity](https://arxiv.org/abs/2508.07902)
*Chen Cecilia Liu,Hiba Arnaout,Nils Kovačić,Dana Atzil-Slonim,Iryna Gurevych*

Main category: cs.CL

TL;DR: 论文介绍了CultureCare数据集，用于研究大型语言模型（LLMs）在提供文化敏感情感支持方面的能力，并测试了四种适应策略。结果显示，经过调整的LLMs表现优于匿名在线同行回应，并探讨了LLMs在临床培训中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在提供文化敏感情感支持方面的能力，填补现有资源不足的空白。

Method: 引入CultureCare数据集，开发四种适应策略，评估三种先进LLMs的表现，并通过LLM评审、文化内人类标注者和临床心理学家进行综合评估。

Result: 调整后的LLMs表现优于匿名在线同行回应，但简单的文化角色扮演不足以实现文化敏感性。

Conclusion: LLMs在提供文化敏感支持和临床培训中具有潜力，但需要更复杂的适应策略。

Abstract: Large language models (LLMs) show promise in offering emotional support and
generating empathetic responses for individuals in distress, but their ability
to deliver culturally sensitive support remains underexplored due to lack of
resources. In this work, we introduce CultureCare, the first dataset designed
for this task, spanning four cultures and including 1729 distress messages,
1523 cultural signals, and 1041 support strategies with fine-grained emotional
and cultural annotations. Leveraging CultureCare, we (i) develop and test four
adaptation strategies for guiding three state-of-the-art LLMs toward culturally
sensitive responses; (ii) conduct comprehensive evaluations using LLM judges,
in-culture human annotators, and clinical psychologists; (iii) show that
adapted LLMs outperform anonymous online peer responses, and that simple
cultural role-play is insufficient for cultural sensitivity; and (iv) explore
the application of LLMs in clinical training, where experts highlight their
potential in fostering cultural competence in future therapists.

</details>


### [72] [Challenges and opportunities in portraying emotion in generated sign language](https://arxiv.org/abs/2508.07937)
*John C. McDonald,Rosalee Wolfe,Fabrizio Nunnari*

Main category: cs.CL

TL;DR: 论文提出了一种直观的双参数表示法，用于控制手语虚拟角色Paula的情感非手动信号，通过EASIER文本表示实现更一致的情感表达。


<details>
  <summary>Details</summary>
Motivation: 手语虚拟角色在情感非手动信号表达上缺乏标准化方法，导致情感内容难以融入。

Method: 采用双参数表示法，通过EASIER文本表示控制虚拟角色的情感面部表情。

Result: 该方法能够更一致地指定情感非手动信号，并支持更细腻的情感状态表达。

Conclusion: 双参数表示法为手语虚拟角色的情感表达提供了一种更连贯的解决方案。

Abstract: Non-manual signals in sign languages continue to be a challenge for signing
avatars. More specifically, emotional content has been difficult to incorporate
because of a lack of a standard method of specifying the avatar's emotional
state. This paper explores the application of an intuitive two-parameter
representation for emotive non-manual signals to the Paula signing avatar that
shows promise for facilitating the linguistic specification of emotional facial
expressions in a more coherent manner than previous methods. Users can apply
these parameters to control Paula's emotional expressions through a textual
representation called the EASIER notation. The representation can allow avatars
to express more nuanced emotional states using two numerical parameters. It
also has the potential to enable more consistent specification of emotional
non-manual signals in linguistic annotations which drive signing avatars.

</details>


### [73] [Expert Preference-based Evaluation of Automated Related Work Generation](https://arxiv.org/abs/2508.07955)
*Furkan Şahinuç,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 论文提出GREP框架，用于评估自动生成的科学写作质量，特别是相关工作部分，结合专家偏好和领域标准，提供细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 科学写作质量评估缺乏捕捉专家偏好和领域标准的方法，现有自动指标和LLM评估系统不足。

Method: 提出GREP框架，分解评估为多维度，结合专家偏好和对比示例，支持开源和专有LLM评估。

Result: GREP评估更稳健，与专家评估强相关，现有LLM生成内容难以满足要求且难以改进。

Conclusion: GREP为科学写作提供高质量评估，支持人机协作，但LLM生成内容仍需改进。

Abstract: Expert domain writing, such as scientific writing, typically demands
extensive domain knowledge. Recent advances in LLMs show promising potential in
reducing the expert workload. However, evaluating the quality of automatically
generated scientific writing is a crucial open issue, as it requires knowledge
of domain-specific evaluation criteria and the ability to discern expert
preferences. Conventional automatic metrics and LLM-as-a-judge systems are
insufficient to grasp expert preferences and domain-specific quality standards.
To address this gap and support human-AI collaborative writing, we focus on
related work generation, one of the most challenging scientific tasks, as an
exemplar. We propose GREP, a multi-turn evaluation framework that integrates
classical related work evaluation criteria with expert-specific preferences.
Instead of assigning a single score, our framework decomposes the evaluation
into fine-grained dimensions. This localized evaluation approach is further
augmented with contrastive few-shot examples to provide detailed contextual
guidance for the evaluation dimensions. The design principles allow our
framework to deliver cardinal assessment of quality, which can facilitate
better post-training compared to ordinal preference data. For better
accessibility, we design two variants of GREP: a more precise variant with
proprietary LLMs as evaluators, and a cheaper alternative with open-weight
LLMs. Empirical investigation reveals that our framework is able to assess the
quality of related work sections in a much more robust manner compared to
standard LLM judges, reflects natural scenarios of scientific writing, and
bears a strong correlation with the human expert assessment. We also observe
that generations from state-of-the-art LLMs struggle to satisfy validation
constraints of a suitable related work section. They (mostly) fail to improve
based on feedback as well.

</details>


### [74] [Large Language Models for Subjective Language Understanding: A Survey](https://arxiv.org/abs/2508.07959)
*Changhao Song,Yazhou Zhang,Hui Gao,Ben Yao,Peng Zhang*

Main category: cs.CL

TL;DR: 该论文综述了大型语言模型（LLMs）在主观语言理解任务中的应用，包括情感分析、情绪识别等，并探讨了其挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等LLMs的出现，主观语言任务的处理方式发生了范式转变，需要系统梳理其应用与挑战。

Method: 通过定义主观语言、总结LLM架构演变，并针对八项任务分析其数据集、方法和挑战。

Result: LLMs在建模人类主观判断方面表现出色，但仍面临数据限制、模型偏见等问题。

Conclusion: 该综述为情感计算与LLMs交叉领域的研究者提供了宝贵资源，并提出了未来研究方向。

Abstract: Subjective language understanding refers to a broad set of natural language
processing tasks where the goal is to interpret or generate content that
conveys personal feelings, opinions, or figurative meanings rather than
objective facts. With the advent of large language models (LLMs) such as
ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach
these inherently nuanced tasks. In this survey, we provide a comprehensive
review of recent advances in applying LLMs to subjective language tasks,
including sentiment analysis, emotion recognition, sarcasm detection, humor
understanding, stance detection, metaphor interpretation, intent detection, and
aesthetics assessment. We begin by clarifying the definition of subjective
language from linguistic and cognitive perspectives, and we outline the unique
challenges posed by subjective language (e.g. ambiguity, figurativeness,
context dependence). We then survey the evolution of LLM architectures and
techniques that particularly benefit subjectivity tasks, highlighting why LLMs
are well-suited to model subtle human-like judgments. For each of the eight
tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based
methods, and remaining challenges. We provide comparative insights, discussing
commonalities and differences among tasks and how multi-task LLM approaches
might yield unified models of subjectivity. Finally, we identify open issues
such as data limitations, model bias, and ethical considerations, and suggest
future research directions. We hope this survey will serve as a valuable
resource for researchers and practitioners interested in the intersection of
affective computing, figurative language processing, and large-scale language
models.

</details>


### [75] [Toward Machine Interpreting: Lessons from Human Interpreting Studies](https://arxiv.org/abs/2508.07964)
*Matthias Sperber,Maureen de Seyssel,Jiajun Bao,Matthias Paulik*

Main category: cs.CL

TL;DR: 论文探讨如何通过借鉴人类口译原则改进语音翻译系统，以实现更接近真实口译的体验。


<details>
  <summary>Details</summary>
Motivation: 当前语音翻译系统缺乏人类口译的动态适应性，限制了其实际应用价值。

Method: 通过分析人类口译文献，结合机器翻译领域视角，探讨操作性和定性方面。

Result: 发现利用现有建模技术可以采纳许多人类口译原则。

Conclusion: 研究为缩小语音翻译系统与真实口译的可用性差距提供了启发，并推动机器口译的发展。

Abstract: Current speech translation systems, while having achieved impressive
accuracies, are rather static in their behavior and do not adapt to real-world
situations in ways human interpreters do. In order to improve their practical
usefulness and enable interpreting-like experiences, a precise understanding of
the nature of human interpreting is crucial. To this end, we discuss human
interpreting literature from the perspective of the machine translation field,
while considering both operational and qualitative aspects. We identify
implications for the development of speech translation systems and argue that
there is great potential to adopt many human interpreting principles using
recent modeling techniques. We hope that our findings provide inspiration for
closing the perceived usability gap, and can motivate progress toward true
machine interpreting.

</details>


### [76] [Understanding Syntactic Generalization in Structure-inducing Language Models](https://arxiv.org/abs/2508.07969)
*David Arps,Hassan Sajjad,Laura Kallmeyer*

Main category: cs.CL

TL;DR: 本文研究了三种结构诱导语言模型（SiLM）在自然语言和合成数据上的表现，发现GPST在多数评估中表现最稳定。


<details>
  <summary>Details</summary>
Motivation: 评估不同SiLM架构在语法表示、语法判断任务和训练动态方面的表现，填补现有研究的系统性空白。

Method: 比较了Structformer、UDGN和GPST三种模型，使用英语语料和合成括号表达式进行评估。

Result: GPST在长距离依赖任务中表现最佳，且小型模型在合成数据上的训练为评估提供了有效测试平台。

Conclusion: 不同SiLM架构各有优劣，GPST在多数任务中表现最稳定，合成数据训练的小模型有助于基础评估。

Abstract: Structure-inducing Language Models (SiLM) are trained on a self-supervised
language modeling task, and induce a hierarchical sentence representation as a
byproduct when processing an input. A wide variety of SiLMs have been proposed.
However, these have typically been evaluated on a relatively small scale, and
evaluation of these models has systematic gaps and lacks comparability. In this
work, we study three different SiLM architectures using both natural language
(English) corpora and synthetic bracketing expressions: Structformer (Shen et
al., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare
them with respect to (i) properties of the induced syntactic representations
(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.
We find that none of the three architectures dominates across all evaluation
metrics. However, there are significant differences, in particular with respect
to the induced syntactic representations. The Generative Pretrained Structured
Transformer (GPST; Hu et al. 2024) performs most consistently across evaluation
settings, and outperforms the other models on long-distance dependencies in
bracketing expressions. Furthermore, our study shows that small models trained
on large amounts of synthetic data provide a useful testbed for evaluating
basic model properties.

</details>


### [77] [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)
*Jiaxuan Gao,Wei Fu,Minyang Xie,Shusheng Xu,Chuyi He,Zhiyu Mei,Banghua Zhu,Yi Wu*

Main category: cs.CL

TL;DR: ASearcher是一个开源项目，通过大规模RL训练提升搜索代理的性能，解决了现有方法在可扩展性、效率和数据质量上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有开源代理在搜索智能（如处理模糊查询、生成精确搜索等）方面表现不足，且现有RL方法因轮次限制无法学习复杂策略。

Method: 提出ASearcher，包括：1）可扩展的全异步RL训练，支持长时程搜索；2）基于提示的LLM代理，自主合成高质量QA数据集。

Result: ASearcher在xBench和GAIA上分别取得46.7%和20.8%的Avg@4提升，支持超长时程搜索（工具调用超40轮，输出超150k token）。

Conclusion: ASearcher在简单设计且无外部LLM的情况下，性能超越现有开源32B代理，相关资源已开源。

Abstract: Recent advancements in LLM-based agents have demonstrated remarkable
capabilities in handling complex, knowledge-intensive tasks by integrating
external tools. Among diverse choices of tools, search tools play a pivotal
role in accessing vast external knowledge. However, open-source agents still
fall short of achieving expert-level Search Intelligence, the ability to
resolve ambiguous queries, generate precise searches, analyze results, and
conduct thorough exploration. Existing approaches fall short in scalability,
efficiency, and data quality. For example, small turn limits in existing online
RL methods, e.g. <=10, restrict complex strategy learning. This paper
introduces ASearcher, an open-source project for large-scale RL training of
search agents. Our key contributions include: (1) Scalable fully asynchronous
RL training that enables long-horizon search while maintaining high training
efficiency. (2) A prompt-based LLM agent that autonomously synthesizes
high-quality and challenging QAs, creating a large-scale QA dataset. Through RL
training, our prompt-based QwQ-32B agent achieves substantial improvements,
with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our
agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns
and output tokens exceeding 150k during training time. With a simple agent
design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on
xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We
open-source our models, training data, and codes in
https://github.com/inclusionAI/ASearcher.

</details>


### [78] [The Medical Metaphors Corpus (MCC)](https://arxiv.org/abs/2508.07993)
*Anna Sofia Lippolis,Andrea Giovanni Nuzzolese,Aldo Gangemi*

Main category: cs.CL

TL;DR: 论文介绍了医学隐喻语料库（MCC），一个包含792个标注科学概念隐喻的数据集，填补了领域特定隐喻检测的空白。


<details>
  <summary>Details</summary>
Motivation: 科学话语中隐喻普遍存在，但现有隐喻检测资源主要针对通用领域，缺乏领域特定应用。

Method: MCC整合了多种来源的隐喻表达，包括同行评审文献、新闻媒体、社交媒体和众包贡献，提供二元和分级隐喻性标注。

Result: 评估显示，现有语言模型在科学隐喻检测上表现一般，领域特定隐喻理解仍有提升空间。

Conclusion: MCC为计算科学隐喻研究提供了首个标注资源，支持多种应用，如隐喻检测基准测试和质量感知生成系统。

Abstract: Metaphor is a fundamental cognitive mechanism that shapes scientific
understanding, enabling the communication of complex concepts while potentially
constraining paradigmatic thinking. Despite the prevalence of figurative
language in scientific discourse, existing metaphor detection resources
primarily focus on general-domain text, leaving a critical gap for
domain-specific applications. In this paper, we present the Medical Metaphors
Corpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual
metaphors spanning medical and biological domains. MCC aggregates metaphorical
expressions from diverse sources including peer-reviewed literature, news
media, social media discourse, and crowdsourced contributions, providing both
binary and graded metaphoricity judgments validated through human annotation.
Each instance includes source-target conceptual mappings and perceived
metaphoricity scores on a 0-7 scale, establishing the first annotated resource
for computational scientific metaphor research. Our evaluation demonstrates
that state-of-the-art language models achieve modest performance on scientific
metaphor detection, revealing substantial room for improvement in
domain-specific figurative language understanding. MCC enables multiple
research applications including metaphor detection benchmarking, quality-aware
generation systems, and patient-centered communication tools.

</details>


### [79] [WideSearch: Benchmarking Agentic Broad Info-Seeking](https://arxiv.org/abs/2508.07999)
*Ryan Wong,Jiawei Wang,Junjie Zhao,Li Chen,Yan Gao,Long Zhang,Xuan Zhou,Zuo Wang,Kai Xiang,Ge Zhang,Wenhao Huang,Yang Wang,Ke Wang*

Main category: cs.CL

TL;DR: 论文提出了WideSearch基准，用于评估LLM驱动的搜索代理在大规模信息收集任务中的可靠性，发现现有系统表现不佳，成功率为0%至5%，而人工测试可达100%。


<details>
  <summary>Details</summary>
Motivation: 大规模信息收集任务虽然重复性高但认知复杂度低，现有LLM驱动的搜索代理能力未得到充分评估，缺乏合适的基准。

Method: 开发了包含200个问题的WideSearch基准，涵盖15个领域，通过五阶段质量控制确保数据质量，并测试了10种搜索系统。

Result: 现有搜索代理成功率极低（0%-5%），而人工测试可达100%，表明其在信息收集方面存在严重不足。

Conclusion: 当前搜索代理在大规模信息收集中表现不佳，需进一步研究改进。

Abstract: From professional research to everyday planning, many tasks are bottlenecked
by wide-scale information seeking, which is more repetitive than cognitively
complex. With the rapid development of Large Language Models (LLMs), automated
search agents powered by LLMs offer a promising solution to liberate humans
from this tedious work. However, the capability of these agents to perform such
"wide-context" collection reliably and completely remains largely unevaluated
due to a lack of suitable benchmarks. To bridge this gap, we introduce
WideSearch, a new benchmark engineered to evaluate agent reliability on these
large-scale collection tasks. The benchmark features 200 manually curated
questions (100 in English, 100 in Chinese) from over 15 diverse domains,
grounded in real user queries. Each task requires agents to collect large-scale
atomic information, which could be verified one by one objectively, and arrange
it into a well-organized output. A rigorous five-stage quality control pipeline
ensures the difficulty, completeness, and verifiability of the dataset. We
benchmark over 10 state-of-the-art agentic search systems, including
single-agent, multi-agent frameworks, and end-to-end commercial systems. Most
systems achieve overall success rates near 0\%, with the best performer
reaching just 5\%. However, given sufficient time, cross-validation by multiple
human testers can achieve a near 100\% success rate. These results demonstrate
that present search agents have critical deficiencies in large-scale
information seeking, underscoring urgent areas for future research and
development in agentic search. Our dataset, evaluation pipeline, and benchmark
results have been publicly released at https://widesearch-seed.github.io/

</details>


### [80] [Progressive Depth Up-scaling via Optimal Transport](https://arxiv.org/abs/2508.08011)
*Mingzi Cao,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: OpT-DeUS利用最优传输对齐和融合Transformer块，提升大语言模型深度扩展的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度扩展方法忽视神经元排列差异，可能导致性能下降。

Method: 通过最优传输对齐和融合相邻基础层的Transformer块，创建新层。

Result: OpT-DeUS在持续预训练和监督微调中表现优于现有方法，且插入新层靠近顶部时效率更高。

Conclusion: OpT-DeUS有效解决了神经元排列不匹配问题，提升了训练效率和模型性能。

Abstract: Scaling Large Language Models (LLMs) yields performance gains but incurs
substantial training costs. Depth up-scaling offers training efficiency by
adding new layers to pre-trained models. However, most existing methods copy or
average weights from base layers, neglecting neuron permutation differences.
This limitation can potentially cause misalignment that harms performance.
Inspired by applying Optimal Transport (OT) for neuron alignment, we propose
Optimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses
Transformer blocks in adjacent base layers via OT for new layer creation, to
mitigate neuron permutation mismatch between layers. OpT-DeUS achieves better
overall performance and offers improved training efficiency than existing
methods for continual pre-training and supervised fine-tuning across different
model sizes. To further evaluate the impact of interpolation positions, our
extensive analysis shows that inserting new layers closer to the top results in
higher training efficiency due to shorter back-propagation time while obtaining
additional performance gains.

</details>


### [81] [9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)](https://arxiv.org/abs/2508.08050)
*Fabrizio Nunnari,Cristina Luna Jiménez,Rosalee Wolfe,John C. McDonald,Michael Filhol,Eleni Efthimiou,Evita Fotinea,Thomas Hanke*

Main category: cs.CL

TL;DR: SLTAT 2025研讨会聚焦于通过非侵入性手段改善聋人/人类交流，涵盖手语翻译、虚拟化身技术及相关领域。


<details>
  <summary>Details</summary>
Motivation: 促进聋人与人类之间的无障碍交流，结合虚拟化身技术提升沟通效率。

Method: 通过研讨会汇集手语识别、数据收集与分析、工具开发、伦理与情感计算等多领域研究。

Result: 展示了手语翻译与虚拟化身技术的进展，并促进了不同研究社区的合作。

Conclusion: SLTAT 2025为跨领域合作提供了平台，推动了聋人交流技术的创新与发展。

Abstract: The Sign Language Translation and Avatar Technology (SLTAT) workshops
continue a series of gatherings to share recent advances in improving deaf /
human communication through non-invasive means. This 2025 edition, the 9th
since its first appearance in 2011, is hosted by the International Conference
on Intelligent Virtual Agents (IVA), giving the opportunity for contamination
between two research communities, using digital humans as either virtual
interpreters or as interactive conversational agents. As presented in this
summary paper, SLTAT sees contributions beyond avatar technologies, with a
consistent number of submissions on sign language recognition, and other work
on data collection, data analysis, tools, ethics, usability, and affective
computing.

</details>


### [82] [Dual Information Speech Language Models for Emotional Conversations](https://arxiv.org/abs/2508.08095)
*Chun Wang,Chenyang Liu,Wenze Xu,Weihong Deng*

Main category: cs.CL

TL;DR: 论文提出了一种通过异构适配器和弱监督训练策略解决语音语言模型（SLM）在捕捉副语言信息和上下文理解上的问题的方法。


<details>
  <summary>Details</summary>
Motivation: 基于文本的大型语言模型（LLM）在对话系统中常忽略副语言线索，而语音语言模型（SLM）虽能解决此问题，但现有方法在副语言信息捕捉和上下文理解上表现不佳。

Method: 提出两种异构适配器和弱监督训练策略，分离副语言与语言信息，并通过结构化表示解释语音，同时避免生成任务特定向量以保持上下文理解。

Result: 实验表明，该方法在情感对话任务中表现优异，能有效整合副语言和语言信息。

Conclusion: 该方法通过高效训练适配器，提升了SLM在副语言和语言信息整合上的能力，同时保持了参数和数据效率。

Abstract: Conversational systems relying on text-based large language models (LLMs)
often overlook paralinguistic cues, essential for understanding emotions and
intentions. Speech-language models (SLMs), which use speech as input, are
emerging as a promising solution. However, SLMs built by extending frozen LLMs
struggle to capture paralinguistic information and exhibit reduced context
understanding. We identify entangled information and improper training
strategies as key issues. To address these issues, we propose two heterogeneous
adapters and suggest a weakly supervised training strategy. Our approach
disentangles paralinguistic and linguistic information, enabling SLMs to
interpret speech through structured representations. It also preserves
contextual understanding by avoiding the generation of task-specific vectors
through controlled randomness. This approach trains only the adapters on common
datasets, ensuring parameter and data efficiency. Experiments demonstrate
competitive performance in emotional conversation tasks, showcasing the model's
ability to effectively integrate both paralinguistic and linguistic information
within contextual settings.

</details>


### [83] [Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?](https://arxiv.org/abs/2508.08096)
*Lukas Gehring,Benjamin Paaßen*

Main category: cs.CL

TL;DR: 论文评估了教育场景中检测大语言模型（LLM）生成文本的现有方法，提出了新数据集GEDE，并发现多数检测器难以准确识别学生贡献程度中等的文本（如LLM改进的人类文本）。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的普及，学生可能利用其自动生成文本，挑战学术诚信。需开发有效检测方法以保障学习效果。

Method: 提出GEDE数据集（含900篇学生作文和12,500篇LLM生成文本），引入贡献程度概念，评估多种检测器性能。

Result: 多数检测器对中等贡献程度的文本（如LLM改进的人类文本）分类不准，易产生假阳性。

Conclusion: 现有检测器在教育场景中存在局限性，需进一步改进以减少误判对学生的影响。

Abstract: Recent advancements in Large Language Models (LLMs) and their increased
accessibility have made it easier than ever for students to automatically
generate texts, posing new challenges for educational institutions. To enforce
norms of academic integrity and ensure students' learning, learning analytics
methods to automatically detect LLM-generated text appear increasingly
appealing. This paper benchmarks the performance of different state-of-the-art
detectors in educational contexts, introducing a novel dataset, called
Generative Essay Detection in Education (GEDE), containing over 900
student-written essays and over 12,500 LLM-generated essays from various
domains. To capture the diversity of LLM usage practices in generating text, we
propose the concept of contribution levels, representing students' contribution
to a given assignment. These levels range from purely human-written texts, to
slightly LLM-improved versions, to fully LLM-generated texts, and finally to
active attacks on the detector by "humanizing" generated texts. We show that
most detectors struggle to accurately classify texts of intermediate student
contribution levels, like LLM-improved human-written texts. Detectors are
particularly likely to produce false positives, which is problematic in
educational settings where false suspicions can severely impact students'
lives. Our dataset, code, and additional supplementary materials are publicly
available at
https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.

</details>


### [84] [Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0](https://arxiv.org/abs/2508.08110)
*Robin Huo,Ewan Dunbar*

Main category: cs.CL

TL;DR: 研究比较了HuBERT和wav2vec 2.0两种自监督语音表示学习模型，发现训练迭代次数而非训练目标影响隐藏表示对语言信息的编码效果。


<details>
  <summary>Details</summary>
Motivation: 自监督语音表示学习模型的应用广泛，但模型架构对其学习语言信息的影响研究不足。

Method: 比较HuBERT和wav2vec 2.0的两种架构差异：训练目标和多轮训练迭代的伪标签细化。

Result: 隐藏表示与词、音素和说话者身份的典型相关性差异主要由训练迭代次数决定。

Conclusion: 未来研究应探讨迭代细化在编码语言信息中的有效性原因。

Abstract: Self-supervised models for speech representation learning now see widespread
use for their versatility and performance on downstream tasks, but the effect
of model architecture on the linguistic information learned in their
representations remains under-studied. This study investigates two such models,
HuBERT and wav2vec 2.0, and minimally compares two of their architectural
differences: training objective and iterative pseudo-label refinement through
multiple training iterations. We find that differences in canonical correlation
of hidden representations to word identity, phoneme identity, and speaker
identity are explained by training iteration, not training objective. We
suggest that future work investigate the reason for the effectiveness of
iterative refinement in encoding linguistic information in self-supervised
speech representations.

</details>


### [85] [Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.08125)
*Jakub Šmíd,Pavel Přibáň,Ondřej Pražák,Pavel Král*

Main category: cs.CL

TL;DR: 本文介绍了一个新的捷克语数据集，用于基于方面的情感分析（ABSA），包含3.1K手动标注的餐厅评论，支持更复杂的任务，如目标-方面-类别检测。


<details>
  <summary>Details</summary>
Motivation: 改进现有捷克语数据集，支持更复杂的ABSA任务，并促进跨语言比较。

Method: 基于SemEval-2016格式统一标注，由两名标注者完成，标注一致性达90%。

Result: 提供了24M未标注评论用于无监督学习，并展示了基于Transformer的基线模型结果。

Conclusion: 数据集和代码公开，支持非商业研究用途。

Abstract: In this paper, we introduce a novel Czech dataset for aspect-based sentiment
analysis (ABSA), which consists of 3.1K manually annotated reviews from the
restaurant domain. The dataset is built upon the older Czech dataset, which
contained only separate labels for the basic ABSA tasks such as aspect term
extraction or aspect polarity detection. Unlike its predecessor, our new
dataset is specifically designed for more complex tasks, e.g.
target-aspect-category detection. These advanced tasks require a unified
annotation format, seamlessly linking sentiment elements (labels) together. Our
dataset follows the format of the well-known SemEval-2016 datasets. This design
choice allows effortless application and evaluation in cross-lingual scenarios,
ultimately fostering cross-language comparisons with equivalent counterpart
datasets in other languages. The annotation process engaged two trained
annotators, yielding an impressive inter-annotator agreement rate of
approximately 90%. Additionally, we provide 24M reviews without annotations
suitable for unsupervised learning. We present robust monolingual baseline
results achieved with various Transformer-based models and insightful error
analysis to supplement our contributions. Our code and dataset are freely
available for non-commercial research purposes.

</details>


### [86] [Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models](https://arxiv.org/abs/2508.08131)
*Wenze Xu,Chun Wang,Jiazhen Yu,Sheng Chen,Liang Gao,Weihong Deng*

Main category: cs.CL

TL;DR: OTReg通过最优传输正则化方法减少语音与文本表示之间的模态差距，提升SLM的泛化能力。


<details>
  <summary>Details</summary>
Motivation: SLM在处理语音输入时泛化能力不足，可能因语音与文本表示间的模态差距导致。

Method: 提出OTReg方法，将语音-文本对齐建模为最优传输问题，通过正则化损失优化SLM训练。

Result: 实验表明OTReg改善了语音-文本对齐，减少了模态差距，提升了SLM在多语言ASR任务中的泛化性能。

Conclusion: OTReg是一种轻量级方法，无需额外标签或参数，能有效提升SLM的泛化能力。

Abstract: Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to
perceive speech inputs, have gained increasing attention for their potential to
advance speech understanding tasks. However, despite recent progress, studies
show that SLMs often struggle to generalize across datasets, even for trained
languages and tasks, raising concerns about whether they process speech in a
text-like manner as intended. A key challenge underlying this limitation is the
modality gap between speech and text representations. The high variability in
speech embeddings may allow SLMs to achieve strong in-domain performance by
exploiting unintended speech variations, ultimately hindering generalization.
To mitigate this modality gap, we introduce Optimal Transport Regularization
(OTReg), a method that formulates speech-text alignment as an optimal transport
problem and derives a regularization loss to improve SLM training. In each
training iteration, OTReg first establishes a structured correspondence between
speech and transcript embeddings by determining the optimal transport plan,
then incorporates the regularization loss based on this transport plan to
optimize SLMs in generating speech embeddings that align more effectively with
transcript embeddings. OTReg is lightweight, requiring no additional labels or
learnable parameters, and integrates seamlessly into existing SLM training
procedures. Extensive multilingual ASR experiments demonstrate that OTReg
enhances speech-text alignment, mitigates the modality gap, and consequently
improves SLM generalization across diverse datasets.

</details>


### [87] [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](https://arxiv.org/abs/2508.08139)
*Tianyi Zhou,Johanne Medina,Sanjay Chawla*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）生成不可靠内容的问题，提出了一种基于标记级不确定性的可靠性估计方法，用于检测不可靠输出。


<details>
  <summary>Details</summary>
Motivation: LLMs在多轮或代理应用中容易生成流畅但不正确的内容（虚构），这增加了风险。研究旨在探索上下文信息如何影响模型行为，以及LLMs是否能识别其不可靠响应。

Method: 提出了一种可靠性估计方法，利用输出logits计算偶然性和认知不确定性，识别关键标记并将其隐藏状态聚合为紧凑表示，用于预测响应级可靠性。

Result: 实验表明，正确的上下文信息提高了答案准确性和模型置信度，而误导性上下文常导致模型自信地生成错误答案。提出的方法能有效检测不可靠输出。

Conclusion: 研究揭示了直接不确定性信号的局限性，强调了不确定性引导探测在可靠性感知生成中的潜力。

Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect
content, known as confabulation, which poses increasing risks in multi-turn or
agentic applications where outputs may be reused as context. In this work, we
investigate how in-context information influences model behavior and whether
LLMs can identify their unreliable responses. We propose a reliability
estimation that leverages token-level uncertainty to guide the aggregation of
internal model representations. Specifically, we compute aleatoric and
epistemic uncertainty from output logits to identify salient tokens and
aggregate their hidden states into compact representations for response-level
reliability prediction. Through controlled experiments on open QA benchmarks,
we find that correct in-context information improves both answer accuracy and
model confidence, while misleading context often induces confidently incorrect
responses, revealing a misalignment between uncertainty and correctness. Our
probing-based method captures these shifts in model behavior and improves the
detection of unreliable outputs across multiple open-source LLMs. These results
underscore the limitations of direct uncertainty signals and highlight the
potential of uncertainty-guided probing for reliability-aware generation.

</details>


### [88] [Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective](https://arxiv.org/abs/2508.08140)
*Jun Wang,Zaifu Zhan,Qixin Zhang,Mingquan Lin,Meijia Song,Rui Zhang*

Main category: cs.CL

TL;DR: Dual-Div框架通过两阶段检索和排序，优化生物医学NLP任务中的示例选择，提升LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在示例选择中偏重代表性而忽视多样性，Dual-Div旨在填补这一空白。

Method: 采用两阶段检索和排序：先优化代表性和多样性选择候选示例，再根据测试查询排名选择最相关且非冗余的示例。

Result: 在三个生物医学NLP任务中，Dual-Div比基线方法性能提升高达5%的macro-F1分数。

Conclusion: 初始检索阶段的多样性比排序阶段优化更重要，且3-5个示例能最大化性能效率。

Abstract: Recent progress in large language models (LLMs) has leveraged their
in-context learning (ICL) abilities to enable quick adaptation to unseen
biomedical NLP tasks. By incorporating only a few input-output examples into
prompts, LLMs can rapidly perform these new tasks. While the impact of these
demonstrations on LLM performance has been extensively studied, most existing
approaches prioritize representativeness over diversity when selecting examples
from large corpora. To address this gap, we propose Dual-Div, a
diversity-enhanced data-efficient framework for demonstration selection in
biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:
First, it identifies a limited set of candidate examples from a corpus by
optimizing both representativeness and diversity (with optional annotation for
unlabeled data). Second, it ranks these candidates against test queries to
select the most relevant and non-redundant demonstrations. Evaluated on three
biomedical NLP tasks (named entity recognition (NER), relation extraction (RE),
and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along
with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently
outperforms baselines-achieving up to 5% higher macro-F1 scores-while
demonstrating robustness to prompt permutations and class imbalance. Our
findings establish that diversity in initial retrieval is more critical than
ranking-stage optimization, and limiting demonstrations to 3-5 examples
maximizes performance efficiency.

</details>


### [89] [REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08149)
*Wentao Jiang,Xiang Feng,Zengmao Wang,Yong Luo,Pingbo Xu,Zhe Chen,Bo Du,Jing Zhang*

Main category: cs.CL

TL;DR: 论文提出REX-RAG框架，通过混合采样策略和政策校正机制解决LLM在强化学习中陷入无效推理路径的问题，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: LLM在强化学习中常陷入无效推理路径（dead ends），导致决策不准确，影响策略优化。

Method: 提出REX-RAG框架，包括混合采样策略（结合探针采样和探索性提示）和政策校正机制（重要性采样）。

Result: 在七个问答基准测试中，REX-RAG平均性能提升5.1%（Qwen2.5-3B）和3.6%（Qwen2.5-7B）。

Conclusion: REX-RAG有效解决了LLM在强化学习中的推理路径问题，提升了性能。

Abstract: Reinforcement learning (RL) is emerging as a powerful paradigm for enabling
large language models (LLMs) to perform complex reasoning tasks. Recent
advances indicate that integrating RL with retrieval-augmented generation (RAG)
allows LLMs to dynamically incorporate external knowledge, leading to more
informed and robust decision making. However, we identify a critical challenge
during policy-driven trajectory sampling: LLMs are frequently trapped in
unproductive reasoning paths, which we refer to as "dead ends", committing to
overconfident yet incorrect conclusions. This severely hampers exploration and
undermines effective policy optimization. To address this challenge, we propose
REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented
Generation), a novel framework that explores alternative reasoning paths while
maintaining rigorous policy learning through principled distributional
corrections. Our approach introduces two key innovations: (1) Mixed Sampling
Strategy, which combines a novel probe sampling method with exploratory prompts
to escape dead ends; and (2) Policy Correction Mechanism, which employs
importance sampling to correct distribution shifts induced by mixed sampling,
thereby mitigating gradient estimation bias. We evaluate it on seven
question-answering benchmarks, and the experimental results show that REX-RAG
achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B
over strong baselines, demonstrating competitive results across multiple
datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.

</details>


### [90] [LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo](https://arxiv.org/abs/2508.08163)
*Mandira Sawkar,Samay U. Shetty,Deepak Pandita,Tharindu Cyril Weerasooriya,Christopher M. Homan*

Main category: cs.CL

TL;DR: LeWiDi 2025任务通过改进DisCo模型，结合标注者元数据和优化损失函数，显著提升了软标签分布预测和视角评估的性能。


<details>
  <summary>Details</summary>
Motivation: 解决标注者分歧问题，通过建模标注者行为和数据复杂性，提升模型对分歧的捕捉能力。

Method: 扩展DisCo模型，加入标注者元数据、增强输入表示，并优化损失函数以更好地捕捉分歧模式。

Result: 在三个数据集上，软标签和视角评估指标均有显著提升，并通过错误和校准分析验证了改进条件。

Conclusion: 分歧感知建模具有重要价值，系统组件与复杂标注数据的交互提供了深入见解。

Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task is to model
annotator disagreement through soft label distribution prediction and
perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution
from Context), a neural architecture that jointly models item-level and
annotator-level label distributions, and present detailed analysis and
improvements. In this paper, we extend the DisCo by incorporating annotator
metadata, enhancing input representations, and modifying the loss functions to
capture disagreement patterns better. Through extensive experiments, we
demonstrate substantial improvements in both soft and perspectivist evaluation
metrics across three datasets. We also conduct in-depth error and calibration
analyses, highlighting the conditions under which improvements occur. Our
findings underscore the value of disagreement-aware modeling and offer insights
into how system components interact with the complexity of human-annotated
data.

</details>


### [91] [Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions](https://arxiv.org/abs/2508.08192)
*Bangsheng Tang,Carl Chengyan Fu,Fei Kou,Grigory Sizov,Haoci Zhang,Jason Park,Jiawen Liu,Jie You,Qirui Yang,Sachin Mehta,Shengyong Cai,Xiaodong Wang,Xingyu Liu,Yunlu Li,Yanjun Zhou,Wei Wei,Zhiwei Zhao,Zixi Qi,Adolfo Victoria,Aya Ibrahim,Bram Wasti,Changkyu Kim,Daniel Haziza,Fei Sun,Giancarlo Delfin,Emily Guo,Jialin Ouyang,Jaewon Lee,Jianyu Huang,Jeremy Reizenstein,Lu Fang,Quinn Zhu,Ria Verma,Vlad Mihailescu,Xingwen Guo,Yan Cui,Ye Hu,Yejin Lee*

Main category: cs.CL

TL;DR: 本文介绍了针对Llama模型的EAGLE-based推测解码优化技术，实现了生产规模下的最新推理延迟记录，比之前最佳方法快10%，并在大批量处理时提速1.4x至2.0x。


<details>
  <summary>Details</summary>
Motivation: 推测解码是加速大语言模型推理的标准方法，但在生产环境中扩展时面临工程挑战，如高效实现GPU上的树注意力和多轮推测解码操作。

Method: 详细介绍了训练和推理优化技术，特别是针对EAGLE-based推测解码的改进。

Result: 在8个NVIDIA H100 GPU上，Llama4 Maverick的推理延迟达到每token约4毫秒（批量大小为1），比之前最佳方法快10%；大批量处理时提速1.4x至2.0x。

Conclusion: 通过优化技术，实现了生产规模下Llama模型的EAGLE-based推测解码，显著提升了推理速度。

Abstract: Speculative decoding is a standard method for accelerating the inference
speed of large language models. However, scaling it for production environments
poses several engineering challenges, including efficiently implementing
different operations (e.g., tree attention and multi-round speculative
decoding) on GPU. In this paper, we detail the training and inference
optimization techniques that we have implemented to enable EAGLE-based
speculative decoding at a production scale for Llama models. With these
changes, we achieve a new state-of-the-art inference latency for Llama models.
For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a
batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the
previously best known method. Furthermore, for EAGLE-based speculative
decoding, our optimizations enable us to achieve a speed-up for large batch
sizes between 1.4x and 2.0x at production scale.

</details>


### [92] [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](https://arxiv.org/abs/2508.08204)
*Kyle Moore,Jesse Roberts,Daryl Watson*

Main category: cs.CL

TL;DR: 评估大语言模型的不确定性校准，以提升模型控制和用户信任，特别关注推理时不确定性。研究发现多种度量与人类不确定性高度一致，且模型校准效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注模型校准，但较少探讨模型不确定性与人类不确定性的对齐程度，这对提升LLM用户体验至关重要。

Method: 使用现有指标和新变体评估推理时不确定性度量，分析其与人类群体不确定性和传统模型校准的关联。

Result: 多种度量与人类不确定性高度一致，且模型校准效果良好，尽管与人类答案偏好不一致。

Conclusion: 研究显示，某些度量能有效对齐人类不确定性，同时保持模型校准，为实际应用提供了重要参考。

Abstract: There has been much recent interest in evaluating large language models for
uncertainty calibration to facilitate model control and modulate user trust.
Inference time uncertainty, which may provide a real-time signal to the model
or external control modules, is particularly important for applying these
concepts to improve LLM-user experience in practice. While many of the existing
papers consider model calibration, comparatively little work has sought to
evaluate how closely model uncertainty aligns to human uncertainty. In this
work, we evaluate a collection of inference-time uncertainty measures, using
both established metrics and novel variations, to determine how closely they
align with both human group-level uncertainty and traditional notions of model
calibration. We find that numerous measures show evidence of strong alignment
to human uncertainty, even despite the lack of alignment to human answer
preference. For those successful metrics, we find moderate to strong evidence
of model calibration in terms of both correctness correlation and
distributional analysis.

</details>


### [93] [SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling](https://arxiv.org/abs/2508.08211)
*Zhuohao Yu,Xingru Jiang,Weizheng Gu,Yidong Wang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: SAEMark是一种后处理多比特水印框架，通过推理时基于特征的拒绝采样嵌入个性化消息，不改变模型逻辑或需要训练，适用于多语言和领域，保持文本质量。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法会降低文本质量，且需要白盒模型访问和逻辑操作，限制了API模型和多语言场景的应用。

Method: 基于生成文本的确定性特征，选择特征统计与密钥目标对齐的输出，通过采样而非修改嵌入水印。

Result: 实验表明SAEMark在4个数据集上表现一致，英语F1达99.7%，多比特检测准确率高。

Conclusion: SAEMark为闭源LLM提供了一种可扩展的水印范式，支持内容溯源。

Abstract: Watermarking LLM-generated text is critical for content attribution and
misinformation prevention. However, existing methods compromise text quality,
require white-box model access and logit manipulation. These limitations
exclude API-based models and multilingual scenarios. We propose SAEMark, a
general framework for post-hoc multi-bit watermarking that embeds personalized
messages solely via inference-time, feature-based rejection sampling without
altering model logits or requiring training. Our approach operates on
deterministic features extracted from generated text, selecting outputs whose
feature statistics align with key-derived targets. This framework naturally
generalizes across languages and domains while preserving text quality through
sampling LLM outputs instead of modifying. We provide theoretical guarantees
relating watermark success probability and compute budget that hold for any
suitable feature extractor. Empirically, we demonstrate the framework's
effectiveness using Sparse Autoencoders (SAEs), achieving superior detection
accuracy and text quality. Experiments across 4 datasets show SAEMark's
consistent performance, with 99.7% F1 on English and strong multi-bit detection
accuracy. SAEMark establishes a new paradigm for scalable watermarking that
works out-of-the-box with closed-source LLMs while enabling content
attribution.

</details>


### [94] [Capabilities of GPT-5 on Multimodal Medical Reasoning](https://arxiv.org/abs/2508.08224)
*Shansong Wang,Mingzhe Hu,Qiang Li,Mojtaba Safari,Xiaofeng Yang*

Main category: cs.CL

TL;DR: GPT-5作为多模态医疗决策支持系统，在零样本思维链推理任务中表现优于GPT-4o和人类专家，尤其在多模态推理方面提升显著。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（如GPT-5）在医疗领域中的多模态推理能力，以支持复杂的医疗决策。

Method: 通过标准化的文本和多模态问答任务（如MedQA、MedXpertQA等）评估GPT-5及其变体的性能。

Result: GPT-5在所有基准测试中均达到最先进水平，多模态推理能力显著提升，超越人类专家表现。

Conclusion: GPT-5在医疗决策支持中表现出色，未来可能显著影响临床决策系统的设计。

Abstract: Recent advances in large language models (LLMs) have enabled general-purpose
systems to perform increasingly complex domain-specific reasoning without
extensive fine-tuning. In the medical domain, decision-making often requires
integrating heterogeneous information sources, including patient narratives,
structured data, and medical images. This study positions GPT-5 as a generalist
multimodal reasoner for medical decision support and systematically evaluates
its zero-shot chain-of-thought reasoning performance on both text-based
question answering and visual question answering tasks under a unified
protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20
against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU
medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that
GPT-5 consistently outperforms all baselines, achieving state-of-the-art
accuracy across all QA benchmarks and delivering substantial gains in
multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and
understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and
surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in
understanding. In contrast, GPT-4o remains below human expert performance in
most dimensions. A representative case study demonstrates GPT-5's ability to
integrate visual and textual cues into a coherent diagnostic reasoning chain,
recommending appropriate high-stakes interventions. Our results show that, on
these controlled multimodal reasoning benchmarks, GPT-5 moves from
human-comparable to above human-expert performance. This improvement may
substantially inform the design of future clinical decision-support systems.

</details>


### [95] [Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge](https://arxiv.org/abs/2508.08236)
*Yunna Cai,Fan Wang,Haowei Wang,Kun Wang,Kailai Yang,Sophia Ananiadou,Moyan Li,Mingming Fan*

Main category: cs.CL

TL;DR: PsyCrisis-Bench是一个基于真实中文心理健康对话的无参考评估基准，用于评估LLM响应是否符合专家定义的安全原则。


<details>
  <summary>Details</summary>
Motivation: 由于高风险心理健康对话缺乏黄金标准答案且涉及伦理敏感性，评估LLM响应的安全对齐具有挑战性。

Method: 采用基于提示的LLM-as-Judge方法，结合专家定义的心理学干预原则进行上下文评估，并使用多维度二元评分。

Result: 在3600次评估中，该方法与专家评估一致性最高，且评估理由更易解释。

Conclusion: PsyCrisis-Bench及其评估工具公开可用，以推动进一步研究。

Abstract: Evaluating the safety alignment of LLM responses in high-risk mental health
dialogues is particularly difficult due to missing gold-standard answers and
the ethically sensitive nature of these interactions. To address this
challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark
based on real-world Chinese mental health dialogues. It evaluates whether the
model responses align with the safety principles defined by experts.
Specifically designed for settings without standard references, our method
adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation
using expert-defined reasoning chains grounded in psychological intervention
principles. We employ binary point-wise scoring across multiple safety
dimensions to enhance the explainability and traceability of the evaluation.
Additionally, we present a manually curated, high-quality Chinese-language
dataset covering self-harm, suicidal ideation, and existential distress,
derived from real-world online discourse. Experiments on 3600 judgments show
that our method achieves the highest agreement with expert assessments and
produces more interpretable evaluation rationales compared to existing
approaches. Our dataset and evaluation tool are publicly available to
facilitate further research.

</details>


### [96] [Jinx: Unlimited LLMs for Probing Alignment Failures](https://arxiv.org/abs/2508.08243)
*Jiahao Zhao,Liwei Dong*

Main category: cs.CL

TL;DR: 论文介绍了Jinx，一种无安全限制的语言模型变体，用于评估对齐失败和研究安全边界。


<details>
  <summary>Details</summary>
Motivation: 研究社区缺乏无安全限制的语言模型工具，用于评估安全对齐的失败和边界。

Method: 开发了Jinx，一种基于流行开源权重LLM的无限制变体，保留基础模型的推理和指令遵循能力。

Result: Jinx为研究人员提供了一个工具，用于探测对齐失败、评估安全边界和系统研究语言模型的安全失败模式。

Conclusion: Jinx填补了研究社区在无限制语言模型工具方面的空白，有助于更全面地评估和改进模型安全对齐。

Abstract: Unlimited, or so-called helpful-only language models are trained without
safety alignment constraints and never refuse user queries. They are widely
used by leading AI companies as internal tools for red teaming and alignment
evaluation. For example, if a safety-aligned model produces harmful outputs
similar to an unlimited model, this indicates alignment failures that require
further attention. Despite their essential role in assessing alignment, such
models are not available to the research community.
  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx
responds to all queries without refusals or safety filtering, while preserving
the base model's capabilities in reasoning and instruction following. It
provides researchers with an accessible tool for probing alignment failures,
evaluating safety boundaries, and systematically studying failure modes in
language model safety.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [97] [Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems](https://arxiv.org/abs/2508.06539)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: 论文提出了一种基于几何流形的生存模型理论（SOSM），将生存视为生物状态空间中的几何属性，而非依赖标注标签。


<details>
  <summary>Details</summary>
Motivation: 传统生存模型依赖标注标签和固定协变量，本文认为生存是生物状态空间中的几何属性，提出无需标注的理论框架。

Method: 提出自组织生存流形（SOSM）理论，基于低曲率测地流和生存能量泛函，推导离散和连续目标函数。

Result: 证明了在生物合理条件下，生存对齐轨迹的涌现和收敛，并将生存建模与物理定律（如热力学、熵流）联系起来。

Conclusion: 将健康、疾病、衰老和死亡视为流形结构的几何相变，为生存建模提供了无需标注的通用理论基础。

Abstract: Survival is traditionally modeled as a supervised learning task, reliant on
curated outcome labels and fixed covariates. This work rejects that premise. It
proposes that survival is not an externally annotated target but a geometric
consequence: an emergent property of the curvature and flow inherent in
biological state space. We develop a theory of Self-Organizing Survival
Manifolds (SOSM), in which survival-relevant dynamics arise from low-curvature
geodesic flows on latent manifolds shaped by internal biological constraints. A
survival energy functional based on geodesic curvature minimization is
introduced and shown to induce structures where prognosis aligns with geometric
flow stability. We derive discrete and continuous formulations of the objective
and prove theoretical results demonstrating the emergence and convergence of
survival-aligned trajectories under biologically plausible conditions. The
framework draws connections to thermodynamic efficiency, entropy flow, Ricci
curvature, and optimal transport, grounding survival modeling in physical law.
Health, disease, aging, and death are reframed as geometric phase transitions
in the manifold's structure. This theory offers a universal, label-free
foundation for modeling survival as a property of form, not annotation-bridging
machine learning, biophysics, and the geometry of life itself.

</details>


### [98] [Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering](https://arxiv.org/abs/2508.06574)
*Fatemeh Moradi,Mehran Tarif,Mohammadhossein Homaei*

Main category: cs.LG

TL;DR: 论文提出了一种两阶段学习框架，结合无监督异常检测和半监督学习，用于供应链欺诈检测，效果显著。


<details>
  <summary>Details</summary>
Motivation: 现代供应链欺诈检测面临复杂性和标记数据稀缺的挑战，传统方法因类别不平衡和有限监督而效果不佳。

Method: 第一阶段使用Isolation Forest进行无监督异常检测，减少数据量；第二阶段通过自训练SVM结合标记和高置信度伪标记样本进行半监督学习。

Result: 在DataCo数据集上F1-score达0.817，误报率低于3.0%。

Conclusion: 该方法有效结合无监督预过滤和半监督优化，但需进一步解决概念漂移问题并与深度学习方法比较。

Abstract: Detecting fraud in modern supply chains is a growing challenge, driven by the
complexity of global networks and the scarcity of labeled data. Traditional
detection methods often struggle with class imbalance and limited supervision,
reducing their effectiveness in real-world applications. This paper proposes a
novel two-phase learning framework to address these challenges. In the first
phase, the Isolation Forest algorithm performs unsupervised anomaly detection
to identify potential fraud cases and reduce the volume of data requiring
further analysis. In the second phase, a self-training Support Vector Machine
(SVM) refines the predictions using both labeled and high-confidence
pseudo-labeled samples, enabling robust semi-supervised learning. The proposed
method is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive
real-world supply chain dataset with fraud indicators. It achieves an F1-score
of 0.817 while maintaining a false positive rate below 3.0%. These results
demonstrate the effectiveness and efficiency of combining unsupervised
pre-filtering with semi-supervised refinement for supply chain fraud detection
under real-world constraints, though we acknowledge limitations regarding
concept drift and the need for comparison with deep learning approaches.

</details>


### [99] [GFlowNets for Learning Better Drug-Drug Interaction Representations](https://arxiv.org/abs/2508.06576)
*Azmine Toushik Wasi*

Main category: cs.LG

TL;DR: 提出了一种结合生成流网络（GFlowNet）和变分图自编码器（VGAE）的框架，用于生成罕见药物相互作用的合成样本，以解决数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用（DDI）预测中，数据严重不平衡导致模型对罕见但关键相互作用的预测性能较差。现有方法多将问题简化为二分类，忽略了类别特异性。

Method: 结合GFlowNet和VGAE生成罕见类别的合成样本，改善数据平衡，并生成新颖有效的DDI对。

Result: 该方法提高了对所有类型相互作用的预测性能，增强了临床可靠性。

Conclusion: 提出的框架有效解决了DDI预测中的数据不平衡问题，提升了模型在罕见类别上的表现。

Abstract: Drug-drug interactions pose a significant challenge in clinical pharmacology,
with severe class imbalance among interaction types limiting the effectiveness
of predictive models. Common interactions dominate datasets, while rare but
critical interactions remain underrepresented, leading to poor model
performance on infrequent cases. Existing methods often treat DDI prediction as
a binary problem, ignoring class-specific nuances and exacerbating bias toward
frequent interactions. To address this, we propose a framework combining
Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)
to generate synthetic samples for rare classes, improving model balance and
generate effective and novel DDI pairs. Our approach enhances predictive
performance across interaction types, ensuring better clinical reliability.

</details>


### [100] [Hypergraph Neural Network with State Space Models for Node Classification](https://arxiv.org/abs/2508.06587)
*A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了一种新型超图神经网络HGMN，结合角色感知表示和状态空间模型，显著提升节点分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统GNN主要关注节点间的邻接关系，忽略了角色特征的重要性，现有方法多为无监督且性能不佳。

Method: HGMN利用超图构建技术建模高阶关系，通过可学习的mamba transformer机制结合角色和邻接表示，并引入残差网络防止过平滑。

Result: 在多个数据集上表现优于现有GNN方法，显著提升了节点分类任务的性能。

Conclusion: HGMN通过有效整合角色特征和邻接信息，成为图学习任务的强大工具。

Abstract: In recent years, graph neural networks (GNNs) have gained significant
attention for node classification tasks on graph-structured data. However,
traditional GNNs primarily focus on adjacency relationships between nodes,
often overlooking the rich role-based characteristics that are crucial for
learning more expressive node representations. Existing methods for capturing
role-based features are largely unsupervised and fail to achieve optimal
performance in downstream tasks. To address these limitations, we propose a
novel hypergraph neural network with state space model (HGMN) that effectively
integrates role-aware representations into GNNs and the state space model. HGMN
utilizes hypergraph construction techniques to model higher-order relationships
and combines role-based and adjacency-based representations through a learnable
mamba transformer mechanism. By leveraging two distinct hypergraph construction
methods-based on node degree and neighborhood levels, it strengthens the
connections among nodes with similar roles, enhancing the model's
representational power. Additionally, the inclusion of hypergraph convolution
layers enables the model to capture complex dependencies within hypergraph
structures. To mitigate the over-smoothing problem inherent in deep GNNs, we
incorporate a residual network, ensuring improved stability and better feature
propagation across layers. Extensive experiments conducted on one newly
introduced dataset and four benchmark datasets demonstrate the superiority of
HGMN. The model achieves significant performance improvements on node
classification tasks compared to state-of-the-art GNN methods. These results
highlight HGMN's ability to provide enriched node representations by
effectively embedding role-based features alongside adjacency information,
making it a versatile and powerful tool for a variety of graph-based learning
applications.

</details>


### [101] [Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning](https://arxiv.org/abs/2508.06588)
*Zian Zhai,Fan Li,Xingyu Tan,Xiaoyang Wang,Wenjie Zhang*

Main category: cs.LG

TL;DR: 论文提出RGVQ框架，通过引入图拓扑和特征相似性作为正则化信号，解决图数据向量量化中的码本坍塌问题，提升表达性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 图数据向量量化（VQ）中的码本坍塌问题限制了图标记的表达性和泛化能力，现有方法在视觉或语言领域的缓解策略对图数据效果不佳。

Method: 提出RGVQ框架，结合图拓扑和特征相似性作为正则化信号，使用Gumbel-Softmax重新参数化实现软分配，并引入结构感知对比正则化。

Result: 实验表明RGVQ显著提高了码本利用率，并在多个下游任务中提升了图VQ骨干模型的性能。

Conclusion: RGVQ通过增强码本利用和标记多样性，为图数据提供了更具表达性和可迁移性的表示。

Abstract: Vector Quantization (VQ) has recently emerged as a promising approach for
learning discrete representations of graph-structured data. However, a
fundamental challenge, i.e., codebook collapse, remains underexplored in the
graph domain, significantly limiting the expressiveness and generalization of
graph tokens.In this paper, we present the first empirical study showing that
codebook collapse consistently occurs when applying VQ to graph data, even with
mitigation strategies proposed in vision or language domains. To understand why
graph VQ is particularly vulnerable to collapse, we provide a theoretical
analysis and identify two key factors: early assignment imbalances caused by
redundancy in graph features and structural patterns, and self-reinforcing
optimization loops in deterministic VQ. To address these issues, we propose
RGVQ, a novel framework that integrates graph topology and feature similarity
as explicit regularization signals to enhance codebook utilization and promote
token diversity. RGVQ introduces soft assignments via Gumbel-Softmax
reparameterization, ensuring that all codewords receive gradient updates. In
addition, RGVQ incorporates a structure-aware contrastive regularization to
penalize the token co-assignments among similar node pairs. Extensive
experiments demonstrate that RGVQ substantially improves codebook utilization
and consistently boosts the performance of state-of-the-art graph VQ backbones
across multiple downstream tasks, enabling more expressive and transferable
graph token representations.

</details>


### [102] [A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis](https://arxiv.org/abs/2508.06589)
*Xinglin Zhao,Yanwen Wang,Xiaobo Liu,Yanrong Hao,Rui Cao,Xin Wen*

Main category: cs.LG

TL;DR: 提出了一种针对神经影像CAD系统的联邦学习框架，通过动态导航和元整合模块处理数据异质性和亚型混淆，显著提高了诊断准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决小样本研究可重复性低和大规模数据集中亚型异质性导致的混淆问题。

Method: 采用动态导航模块将样本路由到最适合的本地模型，并通过元整合模块将异质本地模型的预测结果统一为诊断输出。

Result: 在包含1300多名MDD患者和1100名健康对照的fMRI数据上测试，平均准确率达74.06%，优于传统方法。

Conclusion: 该框架通过处理数据异质性和亚型混淆，提升了神经影像CAD系统的可靠性和可重复性，对个性化医疗和临床决策具有重要潜力。

Abstract: Computer-aided diagnosis (CAD) systems play a crucial role in analyzing
neuroimaging data for neurological and psychiatric disorders. However,
small-sample studies suffer from low reproducibility, while large-scale
datasets introduce confounding heterogeneity due to multiple disease subtypes
being labeled under a single category. To address these challenges, we propose
a novel federated learning framework tailored for neuroimaging CAD systems. Our
approach includes a dynamic navigation module that routes samples to the most
suitable local models based on latent subtype representations, and a
meta-integration module that combines predictions from heterogeneous local
models into a unified diagnostic output. We evaluated our framework using a
comprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100
healthy controls across multiple study cohorts. Experimental results
demonstrate significant improvements in diagnostic accuracy and robustness
compared to traditional methods. Specifically, our framework achieved an
average accuracy of 74.06\% across all tested sites, showcasing its
effectiveness in handling subtype heterogeneity and enhancing model
generalizability. Ablation studies further confirmed the importance of both the
dynamic navigation and meta-integration modules in improving performance. By
addressing data heterogeneity and subtype confounding, our framework advances
reliable and reproducible neuroimaging CAD systems, offering significant
potential for personalized medicine and clinical decision-making in neurology
and psychiatry.

</details>


### [103] [Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials](https://arxiv.org/abs/2508.06591)
*Rachel K. Luu,Jingyu Deng,Mohammed Shahrudin Ibrahim,Nam-Joon Cho,Ming Dao,Subra Suresh,Markus J. Buehler*

Main category: cs.LG

TL;DR: 论文提出了一种结合生成式AI和多学科文献的新框架，用于生物启发材料设计和实验，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在跨学科实验科学（如材料科学）中的应用，以解决传统方法难以处理的复杂问题。

Method: 结合BioinspiredLLM、RAG、代理系统和分层采样策略，提取结构-性能关系并生成实验假设。

Result: 成功设计并实验验证了一种新型花粉基粘合剂，展示了AI辅助设计的实际应用潜力。

Conclusion: AI辅助设计能够推动现实世界的材料创新，并促进人机协作的有效性。

Abstract: Large language models (LLMs) have reshaped the research landscape by enabling
new approaches to knowledge retrieval and creative ideation. Yet their
application in discipline-specific experimental science, particularly in highly
multi-disciplinary domains like materials science, remains limited. We present
a first-of-its-kind framework that integrates generative AI with literature
from hitherto-unconnected fields such as plant science, biomimetics, and
materials engineering to extract insights and design experiments for materials.
We focus on humidity-responsive systems such as pollen-based materials and
Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and
adaptive performance. Using a suite of AI tools, including a fine-tuned model
(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a
Hierarchical Sampling strategy, we extract structure-property relationships and
translate them into new classes of bioinspired materials. Structured inference
protocols generate and evaluate hundreds of hypotheses from a single query,
surfacing novel and experimentally tractable ideas. We validate our approach
through real-world implementation: LLM-generated procedures, materials designs,
and mechanical predictions were tested in the laboratory, culminating in the
fabrication of a novel pollen-based adhesive with tunable morphology and
measured shear strength, establishing a foundation for future plant-derived
adhesive design. This work demonstrates how AI-assisted ideation can drive
real-world materials design and enable effective human-AI collaboration.

</details>


### [104] [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601)
*Kyle O'Brien,Stephen Casper,Quentin Anthony,Tomek Korbak,Robert Kirk,Xander Davies,Ishan Mishra,Geoffrey Irving,Yarin Gal,Stella Biderman*

Main category: cs.LG

TL;DR: 研究探讨通过过滤训练数据中的双用途主题文本，提高开放权重AI系统的抗篡改能力，并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 开放权重AI系统易受篡改攻击，现有安全微调方法效果有限，需探索更有效的防护措施。

Method: 提出多阶段数据过滤流程，预训练6.9B参数模型，测试其对生物威胁相关文本的抵抗能力。

Result: 过滤模型在对抗性微调攻击中表现优异，但对上下文提供的危险信息仍敏感。

Conclusion: 预训练数据过滤是开放权重AI系统防护的有效手段，但仍需多层次防御策略。

Abstract: Open-weight AI systems offer unique benefits, including enhanced
transparency, open research, and decentralized access. However, they are
vulnerable to tampering attacks which can efficiently elicit harmful behaviors
by modifying weights or activations. Currently, there is not yet a robust
science of open-weight model risk management. Existing safety fine-tuning
methods and other post-training techniques have struggled to make LLMs
resistant to more than a few dozen steps of adversarial fine-tuning. In this
paper, we investigate whether filtering text about dual-use topics from
training data can prevent unwanted capabilities and serve as a more
tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable
data filtering and show that it offers a tractable and effective method for
minimizing biothreat proxy knowledge in LLMs. We pretrain multiple
6.9B-parameter models from scratch and find that they exhibit substantial
resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M
tokens of biothreat-related text -- outperforming existing post-training
baselines by over an order of magnitude -- with no observed degradation to
unrelated capabilities. However, while filtered models lack internalized
dangerous knowledge, we find that they can still leverage such information when
it is provided in context (e.g., via search tool augmentation), demonstrating a
need for a defense-in-depth approach. Overall, these findings help to establish
pretraining data curation as a promising layer of defense for open-weight AI
systems.

</details>


### [105] [Local Diffusion Models and Phases of Data Distributions](https://arxiv.org/abs/2508.06614)
*Fangjun Hu,Guangkuo Liu,Yifan Zhang,Xun Gao*

Main category: cs.LG

TL;DR: 扩散模型通过局部去噪器降低计算成本，提出数据分布相的概念，并证明在相变点附近需全局网络。


<details>
  <summary>Details</summary>
Motivation: 现实数据（如图像）具有低维空间结构，但传统扩散模型忽略局部结构，计算成本高。

Method: 定义数据分布相，通过局部操作连接分布，分析去噪过程的相变，提出信息论边界。

Result: 局部去噪器在相变点外有效，相变点附近需全局网络，实验验证了理论。

Conclusion: 简化扩散模型架构，局部网络适用于大部分时间，全局网络仅需用于相变点附近，为生成AI研究提供新方向。

Abstract: As a class of generative artificial intelligence frameworks inspired by
statistical physics, diffusion models have shown extraordinary performance in
synthesizing complicated data distributions through a denoising process
gradually guided by score functions. Real-life data, like images, is often
spatially structured in low-dimensional spaces. However, ordinary diffusion
models ignore this local structure and learn spatially global score functions,
which are often computationally expensive. In this work, we introduce a new
perspective on the phases of data distributions, which provides insight into
constructing local denoisers with reduced computational costs. We define two
distributions as belonging to the same data distribution phase if they can be
mutually connected via spatially local operations such as local denoisers.
Then, we show that the reverse denoising process consists of an early trivial
phase and a late data phase, sandwiching a rapid phase transition where local
denoisers must fail. To diagnose such phase transitions, we prove an
information-theoretic bound on the fidelity of local denoisers based on
conditional mutual information, and conduct numerical experiments in a
real-world dataset. This work suggests simpler and more efficient architectures
of diffusion models: far from the phase transition point, we can use small
local neural networks to compute the score function; global neural networks are
only necessary around the narrow time interval of phase transitions. This
result also opens up new directions for studying phases of data distributions,
the broader science of generative artificial intelligence, and guiding the
design of neural networks inspired by physics concepts.

</details>


### [106] [Generalizing Scaling Laws for Dense and Sparse Large Language Models](https://arxiv.org/abs/2508.06617)
*Md Arafat Hossain,Xingfu Wu,Valerie Taylor,Ali Jannesari*

Main category: cs.LG

TL;DR: 本文提出了一种适用于密集和稀疏大型语言模型的通用扩展定律，以解决现有扩展定律的架构特定性问题。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模和训练计算成本的快速增长，研究人员需要更高效的训练技术，但现有扩展定律多为架构特定，缺乏通用性。

Method: 重新审视现有扩展定律，提出一种通用扩展定律框架，适用于密集和稀疏模型，并通过评估与比较验证其有效性。

Result: 提出的通用扩展定律在密集和稀疏模型上均表现出有效性，优于现有架构特定的扩展定律。

Conclusion: 通用扩展定律为大型语言模型的训练资源分配和规模预测提供了统一框架，具有广泛适用性。

Abstract: Over the past few years, the size of language models has grown exponentially,
as has the computational cost to train these large models. This rapid growth
has motivated researchers to develop new techniques aimed at enhancing the
efficiency of the training process. Despite these advancements, optimally
predicting the model size or allocating optimal resources remains a challenge.
Several efforts have addressed the challenge by proposing different scaling
laws, but almost all of them are architecture-specific (dense or sparse). In
this work we revisit existing scaling laws and propose a generalized scaling
law to provide a unified framework that is applicable to both dense and sparse
large language models. We evaluate and compare our proposed scaling law with
existing scaling laws to demonstrate its effectiveness.

</details>


### [107] [Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels](https://arxiv.org/abs/2508.06622)
*Jeremiah Birrell,Reza Ebrahimi*

Main category: cs.LG

TL;DR: ANTIDOTE是一种针对噪声标签学习的新目标函数，通过信息散度邻域的松弛定义，利用凸对偶性转化为对抗训练方法，计算成本与标准交叉熵损失相近。它能自适应减少噪声标签样本的影响，类似于遗忘这些样本。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据中固有或由对手引入的噪声标签问题，提升模型在噪声环境下的鲁棒性。

Method: 通过信息散度邻域的松弛定义目标函数，利用凸对偶性转化为对抗训练方法。

Result: 在不同类型（对称、非对称、人工标注和真实世界）的噪声标签下表现优异，计算效率接近标准交叉熵损失。

Conclusion: ANTIDOTE在噪声标签学习中表现优越，计算高效，适用于实际噪声环境。

Abstract: We introduce ANTIDOTE, a new class of objectives for learning under noisy
labels which are defined in terms of a relaxation over an
information-divergence neighborhood. Using convex duality, we provide a
reformulation as an adversarial training method that has similar computational
cost to training with standard cross-entropy loss. We show that our approach
adaptively reduces the influence of the samples with noisy labels during
learning, exhibiting a behavior that is analogous to forgetting those samples.
ANTIDOTE is effective in practical environments where label noise is inherent
in the training data or where an adversary can alter the training labels.
Extensive empirical evaluations on different levels of symmetric, asymmetric,
human annotation, and real-world label noise show that ANTIDOTE outperforms
leading comparable losses in the field and enjoys a time complexity that is
very close to that of the standard cross entropy loss.

</details>


### [108] [Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record](https://arxiv.org/abs/2508.06627)
*Mosbah Aouad,Anirudh Choudhary,Awais Farooq,Steven Nevers,Lusine Demirkhanyan,Bhrandon Harris,Suguna Pappu,Christopher Gondi,Ravishankar Iyer*

Main category: cs.LG

TL;DR: 提出一种多模态方法，结合电子健康记录中的诊断代码和实验室数据，用于早期检测胰腺导管腺癌（PDAC）。


<details>
  <summary>Details</summary>
Motivation: PDAC早期检测困难，缺乏特异性症状和可靠生物标志物。

Method: 结合神经控制微分方程、预训练语言模型、循环网络和交叉注意力机制，整合诊断代码和实验室数据。

Result: 在4700名患者数据集上，AUC提升6.5%至15.5%，并识别出新的生物标志物。

Conclusion: 该方法显著提升PDAC早期检测性能，并发现新的风险标志物。

Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and
early detection remains a major clinical challenge due to the absence of
specific symptoms and reliable biomarkers. In this work, we propose a new
multimodal approach that integrates longitudinal diagnosis code histories and
routinely collected laboratory measurements from electronic health records to
detect PDAC up to one year prior to clinical diagnosis. Our method combines
neural controlled differential equations to model irregular lab time series,
pretrained language models and recurrent networks to learn diagnosis code
trajectory representations, and cross-attention mechanisms to capture
interactions between the two modalities. We develop and evaluate our approach
on a real-world dataset of nearly 4,700 patients and achieve significant
improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods.
Furthermore, our model identifies diagnosis codes and laboratory panels
associated with elevated PDAC risk, including both established and new
biomarkers. Our code is available at
https://github.com/MosbahAouad/EarlyPDAC-MML.

</details>


### [109] [Using Imperfect Synthetic Data in Downstream Inference Tasks](https://arxiv.org/abs/2508.06635)
*Yewon Byun,Shantanu Gupta,Zachary C. Lipton,Rachel Leah Childers,Bryan Wilder*

Main category: cs.LG

TL;DR: 论文提出了一种基于广义矩方法的新估计器，用于结合大语言模型生成的合成数据与真实数据，以在计算社会科学中实现统计有效的结论。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决如何将大语言模型生成的合成数据与真实数据结合，并确保统计结论的有效性。

Method: 方法是通过广义矩方法构建一个无需超参数的新估计器，利用合成数据与真实数据的矩残差交互来改进目标参数估计。

Result: 实证结果表明，该方法在计算社会科学的不同回归任务中表现优异，显著提升了估计效果。

Conclusion: 结论是该方法为合成数据与真实数据的结合提供了理论保障和实际应用价值。

Abstract: Predictions and generations from large language models are increasingly being
explored as an aid to computational social science and human subject research
in limited data regimes. While previous technical work has explored the
potential to use model-predicted labels for unlabeled data in a principled
manner, there is increasing interest in using large language models to generate
entirely new synthetic samples (also termed as synthetic simulations), such as
in responses to surveys. However, it is not immediately clear by what means
practitioners can combine such data with real data and yet produce
statistically valid conclusions upon them. In this work, we introduce a new
estimator based on generalized method of moments, providing a
hyperparameter-free solution with strong theoretical guarantees to address the
challenge at hand. Surprisingly, we find that interactions between the moment
residuals of synthetic data and those of real data can improve estimates of the
target parameter. We empirically validate the finite-sample performance of our
estimator across different regression tasks in computational social science
applications, demonstrating large empirical gains.

</details>


### [110] [Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series](https://arxiv.org/abs/2508.06638)
*Muyan Anna Li,Aditi Gautam*

Main category: cs.LG

TL;DR: 论文提出两种自适应阈值框架（SCS和MACS），用于非平稳时间序列中的异常检测，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统静态阈值在非平稳环境中失效，需适应统计特性随时间变化的需求。

Method: 引入SCS和MACS框架，结合在线学习和分段原理，实现局部自适应。

Result: 在Wafer Manufacturing数据集上，F1分数显著优于传统百分位和滚动分位数方法。

Conclusion: 自适应阈值能实现可靠、可解释且及时的异常检测。

Abstract: As time series data become increasingly prevalent in domains such as
manufacturing, IT, and infrastructure monitoring, anomaly detection must adapt
to nonstationary environments where statistical properties shift over time.
Traditional static thresholds are easily rendered obsolete by regime shifts,
concept drift, or multi-scale changes. To address these challenges, we
introduce and empirically evaluate two novel adaptive thresholding frameworks:
Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence
Segments (MACS). Both leverage statistical online learning and segmentation
principles for local, contextually sensitive adaptation, maintaining guarantees
on false alarm rates even under evolving distributions. Our experiments across
Wafer Manufacturing benchmark datasets show significant F1-score improvement
compared to traditional percentile and rolling quantile approaches. This work
demonstrates that robust, statistically principled adaptive thresholds enable
reliable, interpretable, and timely detection of diverse real-world anomalies.

</details>


### [111] [Fractal Language Modelling by Universal Sequence Maps (USM)](https://arxiv.org/abs/2508.06641)
*Jonas S Almeida,Daniel E Russ,Susana Vinga,Ines Duarte,Lee Mason,Praphulla Bhawsar,Aaron Ge,Arlindo Oliveira,Jeya Balaji Balasubramanian*

Main category: cs.LG

TL;DR: 论文提出了一种改进的通用序列映射（USM）方法，解决了迭代过程中的种子偏差问题，并揭示了USM作为高效数值过程的特性。


<details>
  <summary>Details</summary>
Motivation: 随着基于Transformer的语言模型（如ChatGPT）的兴起，研究者对多尺度和嵌入维度的符号序列数值表示方法重新产生兴趣。编码的挑战在于需要保留符号序列的上下文信息。

Method: USM由两个混沌游戏表示（CGR）组成，通过前向和后向迭代将符号序列双射编码到数值空间，并投影到频域（FCGR）。改进后的USM解决了种子偏差问题。

Result: 1）完全实现了数值定位与序列身份的一致性；2）揭示了USM作为高效数值过程收敛于稳态序列嵌入解的特性。实验以基因组序列为例，但适用于任意基数字母表。

Conclusion: 改进后的USM不仅解决了种子偏差问题，还展示了其作为高效数值过程的潜力，适用于多种符号序列的编码需求。

Abstract: Motivation: With the advent of Language Models using Transformers,
popularized by ChatGPT, there is a renewed interest in exploring encoding
procedures that numerically represent symbolic sequences at multiple scales and
embedding dimensions. The challenge that encoding addresses is the need for
mechanisms that uniquely retain contextual information about the succession of
individual symbols, which can then be modeled by nonlinear formulations such as
neural networks.
  Context: Universal Sequence Maps(USM) are iterated functions that bijectively
encode symbolic sequences onto embedded numerical spaces. USM is composed of
two Chaos Game Representations (CGR), iterated forwardly and backwardly, that
can be projected into the frequency domain (FCGR). The corresponding USM
coordinates can be used to compute a Chebyshev distance metric as well as k-mer
frequencies, without having to recompute the embedded numeric coordinates, and,
paradoxically, allowing for non-integers values of k.
  Results: This report advances the bijective fractal encoding by Universal
Sequence Maps (USM) by resolving seeding biases affecting the iterated process.
The resolution had two results, the first expected, the second an intriguing
outcome: 1) full reconciliation of numeric positioning with sequence identity;
and 2) uncovering the nature of USM as an efficient numeric process converging
towards a steady state sequence embedding solution. We illustrate these results
for genomic sequences because of the convenience of a planar representation
defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless,
the application to alphabet of arbitrary cardinality was found to be
straightforward.

</details>


### [112] [Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN](https://arxiv.org/abs/2508.06647)
*Andrey Sidorenko,Paul Tiwald*

Main category: cs.LG

TL;DR: TabularARGN是一种专为生成高质量合成表格数据设计的神经网络架构，具有高数据保真度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统匿名化技术无法充分保护隐私，需要更有效的合成数据生成方法。

Method: 采用基于离散化的自回归方法设计TabularARGN。

Result: 在统计相似性、机器学习实用性和检测鲁棒性方面表现优异，隐私评估显示其具有稳健的隐私-效用平衡。

Conclusion: TabularARGN在合成数据生成中表现出色，平衡了隐私保护和数据实用性。

Abstract: Synthetic data generation has become essential for securely sharing and
analyzing sensitive data sets. Traditional anonymization techniques, however,
often fail to adequately preserve privacy. We introduce the Tabular
Auto-Regressive Generative Network (TabularARGN), a neural network architecture
specifically designed for generating high-quality synthetic tabular data. Using
a discretization-based auto-regressive approach, TabularARGN achieves high data
fidelity while remaining computationally efficient. We evaluate TabularARGN
against existing synthetic data generation methods, showing competitive results
in statistical similarity, machine learning utility, and detection robustness.
We further perform an in-depth privacy evaluation using systematic
membership-inference attacks, highlighting the robustness and effective
privacy-utility balance of our approach.

</details>


### [113] [In-Context Reinforcement Learning via Communicative World Models](https://arxiv.org/abs/2508.06659)
*Fernando Martinez-Lopez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: 论文提出CORAL框架，通过将表征学习与控制解耦，提升强化学习代理的上下文适应能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理在未更新参数时难以泛化到新任务，因其表征和策略过拟合于训练环境。

Method: CORAL将上下文强化学习视为双代理通信问题，引入信息代理（IA）和控制代理（CA）。IA预训练为世界模型，生成简洁消息，CA利用消息解决任务。

Result: 实验表明，CORAL显著提升样本效率，并在未见稀疏奖励环境中实现零样本适应。

Conclusion: 学习可迁移的通信表征能有效提升强化学习代理的上下文适应能力。

Abstract: Reinforcement learning (RL) agents often struggle to generalize to new tasks
and contexts without updating their parameters, mainly because their learned
representations and policies are overfit to the specifics of their training
environments. To boost agents' in-context RL (ICRL) ability, this work
formulates ICRL as a two-agent emergent communication problem and introduces
CORAL (Communicative Representation for Adaptive RL), a framework that learns a
transferable communicative context by decoupling latent representation learning
from control. In CORAL, an Information Agent (IA) is pre-trained as a world
model on a diverse distribution of tasks. Its objective is not to maximize task
reward, but to build a world model and distill its understanding into concise
messages. The emergent communication protocol is shaped by a novel Causal
Influence Loss, which measures the effect that the message has on the next
action. During deployment, the previously trained IA serves as a fixed
contextualizer for a new Control Agent (CA), which learns to solve tasks by
interpreting the provided communicative context. Our experiments demonstrate
that this approach enables the CA to achieve significant gains in sample
efficiency and successfully perform zero-shot adaptation with the help of
pre-trained IA in entirely unseen sparse-reward environments, validating the
efficacy of learning a transferable communicative representation.

</details>


### [114] [Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.06663)
*Yuan-Hung Chao,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 将KANs集成到GNN架构中，提出KGAT、KSGC和KAPPNP模型，并通过知识融合提升性能。


<details>
  <summary>Details</summary>
Motivation: GNN依赖图连接性，限制了可扩展性和效率，而KANs具有强非线性表达能力和高效推理能力。

Method: 将KANs集成到GAT、SGC和APPNP三种GNN架构中，并采用多教师知识融合框架。

Result: 实验表明，新模型提高了节点分类准确性，知识融合显著提升了学生模型性能。

Conclusion: KANs能增强GNN表达能力，并实现高效的图无关推理。

Abstract: Graph Neural Networks (GNNs) have shown strong performance on
graph-structured data, but their reliance on graph connectivity often limits
scalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent
architecture with learnable univariate functions, offer strong nonlinear
expressiveness and efficient inference. In this work, we integrate KANs into
three popular GNN architectures-GAT, SGC, and APPNP-resulting in three new
models: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge
amalgamation framework, where knowledge from multiple KAN-based GNNs is
distilled into a graph-independent KAN student model. Experiments on benchmark
datasets show that the proposed models improve node classification accuracy,
and the knowledge amalgamation approach significantly boosts student model
performance. Our findings highlight the potential of KANs for enhancing GNN
expressiveness and for enabling efficient, graph-free inference.

</details>


### [115] [Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation](https://arxiv.org/abs/2508.06676)
*Chia-Hsun Lu,Guan-Jhih Wu,Ya-Chi Ho,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 提出了一种针对Kolmogorov-Arnold Networks（KAN）的新型水印方法DCT-AW，通过离散余弦变换扰动激活输出，确保任务独立性并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中知识产权保护的重要性增加，现有水印方法难以适应KAN的新型架构，需要一种专门的水印技术。

Method: 利用KAN的可学习激活函数，通过离散余弦变换（DCT）扰动激活输出嵌入水印。

Result: 实验表明DCT-AW对模型性能影响小，且对微调、剪枝等攻击具有强鲁棒性。

Conclusion: DCT-AW是一种适用于KAN的有效水印方法，兼具任务独立性和鲁棒性。

Abstract: With the increasing importance of protecting intellectual property in machine
learning, watermarking techniques have gained significant attention. As
advanced models are increasingly deployed in domains such as social network
analysis, the need for robust model protection becomes even more critical.
While existing watermarking methods have demonstrated effectiveness for
conventional deep neural networks, they often fail to adapt to the novel
architecture, Kolmogorov-Arnold Networks (KAN), which feature learnable
activation functions. KAN holds strong potential for modeling complex
relationships in network-structured data. However, their unique design also
introduces new challenges for watermarking. Therefore, we propose a novel
watermarking method, Discrete Cosine Transform-based Activation Watermarking
(DCT-AW), tailored for KAN. Leveraging the learnable activation functions of
KAN, our method embeds watermarks by perturbing activation outputs using
discrete cosine transform, ensuring compatibility with diverse tasks and
achieving task independence. Experimental results demonstrate that DCT-AW has a
small impact on model performance and provides superior robustness against
various watermark removal attacks, including fine-tuning, pruning, and
retraining after pruning.

</details>


### [116] [Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select](https://arxiv.org/abs/2508.06692)
*Md. Akmol Masud,Md Abrar Jahin,Mahmud Hasan*

Main category: cs.LG

TL;DR: HeteRo-Select框架通过智能选择客户端子集，解决了联邦学习中因数据异构性导致的训练不稳定问题，显著提升了准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）因客户端数据多样性常导致训练不稳定，现有方法（如Oort）在后期训练中准确性下降明显。

Method: 提出HeteRo-Select框架，通过评分系统综合考虑客户端有用性、公平性、更新速度和数据多样性，并在强正则化下提供收敛保证。

Result: 在CIFAR-10数据集上，HeteRo-Select的峰值准确率为74.75%，最终准确率为72.76%，稳定性下降仅1.99%，优于Oort。

Conclusion: HeteRo-Select为实际异构FL问题提供了可靠解决方案，理论和实验结果均支持其优越性。

Abstract: Federated Learning (FL) is a machine learning technique that often suffers
from training instability due to the diverse nature of client data. Although
utility-based client selection methods like Oort are used to converge by
prioritizing high-loss clients, they frequently experience significant drops in
accuracy during later stages of training. We propose a theoretical
HeteRo-Select framework designed to maintain high performance and ensure
long-term training stability. We provide a theoretical analysis showing that
when client data is very different (high heterogeneity), choosing a smart
subset of client participation can reduce communication more effectively
compared to full participation. Our HeteRo-Select method uses a clear,
step-by-step scoring system that considers client usefulness, fairness, update
speed, and data variety. It also shows convergence guarantees under strong
regularization. Our experimental results on the CIFAR-10 dataset under
significant label skew ($\alpha=0.1$) support the theoretical findings. The
HeteRo-Select method performs better than existing approaches in terms of peak
accuracy, final accuracy, and training stability. Specifically, HeteRo-Select
achieves a peak accuracy of $74.75\%$, a final accuracy of $72.76\%$, and a
minimal stability drop of $1.99\%$. In contrast, Oort records a lower peak
accuracy of $73.98\%$, a final accuracy of $71.25\%$, and a larger stability
drop of $2.73\%$. The theoretical foundations and empirical performance in our
study make HeteRo-Select a reliable solution for real-world heterogeneous FL
problems.

</details>


### [117] [CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations](https://arxiv.org/abs/2508.06704)
*Hager Radi Abdelwahed,Mélisande Teng,Robin Zbinden,Laura Pollock,Hugo Larochelle,Devis Tuia,David Rolnick*

Main category: cs.LG

TL;DR: CISO是一种基于深度学习的物种分布模型方法，能够结合不完整的物种观测数据和环境变量，提高预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统物种分布模型（SDMs）通常忽略物种间的生物相互作用，且现有方法对数据要求严格，难以处理稀疏和不一致的观测数据。

Method: 提出CISO方法，利用深度学习灵活处理不完整物种观测数据，并结合环境变量进行预测。

Result: 实验表明，CISO在预测性能上优于其他方法，尤其是在结合多数据集时表现更佳。

Conclusion: CISO是一种有前景的生态学工具，能够整合不完整的生物信息并识别跨类群的潜在物种相互作用。

Abstract: Species distribution models (SDMs) are widely used to predict species'
geographic distributions, serving as critical tools for ecological research and
conservation planning. Typically, SDMs relate species occurrences to
environmental variables representing abiotic factors, such as temperature,
precipitation, and soil properties. However, species distributions are also
strongly influenced by biotic interactions with other species, which are often
overlooked. While some methods partially address this limitation by
incorporating biotic interactions, they often assume symmetrical pairwise
relationships between species and require consistent co-occurrence data. In
practice, species observations are sparse, and the availability of information
about the presence or absence of other species varies significantly across
locations. To address these challenges, we propose CISO, a deep learning-based
method for species distribution modeling Conditioned on Incomplete Species
Observations. CISO enables predictions to be conditioned on a flexible number
of species observations alongside environmental variables, accommodating the
variability and incompleteness of available biotic data. We demonstrate our
approach using three datasets representing different species groups: sPlotOpen
for plants, SatBird for birds, and a new dataset, SatButterfly, for
butterflies. Our results show that including partial biotic information
improves predictive performance on spatially separate test sets. When
conditioned on a subset of species within the same dataset, CISO outperforms
alternative methods in predicting the distribution of the remaining species.
Furthermore, we show that combining observations from multiple datasets can
improve performance. CISO is a promising ecological tool, capable of
incorporating incomplete biotic information and identifying potential
interactions between species from disparate taxa.

</details>


### [118] [Analysis of Schedule-Free Nonconvex Optimization](https://arxiv.org/abs/2508.06743)
*Connor Brown*

Main category: cs.LG

TL;DR: 论文提出了一种鲁棒的Lyapunov框架，用于分析Schedule-Free（SF）方法在非凸优化中的性能，证明了其无需依赖总时间步数T的超参数设置即可实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 经典的一阶方法依赖于预先知道总时间步数T的步长调度，而SF方法通过结合Polyak-Ruppert平均和动量，实现了超参数与T无关的性能优化，但其在非凸优化中的分析仍有限。

Method: 引入Lyapunov框架，仅需L-平滑性和下界性假设，将SF分析简化为单步下降不等式，从而推导出与T无关的收敛界。

Result: 在非凸设定下，证明了多种收敛速率：常数步长+PR平均为O(1/log T)，线性增长步长为O(log T/T)，多项式平均为O(T^{-(1−α)})。数值实验验证了这些速率。

Conclusion: 研究将SF方法的无时间步数依赖保证扩展到非凸优化，并为未来最优非凸速率的研究提供了方向。

Abstract: First-order methods underpin most large-scale learning algorithms, yet their
classical convergence guarantees hinge on carefully scheduled step-sizes that
depend on the total horizon $T$, which is rarely known in advance. The
Schedule-Free (SF) method promises optimal performance with hyperparameters
that are independent of $T$ by interpolating between Polyak--Ruppert averaging
and momentum, but nonconvex analysis of SF has been limited or reliant on
strong global assumptions. We introduce a robust Lyapunov framework that, under
only $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step
descent inequality. This yields horizon-agnostic bounds in the nonconvex
setting: $O(1/\log T)$ for constant step + PR averaging, $O(\log T/T)$ for a
linearly growing step-size, and a continuum of $O(T^{-(1-\alpha)})$ rates for
polynomial averaging. We complement these proofs with Performance Estimation
Problem (PEP) experiments that numerically validate our rates and suggest that
our $O(1/\log T)$ bound on the original nonconvex SF algorithm may tighten to
$O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex
optimization and charts future directions for optimal nonconvex rates.

</details>


### [119] [Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning](https://arxiv.org/abs/2508.06765)
*Xingke Yang,Liang Li,Sicong Li,Liwei Guan,Hao Wang,Xiaoqi Qi,Jiang Liu,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: Fed MobiLLM提出了一种高效的联邦学习框架，用于在异构移动设备上微调大型语言模型，显著降低了计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习方法在移动设备上计算和内存负担过重，以及同步协议导致的延迟问题。

Method: 采用服务器辅助的联邦侧调优范式，移动设备仅执行轻量级前向传播，服务器独立训练共享侧网络，并引入自适应分层特征对齐方法。

Result: 实验显示，Fed MobiLLM显著降低了计算和通信开销（分别减少95.2%和93.2%），并加速了收敛速度（5.1倍）。

Conclusion: Fed MobiLLM是一种高效且实用的方法，适用于异构移动设备上的LLM微调。

Abstract: Collaboratively fine-tuning (FT) large language models (LLMs) over
heterogeneous mobile devices fosters immense potential applications of
personalized intelligence. However, such a vision faces critical system
challenges. Conventional federated LLM FT approaches place prohibitive
computational and memory burdens on mobile hardware, and their synchronous
model aggregation protocols stall for slower devices. In this paper, we propose
Fed MobiLLM, a novel design to facilitate efficient federated LLM FT across
mobile devices with diverse computing/communication speeds and local model
architectures. In particular, Fed MobiLLM implements a pioneering
server-assisted federated side-tuning paradigm. Briefly, mobile devices perform
lightweight forward propagation computations on local data using their frozen
pre-scaled backbone LLMs, and then upload selected intermediate activations.
The server trains a shared side-network independently, eliminating client-side
backpropagation and enabling asynchronous updates. To bridge model
heterogeneity across different devices, we introduce an adaptive layer-wise
feature alignment method, which ensures consistent representations for
collaboratively tuning a shared side network. Extensive experimental results
demonstrate that Fed MobiLLM can maintain robust fine-tuning performance while
achieving extremely low on-device memory, with at least 95.2% reduction in
computation overhead, 93.2% reduction in communication costs and 5.1x faster
convergence compared to existing methods, validating its efficacy for practical
LLM adaptation over heterogeneous mobile devices.

</details>


### [120] [PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems](https://arxiv.org/abs/2508.06767)
*Arman Dogru,R. Irem Bor-Yaliniz,Nimal Gamini Senarath*

Main category: cs.LG

TL;DR: PANAMA是一种基于多智能体强化学习（MARL）的算法，用于数字孪生（DT）生态系统中的多智能体路径规划（MAPF），通过优先不对称性和网络感知优化数据共享和决策。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生和自动化系统的普及，高效的数据共享框架和鲁棒算法变得至关重要。本文旨在探索数据处理的角色，并解决应用与网络提供商（AP/NP）之间的动态问题。

Method: 提出PANAMA算法，采用集中训练与分散执行（CTDE）框架和异步执行者-学习者架构，优化多智能体路径规划。

Result: PANAMA在准确性、速度和可扩展性上优于现有基准，并通过仿真验证了其在复杂环境中的优化数据共享策略。

Conclusion: PANAMA填补了网络感知决策与多智能体协调之间的空白，推动了数字孪生、无线网络和AI自动化之间的协同发展。

Abstract: Digital Twins (DTs) are transforming industries through advanced data
processing and analysis, positioning the world of DTs, Digital World, as a
cornerstone of nextgeneration technologies including embodied AI. As robotics
and automated systems scale, efficient data-sharing frameworks and robust
algorithms become critical. We explore the pivotal role of data handling in
next-gen networks, focusing on dynamics between application and network
providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with
Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)
based multi-agent path finding (MAPF). By adopting a Centralized Training with
Decentralized Execution (CTDE) framework and asynchronous actor-learner
architectures, PANAMA accelerates training while enabling autonomous task
execution by embodied AI. Our approach demonstrates superior pathfinding
performance in accuracy, speed, and scalability compared to existing
benchmarks. Through simulations, we highlight optimized data-sharing strategies
for scalable, automated systems, ensuring resilience in complex, real-world
environments. PANAMA bridges the gap between network-aware decision-making and
robust multi-agent coordination, advancing the synergy between DTs, wireless
networks, and AI-driven automation.

</details>


### [121] [Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift](https://arxiv.org/abs/2508.06776)
*Amit Pandey*

Main category: cs.LG

TL;DR: Zero-Direction Probing (ZDP) 是一种无需任务标签或输出评估的理论框架，通过检测变压器激活的零方向来识别模型漂移。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一种无需依赖任务标签或输出评估的方法，仅通过理论分析检测模型漂移。

Method: 基于假设A1-A6，提出了Variance-Leak定理、Fisher Null-Conservation、Rank-Leak边界和在线零空间跟踪器的对数遗憾保证。

Result: 推导了Spectral Null-Leakage (SNL) 度量，并提供了非渐近尾部边界和集中不等式，为高斯零模型下的漂移提供了先验阈值。

Conclusion: 通过监控层激活的左右零空间及其Fisher几何，可以具体且可测试地保证表示变化。

Abstract: We present Zero-Direction Probing (ZDP), a theory-only framework for
detecting model drift from null directions of transformer activations without
task labels or output evaluations. Under assumptions A1--A6, we prove: (i) the
Variance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound
for low-rank updates, and (iv) a logarithmic-regret guarantee for online
null-space trackers. We derive a Spectral Null-Leakage (SNL) metric with
non-asymptotic tail bounds and a concentration inequality, yielding a-priori
thresholds for drift under a Gaussian null model. These results show that
monitoring right/left null spaces of layer activations and their Fisher
geometry provides concrete, testable guarantees on representational change.

</details>


### [122] [PROPS: Progressively Private Self-alignment of Large Language Models](https://arxiv.org/abs/2508.06783)
*Noel Teku,Fengwei Tian,Payel Bhattacharjee,Souradip Chakraborty,Amrit Singh Bedi,Ravi Tandon*

Main category: cs.LG

TL;DR: 论文提出PROPS框架，通过多阶段隐私保护对齐方法，解决LLM对齐中人类偏好标签的隐私问题，并在保持高隐私的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 依赖人类反馈的LLM对齐可能泄露标注者的个人偏好和隐私，现有方法如DP-SGD虽提供隐私保证但可能过度保护且损害模型性能。

Method: 提出PROPS框架，通过多阶段对齐，利用前一阶段私有化对齐的模型为后续阶段提供标注数据。

Result: 在相同隐私预算下，PROPS的胜率比DP-SGD高3倍，比基于随机响应对齐方法高2.5倍。

Conclusion: PROPS在保护隐私的同时显著提升了LLM对齐的效用，优于现有方法。

Abstract: Alignment is a key step in developing Large Language Models (LLMs) using
human feedback to ensure adherence to human values and societal norms.
Dependence on human feedback raises privacy concerns about how much a labeler's
preferences may reveal about their personal values, beliefs, and personality
traits. Existing approaches, such as Differentially Private SGD (DP-SGD),
provide rigorous privacy guarantees by privatizing gradients during fine-tuning
and alignment but can provide more privacy than necessary as human preferences
are tied only to labels of (prompt, response) pairs and can degrade model
utility. This work focuses on LLM alignment with preference-level privacy,
which preserves the privacy of preference labels provided by humans. We propose
PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving
alignment framework where privately aligned models in previous stages can serve
as labelers for supplementing training data in the subsequent stages of
alignment. We present theoretical guarantees for PROPS as well as comprehensive
validation using multiple models (Pythia and GPT) and datasets (AlpacaEval,
Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over
existing methods while still providing high privacy. For the same privacy
budget, alignment via PROPS can achieve up to 3x higher win-rates compared to
DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based
alignment.

</details>


### [123] [Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning](https://arxiv.org/abs/2508.06784)
*Junjing Zheng,Chengliang Song,Weidong Jiang,Xinyu Zhang*

Main category: cs.LG

TL;DR: MA-NTAE是一种新型自监督学习模型，通过非线性Tucker分解和Pick-and-Unfold策略，高效处理高维张量数据，解决了传统方法的计算复杂性和非线性关系学习不足的问题。


<details>
  <summary>Details</summary>
Motivation: 高维张量数据在自监督学习中面临计算复杂性和非线性关系学习的挑战，传统方法如MLP自编码器因维度灾难而效率低下。

Method: 提出MA-NTAE，将经典Tucker分解推广到非线性框架，采用Pick-and-Unfold策略，通过递归展开-编码-折叠操作实现高效模式感知编码。

Result: MA-NTAE在压缩和聚类任务中优于标准自编码器和现有张量网络，尤其在高维高阶张量数据上表现更优。

Conclusion: MA-NTAE为高维张量数据提供了一种高效的非线性自监督学习解决方案，显著提升了性能和计算效率。

Abstract: High-dimensional data, particularly in the form of high-order tensors,
presents a major challenge in self-supervised learning. While MLP-based
autoencoders (AE) are commonly employed, their dependence on flattening
operations exacerbates the curse of dimensionality, leading to excessively
large model sizes, high computational overhead, and challenging optimization
for deep structural feature capture. Although existing tensor networks
alleviate computational burdens through tensor decomposition techniques, most
exhibit limited capability in learning non-linear relationships. To overcome
these limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder
(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear
framework and employs a Pick-and-Unfold strategy, facilitating flexible
per-mode encoding of high-order tensors via recursive unfold-encode-fold
operations, effectively integrating tensor structural priors. Notably, MA-NTAE
exhibits linear growth in computational complexity with tensor order and
proportional growth with mode dimensions. Extensive experiments demonstrate
MA-NTAE's performance advantages over standard AE and current tensor networks
in compression and clustering tasks, which become increasingly pronounced for
higher-order, higher-dimensional tensors.

</details>


### [124] [Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities](https://arxiv.org/abs/2508.06800)
*Rui Liu,Haolin Zuo,Zheng Lian,Hongyu Yuan,Qi Fan*

Main category: cs.LG

TL;DR: 提出了一种名为HARDY-MER的硬度感知动态课程学习框架，用于解决多模态情感识别中缺失模态的问题，通过动态调整训练课程提升模型对困难样本的处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过缺失模态重建解决多模态情感识别问题，但未考虑样本间重建难度的差异，限制了模型对困难样本的处理能力。

Method: HARDY-MER框架分为两阶段：1) 通过多视角硬度评估机制量化样本的硬度（直接硬度和间接硬度）；2) 采用基于检索的动态课程学习策略，动态调整训练课程。

Result: 在基准数据集上的实验表明，HARDY-MER在缺失模态场景下优于现有方法。

Conclusion: HARDY-MER通过动态课程学习有效提升了模型对困难样本的处理能力，为多模态情感识别中的缺失模态问题提供了新的解决方案。

Abstract: Missing modalities have recently emerged as a critical research direction in
multimodal emotion recognition (MER). Conventional approaches typically address
this issue through missing modality reconstruction. However, these methods fail
to account for variations in reconstruction difficulty across different
samples, consequently limiting the model's ability to handle hard samples
effectively. To overcome this limitation, we propose a novel Hardness-Aware
Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates
in two key stages: first, it estimates the hardness level of each sample, and
second, it strategically emphasizes hard samples during training to enhance
model performance on these challenging instances. Specifically, we first
introduce a Multi-view Hardness Evaluation mechanism that quantifies
reconstruction difficulty by considering both Direct Hardness (modality
reconstruction errors) and Indirect Hardness (cross-modal mutual information).
Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy
that dynamically adjusts the training curriculum by retrieving samples with
similar semantic information and balancing the learning focus between easy and
hard instances. Extensive experiments on benchmark datasets demonstrate that
HARDY-MER consistently outperforms existing methods in missing-modality
scenarios. Our code will be made publicly available at
https://github.com/HARDY-MER/HARDY-MER.

</details>


### [125] [Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation](https://arxiv.org/abs/2508.06806)
*Xiao Huang,Xu Liu,Enze Zhang,Tong Yu,Shuai Li*

Main category: cs.LG

TL;DR: 提出了一种新的数据增强方法CFDG，通过无分类器引导扩散生成技术提升离线与在线数据生成质量，显著提升离线到在线强化学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的离线数据与在线数据存在分布差异，限制了性能提升。

Method: 采用无分类器引导扩散生成（CFDG）技术，结合重加权方法，使生成数据更贴近在线数据分布。

Result: 在D4RL基准测试中，CFDG比现有方法平均提升15%的性能。

Conclusion: CFDG是一种通用方法，可集成到现有离线到在线强化学习算法中，显著提升性能。

Abstract: Offline-to-online Reinforcement Learning (O2O RL) aims to perform online
fine-tuning on an offline pre-trained policy to minimize costly online
interactions. Existing work used offline datasets to generate data that conform
to the online data distribution for data augmentation. However, generated data
still exhibits a gap with the online data, limiting overall performance. To
address this, we propose a new data augmentation approach, Classifier-Free
Diffusion Generation (CFDG). Without introducing additional classifier training
overhead, CFDG leverages classifier-free guidance diffusion to significantly
enhance the generation quality of offline and online data with different
distributions. Additionally, it employs a reweighting method to enable more
generated data to align with the online data, enhancing performance while
maintaining the agent's stability. Experimental results show that CFDG
outperforms replaying the two data types or using a standard diffusion model to
generate new data. Our method is versatile and can be integrated with existing
offline-to-online RL algorithms. By implementing CFDG to popular methods IQL,
PEX and APL, we achieve a notable 15% average improvement in empirical
performance on the D4RL benchmark such as MuJoCo and AntMaze.

</details>


### [126] [Technical Report: Full-Stack Fine-Tuning for the Q Programming Language](https://arxiv.org/abs/2508.06813)
*Brendan R. Hogan,Will Brown,Adel Boyarsky,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 该论文提出了一种开源方法，将大语言模型（LLMs）适配到Q编程语言（一种在量化金融中流行但互联网上较少见的语言），并通过预训练、监督微调和强化学习训练了一系列模型，其最佳模型在Q基准测试中表现优于主流前沿模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在Q编程语言等小众或私有领域任务中的表现不足问题。

Method: 构建Q语言的Leetcode风格评估数据集，对主流前沿模型进行基准测试，并通过预训练、监督微调和强化学习训练不同规模的Qwen-2.5系列模型。

Result: 最佳模型在Q基准测试中的pass@1准确率达到59%，优于Claude Opus-4（29.5%提升），所有模型均超过GPT-4.1。

Conclusion: 该方法不仅适用于Q语言，还可推广到其他依赖软性或主观信号评估的任务。

Abstract: Even though large language models are becoming increasingly capable, it is
still unreasonable to expect them to excel at tasks that are under-represented
on the Internet. Leveraging LLMs for specialized applications, particularly in
niche programming languages and private domains, remains challenging and
largely unsolved. In this work, we address this gap by presenting a
comprehensive, open-source approach for adapting LLMs to the Q programming
language, a popular tool in quantitative finance that is much less present on
the Internet compared to Python, C, Java, and other ``mainstream" languages and
is therefore not a strong suit of general-purpose AI models. We introduce a new
Leetcode style evaluation dataset for Q, benchmark major frontier models on the
dataset, then do pretraining, supervised fine tuning, and reinforcement
learning to train a suite of reasoning and non-reasoning models based on the
Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our
best model achieves a pass@1 accuracy of 59 percent on our Q benchmark,
surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.
Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.
In addition to releasing models, code, and data, we provide a detailed
blueprint for dataset construction, model pretraining, supervised fine-tuning,
and reinforcement learning. Our methodology is broadly applicable, and we
discuss how these techniques can be extended to other tasks, including those
where evaluation may rely on soft or subjective signals.

</details>


### [127] [Who's the Evil Twin? Differential Auditing for Undesired Behavior](https://arxiv.org/abs/2508.06827)
*Ishwar Balappanawar,Venkata Hasith Vattikuti,Greta Kintzley,Ronan Azimi-Mancel,Satvik Golechha*

Main category: cs.LG

TL;DR: 论文探讨了通过对抗性游戏检测神经网络中的隐藏行为，红队训练两个模型（一个良性，一个含隐藏有害行为），蓝队尝试识别被篡改的模型。实验显示基于对抗攻击的方法效果最佳，LLM审计则需额外提示。


<details>
  <summary>Details</summary>
Motivation: 检测神经网络中的隐藏行为因缺乏先验知识和对抗性混淆而具有挑战性，研究旨在探索有效的检测方法。

Method: 采用对抗性游戏框架，红队训练两个模型，蓝队使用多种策略（如高斯噪声分析、模型差异、对抗攻击等）进行识别。

Result: 基于对抗攻击的方法准确率最高（100%），其他方法表现不一；LLM审计需额外提示才能有效。

Conclusion: 研究为设计更好的审计方法提供了参考，并开源了相关模型和数据。

Abstract: Detecting hidden behaviors in neural networks poses a significant challenge
due to minimal prior knowledge and potential adversarial obfuscation. We
explore this problem by framing detection as an adversarial game between two
teams: the red team trains two similar models, one trained solely on benign
data and the other trained on data containing hidden harmful behavior, with the
performance of both being nearly indistinguishable on the benign dataset. The
blue team, with limited to no information about the harmful behaviour, tries to
identify the compromised model. We experiment using CNNs and try various blue
team strategies, including Gaussian noise analysis, model diffing, integrated
gradients, and adversarial attacks under different levels of hints provided by
the red team. Results show high accuracy for adversarial-attack-based methods
(100\% correct prediction, using hints), which is very promising, whilst the
other techniques yield more varied performance. During our LLM-focused rounds,
we find that there are not many parallel methods that we could apply from our
study with CNNs. Instead, we find that effective LLM auditing methods require
some hints about the undesired distribution, which can then used in standard
black-box and open-weight methods to probe the models further and reveal their
misalignment. We open-source our auditing games (with the model and data) and
hope that our findings contribute to designing better audits.

</details>


### [128] [Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning](https://arxiv.org/abs/2508.06871)
*Aleksandar Todorov,Juan Cardenas-Cartagena,Rafael F. Cunha,Marco Zullich,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 论文研究了深度强化学习中塑性损失问题，探索了稀疏化方法（GMP和SET）如何提升多任务强化学习（MTRL）的塑性，并验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 塑性损失是深度强化学习中的关键挑战，尤其在多任务学习中，更高的表示灵活性对处理多样且可能冲突的任务需求至关重要。

Method: 通过系统评估稀疏化方法（GMP和SET）在不同MTRL架构（共享主干、专家混合、正交专家混合）上的表现，并与密集基线及其他塑性增强方法对比。

Result: GMP和SET有效缓解了塑性退化指标（如神经元休眠和表示崩溃），稀疏代理在多任务性能上常优于密集代理，并与显式塑性干预方法竞争。

Conclusion: 动态稀疏化是开发更具适应性MTRL系统的有效工具，但需结合具体情境。

Abstract: Plasticity loss, a diminishing capacity to adapt as training progresses, is a
critical challenge in deep reinforcement learning. We examine this issue in
multi-task reinforcement learning (MTRL), where higher representational
flexibility is crucial for managing diverse and potentially conflicting task
demands. We systematically explore how sparsification methods, particularly
Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance
plasticity and consequently improve performance in MTRL agents. We evaluate
these approaches across distinct MTRL architectures (shared backbone, Mixture
of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks,
comparing against dense baselines, and a comprehensive range of alternative
plasticity-inducing or regularization methods. Our results demonstrate that
both GMP and SET effectively mitigate key indicators of plasticity degradation,
such as neuron dormancy and representational collapse. These plasticity
improvements often correlate with enhanced multi-task performance, with sparse
agents frequently outperforming dense counterparts and achieving competitive
results against explicit plasticity interventions. Our findings offer insights
into the interplay between plasticity, network sparsity, and MTRL designs,
highlighting dynamic sparsification as a robust but context-sensitive tool for
developing more adaptable MTRL systems.

</details>


### [129] [Conformal Prediction and Trustworthy AI](https://arxiv.org/abs/2508.06885)
*Anthony Bellotti,Xindi Zhao*

Main category: cs.LG

TL;DR: 本文回顾了保形预测在可信AI中的潜力，探讨了其超越边际有效性的应用，如泛化风险和AI治理，并通过实验展示了其作为校准预测器和偏识别与缓解工具的作用。


<details>
  <summary>Details</summary>
Motivation: 保形预测作为一种提供置信度保证的机器学习方法，近年来在不确定性量化领域受到广泛关注。其可靠的不确定性量化能力使其成为构建可信AI的重要工具。

Method: 文章通过实验和案例研究，展示了保形预测在可信AI中的应用，包括校准预测、偏识别与缓解等方面。

Result: 保形预测不仅能够提供校准的预测结果，还能有效识别和缓解模型中的偏问题，进一步支持可信AI的发展。

Conclusion: 保形预测在可信AI中具有广泛的应用潜力，尤其是在泛化风险和AI治理等领域，为构建更可靠的AI系统提供了重要支持。

Abstract: Conformal predictors are machine learning algorithms developed in the 1990's
by Gammerman, Vovk, and their research team, to provide set predictions with
guaranteed confidence level. Over recent years, they have grown in popularity
and have become a mainstream methodology for uncertainty quantification in the
machine learning community. From its beginning, there was an understanding that
they enable reliable machine learning with well-calibrated uncertainty
quantification. This makes them extremely beneficial for developing trustworthy
AI, a topic that has also risen in interest over the past few years, in both
the AI community and society more widely. In this article, we review the
potential for conformal prediction to contribute to trustworthy AI beyond its
marginal validity property, addressing problems such as generalization risk and
AI governance. Experiments and examples are also provided to demonstrate its
use as a well-calibrated predictor and for bias identification and mitigation.

</details>


### [130] [QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting](https://arxiv.org/abs/2508.06915)
*Shichao Ma,Zhengyang Zhou,Qihe Huang,Binwu Wang,Kuo Yang,Huan Li,Yang Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为QuiZSF的轻量级模块化框架，通过结合检索增强生成（RAG）和时间序列预训练模型（TSPMs）来提升零样本时间序列预测（ZSF）的性能。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的场景（如领域迁移或极端条件下的预测）中，传统模型难以处理零样本时间序列预测。虽然时间序列预训练模型表现优异，但缺乏动态整合外部知识的机制。

Method: 提出了QuiZSF框架，包括层次化树结构的ChronoRAG Base（CRB）用于存储和检索时间序列数据，多粒度序列交互学习器（MSIL）提取特征，以及双分支模型协作器（MCC）对齐检索知识与TSPMs。

Result: QuiZSF在75%（非LLM基础）和87.5%（LLM基础）的预测设置中排名第一，同时保持高效的内存和推理时间。

Conclusion: QuiZSF通过结合RAG和TSPMs，显著提升了零样本时间序列预测的性能和效率。

Abstract: Time series forecasting has become increasingly important to empower diverse
applications with streaming data. Zero-shot time-series forecasting (ZSF),
particularly valuable in data-scarce scenarios, such as domain transfer or
forecasting under extreme conditions, is difficult for traditional models to
deal with. While time series pre-trained models (TSPMs) have demonstrated
strong performance in ZSF, they often lack mechanisms to dynamically
incorporate external knowledge. Fortunately, emerging retrieval-augmented
generation (RAG) offers a promising path for injecting such knowledge on
demand, yet they are rarely integrated with TSPMs. To leverage the strengths of
both worlds, we introduce RAG into TSPMs to enhance zero-shot time series
forecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series
Forecaster), a lightweight and modular framework that couples efficient
retrieval with representation learning and model adaptation for ZSF.
Specifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)
for scalable time-series storage and domain-aware retrieval, introduce a
Multi-grained Series Interaction Learner (MSIL) to extract fine- and
coarse-grained relational features, and develop a dual-branch Model Cooperation
Coherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM
based and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM
based and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and
87.5% of prediction settings, while maintaining high efficiency in memory and
inference time.

</details>


### [131] [Class Unbiasing for Generalization in Medical Diagnosis](https://arxiv.org/abs/2508.06943)
*Lishi Zuo,Man-Wai Mak,Lu Yi,Youzhi Tu*

Main category: cs.LG

TL;DR: 论文提出了一种解决医学诊断中类别特征偏差和类别不平衡的方法，通过类别不平等损失和加权优化目标，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学诊断中模型可能因依赖与部分类别强相关的特征而产生偏差，导致性能偏颇和泛化能力差。

Method: 提出类别不平等损失和类别加权分布鲁棒优化目标，平衡正负类别样本的贡献，并提升低性能类别的权重。

Result: 实验证明类别特征偏差会损害模型性能，所提方法有效缓解了偏差和不平衡问题。

Conclusion: 该方法显著改善了模型的泛化能力，适用于医学诊断等实际场景。

Abstract: Medical diagnosis might fail due to bias. In this work, we identified
class-feature bias, which refers to models' potential reliance on features that
are strongly correlated with only a subset of classes, leading to biased
performance and poor generalization on other classes. We aim to train a
class-unbiased model (Cls-unbias) that mitigates both class imbalance and
class-feature bias simultaneously. Specifically, we propose a class-wise
inequality loss which promotes equal contributions of classification loss from
positive-class and negative-class samples. We propose to optimize a class-wise
group distributionally robust optimization objective-a class-weighted training
objective that upweights underperforming classes-to enhance the effectiveness
of the inequality loss under class imbalance. Through synthetic and real-world
datasets, we empirically demonstrate that class-feature bias can negatively
impact model performance. Our proposed method effectively mitigates both
class-feature bias and class imbalance, thereby improving the model's
generalization ability.

</details>


### [132] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为AMFT的单阶段算法，通过隐式奖励理论动态平衡监督微调（SFT）和强化学习（RL），解决了传统两阶段方法的灾难性遗忘和探索-模仿权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法（SFT+RL）存在灾难性遗忘和探索-模仿权衡问题，而现有单阶段方法缺乏动态平衡机制。

Method: 提出AMFT算法，利用元梯度自适应权重控制器动态优化SFT和RL的平衡，并通过策略熵正则化确保稳定性。

Result: 在数学推理、抽象视觉推理和视觉语言导航等任务上，AMFT表现优异，并在OOD任务中展现出更强的泛化能力。

Conclusion: AMFT通过动态平衡SFT和RL，提供了一种更稳定、高效和性能优越的LLM对齐范式。

Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

</details>


### [133] [BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity](https://arxiv.org/abs/2508.06953)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: BoRA是一种改进的低秩适应方法，通过块矩阵乘法和块间对角矩阵提升LoRA权重秩，仅需少量额外参数即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在大型语言模型中的参数高效微调方法中表现优异，但增加秩会显著增加可训练参数数量。BoRA旨在以少量额外参数提升秩，从而优化性能。

Method: BoRA将LoRA的权重矩阵分解为块矩阵乘法，并引入块间对角矩阵以增强多样性，从而提升秩。仅需b²r额外参数即可实现秩的b倍提升。

Result: 实验表明，BoRA在多个数据集和模型上表现优于传统LoRA，且消融研究验证了其可扩展性。

Conclusion: BoRA通过创新的块矩阵设计，以更少的参数实现了更高的秩和性能，为参数高效微调提供了新思路。

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). It approximates the update of a
pretrained weight matrix $W\in\mathbb{R}^{m\times n}$ by the product of two
low-rank matrices, $BA$, where $A \in\mathbb{R}^{r\times n}$ and
$B\in\mathbb{R}^{m\times r} (r\ll\min\{m,n\})$. Increasing the dimension $r$
can raise the rank of LoRA weights (i.e., $BA$), which typically improves
fine-tuning performance but also significantly increases the number of
trainable parameters. In this paper, we propose Block Diversified Low-Rank
Adaptation (BoRA), which improves the rank of LoRA weights with a small number
of additional parameters. Specifically, BoRA treats the product $BA$ as a block
matrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along
the columns and rows, respectively (i.e., $A=[A_1,\dots,A_b]$ and
$B=[B_1,\dots,B_b]^\top$). Consequently, the product $BA$ becomes the
concatenation of the block products $B_iA_j$ for $i,j\in[b]$. To enhance the
diversity of different block products, BoRA introduces a unique diagonal matrix
$\Sigma_{i,j} \in \mathbb{R}^{r\times r}$ for each block multiplication,
resulting in $B_i \Sigma_{i,j} A_j$. By leveraging these block-wise diagonal
matrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only
requiring $b^2r$ additional parameters. Extensive experiments across multiple
datasets and models demonstrate the superiority of BoRA, and ablation studies
further validate its scalability.

</details>


### [134] [Can Multitask Learning Enhance Model Explainability?](https://arxiv.org/abs/2508.06966)
*Hiba Najjar,Bushra Alshbib,Andreas Dengel*

Main category: cs.LG

TL;DR: 该研究提出了一种利用多任务学习提升遥感数据模型解释性的方法，通过将某些模态作为辅助任务预测，而非额外输入，从而在不牺牲性能的情况下增强模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 遥感数据多样但复杂，多模态学习网络性能虽优但解释性差。本研究旨在通过多任务学习利用模态信息，提升模型的内在解释性。

Method: 将某些模态作为辅助任务预测，而非额外输入，利用卫星数据的丰富信息。

Result: 方法在数据稀缺时无需额外模态，性能与多模态基线相当或更优，且能通过辅助任务解释主任务预测误差。

Conclusion: 该方法在三个数据集上验证了其高效性，为遥感数据模型提供了一种解释性强且性能优越的解决方案。

Abstract: Remote sensing provides satellite data in diverse types and formats. The
usage of multimodal learning networks exploits this diversity to improve model
performance, except that the complexity of such networks comes at the expense
of their interpretability. In this study, we explore how modalities can be
leveraged through multitask learning to intrinsically explain model behavior.
In particular, instead of additional inputs, we use certain modalities as
additional targets to be predicted along with the main task. The success of
this approach relies on the rich information content of satellite data, which
remains as input modalities. We show how this modeling context provides
numerous benefits: (1) in case of data scarcity, the additional modalities do
not need to be collected for model inference at deployment, (2) the model
performance remains comparable to the multimodal baseline performance, and in
some cases achieves better scores, (3) prediction errors in the main task can
be explained via the model behavior in the auxiliary task(s). We demonstrate
the efficiency of our approach on three datasets, including segmentation,
classification, and regression tasks. Code available at
git.opendfki.de/hiba.najjar/mtl_explainability/.

</details>


### [135] [Structure-Preserving Digital Twins via Conditional Neural Whitney Forms](https://arxiv.org/abs/2508.06981)
*Brooks Kinch,Benjamin Shaffer,Elizabeth Armstrong,Michael Meehan,John Hewson,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出了一种基于结构保持的降阶有限元模型构建实时数字孪生的框架，利用条件注意力机制学习降阶基和非线性守恒律，支持实时校准和闭环推理。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀疏或优化误差下的数值适定性和守恒量精确保持问题，支持复杂几何和传感器数据的实时校准。

Method: 采用条件注意力机制和有限元外微积分（FEEC）框架，学习降阶基和非线性守恒律，非侵入式集成传统有限元方法。

Result: 在复杂几何和稀疏数据（25次LES模拟）下实现准确预测，实时推理速度提升3.1x10^8倍。

Conclusion: 该框架为复杂系统的实时数字孪生提供了高效、准确的解决方案，并支持开源实现。

Abstract: We present a framework for constructing real-time digital twins based on
structure-preserving reduced finite element models conditioned on a latent
variable Z. The approach uses conditional attention mechanisms to learn both a
reduced finite element basis and a nonlinear conservation law within the
framework of finite element exterior calculus (FEEC). This guarantees numerical
well-posedness and exact preservation of conserved quantities, regardless of
data sparsity or optimization error. The conditioning mechanism supports
real-time calibration to parametric variables, allowing the construction of
digital twins which support closed loop inference and calibration to sensor
data. The framework interfaces with conventional finite element machinery in a
non-invasive manner, allowing treatment of complex geometries and integration
of learned models with conventional finite element techniques.
  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics,
and a complex battery thermal runaway problem. The method achieves accurate
predictions on complex geometries with sparse data (25 LES simulations),
including capturing the transition to turbulence and achieving real-time
inference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source
implementation is available on GitHub.

</details>


### [136] [Discovery Learning accelerates battery design evaluation](https://arxiv.org/abs/2508.06985)
*Jiawei Zhang,Yifei Zhang,Baozhao Yi,Yao Ren,Qi Jiao,Hanyu Bai,Weiran Jiang,Ziyou Song*

Main category: cs.LG

TL;DR: 论文提出了一种名为“发现学习”（DL）的科学机器学习范式，通过结合主动学习、物理引导学习和零样本学习，显著减少了电池设计验证的时间和能源成本。


<details>
  <summary>Details</summary>
Motivation: 电池研发因原型设计和寿命测试的高成本而受限，现有数据驱动方法需目标设计的标记数据且效率不足。

Method: DL整合了主动学习、物理引导学习和零样本学习，利用历史设计数据减少原型需求，实现快速寿命评估。

Result: 在123个工业级锂离子电池上测试，DL仅用公开数据集训练，预测寿命误差为7.2%，节省98%时间和95%能源。

Conclusion: DL展示了从历史设计中提取洞察以加速下一代电池技术开发的潜力，是数据驱动建模的重要进展。

Abstract: Fast and reliable validation of novel designs in complex physical systems
such as batteries is critical to accelerating technological innovation.
However, battery research and development remain bottlenecked by the
prohibitively high time and energy costs required to evaluate numerous new
design candidates, particularly in battery prototyping and life testing.
Despite recent progress in data-driven battery lifetime prediction, existing
methods require labeled data of target designs to improve accuracy and cannot
make reliable predictions until after prototyping, thus falling far short of
the efficiency needed to enable rapid feedback for battery design. Here, we
introduce Discovery Learning (DL), a scientific machine-learning paradigm that
integrates active learning, physics-guided learning, and zero-shot learning
into a human-like reasoning loop, drawing inspiration from learning theories in
educational psychology. DL can learn from historical battery designs and
actively reduce the need for prototyping, thus enabling rapid lifetime
evaluation for unobserved material-design combinations without requiring
additional data labeling. To test DL, we present 123 industrial-grade
large-format lithium-ion pouch cells, spanning eight material-design
combinations and diverse cycling protocols. Trained solely on public datasets
of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting
the average cycle life under unknown device variability. This results in
savings of 98% in time and 95% in energy compared to industrial practices. This
work highlights the potential of uncovering insights from historical designs to
inform and accelerate the development of next-generation battery technologies.
DL represents a key advance toward efficient data-driven modeling and helps
realize the promise of machine learning for accelerating scientific discovery
and engineering innovation.

</details>


### [137] [UniMove: A Unified Model for Multi-city Human Mobility Prediction](https://arxiv.org/abs/2508.06986)
*Chonghua Han,Yuan Yuan,Yukun Liu,Jingtao Ding,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: UniMove是一个统一的多城市人类移动预测模型，通过通用空间表示和轨迹-位置双塔架构，解决了城市间异质性问题，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 人类移动预测对城市规划和服务优化至关重要，但现有方法因城市异质性需为每个城市单独训练模型，效率低下。

Method: 提出轨迹-位置双塔架构和MoE Transformer块，实现通用空间编码和自适应移动模式建模。

Result: 在多城市数据集上，UniMove通过联合训练和相互数据增强，预测准确性提升超过10.2%。

Conclusion: UniMove为实现人类移动预测的基础模型迈出了重要一步，并开源了实现。

Abstract: Human mobility prediction is vital for urban planning, transportation
optimization, and personalized services. However, the inherent randomness,
non-uniform time intervals, and complex patterns of human mobility, compounded
by the heterogeneity introduced by varying city structures, infrastructure, and
population densities, present significant challenges in modeling. Existing
solutions often require training separate models for each city due to distinct
spatial representations and geographic coverage. In this paper, we propose
UniMove, a unified model for multi-city human mobility prediction, addressing
two challenges: (1) constructing universal spatial representations for
effective token sharing across cities, and (2) modeling heterogeneous mobility
patterns from varying city characteristics. We propose a trajectory-location
dual-tower architecture, with a location tower for universal spatial encoding
and a trajectory tower for sequential mobility modeling. We also design MoE
Transformer blocks to adaptively select experts to handle diverse movement
patterns. Extensive experiments across multiple datasets from diverse cities
demonstrate that UniMove truly embodies the essence of a unified model. By
enabling joint training on multi-city data with mutual data enhancement, it
significantly improves mobility prediction accuracy by over 10.2\%. UniMove
represents a key advancement toward realizing a true foundational model with a
unified architecture for human mobility. We release the implementation at
https://github.com/tsinghua-fib-lab/UniMove/.

</details>


### [138] [A Comparative Study of Feature Selection in Tsetlin Machines](https://arxiv.org/abs/2508.06991)
*Vojtech Halenka,Ole-Christoffer Granmo,Lei Jiao,Per-Arne Andersen*

Main category: cs.LG

TL;DR: 本文探讨了特征选择（FS）在Tsetlin机器（TM）中的应用，评估了多种FS方法，包括传统过滤和嵌入方法，以及后解释方法（如SHAP和LIME），并提出了一种基于TM条款权重和Tsetlin自动机状态的新型嵌入评分器。实验结果表明，TM内部评分器不仅性能优异，还能利用条款的可解释性揭示特征交互模式。


<details>
  <summary>Details</summary>
Motivation: TM虽然提供了基于条款的可解释学习，但缺乏估计特征重要性的工具。本文旨在填补这一空白，为TM开发专用的特征选择方法。

Method: 本文评估了多种FS技术，包括传统方法、后解释方法（如SHAP和LIME），以及基于TM条款权重和TA状态的新型嵌入评分器。实验在12个数据集上进行，使用ROAR和ROAD等评估协议。

Result: TM内部评分器性能优异，能利用条款可解释性揭示特征交互模式。简单的TM专用评分器在计算成本更低的情况下实现了相似的精度保持。

Conclusion: 本研究为TM中的FS建立了首个全面基线，并为开发专用的TM可解释性技术铺平了道路。

Abstract: Feature Selection (FS) is crucial for improving model interpretability,
reducing complexity, and sometimes for enhancing accuracy. The recently
introduced Tsetlin machine (TM) offers interpretable clause-based learning, but
lacks established tools for estimating feature importance. In this paper, we
adapt and evaluate a range of FS techniques for TMs, including classical filter
and embedded methods as well as post-hoc explanation methods originally
developed for neural networks (e.g., SHAP and LIME) and a novel family of
embedded scorers derived from TM clause weights and Tsetlin automaton (TA)
states. We benchmark all methods across 12 datasets, using evaluation
protocols, like Remove and Retrain (ROAR) strategy and Remove and Debias
(ROAD), to assess causal impact. Our results show that TM-internal scorers not
only perform competitively but also exploit the interpretability of clauses to
reveal interacting feature patterns. Simpler TM-specific scorers achieve
similar accuracy retention at a fraction of the computational cost. This study
establishes the first comprehensive baseline for FS in TM and paves the way for
developing specialized TM-specific interpretability techniques.

</details>


### [139] [Conformal Set-based Human-AI Complementarity with Multiple Experts](https://arxiv.org/abs/2508.06997)
*Helbert Paat,Guohao Shen*

Main category: cs.LG

TL;DR: 论文提出了一种基于共形预测集的贪心算法，用于从多个专家中选择实例相关的子集，以提高分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单一专家场景，而本研究探讨了在多个专家中选择实例相关子集的条件和方法。

Method: 提出了一种贪心算法，利用共形预测集选择专家预测子集，用于分类任务。

Result: 在CIFAR-10H和ImageNet-16H数据集上的模拟研究表明，该算法能接近最优地选择子集，提升分类性能。

Conclusion: 多专家场景下，实例相关的专家子集选择能显著提升分类性能，贪心算法是一种有效方法。

Abstract: Decision support systems are designed to assist human experts in
classification tasks by providing conformal prediction sets derived from a
pre-trained model. This human-AI collaboration has demonstrated enhanced
classification performance compared to using either the model or the expert
independently. In this study, we focus on the selection of instance-specific
experts from a pool of multiple human experts, contrasting it with existing
research that typically focuses on single-expert scenarios. We characterize the
conditions under which multiple experts can benefit from the conformal sets.
With the insight that only certain experts may be relevant for each instance,
we explore the problem of subset selection and introduce a greedy algorithm
that utilizes conformal sets to identify the subset of expert predictions that
will be used in classifying an instance. This approach is shown to yield better
performance compared to naive methods for human subset selection. Based on real
expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation
study indicates that our proposed greedy algorithm achieves near-optimal
subsets, resulting in improved classification performance among multiple
experts.

</details>


### [140] [TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations](https://arxiv.org/abs/2508.07016)
*Jianfei Wu,Wenmian Yang,Bingning Liu,Weijia Jia*

Main category: cs.LG

TL;DR: 提出了一种基于时滞交叉相关性的序列预测框架（TLCCSP），通过SSDTW算法和对比学习编码器提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型常忽略时滞交叉相关性，而这对捕捉复杂时间关系至关重要。

Method: TLCCSP框架结合SSDTW算法捕捉时滞相关性，并使用对比学习编码器高效近似SSDTW距离。

Result: 在天气、金融和房地产数据集上，TLCCSP显著降低MSE，对比学习编码器还大幅减少计算时间。

Conclusion: TLCCSP框架在提升预测精度的同时保证了实时性和可扩展性。

Abstract: Time series forecasting is critical across various domains, such as weather,
finance and real estate forecasting, as accurate forecasts support informed
decision-making and risk mitigation. While recent deep learning models have
improved predictive capabilities, they often overlook time-lagged
cross-correlations between related sequences, which are crucial for capturing
complex temporal relationships. To address this, we propose the Time-Lagged
Cross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances
forecasting accuracy by effectively integrating time-lagged cross-correlated
sequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)
algorithm to capture lagged correlations and a contrastive learning-based
encoder to efficiently approximate SSDTW distances.
  Experimental results on weather, finance and real estate time series datasets
demonstrate the effectiveness of our framework. On the weather dataset, SSDTW
reduces mean squared error (MSE) by 16.01% compared with single-sequence
methods, while the contrastive learning encoder (CLE) further decreases MSE by
17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE
reduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by
21.29% and 8.62%, respectively. Additionally, the contrastive learning approach
decreases SSDTW computational time by approximately 99%, ensuring scalability
and real-time applicability across multiple time series forecasting tasks.

</details>


### [141] [From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving](https://arxiv.org/abs/2508.07029)
*Antonio Guillen-Perez*

Main category: cs.LG

TL;DR: 论文提出了一种通过离线强化学习（CQL）从静态专家数据中学习鲁棒驾驶策略的方法，显著优于行为克隆（BC）基线。


<details>
  <summary>Details</summary>
Motivation: 解决行为克隆在闭环执行中的脆弱性和累积误差问题，提升自动驾驶策略的鲁棒性。

Method: 开发了基于Transformer的BC基线模型，并应用CQL算法结合精心设计的奖励函数。

Result: 在Waymo数据集上，CQL策略的成功率提高了3.2倍，碰撞率降低了7.4倍。

Conclusion: 离线强化学习是学习鲁棒、长期驾驶策略的关键方法。

Abstract: Learning robust driving policies from large-scale, real-world datasets is a
central challenge in autonomous driving, as online data collection is often
unsafe and impractical. While Behavioral Cloning (BC) offers a straightforward
approach to imitation learning, policies trained with BC are notoriously
brittle and suffer from compounding errors in closed-loop execution. This work
presents a comprehensive pipeline and a comparative study to address this
limitation. We first develop a series of increasingly sophisticated BC
baselines, culminating in a Transformer-based model that operates on a
structured, entity-centric state representation. While this model achieves low
imitation loss, we show that it still fails in long-horizon simulations. We
then demonstrate that by applying a state-of-the-art Offline Reinforcement
Learning algorithm, Conservative Q-Learning (CQL), to the same data and
architecture, we can learn a significantly more robust policy. Using a
carefully engineered reward function, the CQL agent learns a conservative value
function that enables it to recover from minor errors and avoid
out-of-distribution states. In a large-scale evaluation on 1,000 unseen
scenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a
3.2x higher success rate and a 7.4x lower collision rate than the strongest BC
baseline, proving that an offline RL approach is critical for learning robust,
long-horizon driving policies from static expert data.

</details>


### [142] [A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling](https://arxiv.org/abs/2508.07032)
*Tiantian He,Keyue Jiang,An Zhao,Anna Schroder,Elinor Thompson,Sonja Soskic,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.LG

TL;DR: 论文提出了一种阶段感知的Mixture of Experts（MoE）框架，结合非均匀图神经扩散模型（IGND）和局部神经反应模块，动态建模神经退行性疾病的进展机制。


<details>
  <summary>Details</summary>
Motivation: 神经退行性疾病的进展通常被视为时空扩散过程，但由于纵向数据稀缺和病理机制复杂，传统模型难以准确建模。

Method: 提出IGND-MoE模型，通过时间依赖的专家权重建模不同阶段的病理机制，并利用双优化方法估计个体观测的时间位置。

Result: 模型揭示了早期阶段图相关过程更显著，后期其他物理过程占主导，与文献一致。

Conclusion: IGND-MoE为理解疾病进展提供了动态且灵活的方法，具有临床意义。

Abstract: The long-term progression of neurodegenerative diseases is commonly
conceptualized as a spatiotemporal diffusion process that consists of a graph
diffusion process across the structural brain connectome and a localized
reaction process within brain regions. However, modeling this progression
remains challenging due to 1) the scarcity of longitudinal data obtained
through irregular and infrequent subject visits and 2) the complex interplay of
pathological mechanisms across brain regions and disease stages, where
traditional models assume fixed mechanisms throughout disease progression. To
address these limitations, we propose a novel stage-aware Mixture of Experts
(MoE) framework that explicitly models how different contributing mechanisms
dominate at different disease stages through time-dependent expert
weighting.Data-wise, we utilize an iterative dual optimization method to
properly estimate the temporal position of individual observations,
constructing a co hort-level progression trajectory from irregular snapshots.
Model-wise, we enhance the spatial component with an inhomogeneous graph neural
diffusion model (IGND) that allows diffusivity to vary based on node states and
time, providing more flexible representations of brain networks. We also
introduce a localized neural reaction module to capture complex dynamics beyond
standard processes.The resulting IGND-MoE model dynamically integrates these
components across temporal states, offering a principled way to understand how
stage-specific pathological mechanisms contribute to progression. The
stage-wise weights yield novel clinical insights that align with literature,
suggesting that graph-related processes are more influential at early stages,
while other unknown physical processes become dominant later on.

</details>


### [143] [Differentiable Adaptive Kalman Filtering via Optimal Transport](https://arxiv.org/abs/2508.07037)
*Yangguang He,Wenhao Li,Minzhe Li,Juan Zhang,Xiangfeng Wang,Bo Jin*

Main category: cs.LG

TL;DR: OTAKNet是一种在线学习方法，用于解决学习型自适应卡尔曼滤波中的噪声统计漂移问题，通过最优传输实现无需标签或重新训练的在线适应。


<details>
  <summary>Details</summary>
Motivation: 现实环境中，如风力变化或电磁干扰等因素会导致噪声统计漂移，从而降低学习型方法的性能。

Method: OTAKNet通过一步预测测量似然性连接状态估计与漂移，并利用最优传输的几何感知成本和稳定梯度实现在线适应。

Result: 在合成和真实NCLT数据集上，OTAKNet表现优于传统模型自适应卡尔曼滤波和离线学习型滤波，尤其在训练数据有限时。

Conclusion: OTAKNet为解决噪声统计漂移问题提供了一种有效的在线解决方案，无需地面真值标签或重新训练。

Abstract: Learning-based filtering has demonstrated strong performance in non-linear
dynamical systems, particularly when the statistics of noise are unknown.
However, in real-world deployments, environmental factors, such as changing
wind conditions or electromagnetic interference, can induce unobserved
noise-statistics drift, leading to substantial degradation of learning-based
methods. To address this challenge, we propose OTAKNet, the first online
solution to noise-statistics drift within learning-based adaptive Kalman
filtering. Unlike existing learning-based methods that perform offline
fine-tuning using batch pointwise matching over entire trajectories, OTAKNet
establishes a connection between the state estimate and the drift via one-step
predictive measurement likelihood, and addresses it using optimal transport.
This leverages OT's geometry - aware cost and stable gradients to enable fully
online adaptation without ground truth labels or retraining. We compare OTAKNet
against classical model-based adaptive Kalman filtering and offline
learning-based filtering. The performance is demonstrated on both synthetic and
real-world NCLT datasets, particularly under limited training data.

</details>


### [144] [Membership and Memorization in LLM Knowledge Distillation](https://arxiv.org/abs/2508.07054)
*Ziqi Zhang,Ali Shahin Shamsabadi,Hanxiao Lu,Yifeng Cai,Hamed Haddadi*

Main category: cs.LG

TL;DR: 研究发现，大型语言模型（LLM）的知识蒸馏（KD）技术会传递教师的隐私风险，包括成员和记忆隐私风险，且风险程度因技术而异。


<details>
  <summary>Details</summary>
Motivation: 解决知识蒸馏技术中教师模型隐私风险传递给学生模型的问题。

Method: 系统评估六种LLM KD技术，使用指令微调设置、三种教师模型家族（GPT-2、LLAMA-2、OPT）及不同规模学生模型，分析隐私风险。

Result: 所有KD技术均存在隐私风险传递，但程度不同；记忆与成员隐私风险存在显著差异；不同模块隐私风险差异大。

Conclusion: KD技术需考虑隐私风险，未来研究应关注风险缓解方法。

Abstract: Recent advances in Knowledge Distillation (KD) aim to mitigate the high
computational demands of Large Language Models (LLMs) by transferring knowledge
from a large ''teacher'' to a smaller ''student'' model. However, students may
inherit the teacher's privacy when the teacher is trained on private data. In
this work, we systematically characterize and investigate membership and
memorization privacy risks inherent in six LLM KD techniques. Using
instruction-tuning settings that span seven NLP tasks, together with three
teacher model families (GPT-2, LLAMA-2, and OPT), and various size student
models, we demonstrate that all existing LLM KD approaches carry membership and
memorization privacy risks from the teacher to its students. However, the
extent of privacy risks varies across different KD techniques. We
systematically analyse how key LLM KD components (KD objective functions,
student training data and NLP tasks) impact such privacy risks. We also
demonstrate a significant disagreement between memorization and membership
privacy risks of LLM KD techniques. Finally, we characterize per-block privacy
risk and demonstrate that the privacy risk varies across different blocks by a
large margin.

</details>


### [145] [Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation](https://arxiv.org/abs/2508.07075)
*Stanley Ngugi*

Main category: cs.LG

TL;DR: 论文提出了一种“先遗忘再学习”的策略，结合参数高效微调技术（IA³），用于解决大语言模型（LLMs）在动态知识更新中的冲突问题。该方法通过定位和干预特定内部组件，显著提升了新知识的准确性和旧知识的遗忘率，同时减少了灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: LLMs在动态知识更新中面临新知识与旧知识冲突的问题，导致模型难以接受新知识或遗忘无关知识。

Method: 采用“先遗忘再学习”策略，结合IA³技术，通过定位冲突知识的内部组件进行干预。

Result: 实验显示，该方法在新知识准确率（98.50%）和旧知识遗忘率（96.00%）上表现优异，同时显著减少了灾难性遗忘（72.00% F_control准确率）。

Conclusion: 该方法为紧凑型LLMs提供了精确、局部化且安全的知识管理方案，是知识编辑领域的重要进展。

Abstract: Large Language Models (LLMs) struggle with dynamic knowledge updates,
especially when new information conflicts with deeply embedded facts. Such
conflicting factual edits often lead to two critical issues: resistance to
adopting the new fact and severe catastrophic forgetting of unrelated
knowledge. This paper introduces and evaluates a novel "unlearn-then-learn"
strategy for precise knowledge editing in LLMs, leveraging the
parameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting
and Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach
is powered by an initial circuit localization phase that identifies and targets
the specific internal components responsible for encoding the conflicting fact.
Through a rigorous experimental methodology on
microsoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically
informed two-stage approach achieves near-perfect accuracy (98.50%) for the
new, modulated fact while simultaneously effectively suppressing the original
conflicting fact (96.00% forget rate). Critically, our strategy exhibits
unprecedented localization (72.00% F_control accuracy), dramatically mitigating
catastrophic forgetting observed in direct fine-tuning approaches (which showed
as low as ~20% F_control accuracy), a direct benefit of our targeted
interpretability-guided intervention. Furthermore, qualitative analysis reveals
a nuanced mechanism of "soft forgetting," where original knowledge is
suppressed from default retrieval but remains latent and conditionally
accessible, enhancing model safety and control. These findings represent a
significant advancement towards precise, localized, and safe knowledge
management in compact LLMs.

</details>


### [146] [Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework](https://arxiv.org/abs/2508.07085)
*N Harshit,K Mounvik*

Main category: cs.LG

TL;DR: 提出了一种结合Transformer和Autoencoder的混合框架，用于在线检测概念漂移，并通过Trust Score方法提高检测的敏感性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 概念漂移（数据分布的变化）会显著降低模型性能，现有检测方法多为被动且对早期检测不敏感。

Method: 使用Transformer和Autoencoder建模复杂时序动态，结合Trust Score方法（包括统计、重构、预测不确定性、规则违反和分类器误差趋势等指标）。

Result: 在航空乘客数据集上，该方法比基线方法更早、更敏感地检测到漂移，并减少了错误率和逻辑违反。

Conclusion: 开发了一个可靠的框架，用于实时监测概念漂移。

Abstract: In applied machine learning, concept drift, which is either gradual or abrupt
changes in data distribution, can significantly reduce model performance.
Typical detection methods,such as statistical tests or reconstruction-based
models,are generally reactive and not very sensitive to early detection. Our
study proposes a hybrid framework consisting of Transformers and Autoencoders
to model complex temporal dynamics and provide online drift detection. We
create a distinct Trust Score methodology, which includes signals on (1)
statistical and reconstruction-based drift metrics, more specifically, PSI,
JSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations,
and (4) trend of classifier error aligned with the combined metrics defined by
the Trust Score. Using a time sequenced airline passenger data set with
synthetic drift, our proposed model allows for a better detection of drift
using as a whole and at different detection thresholds for both sensitivity and
interpretability compared to baseline methods and provides a strong pipeline
for drift detection in real time for applied machine learning. We evaluated
performance using a time-sequenced airline passenger dataset having the
gradually injected stimulus of drift in expectations,e.g. permuted ticket
prices in later batches, broken into 10 time segments [1].In the data, our
results support that the Transformation-Autoencoder detected drift earlier and
with more sensitivity than the autoencoders commonly used in the literature,
and provided improved modeling over more error rates and logical violations.
Therefore, a robust framework was developed to reliably monitor concept drift.

</details>


### [147] [Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria](https://arxiv.org/abs/2508.07102)
*Yang Cao,Yubin Chen,Zhao Song,Jiahao Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为Second-Order MeanFlow的新方法，通过引入平均加速度场扩展了MeanFlow框架，支持高效单步采样，并证明了其理论可行性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 生成建模在无模拟范式（如MeanFlow）中取得了显著进展，但现有方法仅关注瞬时速度场。本研究旨在通过引入平均加速度场，进一步丰富动力学特性并提升采样效率。

Method: 提出Second-Order MeanFlow，扩展MeanFlow目标以包含平均加速度场，证明其满足广义一致性条件，并通过电路复杂性分析和注意力计算近似实现高效采样。

Result: 证明了Second-Order MeanFlow的可行性和高效性，其采样过程可在TC0类电路中实现，且注意力计算可高效近似。

Conclusion: 研究为高阶流匹配模型奠定了理论基础，结合了丰富的动力学特性和实际采样效率。

Abstract: Generative modelling has seen significant advances through simulation-free
paradigms such as Flow Matching, and in particular, the MeanFlow framework,
which replaces instantaneous velocity fields with average velocities to enable
efficient single-step sampling. In this work, we introduce a theoretical study
on Second-Order MeanFlow, a novel extension that incorporates average
acceleration fields into the MeanFlow objective. We first establish the
feasibility of our approach by proving that the average acceleration satisfies
a generalized consistency condition analogous to first-order MeanFlow, thereby
supporting stable, one-step sampling and tractable loss functions. We then
characterize its expressivity via circuit complexity analysis, showing that
under mild assumptions, the Second-Order MeanFlow sampling process can be
implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class.
Finally, we derive provably efficient criteria for scalable implementation by
leveraging fast approximate attention computations: we prove that attention
operations within the Second-Order MeanFlow architecture can be approximated to
within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results
lay the theoretical foundation for high-order flow matching models that combine
rich dynamics with practical sampling efficiency.

</details>


### [148] [BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation](https://arxiv.org/abs/2508.07106)
*Yiran Huang,Amirhossein Nouranizadeh,Christine Ahrends,Mengjia Xu*

Main category: cs.LG

TL;DR: 提出了一种名为BrainATCL的无监督、非参数框架，用于动态fMRI数据的自适应时间脑连接学习，解决了传统GNN难以捕捉长时程依赖的问题。


<details>
  <summary>Details</summary>
Motivation: fMRI信号的功能连接动态可能与行为和神经精神疾病相关，但传统GNN难以捕捉动态fMRI数据中的长时程依赖。

Method: 提出BrainATCL框架，动态调整时间窗口，使用GINE-Mamba2骨干网络学习时空表征，并结合脑结构和功能信息。

Result: 在功能连接预测和年龄估计任务中表现优异，具有强泛化能力。

Conclusion: BrainATCL为动态fMRI数据分析提供了有效工具，能够捕捉生物意义拓扑模式。

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely
used to study human brain activity. fMRI signals in areas across the brain
transiently synchronise and desynchronise their activity in a highly structured
manner, even when an individual is at rest. These functional connectivity
dynamics may be related to behaviour and neuropsychiatric disease. To model
these dynamics, temporal brain connectivity representations are essential, as
they reflect evolving interactions between brain regions and provide insight
into transient neural states and network reconfigurations. However,
conventional graph neural networks (GNNs) often struggle to capture long-range
temporal dependencies in dynamic fMRI data. To address this challenge, we
propose BrainATCL, an unsupervised, nonparametric framework for adaptive
temporal brain connectivity learning, enabling functional link prediction and
age estimation. Our method dynamically adjusts the lookback window for each
snapshot based on the rate of newly added edges. Graph sequences are
subsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal
representations of dynamic functional connectivity in resting-state fMRI data
of 1,000 participants from the Human Connectome Project. To further improve
spatial modeling, we incorporate brain structure and function-informed edge
attributes, i.e., the left/right hemispheric identity and subnetwork membership
of brain regions, enabling the model to capture biologically meaningful
topological patterns. We evaluate our BrainATCL on two tasks: functional link
prediction and age estimation. The experimental results demonstrate superior
performance and strong generalization, including in cross-session prediction
scenarios.

</details>


### [149] [Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning](https://arxiv.org/abs/2508.07114)
*Atakan Azakli,Bernd Stelzer*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习方法，通过多实例学习（MIL）提高假设检验中参数预测的精确度和判别力，并系统减少预测误差。


<details>
  <summary>Details</summary>
Motivation: 解决现有分类器在极端情况下难以准确预测的问题，并提升机器学习模型的判别能力。

Method: 利用多实例学习（MIL）的理论优势，分析其在不同实例数量下的缩放行为，并应用于标准模型有效场论（SMEFT）的Wilson系数约束。

Result: 在某些情况下，可能从数据集中提取理论上的最大Fisher信息。

Conclusion: 多实例学习在提升预测精度和判别力方面具有潜力，尤其在复杂物理问题中表现突出。

Abstract: In this work, we propose a new machine learning (ML) methodology to obtain
more precise predictions for some parameters of interest in a given hypotheses
testing problem. Our proposed method also allows ML models to have more
discriminative power in cases where it is extremely challenging for
state-of-the-art classifiers to have any level of accurate predictions. This
method can also allow us to systematically decrease the error from ML models in
their predictions. In this paper, we provide a mathematical motivation why
Multiple Instance Learning (MIL) would have more predictive power over their
single-instance counterparts. We support our theoretical claims by analyzing
the behavior of the MIL models through their scaling behaviors with respect to
the number of instances on which the model makes predictions. As a concrete
application, we constrain Wilson coefficients of the Standard Model Effective
Field Theory (SMEFT) using kinematic information from subatomic particle
collision events at the Large Hadron Collider (LHC). We show that under certain
circumstances, it might be possible to extract the theoretical maximum Fisher
Information latent in a dataset.

</details>


### [150] [From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context](https://arxiv.org/abs/2508.07117)
*Peyman Baghershahi,Gregoire Fournier,Pranav Nyati,Sourav Medya*

Main category: cs.LG

TL;DR: LOGIC是一个轻量级框架，利用LLM为GNN预测生成忠实且可解释的解释，通过将GNN嵌入投影到LLM空间并结合混合提示，显著提升了解释的直观性和稀疏性。


<details>
  <summary>Details</summary>
Motivation: GNN在结构化数据学习中表现强大，但其解释性不足，尤其在处理富含自然语言的节点属性时，现有方法难以生成细粒度的解释。

Method: LOGIC通过将GNN节点嵌入投影到LLM空间，结合软提示和文本输入构建混合提示，使LLM能够推理GNN内部表示并生成自然语言解释。

Result: 在四个真实TAG数据集上的实验表明，LOGIC在忠实性和稀疏性之间取得了良好平衡，并显著提升了人类中心指标（如洞察力）。

Conclusion: LOGIC为图学习中的LLM解释性设定了新方向，通过将GNN内部表示与人类推理对齐，提升了可解释性。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over
structured data, including text-attributed graphs, which are common in domains
such as citation networks, social platforms, and knowledge graphs. GNNs are not
inherently interpretable and thus, many explanation methods have been proposed.
However, existing explanation methods often struggle to generate interpretable,
fine-grained rationales, especially when node attributes include rich natural
language. In this work, we introduce LOGIC, a lightweight, post-hoc framework
that uses large language models (LLMs) to generate faithful and interpretable
explanations for GNN predictions. LOGIC projects GNN node embeddings into the
LLM embedding space and constructs hybrid prompts that interleave soft prompts
with textual inputs from the graph structure. This enables the LLM to reason
about GNN internal representations and produce natural language explanations
along with concise explanation subgraphs. Our experiments across four
real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off
between fidelity and sparsity, while significantly improving human-centric
metrics such as insightfulness. LOGIC sets a new direction for LLM-based
explainability in graph learning by aligning GNN internals with human
reasoning.

</details>


### [151] [Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks](https://arxiv.org/abs/2508.07122)
*Zhihao Xue,Yun Zi,Nia Qi,Ming Gong,Yujun Zou*

Main category: cs.LG

TL;DR: 提出一种基于时空图神经网络的性能预测算法，用于预测具有多级服务调用结构的分布式后端系统的性能波动。


<details>
  <summary>Details</summary>
Motivation: 解决分布式后端系统中多级服务调用结构带来的性能波动预测挑战。

Method: 将系统状态抽象为图结构序列，结合运行时特征和服务调用关系，构建统一的时空建模框架，使用图卷积网络和高门控循环网络分别提取拓扑依赖和时间动态信息。

Result: 实验表明，该模型在MAE、RMSE和R2等关键指标上优于现有方法，且在负载和结构复杂度变化下保持强鲁棒性。

Conclusion: 该模型在后台服务性能管理任务中具有实际应用潜力。

Abstract: This paper proposes a spatiotemporal graph neural network-based performance
prediction algorithm to address the challenge of forecasting performance
fluctuations in distributed backend systems with multi-level service call
structures. The method abstracts system states at different time slices into a
sequence of graph structures. It integrates the runtime features of service
nodes with the invocation relationships among services to construct a unified
spatiotemporal modeling framework. The model first applies a graph
convolutional network to extract high-order dependency information from the
service topology. Then it uses a gated recurrent network to capture the dynamic
evolution of performance metrics over time. A time encoding mechanism is also
introduced to enhance the model's ability to represent non-stationary temporal
sequences. The architecture is trained in an end-to-end manner, optimizing the
multi-layer nested structure to achieve high-precision regression of future
service performance metrics. To validate the effectiveness of the proposed
method, a large-scale public cluster dataset is used. A series of
multi-dimensional experiments are designed, including variations in time
windows and concurrent load levels. These experiments comprehensively evaluate
the model's predictive performance and stability. The experimental results show
that the proposed model outperforms existing representative methods across key
metrics such as MAE, RMSE, and R2. It maintains strong robustness under varying
load intensities and structural complexities. These results demonstrate the
model's practical potential for backend service performance management tasks.

</details>


### [152] [Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning](https://arxiv.org/abs/2508.07126)
*Zhengran Ji,Boyuan Chen*

Main category: cs.LG

TL;DR: Pref-GUIDE框架将实时标量反馈转化为基于偏好的数据，以改进奖励模型学习，用于持续策略训练。


<details>
  <summary>Details</summary>
Motivation: 在在线强化学习中，标量反馈通常噪声大且不一致，限制了奖励模型的准确性和泛化能力。

Method: Pref-GUIDE通过比较短时间窗口内的行为并过滤模糊反馈（Individual），以及通过用户群体的共识偏好（Voting）来提升鲁棒性。

Result: 在三个挑战性环境中，Pref-GUIDE显著优于标量反馈基线，投票变体甚至超过专家设计的密集奖励。

Conclusion: Pref-GUIDE通过将标量反馈重构为结构化偏好，为在线强化学习中利用人类输入提供了可扩展且原则性的方法。

Abstract: Training reinforcement learning agents with human feedback is crucial when
task objectives are difficult to specify through dense reward functions. While
prior methods rely on offline trajectory comparisons to elicit human
preferences, such data is unavailable in online learning scenarios where agents
must adapt on the fly. Recent approaches address this by collecting real-time
scalar feedback to guide agent behavior and train reward models for continued
learning after human feedback becomes unavailable. However, scalar feedback is
often noisy and inconsistent, limiting the accuracy and generalization of
learned rewards. We propose Pref-GUIDE, a framework that transforms real-time
scalar feedback into preference-based data to improve reward model learning for
continual policy training. Pref-GUIDE Individual mitigates temporal
inconsistency by comparing agent behaviors within short windows and filtering
ambiguous feedback. Pref-GUIDE Voting further enhances robustness by
aggregating reward models across a population of users to form consensus
preferences. Across three challenging environments, Pref-GUIDE significantly
outperforms scalar-feedback baselines, with the voting variant exceeding even
expert-designed dense rewards. By reframing scalar feedback as structured
preferences with population feedback, Pref-GUIDE offers a scalable and
principled approach for harnessing human input in online reinforcement
learning.

</details>


### [153] [How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?](https://arxiv.org/abs/2508.07127)
*Niranjana Arun Menon,Iqra Farooq,Yulong Li,Sara Ahmed,Yutong Xie,Muhammad Awais,Imran Razzak*

Main category: cs.LG

TL;DR: 利用微调的大型语言模型（LLMs）预测心血管疾病（CVD）及其相关SNPs，通过基因组数据学习潜在生物学关系，并评估其在早期检测和个性化医疗中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病预测因多因素病因和全球高发病率而具有挑战性，现有基因组数据的高维性和噪声问题使得提取有意义信息困难。

Method: 通过微调LLMs，将其应用于基因组数据，以链式思维（CoT）推理任务生成疾病标签和临床推论。

Result: 研究发现LLMs能够有效学习基因组数据中的潜在生物学关系，为早期检测和风险预测提供支持。

Conclusion: LLMs在心血管疾病预测和个性化医疗中展现出潜力，有望推动早期检测和风险管理的进步。

Abstract: Cardiovascular disease (CVD) prediction remains a tremendous challenge due to
its multifactorial etiology and global burden of morbidity and mortality.
Despite the growing availability of genomic and electrophysiological data,
extracting biologically meaningful insights from such high-dimensional, noisy,
and sparsely annotated datasets remains a non-trivial task. Recently, LLMs has
been applied effectively to predict structural variations in biological
sequences. In this work, we explore the potential of fine-tuned LLMs to predict
cardiac diseases and SNPs potentially leading to CVD risk using genetic markers
derived from high-throughput genomic profiling. We investigate the effect of
genetic patterns associated with cardiac conditions and evaluate how LLMs can
learn latent biological relationships from structured and semi-structured
genomic data obtained by mapping genetic aspects that are inherited from the
family tree. By framing the problem as a Chain of Thought (CoT) reasoning task,
the models are prompted to generate disease labels and articulate informed
clinical deductions across diverse patient profiles and phenotypes. The
findings highlight the promise of LLMs in contributing to early detection, risk
assessment, and ultimately, the advancement of personalized medicine in cardiac
care.

</details>


### [154] [A Globally Optimal Analytic Solution for Semi-Nonnegative Matrix Factorization with Nonnegative or Mixed Inputs](https://arxiv.org/abs/2508.07134)
*Lu Chenggang*

Main category: cs.LG

TL;DR: 提出一种全局最优的半非负矩阵分解方法，通过正交分解实现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半非负矩阵分解算法多为迭代、非凸且易陷入局部最优，需要一种全局最优的解决方案。

Method: 基于输入数据的散布矩阵进行正交分解，得到全局最优解。

Result: 在Frobenius范数下达到全局最小重构误差，且在低秩情况下恢复NMF结构。

Conclusion: 该方法在理论和实验上均优于现有方法，为矩阵分解提供了新视角。

Abstract: Semi-Nonnegative Matrix Factorization (semi-NMF) extends classical
Nonnegative Matrix Factorization (NMF) by allowing the basis matrix to contain
both positive and negative entries, making it suitable for decomposing data
with mixed signs. However, most existing semi-NMF algorithms are iterative,
non-convex, and prone to local minima. In this paper, we propose a novel method
that yields a globally optimal solution to the semi-NMF problem under the
Frobenius norm, through an orthogonal decomposition derived from the scatter
matrix of the input data. We rigorously prove that our solution attains the
global minimum of the reconstruction error. Furthermore, we demonstrate that
when the input matrix is nonnegative, our method often achieves lower
reconstruction error than standard NMF algorithms, although unfortunately the
basis matrix may not satisfy nonnegativity. In particular, in low-rank cases
such as rank 1 or 2, our solution reduces exactly to a nonnegative
factorization, recovering the NMF structure. We validate our approach through
experiments on both synthetic data and the UCI Wine dataset, showing that our
method consistently outperforms existing NMF and semi-NMF methods in terms of
reconstruction accuracy. These results confirm that our globally optimal,
non-iterative formulation offers both theoretical guarantees and empirical
advantages, providing a new perspective on matrix factorization in optimization
and data analysis.

</details>


### [155] [A Stable and Principled Loss Function for Direct Language Model Alignment](https://arxiv.org/abs/2508.07137)
*Yuandong Tan*

Main category: cs.LG

TL;DR: 论文提出了一种新的损失函数，解决了DPO方法中因无限最大化对数差导致的不稳定性和奖励攻击问题。


<details>
  <summary>Details</summary>
Motivation: DPO方法在理论推导与损失函数设计上存在不一致，可能导致训练不稳定和奖励攻击。

Method: 从RLHF最优条件直接推导出新的损失函数，针对对数差设定有限目标值。

Result: 新方法避免了梯度爆炸问题，显著提升了Qwen2.5-7B模型的胜率，性能媲美更大模型。

Conclusion: 提出的损失函数更稳定且有效，为LLM对齐提供了改进方案。

Abstract: The alignment of large language models (LLMs) with human preferences is
commonly achieved through Reinforcement Learning from Human Feedback (RLHF).
Direct Preference Optimization (DPO) simplified this paradigm by establishing a
direct mapping between the optimal policy and a reward function, eliminating
the need for an explicit reward model. However, we argue that the DPO loss
function is theoretically misaligned with its own derivation, as it promotes
the indefinite maximization of a logits difference, which can lead to training
instability and reward hacking. In this paper, we propose a novel loss function
derived directly from the RLHF optimality condition. Our proposed loss targets
a specific, finite value for the logits difference, which is dictated by the
underlying reward, rather than its maximization. We provide a theoretical
analysis, including a gradient-based comparison, to demonstrate that our method
avoids the large gradients that plague DPO when the probability of dispreferred
responses approaches zero. This inherent stability prevents reward hacking and
leads to more effective alignment. We validate our approach by fine-tuning a
Qwen2.5-7B model, showing significant win-rate improvements over a standard DPO
baseline and achieving competitive performance against larger models like
Llama-3.1-8B.

</details>


### [156] [Strategic Incentivization for Locally Differentially Private Federated Learning](https://arxiv.org/abs/2508.07138)
*Yashwant Krishna Pagoti,Arunesh Sinha,Shamik Sural*

Main category: cs.LG

TL;DR: 该论文提出了一种基于博弈论的激励机制，用于解决联邦学习中隐私与准确性的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，客户端通过添加噪声保护隐私，但会导致模型准确性下降。论文旨在通过激励机制平衡这一矛盾。

Method: 引入基于令牌的激励机制，客户端根据梯度扰动程度获得令牌，用于访问更新后的全局模型。

Result: 通过博弈论分析和实验验证，展示了不同参数对隐私与准确性权衡的影响。

Conclusion: 提出的激励机制有效平衡了隐私保护与模型准确性，为联邦学习提供了一种新思路。

Abstract: In Federated Learning (FL), multiple clients jointly train a machine learning
model by sharing gradient information, instead of raw data, with a server over
multiple rounds. To address the possibility of information leakage in spite of
sharing only the gradients, Local Differential Privacy (LDP) is often used. In
LDP, clients add a selective amount of noise to the gradients before sending
the same to the server. Although such noise addition protects the privacy of
clients, it leads to a degradation in global model accuracy. In this paper, we
model this privacy-accuracy trade-off as a game, where the sever incentivizes
the clients to add a lower degree of noise for achieving higher accuracy, while
the clients attempt to preserve their privacy at the cost of a potential loss
in accuracy. A token based incentivization mechanism is introduced in which the
quantum of tokens credited to a client in an FL round is a function of the
degree of perturbation of its gradients. The client can later access a newly
updated global model only after acquiring enough tokens, which are to be
deducted from its balance. We identify the players, their actions and payoff,
and perform a strategic analysis of the game. Extensive experiments were
carried out to study the impact of different parameters.

</details>


### [157] [SGD Convergence under Stepsize Shrinkage in Low-Precision Training](https://arxiv.org/abs/2508.07142)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: 本文研究了低精度训练中梯度收缩对SGD收敛的影响，发现收缩会降低收敛速度并增加误差。


<details>
  <summary>Details</summary>
Motivation: 低精度训练虽能减少计算和内存成本，但梯度量化会引入收缩和噪声，影响SGD的收敛行为。

Method: 通过梯度收缩模型分析SGD的收敛性，将低精度量化建模为梯度缩放和噪声扰动。

Result: 证明低精度SGD仍能收敛，但收敛速度因收缩因子降低，且量化噪声增加了渐近误差。

Conclusion: 低精度训练通过梯度收缩和噪声影响SGD收敛，需权衡计算效率与收敛性能。

Abstract: Low-precision training has become essential for reducing the computational
and memory costs of large-scale deep learning. However, quantization of
gradients introduces both magnitude shrinkage and additive noise, which can
alter the convergence behavior of stochastic gradient descent (SGD). In this
work, we study the convergence of SGD under a gradient shrinkage model, where
each stochastic gradient is scaled by a factor $q_k \in (0,1]$ and perturbed by
zero-mean quantization noise. We show that this shrinkage is equivalent to
replacing the nominal stepsize $\mu_k$ with an effective stepsize $\mu_k q_k$,
which slows convergence when $q_{\min} < 1$. Under standard smoothness and
bounded-variance assumptions, we prove that low-precision SGD still converges,
but at a reduced rate determined by $q_{\min}$, and with an increased
asymptotic error floor due to quantization noise. We theoretically analyze how
reduced numerical precision slows down training by modeling it as gradient
shrinkage in the standard SGD convergence framework.

</details>


### [158] [What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains](https://arxiv.org/abs/2508.07208)
*Chanakya Ekbote,Marco Bondaschi,Nived Rajaraman,Jason D. Lee,Michael Gastpar,Ashok Vardhan Makkuva,Paul Pu Liang*

Main category: cs.LG

TL;DR: 本文证明了双层单头Transformer可以表示任何k阶马尔可夫过程，填补了Transformer深度与ICL能力关系的理论空白。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer深度与上下文学习（ICL）能力的关系，特别是双层单头Transformer是否能表示高阶马尔可夫过程。

Method: 通过理论分析，证明双层单头Transformer可以表示任何条件k-gram，并进一步分析其学习动态。

Result: 双层单头Transformer能够表示任何k阶马尔可夫过程，且在学习过程中能有效形成上下文表示。

Conclusion: 研究深化了对Transformer ICL能力的理解，表明浅层架构在结构化序列建模任务中也能表现出强大的ICL能力。

Abstract: In-context learning (ICL) is a hallmark capability of transformers, through
which trained models learn to adapt to new tasks by leveraging information from
the input context. Prior work has shown that ICL emerges in transformers due to
the presence of special circuits called induction heads. Given the equivalence
between induction heads and conditional k-grams, a recent line of work modeling
sequential inputs as Markov processes has revealed the fundamental impact of
model depth on its ICL capabilities: while a two-layer transformer can
efficiently represent a conditional 1-gram model, its single-layer counterpart
cannot solve the task unless it is exponentially large. However, for higher
order Markov sources, the best known constructions require at least three
layers (each with a single attention head) - leaving open the question: can a
two-layer single-head transformer represent any kth-order Markov process? In
this paper, we precisely address this and theoretically show that a two-layer
transformer with one head per layer can indeed represent any conditional
k-gram. Thus, our result provides the tightest known characterization of the
interplay between transformer depth and Markov order for ICL. Building on this,
we further analyze the learning dynamics of our two-layer construction,
focusing on a simplified variant for first-order Markov chains, illustrating
how effective in-context representations emerge during training. Together,
these results deepen our current understanding of transformer-based ICL and
illustrate how even shallow architectures can surprisingly exhibit strong ICL
capabilities on structured sequence modeling tasks.

</details>


### [159] [Neural Bridge Processes](https://arxiv.org/abs/2508.07220)
*Jian Xu,Yican Liu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 提出了一种名为Neural Bridge Processes (NBPs)的新方法，用于建模随机函数，通过动态锚定输入x来增强扩散轨迹的耦合性和终点一致性。


<details>
  <summary>Details</summary>
Motivation: 传统方法如高斯过程（GPs）和神经过程（NPs）在可扩展性和表达能力上存在局限，而神经扩散过程（NDPs）在输入耦合和终点语义一致性上表现不佳。

Method: 通过重新设计前向核使其显式依赖于输入x，NBP强制约束扩散路径严格终止于监督目标，从而提供更强的梯度信号和终点一致性。

Result: 在合成数据、EEG信号回归和图像回归任务中，NBP显著优于基线方法。

Conclusion: NBP通过DDPM风格的桥采样，在性能和理论一致性上均表现出色，适用于结构化预测任务。

Abstract: Learning stochastic functions from partially observed context-target pairs is
a fundamental problem in probabilistic modeling. Traditional models like
Gaussian Processes (GPs) face scalability issues with large datasets and assume
Gaussianity, limiting their applicability. While Neural Processes (NPs) offer
more flexibility, they struggle with capturing complex, multi-modal target
distributions. Neural Diffusion Processes (NDPs) enhance expressivity through a
learned diffusion process but rely solely on conditional signals in the
denoising network, resulting in weak input coupling from an unconditional
forward process and semantic mismatch at the diffusion endpoint. In this work,
we propose Neural Bridge Processes (NBPs), a novel method for modeling
stochastic functions where inputs x act as dynamic anchors for the entire
diffusion trajectory. By reformulating the forward kernel to explicitly depend
on x, NBP enforces a constrained path that strictly terminates at the
supervised target. This approach not only provides stronger gradient signals
but also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG
signal regression and image regression tasks, achieving substantial
improvements over baselines. These results underscore the effectiveness of
DDPM-style bridge sampling in enhancing both performance and theoretical
consistency for structured prediction tasks.

</details>


### [160] [LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference](https://arxiv.org/abs/2508.07221)
*Po-Han Lee,Yu-Cheng Lin,Chan-Tung Ku,Chan Hsu,Pei-Cing Huang,Ping-Hsun Wu,Yihuang Kang*

Main category: cs.LG

TL;DR: 论文提出了一种基于大型语言模型（LLM）的代理方法，用于自动化混杂因素发现和亚组分析，以提升因果机器学习在复杂环境中的效果。


<details>
  <summary>Details</summary>
Motivation: 观测数据中的个体化治疗效果估计因未测量的混杂因素和结构偏差而具有挑战性，现有方法在复杂环境中效果有限且依赖专家标注。

Method: 利用LLM代理的推理能力，自动化发现混杂因素和亚组，减少人工依赖并保持可解释性。

Result: 在真实医疗数据集上的实验表明，该方法通过缩小置信区间和发现未识别的混杂偏差，提升了治疗效果估计的鲁棒性。

Conclusion: LLM代理为可扩展、可信赖且语义感知的因果推断提供了新方向。

Abstract: Estimating individualized treatment effects from observational data presents
a persistent challenge due to unmeasured confounding and structural bias.
Causal Machine Learning (causal ML) methods, such as causal trees and doubly
robust estimators, provide tools for estimating conditional average treatment
effects. These methods have limited effectiveness in complex real-world
environments due to the presence of latent confounders or those described in
unstructured formats. Moreover, reliance on domain experts for confounder
identification and rule interpretation introduces high annotation cost and
scalability concerns. In this work, we proposed Large Language Model-based
agents for automated confounder discovery and subgroup analysis that integrate
agents into the causal ML pipeline to simulate domain expertise. Our framework
systematically performs subgroup identification and confounding structure
discovery by leveraging the reasoning capabilities of LLM-based agents, which
reduces human dependency while preserving interpretability. Experiments on
real-world medical datasets show that our proposed approach enhances treatment
effect estimation robustness by narrowing confidence intervals and uncovering
unrecognized confounding biases. Our findings suggest that LLM-based agents
offer a promising path toward scalable, trustworthy, and semantically aware
causal inference.

</details>


### [161] [EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning](https://arxiv.org/abs/2508.07224)
*Ananda Prakash Verma*

Main category: cs.LG

TL;DR: EDGE是一个通用的、基于误区的自适应学习框架，包含评估、诊断、生成和练习四个阶段，结合了心理测量学、认知诊断、对比项生成和调度策略。


<details>
  <summary>Details</summary>
Motivation: 解决传统自适应学习系统未能有效识别和纠正学习者误区的局限性。

Method: 结合IRT/Bayesian状态空间模型、误区发现、对比项生成和基于索引的调度策略。

Result: 提出了EdgeScore，证明了其单调性和Lipschitz连续性，并推导出近似最优的索引策略。

Conclusion: EDGE框架在理论上和实现上展示了潜力，未来需进一步实证研究。

Abstract: We present EDGE, a general-purpose, misconception-aware adaptive learning
framework composed of four stages: Evaluate (ability and state estimation),
Diagnose (posterior infer-ence of misconceptions), Generate (counterfactual
item synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies
psychometrics (IRT/Bayesian state space models), cog-nitive diagnostics
(misconception discovery from distractor patterns and response latencies),
contrastive item generation (minimal perturbations that invalidate learner
shortcuts while pre-serving psychometric validity), and principled scheduling
(a restless bandit approximation to spaced retrieval). We formalize a composite
readiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity,
and derive an index policy that is near-optimal under mild assumptions on
forgetting and learning gains. We further establish conditions under which
counterfactual items provably reduce the posterior probability of a targeted
misconception faster than standard practice. The paper focuses on theory and
implementable pseudocode; empirical study is left to future work.

</details>


### [162] [Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation](https://arxiv.org/abs/2508.07243)
*Chu Zhao,Eneng Yang,Yizhou Dang,Jianzhe Zhao,Guibing Guo,Xingwei Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为CNSDiff的新方法，通过扩散过程在潜在空间中合成负样本，避免了预定义候选池引入的偏差，并利用因果正则化减少环境混杂因素的影响，从而提升推荐系统的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 启发式负采样方法可能因环境混杂因素（如曝光或流行度偏差）引入虚假硬负样本（FHNS），导致模型学习虚假相关性，影响泛化能力。

Method: 提出CNSDiff方法，通过条件扩散过程在潜在空间合成负样本，并加入因果正则化以减少混杂因素的影响。

Result: 在四种代表性分布偏移场景下，CNSDiff相比基线方法平均提升了13.96%的性能。

Conclusion: CNSDiff有效减少了FHNS的生成，提升了推荐系统在分布偏移下的泛化能力。

Abstract: Heuristic negative sampling enhances recommendation performance by selecting
negative samples of varying hardness levels from predefined candidate pools to
guide the model toward learning more accurate decision boundaries. However, our
empirical and theoretical analyses reveal that unobserved environmental
confounders (e.g., exposure or popularity biases) in candidate pools may cause
heuristic sampling methods to introduce false hard negatives (FHNS). These
misleading samples can encourage the model to learn spurious correlations
induced by such confounders, ultimately compromising its generalization ability
under distribution shifts. To address this issue, we propose a novel method
named Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing
negative samples in the latent space via a conditional diffusion process,
CNSDiff avoids the bias introduced by predefined candidate pools and thus
reduces the likelihood of generating FHNS. Moreover, it incorporates a causal
regularization term to explicitly mitigate the influence of environmental
confounders during the negative sampling process, leading to robust negatives
that promote out-of-distribution (OOD) generalization. Comprehensive
experiments under four representative distribution shift scenarios demonstrate
that CNSDiff achieves an average improvement of 13.96% across all evaluation
metrics compared to state-of-the-art baselines, verifying its effectiveness and
robustness in OOD recommendation tasks.

</details>


### [163] [Policy Newton methods for Distortion Riskmetrics](https://arxiv.org/abs/2508.07249)
*Soumen Pachal,Mizhaan Prajit Maniyar,Prashanth L. A*

Main category: cs.LG

TL;DR: 论文研究了风险敏感控制在强化学习框架下的问题，提出了一种基于失真风险度量的风险最优策略，并开发了一种收敛到二阶稳定点的算法。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要关注风险中性目标或风险敏感目标的一阶稳定点，本文旨在填补风险敏感目标二阶稳定点收敛的空白。

Method: 使用似然比方法推导了失真风险度量的策略Hessian定理，并提出了基于样本轨迹的Hessian估计器。开发了一种立方正则化策略牛顿算法。

Result: 算法收敛到失真风险度量的二阶稳定点，样本复杂度为O(ε^{-3.5})，实验验证了理论结果。

Conclusion: 本文首次实现了风险敏感目标二阶稳定点的收敛，填补了现有研究的空白。

Abstract: We consider the problem of risk-sensitive control in a reinforcement learning
(RL) framework. In particular, we aim to find a risk-optimal policy by
maximizing the distortion riskmetric (DRM) of the discounted reward in a finite
horizon Markov decision process (MDP). DRMs are a rich class of risk measures
that include several well-known risk measures as special cases. We derive a
policy Hessian theorem for the DRM objective using the likelihood ratio method.
Using this result, we propose a natural DRM Hessian estimator from sample
trajectories of the underlying MDP. Next, we present a cubic-regularized policy
Newton algorithm for solving this problem in an on-policy RL setting using
estimates of the DRM gradient and Hessian. Our proposed algorithm is shown to
converge to an $\epsilon$-second-order stationary point ($\epsilon$-SOSP) of
the DRM objective, and this guarantee ensures the escaping of saddle points.
The sample complexity of our algorithms to find an $ \epsilon$-SOSP is
$\mathcal{O}(\epsilon^{-3.5})$. Our experiments validate the theoretical
findings. To the best of our knowledge, our is the first work to present
convergence to an $\epsilon$-SOSP of a risk-sensitive objective, while existing
works in the literature have either shown convergence to a first-order
stationary point of a risk-sensitive objective, or a SOSP of a risk-neutral
one.

</details>


### [164] [PySeizure: A single machine learning classifier framework to detect seizures in diverse datasets](https://arxiv.org/abs/2508.07253)
*Bartlomiej Chybowski,Shima Abdullateef,Hollan Haule,Alfredo Gonzalez-Sulser,Javier Escudero*

Main category: cs.LG

TL;DR: 论文提出了一种开源机器学习框架，用于跨不同临床数据集的可靠癫痫发作检测，通过自动预处理和多数投票机制提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前癫痫诊断依赖耗时的手动EEG分析，现有机器学习方法因数据集特定优化而受限，缺乏普适性和可重复性。

Method: 引入自动预处理管道标准化数据，采用多数投票机制，多个模型独立评估EEG数据后综合决策，并在不同数据集上训练和评估模型。

Result: 模型在数据集内表现优异（AUC 0.904和0.864），跨数据集泛化能力强（AUC 0.615和0.762），轻微后处理进一步提升性能。

Conclusion: 该框架为临床可行的、数据集无关的癫痫检测系统提供了基础，具有广泛应用的潜力，可加速临床整合。

Abstract: Reliable seizure detection is critical for diagnosing and managing epilepsy,
yet clinical workflows remain dependent on time-consuming manual EEG
interpretation. While machine learning has shown promise, existing approaches
often rely on dataset-specific optimisations, limiting their real-world
applicability and reproducibility. Here, we introduce an innovative,
open-source machine-learning framework that enables robust and generalisable
seizure detection across varied clinical datasets. We evaluate our approach on
two publicly available EEG datasets that differ in patient populations and
electrode configurations. To enhance robustness, the framework incorporates an
automated pre-processing pipeline to standardise data and a majority voting
mechanism, in which multiple models independently assess each second of EEG
before reaching a final decision. We train, tune, and evaluate models within
each dataset, assessing their cross-dataset transferability. Our models achieve
high within-dataset performance (AUC 0.904+/-0.059 for CHB-MIT and
0.864+/-0.060 for TUSZ) and demonstrate strong generalisation across datasets
despite differences in EEG setups and populations (AUC 0.615+/-0.039 for models
trained on CHB-MIT and tested on TUSZ and 0.762+/-0.175 in the reverse case)
without any post-processing. Furthermore, a mild post-processing improved the
within-dataset results to 0.913+/-0.064 and 0.867+/-0.058 and cross-dataset
results to 0.619+/-0.036 and 0.768+/-0.172. These results underscore the
potential of, and essential considerations for, deploying our framework in
diverse clinical settings. By making our methodology fully reproducible, we
provide a foundation for advancing clinically viable, dataset-agnostic seizure
detection systems. This approach has the potential for widespread adoption,
complementing rather than replacing expert interpretation, and accelerating
clinical integration.

</details>


### [165] [Revisiting Data Attribution for Influence Functions](https://arxiv.org/abs/2508.07297)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.LG

TL;DR: 本文综述了影响函数在深度学习中的数据归因能力，探讨了其理论基础、高效逆Hessian-向量积估计算法，并评估了其在数据归因和错误标签检测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 理解训练数据如何影响模型预测是机器学习可解释性、数据调试和模型问责的基础。影响函数提供了一种高效的一阶近似方法，无需重新训练即可估计数据点对模型参数和预测的影响。

Method: 通过影响函数理论及其算法实现（如逆Hessian-向量积估计），评估数据归因和错误标签检测的有效性。

Result: 影响函数在数据归因和错误标签检测中表现出有效性，但仍面临大规模实际应用中的挑战。

Conclusion: 影响函数在深度学习中有巨大潜力，但需进一步解决大规模应用中的挑战，并探索未来发展方向。

Abstract: The goal of data attribution is to trace the model's predictions through the
learning algorithm and back to its training data. thereby identifying the most
influential training samples and understanding how the model's behavior leads
to particular predictions. Understanding how individual training examples
influence a model's predictions is fundamental for machine learning
interpretability, data debugging, and model accountability. Influence
functions, originating from robust statistics, offer an efficient, first-order
approximation to estimate the impact of marginally upweighting or removing a
data point on a model's learned parameters and its subsequent predictions,
without the need for expensive retraining. This paper comprehensively reviews
the data attribution capability of influence functions in deep learning. We
discuss their theoretical foundations, recent algorithmic advances for
efficient inverse-Hessian-vector product estimation, and evaluate their
effectiveness for data attribution and mislabel detection. Finally,
highlighting current challenges and promising directions for unleashing the
huge potential of influence functions in large-scale, real-world deep learning
scenarios.

</details>


### [166] [When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective](https://arxiv.org/abs/2508.07299)
*Lin-Han Jia,Si-Yu Han,Wen-Chao Hu,Jie-Jing Shao,Wen-Da Wei,Zhi Zhou,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: 论文通过将神经符号学习（Nesy）理论扩展到不可靠知识（假设）场景，统一了半监督/自监督学习（SSL）与Nesy的理论框架，提出了预测前置任务有效性的方法。


<details>
  <summary>Details</summary>
Motivation: 当前无监督前置任务的选择缺乏理论依据，论文旨在通过理论分析和方法设计，改变这种启发式选择的现状。

Method: 基于知识可学习性、可靠性和完整性的理论分析，提出预测前置任务有效性的方案，并通过实验验证。

Result: 实验证实预测性能与实际性能高度相关，验证了理论和评估方法的有效性。

Conclusion: 论文统一了SSL与Nesy的理论框架，并提出了一种理论驱动的无监督任务选择方法，具有实际应用价值。

Abstract: Neuro-symbolic (Nesy) learning improves the target task performance of models
by enabling them to satisfy knowledge, while semi/self-supervised learning
(SSL) improves the target task performance by designing unsupervised pretext
tasks for unlabeled data to make models satisfy corresponding assumptions. We
extend the Nesy theory based on reliable knowledge to the scenario of
unreliable knowledge (i.e., assumptions), thereby unifying the theoretical
frameworks of SSL and Nesy. Through rigorous theoretical analysis, we
demonstrate that, in theory, the impact of pretext tasks on target performance
hinges on three factors: knowledge learnability with respect to the model,
knowledge reliability with respect to the data, and knowledge completeness with
respect to the target. We further propose schemes to operationalize these
theoretical metrics, and thereby develop a method that can predict the
effectiveness of pretext tasks in advance. This will change the current status
quo in practical applications, where the selections of unsupervised tasks are
heuristic-based rather than theory-based, and it is difficult to evaluate the
rationality of unsupervised pretext task selection before testing the model on
the target task. In experiments, we verify a high correlation between the
predicted performance-estimated using minimal data-and the actual performance
achieved after large-scale semi-supervised or self-supervised learning, thus
confirming the validity of the theory and the effectiveness of the evaluation
method.

</details>


### [167] [Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative](https://arxiv.org/abs/2508.07329)
*Tuo Zhang,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于Hessian感知量化（HAQ）和CPU-GPU协同推理的高效MoE边缘部署方案，解决了量化精度和内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在资源受限的边缘设备上部署面临量化精度下降和内存限制的挑战。

Method: 采用Hessian感知量化实现8位量化，并设计专家级协同卸载和推理机制。

Result: 在OPT系列和Mixtral 8*7B等模型上，量化模型精度接近全精度模型，GPU内存使用减少60%，推理延迟显著改善。

Conclusion: 该方法有效提升了MoE架构在边缘设备上的部署效率和性能。

Abstract: With the breakthrough progress of large language models (LLMs) in natural
language processing and multimodal tasks, efficiently deploying them on
resource-constrained edge devices has become a critical challenge. The Mixture
of Experts (MoE) architecture enhances model capacity through sparse
activation, but faces two major difficulties in practical deployment: (1) The
presence of numerous outliers in activation distributions leads to severe
degradation in quantization accuracy for both activations and weights,
significantly impairing inference performance; (2) Under limited memory,
efficient offloading and collaborative inference of expert modules struggle to
balance latency and throughput. To address these issues, this paper proposes an
efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ)
and CPU-GPU collaborative inference. First, by introducing smoothed Hessian
matrix quantization, we achieve joint 8-bit quantization of activations and
weights, which significantly alleviates the accuracy loss caused by outliers
while ensuring efficient implementation on mainstream hardware. Second, we
design an expert-level collaborative offloading and inference mechanism, which,
combined with expert activation path statistics, enables efficient deployment
and scheduling of expert modules between CPU and GPU, greatly reducing memory
footprint and inference latency. Extensive experiments validate the
effectiveness of our method on mainstream large models such as the OPT series
and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of
the low-bit quantized model approaches that of the full-precision model, while
GPU memory usage is reduced by about 60%, and inference latency is
significantly improved.

</details>


### [168] [Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants](https://arxiv.org/abs/2508.07333)
*Yuhao Liu,Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: 论文研究了随机插值在生成模型中的应用，分析了两种数值积分方法（欧拉法和Heun法）的有限时间收敛性，并提出了优化计算效率的调度策略。


<details>
  <summary>Details</summary>
Motivation: 随机插值在数据分布转换和生成模型中具有潜力，但其数值实现的有限时间收敛性缺乏严格分析。

Method: 通过分析随机插值构造的ODE，研究了欧拉法和Heun法的有限时间误差界限，并优化了迭代复杂度调度。

Result: 建立了两种数值方法的有限时间误差界限，并通过实验验证了理论结果。

Conclusion: 研究为随机插值的数值实现提供了理论支持，优化了计算效率。

Abstract: Stochastic interpolants offer a robust framework for continuously
transforming samples between arbitrary data distributions, holding significant
promise for generative modeling. Despite their potential, rigorous finite-time
convergence guarantees for practical numerical schemes remain largely
unexplored. In this work, we address the finite-time convergence analysis of
numerical implementations for ordinary differential equations (ODEs) derived
from stochastic interpolants. Specifically, we establish novel finite-time
error bounds in total variation distance for two widely used numerical
integrators: the first-order forward Euler method and the second-order Heun's
method. Furthermore, our analysis on the iteration complexity of specific
stochastic interpolant constructions provides optimized schedules to enhance
computational efficiency. Our theoretical findings are corroborated by
numerical experiments, which validate the derived error bounds and complexity
analyses.

</details>


### [169] [ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis](https://arxiv.org/abs/2508.07345)
*Samiha Afaf Neha,Abir Ahammed Bhuiyan,Md. Ishrak Khan*

Main category: cs.LG

TL;DR: 论文提出了一种名为ProteoKnight的图像编码方法，用于噬菌体病毒蛋白（PVP）的预测，通过预训练的卷积神经网络实现高准确度分类，并评估了预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 由于噬菌体病毒蛋白（PVP）在基因组研究中的重要性，需要有效的计算工具进行预测。现有方法存在空间信息丢失的问题，因此提出新的图像编码方法。

Method: ProteoKnight基于DNA-Walk算法改进，通过像素颜色和步长调整捕捉蛋白质特征，使用预训练CNN分类，并通过蒙特卡洛Dropout评估不确定性。

Result: 实验显示二元分类准确率达90.8%，多分类效果较差。不确定性分析揭示了预测置信度受蛋白质类别和序列长度影响。

Conclusion: ProteoKnight克服了现有方法的局限性，提供了高准确度的PVP预测，并能识别低置信度预测。

Abstract: \textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is
essential for genomic studies due to their crucial role as structural elements
in bacteriophages. Computational tools, particularly machine learning, have
emerged for annotating phage protein sequences from high-throughput sequencing.
However, effective annotation requires specialized sequence encodings. Our
paper introduces ProteoKnight, a new image-based encoding method that addresses
spatial constraints in existing techniques, yielding competitive performance in
PVP classification using pre-trained convolutional neural networks.
Additionally, our study evaluates prediction uncertainty in binary PVP
classification through Monte Carlo Dropout (MCD). \textbf{Methods:}
ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences,
incorporating pixel colors and adjusting walk distances to capture intricate
protein features. Encoded sequences were classified using multiple pre-trained
CNNs. Variance and entropy measures assessed prediction uncertainty across
proteins of various classes and lengths. \textbf{Results:} Our experiments
achieved 90.8% accuracy in binary classification, comparable to
state-of-the-art methods. Multi-class classification accuracy remains
suboptimal. Our uncertainty analysis unveils variability in prediction
confidence influenced by protein class and sequence length.
\textbf{Conclusions:} Our study surpasses frequency chaos game representation
(FCGR) by introducing novel image encoding that mitigates spatial information
loss limitations. Our classification technique yields accurate and robust PVP
predictions while identifying low-confidence predictions.

</details>


### [170] [Intrinsic training dynamics of deep neural networks](https://arxiv.org/abs/2508.07370)
*Sibylle Marcotte,Gabriel Peyré,Rémi Gribonval*

Main category: cs.LG

TL;DR: 论文研究了高维参数空间中梯度流是否能简化为低维结构（隐式偏差），提出了基于核包含的简单判据，并应用于ReLU网络和线性网络。


<details>
  <summary>Details</summary>
Motivation: 理解高维参数空间中基于梯度的训练是否能被低维结构捕获，从而揭示隐式偏差的机制。

Method: 通过研究梯度流在高维变量θ和低维变量z=ϕ(θ)之间的关系，提出基于核包含的判据，并应用于ReLU网络和线性网络。

Result: 证明了对于任意初始化的ReLU网络，可以将梯度流重写为低维动态；对于线性网络，放宽的平衡初始化是唯一确保低维动态的条件。

Conclusion: 论文为理解梯度流的低维动态提供了理论框架，并展示了其在深度网络中的具体应用。

Abstract: A fundamental challenge in the theory of deep learning is to understand
whether gradient-based training in high-dimensional parameter spaces can be
captured by simpler, lower-dimensional structures, leading to so-called
implicit bias. As a stepping stone, we study when a gradient flow on a
high-dimensional variable $\theta$ implies an intrinsic gradient flow on a
lower-dimensional variable $z = \phi(\theta)$, for an architecture-related
function $\phi$. We express a so-called intrinsic dynamic property and show how
it is related to the study of conservation laws associated with the
factorization $\phi$. This leads to a simple criterion based on the inclusion
of kernels of linear maps which yields a necessary condition for this property
to hold. We then apply our theory to general ReLU networks of arbitrary depth
and show that, for any initialization, it is possible to rewrite the flow as an
intrinsic dynamic in a lower dimension that depends only on $z$ and the
initialization, when $\phi$ is the so-called path-lifting. In the case of
linear networks with $\phi$ the product of weight matrices, so-called balanced
initializations are also known to enable such a dimensionality reduction; we
generalize this result to a broader class of {\em relaxed balanced}
initializations, showing that, in certain configurations, these are the
\emph{only} initializations that ensure the intrinsic dynamic property.
Finally, for the linear neural ODE associated with the limit of infinitely deep
linear networks, with relaxed balanced initialization, we explicitly express
the corresponding intrinsic dynamics.

</details>


### [171] [Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems](https://arxiv.org/abs/2508.07392)
*Nikita Puchkin,Denis Suchkov,Alexey Naumov,Denis Belomestny*

Main category: cs.LG

TL;DR: 论文研究了基于Schrödinger桥和随机最优控制理论的生成建模和无配对图像转换方法，通过Ornstein-Uhlenbeck过程估计Schrödinger势，并推导了经验风险最小化器的泛化能力界限。


<details>
  <summary>Details</summary>
Motivation: 解决在仅能获取初始和最终分布样本的情况下，如何通过随机最优控制理论实现生成建模和无配对图像转换的问题。

Method: 采用Ornstein-Uhlenbeck过程作为参考过程，估计Schrödinger势，并通过Kullback-Leibler散度定义风险函数。

Result: 在包括高斯混合的Schrödinger势类中，推导了经验风险最小化器的紧致泛化界限，并在有利情况下接近快速收敛速率。

Conclusion: 通过数值实验验证了方法的有效性，展示了其在生成建模和无配对图像转换中的潜力。

Abstract: Modern methods of generative modelling and unpaired image-to-image
translation based on Schr\"odinger bridges and stochastic optimal control
theory aim to transform an initial density to a target one in an optimal way.
In the present paper, we assume that we only have access to i.i.d. samples from
initial and final distributions. This makes our setup suitable for both
generative modelling and unpaired image-to-image translation. Relying on the
stochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as
the reference one and estimate the corresponding Schr\"odinger potential.
Introducing a risk function as the Kullback-Leibler divergence between
couplings, we derive tight bounds on generalization ability of an empirical
risk minimizer in a class of Schr\"odinger potentials including Gaussian
mixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we
almost achieve fast rates of convergence up to some logarithmic factors in
favourable scenarios. We also illustrate performance of the suggested approach
with numerical experiments.

</details>


### [172] [Parity Requires Unified Input Dependence and Negative Eigenvalues in SSMs](https://arxiv.org/abs/2508.07395)
*Behnoush Khavari,Mehran Shakerinava,Jayesh Khullar,Jerry Huang,François Rivest,Siamak Ravanbakhsh,Sarath Chandar*

Main category: cs.LG

TL;DR: 论文探讨了LRNN模型（如S4D、Mamba和DeltaNet）在状态跟踪任务中的局限性，并提出输入依赖的转移矩阵可能提升性能。研究发现，即使结合多层SSM，对角线转移矩阵仍无法解决简单任务（如奇偶校验），表明需要输入依赖且包含负特征值的递归层。


<details>
  <summary>Details</summary>
Motivation: 现有LRNN模型在状态跟踪任务中表现不佳，原因是时间不变的转移矩阵或受限的特征值范围。论文旨在探索输入依赖的转移矩阵是否能改善性能。

Method: 研究结合多层SSM（对角线转移矩阵）是否能解决简单状态跟踪任务（如奇偶校验）。实验分析了结合S4D和Mamba层的SSM模型。

Result: 研究发现，即使结合多层SSM，对角线转移矩阵仍无法解决奇偶校验任务。

Conclusion: 递归层需同时具备输入依赖性和负特征值才能有效解决状态跟踪任务。

Abstract: Recent work has shown that LRNN models such as S4D, Mamba, and DeltaNet lack
state-tracking capability due to either time-invariant transition matrices or
restricted eigenvalue ranges. To address this, input-dependent transition
matrices, particularly those that are complex or non-triangular, have been
proposed to enhance SSM performance on such tasks. While existing theorems
demonstrate that both input-independent and non-negative SSMs are incapable of
solving simple state-tracking tasks, such as parity, regardless of depth, they
do not explore whether combining these two types in a multilayer SSM could
help. We investigate this question for efficient SSMs with diagonal transition
matrices and show that such combinations still fail to solve parity. This
implies that a recurrence layer must both be input-dependent and include
negative eigenvalues. Our experiments support this conclusion by analyzing an
SSM model that combines S4D and Mamba layers.

</details>


### [173] [Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors](https://arxiv.org/abs/2508.07400)
*Mohamad Louai Shehab,Alperen Tercan,Necmiye Ozay*

Main category: cs.LG

TL;DR: 论文研究了从最优策略或最大熵强化学习演示中恢复时变奖励函数的问题，提出了两种先验假设，并分别转化为稀疏化和秩最小化问题，给出了高效算法。


<details>
  <summary>Details</summary>
Motivation: 从最优策略或演示中恢复奖励函数是一个高度不适定问题，但在许多应用中奖励函数具有稀疏性或可表示为少量特征的线性组合，因此需要有效的恢复方法。

Method: 提出了两种先验假设：1) 奖励函数大多为常数且变化稀疏；2) 奖励函数可表示为少量特征的线性组合。分别转化为稀疏化和秩最小化问题，并设计了多项式时间算法和凸松弛方法。

Result: 实验表明，提出的算法能够准确恢复奖励函数，并具有良好的泛化能力。

Conclusion: 通过先验假设和优化方法，论文成功解决了奖励函数恢复问题，为实际应用提供了高效的工具。

Abstract: In this paper, we consider the problem of recovering time-varying reward
functions from either optimal policies or demonstrations coming from a max
entropy reinforcement learning problem. This problem is highly ill-posed
without additional assumptions on the underlying rewards. However, in many
applications, the rewards are indeed parsimonious, and some prior information
is available. We consider two such priors on the rewards: 1) rewards are mostly
constant and they change infrequently, 2) rewards can be represented by a
linear combination of a small number of feature functions. We first show that
the reward identification problem with the former prior can be recast as a
sparsification problem subject to linear constraints. Moreover, we give a
polynomial-time algorithm that solves this sparsification problem exactly.
Then, we show that identifying rewards representable with the minimum number of
features can be recast as a rank minimization problem subject to linear
constraints, for which convex relaxations of rank can be invoked. In both
cases, these observations lead to efficient optimization-based reward
identification algorithms. Several examples are given to demonstrate the
accuracy of the recovered rewards as well as their generalizability.

</details>


### [174] [Lightning Prediction under Uncertainty: DeepLight with Hazy Loss](https://arxiv.org/abs/2508.07428)
*Md Sultanul Arifin,Abu Nowshed Sakib,Yeasir Rayhan,Tanzima Hashem*

Main category: cs.LG

TL;DR: DeepLight是一种新型深度学习架构，用于预测闪电事件，通过多源气象数据和双编码器架构提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 闪电对人身安全和经济发展构成重大威胁，现有预测模型存在动态空间捕捉不足、数据利用不充分等问题。

Method: DeepLight结合雷达反射率、云属性和历史闪电数据，采用多分支卷积技术和Hazy Loss函数处理时空不确定性。

Result: 实验表明，DeepLight的公平威胁评分（ETS）比现有方法提高18%-30%。

Conclusion: DeepLight为闪电预测提供了更可靠的解决方案。

Abstract: Lightning, a common feature of severe meteorological conditions, poses
significant risks, from direct human injuries to substantial economic losses.
These risks are further exacerbated by climate change. Early and accurate
prediction of lightning would enable preventive measures to safeguard people,
protect property, and minimize economic losses. In this paper, we present
DeepLight, a novel deep learning architecture for predicting lightning
occurrences. Existing prediction models face several critical limitations: they
often struggle to capture the dynamic spatial context and inherent uncertainty
of lightning events, underutilize key observational data, such as radar
reflectivity and cloud properties, and rely heavily on Numerical Weather
Prediction (NWP) systems, which are both computationally expensive and highly
sensitive to parameter settings. To overcome these challenges, DeepLight
leverages multi-source meteorological data, including radar reflectivity, cloud
properties, and historical lightning occurrences through a dual-encoder
architecture. By employing multi-branch convolution techniques, it dynamically
captures spatial correlations across varying extents. Furthermore, its novel
Hazy Loss function explicitly addresses the spatio-temporal uncertainty of
lightning by penalizing deviations based on proximity to true events, enabling
the model to better learn patterns amidst randomness. Extensive experiments
show that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over
state-of-the-art methods, establishing it as a robust solution for lightning
prediction.

</details>


### [175] [Unsupervised operator learning approach for dissipative equations via Onsager principle](https://arxiv.org/abs/2508.07440)
*Zhipeng Chang,Zhenye Wen,Xiaofei Zhao*

Main category: cs.LG

TL;DR: 提出了一种名为DOOL的无监督学习方法，用于解决耗散方程，无需高保真模拟数据，通过最小化Onsager变分原理定义的Rayleighian函数进行训练。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法依赖高保真模拟数据的监督训练，计算成本高。

Method: 基于Onsager变分原理，提出DOOL方法，通过直接最小化Rayleighian函数进行无监督训练，采用时空解耦策略提高效率。

Result: 数值实验验证了DOOL的有效性，与监督方法DeepONet和MIONet相比性能更优。

Conclusion: DOOL是一种高效的无监督算子学习方法，适用于耗散方程，并可扩展至不直接遵循OVP的二阶波模型。

Abstract: Existing operator learning methods rely on supervised training with
high-fidelity simulation data, introducing significant computational cost. In
this work, we propose the deep Onsager operator learning (DOOL) method, a novel
unsupervised framework for solving dissipative equations. Rooted in the Onsager
variational principle (OVP), DOOL trains a deep operator network by directly
minimizing the OVP-defined Rayleighian functional, requiring no labeled data,
and then proceeds in time explicitly through conservation/change laws for the
solution. Another key innovation here lies in the spatiotemporal decoupling
strategy: the operator's trunk network processes spatial coordinates
exclusively, thereby enhancing training efficiency, while integrated external
time stepping enables temporal extrapolation. Numerical experiments on typical
dissipative equations validate the effectiveness of the DOOL method, and
systematic comparisons with supervised DeepONet and MIONet demonstrate its
enhanced performance. Extensions are made to cover the second-order wave models
with dissipation that do not directly follow OVP.

</details>


### [176] [Stackelberg Coupling of Online Representation Learning and Reinforcement Learning](https://arxiv.org/abs/2508.07452)
*Fernando Martinez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: 论文提出SCORER框架，通过博弈论动态结构化感知与控制网络的交互，提升深度强化学习的性能。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏奖励信号下学习有效特征的挑战，避免复杂辅助目标或完全解耦带来的设计复杂性。

Method: 引入Stackelberg博弈模型，感知网络（领导者）学习特征以优化控制网络（追随者）的Bellman误差，采用双时间尺度算法近似均衡。

Result: 在标准DQN变体和基准任务中，SCORER提高了样本效率和最终性能。

Conclusion: 通过结构化感知与控制动态的算法设计，无需复杂辅助目标或架构即可实现性能提升。

Abstract: Integrated, end-to-end learning of representations and policies remains a
cornerstone of deep reinforcement learning (RL). However, to address the
challenge of learning effective features from a sparse reward signal, recent
trends have shifted towards adding complex auxiliary objectives or fully
decoupling the two processes, often at the cost of increased design complexity.
This work proposes an alternative to both decoupling and naive end-to-end
learning, arguing that performance can be significantly improved by structuring
the interaction between distinct perception and control networks with a
principled, game-theoretic dynamic. We formalize this dynamic by introducing
the Stackelberg Coupled Representation and Reinforcement Learning (SCORER)
framework, which models the interaction between perception and control as a
Stackelberg game. The perception network (leader) strategically learns features
to benefit the control network (follower), whose own objective is to minimize
its Bellman error. We approximate the game's equilibrium with a practical
two-timescale algorithm. Applied to standard DQN variants on benchmark tasks,
SCORER improves sample efficiency and final performance. Our results show that
performance gains can be achieved through principled algorithmic design of the
perception-control dynamic, without requiring complex auxiliary objectives or
architectures.

</details>


### [177] [Towards Unveiling Predictive Uncertainty Vulnerabilities in the Context of the Right to Be Forgotten](https://arxiv.org/abs/2508.07458)
*Wei Qian,Chenxu Zhao,Yangyi Li,Wenqian Ye,Mengdi Huai*

Main category: cs.LG

TL;DR: 本文提出了一种针对预测不确定性的恶意遗忘攻击，旨在操纵特定预测不确定性结果，并通过实验验证其比传统攻击更有效。


<details>
  <summary>Details</summary>
Motivation: 随着对遗忘权的需求增加，机器遗忘被广泛研究，但预测不确定性在恶意遗忘攻击下的脆弱性尚未被探索。

Method: 设计了新的优化框架进行恶意遗忘攻击，并在黑盒场景下进行了广泛实验。

Result: 实验表明，该攻击在操纵预测不确定性方面比传统攻击更有效，且现有防御措施对其无效。

Conclusion: 首次揭示了预测不确定性在恶意遗忘攻击下的脆弱性，并提出了一种新型攻击方法。

Abstract: Currently, various uncertainty quantification methods have been proposed to
provide certainty and probability estimates for deep learning models' label
predictions. Meanwhile, with the growing demand for the right to be forgotten,
machine unlearning has been extensively studied as a means to remove the impact
of requested sensitive data from a pre-trained model without retraining the
model from scratch. However, the vulnerabilities of such generated predictive
uncertainties with regard to dedicated malicious unlearning attacks remain
unexplored. To bridge this gap, for the first time, we propose a new class of
malicious unlearning attacks against predictive uncertainties, where the
adversary aims to cause the desired manipulations of specific predictive
uncertainty results. We also design novel optimization frameworks for our
attacks and conduct extensive experiments, including black-box scenarios.
Notably, our extensive experiments show that our attacks are more effective in
manipulating predictive uncertainties than traditional attacks that focus on
label misclassifications, and existing defenses against conventional attacks
are ineffective against our attacks.

</details>


### [178] [MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification](https://arxiv.org/abs/2508.07465)
*Tiantian Yang,Zhiqian Chen*

Main category: cs.LG

TL;DR: MOTGNN是一种新型的多组学数据整合框架，通过XGBoost和GNN实现疾病分类，显著提升预测性能并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 多组学数据整合能全面揭示疾病机制，但其高维度和复杂交互为建模带来挑战。

Method: 使用XGBoost构建组学特异性图，结合GNN进行层次表示学习，并通过深度网络整合跨组学数据。

Result: 在三个真实疾病数据集上，MOTGNN在准确性、ROC-AUC和F1分数上优于基线5-10%，且对类别不平衡鲁棒。

Conclusion: MOTGNN在多组学疾病建模中提升了预测准确性和可解释性。

Abstract: Integrating multi-omics data, such as DNA methylation, mRNA expression, and
microRNA (miRNA) expression, offers a comprehensive view of the biological
mechanisms underlying disease. However, the high dimensionality and complex
interactions among omics layers present major challenges for predictive
modeling. We propose Multi-Omics integration with Tree-generated Graph Neural
Network (MOTGNN), a novel and interpretable framework for binary disease
classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform
omics-specific supervised graph construction, followed by modality-specific
Graph Neural Networks (GNNs) for hierarchical representation learning, and a
deep feedforward network for cross-omics integration. On three real-world
disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in
accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance
(e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains
computational efficiency through sparse graphs (2.1-2.8 edges per node) and
provides built-in interpretability, revealing both top-ranked biomarkers and
the relative contributions of each omics modality. These results highlight
MOTGNN's potential to improve both predictive accuracy and interpretability in
multi-omics disease modeling.

</details>


### [179] [Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications](https://arxiv.org/abs/2508.07473)
*Zijian Liu*

Main category: cs.LG

TL;DR: 该论文研究了在线凸优化（OCO）中梯度估计具有重尾分布时的性能，证明了经典算法在无需修改的情况下仍能实现最优遗憾界，并拓展了应用场景。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于现有OCO算法在梯度估计具有有限方差时的表现良好，但在重尾分布（即梯度估计仅具有有限p阶中心矩，p∈(1,2]）时的性能尚不明确。

Method: 论文分析了经典OCO算法（如在线梯度下降）在重尾分布下的表现，未对算法进行任何修改，仅基于标准有界域假设。

Result: 结果表明，这些算法在重尾分布下仍能实现完全最优的遗憾界，且无需额外操作（如梯度裁剪）。此外，研究还拓展到非光滑非凸优化和乐观算法等场景。

Conclusion: 结论指出，OCO在重尾分布下无需额外操作即可有效解决，且结果具有广泛的应用价值，特别是在非光滑非凸优化中首次证明了无需梯度裁剪的收敛性。

Abstract: In Online Convex Optimization (OCO), when the stochastic gradient has a
finite variance, many algorithms provably work and guarantee a sublinear
regret. However, limited results are known if the gradient estimate has a heavy
tail, i.e., the stochastic gradient only admits a finite $\mathsf{p}$-th
central moment for some $\mathsf{p}\in\left(1,2\right]$. Motivated by it, this
work examines different old algorithms for OCO (e.g., Online Gradient Descent)
in the more challenging heavy-tailed setting. Under the standard bounded domain
assumption, we establish new regrets for these classical methods without any
algorithmic modification. Remarkably, these regret bounds are fully optimal in
all parameters (can be achieved even without knowing $\mathsf{p}$), suggesting
that OCO with heavy tails can be solved effectively without any extra operation
(e.g., gradient clipping). Our new results have several applications. A
particularly interesting one is the first provable convergence result for
nonsmooth nonconvex optimization under heavy-tailed noise without gradient
clipping. Furthermore, we explore broader settings (e.g., smooth OCO) and
extend our ideas to optimistic algorithms to handle different cases
simultaneously.

</details>


### [180] [N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time Series Forecasting](https://arxiv.org/abs/2508.07490)
*Ricardo Matos,Luis Roque,Vitor Cerqueira*

Main category: cs.LG

TL;DR: N-BEATS-MOE是N-BEATS的扩展，通过引入Mixture-of-Experts层和动态块加权策略，提升了时间序列预测的适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法（如N-BEATS）在时间序列预测中表现优异，但仍有改进空间，尤其是在适应性和可解释性方面。

Method: N-BEATS-MOE基于N-BEATS框架，引入Mixture-of-Experts层和动态块加权策略，通过门控网络优化模型对不同时间序列特征的适应性。

Result: 在12个基准数据集上的实验表明，N-BEATS-MOE在异构时间序列上表现尤为突出，性能优于其他方法。

Conclusion: N-BEATS-MOE通过动态门控机制提升了模型的适应性和可解释性，为时间序列预测提供了新的解决方案。

Abstract: Deep learning approaches are increasingly relevant for time series
forecasting tasks. Methods such as N-BEATS, which is built on stacks of
multilayer perceptrons (MLPs) blocks, have achieved state-of-the-art results on
benchmark datasets and competitions. N-BEATS is also more interpretable
relative to other deep learning approaches, as it decomposes forecasts into
different time series components, such as trend and seasonality. In this work,
we present N-BEATS-MOE, an extension of N-BEATS based on a Mixture-of-Experts
(MoE) layer. N-BEATS-MOE employs a dynamic block weighting strategy based on a
gating network which allows the model to better adapt to the characteristics of
each time series. We also hypothesize that the gating mechanism provides
additional interpretability by identifying which expert is most relevant for
each series. We evaluate our method across 12 benchmark datasets against
several approaches, achieving consistent improvements on several datasets,
especially those composed of heterogeneous time series.

</details>


### [181] [Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach](https://arxiv.org/abs/2508.07505)
*Yueyang Quan,Chang Wang,Shengjie Zhai,Minghong Fang,Zhuqing Liu*

Main category: cs.LG

TL;DR: 提出了一种名为DPMixSGD的隐私保护算法，用于非凸分散式最小-最大优化，结合差分隐私和STORM算法，确保隐私保护的同时不影响收敛性能。


<details>
  <summary>Details</summary>
Motivation: 分散式最小-最大优化中，模型更新共享可能暴露敏感数据，差分隐私虽能保护隐私，但噪声可能影响收敛性能。

Method: 基于STORM算法，设计DPMixSGD算法，在本地梯度中添加噪声，并理论证明其不影响收敛。

Result: 实验验证了算法的有效性，噪声未显著影响收敛性能，同时提供隐私保障。

Conclusion: DPMixSGD在隐私保护和性能之间取得了平衡，适用于复杂非凸场景。

Abstract: Decentralized min-max optimization allows multi-agent systems to
collaboratively solve global min-max optimization problems by facilitating the
exchange of model updates among neighboring agents, eliminating the need for a
central server. However, sharing model updates in such systems carry a risk of
exposing sensitive data to inference attacks, raising significant privacy
concerns. To mitigate these privacy risks, differential privacy (DP) has become
a widely adopted technique for safeguarding individual data. Despite its
advantages, implementing DP in decentralized min-max optimization poses
challenges, as the added noise can hinder convergence, particularly in
non-convex scenarios with complex agent interactions in min-max optimization
problems. In this work, we propose an algorithm called DPMixSGD (Differential
Private Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving
algorithm specifically designed for non-convex decentralized min-max
optimization. Our method builds on the state-of-the-art STORM-based algorithm,
one of the fastest decentralized min-max solutions. We rigorously prove that
the noise added to local gradients does not significantly compromise
convergence performance, and we provide theoretical bounds to ensure privacy
guarantees. To validate our theoretical findings, we conduct extensive
experiments across various tasks and models, demonstrating the effectiveness of
our approach.

</details>


### [182] [FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction](https://arxiv.org/abs/2508.07518)
*Sichen Zhao,Wei Shao,Jeffrey Chan,Ziqi Xu,Flora Salim*

Main category: cs.LG

TL;DR: 论文提出了一种基于解耦表示学习的新框架FairDRL-ST，用于解决时空预测中的公平性问题，特别关注移动需求预测。


<details>
  <summary>Details</summary>
Motivation: 由于时空预测中的偏见可能加剧社会经济不平等，影响公共服务的伦理部署，因此需要一种兼顾公平性和性能的方法。

Method: 通过对抗学习和解耦表示学习，框架能够分离敏感信息属性，以无监督方式实现公平性。

Result: 在真实城市移动数据集上验证，该框架能缩小公平性差距，同时保持与现有公平感知方法相当的预测性能。

Conclusion: FairDRL-ST为时空预测中的公平性问题提供了一种有效且性能损失小的解决方案。

Abstract: As deep spatio-temporal neural networks are increasingly utilised in urban
computing contexts, the deployment of such methods can have a direct impact on
users of critical urban infrastructure, such as public transport, emergency
services, and traffic management systems. While many spatio-temporal methods
focus on improving accuracy, fairness has recently gained attention due to
growing evidence that biased predictions in spatio-temporal applications can
disproportionately disadvantage certain demographic or geographic groups,
thereby reinforcing existing socioeconomic inequalities and undermining the
ethical deployment of AI in public services. In this paper, we propose a novel
framework, FairDRL-ST, based on disentangled representation learning, to
address fairness concerns in spatio-temporal prediction, with a particular
focus on mobility demand forecasting. By leveraging adversarial learning and
disentangled representation learning, our framework learns to separate
attributes that contain sensitive information. Unlike existing methods that
enforce fairness through supervised learning, which may lead to
overcompensation and degraded performance, our framework achieves fairness in
an unsupervised manner with minimal performance loss. We apply our framework to
real-world urban mobility datasets and demonstrate its ability to close
fairness gaps while delivering competitive predictive performance compared to
state-of-the-art fairness-aware methods.

</details>


### [183] [Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning](https://arxiv.org/abs/2508.07536)
*Tasfiq E. Alam,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的多模态CNN模型，结合振动和电机电流信号，通过物理特征提取和损失函数优化，显著提升了轴承故障分类的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决变工况下轴承故障分类的准确性和可解释性问题，克服领域偏移对模型性能的影响。

Method: 采用多模态CNN架构，结合物理特征提取分支和物理信息损失函数，评估了三种迁移学习策略。

Result: 在Paderborn和KAIST数据集上表现优异，最高准确率达98%，统计显著改进（p < 0.01）。

Conclusion: 物理信息与数据驱动结合的方法在故障诊断中具有鲁棒性和泛化能力，适用于工业应用。

Abstract: Accurate and interpretable bearing fault classification is critical for
ensuring the reliability of rotating machinery, particularly under variable
operating conditions where domain shifts can significantly degrade model
performance. This study proposes a physics-informed multimodal convolutional
neural network (CNN) with a late fusion architecture, integrating vibration and
motor current signals alongside a dedicated physics-based feature extraction
branch. The model incorporates a novel physics-informed loss function that
penalizes physically implausible predictions based on characteristic bearing
fault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass Frequency
Inner (BPFI) - derived from bearing geometry and shaft speed. Comprehensive
experiments on the Paderborn University dataset demonstrate that the proposed
physics-informed approach consistently outperforms a non-physics-informed
baseline, achieving higher accuracy, reduced false classifications, and
improved robustness across multiple data splits. To address performance
degradation under unseen operating conditions, three transfer learning (TL)
strategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy
(LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LAS
yields the best generalization, with additional performance gains when combined
with physics-informed modeling. Validation on the KAIST bearing dataset
confirms the framework's cross-dataset applicability, achieving up to 98
percent accuracy. Statistical hypothesis testing further verifies significant
improvements (p < 0.01) in classification performance. The proposed framework
demonstrates the potential of integrating domain knowledge with data-driven
learning to achieve robust, interpretable, and generalizable fault diagnosis
for real-world industrial applications.

</details>


### [184] [Multimodal Remote Inference](https://arxiv.org/abs/2508.07555)
*Keyuan Zhang,Yin Sun,Bo Ji*

Main category: cs.LG

TL;DR: 研究多模态远程推理系统中的调度问题，提出基于索引的阈值策略以最小化推理误差，证明其最优性，并显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于网络资源有限，实时传输所有模态的特征不可行，而特征的新鲜度对推理任务至关重要。因此，研究如何调度模态以最小化推理误差。

Method: 提出一种基于索引的阈值策略，调度器在当前模态的索引函数超过阈值时切换模态。证明该策略在非单调、非加性AoI函数和异构传输时间下的最优性。

Result: 数值结果显示，该策略比轮询和随机策略减少推理误差高达55%。

Conclusion: 通过优化面向任务的AoI函数，可显著提高远程推理的准确性。

Abstract: We consider a remote inference system with multiple modalities, where a
multimodal machine learning (ML) model performs real-time inference using
features collected from remote sensors. As sensor observations may change
dynamically over time, fresh features are critical for inference tasks.
However, timely delivering features from all modalities is often infeasible due
to limited network resources. To this end, we study a two-modality scheduling
problem to minimize the ML model's inference error, which is expressed as a
penalty function of AoI for both modalities. We develop an index-based
threshold policy and prove its optimality. Specifically, the scheduler switches
modalities when the current modality's index function exceeds a threshold. We
show that the two modalities share the same threshold, and both the index
functions and the threshold can be computed efficiently. The optimality of our
policy holds for (i) general AoI functions that are \emph{non-monotonic} and
\emph{non-additive} and (ii) \emph{heterogeneous} transmission times. Numerical
results show that our policy reduces inference error by up to 55% compared to
round-robin and uniform random policies, which are oblivious to the AoI-based
inference error function. Our results shed light on how to improve remote
inference accuracy by optimizing task-oriented AoI functions.

</details>


### [185] [Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning](https://arxiv.org/abs/2508.07556)
*Stephan Rabanser*

Main category: cs.LG

TL;DR: 该论文研究了如何通过不确定性估计提升机器学习的安全性和可信度，提出了一种轻量级的后验弃权方法，分析了隐私噪声对不确定性的影响，并设计了防御机制。


<details>
  <summary>Details</summary>
Motivation: 在机器学习系统应用于高风险领域时，可靠性和安全性至关重要。论文旨在通过不确定性估计提升模型的信任度，特别是在选择性预测中。

Method: 通过利用模型的训练轨迹中的不确定性信号，提出了一种轻量级的后验弃权方法，并研究了差分隐私对不确定性的影响。此外，还分析了选择性分类差距的误差来源，并设计了防御机制。

Result: 提出的方法在不改变模型架构或损失的情况下实现了最优的选择性预测性能，并在隐私噪声下保持鲁棒性。同时，揭示了校准无法解决排序误差的问题。

Conclusion: 论文通过改进、评估和保护不确定性估计，推动了可靠机器学习的发展，使模型不仅能做出准确预测，还能知道何时弃权。

Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes
domains where reliability is paramount. This thesis investigates how
uncertainty estimation can enhance the safety and trustworthiness of ML,
focusing on selective prediction -- where models abstain when confidence is
low.
  We first show that a model's training trajectory contains rich uncertainty
signals that can be exploited without altering its architecture or loss. By
ensembling predictions from intermediate checkpoints, we propose a lightweight,
post-hoc abstention method that works across tasks, avoids the cost of deep
ensembles, and achieves state-of-the-art selective prediction performance.
Crucially, this approach is fully compatible with differential privacy (DP),
allowing us to study how privacy noise affects uncertainty quality. We find
that while many methods degrade under DP, our trajectory-based approach remains
robust, and we introduce a framework for isolating the privacy-uncertainty
trade-off. Next, we then develop a finite-sample decomposition of the selective
classification gap -- the deviation from the oracle accuracy-coverage curve --
identifying five interpretable error sources and clarifying which interventions
can close the gap. This explains why calibration alone cannot fix ranking
errors, motivating methods that improve uncertainty ordering. Finally, we show
that uncertainty signals can be adversarially manipulated to hide errors or
deny service while maintaining high accuracy, and we design defenses combining
calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating,
and safeguarding uncertainty estimation, enabling models that not only make
accurate predictions -- but also know when to say "I do not know".

</details>


### [186] [Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression](https://arxiv.org/abs/2508.07571)
*Xingwu Chen,Miao Lu,Beining Wu,Difan Zou*

Main category: cs.LG

TL;DR: 论文探讨了在语言模型推理中增加测试时计算（如生成更多中间思考或采样多个候选答案）对性能提升的作用，并通过理论和实验结合的方式分析了随机性和采样的影响。


<details>
  <summary>Details</summary>
Motivation: 弥合实际语言模型推理与理论分析之间的差距，探索随机性和采样在推理中的作用。

Method: 采用基于噪声注入和二进制系数采样的框架，模拟语言模型解码过程，重点研究上下文线性回归问题。

Result: 理论和实证结果表明，该框架能有效分析广泛采用的推理技术，为理解实际语言模型的推理行为提供新视角。

Conclusion: 研究为语言模型推理行为的理论分析提供了新工具，展示了随机性和采样在提升性能中的潜力。

Abstract: Using more test-time computation during language model inference, such as
generating more intermediate thoughts or sampling multiple candidate answers,
has proven effective in significantly improving model performance. This paper
takes an initial step toward bridging the gap between practical language model
inference and theoretical transformer analysis by incorporating randomness and
sampling. We focus on in-context linear regression with continuous/binary
coefficients, where our framework simulates language model decoding through
noise injection and binary coefficient sampling. Through this framework, we
provide detailed analyses of widely adopted inference techniques. Supported by
empirical results, our theoretical framework and analysis demonstrate the
potential for offering new insights into understanding inference behaviors in
real-world language models.

</details>


### [187] [When and how can inexact generative models still sample from the data manifold?](https://arxiv.org/abs/2508.07581)
*Nisha Chandramoorthy,Adriaan de Clercq*

Main category: cs.LG

TL;DR: 论文研究了生成模型中学习误差导致样本沿数据分布支撑移动而非偏离的现象，揭示了其动力学机制，并提出了支持鲁棒性的充分条件。


<details>
  <summary>Details</summary>
Motivation: 观察生成模型中学习误差导致样本沿数据分布支撑移动而非偏离的现象，探究其动力学机制。

Method: 采用动力学系统方法分析生成过程，通过扰动分析揭示概率流的影响，并研究Lyapunov向量与数据流形切空间的对齐条件。

Result: 发现学习误差仅导致预测密度在数据流形上变化，且对齐条件能高效计算并自动估计数据流形的切丛。

Conclusion: 研究为生成模型提供了理论保证，适用于多种动力学生成模型和目标分布。

Abstract: A curious phenomenon observed in some dynamical generative models is the
following: despite learning errors in the score function or the drift vector
field, the generated samples appear to shift \emph{along} the support of the
data distribution but not \emph{away} from it. In this work, we investigate
this phenomenon of \emph{robustness of the support} by taking a dynamical
systems approach on the generating stochastic/deterministic process. Our
perturbation analysis of the probability flow reveals that infinitesimal
learning errors cause the predicted density to be different from the target
density only on the data manifold for a wide class of generative models.
Further, what is the dynamical mechanism that leads to the robustness of the
support? We show that the alignment of the top Lyapunov vectors (most sensitive
infinitesimal perturbation directions) with the tangent spaces along the
boundary of the data manifold leads to robustness and prove a sufficient
condition on the dynamics of the generating process to achieve this alignment.
Moreover, the alignment condition is efficient to compute and, in practice, for
robust generative models, automatically leads to accurate estimates of the
tangent bundle of the data manifold. Using a finite-time linear perturbation
analysis on samples paths as well as probability flows, our work complements
and extends existing works on obtaining theoretical guarantees for generative
models from a stochastic analysis, statistical learning and uncertainty
quantification points of view. Our results apply across different dynamical
generative models, such as conditional flow-matching and score-based generative
models, and for different target distributions that may or may not satisfy the
manifold hypothesis.

</details>


### [188] [Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization](https://arxiv.org/abs/2508.07629)
*Zhenpeng Su,Leiyu Pan,Xue Bai,Dening Liu,Guanting Dong,Jiaming Huang,Wenping Hu,Guorui Zhou*

Main category: cs.LG

TL;DR: Klear-Reasoner是一个具有长推理能力的模型，通过详细的数据准备、长链思维监督微调和强化学习，实现了在数学和编程领域的卓越表现。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型的高性能复现因训练细节不完整而存在困难，本文旨在提供完整的训练流程分析和改进方法。

Method: 采用长链思维监督微调（long CoT SFT）和强化学习（RL），并提出Gradient-Preserving clipping Policy Optimization（GPPO）解决RL中的裁剪问题。

Result: 在AIME 2024和2025、LiveCodeBench V5和V6上分别取得90.5%、83.2%、66.0%和58.1%的高分。

Conclusion: Klear-Reasoner通过高质量数据、改进的RL机制和详细实验分析，显著提升了推理能力。

Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that
demonstrates careful deliberation during problem solving, achieving outstanding
performance across multiple benchmarks. Although there are already many
excellent works related to inference models in the current community, there are
still many problems with reproducing high-performance inference models due to
incomplete disclosure of training details. This report provides an in-depth
analysis of the reasoning model, covering the entire post-training workflow
from data preparation and long Chain-of-Thought supervised fine-tuning (long
CoT SFT) to reinforcement learning (RL), along with detailed ablation studies
for each experimental component. For SFT data, our experiments show that a
small number of high-quality data sources are more effective than a large
number of diverse data sources, and that difficult samples can achieve better
results without accuracy filtering. In addition, we investigate two key issues
with current clipping mechanisms in RL: Clipping suppresses critical
exploration signals and ignores suboptimal trajectories. To address these
challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)
that gently backpropagates gradients from clipped tokens. GPPO not only
enhances the model's exploration capacity but also improves its efficiency in
learning from negative samples. Klear-Reasoner exhibits exceptional reasoning
abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\%
on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.

</details>


### [189] [Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo](https://arxiv.org/abs/2508.07631)
*Advait Parulekar,Litu Rout,Karthikeyan Shanmugam,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 论文研究了基于分数的生成模型中的后验采样问题，提出了在多项式时间内近似采样后验分布的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管后验采样在KL散度下是难以处理的，但实际应用中（如图像超分辨率、风格化等）的算法却取得了成功。论文旨在探索一种更通用的方法，而不依赖于严格的分布假设或限制条件。

Method: 将后验采样视为一个“倾斜”问题，通过最小化假设，从噪声先验的后验分布中采样，同时确保样本与测量和先验一致。

Result: 证明了可以在多项式时间内采样一个分布，该分布在KL散度上接近噪声先验的后验，在Fisher散度上接近真实后验。

Conclusion: 这是首次在多项式时间内实现近似后验采样的正式结果，为相关任务提供了理论支持。

Abstract: We study the problem of posterior sampling in the context of score based
generative models. We have a trained score network for a prior $p(x)$, a
measurement model $p(y|x)$, and are tasked with sampling from the posterior
$p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case)
under well-accepted computational hardness assumptions. Despite this, popular
algorithms for tasks such as image super-resolution, stylization, and
reconstruction enjoy empirical success. Rather than establishing distributional
assumptions or restricted settings under which exact posterior sampling is
tractable, we view this as a more general "tilting" problem of biasing a
distribution towards a measurement. Under minimal assumptions, we show that one
can tractably sample from a distribution that is simultaneously close to the
posterior of a noised prior in KL divergence and the true posterior in Fisher
divergence. Intuitively, this combination ensures that the resulting sample is
consistent with both the measurement and the prior. To the best of our
knowledge these are the first formal results for (approximate) posterior
sampling in polynomial time.

</details>


### [190] [Attribution Explanations for Deep Neural Networks: A Theoretical Perspective](https://arxiv.org/abs/2508.07636)
*Huiqi Deng,Hongbin Pei,Quanshi Zhang,Mengnan Du*

Main category: cs.LG

TL;DR: 论文探讨了深度神经网络（DNN）归因解释方法的可靠性问题，总结了三个核心挑战，并提出了三个关键方向以解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 归因解释方法在解释DNN时存在可靠性问题，缺乏统一的理论基础和评估标准，影响了其实际应用价值。

Method: 通过理论统一、理论基础和理论评估三个方向，系统比较和验证归因方法的忠实性。

Result: 总结了近期理论进展，为归因方法的比较、选择和设计提供了理论支持。

Conclusion: 论文为归因解释方法的理论研究提供了方向，并指出了未来可能的开放性问题。

Abstract: Attribution explanation is a typical approach for explaining deep neural
networks (DNNs), inferring an importance or contribution score for each input
variable to the final output. In recent years, numerous attribution methods
have been developed to explain DNNs. However, a persistent concern remains
unresolved, i.e., whether and which attribution methods faithfully reflect the
actual contribution of input variables to the decision-making process. The
faithfulness issue undermines the reliability and practical utility of
attribution explanations. We argue that these concerns stem from three core
challenges. First, difficulties arise in comparing attribution methods due to
their unstructured heterogeneity, differences in heuristics, formulations, and
implementations that lack a unified organization. Second, most methods lack
solid theoretical underpinnings, with their rationales remaining absent,
ambiguous, or unverified. Third, empirically evaluating faithfulness is
challenging without ground truth. Recent theoretical advances provide a
promising way to tackle these challenges, attracting increasing attention. We
summarize these developments, with emphasis on three key directions: (i)
Theoretical unification, which uncovers commonalities and differences among
methods, enabling systematic comparisons; (ii) Theoretical rationale,
clarifying the foundations of existing methods; (iii) Theoretical evaluation,
rigorously proving whether methods satisfy faithfulness principles. Beyond a
comprehensive review, we provide insights into how these studies help deepen
theoretical understanding, inform method selection, and inspire new attribution
methods. We conclude with a discussion of promising open problems for further
work.

</details>


### [191] [Extracting Complex Topology from Multivariate Functional Approximation: Contours, Jacobi Sets, and Ridge-Valley Graphs](https://arxiv.org/abs/2508.07637)
*Guanqun Ma,David Lenz,Hanqi Guo,Tom Peterka,Bei Wang*

Main category: cs.LG

TL;DR: 提出了一种直接从连续隐式模型（MFA）中提取复杂拓扑特征（如轮廓、雅可比集和脊谷图）的框架，无需离散化。


<details>
  <summary>Details</summary>
Motivation: 连续隐式模型（如MFA）为科学数据的存储、传输和分析提供了新视角，但缺乏直接提取拓扑特征的方法。

Method: 基于MFA模型，直接提取拓扑特征，支持函数值和高阶导数查询。

Result: 实现了从连续隐式模型中直接提取复杂拓扑特征，适用于任何支持相关查询的连续模型。

Conclusion: 为连续隐式模型的拓扑数据分析和可视化奠定了基础。

Abstract: Implicit continuous models, such as functional models and implicit neural
networks, are an increasingly popular method for replacing discrete data
representations with continuous, high-order, and differentiable surrogates.
These models offer new perspectives on the storage, transfer, and analysis of
scientific data. In this paper, we introduce the first framework to directly
extract complex topological features -- contours, Jacobi sets, and ridge-valley
graphs -- from a type of continuous implicit model known as multivariate
functional approximation (MFA). MFA replaces discrete data with continuous
piecewise smooth functions. Given an MFA model as the input, our approach
enables direct extraction of complex topological features from the model,
without reverting to a discrete representation of the model. Our work is easily
generalizable to any continuous implicit model that supports the queries of
function values and high-order derivatives. Our work establishes the building
blocks for performing topological data analysis and visualization on implicit
continuous models.

</details>


### [192] [Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals](https://arxiv.org/abs/2508.07638)
*Jia Zhang,Yao Liu,Chen-Xi Zhang,Yi Liu,Yi-Xuan Jin,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为DMPO的方法，通过量化偏好分歧（PD）并选择高共识数据，优化LLM对齐，显著提升了效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如DPO）在处理细粒度偏好数据时存在噪声和冲突问题，需要一种更可靠的数据选择策略。

Method: 提出DMPO目标，利用PD项量化偏好冲突，并基于此选择高共识数据进行训练。

Result: 在UltraFeedback数据集上，DMPO相对标准方法提升了10%以上，同时提高了训练效率。

Conclusion: DMPO通过数据选择策略有效解决了偏好冲突问题，为LLM对齐提供了新思路。

Abstract: Aligning Large Language Models (LLMs) with diverse human values requires
moving beyond a single holistic "better-than" preference criterion. While
collecting fine-grained, aspect-specific preference data is more reliable and
scalable, existing methods like Direct Preference Optimization (DPO) struggle
with the severe noise and conflicts inherent in such aggregated datasets. In
this paper, we tackle this challenge from a data-centric perspective. We first
derive the Direct Multi-Preference Optimization (DMPO) objective, and uncover a
key Preference Divergence (PD) term that quantifies inter-aspect preference
conflicts. Instead of using this term for direct optimization, we leverage it
to formulate a novel, theoretically-grounded data selection principle. Our
principle advocates for selecting a subset of high-consensus data-identified by
the most negative PD values-for efficient DPO training. We prove the optimality
of this strategy by analyzing the loss bounds of the DMPO objective in the
selection problem. To operationalize our approach, we introduce practical
methods of PD term estimation and length bias mitigation, thereby proposing our
PD selection method. Evaluation on the UltraFeedback dataset with three varying
conflict levels shows that our simple yet effective strategy achieves over 10%
relative improvement against both the standard holistic preference and a
stronger oracle using aggregated preference signals, all while boosting
training efficiency and obviating the need for intractable holistic preference
annotating, unlocking the potential of robust LLM alignment via fine-grained
preference signals.

</details>


### [193] [Multi-Turn Jailbreaks Are Simpler Than They Seem](https://arxiv.org/abs/2508.07646)
*Xiaoxue Yang,Jaeha Lee,Anna-Katharina Dick,Jasper Timm,Fei Xie,Diogo Cruz*

Main category: cs.LG

TL;DR: 研究发现多轮越狱攻击的成功率与单轮攻击的多次重采样相当，且攻击成功率在相似模型间相关，推理能力越强的模型越容易被攻击。


<details>
  <summary>Details</summary>
Motivation: 探讨多轮越狱攻击对大型语言模型的威胁，揭示其实际复杂性与防御系统的不足。

Method: 使用StrongREJECT基准对GPT-4、Claude和Gemini等先进模型进行多轮越狱攻击的实证分析。

Result: 多轮攻击成功率与单轮攻击多次重采样相当，攻击成功率在相似模型间相关，推理能力强的模型更易被攻击。

Conclusion: 研究结果对AI安全评估和抗越狱系统设计具有重要意义，并开源了相关代码。

Abstract: While defenses against single-turn jailbreak attacks on Large Language Models
(LLMs) have improved significantly, multi-turn jailbreaks remain a persistent
vulnerability, often achieving success rates exceeding 70% against models
optimized for single-turn protection. This work presents an empirical analysis
of automated multi-turn jailbreak attacks across state-of-the-art models
including GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark.
Our findings challenge the perceived sophistication of multi-turn attacks: when
accounting for the attacker's ability to learn from how models refuse harmful
requests, multi-turn jailbreaking approaches are approximately equivalent to
simply resampling single-turn attacks multiple times. Moreover, attack success
is correlated among similar models, making it easier to jailbreak newly
released ones. Additionally, for reasoning models, we find surprisingly that
higher reasoning effort often leads to higher attack success rates. Our results
have important implications for AI safety evaluation and the design of
jailbreak-resistant systems. We release the source code at
https://github.com/diogo-cruz/multi_turn_simpler

</details>


### [194] [Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning](https://arxiv.org/abs/2508.07659)
*Hyeon-Ju Jeon,Jeon-Ho Kang,In-Hyuk Kwon,O-Joun Lee*

Main category: cs.LG

TL;DR: 该研究旨在通过时空图神经网络（STGNN）和结构学习，动态捕捉地球观测与大气状态之间的空间相关性，以提高全球大气状态估计的预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报（NWP）系统在固定网格点上预测大气状态，但观测数据位置不固定，导致空间相关性复杂且动态变化。

Method: 采用STGNN结合结构学习，通过自适应节点度控制和空间距离调节，解决结构信息丢失和过平滑问题。

Result: 在东亚真实数据上的实验表明，该方法在高大气变异性区域优于现有STGNN模型。

Conclusion: 提出的方法有效提升了大气状态估计的准确性，尤其在复杂动态空间相关性场景中表现优异。

Abstract: This study aims to discover spatial correlations between Earth observations
and atmospheric states to improve the forecasting accuracy of global
atmospheric state estimation, which are usually conducted using conventional
numerical weather prediction (NWP) systems and is the beginning of weather
forecasting. NWP systems predict future atmospheric states at fixed locations,
which are called NWP grid points, by analyzing previous atmospheric states and
newly acquired Earth observations without fixed locations. Thus, surrounding
meteorological context and the changing locations of the observations make
spatial correlations between atmospheric states and observations over time. To
handle complicated spatial correlations, which change dynamically, we employ
spatiotemporal graph neural networks (STGNNs) with structure learning. However,
structure learning has an inherent limitation that this can cause structural
information loss and over-smoothing problem by generating excessive edges. To
solve this problem, we regulate edge sampling by adaptively determining node
degrees and considering the spatial distances between NWP grid points and
observations. We validated the effectiveness of the proposed method by using
real-world atmospheric state and observation data from East Asia. Even in areas
with high atmospheric variability, the proposed method outperformed existing
STGNN models with and without structure learning.

</details>


### [195] [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662)
*Ihor Stepanov,Mykhailo Shtopko,Dmytro Vodianytskyi,Oleksandr Lukashov,Alexander Yavorskyi,Mykyta Yaroshenko*

Main category: cs.LG

TL;DR: GLiClass是一种基于GLiNER架构的新方法，用于序列分类任务，兼具高准确性和效率，适用于零样本和小样本学习场景。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统需要高效且准确的分类方法，尤其是在动态需求和零样本能力方面。现有方法如生成式LLMs和交叉编码器存在效率或灵活性不足的问题。

Method: 提出GLiClass方法，基于GLiNER架构优化序列分类任务，并采用近端策略优化（PPO）进行多标签文本分类训练。

Result: GLiClass在准确性和效率上与基于嵌入的方法相当，同时保持了零样本和小样本学习的灵活性。

Conclusion: GLiClass为解决分类任务中的效率和灵活性挑战提供了一种有效的新方法。

Abstract: Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative LLMs have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-sparse conditions or from human feedback.

</details>


### [196] [AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting](https://arxiv.org/abs/2508.07668)
*Hyobin Park,Jinwook Jung,Minseok Seo,Hyunsoo Choi,Deukjae Cho,Sekil Park,Dong-Geol Choi*

Main category: cs.LG

TL;DR: AIS-LLM框架整合时间序列AIS数据与大型语言模型，实现多任务协同处理，提升海事交通管理效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法单独处理海事任务，难以全面考虑复杂情况，需一体化解决方案。

Method: AIS-LLM包含时间序列编码器、提示编码器、跨模态对齐模块和多任务解码器，支持轨迹预测、异常检测和碰撞风险评估。

Result: 实验表明AIS-LLM在各项任务中优于现有方法，并能生成综合情境简报。

Conclusion: AIS-LLM为智能高效的海事交通管理提供了新思路。

Abstract: With the increase in maritime traffic and the mandatory implementation of the
Automatic Identification System (AIS), the importance and diversity of maritime
traffic analysis tasks based on AIS data, such as vessel trajectory prediction,
anomaly detection, and collision risk assessment, is rapidly growing. However,
existing approaches tend to address these tasks individually, making it
difficult to holistically consider complex maritime situations. To address this
limitation, we propose a novel framework, AIS-LLM, which integrates time-series
AIS data with a large language model (LLM). AIS-LLM consists of a Time-Series
Encoder for processing AIS sequences, an LLM-based Prompt Encoder, a
Cross-Modality Alignment Module for semantic alignment between time-series data
and textual prompts, and an LLM-based Multi-Task Decoder. This architecture
enables the simultaneous execution of three key tasks: trajectory prediction,
anomaly detection, and risk assessment of vessel collisions within a single
end-to-end system. Experimental results demonstrate that AIS-LLM outperforms
existing methods across individual tasks, validating its effectiveness.
Furthermore, by integratively analyzing task outputs to generate situation
summaries and briefings, AIS-LLM presents the potential for more intelligent
and efficient maritime traffic management.

</details>


### [197] [Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation](https://arxiv.org/abs/2508.07675)
*Xutong Liu,Baran Atalar,Xiangxiang Dai,Jinhang Zuo,Siwei Wang,John C. S. Lui,Wei Chen,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 论文提出了一种基于学习的原则性框架，用于解决语义缓存中未知查询和成本分布下的缓存淘汰问题，并开发了高效的算法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的高推理成本带来了可扩展性和可持续性挑战，传统缓存方法无法充分利用语义相似性，且现有语义缓存方法缺乏理论基础和适应性。

Method: 提出了一个基于学习的框架，包括离线优化和在线学习两种问题变体，并开发了高效的算法。

Result: 在合成数据集上的评估表明，所提算法性能优于或匹配基线方法。

Conclusion: 该框架为语义缓存提供了理论基础和实际解决方案，能够适应现实世界的不确定性。

Abstract: Large Language Models (LLMs) are revolutionizing how users interact with
information systems, yet their high inference cost poses serious scalability
and sustainability challenges. Caching inference responses, allowing them to be
retrieved without another forward pass through the LLM, has emerged as one
possible solution. Traditional exact-match caching, however, overlooks the
semantic similarity between queries, leading to unnecessary recomputation.
Semantic caching addresses this by retrieving responses based on semantic
similarity, but introduces a fundamentally different cache eviction problem:
one must account for mismatch costs between incoming queries and cached
responses. Moreover, key system parameters, such as query arrival probabilities
and serving costs, are often unknown and must be learned over time. Existing
semantic caching methods are largely ad-hoc, lacking theoretical foundations
and unable to adapt to real-world uncertainty. In this paper, we present a
principled, learning-based framework for semantic cache eviction under unknown
query and cost distributions. We formulate both offline optimization and online
learning variants of the problem, and develop provably efficient algorithms
with state-of-the-art guarantees. We also evaluate our framework on a synthetic
dataset, showing that our proposed algorithms perform matching or superior
performance compared with baselines.

</details>


### [198] [Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks](https://arxiv.org/abs/2508.07676)
*Chenchen Lin,Xuehe Wang*

Main category: cs.LG

TL;DR: 提出了一种社交感知的联邦学习隐私保护机制，通过多跳传播模型量化间接隐私泄露，采用Stackelberg博弈优化激励策略，提升客户端效用并降低服务器成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中社交网络连接带来的隐私外部性问题，即客户隐私损失不仅取决于自身策略，还受他人决策影响。

Method: 提出多跳传播模型量化隐私泄露，采用两阶段Stackelberg博弈优化激励策略，引入均值场估计器解决信息不对称问题。

Result: 实验证明该方法显著提升客户端效用、降低服务器成本，同时保持模型性能，优于社交无关基线和其他考虑社交外部性的方法。

Conclusion: 该机制在客户端激励视角下实现近似最优社会福利，为联邦学习中的隐私保护提供了有效解决方案。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, thereby enhancing privacy and
facilitating collaboration among clients connected via social networks.
However, these social connections introduce privacy externalities: a client's
privacy loss depends not only on its privacy protection strategy but also on
the privacy decisions of others, propagated through the network via multi-hop
interactions. In this work, we propose a socially-aware privacy-preserving FL
mechanism that systematically quantifies indirect privacy leakage through a
multi-hop propagation model. We formulate the server-client interaction as a
two-stage Stackelberg game, where the server, as the leader, optimizes
incentive policies, and clients, as followers, strategically select their
privacy budgets, which determine their privacy-preserving levels by controlling
the magnitude of added noise. To mitigate information asymmetry in networked
privacy estimation, we introduce a mean-field estimator to approximate the
average external privacy risk. We theoretically prove the existence and
convergence of the fixed point of the mean-field estimator and derive
closed-form expressions for the Stackelberg Nash Equilibrium. Despite being
designed from a client-centric incentive perspective, our mechanism achieves
approximately-optimal social welfare, as revealed by Price of Anarchy (PoA)
analysis. Experiments on diverse datasets demonstrate that our approach
significantly improves client utilities and reduces server costs while
maintaining model performance, outperforming both Social-Agnostic (SA)
baselines and methods that account for social externalities.

</details>


### [199] [MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation](https://arxiv.org/abs/2508.07681)
*Yooseok Lim,ByoungJun Jeon,Seong-A Park,Jisoo Lee,Sae Won Choi,Chang Wook Jeong,Ho-Geol Ryu,Hongyeol Lee,Hyun-Lim Yang*

Main category: cs.LG

TL;DR: MORE-CLEAR框架利用多模态离线强化学习和大型语言模型，通过临床笔记提升脓毒症患者状态表示，显著提高生存率和策略性能。


<details>
  <summary>Details</summary>
Motivation: 脓毒症的早期检测和优化管理至关重要，但现有强化学习方法依赖结构化数据且缺乏对患者状态的全面理解。

Method: 结合预训练大型语言模型提取临床笔记的语义信息，通过门控融合和跨模态注意力动态整合多模态数据。

Result: 在公开和私有数据集上的验证表明，MORE-CLEAR显著优于单模态强化学习方法。

Conclusion: 该框架首次将大型语言模型用于多模态离线强化学习，有望通过更全面的患者状态理解提升脓毒症管理效果。

Abstract: Sepsis, a life-threatening inflammatory response to infection, causes organ
dysfunction, making early detection and optimal management critical. Previous
reinforcement learning (RL) approaches to sepsis management rely primarily on
structured data, such as lab results or vital signs, and on a dearth of a
comprehensive understanding of the patient's condition. In this work, we
propose a Multimodal Offline REinforcement learning for Clinical notes
Leveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis
control in intensive care units. MORE-CLEAR employs pre-trained large-scale
language models (LLMs) to facilitate the extraction of rich semantic
representations from clinical notes, preserving clinical context and improving
patient state representation. Gated fusion and cross-modal attention allow
dynamic weight adjustment in the context of time and the effective integration
of multimodal data. Extensive cross-validation using two public (MIMIC-III and
MIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly
improves estimated survival rate and policy performance compared to
single-modal RL approaches. To our knowledge, this is the first to leverage LLM
capabilities within a multimodal offline RL for better state representation in
medical applications. This approach can potentially expedite the treatment and
management of sepsis by enabling reinforcement learning models to propose
enhanced actions based on a more comprehensive understanding of patient
conditions.

</details>


### [200] [Semantic-Enhanced Time-Series Forecasting via Large Language Models](https://arxiv.org/abs/2508.07697)
*Hao Liu,Chun Yang,Zhang xiaoxing,Xiaobin Zhu*

Main category: cs.LG

TL;DR: 提出了一种语义增强的大型语言模型（SE-LLM），通过嵌入时间序列的周期性和异常特征来增强语义表示，并引入插件模块优化长短期依赖建模，显著提升了时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注令牌级模态对齐，未能弥合语言知识结构与时间序列数据模式之间的模态差距，限制了语义表示能力。

Method: 提出SE-LLM，嵌入时间序列的周期性和异常特征以增强语义空间；设计插件模块优化长短期依赖建模；冻结LLM并降低令牌序列维度以减少计算消耗。

Result: 实验表明SE-LLM在性能上优于现有最先进方法。

Conclusion: SE-LLM通过语义增强和依赖建模优化，显著提升了LLM在时间序列分析中的表现。

Abstract: Time series forecasting plays a significant role in finance, energy,
meteorology, and IoT applications. Recent studies have leveraged the
generalization capabilities of large language models (LLMs) to adapt to time
series forecasting, achieving promising performance. However, existing studies
focus on token-level modal alignment, instead of bridging the intrinsic
modality gap between linguistic knowledge structures and time series data
patterns, greatly limiting the semantic representation. To address this issue,
we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent
periodicity and anomalous characteristics of time series to embed into the
semantic space to enhance the token embedding. This process enhances the
interpretability of tokens for LLMs, thereby activating the potential of LLMs
for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel
at capturing long-range dependencies but are weak at modeling short-term
anomalies in time-series data. Hence, we propose a plugin module embedded
within self-attention that models long-term and short-term dependencies to
effectively adapt LLMs to time-series analysis. Our approach freezes the LLM
and reduces the sequence dimensionality of tokens, greatly reducing
computational consumption. Experiments demonstrate the superiority performance
of our SE-LLM against the state-of-the-art (SOTA) methods.

</details>


### [201] [Energy Consumption in Parallel Neural Network Training](https://arxiv.org/abs/2508.07706)
*Philipp Huber,David Li,Juan Pedro Gutiérrez Hermosillo Muriedas,Deifilia Kieckhefen,Markus Götz,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: 论文研究了数据并行训练中GPU数量、全局批量大小和本地批量大小对能耗的影响，发现能耗与GPU小时数近似线性相关，但不同模型和硬件间差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络训练对计算资源需求的增加，能耗问题日益突出，但并行化对能耗的影响常被忽视。

Method: 通过数据并行训练ResNet50和FourCastNet模型，评估不同并行化参数对预测性能、训练时间和能耗的影响。

Result: 能耗与GPU小时数近似线性相关，但不同模型和硬件间的缩放因子差异显著，且受每GPU小时的样本数和梯度更新数影响。

Conclusion: 研究揭示了神经网络训练扩展与能耗的复杂关系，为未来可持续AI研究提供了参考。

Abstract: The increasing demand for computational resources of training neural networks
leads to a concerning growth in energy consumption. While parallelization has
enabled upscaling model and dataset sizes and accelerated training, its impact
on energy consumption is often overlooked. To close this research gap, we
conducted scaling experiments for data-parallel training of two models,
ResNet50 and FourCastNet, and evaluated the impact of parallelization
parameters, i.e., GPU count, global batch size, and local batch size, on
predictive performance, training time, and energy consumption. We show that
energy consumption scales approximately linearly with the consumed resources,
i.e., GPU hours; however, the respective scaling factor differs substantially
between distinct model trainings and hardware, and is systematically influenced
by the number of samples and gradient updates per GPU hour. Our results shed
light on the complex interplay of scaling up neural network training and can
inform future developments towards more sustainable AI research.

</details>


### [202] [Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer](https://arxiv.org/abs/2508.07710)
*Jingya Wang,Xin Deng,Wenjie Wei,Dehao Zhang,Shuai Wang,Qian Sun,Jieyuan Zhang,Hanwen Liu,Ning Xie,Malu Zhang*

Main category: cs.LG

TL;DR: 提出了一种无需训练的高性能ANN-to-SNN转换框架，通过MBE神经元高效近似非线性操作，显著降低延迟并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有ANN-to-SNN转换方法在Transformer架构中处理非线性操作效果不佳，且需额外微调，限制了效率。

Method: 引入Multi-basis Exponential Decay (MBE)神经元，采用指数衰减和多基编码方法，无需修改预训练ANN权重。

Result: 在多种任务和Transformer架构上实现近乎无损的转换精度，且延迟显著降低。

Conclusion: 为Spiking Transformers的高效部署提供了可行方案。

Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a
promising approach for constructing energy-efficient Transformer architectures.
Compared to directly trained Spiking Transformers, ANN-to-SNN conversion
methods bypass the high training costs. However, existing methods still suffer
from notable limitations, failing to effectively handle nonlinear operations in
Transformer architectures and requiring additional fine-tuning processes for
pre-trained ANNs. To address these issues, we propose a high-performance and
training-free ANN-to-SNN conversion framework tailored for Transformer
architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE)
neuron, which employs an exponential decay strategy and multi-basis encoding
method to efficiently approximate various nonlinear operations. It removes the
requirement for weight modifications in pre-trained ANNs. Extensive experiments
across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures
(ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless
conversion accuracy with significantly lower latency. This provides a promising
pathway for the efficient and scalable deployment of Spiking Transformers in
real-world applications.

</details>


### [203] [Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information](https://arxiv.org/abs/2508.07713)
*Jinghan Yang,Jiayu Weng*

Main category: cs.LG

TL;DR: 提出了一种基于互信息的数据选择框架，用于处理混合噪声场景，通过量化输入与标签之间的统计依赖性来筛选高质量数据。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络会记忆损坏的标签，而现实数据集常受标签噪声和输入噪声影响，数据质量对模型性能至关重要。

Method: 计算每个样本对整体互信息的点贡献，低贡献样本被视为噪声或错误标记实例。

Result: 在MNIST数据集上验证，该方法能有效过滤低质量样本，标签噪声下训练高互信息样本可使分类准确率提升15%。

Conclusion: 该方法对良性输入修改具有鲁棒性，能保留语义有效数据并过滤真正损坏的样本。

Abstract: Deep neural networks can memorize corrupted labels, making data quality
critical for model performance, yet real-world datasets are frequently
compromised by both label noise and input noise. This paper proposes a mutual
information-based framework for data selection under hybrid noise scenarios
that quantifies statistical dependencies between inputs and labels. We compute
each sample's pointwise contribution to the overall mutual information and find
that lower contributions indicate noisy or mislabeled instances. Empirical
validation on MNIST with different synthetic noise settings demonstrates that
the method effectively filters low-quality samples. Under label corruption,
training on high-MI samples improves classification accuracy by up to 15\%
compared to random sampling. Furthermore, the method exhibits robustness to
benign input modifications, preserving semantically valid data while filtering
truly corrupted samples.

</details>


### [204] [Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations](https://arxiv.org/abs/2508.07722)
*Pietro Talli,Federico Mason,Federico Chiariotti,Andrea Zanella*

Main category: cs.LG

TL;DR: 提出了一种名为HR3L的新架构，用于在非理想无线信道上训练远程强化学习（RL）代理，解决了传统RL在通信网络中的延迟和丢包问题。


<details>
  <summary>Details</summary>
Motivation: 传统RL需要即时感知状态变化，但在无线通信网络中，由于延迟和丢包，代理无法获取完整信息。现有解决方案计算负担大，HR3L旨在解决这一问题。

Method: HR3L包含发送端和接收端，发送端编码环境信息，接收端解码并执行动作以最大化奖励。无需交换梯度信息，降低了通信开销。

Result: 实验表明，HR3L在样本效率和适应不同通信场景（如丢包、延迟和容量限制）方面显著优于基线方法。

Conclusion: HR3L是一种高效且适应性强的远程RL训练架构，适用于非理想通信环境。

Abstract: In this work, we address the problem of training Reinforcement Learning (RL)
agents over communication networks. The RL paradigm requires the agent to
instantaneously perceive the state evolution to infer the effects of its
actions on the environment. This is impossible if the agent receives state
updates over lossy or delayed wireless systems and thus operates with partial
and intermittent information. In recent years, numerous frameworks have been
proposed to manage RL with imperfect feedback; however, they often offer
specific solutions with a substantial computational burden. To address these
limits, we propose a novel architecture, named Homomorphic Robust Remote
Reinforcement Learning (HR3L), that enables the training of remote RL agents
exchanging observations across a non-ideal wireless channel. HR3L considers two
units: the transmitter, which encodes meaningful representations of the
environment, and the receiver, which decodes these messages and performs
actions to maximize a reward signal. Importantly, HR3L does not require the
exchange of gradient information across the wireless channel, allowing for
quicker training and a lower communication overhead than state-of-the-art
solutions. Experimental results demonstrate that HR3L significantly outperforms
baseline methods in terms of sample efficiency and adapts to different
communication scenarios, including packet losses, delayed transmissions, and
capacity limitations.

</details>


### [205] [Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning](https://arxiv.org/abs/2508.07738)
*Jialu Zhou,Dianxi Shi,Shaowu Yang,Xinyu Wei,Mingyue Yang,Leqian Li,Mengzhu Wang,Chunping Qiu*

Main category: cs.LG

TL;DR: 论文提出了一种名为TRGE的方法，通过动态扩展预训练模型和任务特定专家组，解决了多域持续学习中的灾难性遗忘和前瞻遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 多域持续学习面临任务类别和分布变化的双重异质性挑战，现有方法难以同时解决灾难性遗忘和前瞻遗忘。

Method: TRGE方法采用两级路由分组混合专家机制，动态扩展预训练模型，并结合任务标识符和原型距离选择专家组。同时利用MLLM生成任务描述以识别任务标识符。

Result: 实验表明，TRGE在多种设置下优于其他先进方法，且训练参数更少。

Conclusion: TRGE通过动态路由和任务标识符识别，有效缓解了灾难性遗忘和前瞻遗忘，提升了多域持续学习的性能。

Abstract: Multi-Domain Continual Learning (MDCL) acquires knowledge from sequential
tasks with shifting class sets and distribution. Despite the
Parameter-Efficient Fine-Tuning (PEFT) methods can adapt for this dual
heterogeneity, they still suffer from catastrophic forgetting and forward
forgetting. To address these challenges, we propose a Two-Level Routing Grouped
Mixture-of-Experts (TRGE) method. Firstly, TRGE dynamically expands the
pre-trained CLIP model, assigning specific expert group for each task to
mitigate catastrophic forgetting. With the number of experts continually grows
in this process, TRGE maintains the static experts count within the group and
introduces the intra-group router to alleviate routing overfitting caused by
the increasing routing complexity. Meanwhile, we design an inter-group routing
policy based on task identifiers and task prototype distance, which dynamically
selects relevant expert groups and combines their outputs to enhance inter-task
collaboration. Secondly, to get the correct task identifiers, we leverage
Multimodal Large Language Models (MLLMs) which own powerful multimodal
comprehension capabilities to generate semantic task descriptions and recognize
the correct task identifier. Finally, to mitigate forward forgetting, we
dynamically fuse outputs for unseen samples from the frozen CLIP model and TRGE
adapter based on training progress, leveraging both pre-trained and learned
knowledge. Through extensive experiments across various settings, our method
outperforms other advanced methods with fewer trainable parameters.

</details>


### [206] [A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory](https://arxiv.org/abs/2508.07746)
*Fengdi Che*

Main category: cs.LG

TL;DR: 该论文综述了离线强化学习的理论挑战与实践算法设计之间的桥梁，探讨了理论洞察对算法设计的影响。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习旨在通过固定数据集优化回报，但理论与实践的差距仍是一个挑战。本文旨在通过理论洞察指导算法设计。

Method: 通过分析理论证明所需条件（如函数表示和数据覆盖假设）、反例以及缓解挑战的技术，探讨离线RL的可行性条件。

Result: 揭示了离线RL的固有难度和算法局限性，并提出了满足条件时的解决方案。

Conclusion: 理论条件不仅是证明的基础，也揭示了算法的局限性，提醒我们在条件不满足时寻求新方法。

Abstract: Offline reinforcement learning (RL) aims to optimize the return given a fixed
dataset of agent trajectories without additional interactions with the
environment. While algorithm development has progressed rapidly, significant
theoretical advances have also been made in understanding the fundamental
challenges of offline RL. However, bridging these theoretical insights with
practical algorithm design remains an ongoing challenge. In this survey, we
explore key intuitions derived from theoretical work and their implications for
offline RL algorithms.
  We begin by listing the conditions needed for the proofs, including function
representation and data coverage assumptions. Function representation
conditions tell us what to expect for generalization, and data coverage
assumptions describe the quality requirement of the data. We then examine
counterexamples, where offline RL is not solvable without an impractically
large amount of data. These cases highlight what cannot be achieved for all
algorithms and the inherent hardness of offline RL. Building on techniques to
mitigate these challenges, we discuss the conditions that are sufficient for
offline RL. These conditions are not merely assumptions for theoretical proofs,
but they also reveal the limitations of these algorithms and remind us to
search for novel solutions when the conditions cannot be satisfied.

</details>


### [207] [Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment](https://arxiv.org/abs/2508.07750)
*Haowen Wang,Yun Yue,Zhiling Ye,Shuowen Zhang,Lei Fan,Jiaxin Liang,Jiadi Jiang,Cheng Wei,Jingyuan Deng,Xudong Han,Ji Li,Chunxiao Guo,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.LG

TL;DR: GRAO（Group Relative Alignment Optimization）是一种结合SFT和RL优势的统一框架，通过多样本生成、组内相对优势加权和参考感知参数更新，显著提升了语言模型的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 解决SFT受限于离线策略轨迹和RL样本效率低、依赖高质量基础模型的双重挑战。

Method: 提出GRAO框架，包含多样本生成策略、组内相对优势加权损失函数和参考感知参数更新。

Result: 在复杂人类对齐任务中，GRAO相对SFT、DPO、PPO和GRPO基线分别提升了57.70%、17.65%、7.95%和5.18%。

Conclusion: GRAO提供了一个理论支持的对齐框架，并实证了语言模型能力的高效进化。

Abstract: Alignment methodologies have emerged as a critical pathway for enhancing
language model alignment capabilities. While SFT (supervised fine-tuning)
accelerates convergence through direct token-level loss intervention, its
efficacy is constrained by offline policy trajectory. In contrast,
RL(reinforcement learning) facilitates exploratory policy optimization, but
suffers from low sample efficiency and stringent dependency on high-quality
base models. To address these dual challenges, we propose GRAO (Group Relative
Alignment Optimization), a unified framework that synergizes the respective
strengths of SFT and RL through three key innovations: 1) A multi-sample
generation strategy enabling comparative quality assessment via reward
feedback; 2) A novel Group Direct Alignment Loss formulation leveraging
intra-group relative advantage weighting; 3) Reference-aware parameter updates
guided by pairwise preference dynamics. Our theoretical analysis establishes
GRAO's convergence guarantees and sample efficiency advantages over
conventional approaches. Comprehensive evaluations across complex human
alignment tasks demonstrate GRAO's superior performance, achieving
57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and
GRPO baselines respectively. This work provides both a theoretically grounded
alignment framework and empirical evidence for efficient capability evolution
in language models.

</details>


### [208] [Sparse Probabilistic Graph Circuits](https://arxiv.org/abs/2508.07763)
*Martin Rektoris,Milan Papež,Václav Šmídl,Tomáš Pevný*

Main category: cs.LG

TL;DR: 论文提出了一种稀疏概率图电路（SPGCs），解决了传统深度生成模型（DGMs）在概率推断上的不可解问题，同时降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统DGMs因非线性特性导致概率推断不可解，而现有的PGCs虽解决了可解性问题，但计算复杂度高（O(n^2)），限制了其在大规模稀疏图上的应用。

Method: 提出SPGCs，直接操作稀疏图表示，将复杂度降至O(n + m)，适用于边数远小于节点数平方的稀疏图。

Result: 在药物设计中验证了SPGCs的精确推断能力、内存效率和推理速度，性能与不可解DGMs相当。

Conclusion: SPGCs在保持精确推断的同时显著提升了计算效率，适用于稀疏图场景。

Abstract: Deep generative models (DGMs) for graphs achieve impressively high expressive
power thanks to very efficient and scalable neural networks. However, these
networks contain non-linearities that prevent analytical computation of many
standard probabilistic inference queries, i.e., these DGMs are considered
\emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs)
address this issue by enabling \emph{tractable} probabilistic inference, they
operate on dense graph representations with $\mathcal{O}(n^2)$ complexity for
graphs with $n$ nodes and \emph{$m$ edges}. To address this scalability issue,
we introduce Sparse PGCs, a new class of tractable generative models that
operate directly on sparse graph representation, reducing the complexity to
$\mathcal{O}(n + m)$, which is particularly beneficial for $m \ll n^2$. In the
context of de novo drug design, we empirically demonstrate that SPGCs retain
exact inference capabilities, improve memory efficiency and inference speed,
and match the performance of intractable DGMs in key metrics.

</details>


### [209] [Pareto Multi-Objective Alignment for Language Models](https://arxiv.org/abs/2508.07768)
*Qiang He,Setareh Maghsudi*

Main category: cs.LG

TL;DR: 提出了一种名为PAMA的高效多目标对齐算法，用于解决大型语言模型（LLM）在现实应用中多目标冲突的问题，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于RLHF的对齐方法仅优化单一奖励函数，导致模型行为僵化，无法适应多样的人类偏好，限制了LLM的实用性。

Method: PAMA将多目标RLHF转化为凸优化问题，提供闭式解，将复杂度从O(n^2*d)降至O(n)，显著提升可扩展性。

Result: 实验证明PAMA在125M到7B参数的模型上表现优异，能高效收敛到Pareto稳定点。

Conclusion: PAMA为多目标对齐问题提供了高效且理论可靠的解决方案，推动了LLM在现实中的灵活应用。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications that require careful balancing of multiple, often conflicting,
objectives, such as informativeness versus conciseness, or helpfulness versus
creativity. However, current alignment methods, primarily based on RLHF,
optimize LLMs toward a single reward function, resulting in rigid behavior that
fails to capture the complexity and diversity of human preferences. This
limitation hinders the adaptability of LLMs to practical scenarios, making
multi-objective alignment (MOA) a critical yet underexplored area. To bridge
this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and
computationally efficient algorithm designed explicitly for MOA in LLMs. In
contrast to computationally prohibitive multi-objective optimization (MOO)
methods, PAMA transforms multi-objective RLHF into a convex optimization with a
closed-form solution, significantly enhancing scalability. Traditional MOO
approaches suffer from prohibitive O(n^2*d) complexity, where d represents the
number of model parameters, typically in the billions for LLMs, rendering
direct optimization infeasible. PAMA reduces this complexity to O(n) where n is
the number of objectives, enabling optimization to be completed within
milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto
stationary point, where no objective can be improved without degrading at least
one other. Extensive experiments across language models ranging from 125M to 7B
parameters demonstrate PAMA's robust and effective MOA capabilities, aligning
with its theoretical advantages. PAMA provides a highly efficient solution to
the MOA problem that was previously considered intractable, offering a
practical and theoretically grounded approach to aligning LLMs with diverse
human values, paving the way for versatile and adaptable real-world AI
deployments.

</details>


### [210] [Topological Feature Compression for Molecular Graph Neural Networks](https://arxiv.org/abs/2508.07807)
*Rahul Khorana*

Main category: cs.LG

TL;DR: 提出了一种新型图神经网络架构，结合高阶拓扑信号与标准分子特征，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决分子表示学习中平衡预测准确性、可解释性和计算效率的挑战。

Method: 结合压缩的高阶拓扑信号与标准分子特征，设计了一种新型GNN架构。

Result: 在多个基准测试中取得最佳性能，包括准确性和鲁棒性。

Conclusion: 该方法在保持计算效率和可解释性的同时，显著提升了分子表示的性能。

Abstract: Recent advances in molecular representation learning have produced highly
effective encodings of molecules for numerous cheminformatics and
bioinformatics tasks. However, extracting general chemical insight while
balancing predictive accuracy, interpretability, and computational efficiency
remains a major challenge. In this work, we introduce a novel Graph Neural
Network (GNN) architecture that combines compressed higher-order topological
signals with standard molecular features. Our approach captures global
geometric information while preserving computational tractability and
human-interpretable structure. We evaluate our model across a range of
benchmarks, from small-molecule datasets to complex material datasets, and
demonstrate superior performance using a parameter-efficient architecture. We
achieve the best performing results in both accuracy and robustness across
almost all benchmarks. We open source all code \footnote{All code and results
can be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.

</details>


### [211] [EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning](https://arxiv.org/abs/2508.07809)
*Huanyu Liu,Jia Li,Chang Yu,Taozhi Chen,Yihong Dong,Lecheng Wang,Hu XiaoLong,Ge Li*

Main category: cs.LG

TL;DR: EvoCoT是一种基于两阶段思维链推理优化的自进化课程学习框架，通过自生成和验证CoT轨迹来约束探索空间，逐步缩短轨迹以扩展空间，帮助LLMs在稀疏奖励下稳定学习困难问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖更强的LLMs进行蒸馏或过滤困难问题，限制了可扩展性或通过探索提升推理能力。

Method: EvoCoT通过两阶段CoT推理优化，自生成和验证CoT轨迹，逐步缩短轨迹以扩展探索空间。

Result: 实验表明，EvoCoT使LLMs能解决之前未解决的问题，无需外部CoT监督即可提升推理能力，并与多种RL微调方法兼容。

Conclusion: EvoCoT为LLMs在稀疏奖励下学习困难问题提供了有效解决方案，并支持未来研究。

Abstract: Reinforcement learning with verifiable reward (RLVR) has become a promising
paradigm for post-training large language models (LLMs) to improve their
reasoning capability. However, when the rollout accuracy is low on hard
problems, the reward becomes sparse, limiting learning efficiency and causing
exploration bottlenecks. Existing approaches either rely on stronger LLMs for
distillation or filter out difficult problems, which limits scalability or
restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on
two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the
exploration space by self-generating and verifying CoT trajectories, then
gradually shortens them to expand the space in a controlled way. This enables
LLMs to stably learn from initially unsolved hard problems under sparse
rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,
and Llama. Experiments show that EvoCoT enables LLMs to solve previously
unsolved problems, improves reasoning capability without external CoT
supervision, and is compatible with various RL fine-tuning methods. We release
the source code to support future research.

</details>


### [212] [Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow](https://arxiv.org/abs/2508.07841)
*Carlo Cena,Mauro Martini,Marcello Chiaberge*

Main category: cs.LG

TL;DR: 论文研究了在航天器姿态控制中，结合物理信息的神经网络（PINNs）相较于纯数据驱动方法的表现优势。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型在复杂或计算昂贵时难以适用，而纯数据驱动方法在泛化和稳定性上存在问题。

Method: 使用Real NVP神经网络架构和自注意力机制，结合物理信息训练模型，并与纯数据驱动方法对比。

Result: 物理信息模型的平均相对误差降低了27.08%，在MPC框架中表现更优，控制精度和鲁棒性提升42.86%。

Conclusion: 物理信息神经网络显著提升了航天器姿态控制的性能和稳定性。

Abstract: Attitude control is a fundamental aspect of spacecraft operations. Model
Predictive Control (MPC) has emerged as a powerful strategy for these tasks,
relying on accurate models of the system dynamics to optimize control actions
over a prediction horizon. In scenarios where physics models are incomplete,
difficult to derive, or computationally expensive, machine learning offers a
flexible alternative by learning the system behavior directly from data.
However, purely data-driven models often struggle with generalization and
stability, especially when applied to inputs outside their training domain. To
address these limitations, we investigate the benefits of incorporating
Physics-Informed Neural Networks (PINNs) into the learning of spacecraft
attitude dynamics, comparing their performance with that of purely data-driven
approaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network
architecture with a self-attention mechanism, we trained several models on
simulated data generated with the Basilisk simulator. Two training strategies
were considered: a purely data-driven baseline and a physics-informed variant
to improve robustness and stability. Our results demonstrate that the inclusion
of physics-based information significantly enhances the performance in terms of
the mean relative error of the best architectures found by 27.08%. These
advantages are particularly evident when the learned models are integrated into
an MPC framework, where PINN-based models consistently outperform their purely
data-driven counterparts in terms of control accuracy and robustness, yielding
improvements of up to 42.86% in performance stability error and increased
robustness-to-noise.

</details>


### [213] [Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant](https://arxiv.org/abs/2508.07887)
*Sabrina Namazova,Alessandra Brondetta,Younes Strittmatter,Matthew Nassar,Sebastian Musslick*

Main category: cs.LG

TL;DR: 论文探讨了模拟器在科学实践中的重要性，特别是行为科学中的参与者模拟器Centaur，并评估其是否符合可靠模拟器的标准。


<details>
  <summary>Details</summary>
Motivation: 模拟器如AlphaFold在自然科学中取得了巨大成功，行为科学需要一个类似的可靠参与者模拟器来加速研究。

Method: 评估Centaur（一个基于160项实验数据微调的大型语言模型）作为参与者模拟器的核心标准。

Result: Centaur在预测准确性上表现良好，但在生成行为上与人数据有系统性差异。

Conclusion: Centaur虽在预测人类行为上迈出重要一步，但尚未达到可靠参与者模拟器的标准。

Abstract: Simulators have revolutionized scientific practice across the natural
sciences. By generating data that reliably approximate real-world phenomena,
they enable scientists to accelerate hypothesis testing and optimize
experimental designs. This is perhaps best illustrated by AlphaFold, a
Nobel-prize winning simulator in chemistry that predicts protein structures
from amino acid sequences, enabling rapid prototyping of molecular
interactions, drug targets, and protein functions. In the behavioral sciences,
a reliable participant simulator - a system capable of producing human-like
behavior across cognitive tasks - would represent a similarly transformative
advance. Recently, Binz et al. introduced Centaur, a large language model (LLM)
fine-tuned on human data from 160 experiments, proposing its use not only as a
model of cognition but also as a participant simulator for "in silico
prototyping of experimental studies", e.g., to advance automated cognitive
science. Here, we review the core criteria for a participant simulator and
assess how well Centaur meets them. Although Centaur demonstrates strong
predictive accuracy, its generative behavior - a critical criterion for a
participant simulator - systematically diverges from human data. This suggests
that, while Centaur is a significant step toward predicting human behavior, it
does not yet meet the standards of a reliable participant simulator or an
accurate model of cognition.

</details>


### [214] [Score Augmentation for Diffusion Models](https://arxiv.org/abs/2508.07926)
*Liang Hou,Yuan Gao,Boyuan Jiang,Xin Tao,Qi Yan,Renjie Liao,Pengfei Wan,Di Zhang,Kun Gai*

Main category: cs.LG

TL;DR: 本文提出了一种名为ScoreAug的新型数据增强框架，专为扩散模型设计，通过处理噪声数据来缓解训练中的过拟合问题，并在多个基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中表现出色，但在数据有限的情况下容易过拟合。本文旨在解决这一问题。

Method: 提出ScoreAug框架，对噪声数据进行变换，并要求去噪器预测原始目标的增强，从而实现分数增强。

Result: 实验表明，ScoreAug在多个数据集上显著提升了性能，有效缓解了过拟合，且能与传统数据增强技术结合使用。

Conclusion: ScoreAug是一种有效的扩散模型数据增强方法，能够提升性能并避免数据泄漏问题。

Abstract: Diffusion models have achieved remarkable success in generative modeling.
However, this study confirms the existence of overfitting in diffusion model
training, particularly in data-limited regimes. To address this challenge, we
propose Score Augmentation (ScoreAug), a novel data augmentation framework
specifically designed for diffusion models. Unlike conventional augmentation
approaches that operate on clean data, ScoreAug applies transformations to
noisy data, aligning with the inherent denoising mechanism of diffusion.
Crucially, ScoreAug further requires the denoiser to predict the augmentation
of the original target. This design establishes an equivariant learning
objective, enabling the denoiser to learn scores across varied denoising
spaces, thereby realizing what we term score augmentation. We also
theoretically analyze the relationship between scores in different spaces under
general transformations. In experiments, we extensively validate ScoreAug on
multiple benchmarks including CIFAR-10, FFHQ, AFHQv2, and ImageNet, with
results demonstrating significant performance improvements over baselines.
Notably, ScoreAug effectively mitigates overfitting across diverse scenarios,
such as varying data scales and model capacities, while exhibiting stable
convergence properties. Another advantage of ScoreAug over standard data
augmentation lies in its ability to circumvent data leakage issues under
certain conditions. Furthermore, we show that ScoreAug can be synergistically
combined with traditional data augmentation techniques to achieve additional
performance gains.

</details>


### [215] [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
*Sven Weinzierl,Sandra Zilker,Annina Liessmann,Martin Käppel,Weixin Wang,Martin Matzner*

Main category: cs.LG

TL;DR: 该论文提出了一种基于迁移学习的预测过程监控（PPM）技术，帮助缺乏足够事件数据的组织实现有效的决策支持。


<details>
  <summary>Details</summary>
Motivation: 现有PPM技术需要大量事件数据或其他资源，而某些组织可能无法满足这些条件，因此需要一种无需大量数据的PPM方法。

Method: 采用迁移学习技术，将一种业务流程的知识迁移到相似业务流程中，支持跨组织或组织内的PPM实现。

Result: 实验结果表明，迁移学习可以在目标环境中实现有效的PPM，资源（如预训练模型）可在组织内或跨组织共享。

Conclusion: 提出的技术为资源有限的组织提供了可行的PPM解决方案，扩展了迁移学习在业务流程管理中的应用。

Abstract: Event logs reflect the behavior of business processes that are mapped in
organizational information systems. Predictive process monitoring (PPM)
transforms these data into value by creating process-related predictions that
provide the insights required for proactive interventions at process runtime.
Existing PPM techniques require sufficient amounts of event data or other
relevant resources that might not be readily available, preventing some
organizations from utilizing PPM. The transfer learning-based PPM technique
presented in this paper allows organizations without suitable event data or
other relevant resources to implement PPM for effective decision support. The
technique is instantiated in two real-life use cases, based on which numerical
experiments are performed using event logs for IT service management processes
in an intra- and inter-organizational setting. The results of the experiments
suggest that knowledge of one business process can be transferred to a similar
business process in the same or a different organization to enable effective
PPM in the target context. With the proposed technique, organizations can
benefit from transfer learning in an intra- and inter-organizational setting,
where resources like pre-trained models are transferred within and across
organizational boundaries.

</details>


### [216] [Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series Forecasting](https://arxiv.org/abs/2508.07927)
*Amal Saadallah,Abdulaziz Al-Ademi*

Main category: cs.LG

TL;DR: 提出了一种通过模型适应和选择增强DNN性能的新框架，用于非平稳环境中的时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境中时间序列预测的挑战，其中底层模式随时间变化。

Method: 离线训练基础DNN，分割验证子集以聚类主导模式，为每个聚类微调DNN，推理时匹配最近模式并部署对应模型，集成概念漂移检测机制。

Result: 在GluonTS库中的传统和先进DNN架构上均表现出显著性能提升。

Conclusion: 该框架通用性强，能有效适应非平稳环境中的模式变化。

Abstract: Time series forecasting poses significant challenges in non-stationary
environments where underlying patterns evolve over time. In this work, we
propose a novel framework that enhances deep neural network (DNN) performance
by leveraging specialized model adaptation and selection. Initially, a base DNN
is trained offline on historical time series data. A reserved validation subset
is then segmented to extract and cluster the most dominant patterns within the
series, thereby identifying distinct regimes. For each identified cluster, the
base DNN is fine-tuned to produce a specialized version that captures unique
pattern characteristics. At inference, the most recent input is matched against
the cluster centroids, and the corresponding fine-tuned version is deployed
based on the closest similarity measure. Additionally, our approach integrates
a concept drift detection mechanism to identify and adapt to emerging patterns
caused by non-stationary behavior. The proposed framework is generalizable
across various DNN architectures and has demonstrated significant performance
gains on both traditional DNNs and recent advanced architectures implemented in
the GluonTS library.

</details>


### [217] [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221)
*Zihe Liu,Jiashun Liu,Yancheng He,Weixun Wang,Jiaheng Liu,Ling Pan,Xinyu Hu,Shaopan Xiong,Ju Huang,Jian Hu,Shengyi Huang,Siran Yang,Jiamang Wang,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 本文系统回顾了强化学习在LLM推理中的应用，通过统一框架分析了不同技术的机制、适用场景和核心原则，并提出了一种简单组合方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在LLM推理领域发展迅速，但缺乏标准化指南和对其机制的深入理解，导致实验结果不一致，为实践者带来困惑。

Method: 通过统一的开源框架，对广泛采用的RL技术进行严格复现和孤立评估，分析其内部机制、适用场景和核心原则。

Result: 研究发现，两种技术的极简组合可以显著提升性能，超越GRPO和DAPO等策略。

Conclusion: 本文提供了针对特定场景选择RL技术的清晰指南，并为实践者提供了可靠的路线图。

Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent
research area, marked by a significant surge in related studies on both
algorithmic innovations and practical applications. Despite this progress,
several critical challenges remain, including the absence of standardized
guidelines for employing RL techniques and a fragmented understanding of their
underlying mechanisms. Additionally, inconsistent experimental settings,
variations in training data, and differences in model initialization have led
to conflicting conclusions, obscuring the key characteristics of these
techniques and creating confusion among practitioners when selecting
appropriate techniques. This paper systematically reviews widely adopted RL
techniques through rigorous reproductions and isolated evaluations within a
unified open-source framework. We analyze the internal mechanisms, applicable
scenarios, and core principles of each technique through fine-grained
experiments, including datasets of varying difficulty, model sizes, and
architectures. Based on these insights, we present clear guidelines for
selecting RL techniques tailored to specific setups, and provide a reliable
roadmap for practitioners navigating the RL for the LLM domain. Finally, we
reveal that a minimalist combination of two techniques can unlock the learning
capability of critic-free policies using vanilla PPO loss. The results
demonstrate that our simple combination consistently improves performance,
surpassing strategies like GRPO and DAPO.

</details>


### [218] [Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters](https://arxiv.org/abs/2508.07952)
*Richard J. Fawley,Renato Cordeiro de Amorim*

Main category: cs.LG

TL;DR: SHARK是一种基于Shapley值的特征加权聚类算法，无需额外参数调整，通过分解k均值目标为Shapley值，高效计算特征相关性，提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 传统聚类算法假设所有特征对数据结构贡献均等，但在高维或噪声场景下不成立。现有特征加权方法需额外参数调优，SHARK旨在解决这一问题。

Method: SHARK利用Shapley值量化特征相关性，将k均值目标分解为各特征的Shapley值之和，迭代重加权特征，强调信息维度，抑制无关维度。

Result: 实验表明，SHARK在合成和真实数据集上优于现有方法，尤其在噪声场景下表现出更强的鲁棒性和准确性。

Conclusion: SHARK提供了一种无需额外参数的特征加权聚类方法，通过Shapley值高效计算特征相关性，显著提升聚类性能。

Abstract: Clustering algorithms often assume all features contribute equally to the
data structure, an assumption that usually fails in high-dimensional or noisy
settings. Feature weighting methods can address this, but most require
additional parameter tuning. We propose SHARK (Shapley Reweighted $k$-means), a
feature-weighted clustering algorithm motivated by the use of Shapley values
from cooperative game theory to quantify feature relevance, which requires no
additional parameters beyond those in $k$-means. We prove that the $k$-means
objective can be decomposed into a sum of per-feature Shapley values, providing
an axiomatic foundation for unsupervised feature relevance and reducing Shapley
computation from exponential to polynomial time. SHARK iteratively re-weights
features by the inverse of their Shapley contribution, emphasising informative
dimensions and down-weighting irrelevant ones. Experiments on synthetic and
real-world data sets show that SHARK consistently matches or outperforms
existing methods, achieving superior robustness and accuracy, particularly in
scenarios where noise may be present. Software:
https://github.com/rickfawley/shark.

</details>


### [219] [WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer](https://arxiv.org/abs/2508.07970)
*Junyu Wu,Weiming Chang,Xiaotao Liu,Guanyou He,Tingfeng Xian,Haoqiang Hong,Boqi Chen,Haotao Tian,Tao Yang,Yunsheng Shi,Feng Lin,Ting Yao*

Main category: cs.LG

TL;DR: WeChat-YATT是一种新型RLHF训练框架，解决了现有框架在复杂多模态工作流和动态负载下的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF训练框架在控制器扩展性和复杂流程编排上存在瓶颈，难以适应动态负载和大规模数据场景。

Method: 提出WeChat-YATT框架，采用并行控制器编程模型和动态资源分配策略。

Result: 实验表明，WeChat-YATT在吞吐量上显著优于现有框架，并已成功应用于微信产品。

Conclusion: WeChat-YATT是一种高效、可扩展的RLHF训练框架，适用于实际大规模应用。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent
paradigm for training large language models and multimodal systems. Despite
notable advances enabled by existing RLHF training frameworks, significant
challenges remain in scaling to complex multimodal workflows and adapting to
dynamic workloads. In particular, current systems often encounter limitations
related to controller scalability when managing large models, as well as
inefficiencies in orchestrating intricate RLHF pipelines, especially in
scenarios that require dynamic sampling and resource allocation. In this paper,
we introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,
scalable, and balanced RLHF training framework specifically designed to address
these challenges. WeChat-YATT features a parallel controller programming model
that enables flexible and efficient orchestration of complex RLHF workflows,
effectively mitigating the bottlenecks associated with centralized controller
architectures and facilitating scalability in large-scale data scenarios. In
addition, we propose a dynamic placement schema that adaptively partitions
computational resources and schedules workloads, thereby significantly reducing
hardware idle time and improving GPU utilization under variable training
conditions. We evaluate WeChat-YATT across a range of experimental scenarios,
demonstrating that it achieves substantial improvements in throughput compared
to state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been
successfully deployed to train models supporting WeChat product features for a
large-scale user base, underscoring its effectiveness and robustness in
real-world applications.

</details>


### [220] [A Physics-informed Deep Operator for Real-Time Freeway Traffic State Estimation](https://arxiv.org/abs/2508.08002)
*Hongxin Yu,Yibing Wang,Fengyue Jin,Meng Zhang,Anni Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息深度算子网络（PI-DeepONet）的实时高速公路交通状态估计方法，扩展了原始架构并显著提升了估计精度。


<details>
  <summary>Details</summary>
Motivation: 结合模型驱动和数据驱动的优势，实现更准确的交通状态估计。

Method: 扩展PI-DeepONet架构，支持2-D数据输入，引入非线性扩展层、注意力机制和MIMO机制，并设计专用神经网络自适应识别交通流模型参数。

Result: 新方法在NGSIM短路段和中国大规模城市快速路上优于四种基线方法，实现了高精度的流量和平均速度估计。

Conclusion: 扩展的PI-DeepONet架构为交通状态估计提供了更高效和精确的解决方案。

Abstract: Traffic state estimation (TSE) falls methodologically into three categories:
model-driven, data-driven, and model-data dual-driven. Model-driven TSE relies
on macroscopic traffic flow models originated from hydrodynamics. Data-driven
TSE leverages historical sensing data and employs statistical models or machine
learning methods to infer traffic state. Model-data dual-driven traffic state
estimation attempts to harness the strengths of both aspects to achieve more
accurate TSE. From the perspective of mathematical operator theory, TSE can be
viewed as a type of operator that maps available measurements of inerested
traffic state into unmeasured traffic state variables in real time. For the
first time this paper proposes to study real-time freeway TSE in the idea of
physics-informed deep operator network (PI-DeepONet), which is an
operator-oriented architecture embedding traffic flow models based on deep
neural networks. The paper has developed an extended architecture from the
original PI-DeepONet. The extended architecture is featured with: (1) the
acceptance of 2-D data input so as to support CNN-based computations; (2) the
introduction of a nonlinear expansion layer, an attention mechanism, and a MIMO
mechanism; (3) dedicated neural network design for adaptive identification of
traffic flow model parameters. A traffic state estimator built on the basis of
this extended PI-DeepONet architecture was evaluated with respect to a short
freeway stretch of NGSIM and a large-scale urban expressway in China, along
with other four baseline TSE methods. The evaluation results demonstrated that
this novel TSE method outperformed the baseline methods with high-precision
estimation results of flow and mean speed.

</details>


### [221] [Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP](https://arxiv.org/abs/2508.08005)
*Xiang Li,Shanshan Wang,Chenglong Xiao*

Main category: cs.LG

TL;DR: 提出了一种基于学习的框架GAT-MLP，结合传统机器学习和图神经网络，用于最大团问题的算法选择。实验表明，随机森林和双通道架构表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏针对最大团问题的算法选择方法，需结合实例特征优化算法性能。

Method: 构建标注数据集，评估四种传统分类器，并开发双通道模型GAT-MLP，结合图注意力网络和多层感知机。

Result: 随机森林表现稳定，GAT-MLP在所有指标上表现优异，连通性和拓扑结构是关键特征。

Conclusion: 双通道架构和图神经网络在组合算法选择中具有潜力。

Abstract: Extensive experiments and prior studies show that no single maximum clique
algorithm consistently performs best across all instances, highlighting the
importance of selecting suitable algorithms based on instance features. Through
an extensive analysis of relevant studies, it is found that there is a lack of
research work concerning algorithm selection oriented toward the Maximum Clique
Problem (MCP). In this work, we propose a learning-based framework that
integrates both traditional machine learning and graph neural networks to
address this gap. We construct a labeled dataset by running four exact MCP
algorithms on a diverse collection of graph instances, accompanied by
structural and global statistical features extracted from each graph. We first
evaluate four conventional classifiers: Support Vector Machine (SVM), Random
Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple
dataset variants. Experimental results show that RF consistently shows strong
performance across metrics and dataset variants, making it a reliable baseline.
In addition, feature importance analysis indicates that connectivity and
topological structure are strong predictors of algorithm performance. Building
on these findings, we develop a dual-channel model named GAT-MLP, which
combines a Graph Attention Network (GAT) for local structural encoding with a
Multilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model
shows strong and consistent performance across all metrics. Our results
highlight the effectiveness of dual-channel architectures and the promise of
graph neural networks in combinatorial algorithm selection.

</details>


### [222] [Communication-Efficient Zero-Order and First-Order Federated Learning Methods over Wireless Networks](https://arxiv.org/abs/2508.08013)
*Mohamad Assaad,Zeinab Nehme,Merouane Debbah*

Main category: cs.LG

TL;DR: 论文提出了两种通信高效的联邦学习方法，通过减少通信开销和利用信道信息，解决了无线系统中信息交换的挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘设备协作训练时面临通信开销大的问题，尤其是在无线系统中。

Method: 1. 使用零阶优化技术和两点梯度估计器；2. 采用一阶梯度计算策略，并利用信道信息。

Result: 提供了两种方法的收敛性保证和性能界限。

Conclusion: 两种方法有效减少了通信开销，同时无需额外获取信道状态信息。

Abstract: Federated Learning (FL) is an emerging learning framework that enables edge
devices to collaboratively train ML models without sharing their local data. FL
faces, however, a significant challenge due to the high amount of information
that must be exchanged between the devices and the aggregator in the training
phase, which can exceed the limited capacity of wireless systems. In this
paper, two communication-efficient FL methods are considered where
communication overhead is reduced by communicating scalar values instead of
long vectors and by allowing high number of users to send information
simultaneously. The first approach employs a zero-order optimization technique
with two-point gradient estimator, while the second involves a first-order
gradient computation strategy. The novelty lies in leveraging channel
information in the learning algorithms, eliminating hence the need for
additional resources to acquire channel state information (CSI) and to remove
its impact, as well as in considering asynchronous devices. We provide a
rigorous analytical framework for the two methods, deriving convergence
guarantees and establishing appropriate performance bounds.

</details>


### [223] [Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles](https://arxiv.org/abs/2508.08034)
*Roksana Yahyaabadi,Ghazal Farhani,Taufiq Rahman,Soodeh Nikan,Abdullah Jirjees,Fadi Araji*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动的可扩展方法，用于预测内燃机、电动汽车和混合动力汽车的瞬时和累计功耗，效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统方法在大规模实际应用中不切实际，因此需要一种更高效、可扩展的功耗预测方法。

Method: 利用动力系统动态特征集，结合传统机器学习和深度神经网络（如Transformer和LSTM）进行预测。

Result: 内燃机瞬时误差低至10^-3，累计误差低于3%；电动汽车和混合动力汽车的累计误差分别低于4.1%和2.1%。

Conclusion: 该方法在不同车型中均有效，但电动汽车和混合动力汽车的数据变异性更高，需更稳健的模型。

Abstract: Accurate power consumption prediction is crucial for improving efficiency and
reducing environmental impact, yet traditional methods relying on specialized
instruments or rigid physical models are impractical for large-scale,
real-world deployment. This study introduces a scalable data-driven method
using powertrain dynamic feature sets and both traditional machine learning and
deep neural networks to estimate instantaneous and cumulative power consumption
in internal combustion engine (ICE), electric vehicle (EV), and hybrid electric
vehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with
mean absolute error and root mean squared error on the order of $10^{-3}$, and
cumulative errors under 3%. Transformer and long short-term memory models
performed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,
respectively. Results confirm the approach's effectiveness across vehicles and
models. Uncertainty analysis revealed greater variability in EV and HEV
datasets than ICE, due to complex power management, emphasizing the need for
robust models for advanced powertrains.

</details>


### [224] [BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models](https://arxiv.org/abs/2508.08040)
*Maozhen Zhang,Mengnan Zhao,Bo Wang*

Main category: cs.LG

TL;DR: 论文提出了一种针对多模态对比模型中基于提示的联邦学习的后门攻击方法（BadPromptFL），通过优化本地后门触发器和提示嵌入，将毒化提示传播到全局聚合中，实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 探索基于提示的联邦学习在安全方面的漏洞，填补了多模态联邦学习中提示聚合攻击面的研究空白。

Method: 提出BadPromptFL攻击方法，通过联合优化本地后门触发器和提示嵌入，将毒化提示注入全局聚合过程。

Result: 实验表明，攻击成功率超过90%，且具有隐蔽性和通用性。

Conclusion: 研究揭示了基于提示的联邦学习在实际部署中的脆弱性，呼吁对其鲁棒性进行更多关注。

Abstract: Prompt-based tuning has emerged as a lightweight alternative to full
fine-tuning in large vision-language models, enabling efficient adaptation via
learned contextual prompts. This paradigm has recently been extended to
federated learning settings (e.g., PromptFL), where clients collaboratively
train prompts under data privacy constraints. However, the security
implications of prompt-based aggregation in federated multimodal learning
remain largely unexplored, leaving a critical attack surface unaddressed. In
this paper, we introduce \textbf{BadPromptFL}, the first backdoor attack
targeting prompt-based federated learning in multimodal contrastive models. In
BadPromptFL, compromised clients jointly optimize local backdoor triggers and
prompt embeddings, injecting poisoned prompts into the global aggregation
process. These prompts are then propagated to benign clients, enabling
universal backdoor activation at inference without modifying model parameters.
Leveraging the contextual learning behavior of CLIP-style architectures,
BadPromptFL achieves high attack success rates (e.g., \(>90\%\)) with minimal
visibility and limited client participation. Extensive experiments across
multiple datasets and aggregation protocols validate the effectiveness,
stealth, and generalizability of our attack, raising critical concerns about
the robustness of prompt-based federated learning in real-world deployments.

</details>


### [225] [On Understanding of the Dynamics of Model Capacity in Continual Learning](https://arxiv.org/abs/2508.08052)
*Supriyo Chakraborty,Krishnan Raghavan*

Main category: cs.LG

TL;DR: 论文提出了一种称为CLEMC的模型，用于描述持续学习中稳定性与可塑性平衡的动态行为，并通过理论和实验证明这种平衡点是非静态的。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的稳定性与可塑性困境，研究神经网络在不同任务分布下的表现能力。

Method: 提出CLEMC模型，建立差分方程描述神经网络、任务数据和优化过程的交互，并进行广泛的实验验证。

Result: 无论网络架构或优化方法如何，当新任务分布与之前不同时，神经网络的表示能力会下降。

Conclusion: CLEMC揭示了持续学习中稳定性与可塑性平衡的动态特性，为未来研究提供了理论基础。

Abstract: The stability-plasticity dilemma, closely related to a neural network's (NN)
capacity-its ability to represent tasks-is a fundamental challenge in continual
learning (CL). Within this context, we introduce CL's effective model capacity
(CLEMC) that characterizes the dynamic behavior of the stability-plasticity
balance point. We develop a difference equation to model the evolution of the
interplay between the NN, task data, and optimization procedure. We then
leverage CLEMC to demonstrate that the effective capacity-and, by extension,
the stability-plasticity balance point is inherently non-stationary. We show
that regardless of the NN architecture or optimization method, a NN's ability
to represent new tasks diminishes when incoming task distributions differ from
previous ones. We conduct extensive experiments to support our theoretical
findings, spanning a range of architectures-from small feedforward network and
convolutional networks to medium-sized graph neural networks and
transformer-based large language models with millions of parameters.

</details>


### [226] [C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction](https://arxiv.org/abs/2508.08071)
*Yunqing Li,Zixiang Tang,Jiaying Zhuang,Zhenyu Yang,Farhad Ameri,Jianbang Zhang*

Main category: cs.LG

TL;DR: 论文提出了PMGraph基准和C-MAG架构，用于解决供应链中多模态数据链接预测问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉复杂的制造商能力和多模态数据，需要更高效的供应链链接预测方法。

Method: 提出C-MAG架构，通过两阶段方法（对齐多模态属性和多尺度消息传递）提升链接预测准确性。

Result: 构建了包含8,888制造商和70k产品的PMGraph基准，C-MAG在多模态数据中表现优异。

Conclusion: C-MAG为多模态供应链数据提供了有效的链接预测解决方案，并适用于实际噪声环境。

Abstract: Connecting an ever-expanding catalogue of products with suitable
manufacturers and suppliers is critical for resilient, efficient global supply
chains, yet traditional methods struggle to capture complex capabilities,
certifications, geographic constraints, and rich multimodal data of real-world
manufacturer profiles. To address these gaps, we introduce PMGraph, a public
benchmark of bipartite and heterogeneous multimodal supply-chain graphs linking
8,888 manufacturers, over 70k products, more than 110k manufacturer-product
edges, and over 29k product images. Building on this benchmark, we propose the
Cascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first
aligns and aggregates textual and visual attributes into intermediate group
embeddings, then propagates them through a manufacturer-product hetero-graph
via multiscale message passing to enhance link prediction accuracy. C-MAG also
provides practical guidelines for modality-aware fusion, preserving predictive
performance in noisy, real-world settings.

</details>


### [227] [ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring](https://arxiv.org/abs/2508.08073)
*Dimitris Tsaras,Xing Li,Lei Chen,Zhiyao Xie,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 通过分类器预剪枝无效切割，显著提升逻辑优化速度。


<details>
  <summary>Details</summary>
Motivation: 传统逻辑优化算子计算成本高，且大部分切割尝试失败，需减少无效操作。

Method: 利用分类器预判并剪枝无效切割，避免不必要的重综合操作。

Result: 在EPFL基准套件和10个大型工业设计上，平均提速3.9倍。

Conclusion: 分类器剪枝方法显著提升逻辑优化效率，优于现有技术。

Abstract: In electronic design automation, logic optimization operators play a crucial
role in minimizing the gate count of logic circuits. However, their computation
demands are high. Operators such as refactor conventionally form iterative cuts
for each node, striving for a more compact representation - a task which often
fails 98% on average. Prior research has sought to mitigate computational cost
through parallelization. In contrast, our approach leverages a classifier to
prune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis
operations. Experiments on the refactor operator using the EPFL benchmark suite
and 10 large industrial designs demonstrate that this technique can speedup
logic optimization by 3.9x on average compared with the state-of-the-art ABC
implementation.

</details>


### [228] [Symbolic Quantile Regression for the Interpretable Prediction of Conditional Quantiles](https://arxiv.org/abs/2508.08080)
*Cas Oude Hoekstra,Floris den Hengst*

Main category: cs.LG

TL;DR: 本文介绍了符号分位数回归（SQR），一种基于符号回归（SR）的方法，用于预测条件分位数，并在透明性和性能上表现优异。


<details>
  <summary>Details</summary>
Motivation: 符号回归（SR）通常用于预测结果的平均值，但在其他分布点（如中位数或极值）上的变量关系估计尚未充分研究。这些估计在高风险、安全关键领域尤为重要。

Method: 提出符号分位数回归（SQR），通过符号回归预测条件分位数，并在广泛评估中验证其性能。

Result: SQR在透明模型中表现优异，与黑盒基线模型性能相当，同时保持透明性。通过航空燃油使用案例，展示了SQR如何解释目标分布差异。

Conclusion: SQR适用于预测条件分位数，并在不同分位数下理解特征影响。

Abstract: Symbolic Regression (SR) is a well-established framework for generating
interpretable or white-box predictive models. Although SR has been successfully
applied to create interpretable estimates of the average of the outcome, it is
currently not well understood how it can be used to estimate the relationship
between variables at other points in the distribution of the target variable.
Such estimates of e.g. the median or an extreme value provide a fuller picture
of how predictive variables affect the outcome and are necessary in
high-stakes, safety-critical application domains. This study introduces
Symbolic Quantile Regression (SQR), an approach to predict conditional
quantiles with SR. In an extensive evaluation, we find that SQR outperforms
transparent models and performs comparably to a strong black-box baseline
without compromising transparency. We also show how SQR can be used to explain
differences in the target distribution by comparing models that predict extreme
and central outcomes in an airline fuel usage case study. We conclude that SQR
is suitable for predicting conditional quantiles and understanding interesting
feature influences at varying quantiles.

</details>


### [229] [Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation](https://arxiv.org/abs/2508.08087)
*Amir Ali Panahi,Daniel Luder,Billy Wu,Gregory Offer,Dirk Uwe Sauer,Weihan Li*

Main category: cs.LG

TL;DR: 论文比较了三种算子学习替代模型（DeepONets、FNOs和PE-FNO）在锂离子电池数字孪生中的应用，PE-FNO在速度和参数灵活性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 实现高物理保真度和亚毫秒级速度的锂离子电池数字孪生。

Method: 训练三种模型（DeepONets、FNOs、PE-FNO）在不同电流和SOC范围内的模拟轨迹。

Result: PE-FNO速度比传统方法快200倍，参数估计误差较低，适用于实时电池管理。

Conclusion: PE-FNO为高速度、高保真度的电化学数字孪生提供了实用路径。

Abstract: Reliable digital twins of lithium-ion batteries must achieve high physical
fidelity with sub-millisecond speed. In this work, we benchmark three
operator-learning surrogates for the Single Particle Model (SPM): Deep Operator
Networks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed
parameter-embedded Fourier Neural Operator (PE-FNO), which conditions each
spectral layer on particle radius and solid-phase diffusivity. Models are
trained on simulated trajectories spanning four current families (constant,
triangular, pulse-train, and Gaussian-random-field) and a full range of
State-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates
constant-current behaviour but struggles with more dynamic loads. The basic FNO
maintains mesh invariance and keeps concentration errors below 1 %, with
voltage mean-absolute errors under 1.7 mV across all load types. Introducing
parameter embedding marginally increases error, but enables generalisation to
varying radii and diffusivities. PE-FNO executes approximately 200 times faster
than a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse
tasks are explored in a parameter estimation task with Bayesian optimisation,
recovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute
percentage error, respectively, and 0.5918 percentage points higher error in
comparison with classical methods. These results pave the way for neural
operators to meet the accuracy, speed and parametric flexibility demands of
real-time battery management, design-of-experiments and large-scale inference.
PE-FNO outperforms conventional neural surrogates, offering a practical path
towards high-speed and high-fidelity electrochemical digital twins.

</details>


### [230] [Grid2Guide: A* Enabled Small Language Model for Indoor Navigation](https://arxiv.org/abs/2508.08100)
*Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: Grid2Guide结合A*算法和小型语言模型（SLM），生成清晰易懂的室内导航指令。


<details>
  <summary>Details</summary>
Motivation: 复杂环境中缺乏外部定位信号和专用基础设施时，可靠的室内导航仍具挑战性。

Method: 通过二进制占用矩阵和A*算法计算最优路径，再用SLM将路径转换为自然语言指令。

Result: 实验证明该方法能生成准确、及时的导航指导。

Conclusion: Grid2Guide是一种轻量级、无需基础设施的实时室内导航解决方案。

Abstract: Reliable indoor navigation remains a significant challenge in complex
environments, particularly where external positioning signals and dedicated
infrastructures are unavailable. This research presents Grid2Guide, a hybrid
navigation framework that combines the A* search algorithm with a Small
Language Model (SLM) to generate clear, human-readable route instructions. The
framework first conducts a binary occupancy matrix from a given indoor map.
Using this matrix, the A* algorithm computes the optimal path between origin
and destination, producing concise textual navigation steps. These steps are
then transformed into natural language instructions by the SLM, enhancing
interpretability for end users. Experimental evaluations across various indoor
scenarios demonstrate the method's effectiveness in producing accurate and
timely navigation guidance. The results validate the proposed approach as a
lightweight, infrastructure-free solution for real-time indoor navigation
support.

</details>


### [231] [Vision-Based Localization and LLM-based Navigation for Indoor Environments](https://arxiv.org/abs/2508.08120)
*Keyan Rahimi,Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 该研究提出了一种结合视觉定位和大型语言模型（LLM）导航的室内导航方法，定位准确率达96%，导航指令准确率为75%。


<details>
  <summary>Details</summary>
Motivation: 解决室内环境中GPS信号不可靠和建筑结构复杂的问题，提供无需基础设施的导航方案。

Method: 使用ResNet-50进行视觉定位，结合LLM（如ChatGPT）生成导航指令。

Result: 定位准确率96%，导航指令准确率75%，但在零样本推理和响应时间上存在局限。

Conclusion: 该方法展示了在资源受限环境中使用普通设备和公开平面图实现可扩展室内导航的潜力。

Abstract: Indoor navigation remains a complex challenge due to the absence of reliable
GPS signals and the architectural intricacies of large enclosed environments.
This study presents an indoor localization and navigation approach that
integrates vision-based localization with large language model (LLM)-based
navigation. The localization system utilizes a ResNet-50 convolutional neural
network fine-tuned through a two-stage process to identify the user's position
using smartphone camera input. To complement localization, the navigation
module employs an LLM, guided by a carefully crafted system prompt, to
interpret preprocessed floor plan images and generate step-by-step directions.
Experimental evaluation was conducted in a realistic office corridor with
repetitive features and limited visibility to test localization robustness. The
model achieved high confidence and an accuracy of 96% across all tested
waypoints, even under constrained viewing conditions and short-duration
queries. Navigation tests using ChatGPT on real building floor maps yielded an
average instruction accuracy of 75%, with observed limitations in zero-shot
reasoning and inference time. This research demonstrates the potential for
scalable, infrastructure-free indoor navigation using off-the-shelf cameras and
publicly available floor plans, particularly in resource-constrained settings
like hospitals, airports, and educational institutions.

</details>


### [232] [MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing](https://arxiv.org/abs/2508.08122)
*Mingrong Lin,Ke Deng,Zhengyang Wu,Zetao Zheng,Jie Li*

Main category: cs.LG

TL;DR: 论文提出memoryKT模型，通过三阶段记忆动态模拟（编码、存储、检索）提升知识追踪性能，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型依赖单一遗忘机制，忽略其他记忆过程及个性化遗忘模式，需改进。

Method: 基于时序变分自编码器，分三阶段模拟记忆动态：学习知识记忆特征分布、重构练习反馈、嵌入个性化遗忘模块。

Result: 在四个公开数据集上显著优于现有基线。

Conclusion: memoryKT通过完整记忆周期建模，提升对个体差异的感知能力，性能优越。

Abstract: Knowledge Tracing (KT) is committed to capturing students' knowledge mastery
from their historical interactions. Simulating students' memory states is a
promising approach to enhance both the performance and interpretability of
knowledge tracing models. Memory consists of three fundamental processes:
encoding, storage, and retrieval. Although forgetting primarily manifests
during the storage stage, most existing studies rely on a single,
undifferentiated forgetting mechanism, overlooking other memory processes as
well as personalized forgetting patterns. To address this, this paper proposes
memoryKT, a knowledge tracing model based on a novel temporal variational
autoencoder. The model simulates memory dynamics through a three-stage process:
(i) Learning the distribution of students' knowledge memory features, (ii)
Reconstructing their exercise feedback, while (iii) Embedding a personalized
forgetting module within the temporal workflow to dynamically modulate memory
storage strength. This jointly models the complete encoding-storage-retrieval
cycle, significantly enhancing the model's perception capability for individual
differences. Extensive experiments on four public datasets demonstrate that our
proposed approach significantly outperforms state-of-the-art baselines.

</details>


### [233] [NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological Disorder Detection](https://arxiv.org/abs/2508.08124)
*Guanghao Jin,Yuan Liang,Yihan Ma,Jingpei Wu,Guoyang Liu*

Main category: cs.LG

TL;DR: NeuroDx-LM是一种新型大规模模型，用于基于EEG的神经障碍检测，通过选择性时频嵌入和渐进特征感知训练策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决EEG大规模模型在实际部署中面临的标记数据不足和临床性能不佳的问题。

Method: 提出选择性时频嵌入机制和两阶段渐进特征感知训练策略。

Result: 在CHB-MIT和Schizophrenia数据集上达到最先进的检测性能。

Conclusion: NeuroDx-LM展示了EEG大规模模型在临床应用中的巨大潜力。

Abstract: Large-scale models pre-trained on Electroencephalography (EEG) have shown
promise in clinical applications such as neurological disorder detection.
However, the practical deployment of EEG-based large-scale models faces
critical challenges such as limited labeled EEG data and suboptimal performance
in clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel
large-scale model specifically designed for detecting EEG-based neurological
disorders. Our key contributions include (i) a Selective Temporal-Frequency
Embedding mechanism that adaptively captures complex temporal and spectral
patterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy
that refines feature representation in a two-stage process. In the first stage,
our model learns the fundamental discriminative features of EEG activities; in
the second stage, the model further extracts more specialized fine-grained
features for accurate diagnostic performance. We evaluated NeuroDx-LM on the
CHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in
EEG-based seizure and schizophrenia detection, respectively. These results
demonstrate the great potential of EEG-based large-scale models to advance
clinical applicability. Our code is available at
https://github.com/LetItBe12345/NeuroDx-LM.

</details>


### [234] [OFAL: An Oracle-Free Active Learning Framework](https://arxiv.org/abs/2508.08126)
*Hadi Khorsand,Vahid Pourahmadi*

Main category: cs.LG

TL;DR: OFAL是一种无需人工标注的主动学习方法，通过神经网络不确定性生成新的不确定样本，提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习依赖人工标注，成本高且复杂。OFAL旨在通过模型自身不确定性减少对人工标注的依赖。

Method: 1. 分离并量化不确定性，使用蒙特卡洛Dropout近似贝叶斯神经网络。2. 通过变分自编码器从置信样本生成新的不确定样本。3. 结合其他主动学习采样方法。

Result: OFAL能够生成信息丰富的不确定样本，提升模型准确性。

Conclusion: OFAL为主动学习提供了一种高效且低成本的新方法，减少了对人工标注的依赖。

Abstract: In the active learning paradigm, using an oracle to label data has always
been a complex and expensive task, and with the emersion of large unlabeled
data pools, it would be highly beneficial If we could achieve better results
without relying on an oracle. This research introduces OFAL, an oracle-free
active learning scheme that utilizes neural network uncertainty. OFAL uses the
model's own uncertainty to transform highly confident unlabeled samples into
informative uncertain samples. First, we start with separating and quantifying
different parts of uncertainty and introduce Monte Carlo Dropouts as an
approximation of the Bayesian Neural Network model. Secondly, by adding a
variational autoencoder, we go on to generate new uncertain samples by stepping
toward the uncertain part of latent space starting from a confidence seed
sample. By generating these new informative samples, we can perform active
learning and enhance the model's accuracy. Lastly, we try to compare and
integrate our method with other widely used active learning sampling methods.

</details>


### [235] [MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08137)
*Pravallika Abbineni,Saoud Aldowaish,Colin Liechty,Soroosh Noorzad,Ali Ghazizadeh,Morteza Fayazi*

Main category: cs.LG

TL;DR: MuaLLM是一种开源多模态大型语言模型代理，用于电路设计辅助，结合了混合检索增强生成框架和自适应向量数据库，显著提升了效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 电路设计文献综述面临快速更新的研究、不一致的数据表示和复杂的设计优化目标，亟需高效工具。

Method: MuaLLM采用Reason + Act工作流，结合多模态处理和动态检索增强生成框架，支持文本和视觉数据。

Result: MuaLLM在RAG-250和Reas-100数据集上分别达到90.1%召回率和86.8%准确率，成本降低10倍，速度提升1.6倍。

Conclusion: MuaLLM为电路设计提供了高效、低成本的自动化解决方案，克服了传统方法的局限性。

Abstract: Conducting a comprehensive literature review is crucial for advancing circuit
design methodologies. However, the rapid influx of state-of-the-art research,
inconsistent data representation, and the complexity of optimizing circuit
design objectives make this task significantly challenging. In this paper, we
propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for
circuit design assistance that integrates a hybrid Retrieval-Augmented
Generation (RAG) framework with an adaptive vector database of circuit design
research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +
Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step
information retrieval. It functions as a question-answering design assistant,
capable of interpreting complex queries and providing reasoned responses
grounded in circuit literature. Its multimodal capabilities enable processing
of both textual and visual data, facilitating more efficient and comprehensive
analysis. The system dynamically adapts using intelligent search tools,
automated document retrieval from the internet, and real-time database updates.
Unlike conventional approaches constrained by model context limits, MuaLLM
decouples retrieval from inference, enabling scalable reasoning over
arbitrarily large corpora. At the maximum context length supported by standard
LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining
the same accuracy. This allows rapid, no-human-in-the-loop database generation,
overcoming the bottleneck of simulation-based dataset creation for circuits. To
evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval
and citation performance, and Reasoning-100 (Reas-100), focused on multistep
reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%
accuracy on Reas-100.

</details>


### [236] [FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks](https://arxiv.org/abs/2508.08151)
*Moses Openja,Paolo Arcaini,Foutse Khomh,Fuyuki Ishikawa*

Main category: cs.LG

TL;DR: FairFLRep是一种自动化的公平性感知故障定位与修复技术，用于识别和修正DNN分类器中可能引发偏见的神经元，以提高公平性同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: DNN在决策应用中可能放大数据偏见，导致不公平行为，现有方法难以有效识别和修正这些偏见。

Method: 通过调整与敏感属性（如种族或性别）相关的神经元权重，分析网络输入输出关系以修正导致预测质量差异的神经元。

Result: 在多个数据集和模型上，FairFLRep在提高公平性和效率方面优于现有方法。

Conclusion: FairFLRep在故障定位和修复阶段均需考虑公平性，且比基线方法更高效。

Abstract: Deep neural networks (DNNs) are being utilized in various aspects of our
daily lives, including high-stakes decision-making applications that impact
individuals. However, these systems reflect and amplify bias from the data used
during training and testing, potentially resulting in biased behavior and
inaccurate decisions. For instance, having different misclassification rates
between white and black sub-populations. However, effectively and efficiently
identifying and correcting biased behavior in DNNs is a challenge. This paper
introduces FairFLRep, an automated fairness-aware fault localization and repair
technique that identifies and corrects potentially bias-inducing neurons in DNN
classifiers. FairFLRep focuses on adjusting neuron weights associated with
sensitive attributes, such as race or gender, that contribute to unfair
decisions. By analyzing the input-output relationships within the network,
FairFLRep corrects neurons responsible for disparities in predictive quality
parity. We evaluate FairFLRep on four image classification datasets using two
DNN classifiers, and four tabular datasets with a DNN model. The results show
that FairFLRep consistently outperforms existing methods in improving fairness
while preserving accuracy. An ablation study confirms the importance of
considering fairness during both fault localization and repair stages. Our
findings also show that FairFLRep is more efficient than the baseline
approaches in repairing the network.

</details>


### [237] [Federated Learning for Epileptic Seizure Prediction Across Heterogeneous EEG Datasets](https://arxiv.org/abs/2508.08159)
*Cem Ata Baykara,Saurav Raj Pandey,Ali Burak Ünal,Harlin Lee,Mete Akgün*

Main category: cs.LG

TL;DR: 论文研究了使用联邦学习（FL）在多中心EEG数据中开发癫痫发作预测模型，提出随机子集聚合策略以解决数据异质性问题，显著提升了模型在代表性不足数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于患者隐私法规和数据异质性（非独立同分布特性），开发跨临床中心的癫痫发作预测模型面临挑战。联邦学习提供隐私保护框架，但标准聚合方法在异质环境中可能偏向主导数据集。

Method: 采用隐私保护的全局归一化，并提出随机子集聚合策略，确保每个客户端在每轮训练中使用固定大小的随机子集，实现公平聚合。

Result: 随机子集聚合显著提升了代表性不足客户端的性能（如赫尔辛基数据集准确率从50.8%提升至81.7%），全局模型的宏平均准确率达到77.1%，池化准确率为80.0%。

Conclusion: 平衡的联邦学习方法在异质多中心环境中能有效构建通用癫痫发作预测系统，同时保护数据隐私。

Abstract: Developing accurate and generalizable epileptic seizure prediction models
from electroencephalography (EEG) data across multiple clinical sites is
hindered by patient privacy regulations and significant data heterogeneity
(non-IID characteristics). Federated Learning (FL) offers a privacy-preserving
framework for collaborative training, but standard aggregation methods like
Federated Averaging (FedAvg) can be biased by dominant datasets in
heterogeneous settings. This paper investigates FL for seizure prediction using
a single EEG channel across four diverse public datasets (Siena, CHB-MIT,
Helsinki, NCH), representing distinct patient populations (adult, pediatric,
neonate) and recording conditions. We implement privacy-preserving global
normalization and propose a Random Subset Aggregation strategy, where each
client trains on a fixed-size random subset of its data per round, ensuring
equal contribution during aggregation. Our results show that locally trained
models fail to generalize across sites, and standard weighted FedAvg yields
highly skewed performance (e.g., 89.0% accuracy on CHB-MIT but only 50.8% on
Helsinki and 50.6% on NCH). In contrast, Random Subset Aggregation
significantly improves performance on under-represented clients (accuracy
increases to 81.7% on Helsinki and 68.7% on NCH) and achieves a superior
macro-average accuracy of 77.1% and pooled accuracy of 80.0% across all sites,
demonstrating a more robust and fair global model. This work highlights the
potential of balanced FL approaches for building effective and generalizable
seizure prediction systems in realistic, heterogeneous multi-hospital
environments while respecting data privacy.

</details>


### [238] [Neural Logic Networks for Interpretable Classification](https://arxiv.org/abs/2508.08172)
*Vincent Perreault,Katsumi Inoue,Richard Labib,Alain Hertz*

Main category: cs.LG

TL;DR: 论文提出了一种改进的神经逻辑网络，通过引入NOT操作和偏差项，增强了模型的解释性和性能，并在布尔网络发现和表格分类中取得了先进成果。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络缺乏可解释性，而神经逻辑网络通过学习逻辑机制（如AND和OR操作）提供了可解释的结构。本文旨在进一步扩展其能力，引入NOT操作和偏差项，以更好地处理未观测数据。

Method: 提出了一种广义化的神经逻辑网络，结合NOT操作和偏差项，并设计了因子化的IF-THEN规则结构和改进的学习算法。

Result: 该方法在布尔网络发现和表格分类任务中表现优异，尤其在需要高解释性的医学领域应用中展现了实用价值。

Conclusion: 改进的神经逻辑网络不仅提升了性能，还保持了模型的解释性，为需要透明决策的领域提供了有力工具。

Abstract: Traditional neural networks have an impressive classification performance,
but what they learn cannot be inspected, verified or extracted. Neural Logic
Networks on the other hand have an interpretable structure that enables them to
learn a logical mechanism relating the inputs and outputs with AND and OR
operations. We generalize these networks with NOT operations and biases that
take into account unobserved data and develop a rigorous logical and
probabilistic modeling in terms of concept combinations to motivate their use.
We also propose a novel factorized IF-THEN rule structure for the model as well
as a modified learning algorithm. Our method improves the state-of-the-art in
Boolean networks discovery and is able to learn relevant, interpretable rules
in tabular classification, notably on an example from the medical field where
interpretability has tangible value.

</details>


### [239] [Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion](https://arxiv.org/abs/2508.08216)
*Nicole Lai-Tan,Xiao Gu,Marios G. Philiastides,Fani Deligianni*

Main category: cs.LG

TL;DR: 提出了一种名为ITSA的新方法，通过结合个体特定的重新中心化、分布匹配和监督旋转对齐，提升跨被试的脑机接口泛化能力。


<details>
  <summary>Details</summary>
Motivation: 脑机接口在个性化音乐干预中具有潜力，但被试间EEG信号的变异性、运动伪迹和运动计划差异导致泛化能力受限。

Method: ITSA方法结合了正则化共同空间模式（RCSP）和黎曼几何，采用并行和序列配置的混合架构。

Result: ITSA在跨被试和条件下表现出显著性能提升，并行融合方法优于序列融合。

Conclusion: ITSA为脑机接口的跨被试泛化提供了有效解决方案，代码将公开。

Abstract: Personalised music-based interventions offer a powerful means of supporting
motor rehabilitation by dynamically tailoring auditory stimuli to provide
external timekeeping cues, modulate affective states, and stabilise gait
patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for
adapting these interventions across individuals. However, inter-subject
variability in EEG signals, further compounded by movement-induced artefacts
and motor planning differences, hinders the generalisability of BCIs and
results in lengthy calibration processes. We propose Individual Tangent Space
Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific
recentering, distribution matching, and supervised rotational alignment to
enhance cross-subject generalisation. Our hybrid architecture fuses Regularised
Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and
sequential configurations, improving class separability while maintaining the
geometric structure of covariance matrices for robust statistical computation.
Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant
performance improvements across subjects and conditions. The parallel fusion
approach shows the greatest enhancement over its sequential counterpart, with
robust performance maintained across varying data conditions and electrode
configurations. The code will be made publicly available at the time of
publication.

</details>


### [240] [Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent](https://arxiv.org/abs/2508.08222)
*Tong Yang,Yu Huang,Yingbin Liang,Yuejie Chi*

Main category: cs.LG

TL;DR: 论文研究了Transformer如何通过训练学习解决符号多步推理问题，特别是树中的路径查找任务，揭示了其机制和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer如何通过训练获得多步推理能力，尤其是从理论角度分析其学习机制。

Method: 分析两个任务：反向推理（从目标节点到根节点的路径）和更复杂的前向推理（两阶段推理）。通过梯度下降动态理论分析单层Transformer的能力。

Result: 训练后的单层Transformer可证明解决两个任务，并具有对未见树的泛化能力。多阶段训练动态揭示了注意力头的专业化和协作。

Conclusion: 研究揭示了Transformer如何实现顺序算法程序，表明结构化任务和中间思维链步骤能使浅层多头Transformer有效解决复杂问题。

Abstract: Transformers have demonstrated remarkable capabilities in multi-step
reasoning tasks. However, understandings of the underlying mechanisms by which
they acquire these abilities through training remain limited, particularly from
a theoretical standpoint. This work investigates how transformers learn to
solve symbolic multi-step reasoning problems through chain-of-thought
processes, focusing on path-finding in trees. We analyze two intertwined tasks:
a backward reasoning task, where the model outputs a path from a goal node to
the root, and a more complex forward reasoning task, where the model implements
two-stage reasoning by first identifying the goal-to-root path and then
reversing it to produce the root-to-goal path. Our theoretical analysis,
grounded in the dynamics of gradient descent, shows that trained one-layer
transformers can provably solve both tasks with generalization guarantees to
unseen trees. In particular, our multi-phase training dynamics for forward
reasoning elucidate how different attention heads learn to specialize and
coordinate autonomously to solve the two subtasks in a single autoregressive
path. These results provide a mechanistic explanation of how trained
transformers can implement sequential algorithmic procedures. Moreover, they
offer insights into the emergence of reasoning abilities, suggesting that when
tasks are structured to take intermediate chain-of-thought steps, even shallow
multi-head transformers can effectively solve problems that would otherwise
require deeper architectures.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [241] [Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization](https://arxiv.org/abs/2508.06559)
*Sina Baghal*

Main category: cs.AI

TL;DR: 本文介绍了一种基于CUDA加速的Pasur纸牌游戏计算框架，通过高效内存管理和CFR算法求解近纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 解决Pasur游戏规则复杂和游戏树庞大的挑战。

Method: 使用PyTorch CUDA张量处理规则，分解游戏树为状态和继承分数，采用逐轮反向训练策略。

Result: 构建了包含超过10^9个节点的完整游戏树，并通过大规模自玩估计牌组公平值。

Conclusion: 该框架可扩展到其他多轮强化学习问题。

Abstract: Pasur is a fishing card game played over six rounds and is played similarly
to games such as Cassino and Scopa, and Bastra. This paper introduces a
CUDA-accelerated computational framework for simulating Pasur, emphasizing
efficient memory management. We use our framework to compute near-Nash
equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm
for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the
large size of its game tree. We handle rule complexity using PyTorch CUDA
tensors and to address the memory-intensive nature of the game, we decompose
the game tree into two key components: (1) actual game states, and (2)
inherited scores from previous rounds. We construct the Full Game Tree by
pairing card states with accumulated scores in the Unfolding Process. This
design reduces memory overhead by storing only essential strategy values and
node connections. To further manage computational complexity, we apply a
round-by-round backward training strategy, starting from the final round and
recursively propagating average utilities to earlier stages. Our approach
constructs the complete game tree, which on average consists of over $10^9$
nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model
to predict these strategies for use during gameplay. We then estimate the fair
value of each deck through large-scale self-play between equilibrium strategies
by simulating, for instance, 10,000 games per matchup, executed in parallel
using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms
where the action tree naturally decomposes into multiple rounds such as
turn-based strategy games or sequential trading decisions in financial markets.

</details>


### [242] [Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop](https://arxiv.org/abs/2508.06569)
*Lance Yao,Suman Samantray,Ayana Ghosh,Kevin Roccapriore,Libor Kovarik,Sarah Allec,Maxim Ziatdinov*

Main category: cs.AI

TL;DR: SciLink是一个开源的多智能体AI框架，旨在通过自动化链接实验观察、新颖性评估和理论模拟，在材料研究中实现偶然发现。


<details>
  <summary>Details</summary>
Motivation: 现代自主实验室虽然高效，但可能忽略偶然发现，SciLink旨在填补这一空白。

Method: 采用混合AI策略，结合机器学习模型和大型语言模型，将原始数据转化为可验证的科学主张，并评估其新颖性。

Result: SciLink在多种研究场景中表现出色，能够整合实时专家指导并提出后续实验建议。

Conclusion: SciLink不仅提高效率，还促进偶然发现，弥合自动化实验与开放式科学探索之间的差距。

Abstract: The history of science is punctuated by serendipitous discoveries, where
unexpected observations, rather than targeted hypotheses, opened new fields of
inquiry. While modern autonomous laboratories excel at accelerating hypothesis
testing, their optimization for efficiency risks overlooking these crucial,
unplanned findings. To address this gap, we introduce SciLink, an open-source,
multi-agent artificial intelligence framework designed to operationalize
serendipity in materials research by creating a direct, automated link between
experimental observation, novelty assessment, and theoretical simulations. The
framework employs a hybrid AI strategy where specialized machine learning
models perform quantitative analysis of experimental data, while large language
models handle higher-level reasoning. These agents autonomously convert raw
data from materials characterization techniques into falsifiable scientific
claims, which are then quantitatively scored for novelty against the published
literature. We demonstrate the framework's versatility across diverse research
scenarios, showcasing its application to atomic-resolution and hyperspectral
data, its capacity to integrate real-time human expert guidance, and its
ability to close the research loop by proposing targeted follow-up experiments.
By systematically analyzing all observations and contextualizing them, SciLink
provides a practical framework for AI-driven materials research that not only
enhances efficiency but also actively cultivates an environment ripe for
serendipitous discoveries, thereby bridging the gap between automated
experimentation and open-ended scientific exploration.

</details>


### [243] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: 本文提出了IRL-VLA框架，通过三阶段方法（模仿学习、逆强化学习奖励世界模型、PPO强化学习）解决VLA模型在自动驾驶中的开环和闭环训练问题，并在NAVSIM v2和CVPR2025挑战赛中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在自动驾驶中存在开环模仿学习的性能限制和闭环训练对高仿真传感器的依赖问题，亟需一种高效解决方案。

Method: 1. 模仿学习预训练VLA策略；2. 逆强化学习构建轻量级奖励世界模型；3. PPO强化学习优化规划性能。

Result: 在NAVSIM v2端到端驾驶基准中达到SOTA，CVPR2025挑战赛中获得亚军。

Conclusion: IRL-VLA框架为闭环自动驾驶中的VLA研究提供了高效解决方案，具有加速研究的潜力。

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [244] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: CountQA是一个新基准，用于评估多模态大语言模型（MLLMs）在复杂场景中的物体计数能力，揭示了其性能不足。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在物体计数方面表现不佳，缺乏真实场景的评估，限制了其实际应用。

Method: 提出CountQA基准，包含1,500个高密度、遮挡和杂乱的现实图像问答对，评估15种MLLMs。

Result: 表现最佳的模型准确率仅为42.9%，且随物体数量增加性能下降。

Conclusion: CountQA为改进MLLMs的计数能力提供了工具，促进更具数值和空间感知能力的模型发展。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [245] [Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis](https://arxiv.org/abs/2508.06668)
*Jessie Galasso*

Main category: cs.AI

TL;DR: 本文总结了形式概念分析（FCA）在变异性分析中的关键属性及其应用。


<details>
  <summary>Details</summary>
Motivation: FCA虽然是一种强大的知识表示和发现框架，但其数学基础文献使其在变异性分析中的应用不够直观。本文旨在填补这一空白。

Method: 通过筛选FCA框架中与变异性分析相关的关键属性，并探讨如何利用这些属性解释概念结构中的变异性信息。

Result: 明确了FCA中可用于变异性分析的关键属性及其应用方式。

Conclusion: 本文为FCA在变异性分析中的应用提供了实用指导，帮助更直观地利用其数学框架。

Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge
representation and discovery. It performs a hierarchical clustering over a set
of objects described by attributes, resulting in conceptual structures in which
objects are organized depending on the attributes they share. These conceptual
structures naturally highlight commonalities and variabilities among similar
objects by categorizing them into groups which are then arranged by similarity,
making it particularly appropriate for variability extraction and analysis.
Despite the potential of FCA, determining which of its properties can be
leveraged for variability-related tasks (and how) is not always
straightforward, partly due to the mathematical orientation of its foundational
literature. This paper attempts to bridge part of this gap by gathering a
selection of properties of the framework which are essential to variability
analysis, and how they can be used to interpret diverse variability information
within the resulting conceptual structures.

</details>


### [246] [Zero-Shot Cellular Trajectory Map Matching](https://arxiv.org/abs/2508.06674)
*Weijie Shi,Yue Cui,Hao Chen,Jiaming Li,Mengze Li,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.AI

TL;DR: 提出了一种基于像素的轨迹校准方法，用于零样本细胞轨迹地图匹配（CTMM），通过迁移地理空间知识和空间-时间感知模块提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖区域特定数据和ID特征，难以适应未探索区域，需开发无需额外训练的高精度CTMM方法。

Method: 结合高斯混合模型的VAE提取场景自适应特征，设计空间-时间感知模块捕获序列特征和位置不确定性，使用约束路径查找算法重建道路ID序列。

Result: 实验表明，模型在零样本CTMM中性能优于现有方法16.8%。

Conclusion: 该方法通过迁移知识和优化路径查找，显著提升了零样本CTMM的准确性和适应性。

Abstract: Cellular Trajectory Map-Matching (CTMM) aims to align cellular location
sequences to road networks, which is a necessary preprocessing in
location-based services on web platforms like Google Maps, including navigation
and route optimization. Current approaches mainly rely on ID-based features and
region-specific data to learn correlations between cell towers and roads,
limiting their adaptability to unexplored areas. To enable high-accuracy CTMM
without additional training in target regions, Zero-shot CTMM requires to
extract not only region-adaptive features, but also sequential and location
uncertainty to alleviate positioning errors in cellular data. In this paper, we
propose a pixel-based trajectory calibration assistant for zero-shot CTMM,
which takes advantage of transferable geospatial knowledge to calibrate
pixelated trajectory, and then guide the path-finding process at the road
network level. To enhance knowledge sharing across similar regions, a Gaussian
mixture model is incorporated into VAE, enabling the identification of
scenario-adaptive experts through soft clustering. To mitigate high positioning
errors, a spatial-temporal awareness module is designed to capture sequential
features and location uncertainty, thereby facilitating the inference of
approximate user positions. Finally, a constrained path-finding algorithm is
employed to reconstruct the road ID sequence, ensuring topological validity
within the road network. This process is guided by the calibrated trajectory
while optimizing for the shortest feasible path, thus minimizing unnecessary
detours. Extensive experiments demonstrate that our model outperforms existing
methods in zero-shot CTMM by 16.8\%.

</details>


### [247] [Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets](https://arxiv.org/abs/2508.06706)
*Jaikrishna Manojkumar Patil,Nathaniel Lee,Al Mehdi Saadat Chowdhury,YooJung Choi,Paulo Shakarian*

Main category: cs.AI

TL;DR: 论文提出了一种基于规则上下文和概率电路的知识图谱补全方法，显著减少了规则数量，同时保持或超越基线性能。


<details>
  <summary>Details</summary>
Motivation: 规则方法需要大量规则才能达到竞争性能，但过多的规则会降低可解释性。

Method: 从训练数据中发现规则上下文，并利用概率分布（概率电路）快速实现完整规则集的性能。

Result: 规则数量减少70-96%，性能最高提升31倍，保留基线91%的峰值性能。

Conclusion: 该方法在8个基准数据集上验证有效，为基于规则的推理提供了新思路。

Abstract: Rule-based methods for knowledge graph completion provide explainable results
but often require a significantly large number of rules to achieve competitive
performance. This can hinder explainability due to overwhelmingly large rule
sets. We discover rule contexts (meaningful subsets of rules that work
together) from training data and use learned probability distribution (i.e.
probabilistic circuits) over these rule contexts to more rapidly achieve
performance of the full rule set. Our approach achieves a 70-96% reduction in
number of rules used while outperforming baseline by up to 31$\times$ when
using equivalent minimal number of rules and preserves 91% of peak baseline
performance even when comparing our minimal rule sets against baseline's full
rule sets. We show that our framework is grounded in well-known semantics of
probabilistic logic, does not require independence assumptions, and that our
tractable inference procedure provides both approximate lower bounds and exact
probability of a given query. The efficacy of our method is validated by
empirical studies on 8 standard benchmark datasets where we show competitive
performance by using only a fraction of the rules required by AnyBURL's
standard inference method, the current state-of-the-art for rule-based
knowledge graph completion. This work may have further implications for general
probabilistic reasoning over learned sets of rules.

</details>


### [248] [GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning](https://arxiv.org/abs/2508.06716)
*Blair Johnson,Clayton Kerce,Faramarz Fekri*

Main category: cs.AI

TL;DR: GLIDR是一种可微分的规则学习方法，通过更灵活的语法和消息传递推理算法，显著提升了知识图谱任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有链式规则结构限制了性能和可解释性，GLIDR旨在解决这一问题。

Method: 使用可微分消息传递推理算法，支持分支和循环等复杂规则结构。

Result: 在知识图谱补全任务中表现优于现有方法，且规则提取后仍保持高性能。

Conclusion: GLIDR是一种高效、灵活且鲁棒的规则学习方法，适用于多模态数据。

Abstract: Differentiable inductive logic programming (ILP) techniques have proven
effective at finding approximate rule-based solutions to link prediction and
node classification problems on knowledge graphs; however, the common
assumption of chain-like rule structure can hamper the performance and
interpretability of existing approaches. We introduce GLIDR, a differentiable
rule learning method that models the inference of logic rules with more
expressive syntax than previous methods. GLIDR uses a differentiable message
passing inference algorithm that generalizes previous chain-like rule learning
methods to allow rules with features like branches and cycles. GLIDR has a
simple and expressive rule search space which is parameterized by a limit on
the maximum number of free variables that may be included in a rule. Explicit
logic rules can be extracted from the weights of a GLIDR model for use with
symbolic solvers. We demonstrate that GLIDR can significantly outperform
existing rule learning methods on knowledge graph completion tasks and even
compete with embedding methods despite the inherent disadvantage of being a
structure-only prediction method. We show that rules extracted from GLIDR
retain significant predictive performance, and that GLIDR is highly robust to
training data noise. Finally, we demonstrate that GLIDR can be chained with
deep neural networks and optimized end-to-end for rule learning on arbitrary
data modalities.

</details>


### [249] [ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search](https://arxiv.org/abs/2508.06736)
*Alican Yilmaz,Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: ParBalans通过并行化Balans算法，提升混合整数规划问题的求解效率，性能接近商业求解器Gurobi。


<details>
  <summary>Details</summary>
Motivation: 混合整数规划问题计算资源需求高，并行化是加速求解的关键策略。

Method: 扩展Balans算法为ParBalans，利用求解器和算法级并行性。

Result: 实验显示ParBalans在复杂问题上性能接近Gurobi。

Conclusion: ParBalans为混合整数规划提供了一种高效的并行求解方法。

Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial
computational resources due to their combinatorial nature. Parallelization has
emerged as a critical strategy to accelerate solution times and enhance
scalability to tackle large, complex instances. This paper investigates the
parallelization capabilities of Balans, a recently proposed multi-armed
bandits-based adaptive large neighborhood search for MIPs. While Balans's
modular architecture inherently supports parallel exploration of diverse
parameter configurations, this potential has not been thoroughly examined. To
address this gap, we introduce ParBalans, an extension that leverages both
solver-level and algorithmic-level parallelism to improve performance on
challenging MIP instances. Our experimental results demonstrate that ParBalans
exhibits competitive performance compared to the state-of-the-art commercial
solver Gurobi, particularly on hard optimization benchmarks.

</details>


### [250] [Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism](https://arxiv.org/abs/2508.06746)
*Xin Tang,Qian Chen,Fengshun Li,Youchun Gong,Yinqiu Liu,Wen Tian,Shaowen Qin,Xiaohuan Li*

Main category: cs.AI

TL;DR: 本文提出了一种结合图扩散策略优化（GDPO）和Stackelberg博弈（SG）激励机制的无人机网络框架，以解决动态移动性和暴露风险带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着无人机网络在敏感应用中的需求增长，确保可靠连接和隐蔽通信变得至关重要。

Method: 采用GDPO方法生成稀疏但连接良好的拓扑结构，并结合SG激励机制引导无人机选择支持合作的转发行为和邻居链接。

Result: 实验验证了框架在模型收敛性、拓扑生成质量和隐蔽通信性能提升方面的有效性。

Conclusion: 该框架能够灵活适应动态环境，提升无人机网络的隐蔽通信能力。

Abstract: With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in
sensitive applications, such as urban monitoring, emergency response, and
secure sensing, ensuring reliable connectivity and covert communication has
become increasingly vital. However, dynamic mobility and exposure risks pose
significant challenges. To tackle these challenges, this paper proposes a
self-organizing UAV network framework combining Graph Diffusion-based Policy
Optimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The
GDPO method uses generative AI to dynamically generate sparse but
well-connected topologies, enabling flexible adaptation to changing node
distributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game
(SG)-based incentive mechanism guides self-interested UAVs to choose relay
behaviors and neighbor links that support cooperation and enhance covert
communication. Extensive experiments are conducted to validate the
effectiveness of the proposed framework in terms of model convergence, topology
generation quality, and enhancement of covert communication performance.

</details>


### [251] [Pushing the Envelope of LLM Inference on AI-PC](https://arxiv.org/abs/2508.06753)
*Evangelos Georganas,Dhiraj Kalamkar,Alexander Heinecke*

Main category: cs.AI

TL;DR: 论文提出了一种针对超低比特LLM模型（1/1.58/2比特）的优化微内核设计，集成到PyTorch-TPP框架中，显著提升了推理效率。


<details>
  <summary>Details</summary>
Motivation: 超低比特LLM模型在资源受限环境中具有潜力，但现有推理运行时的计算效率未被充分探索。

Method: 设计并实现针对现代CPU优化的1比特和2比特微内核，集成到PyTorch-TPP框架中。

Result: 2比特模型推理性能比当前SOTA运行时bitnet.cpp快2.2倍，比16比特模型快7倍。

Conclusion: 优化后的运行时推动了AI PC和边缘设备上LLM推理的发展，为超低比特模型的部署铺平了道路。

Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the
perplexity and end-task performance of their full-precision counterparts using
the same model size, is ushering in a new era of LLM inference for
resource-constrained environments such as edge devices and AI PCs. While these
quantization advances promise models that are more cost-effective in terms of
latency, memory, throughput, and energy consumption, the computational
efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)
used to deploy them remains underexplored. In this work, we take a bottom-up
approach: we first design and implement 1-bit and 2-bit microkernels optimized
for modern CPUs, achieving peak computational efficiency across a variety of
CPU platforms. We integrate these microkernels into a state-of-the-art LLM
inference framework, namely PyTorch-TPP, and present end-to-end inference
results with 2-bit models that outperform the current SOTA runtime bitnet.cpp
by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model
inference. Our optimized runtime advances the state of LLM inference on AI PCs
and edge devices, paving the way for efficient deployment of ultra-low-bit LLM
models.

</details>


### [252] [A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks](https://arxiv.org/abs/2508.06754)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 提出了一种模块化提示框架，支持更安全、更自适应地使用大型语言模型（LLM）于动态、用户中心的任务。


<details>
  <summary>Details</summary>
Motivation: 基于人类学习理论（如最近发展区ZPD），旨在提升LLM在动态任务中的适应性和安全性。

Method: 结合自然语言边界提示与控制模式，采用模糊脚手架逻辑和适应规则，无需微调或外部协调。

Result: 在智能辅导模拟中，框架显著提升了脚手架质量、适应性和教学对齐，优于标准提示基线。

Conclusion: 该框架不仅适用于教育领域，还可扩展至其他交互密集型场景，为不确定或动态环境中的LLM行为提供可重用方法。

Abstract: We introduce a modular prompting framework that supports safer and more
adaptive use of large language models (LLMs) across dynamic, user-centered
tasks. Grounded in human learning theory, particularly the Zone of Proximal
Development (ZPD), our method combines a natural language boundary prompt with
a control schema encoded with fuzzy scaffolding logic and adaptation rules.
This architecture enables LLMs to modulate behavior in response to user state
without requiring fine-tuning or external orchestration. In a simulated
intelligent tutoring setting, the framework improves scaffolding quality,
adaptivity, and instructional alignment across multiple models, outperforming
standard prompting baselines. Evaluation is conducted using rubric-based LLM
graders at scale. While initially developed for education, the framework has
shown promise in other interaction-heavy domains, such as procedural content
generation for games. Designed for safe deployment, it provides a reusable
methodology for structuring interpretable, goal-aligned LLM behavior in
uncertain or evolving contexts.

</details>


### [253] [Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation](https://arxiv.org/abs/2508.06823)
*Xuan Zhao,Jun Tao*

Main category: cs.AI

TL;DR: 提出了一种基于自然语言交互的框架，通过强化学习和CLIP Score机制优化体数据探索的视点选择。


<details>
  <summary>Details</summary>
Motivation: 体数据探索对科学数据集解释至关重要，但缺乏领域知识或3D导航经验的用户难以选择最佳视点。

Method: 将体数据块编码以区分结构，结合CLIP Score提供语义信息，通过强化学习框架搜索与用户意图一致的视点。

Result: 方法自动化视点选择，提升体数据导航效率和复杂科学现象的可解释性。

Conclusion: 该框架通过自然语言交互和语义引导，显著改善了体数据探索的效率和用户体验。

Abstract: Exploring volumetric data is crucial for interpreting scientific datasets.
However, selecting optimal viewpoints for effective navigation can be
challenging, particularly for users without extensive domain expertise or
familiarity with 3D navigation. In this paper, we propose a novel framework
that leverages natural language interaction to enhance volumetric data
exploration. Our approach encodes volumetric blocks to capture and
differentiate underlying structures. It further incorporates a CLIP Score
mechanism, which provides semantic information to the blocks to guide
navigation. The navigation is empowered by a reinforcement learning framework
that leverage these semantic cues to efficiently search for and identify
desired viewpoints that align with the user's intent. The selected viewpoints
are evaluated using CLIP Score to ensure that they best reflect the user
queries. By automating viewpoint selection, our method improves the efficiency
of volumetric data navigation and enhances the interpretability of complex
scientific phenomena.

</details>


### [254] [Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges](https://arxiv.org/abs/2508.06832)
*Haifeng Li,Wang Guo,Haiyang Wu,Mengwei Wu,Jipeng Zhang,Qing Zhu,Yu Liu,Xin Huang,Chao Tao*

Main category: cs.AI

TL;DR: 本文提出从视觉为中心转向语言为中心的遥感图像解释范式，借鉴全局工作空间理论，将大语言模型作为认知中心，整合感知、任务、知识和行动空间，实现统一理解、推理和决策。


<details>
  <summary>Details</summary>
Motivation: 现有视觉为中心的模型在多模态推理、语义抽象和交互决策方面存在局限，缺乏统一的理论框架解释语言在认知中的作用。

Method: 提出语言为中心的框架，利用大语言模型作为认知中心，解决多模态统一表示、知识关联、推理和决策等核心挑战。

Result: 构建了全局工作空间驱动的解释机制，并总结了语言为中心解决方案如何应对各挑战。

Conclusion: 为下一代遥感解释系统提供了概念基础，并制定了认知驱动智能地理空间分析的路线图。

Abstract: The mainstream paradigm of remote sensing image interpretation has long been
dominated by vision-centered models, which rely on visual features for semantic
understanding. However, these models face inherent limitations in handling
multi-modal reasoning, semantic abstraction, and interactive decision-making.
While recent advances have introduced Large Language Models (LLMs) into remote
sensing workflows, existing studies primarily focus on downstream applications,
lacking a unified theoretical framework that explains the cognitive role of
language. This review advocates a paradigm shift from vision-centered to
language-centered remote sensing interpretation. Drawing inspiration from the
Global Workspace Theory (GWT) of human cognition, We propose a
language-centered framework for remote sensing interpretation that treats LLMs
as the cognitive central hub integrating perceptual, task, knowledge and action
spaces to enable unified understanding, reasoning, and decision-making. We
first explore the potential of LLMs as the central cognitive component in
remote sensing interpretation, and then summarize core technical challenges,
including unified multimodal representation, knowledge association, and
reasoning and decision-making. Furthermore, we construct a global
workspace-driven interpretation mechanism and review how language-centered
solutions address each challenge. Finally, we outline future research
directions from four perspectives: adaptive alignment of multimodal data, task
understanding under dynamic knowledge constraints, trustworthy reasoning, and
autonomous interaction. This work aims to provide a conceptual foundation for
the next generation of remote sensing interpretation systems and establish a
roadmap toward cognition-driven intelligent geospatial analysis.

</details>


### [255] [Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.06836)
*Xutong Zhao,Yaqi Xie*

Main category: cs.AI

TL;DR: 论文提出了一种多级优势信用分配方法（MACA），用于解决多智能体强化学习中的信用分配问题，通过显式反事实推理评估不同协作级别下智能体的贡献。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的信用分配问题复杂，尤其是任务多样性导致智能体协作方式和奖励分配多样化。需要一种方法能评估不同协作级别下智能体的贡献。

Method: 提出MACA方法，通过多级优势函数（个体、联合和相关动作）进行信用分配，并利用基于注意力的框架识别智能体间的相关性。

Result: 在Starcraft v1&v2任务上的实验表明，MACA在复杂信用分配场景中表现优异。

Conclusion: MACA通过多级优势函数和注意力机制，有效解决了多智能体协作中的信用分配问题，适用于多样化任务。

Abstract: Cooperative multi-agent reinforcement learning (MARL) aims to coordinate
multiple agents to achieve a common goal. A key challenge in MARL is credit
assignment, which involves assessing each agent's contribution to the shared
reward. Given the diversity of tasks, agents may perform different types of
coordination, with rewards attributed to diverse and often overlapping agent
subsets. In this work, we formalize the credit assignment level as the number
of agents cooperating to obtain a reward, and address scenarios with multiple
coexisting levels. We introduce a multi-level advantage formulation that
performs explicit counterfactual reasoning to infer credits across distinct
levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures
agent contributions at multiple levels by integrating advantage functions that
reason about individual, joint, and correlated actions. Utilizing an
attention-based framework, MACA identifies correlated agent relationships and
constructs multi-level advantages to guide policy learning. Comprehensive
experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior
performance, underscoring its efficacy in complex credit assignment scenarios.

</details>


### [256] [MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams](https://arxiv.org/abs/2508.06851)
*Pengfei Zhou,Xiaopeng Peng,Fanrui Zhang,Zhaopan Xu,Jiaxin Ai,Yansheng Qiu,Chuanhao Li,Zhen Li,Ming Li,Yukang Feng,Jianwen Sun,Haoquan Zhang,Zizhen Li,Xiaofeng Mao,Zekai Li,Wangbo Zhao,Kai Wang,Xiaojun Chang,Wenqi Shao,Yang You,Kaipeng Zhang*

Main category: cs.AI

TL;DR: MDK12-Bench是一个多学科大规模基准测试，用于评估多模态大语言模型（MLLMs）在多个维度的表现，包括难度、时间变化、上下文变化和知识驱动推理。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs的评估基准存在规模小、覆盖窄和知识无结构化的问题，无法全面衡量模型能力。

Method: 提出MDK12-Bench基准，基于K-12真实考试数据构建，包含141K实例和6,225个知识点，采用动态评估框架和知识参考增强生成（KP-RAG）。

Result: 发现当前MLLMs在多个方面存在局限性，为提升模型鲁棒性和可解释性提供了指导。

Conclusion: MDK12-Bench为MLLMs的全面评估提供了新工具，并推动了AI辅助教育的发展。

Abstract: Multimodal large language models (MLLMs), which integrate language and visual
cues for problem-solving, are crucial for advancing artificial general
intelligence (AGI). However, current benchmarks for measuring the intelligence
of MLLMs suffer from limited scale, narrow coverage, and unstructured
knowledge, offering only static and undifferentiated evaluations. To bridge
this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark
built from real-world K-12 exams spanning six disciplines with 141K instances
and 6,225 knowledge points organized in a six-layer taxonomy. Covering five
question formats with difficulty and year annotations, it enables comprehensive
evaluation to capture the extent to which MLLMs perform over four dimensions:
1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,
and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation
framework that introduces unfamiliar visual, textual, and question form shifts
to challenge model generalization while improving benchmark objectivity and
longevity by mitigating data contamination. We further evaluate knowledge-point
reference-augmented generation (KP-RAG) to examine the role of knowledge in
problem-solving. Key findings reveal limitations in current MLLMs in multiple
aspects and provide guidance for enhancing model robustness, interpretability,
and AI-assisted education.

</details>


### [257] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: 论文提出MP-Bench数据集和MMLM模型，解决端到端AI天气预测中的三大挑战，包括数据稀缺、高维气象数据与文本对齐问题，以及多模态模型处理能力不足。


<details>
  <summary>Details</summary>
Motivation: 当前严重天气预警依赖人工专家解读，存在主观性和操作负担，AI技术的发展为自动化预测提供了新可能。

Method: 构建MP-Bench数据集，开发MMLM模型，结合自适应融合模块处理4D气象数据。

Result: MMLM在MP-Bench上表现优异，验证了其在严重天气理解中的有效性。

Conclusion: MMLM为自动化AI天气预测系统迈出关键一步，代码和数据集将公开。

Abstract: Timely and accurate severe weather warnings are critical for disaster
mitigation. However, current forecasting systems remain heavily reliant on
manual expert interpretation, introducing subjectivity and significant
operational burdens. With the rapid development of AI technologies, the
end-to-end "AI weather station" is gradually emerging as a new trend in
predicting severe weather events. Three core challenges impede the development
of end-to-end AI severe weather system: (1) scarcity of severe weather event
samples; (2) imperfect alignment between high-dimensional meteorological data
and textual warnings; (3) existing multimodal language models are unable to
handle high-dimensional meteorological data and struggle to fully capture the
complex dependencies across temporal sequences, vertical pressure levels, and
spatial dimensions. To address these challenges, we introduce MP-Bench, the
first large-scale temporal multimodal dataset for severe weather events
prediction, comprising 421,363 pairs of raw multi-year meteorological data and
corresponding text caption, covering a wide range of severe weather scenarios
across China. On top of this dataset, we develop a meteorology multimodal large
model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is
designed to accommodate the unique characteristics of 4D meteorological data
flow, incorporating three plug-and-play adaptive fusion modules that enable
dynamic feature extraction and integration across temporal sequences, vertical
pressure layers, and spatial dimensions. Extensive experiments on MP-Bench
demonstrate that MMLM performs exceptionally well across multiple tasks,
highlighting its effectiveness in severe weather understanding and marking a
key step toward realizing automated, AI-driven weather forecasting systems. Our
source code and dataset will be made publicly available.

</details>


### [258] [Pushdown Reward Machines for Reinforcement Learning](https://arxiv.org/abs/2508.06894)
*Giovanni Varricchione,Toryn Q. Klassen,Natasha Alechina,Mehdi Dastani,Brian Logan,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 论文提出了推下奖励机（pdRMs），基于确定性下推自动机扩展了奖励机（RMs），能够识别和奖励确定性上下文无关语言表示的行为，提高了表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励机（RMs）只能处理正则语言表示的行为，限制了其在更复杂任务中的应用。推下奖励机（pdRMs）旨在扩展表达能力，支持更复杂的行为模式。

Method: 提出了两种基于pdRM的策略：一种可以访问整个堆栈，另一种只能访问堆栈顶部的k个符号。并提供了检查两种策略在给定环境下是否获得相同最优奖励的方法。

Result: 理论分析表明pdRMs的表达能力更强，并提供了学习问题的空间复杂度结果。实验验证了pdRMs在训练代理执行确定性上下文无关语言任务中的有效性。

Conclusion: 推下奖励机（pdRMs）扩展了奖励机的表达能力，能够处理更复杂的行为模式，为强化学习提供了新的工具。

Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian)
reward functions for reinforcement learning (RL). RMs can reward any behaviour
representable in regular languages and, when paired with RL algorithms that
exploit RM structure, have been shown to significantly improve sample
efficiency in many domains. In this work, we present pushdown reward machines
(pdRMs), an extension of reward machines based on deterministic pushdown
automata. pdRMs can recognize and reward temporally extended behaviours
representable in deterministic context-free languages, making them more
expressive than reward machines. We introduce two variants of pdRM-based
policies, one which has access to the entire stack of the pdRM, and one which
can only access the top $k$ symbols (for a given constant $k$) of the stack. We
propose a procedure to check when the two kinds of policies (for a given
environment, pdRM, and constant $k$) achieve the same optimal expected reward.
We then provide theoretical results establishing the expressive power of pdRMs,
and space complexity results about the proposed learning problems. Finally, we
provide experimental results showing how agents can be trained to perform tasks
representable in deterministic context-free languages using pdRMs.

</details>


### [259] [GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization](https://arxiv.org/abs/2508.06899)
*Yanchen Deng,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 论文提出了一种改进的分布式约束优化问题（DCOP）局部搜索方法DGLS，解决了GDBA的局限性，并在实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: GDBA在解决DCOP时容易陷入局部最优，且性能提升有限，需要改进其约束违反条件、惩罚积累和更新机制。

Method: 提出DGLS框架，包括自适应违反条件、惩罚蒸发机制和同步更新方案，以优化局部搜索。

Result: DGLS在标准基准测试中表现优异，尤其在结构化问题上显著优于现有方法（3.77%--66.3%）。

Conclusion: DGLS通过系统性改进，显著提升了DCOP求解的性能和稳定性。

Abstract: Local search is an important class of incomplete algorithms for solving
Distributed Constraint Optimization Problems (DCOPs) but it often converges to
poor local optima. While GDBA provides a comprehensive rule set to escape
premature convergence, its empirical benefits remain marginal on general-valued
problems. In this work, we systematically examine GDBA and identify three
factors that potentially lead to its inferior performance, i.e.,
over-aggressive constraint violation conditions, unbounded penalty
accumulation, and uncoordinated penalty updates. To address these issues, we
propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs
that incorporates an adaptive violation condition to selectively penalize
constraints with high cost, a penalty evaporation mechanism to control the
magnitude of penalization, and a synchronization scheme for coordinated penalty
updates. We theoretically show that the penalty values are bounded, and agents
play a potential game in our DGLS. Our extensive empirical results on various
standard benchmarks demonstrate the great superiority of DGLS over
state-of-the-art baselines. Particularly, compared to Damped Max-sum with high
damping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance
on general-valued problems, and outperforms it by significant margins
(\textbf{3.77\%--66.3\%}) on structured problems in terms of anytime results.

</details>


### [260] [Automated Formalization via Conceptual Retrieval-Augmented LLMs](https://arxiv.org/abs/2508.06931)
*Wangyue Lu,Lun Du,Sirui Li,Ke Weng,Haozhe Sun,Hengyu Liu,Minghe Yu,Tiancheng Zhang,Ge Yu*

Main category: cs.AI

TL;DR: CRAMF框架通过检索数学概念定义，提升LLM自动形式化的准确性，解决了模型幻觉和语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 交互式定理证明器的手动形式化耗时且依赖专家知识，自动形式化面临模型幻觉和语义鸿沟的挑战。

Method: 提出CRAMF框架，利用检索增强生成技术，从Mathlib4构建知识库，并设计双通道混合检索策略。

Result: 在多个基准测试中，CRAMF显著提升翻译准确性，最高相对改进达62.1%。

Conclusion: CRAMF为自动形式化提供了有效解决方案，尤其在数学领域表现突出。

Abstract: Interactive theorem provers (ITPs) require manual formalization, which is
labor-intensive and demands expert knowledge. While automated formalization
offers a potential solution, it faces two major challenges: model hallucination
(e.g., undefined predicates, symbol misuse, and version incompatibility) and
the semantic gap caused by ambiguous or missing premises in natural language
descriptions. To address these issues, we propose CRAMF, a Concept-driven
Retrieval-Augmented Mathematical Formalization framework. CRAMF enhances
LLM-based autoformalization by retrieving formal definitions of core
mathematical concepts, providing contextual grounding during code generation.
However, applying retrieval-augmented generation (RAG) in this setting is
non-trivial due to the lack of structured knowledge bases, the polymorphic
nature of mathematical concepts, and the high precision required in formal
retrieval. We introduce a framework for automatically constructing a
concept-definition knowledge base from Mathlib4, the standard mathematical
library for the Lean 4 theorem prover, indexing over 26,000 formal definitions
and 1,000+ core mathematical concepts. To address conceptual polymorphism, we
propose contextual query augmentation with domain- and application-level
signals. In addition, we design a dual-channel hybrid retrieval strategy with
reranking to ensure accurate and relevant definition retrieval. Experiments on
miniF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that
CRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding
consistent improvements in translation accuracy, achieving up to 62.1% and an
average of 29.9% relative improvement.

</details>


### [261] [Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction](https://arxiv.org/abs/2508.06939)
*Hiba Najjar,Deepak Pathak,Marlon Nuske,Andreas Dengel*

Main category: cs.AI

TL;DR: 该研究利用Transformer模型的多模态学习能力，结合自注意力机制，提出两种特征归因方法（AR和GA），并比较其与Shapley方法的性能。结果显示Transformer模型在作物产量预测任务中表现更优，且AR方法在时间归因上更可靠。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在农业中具有广泛应用，但模型可解释性常被忽视。本研究旨在利用Transformer的固有可解释性，提升多模态学习网络的可解释性，特别是在子田块级别的作物产量预测任务中。

Method: 采用Transformer模型，结合四种输入模态（多光谱卫星数据、天气时间序列、地形高程图和土壤属性），提出Attention Rollout（AR）和Generic Attention（GA）两种特征归因方法，并与Shapley Value Sampling（SVS）进行比较。

Result: Transformer模型在子田块和田块级别的R2分数分别比卷积和循环网络高0.10和0.04。AR方法在时间归因上比GA和SVS更可靠。

Conclusion: Transformer模型在多模态学习中表现优异，AR方法提供了更稳健的时间归因，有助于结合农学知识解释结果。

Abstract: Multimodal learning enables various machine learning tasks to benefit from
diverse data sources, effectively mimicking the interplay of different factors
in real-world applications, particularly in agriculture. While the
heterogeneous nature of involved data modalities may necessitate the design of
complex architectures, the model interpretability is often overlooked. In this
study, we leverage the intrinsic explainability of Transformer-based models to
explain multimodal learning networks, focusing on the task of crop yield
prediction at the subfield level. The large datasets used cover various crops,
regions, and years, and include four different input modalities: multispectral
satellite and weather time series, terrain elevation maps and soil properties.
Based on the self-attention mechanism, we estimate feature attributions using
two methods, namely the Attention Rollout (AR) and Generic Attention (GA), and
evaluate their performance against Shapley-based model-agnostic estimations,
Shapley Value Sampling (SVS). Additionally, we propose the Weighted Modality
Activation (WMA) method to assess modality attributions and compare it with SVS
attributions. Our findings indicate that Transformer-based models outperform
other architectures, specifically convolutional and recurrent networks,
achieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field
levels, respectively. AR is shown to provide more robust and reliable temporal
attributions, as confirmed through qualitative and quantitative evaluation,
compared to GA and SVS values. Information about crop phenology stages was
leveraged to interpret the explanation results in the light of established
agronomic knowledge. Furthermore, modality attributions revealed varying
patterns across the two methods compared.[...]

</details>


### [262] [Large Language Models Do Not Simulate Human Psychology](https://arxiv.org/abs/2508.06950)
*Sarah Schröder,Thekla Morgenroth,Ulrike Kuhl,Valerie Vaquet,Benjamin Paaßen*

Main category: cs.AI

TL;DR: 论文警告不要用LLMs替代人类参与者进行心理学研究，指出LLMs无法模拟人类心理，并通过实验证明其不可靠性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能够模拟人类心理学，以替代人类参与心理学研究。

Method: 通过概念论证和实验证据（如微小语言变化导致LLMs与人类反应的显著差异）来验证LLMs的不可靠性。

Result: 实验显示LLMs（包括专门优化的CENTAUR模型）与人类反应存在显著差异，且不同LLMs对新问题的反应不一致。

Conclusion: LLMs无法模拟人类心理学，心理学研究者应将其视为需要针对每个新应用验证的工具。

Abstract: Large Language Models (LLMs),such as ChatGPT, are increasingly used in
research, ranging from simple writing assistance to complex data annotation
tasks. Recently, some research has suggested that LLMs may even be able to
simulate human psychology and can, hence, replace human participants in
psychological studies. We caution against this approach. We provide conceptual
arguments against the hypothesis that LLMs simulate human psychology. We then
present empiric evidence illustrating our arguments by demonstrating that
slight changes to wording that correspond to large changes in meaning lead to
notable discrepancies between LLMs' and human responses, even for the recent
CENTAUR model that was specifically fine-tuned on psychological responses.
Additionally, different LLMs show very different responses to novel items,
further illustrating their lack of reliability. We conclude that LLMs do not
simulate human psychology and recommend that psychological researchers should
treat LLMs as useful but fundamentally unreliable tools that need to be
validated against human responses for every new application.

</details>


### [263] [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
*Keyu Li,Mohan Jiang,Dayuan Fu,Yunze Wu,Xiangkun Hu,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 论文提出了DatasetResearch基准，评估AI代理在发现和合成数据集方面的能力，揭示了当前技术与完美数据集发现之间的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，数据可用性成为AI发展的瓶颈，而许多有价值的数据集分散在不同平台。研究旨在探索AI代理是否能超越传统搜索，实现自主需求驱动的数据发现。

Method: 研究引入了DatasetResearch基准，评估AI代理在208个真实需求中的表现，采用三维评估框架分析搜索代理和合成代理的能力。

Result: 结果显示，即使是先进的深度研究系统在挑战性子集上仅得22分，暴露了当前能力的不足。搜索代理在知识任务中表现优异，而合成代理在推理任务中占优，但两者在“极端案例”中均失败。

Conclusion: 研究为数据集发现代理建立了首个严格基准，指明了未来AI系统的发展方向，并为下一代自改进AI系统奠定了基础。

Abstract: The rapid advancement of large language models has fundamentally shifted the
bottleneck in AI development from computational power to data availability-with
countless valuable datasets remaining hidden across specialized repositories,
research appendices, and domain platforms. As reasoning capabilities and deep
research methodologies continue to evolve, a critical question emerges: can AI
agents transcend conventional search to systematically discover any dataset
that meets specific user requirements, enabling truly autonomous demand-driven
data curation? We introduce DatasetResearch, the first comprehensive benchmark
evaluating AI agents' ability to discover and synthesize datasets from 208
real-world demands across knowledge-intensive and reasoning-intensive tasks.
Our tri-dimensional evaluation framework reveals a stark reality: even advanced
deep research systems achieve only 22% score on our challenging
DatasetResearch-pro subset, exposing the vast gap between current capabilities
and perfect dataset discovery. Our analysis uncovers a fundamental
dichotomy-search agents excel at knowledge tasks through retrieval breadth,
while synthesis agents dominate reasoning challenges via structured
generation-yet both catastrophically fail on "corner cases" outside existing
distributions. These findings establish the first rigorous baseline for dataset
discovery agents and illuminate the path toward AI systems capable of finding
any dataset in the digital universe. Our benchmark and comprehensive analysis
provide the foundation for the next generation of self-improving AI systems and
are publicly available at https://github.com/GAIR-NLP/DatasetResearch.

</details>


### [264] [MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair](https://arxiv.org/abs/2508.06963)
*Changqing Li,Tianlin Li,Xiaohan Zhang,Aishan Liu,Li Pan*

Main category: cs.AI

TL;DR: MASteer是一个基于表示工程的端到端框架，用于修复大语言模型（LLMs）的可信度问题，通过自动生成样本和自适应策略选择，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有修复方法（如SFT和RLHF）成本高且慢，而提示工程缺乏鲁棒性和可扩展性，需要更轻量、自动化的解决方案。

Method: MASteer结合AutoTester（多智能体生成样本）和AutoRepairer（自适应策略选择），实现无训练的可信度修复。

Result: 实验显示MASteer在LLaMA-3.1-8B-Chat和Qwen-3-8B-Chat上分别提升15.36%和4.21%的指标。

Conclusion: MASteer具有鲁棒性、泛化性和实用价值，为高效可信度修复提供了可行方案。

Abstract: Large Language Models (LLMs) face persistent and evolving trustworthiness
issues, motivating developers to seek automated and flexible repair methods
that enable convenient deployment across diverse scenarios. Existing repair
methods like supervised fine-tuning (SFT) and reinforcement learning with human
feedback (RLHF) are costly and slow, while prompt engineering lacks robustness
and scalability. Representation engineering, which steers model behavior by
injecting targeted concept vectors during inference, offers a lightweight,
training-free alternative. However, current approaches depend on manually
crafted samples and fixed steering strategies, limiting automation and
adaptability. To overcome these challenges, we propose MASteer, the first
end-to-end framework for trustworthiness repair in LLMs based on representation
engineering. MASteer integrates two core components: AutoTester, a multi-agent
system that generates diverse, high-quality steer samples tailored to developer
needs; and AutoRepairer, which constructs adaptive steering strategies with
anchor vectors for automated, context-aware strategy selection during
inference. Experiments on standard and customized trustworthiness tasks show
MASteer consistently outperforms baselines, improving metrics by 15.36% on
LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model
capabilities. MASteer demonstrates strong robustness, generalization, and
practical value for scalable, efficient trustworthiness repair.

</details>


### [265] [DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning](https://arxiv.org/abs/2508.06972)
*Dan Ivanov,Tristan Freiberg,Haruna Isah*

Main category: cs.AI

TL;DR: DSperse是一个模块化框架，用于分布式机器学习推理，通过战略性的加密验证实现高效验证。


<details>
  <summary>Details</summary>
Motivation: 在分布式零知识机器学习的新兴范式中，避免全模型电路化的高成本和刚性，实现针对性验证。

Method: 通过验证选定的子计算（“切片”），支持局部零知识证明，并通过审计、复制或经济激励确保全局一致性。

Result: 评估了多种证明系统，报告了内存使用、运行时间及切片与非切片配置下的电路行为。

Conclusion: DSperse通过灵活对齐证明边界与模型逻辑结构，支持可扩展的针对性验证策略。

Abstract: DSperse is a modular framework for distributed machine learning inference
with strategic cryptographic verification. Operating within the emerging
paradigm of distributed zero-knowledge machine learning, DSperse avoids the
high cost and rigidity of full-model circuitization by enabling targeted
verification of strategically chosen subcomputations. These verifiable
segments, or "slices", may cover part or all of the inference pipeline, with
global consistency enforced through audit, replication, or economic incentives.
This architecture supports a pragmatic form of trust minimization, localizing
zero-knowledge proofs to the components where they provide the greatest value.
We evaluate DSperse using multiple proving systems and report empirical results
on memory usage, runtime, and circuit behavior under sliced and unsliced
configurations. By allowing proof boundaries to align flexibly with the model's
logical structure, DSperse supports scalable, targeted verification strategies
suited to diverse deployment needs.

</details>


### [266] [Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model](https://arxiv.org/abs/2508.06980)
*Aswin Paul,Moein Khajehnejad,Forough Habibollahi,Brett J. Kagan,Adeel Razi*

Main category: cs.AI

TL;DR: 论文提出了一种基于主动推理的框架，用于模拟生物神经元网络中的决策过程，为可解释AI提供了生物学基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI的快速发展，理解自主代理的有目的行为基础对开发安全高效系统至关重要。生物神经元网络可能提供高效率和可解释性。

Method: 采用主动推理理论，结合实验生成的模型，模拟游戏环境中的决策过程。

Result: 结果显示代理能够学习，揭示了记忆学习和预测规划在智能决策中的作用。

Conclusion: 该研究为可解释AI领域提供了生物学基础和可扩展的方法。

Abstract: With recent and rapid advancements in artificial intelligence (AI),
understanding the foundation of purposeful behaviour in autonomous agents is
crucial for developing safe and efficient systems. While artificial neural
networks have dominated the path to AI, recent studies are exploring the
potential of biologically based systems, such as networks of living biological
neuronal networks. Along with promises of high power and data efficiency, these
systems may also inform more explainable and biologically plausible models. In
this work, we propose a framework rooted in active inference, a general theory
of behaviour, to model decision-making in embodied agents. Using
experiment-informed generative models, we simulate decision-making processes in
a simulated game-play environment, mirroring experimental setups that use
biological neurons. Our results demonstrate learning in these agents, providing
insights into the role of memory-based learning and predictive planning in
intelligent decision-making. This work contributes to the growing field of
explainable AI by offering a biologically grounded and scalable approach to
understanding purposeful behaviour in agents.

</details>


### [267] [Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach](https://arxiv.org/abs/2508.07015)
*Hannes Ihalainen,Dieter Vandesande,André Schidler,Jeremias Berg,Bart Bogaerts,Matti Järvisalo*

Main category: cs.AI

TL;DR: 论文探讨了隐式命中集（IHS）方法在组合优化问题中的应用，比较了基于伪布尔推理和随机局部搜索的替代算法，并评估了其效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索替代算法技术，以解决隐式命中集优化中的效率和可靠性问题，特别是在伪布尔优化背景下。

Method: 通过比较伪布尔推理和随机局部搜索等替代算法，评估其在IHS框架中的实际可行性。

Result: 商业整数规划求解器在效率上表现最佳，但存在数值不稳定性问题；基于伪布尔推理的精确计算在可靠性上更具竞争力。

Conclusion: 伪布尔推理不仅能提高IHS计算的可靠性，还能为计算正确性提供证明，适用于任何可捕获为伪布尔证明格式的IHS实例。

Abstract: The implicit hitting set (IHS) approach offers a general framework for
solving computationally hard combinatorial optimization problems declaratively.
IHS iterates between a decision oracle used for extracting sources of
inconsistency and an optimizer for computing so-called hitting sets (HSs) over
the accumulated sources of inconsistency. While the decision oracle is
language-specific, the optimizers is usually instantiated through integer
programming.
  We explore alternative algorithmic techniques for hitting set optimization
based on different ways of employing pseudo-Boolean (PB) reasoning as well as
stochastic local search. We extensively evaluate the practical feasibility of
the alternatives in particular in the context of pseudo-Boolean (0-1 IP)
optimization as one of the most recent instantiations of IHS. Highlighting a
trade-off between efficiency and reliability, while a commercial IP solver
turns out to remain the most effective way to instantiate HS computations, it
can cause correctness issues due to numerical instability; in fact, we show
that exact HS computations instantiated via PB reasoning can be made
competitive with a numerically exact IP solver. Furthermore, the use of PB
reasoning as a basis for HS computations allows for obtaining certificates for
the correctness of IHS computations, generally applicable to any IHS
instantiation in which reasoning in the declarative language at hand can be
captured in the PB-based proof format we employ.

</details>


### [268] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: MultiMedEdit是首个针对临床多模态任务的知识编辑（KE）基准，揭示了当前方法在复杂临床工作流中的局限性，并提供了未来开发稳健KE技术的基础。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑研究主要集中在通用领域和医学QA任务，缺乏对多模态医学场景的关注，而医学KE需要结合视觉推理以支持安全和可解释的临床决策。

Method: 提出MultiMedEdit基准，涵盖理解和推理任务类型，定义三维度量（可靠性、通用性和局部性），支持跨范式比较，并在单次编辑和终身编辑设置下进行实验。

Result: 当前方法在泛化和长尾推理方面表现不佳，尤其是在复杂临床工作流中。效率分析揭示了实际部署中的权衡。

Conclusion: MultiMedEdit不仅揭示了当前方法的局限性，还为未来开发临床稳健的KE技术奠定了基础。

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [269] [K-Dense Analyst: Towards Fully Automated Scientific Analysis](https://arxiv.org/abs/2508.07043)
*Orion Li,Vinayak Agarwal,Summer Zhou,Ashwin Gopinath,Timothy Kassis*

Main category: cs.AI

TL;DR: K-Dense Analyst是一个分层多智能体系统，通过双循环架构实现自主生物信息学分析，性能优于现有最佳语言模型。


<details>
  <summary>Details</summary>
Motivation: 现代生物信息学分析的复杂性导致数据生成与科学洞察之间存在巨大差距，需要更强大的工具来填补这一空白。

Method: 采用分层多智能体系统和双循环架构，将复杂目标分解为可执行、可验证的任务，并在安全计算环境中执行。

Result: 在BixBench基准测试中，K-Dense Analyst的准确率达到29.2%，比最佳语言模型（GPT-5）高出6.3个百分点。

Conclusion: 自主科学推理需要专门构建的系统，而不仅仅是增强的语言模型，K-Dense Analyst为生命科学领域的发现提供了重要进展。

Abstract: The complexity of modern bioinformatics analysis has created a critical gap
between data generation and developing scientific insights. While large
language models (LLMs) have shown promise in scientific reasoning, they remain
fundamentally limited when dealing with real-world analytical workflows that
demand iterative computation, tool integration and rigorous validation. We
introduce K-Dense Analyst, a hierarchical multi-agent system that achieves
autonomous bioinformatics analysis through a dual-loop architecture. K-Dense
Analyst, part of the broader K-Dense platform, couples planning with validated
execution using specialized agents to decompose complex objectives into
executable, verifiable tasks within secure computational environments. On
BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense
Analyst achieves 29.2% accuracy, surpassing the best-performing language model
(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what
is widely considered the most powerful LLM available. Remarkably, K-Dense
Analyst achieves this performance using Gemini 2.5 Pro, which attains only
18.3% accuracy when used directly, demonstrating that our architectural
innovations unlock capabilities far beyond the underlying model's baseline
performance. Our insights demonstrate that autonomous scientific reasoning
requires more than enhanced language models, it demands purpose-built systems
that can bridge the gap between high-level scientific objectives and low-level
computational execution. These results represent a significant advance toward
fully autonomous computational biologists capable of accelerating discovery
across the life sciences.

</details>


### [270] [Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach](https://arxiv.org/abs/2508.07063)
*Naseem Machlovi,Maryam Saleki,Innocent Ababio,Ruhul Amin*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在内容审核中的局限性，提出了一种新框架SafePhi，其在伦理背景下表现优于现有模型，但仍需改进数据多样性和人类参与。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统融入日常生活，对更安全可靠的内容审核需求增加。尽管LLMs表现优异，但在道德推理和偏见检测方面存在不足。

Method: 开发了一个实验框架，基于SOTA模型评估人类情感和攻击性行为，并引入统一基准数据集和SafePhi模型。

Result: SafePhi在Macro F1得分上达到0.89，优于OpenAI Moderator（0.77）和Llama Guard（0.74）。

Conclusion: LLMs在内容审核中仍有不足，需结合更多样化数据和人类参与以提高鲁棒性和可解释性。

Abstract: As AI systems become more integrated into daily life, the need for safer and
more reliable moderation has never been greater. Large Language Models (LLMs)
have demonstrated remarkable capabilities, surpassing earlier models in
complexity and performance. Their evaluation across diverse tasks has
consistently showcased their potential, enabling the development of adaptive
and personalized agents. However, despite these advancements, LLMs remain prone
to errors, particularly in areas requiring nuanced moral reasoning. They
struggle with detecting implicit hate, offensive language, and gender biases
due to the subjective and context-dependent nature of these issues. Moreover,
their reliance on training data can inadvertently reinforce societal biases,
leading to inconsistencies and ethical concerns in their outputs. To explore
the limitations of LLMs in this role, we developed an experimental framework
based on state-of-the-art (SOTA) models to assess human emotions and offensive
behaviors. The framework introduces a unified benchmark dataset encompassing 49
distinct categories spanning the wide spectrum of human emotions, offensive and
hateful text, and gender and racial biases. Furthermore, we introduced SafePhi,
a QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and
outperforming benchmark moderators by achieving a Macro F1 score of 0.89, where
OpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This
research also highlights the critical domains where LLM moderators consistently
underperformed, pressing the need to incorporate more heterogeneous and
representative data with human-in-the-loop, for better model robustness and
explainability.

</details>


### [271] [Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention](https://arxiv.org/abs/2508.07107)
*Timothy Oluwapelumi Adeyemi,Nadiah Fahad AlOtaibi*

Main category: cs.AI

TL;DR: 论文提出了一种反馈驱动的决策支持系统（DSS），通过闭环架构实现模型的持续优化，提高了学生成绩预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习模型在预测学生成绩时是静态的，无法适应新数据（如干预后的结果）。为了解决这一问题，作者提出了一种自适应系统。

Method: 系统采用LightGBM回归器，并结合增量式重新训练，允许教育者输入更新的学生成绩数据以自动触发模型更新。系统还包含基于Flask的实时交互界面和SHAP解释工具。

Result: 实验结果显示，重新训练后RMSE降低了10.7%，且对干预学生的预测分数有持续上调。

Conclusion: 该研究将静态预测器转化为自改进系统，推动了教育分析向以人为中心、数据驱动和响应式AI的发展。

Abstract: Accurate prediction of student performance is essential for timely academic
intervention. However, most machine learning models in education are static and
cannot adapt when new data, such as post-intervention outcomes, become
available. To address this limitation, we propose a Feedback-Driven Decision
Support System (DSS) with a closed-loop architecture that enables continuous
model refinement. The system integrates a LightGBM-based regressor with
incremental retraining, allowing educators to input updated student results,
which automatically trigger model updates. This adaptive mechanism improves
prediction accuracy by learning from real-world academic progress. The platform
features a Flask-based web interface for real-time interaction and incorporates
SHAP for explainability, ensuring transparency. Experimental results show a
10.7\% reduction in RMSE after retraining, with consistent upward adjustments
in predicted scores for intervened students. By transforming static predictors
into self-improving systems, our approach advances educational analytics toward
human-centered, data-driven, and responsive AI. The framework is designed for
integration into LMS and institutional dashboards.

</details>


### [272] [Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables](https://arxiv.org/abs/2508.07186)
*Amit Dhanda*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的多智能体框架，用于跨维度总结企业结构化数据，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统表格到文本模型难以处理层次结构和上下文感知差异，而企业报告任务需要这些能力。

Method: 采用多智能体流水线，包括数据切片、方差检测、上下文构建和基于LLM的生成。

Result: 框架在数据忠实度（83%）、显著变化覆盖和决策关键洞察相关性（4.4/5）上表现优异。

Conclusion: 该框架在复杂业务场景中（如价格与销量权衡）显著优于基线方法。

Abstract: We propose a novel framework for summarizing structured enterprise data
across multiple dimensions using large language model (LLM)-based agents.
Traditional table-to-text models often lack the capacity to reason across
hierarchical structures and context-aware deltas, which are essential in
business reporting tasks. Our method introduces a multi-agent pipeline that
extracts, analyzes, and summarizes multi-dimensional data using agents for
slicing, variance detection, context construction, and LLM-based generation.
Our results show that the proposed framework outperforms traditional
approaches, achieving 83\% faithfulness to underlying data, superior coverage
of significant changes, and high relevance scores (4.4/5) for decision-critical
insights. The improvements are especially pronounced in categories involving
subtle trade-offs, such as increased revenue due to price changes amid
declining unit volumes, which competing methods either overlook or address with
limited specificity. We evaluate the framework on Kaggle datasets and
demonstrate significant improvements in faithfulness, relevance, and insight
quality over baseline table summarization approaches.

</details>


### [273] [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
*Yi Tang,Kaini Wang,Yang Chen,Guangquan Zhou*

Main category: cs.AI

TL;DR: EndoAgent是一种基于记忆引导的AI代理，用于内窥镜图像诊断，通过双记忆设计和专家工具集成实现灵活决策。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多任务协调和复杂临床流程处理上表现不足，AI代理在内窥镜领域的潜力尚未充分挖掘。

Method: 提出EndoAgent，采用双记忆设计（短期行动跟踪和长期经验学习），集成专家工具，并引入EndoAgentBench基准测试。

Result: 实验表明，EndoAgent在视觉理解和语言生成任务上优于通用及医学多模态模型。

Conclusion: EndoAgent展示了强大的灵活性和推理能力，为内窥镜诊断提供了新思路。

Abstract: Developing general artificial intelligence (AI) systems to support endoscopic
image diagnosis is an emerging research priority. Existing methods based on
large-scale pretraining often lack unified coordination across tasks and
struggle to handle the multi-step processes required in complex clinical
workflows. While AI agents have shown promise in flexible instruction parsing
and tool integration across domains, their potential in endoscopy remains
underexplored. To address this gap, we propose EndoAgent, the first
memory-guided agent for vision-to-decision endoscopic analysis that integrates
iterative reasoning with adaptive tool selection and collaboration. Built on a
dual-memory design, it enables sophisticated decision-making by ensuring
logical coherence through short-term action tracking and progressively
enhancing reasoning acuity through long-term experiential learning. To support
diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools
within a unified reasoning loop. We further introduce EndoAgentBench, a
benchmark of 5,709 visual question-answer pairs that assess visual
understanding and language generation capabilities in realistic scenarios.
Extensive experiments show that EndoAgent consistently outperforms both general
and medical multimodal models, exhibiting its strong flexibility and reasoning
capabilities.

</details>


### [274] [Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape](https://arxiv.org/abs/2508.07334)
*Quan Shi,Wang Xi,Zenghui Ding,Jianqing Gao,Xianjun Yang*

Main category: cs.AI

TL;DR: 本文通过将大语言模型形式化为概率图灵机，证明了幻觉现象的必然性，并提出两种解决方案：检索增强生成（RAG）和持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）幻觉现象的核心问题，以实现其可靠部署。

Method: 构建“计算必要性层次结构”，证明幻觉的必然性，并提出RAG作为预言机模型和持续学习作为“内部化预言机”机制。

Result: 证明了幻觉现象的必然性，并提供了两种理论支持的解决方案。

Conclusion: 通过形式化理论和实践框架，为LLMs的可靠部署提供了新思路。

Abstract: The illusion phenomenon of large language models (LLMs) is the core obstacle
to their reliable deployment. This article formalizes the large language model
as a probabilistic Turing machine by constructing a "computational necessity
hierarchy", and for the first time proves the illusions are inevitable on
diagonalization, incomputability, and information theory boundaries supported
by the new "learner pump lemma". However, we propose two "escape routes": one
is to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving
their absolute escape through "computational jumps", providing the first formal
theory for the effectiveness of RAGs; The second is to formalize continuous
learning as an "internalized oracle" mechanism and implement this path through
a novel neural game theory framework.Finally, this article proposes a

</details>


### [275] [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
*Rubing Chen,Jiaxin Wu,Jian Wang,Xulu Zhang,Wenqi Fan,Chenghua Lin,Xiao-Yong Wei,Qing Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于全面性与紧凑性（Comp-Comp）原则的迭代式基准框架，用于构建领域特定的大语言模型（LLMs）评测标准，挑战了传统的规模扩展法则。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定基准主要依赖规模扩展法则，但语料库和问答集设计对模型精确率和召回率的影响尚未被研究。

Method: 提出Comp-Comp框架，通过全面性确保语义召回，紧凑性提升精确率，指导语料库和问答集构建。

Result: 通过案例研究构建了XUBench，一个大规模且全面的封闭领域基准，验证了框架的有效性。

Conclusion: Comp-Comp框架不仅适用于学术领域，还可扩展到其他领域，为基准构建提供了新思路。

Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities
of large language models (LLMs), highlighting the need for effective and
efficient benchmark construction. Existing domain-specific benchmarks primarily
focus on the scaling law, relying on massive corpora for supervised fine-tuning
or generating extensive question sets for broad coverage. However, the impact
of corpus and question-answer (QA) set design on the precision and recall of
domain-specific LLMs remains unexplored. In this paper, we address this gap and
demonstrate that the scaling law is not always the optimal principle for
benchmark construction in specific domains. Instead, we propose Comp-Comp, an
iterative benchmarking framework based on a comprehensiveness-compactness
principle. Here, comprehensiveness ensures semantic recall of the domain, while
compactness enhances precision, guiding both corpus and QA set construction. To
validate our framework, we conducted a case study in a well-renowned
university, resulting in the creation of XUBench, a large-scale and
comprehensive closed-domain benchmark. Although we use the academic domain as
the case in this work, our Comp-Comp framework is designed to be extensible
beyond academia, providing valuable insights for benchmark construction across
various domains.

</details>


### [276] [Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning](https://arxiv.org/abs/2508.07382)
*He Kong,Die Hu,Jingguo Ge,Liangxiong Li,Hui Li,Tong Li*

Main category: cs.AI

TL;DR: Pentest-R1是一个通过两阶段强化学习优化LLM在渗透测试中推理能力的新框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在渗透测试中存在错误处理差、推理效率低和无法自主完成复杂任务等局限，Pentest-R1旨在解决这些问题。

Method: 采用两阶段强化学习：离线RL学习基础攻击逻辑，在线RL在CTF环境中微调模型以自我纠正错误和适应策略。

Result: 在AutoPenBench上达到24.2%成功率，Cybench上15.0%成功率，表现优于多数先进模型。

Conclusion: 两阶段训练协同是关键，Pentest-R1为开源LLM设定了新标杆。

Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet
current Large Language Models (LLMs) face significant limitations in this
domain, including poor error handling, inefficient reasoning, and an inability
to perform complex end-to-end tasks autonomously. To address these challenges,
we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning
capabilities for this task through a two-stage reinforcement learning pipeline.
We first construct a dataset of over 500 real-world, multi-step walkthroughs,
which Pentest-R1 leverages for offline reinforcement learning (RL) to instill
foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in
an interactive Capture The Flag (CTF) environment, where it learns directly
from environmental feedback to develop robust error self-correction and
adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench
benchmarks demonstrate the framework's effectiveness. On AutoPenBench,
Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art
models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a
15.0\% success rate in unguided tasks, establishing a new state-of-the-art for
open-source LLMs and matching the performance of top proprietary models.
Ablation studies confirm that the synergy of both training stages is critical
to its success.

</details>


### [277] [Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding](https://arxiv.org/abs/2508.07388)
*Zhaoyu Chen,Hongnan Lin,Yongwei Nie,Fei Ma,Xuemiao Xu,Fei Yu,Chengjiang Long*

Main category: cs.AI

TL;DR: 论文提出Invert4TVG框架，通过三个反转任务增强视频片段定位和动作理解，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前TVG方法过度优化时间IoU，忽视了语义动作理解，影响鲁棒性。

Method: 引入三个反转任务（动词补全、动作识别、视频描述），结合强化学习框架优化定位和语义。

Result: 在Charades-STA上R1@0.7提升7.1%，优于现有方法。

Conclusion: 通过反转任务强化语义理解，显著提升了定位精度上限。

Abstract: Temporal Video Grounding (TVG) seeks to localize video segments matching a
given textual query. Current methods, while optimizing for high temporal
Intersection-over-Union (IoU), often overfit to this metric, compromising
semantic action understanding in the video and query, a critical factor for
robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),
a novel framework that enhances both localization accuracy and action
understanding without additional data. Our approach leverages three inversion
tasks derived from existing TVG annotations: (1) Verb Completion, predicting
masked action verbs in queries from video segments; (2) Action Recognition,
identifying query-described actions; and (3) Video Description, generating
descriptions of video segments that explicitly embed query-relevant actions.
These tasks, integrated with TVG via a reinforcement learning framework with
well-designed reward functions, ensure balanced optimization of localization
and semantics. Experiments show our method outperforms state-of-the-art
approaches, achieving a 7.1\% improvement in R1@0.7 on Charades-STA for a 3B
model compared to Time-R1. By inverting TVG to derive query-related actions
from segments, our approach strengthens semantic understanding, significantly
raising the ceiling of localization accuracy.

</details>


### [278] [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
*Jesse Ponnock*

Main category: cs.AI

TL;DR: 论文探讨了利用生成式人工智能（GAI）和大型语言模型（LLMs）为政府组织开发战略计划的模块化模型，并评估了BERTopic和NMF在主题建模中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索GAI如何帮助自动化政府战略计划的开发，以应对传统上难以自动化的专业服务需求。

Method: 方法包括训练BERTopic和NMF模型，使用政府问责局（GAO）的大量报告进行主题建模，并将生成的主题与已发布的战略计划中的愿景要素进行相似性评分比较。

Result: 结果显示，这些技术能够生成与所有评估要素相似的主题，且BERTopic表现更优，超过一半的相关主题达到“中等”或“强”相关性。

Conclusion: 结论是GAI在战略计划开发中具有潜力，未来工作将聚焦于概念的实际操作化及模型中其他模块的可行性。

Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and
Large Language Models (LLMs), more and more professional services are being
augmented through Artificial Intelligence (AI), which once seemed impossible to
automate. This paper presents a modular model for leveraging GAI in developing
strategic plans for large scale government organizations and evaluates leading
machine learning techniques in their application towards one of the identified
modules. Specifically, the performance of BERTopic and Non-negative Matrix
Factorization (NMF) are evaluated in their ability to use topic modeling to
generate themes representative of Vision Elements within a strategic plan. To
accomplish this, BERTopic and NMF models are trained using a large volume of
reports from the Government Accountability Office (GAO). The generated topics
from each model are then scored for similarity against the Vision Elements of a
published strategic plan and the results are compared. Our results show that
these techniques are capable of generating themes similar to 100% of the
elements being evaluated against. Further, we conclude that BERTopic performs
best in this application with more than half of its correlated topics achieving
a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan
development impacts a multi-billion dollar industry and assists the federal
government in overcoming regulatory requirements which are crucial to the
public good. Further work will focus on the operationalization of the concept
proven in this study as well as viability of the remaining modules in the
proposed model for GAI-generated strategic plans.

</details>


### [279] [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
*Jinyuan Fang,Yanwen Peng,Xi Zhang,Yingxu Wang,Xinhao Yi,Guibin Zhang,Yi Xu,Bin Wu,Siwei Liu,Zihao Li,Zhaochun Ren,Nikos Aletras,Xi Wang,Han Zhou,Zaiqiao Meng*

Main category: cs.AI

TL;DR: 综述探讨了自进化AI代理的技术，提出了统一框架并分类回顾了相关方法，同时讨论了评估、安全与伦理问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理系统多为静态配置，无法适应动态环境，需研究自进化技术以提升适应性。

Method: 提出包含系统输入、代理系统、环境和优化器的统一框架，系统回顾自进化技术及领域专用策略。

Result: 总结了自进化代理的多种技术，强调了领域专用策略的重要性，并讨论了评估与伦理问题。

Conclusion: 自进化AI代理为开发更自适应、自主和终身学习的系统奠定了基础，需进一步研究评估与伦理问题。

Abstract: Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
agent systems rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
environments. To this end, recent research has explored agent evolution
techniques that aim to automatically enhance agent systems based on interaction
data and environmental feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
self-evolving agentic systems. Specifically, we first introduce a unified
conceptual framework that abstracts the feedback loop underlying the design of
self-evolving agentic systems. The framework highlights four key components:
System Inputs, Agent System, Environment, and Optimisers, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the agent system. We also investigate
domain-specific evolution strategies developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the evaluation, safety, and ethical considerations for
self-evolving agentic systems, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.

</details>


### [280] [Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs](https://arxiv.org/abs/2508.07466)
*Dom Huh,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 该论文提出了一种系统性框架，将大型语言模型（LLMs）与多智能体决策算法结合，以增强其协作和推理能力。


<details>
  <summary>Details</summary>
Motivation: 语言是协作和推理的基础，建立共同语言有助于清晰沟通和多智能体协调。本文旨在通过整合LLMs和多智能体算法，提升其协作能力。

Method: 提出了一种框架，包括高级提示工程技术、有效记忆架构、多模态信息处理和微调算法的对齐策略。

Result: 通过经典游戏场景的消融实验验证了设计选择的有效性。

Conclusion: 整合LLMs与多智能体决策算法能显著提升协作和推理能力，为未来研究提供了系统性方法。

Abstract: Language is a ubiquitous tool that is foundational to reasoning and
collaboration, ranging from everyday interactions to sophisticated
problem-solving tasks. The establishment of a common language can serve as a
powerful asset in ensuring clear communication and understanding amongst
agents, facilitating desired coordination and strategies. In this work, we
extend the capabilities of large language models (LLMs) by integrating them
with advancements in multi-agent decision-making algorithms. We propose a
systematic framework for the design of multi-agentic large language models
(LLMs), focusing on key integration practices. These include advanced prompt
engineering techniques, the development of effective memory architectures,
multi-modal information processing, and alignment strategies through
fine-tuning algorithms. We evaluate these design choices through extensive
ablation studies on classic game settings with significant underlying social
dilemmas and game-theoretic considerations.

</details>


### [281] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 提出了一种基于纯代理策略的新方法，利用ReAct原则的Python编码代理，成功解决了CP-Bench基准集中的所有101个问题。


<details>
  <summary>Details</summary>
Motivation: 将自然语言问题描述转化为形式化约束模型是一个基础性挑战，现有固定流程方法在大量基准问题上失败。

Method: 开发了一个基于ReAct原则的通用Python编码代理，通过精心设计的项目提示注入领域知识，结合文件操作和代码执行工具动态测试和调试。

Result: 该方法成功解决了CP-Bench基准集中的所有101个问题。

Conclusion: 约束建模任务需要通用编码工具和提示编码的领域知识，而非专用代理架构或预定义流程。

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [282] [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
*Alexander Duffy,Samuel J Paech,Ishana Shastri,Elizabeth Karpinski,Baptiste Alloui-Cros,Tyler Marques,Matthew Lyle Olson*

Main category: cs.AI

TL;DR: 提出了首个无需微调或专门训练即可让本地大型语言模型（LLM）玩完整版《外交》游戏的评估工具。


<details>
  <summary>Details</summary>
Motivation: 《外交》游戏状态复杂且信息密集，以往研究需前沿LLM或微调，限制了研究范围。

Method: 通过数据驱动优化文本游戏状态表示，使24B模型无需微调即可完成游戏，并开发工具支持假设测试和统计分析。

Result: 实验表明，较大模型表现最佳，但较小模型也能胜任；还提出了关键状态分析协议。

Conclusion: 该工具消除了微调需求，为LLM战略推理能力研究提供了新视角。

Abstract: We present the first evaluation harness that enables any out-of-the-box,
local, Large Language Models (LLMs) to play full-press Diplomacy without
fine-tuning or specialized training. Previous work required frontier LLMs, or
fine-tuning, due to the high complexity and information density of Diplomacy's
game state. Combined with the high variance of matches, these factors made
Diplomacy prohibitive for study. In this work, we used data-driven iteration to
optimize a textual game state representation such that a 24B model can reliably
complete matches without any fine tuning. We develop tooling to facilitate
hypothesis testing and statistical analysis, and we present case studies on
persuasion, aggressive playstyles, and performance across a range of models. We
conduct a variety of experiments across many popular LLMs, finding the larger
models perform the best, but the smaller models still play adequately. We also
introduce Critical State Analysis: an experimental protocol for rapidly
iterating and analyzing key moments in a game at depth. Our harness
democratizes the evaluation of strategic reasoning in LLMs by eliminating the
need for fine-tuning, and it provides insights into how these capabilities
emerge naturally from widely used LLMs. Our code is available in the supplement
and will be open sourced.

</details>


### [283] [MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark](https://arxiv.org/abs/2508.07575)
*Shiqing Fan,Xichen Ding,Liang Zhang,Linjian Mo*

Main category: cs.AI

TL;DR: 论文提出了MCPToolBench++，一个用于评估LLMs调用MCP工具性能的大规模多领域基准测试，解决了现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面的数据集或基准来评估LLMs使用MCP工具的能力，且工具调用的响应格式多样，增加了评估难度。

Method: 构建了MCPToolBench++基准，基于来自MCP市场和GitHub社区的4k多个MCP服务器，覆盖40多个类别，包含单步和多步工具调用。

Result: 评估了具有代理能力的SOTA LLMs，并报告了结果。

Conclusion: MCPToolBench++为评估LLMs在调用MCP工具方面的性能提供了有效解决方案。

Abstract: LLMs' capabilities are enhanced by using function calls to integrate various
data sources or API results into the context window. Typical tools include
search, web crawlers, maps, financial data, file systems, and browser usage,
etc. Integrating these data sources or functions requires a standardized
method. The Model Context Protocol (MCP) provides a standardized way to supply
context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use
abilities suffer from several issues. First, there's a lack of comprehensive
datasets or benchmarks to evaluate various MCP tools. Second, the diverse
formats of response from MCP tool call execution further increase the
difficulty of evaluation. Additionally, unlike existing tool-use benchmarks
with high success rates in functions like programming and math functions, the
success rate of real-world MCP tool is not guaranteed and varies across
different MCP servers. Furthermore, the LLMs' context window also limits the
number of available tools that can be called in a single run, because the
textual descriptions of tool and the parameters have long token length for an
LLM to process all at once. To help address the challenges of evaluating LLMs'
performance on calling MCP tools, we propose MCPToolBench++, a large-scale,
multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is
build upon marketplace of over 4k MCP servers from more than 40 categories,
collected from the MCP marketplaces and GitHub communities. The datasets
consist of both single-step and multi-step tool calls across different
categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and
reported the results.

</details>


### [284] [Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method](https://arxiv.org/abs/2508.07586)
*Wenjing Zhang,Ye Hu,Tao Luo,Zhilong Zhang,Mingzhe Chen*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的隐蔽语义通信框架，通过优化语义信息和传输功率，结合友好干扰器，提升隐私和传输质量。


<details>
  <summary>Details</summary>
Motivation: 研究如何在存在攻击者的情况下，保护语义通信的隐私和传输质量。

Method: 采用优先采样辅助的双延迟深度确定性策略梯度算法，联合优化语义信息和传输功率。

Result: 仿真结果显示，相比传统强化学习方法，隐私和传输质量分别提升77.8%和14.3%。

Conclusion: 提出的算法有效解决了无通信干扰器下的优化问题，显著提升了系统性能。

Abstract: In this paper, a novel covert semantic communication framework is
investigated. Within this framework, a server extracts and transmits the
semantic information, i.e., the meaning of image data, to a user over several
time slots. An attacker seeks to detect and eavesdrop the semantic transmission
to acquire details of the original image. To avoid data meaning being
eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming
signals to interfere the attacker so as to hide the transmitted semantic
information. Meanwhile, the server will strategically select time slots for
semantic information transmission. Due to limited energy, the jammer will not
communicate with the server and hence the server does not know the transmit
power of the jammer. Therefore, the server must jointly optimize the semantic
information transmitted at each time slot and the corresponding transmit power
to maximize the privacy and the semantic information transmission quality of
the user. To solve this problem, we propose a prioritised sampling assisted
twin delayed deep deterministic policy gradient algorithm to jointly determine
the transmitted semantic information and the transmit power per time slot
without the communications between the server and the jammer. Compared to
standard reinforcement learning methods, the propose method uses an additional
Q network to estimate Q values such that the agent can select the action with a
lower Q value from the two Q networks thus avoiding local optimal action
selection and estimation bias of Q values. Simulation results show that the
proposed algorithm can improve the privacy and the semantic information
transmission quality by up to 77.8% and 14.3% compared to the traditional
reinforcement learning methods.

</details>


### [285] [HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol](https://arxiv.org/abs/2508.07602)
*Wenpeng Xing,Zhipeng Chen,Changting Lin,Meng Han*

Main category: cs.AI

TL;DR: HGMF是一种概率剪枝方法，通过分层高斯混合模型提高大型语言模型（LLM）在工具选择中的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在大规模、分层结构工具库中选择正确工具的挑战，避免因上下文限制和无关选项导致的低准确性和高计算成本。

Method: HGMF将用户查询和工具描述映射到统一语义空间，分两阶段进行高斯混合模型（GMM）聚类和基于查询可能性的过滤，生成高相关性候选集。

Result: 实验表明，HGMF显著提高了工具选择的准确性，同时降低了推理延迟。

Conclusion: HGMF是一种可扩展且有效的方法，适用于大规模工具库的工具调用。

Abstract: Invoking external tools enables Large Language Models (LLMs) to perform
complex, real-world tasks, yet selecting the correct tool from large,
hierarchically-structured libraries remains a significant challenge. The
limited context windows of LLMs and noise from irrelevant options often lead to
low selection accuracy and high computational costs. To address this, we
propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic
pruning method for scalable tool invocation. HGMF first maps the user query and
all tool descriptions into a unified semantic space. The framework then
operates in two stages: it clusters servers using a Gaussian Mixture Model
(GMM) and filters them based on the query's likelihood. Subsequently, it
applies the same GMM-based clustering and filtering to the tools associated
with the selected servers. This hierarchical process produces a compact,
high-relevance candidate set, simplifying the final selection task for the LLM.
Experiments on a public dataset show that HGMF significantly improves tool
selection accuracy while reducing inference latency, confirming the framework's
scalability and effectiveness for large-scale tool libraries.

</details>


### [286] [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
*Aswin RRV,Jacob Dineen,Divij Handa,Md Nayem Uddin,Mihir Parmar,Chitta Baral,Ben Zhou*

Main category: cs.AI

TL;DR: 论文提出ThinkTuning方法，通过教师模型的反馈指导提升学生模型的多步推理能力，实验显示其优于零样本和GRPO基线。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习（RL）方法仅能激发基础模型中已有的推理行为，而无法真正培养新的推理能力，因此需要一种新方法来训练不具备此类行为的模型。

Method: 采用GRPO基础的交互式训练方法ThinkTuning，通过教师模型对学生模型的输出提供反馈，逐步引导学生模型改进推理能力。

Result: 实验表明，ThinkTuning在多个基准测试中平均提升3.85%，在MATH-500、AIME和GPQA-Diamond上分别提升2.08%、2.23%和3.99%。

Conclusion: ThinkTuning通过教师模型的反馈有效提升了学生模型的推理能力，为培养新推理行为提供了可行方案。

Abstract: Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

</details>


### [287] [Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization](https://arxiv.org/abs/2508.07628)
*Daniel Essien,Suresh Neethirajan*

Main category: cs.AI

TL;DR: 论文提出利用多模态AI技术改进家禽福利监测，通过特征级融合策略实现高效、可扩展的解决方案，并引入新评估工具和部署框架。


<details>
  <summary>Details</summary>
Motivation: 传统家禽福利监测依赖主观人工观察和单一传感器数据，无法全面反映现代养殖场的复杂需求，亟需数据驱动的智能监测系统。

Method: 采用多模态AI技术，整合视觉、声音、环境和生理数据，提出特征级融合策略，并引入域转移评分（DTS）和数据可靠性指数（DRI）作为评估工具。

Result: 特征级融合策略在真实养殖条件下表现最佳，新工具和框架解决了传感器脆弱性、部署成本高和跨农场泛化性差等问题。

Conclusion: 多模态AI为家禽福利监测提供了从被动单模态到主动精准化的转型基础，结合生产效率和科学伦理。

Abstract: The future of poultry production depends on a paradigm shift replacing
subjective, labor-intensive welfare checks with data-driven, intelligent
monitoring ecosystems. Traditional welfare assessments-limited by human
observation and single-sensor data-cannot fully capture the complex,
multidimensional nature of laying hen welfare in modern farms. Multimodal
Artificial Intelligence (AI) offers a breakthrough, integrating visual,
acoustic, environmental, and physiological data streams to reveal deeper
insights into avian welfare dynamics. This investigation highlights multimodal
As transformative potential, showing that intermediate (feature-level) fusion
strategies achieve the best balance between robustness and performance under
real-world poultry conditions, and offer greater scalability than early or late
fusion approaches. Key adoption barriers include sensor fragility in harsh farm
environments, high deployment costs, inconsistent behavioral definitions, and
limited cross-farm generalizability. To address these, we introduce two novel
evaluation tools - the Domain Transfer Score (DTS) to measure model
adaptability across diverse farm settings, and the Data Reliability Index (DRI)
to assess sensor data quality under operational constraints. We also propose a
modular, context-aware deployment framework designed for laying hen
environments, enabling scalable and practical integration of multimodal
sensing. This work lays the foundation for a transition from reactive, unimodal
monitoring to proactive, precision-driven welfare systems that unite
productivity with ethical, science based animal care.

</details>


### [288] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: SkillNav是一个模块化框架，通过将导航任务分解为可解释的原子技能，并利用VLM路由器动态选择最佳代理，提升了VLN任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLN方法在复杂空间和时间推理任务中泛化能力不足，SkillNav旨在通过技能分解和动态路由解决这一问题。

Method: SkillNav将导航任务分解为原子技能，每个技能由专门代理处理，并使用VLM路由器动态选择代理。

Result: 在R2R和GSA-R2R基准测试中，SkillNav取得了最先进的性能，并展现出强大的泛化能力。

Conclusion: SkillNav通过模块化和动态路由显著提升了VLN任务的性能，尤其在复杂和未见场景中表现优异。

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


### [289] [EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration](https://arxiv.org/abs/2508.07671)
*Mohamed Rayan Barhdadi,Mehmet Tuncel,Erchin Serpedin,Hasan Kurban*

Main category: cs.AI

TL;DR: EMPATHIA是一个多智能体框架，用于难民整合，结合文化、情感和伦理因素，提供透明且可解释的决策支持。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法在难民整合中仅关注就业等狭窄目标，忽视了文化、情感和伦理等长期成功的关键维度。

Method: EMPATHIA基于Kegan的建构发展理论，分为SEED、RISE和THRIVE三个模块，采用多智能体架构进行透明决策。

Result: 在UN Kakuma数据集上验证，EMPATHIA达到87.4%的收敛率，并在五个接收国实现可解释的评估。

Conclusion: EMPATHIA通过平衡多种价值系统，为AI驱动的分配任务提供了通用框架，同时支持人机协作。

Abstract: Current AI approaches to refugee integration optimize narrow objectives such
as employment and fail to capture the cultural, emotional, and ethical
dimensions critical for long-term success. We introduce EMPATHIA (Enriched
Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),
a multi-agent framework addressing the central Creative AI question: how do we
preserve human dignity when machines participate in life-altering decisions?
Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes
integration into three modules: SEED (Socio-cultural Entry and Embedding
Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency
Engine) for early independence, and THRIVE (Transcultural Harmony and
Resilience through Integrated Values and Engagement) for sustained outcomes.
SEED employs a selector-validator architecture with three specialized agents -
emotional, cultural, and ethical - that deliberate transparently to produce
interpretable recommendations. Experiments on the UN Kakuma dataset (15,026
individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and
implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic
variables achieved 87.4% validation convergence and explainable assessments
across five host countries. EMPATHIA's weighted integration of cultural,
emotional, and ethical factors balances competing value systems while
supporting practitioner-AI collaboration. By augmenting rather than replacing
human expertise, EMPATHIA provides a generalizable framework for AI-driven
allocation tasks where multiple values must be reconciled.

</details>


### [290] [Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation](https://arxiv.org/abs/2508.07649)
*Jie Li,Haoye Dong,Zhengyang Wu,Zetao Zheng,Mingrong Lin*

Main category: cs.AI

TL;DR: DiMuST是一种基于解耦表示学习的POI推荐模型，通过多时空转换图和社会关系提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法将时空转换分开建模，导致关键节点表示不一致，引入冗余信息，影响模型性能和可解释性。

Method: 使用解耦变分多图自动编码器（DAE），分离共享和私有分布，通过PoE机制融合共享特征，并通过对比约束去噪私有特征。

Result: 在两个数据集上，DiMuST在多个指标上显著优于现有方法。

Conclusion: DiMuST有效捕捉了POI的时空转换表示，同时保持了时空关系的内在相关性。

Abstract: Next Point-of-Interest (POI) recommendation is a research hotspot in business
intelligence, where users' spatial-temporal transitions and social
relationships play key roles. However, most existing works model spatial and
temporal transitions separately, leading to misaligned representations of the
same spatial-temporal key nodes. This misalignment introduces redundant
information during fusion, increasing model uncertainty and reducing
interpretability. To address this issue, we propose DiMuST, a socially enhanced
POI recommendation model based on disentangled representation learning over
multiplex spatial-temporal transition graphs. The model employs a novel
Disentangled variational multiplex graph Auto-Encoder (DAE), which first
disentangles shared and private distributions using a multiplex
spatial-temporal graph strategy. It then fuses the shared features via a
Product of Experts (PoE) mechanism and denoises the private features through
contrastive constraints. The model effectively captures the spatial-temporal
transition representations of POIs while preserving the intrinsic correlation
of their spatial-temporal relationships. Experiments on two challenging
datasets demonstrate that our DiMuST significantly outperforms existing methods
across multiple metrics.

</details>


### [291] [1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning](https://arxiv.org/abs/2508.07667)
*Wenkai Li,Liwen Sun,Zhenxiang Guan,Xuhui Zhou,Maarten Sap*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体框架，通过分解隐私推理任务（如提取、分类）来减少单个智能体的信息负载，从而更可靠地遵守上下文隐私规范。实验表明，该方法显著减少了隐私信息泄露。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在多源信息处理（如会议摘要）中的上下文隐私问题。

Method: 引入多智能体框架，将隐私推理分解为专门子任务，并通过信息流拓扑分析隐私错误的传播。

Result: 在ConfAIde和PrivacyLens基准测试中，多智能体配置显著减少了隐私信息泄露（GPT-4o下分别降低18%和19%）。

Conclusion: 多智能体系统的信息流设计在LLMs的上下文隐私保护中具有潜力。

Abstract: Addressing contextual privacy concerns remains challenging in interactive
settings where large language models (LLMs) process information from multiple
sources (e.g., summarizing meetings with private and public information). We
introduce a multi-agent framework that decomposes privacy reasoning into
specialized subtasks (extraction, classification), reducing the information
load on any single agent while enabling iterative validation and more reliable
adherence to contextual privacy norms. To understand how privacy errors emerge
and propagate, we conduct a systematic ablation over information-flow
topologies, revealing when and why upstream detection mistakes cascade into
downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with
several open-source and closed-sourced LLMs demonstrate that our best
multi-agent configuration substantially reduces private information leakage
(\textbf{18\%} on ConfAIde and \textbf{19\%} on PrivacyLens with GPT-4o) while
preserving the fidelity of public content, outperforming single-agent
baselines. These results highlight the promise of principled information-flow
design in multi-agent systems for contextual privacy with LLMs.

</details>


### [292] [Ethics2vec: aligning automatic agents and human preferences](https://arxiv.org/abs/2508.07673)
*Gianluca Bontempi*

Main category: cs.AI

TL;DR: 论文提出Ethics2Vec方法，通过向量化AI代理的决策策略，评估其与人类价值观的一致性。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理行为中隐含的伦理价值难以量化与对齐的问题，特别是在涉及不可比较的伦理考量时。

Method: 扩展Anything2vec方法，将代理的决策策略映射为多元向量表示，用于比较和评估与人类价值观的对齐程度。

Result: 提出了Ethics2Vec方法，适用于二元决策和自动控制场景（如自动驾驶）。

Conclusion: 通过向量化方法，为AI伦理对齐问题提供了可行的解决方案。

Abstract: Though intelligent agents are supposed to improve human experience (or make
it more efficient), it is hard from a human perspective to grasp the ethical
values which are explicitly or implicitly embedded in an agent behaviour. This
is the well-known problem of alignment, which refers to the challenge of
designing AI systems that align with human values, goals and preferences. This
problem is particularly challenging since most human ethical considerations
refer to \emph{incommensurable} (i.e. non-measurable and/or incomparable)
values and criteria. Consider, for instance, a medical agent prescribing a
treatment to a cancerous patient. How could it take into account (and/or weigh)
incommensurable aspects like the value of a human life and the cost of the
treatment? Now, the alignment between human and artificial values is possible
only if we define a common space where a metric can be defined and used. This
paper proposes to extend to ethics the conventional Anything2vec approach,
which has been successful in plenty of similar and hard-to-quantify domains
(ranging from natural language processing to recommendation systems and graph
analysis). This paper proposes a way to map an automatic agent decision-making
(or control law) strategy to a multivariate vector representation, which can be
used to compare and assess the alignment with human values. The Ethics2Vec
method is first introduced in the case of an automatic agent performing binary
decision-making. Then, a vectorisation of an automatic control law (like in the
case of a self-driving car) is discussed to show how the approach can be
extended to automatic control settings.

</details>


### [293] [Symmetry-Aware Transformer Training for Automated Planning](https://arxiv.org/abs/2508.07743)
*Markus Fritzsche,Elliot Gestrin,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出了一种对比学习目标，使Transformer能够感知对称性，从而弥补其在自动规划任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: Transformer在自动规划任务中表现不佳，主要由于问题对称性导致的组合爆炸。

Method: 采用对比学习目标和架构改进，使Transformer能够感知对称性。

Result: 在多个规划领域中，该方法有效解决了PlanGPT的局限性。

Conclusion: 对称感知训练显著提升了Transformer在规划任务中的表现。

Abstract: While transformers excel in many settings, their application in the field of
automated planning is limited. Prior work like PlanGPT, a state-of-the-art
decoder-only transformer, struggles with extrapolation from easy to hard
planning problems. This in turn stems from problem symmetries: planning tasks
can be represented with arbitrary variable names that carry no meaning beyond
being identifiers. This causes a combinatorial explosion of equivalent
representations that pure transformers cannot efficiently learn from. We
propose a novel contrastive learning objective to make transformers
symmetry-aware and thereby compensate for their lack of inductive bias.
Combining this with architectural improvements, we show that transformers can
be efficiently trained for either plan-generation or heuristic-prediction. Our
results across multiple planning domains demonstrate that our symmetry-aware
training effectively and efficiently addresses the limitations of PlanGPT.

</details>


### [294] [Best-Effort Policies for Robust Markov Decision Processes](https://arxiv.org/abs/2508.07790)
*Alessandro Abate,Thom Badings,Giuseppe De Giacomo,Francesco Fabiano*

Main category: cs.AI

TL;DR: 论文研究了鲁棒马尔可夫决策过程（RMDPs）的推广，提出了一种新的策略选择标准ORBE，旨在在对抗性概率下最大化期望回报的同时，优化非对抗性概率下的表现。


<details>
  <summary>Details</summary>
Motivation: RMDPs中通常存在多个最优鲁棒策略，这些策略在最坏情况下表现相同，但在非对抗性情况下表现不同。因此，需要一种更精细的策略选择标准。

Method: 提出了ORBE策略，结合了博弈论中的优势和尽力而为概念，要求策略不仅在对抗性概率下最优，还在非对抗性概率下表现最佳。

Result: 证明了ORBE策略的存在性，描述了其结构，并提出了计算算法，实验验证了方法的可行性。

Conclusion: ORBE策略为最优鲁棒策略提供了原则性的选择标准，扩展了RMDPs的应用范围。

Abstract: We study the common generalization of Markov decision processes (MDPs) with
sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal
in RMDPs is to compute a policy that maximizes the expected return under an
adversarial choice of the transition probabilities. If the uncertainty in the
probabilities is independent between the states, known as s-rectangularity,
such optimal robust policies can be computed efficiently using robust value
iteration. However, there might still be multiple optimal robust policies,
which, while equivalent with respect to the worst-case, reflect different
expected returns under non-adversarial choices of the transition probabilities.
Hence, we propose a refined policy selection criterion for RMDPs, drawing
inspiration from the notions of dominance and best-effort in game theory.
Instead of seeking a policy that only maximizes the worst-case expected return,
we additionally require the policy to achieve a maximal expected return under
different (i.e., not fully adversarial) transition probabilities. We call such
a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE
policies always exist, characterize their structure, and present an algorithm
to compute them with a small overhead compared to standard robust value
iteration. ORBE policies offer a principled tie-breaker among optimal robust
policies. Numerical experiments show the feasibility of our approach.

</details>


### [295] [KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations](https://arxiv.org/abs/2508.07834)
*Mubaris Nadeem,Johannes Zenkert,Lisa Bender,Christian Weber,Madjid Fathi*

Main category: cs.AI

TL;DR: 本文提出了一种基于知识图谱的人工智能系统，为急救人员提供实时智能治疗建议。


<details>
  <summary>Details</summary>
Motivation: 全球救援需求增加，急救人员需快速提供个性化医疗，但时间紧迫难以充分利用专业知识。

Method: 使用知识图谱作为核心知识表示，结合人工智能预识别情境。

Result: 系统能为急救人员提供智能治疗推荐，优化紧急医疗处理。

Conclusion: 知识图谱与AI结合可显著提升急救效率和效果。

Abstract: Over the years, the need for rescue operations throughout the world has
increased rapidly. Demographic changes and the resulting risk of injury or
health disorders form the basis for emergency calls. In such scenarios, first
responders are in a rush to reach the patient in need, provide first aid, and
save lives. In these situations, they must be able to provide personalized and
optimized healthcare in the shortest possible time and estimate the patients
condition with the help of freshly recorded vital data in an emergency
situation. However, in such a timedependent situation, first responders and
medical experts cannot fully grasp their knowledge and need assistance and
recommendation for further medical treatments. To achieve this, on the spot
calculated, evaluated, and processed knowledge must be made available to
improve treatments by first responders. The Knowledge Graph presented in this
article as a central knowledge representation provides first responders with an
innovative knowledge management that enables intelligent treatment
recommendations with an artificial intelligence-based pre-recognition of the
situation.

</details>


### [296] [\(X\)-evolve: Solution space evolution powered by large language models](https://arxiv.org/abs/2508.07932)
*Yi Zhai,Zhiqiang Wei,Ruohan Li,Keyu Pan,Shuo Liu,Lu Zhang,Jianmin Ji,Wuyang Zhang,Yu Zhang,Yanyong Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为X-evolve的新方法，通过演化解空间而非单个解，显著减少了大型语言模型（LLM）的调用成本，并在多个优化问题上取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 当前结合LLM和进化算法的方法通常演化单个解，导致LLM调用成本高昂，限制了其在高维问题中的应用。

Method: X-evolve通过LLM生成可调程序，定义可调解空间，并利用基于分数的搜索算法高效探索该空间。

Result: 在三个优化问题上验证了X-evolve的有效性：发现了更大的部分可容许集（cap set问题）、更大的独立集（信息论问题）以及更优的启发式算法（在线装箱问题）。

Conclusion: X-evolve通过演化解空间显著提高了搜索效率，使解决高维问题成为可能。

Abstract: While combining large language models (LLMs) with evolutionary algorithms
(EAs) shows promise for solving complex optimization problems, current
approaches typically evolve individual solutions, often incurring high LLM call
costs. We introduce \(X\)-evolve, a paradigm-shifting method that instead
evolves solution spaces \(X\) (sets of individual solutions) - subsets of the
overall search space \(S\). In \(X\)-evolve, LLMs generate tunable programs
wherein certain code snippets, designated as parameters, define a tunable
solution space. A score-based search algorithm then efficiently explores this
parametrically defined space, guided by feedback from objective function
scores. This strategy enables broader and more efficient exploration, which can
potentially accelerate convergence at a much lower search cost, requiring up to
two orders of magnitude fewer LLM calls than prior leading methods. We
demonstrate \(X\)-evolve's efficacy across three distinct hard optimization
problems. For the cap set problem, we discover a larger partial admissible set,
establishing a new tighter asymptotic lower bound for the cap set constant (\(C
\ge 2.2203\)). In information theory, we uncover a larger independent set for
the 15-vertex cycle graph (\(\mathcal{C}_{15}^{\boxtimes 5}\), size 19,946),
thereby raising the known lower bound on its Shannon capacity. Furthermore, for
the NP-hard online bin packing problem, we generate heuristics that
consistently outperform standard strategies across established benchmarks. By
evolving solution spaces, our method considerably improves search
effectiveness, making it possible to tackle high-dimensional problems that were
previously computationally prohibitive.

</details>


### [297] [Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots](https://arxiv.org/abs/2508.07941)
*Olivier Poulet,Frédéric Guinand,François Guérin*

Main category: cs.AI

TL;DR: 提出了一种基于LSTM短期预测的碰撞风险预测方法，通过动态调整DQN的奖励来减少碰撞，实验显示效果显著。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在无通信和标识的受限环境中移动时的碰撞风险问题。

Method: 使用LSTM预测机器人位置，动态调整DQN的奖励以定义碰撞风险。

Result: 在1Hz采样频率下，碰撞次数显著减少，稳定性提高。

Conclusion: 该方法计算成本低，适合嵌入式系统实现。

Abstract: This article proposes a collision risk anticipation method based on
short-term prediction of the agents position. A Long Short-Term Memory (LSTM)
model, trained on past trajectories, is used to estimate the next position of
each robot. This prediction allows us to define an anticipated collision risk
by dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent.
The approach is tested in a constrained environment, where two robots move
without communication or identifiers. Despite a limited sampling frequency (1
Hz), the results show a significant decrease of the collisions number and a
stability improvement. The proposed method, which is computationally
inexpensive, appears particularly attractive for implementation on embedded
systems.

</details>


### [298] [FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis](https://arxiv.org/abs/2508.07950)
*Chen Shen,Wanqing Zhang,Kehan Li,Erwen Huang,Haitao Bi,Aiying Fan,Yiwen Shen,Hongmei Dong,Ji Zhang,Yuming Shao,Zengjia Liu,Xinshe Liu,Tao Li,Chunxia Yan,Shuanliang Fan,Di Wu,Jianhua Ma,Bin Cong,Zhenyuan Wang,Chunfeng Lian*

Main category: cs.AI

TL;DR: FEAT是一个基于多智能体AI框架的自动化法医死亡调查系统，通过领域适应的大语言模型标准化流程，在性能上超越现有AI系统，并接近专家水平。


<details>
  <summary>Details</summary>
Motivation: 解决法医死亡鉴定中的劳动力短缺和诊断差异问题，特别是在中国等案件量大的法医体系中。

Method: FEAT采用多智能体架构，包括任务分解的中央规划器、证据分析的专业本地求解器、迭代优化的记忆与反思模块，以及结论合成的全局求解器。结合工具增强推理、分层检索增强生成和法医调优的LLM。

Result: 在多样化中国案例中，FEAT在长篇幅尸检分析和简洁死因结论上均优于现有AI系统，并在盲法验证中与专家达成高一致性。

Conclusion: FEAT是首个基于LLM的法医AI系统，结合AI效率与人类监督，可提升法医服务的可及性和可靠性。

Abstract: Forensic cause-of-death determination faces systemic challenges, including
workforce shortages and diagnostic variability, particularly in high-volume
systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic
AgenT), a multi-agent AI framework that automates and standardizes death
investigations through a domain-adapted large language model. FEAT's
application-oriented architecture integrates: (i) a central Planner for task
decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a
Memory & Reflection module for iterative refinement, and (iv) a Global Solver
for conclusion synthesis. The system employs tool-augmented reasoning,
hierarchical retrieval-augmented generation, forensic-tuned LLMs, and
human-in-the-loop feedback to ensure legal and medical validity. In evaluations
across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI
systems in both long-form autopsy analyses and concise cause-of-death
conclusions. It demonstrated robust generalization across six geographic
regions and achieved high expert concordance in blinded validations. Senior
pathologists validated FEAT's outputs as comparable to those of human experts,
with improved detection of subtle evidentiary nuances. To our knowledge, FEAT
is the first LLM-based AI agent system dedicated to forensic medicine, offering
scalable, consistent death certification while maintaining expert-level rigor.
By integrating AI efficiency with human oversight, this work could advance
equitable access to reliable medicolegal services while addressing critical
capacity constraints in forensic systems.

</details>


### [299] [Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths](https://arxiv.org/abs/2508.08001)
*Rui Yao,Qi Chai,Jinhai Yao,Siyuan Li,Junhao Chen,Qi Zhang,Hao Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM的不确定性感知框架，用于解析美联储的Fedspeak并分类其货币政策立场，结合领域特定推理和动态不确定性解码模块，显著提升了分类准确性和模型可靠性。


<details>
  <summary>Details</summary>
Motivation: Fedspeak作为美联储的策略性语言工具，对市场预期和经济条件具有重要影响，自动解析Fedspeak对金融预测和算法交易等具有重大意义。

Method: 采用LLM框架，结合货币政策传导机制的领域特定推理，并引入动态不确定性解码模块评估模型预测置信度。

Result: 实验表明该框架在政策立场分析任务中达到最先进性能，且感知不确定性与模型错误率显著正相关。

Conclusion: 该框架有效提升了Fedspeak解析的准确性和可靠性，验证了感知不确定性作为诊断信号的有效性。

Abstract: "Fedspeak", the stylized and often nuanced language used by the U.S. Federal
Reserve, encodes implicit policy signals and strategic stances. The Federal
Open Market Committee strategically employs Fedspeak as a communication tool to
shape market expectations and influence both domestic and global economic
conditions. As such, automatically parsing and interpreting Fedspeak presents a
high-impact challenge, with significant implications for financial forecasting,
algorithmic trading, and data-driven policy analysis. In this paper, we propose
an LLM-based, uncertainty-aware framework for deciphering Fedspeak and
classifying its underlying monetary policy stance. Technically, to enrich the
semantic and contextual representation of Fedspeak texts, we incorporate
domain-specific reasoning grounded in the monetary policy transmission
mechanism. We further introduce a dynamic uncertainty decoding module to assess
the confidence of model predictions, thereby enhancing both classification
accuracy and model reliability. Experimental results demonstrate that our
framework achieves state-of-the-art performance on the policy stance analysis
task. Moreover, statistical analysis reveals a significant positive correlation
between perceptual uncertainty and model error rates, validating the
effectiveness of perceptual uncertainty as a diagnostic signal.

</details>


### [300] [Fitting Description Logic Ontologies to ABox and Query Examples](https://arxiv.org/abs/2508.08007)
*Maurice Funk,Marvin Grosser,Carsten Lutz*

Main category: cs.AI

TL;DR: 论文研究了基于本体介导查询的拟合问题，确定了在不同查询语言下拟合本体存在的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过正负例拟合一个满足特定查询条件的本体，以支持本体介导查询的实际应用。

Method: 使用描述逻辑ALC和ALCI作为本体语言，分析不同查询语言（AQs、CQs、UCQs）下的拟合问题，提供有效特征化并计算复杂度。

Result: 拟合问题对AQs和完整CQs是CO-NP复杂度，对CQs和UCQs是2EXPTIME完全复杂度。

Conclusion: 研究结果为ALC和ALCI下的本体拟合问题提供了明确的复杂度界限，为实际应用提供了理论支持。

Abstract: We study a fitting problem inspired by ontology-mediated querying: given a
collection
  of positive and negative examples of
  the form $(\mathcal{A},q)$ with
  $\mathcal{A}$ an ABox and $q$ a Boolean query, we seek
  an ontology $\mathcal{O}$ that satisfies $\mathcal{A} \cup \mathcal{O} \vDash
q$ for all positive examples and $\mathcal{A} \cup \mathcal{O}\not\vDash q$ for
all negative examples.
  We consider the description logics $\mathcal{ALC}$ and $\mathcal{ALCI}$ as
ontology languages and
  a range of query languages that
  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof
(UCQs).
  For all of the resulting fitting problems,
  we provide
  effective characterizations and determine the computational complexity
  of deciding whether a fitting ontology exists. This problem turns out to be
${\small CO}NP$ for AQs and full CQs
  and $2E{\small XP}T{\small IME}$-complete for CQs and UCQs.
  These results hold for both $\mathcal{ALC}$ and $\mathcal{ALCI}$.

</details>


### [301] [AdaptFlow: Adaptive Workflow Optimization via Meta-Learning](https://arxiv.org/abs/2508.08053)
*Runchuan Zhu,Bowen Jiang,Lingrui Mei,Fangkai Yang,Lu Wang,Haoxiang Gao,Fengshuo Bai,Pu Zhao,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: AdaptFlow是一个基于自然语言的元学习框架，通过双层优化实现任务级快速适应，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态模板或手动设计的工作流适应性差且难以扩展，AdaptFlow旨在解决这一问题。

Method: 采用双层优化方案：内层通过LLM反馈优化子任务工作流，外层更新共享初始化以跨任务泛化。

Result: 在问答、代码生成和数学推理任务中表现优异，优于手动和自动基线。

Conclusion: AdaptFlow通过语言引导的修改实现强泛化能力，为复杂任务提供了高效解决方案。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in agentic workflows, which are structured sequences of LLM invocations
intended to solve complex tasks. However, existing approaches often rely on
static templates or manually designed workflows, which limit adaptability to
diverse tasks and hinder scalability. We propose AdaptFlow, a natural
language-based meta-learning framework inspired by model-agnostic meta-learning
(MAML). AdaptFlow learns a generalizable workflow initialization that enables
rapid subtask-level adaptation. It employs a bi-level optimization scheme: the
inner loop refines the workflow for a specific subtask using LLM-generated
feedback, while the outer loop updates the shared initialization to perform
well across tasks. This setup allows AdaptFlow to generalize effectively to
unseen tasks by adapting the initialized workflow through language-guided
modifications. Evaluated across question answering, code generation, and
mathematical reasoning benchmarks, AdaptFlow consistently outperforms both
manually crafted and automatically searched baselines, achieving
state-of-the-art results with strong generalization across tasks and models.
The source code and data are available at
https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.

</details>


### [302] [FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence](https://arxiv.org/abs/2508.08075)
*Meishen He,Wenjun Ma,Jiao Wang,Huijun Yue,Xiaoma Fan*

Main category: cs.AI

TL;DR: 提出了一种基于Dempster-Shafer理论的开放世界信息融合方法FNBT，解决了异构框架下的证据融合问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，数据或模型常来自不同区域或组织，导致异构框架，传统融合方法效果不佳。

Method: 引入标准判断是否为开放世界，扩展框架以容纳异构元素，采用全否定机制转换质量函数。

Result: 理论证明满足质量函数不变性、遗传性和冲突消除性，实验验证在分类任务中表现优异。

Conclusion: FNBT方法有效解决了异构框架下的信息融合问题，具有理论和实践价值。

Abstract: The Dempster-Shafer theory of evidence has been widely applied in the field
of information fusion under uncertainty. Most existing research focuses on
combining evidence within the same frame of discernment. However, in real-world
scenarios, trained algorithms or data often originate from different regions or
organizations, where data silos are prevalent. As a result, using different
data sources or models to generate basic probability assignments may lead to
heterogeneous frames, for which traditional fusion methods often yield
unsatisfactory results. To address this challenge, this study proposes an
open-world information fusion method, termed Full Negation Belief
Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a
criterion is introduced to determine whether a given fusion task belongs to the
open-world setting. Then, by extending the frames, the method can accommodate
elements from heterogeneous frames. Finally, a full negation mechanism is
employed to transform the mass functions, so that existing combination rules
can be applied to the transformed mass functions for such information fusion.
Theoretically, the proposed method satisfies three desirable properties, which
are formally proven: mass function invariance, heritability, and essential
conflict elimination. Empirically, FNBT demonstrates superior performance in
pattern classification tasks on real-world datasets and successfully resolves
Zadeh's counterexample, thereby validating its practical effectiveness.

</details>


### [303] [TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork](https://arxiv.org/abs/2508.08115)
*Pranav Pushkar Mishra,Mohammad Arvan,Mohan Zalake*

Main category: cs.AI

TL;DR: TeamMedAgents将人类团队合作的心理学模型应用于多智能体医疗决策系统，通过六个核心团队合作组件提升LLM在医疗任务中的表现，并在多个医疗基准测试中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 将人类团队合作的理论模型（如Salas的“Big Five”）转化为计算模型，以提升多智能体在医疗决策中的协作效率和准确性。

Method: 通过模块化实现六个团队合作组件（如团队领导力、共享心智模型等），并在自适应协作架构中评估不同任务和领域下的智能体数量影响。

Result: 在8个医疗基准测试中，7个表现出显著改进，且不同任务和领域需要不同的最优团队配置。

Conclusion: TeamMedAgents为关键决策领域提供了基于证据的多智能体系统设计基础，推动了协作AI的发展。

Abstract: We present TeamMedAgents, a novel multi-agent approach that systematically
integrates evidence-based teamwork components from human-human collaboration
into medical decision-making with large language models (LLMs). Our approach
validates an organizational psychology teamwork model from human collaboration
to computational multi-agent medical systems by operationalizing six core
teamwork components derived from Salas et al.'s "Big Five" model: team
leadership, mutual performance monitoring, team orientation, shared mental
models, closed-loop communication, and mutual trust. We implement and evaluate
these components as modular, configurable mechanisms within an adaptive
collaboration architecture while assessing the effect of the number of agents
involved based on the task's requirements and domain. Systematic evaluation of
computational implementations of teamwork behaviors across eight medical
benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,
Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8
evaluated datasets. Controlled ablation studies conducted on 50 questions per
configuration across 3 independent runs provide mechanistic insights into
individual component contributions, revealing optimal teamwork configurations
that vary by reasoning task complexity and domain-specific requirements. Our
ablation analyses reveal dataset-specific optimal teamwork configurations,
indicating that different medical reasoning modalities benefit from distinct
collaborative patterns. TeamMedAgents represents an advancement in
collaborative AI by providing a systematic translation of established teamwork
theories from human collaboration into agentic collaboration, establishing a
foundation for evidence-based multi-agent system design in critical
decision-making domains.

</details>


### [304] [BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks](https://arxiv.org/abs/2508.08127)
*Rui Miao,Yixin Liu,Yili Wang,Xu Shen,Yue Tan,Yiwei Dai,Shirui Pan,Xin Wang*

Main category: cs.AI

TL;DR: BlindGuard是一种无监督防御方法，用于检测多智能体系统中的恶意代理，无需依赖标记数据或攻击先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有监督防御方法依赖标记数据，实际应用中不切实际，需开发更通用的无监督防御方法。

Method: BlindGuard通过分层代理编码器捕捉个体、邻域和全局交互模式，并结合定向噪声注入和对比学习训练检测模型。

Result: 实验表明，BlindGuard能有效检测多种攻击类型，且泛化能力优于监督基线方法。

Conclusion: BlindGuard为多智能体系统提供了一种实用且通用的无监督防御解决方案。

Abstract: The security of LLM-based multi-agent systems (MAS) is critically threatened
by propagation vulnerability, where malicious agents can distort collective
decision-making through inter-agent message interactions. While existing
supervised defense methods demonstrate promising performance, they may be
impractical in real-world scenarios due to their heavy reliance on labeled
malicious agents to train a supervised malicious detection model. To enable
practical and generalizable MAS defenses, in this paper, we propose BlindGuard,
an unsupervised defense method that learns without requiring any
attack-specific labels or prior knowledge of malicious behaviors. To this end,
we establish a hierarchical agent encoder to capture individual, neighborhood,
and global interaction patterns of each agent, providing a comprehensive
understanding for malicious agent detection. Meanwhile, we design a
corruption-guided detector that consists of directional noise injection and
contrastive learning, allowing effective detection model training solely on
normal agent behaviors. Extensive experiments show that BlindGuard effectively
detects diverse attack types (i.e., prompt injection, memory poisoning, and
tool attack) across MAS with various communication patterns while maintaining
superior generalizability compared to supervised baselines. The code is
available at: https://github.com/MR9812/BlindGuard.

</details>


### [305] [From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework](https://arxiv.org/abs/2508.08147)
*Yunkai Hu,Tianqiao Zhao,Meng Yue*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（LLM）的代理，将电力系统优化的自然语言描述转化为可求解的数学公式，并通过验证与修复生成可靠解。


<details>
  <summary>Details</summary>
Motivation: 直接使用LLM生成解往往不可行或次优，因其缺乏数值精度和约束处理能力。本文旨在结合LLM与优化求解器，提升解的可靠性。

Method: 通过领域感知提示与模式整合LLM，系统验证与迭代修复确保可行性，生成可求解模型与用户友好结果。

Result: 以机组组合问题为例，代理生成最优或接近最优的调度方案及目标成本，验证显著提升解可靠性。

Conclusion: 结合AI与传统优化框架，实现了从高层问题描述到可执行数学模型的桥梁，提升能源系统决策效率。

Abstract: This paper introduces a novel Large Language Models (LLMs)-assisted agent
that automatically converts natural-language descriptions of power system
optimization scenarios into compact, solver-ready formulations and generates
corresponding solutions. In contrast to approaches that rely solely on LLM to
produce solutions directly, the proposed method focuses on discovering a
mathematically compatible formulation that can be efficiently solved by
off-the-shelf optimization solvers. Directly using LLMs to produce solutions
often leads to infeasible or suboptimal results, as these models lack the
numerical precision and constraint-handling capabilities of established
optimization solvers. The pipeline integrates a domain-aware prompt and schema
with an LLM, enforces feasibility through systematic validation and iterative
repair, and returns both solver-ready models and user-facing results. Using the
unit commitment problem as a representative case study, the agent produces
optimal or near-optimal schedules along with the associated objective costs.
Results demonstrate that coupling the solver with task-specific validation
significantly enhances solution reliability. This work shows that combining AI
with established optimization frameworks bridges high-level problem
descriptions and executable mathematical models, enabling more efficient
decision-making in energy systems

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [306] [Accessibility Literacy: Increasing accessibility awareness among young content creators](https://arxiv.org/abs/2508.06512)
*Alina Karakanta*

Main category: cs.HC

TL;DR: 研究探讨了通过小型模块提升年轻内容创作者的无障碍素养，发现短期培训能显著改变其认知和行为意愿。


<details>
  <summary>Details</summary>
Motivation: 随着视听和网络内容的激增，无障碍教育需求增加，但高校课程中优先级低，需探索替代学习方式。

Method: 设计简易培训模块（如信息图和短测验），通过前后调查评估参与者无障碍素养变化。

Result: 参与者无障碍素养有限，但短期培训后认知改变，更愿意使用无障碍工具，效果因内容类型而异。

Conclusion: 小型针对性干预可作为无障碍教育融入正式课程的替代方案，提升包容性和社会公平。

Abstract: The proliferation of audiovisual and web content has created an increasing
need for media accessibility education in various fields. However,
accessibility remains a low priority in university curricula. This project
explores the feasibility of an alternative learning experience aimed at
increasing the accessibility literacy of young content creators, taking web
accessibility as a case study. We propose a mini module that uses simple,
easy-to-use training materials, such as infographics and short quizzes, and can
be easily incorporated in educational programmes along existing courses. A
survey was conducted to investigate the participants' accessibility literacy
before and after training. The findings show that young content creators
generally have limited accessibility literacy but even brief exposure to
accessibility materials contributed to a shift in perceptions. After training,
participants expressed more willingness to implement accessibility tools in
their content, with ways varying depending on content type and purpose. This
suggests that small, yet targeted interventions could be an alternative for
integrating accessibility training into formal education across various
disciplines. While some responses reflected traces of the medical model of
disability and a particularlist view of accessibility, accessibility was
recognised as important for increasing inclusion, improving content, and
shaping a fairer society.

</details>


### [307] [ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets](https://arxiv.org/abs/2508.06732)
*Yuya Kawakami,Daniel Cayan,Dongyu Liu,Kwan-Liu Ma*

Main category: cs.HC

TL;DR: ClimateSOM是一个结合自组织映射（SOM）和大语言模型（LLMs）的可视化分析工作流，用于探索和解释气候集合数据集的变异性。


<details>
  <summary>Details</summary>
Motivation: 气候集合数据集用于捕捉未来条件下的变异性，但理解这些变异性及其模式对气候科学家至关重要。

Method: ClimateSOM通过SOM将时空时间序列抽象为2D空间分布，并利用LLMs辅助解释，支持交互式探索。

Result: 应用于美国西北部和加州的降水预测数据集，展示了ClimateSOM在识别模式和聚类集合模型运行中的实用性。

Conclusion: 专家评估验证了ClimateSOM的实用性和有效性，为气候科学家提供了强大的分析工具。

Abstract: Ensemble datasets are ever more prevalent in various scientific domains. In
climate science, ensemble datasets are used to capture variability in
projections under plausible future conditions including greenhouse and aerosol
emissions. Each ensemble model run produces projections that are fundamentally
similar yet meaningfully distinct. Understanding this variability among
ensemble model runs and analyzing its magnitude and patterns is a vital task
for climate scientists. In this paper, we present ClimateSOM, a visual analysis
workflow that leverages a self-organizing map (SOM) and Large Language Models
(LLMs) to support interactive exploration and interpretation of climate
ensemble datasets. The workflow abstracts climate ensemble model runs -
spatiotemporal time series - into a distribution over a 2D space that captures
the variability among the ensemble model runs using a SOM. LLMs are integrated
to assist in sensemaking of this SOM-defined 2D space, the basis for the visual
analysis tasks. In all, ClimateSOM enables users to explore the variability
among ensemble model runs, identify patterns, compare and cluster the ensemble
model runs. To demonstrate the utility of ClimateSOM, we apply the workflow to
an ensemble dataset of precipitation projections over California and the
Northwestern United States. Furthermore, we conduct a short evaluation of our
LLM integration, and conduct an expert review of the visual workflow and the
insights from the case studies with six domain experts to evaluate our approach
and its utility.

</details>


### [308] [Toward a Logic of Generalization about Visualization as a Decision Aid](https://arxiv.org/abs/2508.06751)
*Alex Kale*

Main category: cs.HC

TL;DR: 本文探讨了可视化研究中泛化逻辑的局限性，特别是作为决策辅助工具时的应用问题，并提出决策理论作为理解情境变化的视角。


<details>
  <summary>Details</summary>
Motivation: 可视化研究在泛化时面临挑战，尤其是在决策支持场景中，需要更清晰的逻辑框架。

Method: 通过决策理论定义决策问题的维度，并分析可视化支持决策时的情境异质性。

Result: 研究发现效用是可视化决策研究中未被充分关注的核心概念，决策理论有助于改善泛化逻辑。

Conclusion: 决策理论可作为可视化研究中理解情境变化的有效工具，提升泛化能力。

Abstract: Visualization as a discipline often grapples with generalization by reasoning
about how study results on the efficacy of a tool in one context might apply to
another context. This work offers an account of the logic of generalization in
visualization research and argues that it struggles in particular with
applications of visualization as a decision aid. We use decision theory to
define the dimensions on which decision problems can vary, and we present an
analysis of heterogeneity in scenarios where visualization supports
decision-making. Our findings identify utility as a focal and under-examined
concept in visualization research on decision-making, demonstrating how the
visualization community's logic of generalization might benefit from using
decision theory as a lens for understanding context variation.

</details>


### [309] [Story Ribbons: Reimagining Storyline Visualizations with Large Language Models](https://arxiv.org/abs/2508.06772)
*Catherine Yeh,Tara Menon,Robin Singh Arya,Helen He,Moira Weigel,Fernanda Viégas,Martin Wattenberg*

Main category: cs.HC

TL;DR: 论文提出了一种基于大语言模型（LLM）的数据解析流程，用于从小说和剧本中自动提取叙事信息，并开发了交互式可视化系统Story Ribbons，帮助文学分析者探索角色和主题的轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以从非结构化故事数据中提取结构化信息，而大语言模型的发展为改进叙事可视化技术提供了机会。

Method: 采用LLM驱动的数据解析流程自动提取叙事信息，并构建交互式可视化系统Story Ribbons。

Result: 通过对36部文学作品进行管道评估和用户研究，证明了LLM在简化叙事可视化创建和揭示新见解方面的潜力。

Conclusion: 论文展示了LLM在叙事分析中的应用潜力，但也指出了AI系统的局限性，并设计了交互模式以应对这些问题。

Abstract: Analyzing literature involves tracking interactions between characters,
locations, and themes. Visualization has the potential to facilitate the
mapping and analysis of these complex relationships, but capturing structured
information from unstructured story data remains a challenge. As large language
models (LLMs) continue to advance, we see an opportunity to use their text
processing and analysis capabilities to augment and reimagine existing
storyline visualization techniques. Toward this goal, we introduce an
LLM-driven data parsing pipeline that automatically extracts relevant narrative
information from novels and scripts. We then apply this pipeline to create
Story Ribbons, an interactive visualization system that helps novice and expert
literary analysts explore detailed character and theme trajectories at multiple
narrative levels. Through pipeline evaluations and user studies with Story
Ribbons on 36 literary works, we demonstrate the potential of LLMs to
streamline narrative visualization creation and reveal new insights about
familiar stories. We also describe current limitations of AI-based systems, and
interaction motifs designed to address these issues.

</details>


### [310] [Methodology for Business Intelligence Solutions in Internet Banking Companies](https://arxiv.org/abs/2508.06773)
*Alex Escalante Viteri,Javier Gamboa Cruzado,Leonidas Asto Huaman*

Main category: cs.HC

TL;DR: 该研究提出了一种新的商业智能方法，以优化互联网银行公司的决策流程，减少时间、人力和成本。


<details>
  <summary>Details</summary>
Motivation: 尽管商业智能在银行业已被广泛研究，但决策效率仍未被高管认可，且缺乏正式的方法论。

Method: 结合基础研究（构建新方法论）和应用研究（实施商业智能解决方案），并通过实验分析30个决策流程。

Result: 新方法显著减少了决策时间、人力和成本。

Conclusion: 新商业智能方法有效优化了互联网银行公司的决策流程。

Abstract: Business intelligence in the banking industry has been studied extensively in
the last decade; however, business executives still do not perceive efficiency
in the decision-making process since the management and treatment of
information are very timeconsuming for the deliverer, generating costs in the
process. On the other hand, there is no formal methodology for developing
business intelligence solutions in this sector. This work aims to optimize
decision-making in a business unit that works with internet banking companies,
reducing the time, the number of people, and the costs involved in
decision-making. To meet the objective, basic and applied research was
conducted. The basic research allowed the construction of a new methodology
from a study of critical success factors and approaches from the business
intelligence literature. The applied research involved the implementation of a
business intelligence solution applying the new methodology in a
pre-experimental study. Thirty decision-making processes were analyzed using
pre-test and post-test data. Tools such as a stopwatch and observation were
used to collect and record data on time spent, the number of people, and the
decision-making costs. This information was processed in the specialized
Minitab18 statistical software, which allowed the observation and confirmation
of relevant results regarding time reduction, the number of people, and the
costs generated. Therefore, it was concluded that the business intelligence
solution, applying the new methodology, optimized decision making in the
business unit that works with internet banking for companies.

</details>


### [311] [Visualization Vibes: The Socio-Indexical Function of Visualization Design](https://arxiv.org/abs/2508.06775)
*Michelle Morgenstern,Amy Rae Fox,Graham M. Jones,Arvind Satyanarayan*

Main category: cs.HC

TL;DR: 论文探讨了数据可视化在传播中不仅传递数据语义，还隐含社会意义，影响公众接受度。


<details>
  <summary>Details</summary>
Motivation: 当前信息环境中，公众对科学和数据的信任缺失，传统可视化研究过于关注数据准确传输，忽视了社会意义的影响。

Method: 结合语言人类学理论，通过民族志访谈分析读者对可视化设计的“氛围”感知及其社会意义。

Result: 研究发现可视化设计特征会引发读者对其社会来源的推断，进而影响接受度。

Conclusion: 论文提出“社会指示性”概念，为公共数据传播中的问题提供了理论和实践解决方案。

Abstract: In contemporary information ecologies saturated with misinformation,
disinformation, and a distrust of science itself, public data communication
faces significant hurdles. Although visualization research has broadened
criteria for effective design, governing paradigms privilege the accurate and
efficient transmission of data. Drawing on theory from linguistic anthropology,
we argue that such approaches-focused on encoding and decoding propositional
content-cannot fully account for how people engage with visualizations and why
particular visualizations might invite adversarial or receptive responses. In
this paper, we present evidence that data visualizations communicate not only
semantic, propositional meaning$\unicode{x2013}$meaning about
data$\unicode{x2013}$but also social, indexical meaning$\unicode{x2013}$meaning
beyond data. From a series of ethnographically-informed interviews, we document
how readers make rich and varied assessments of a visualization's
"vibes"$\unicode{x2013}$inferences about the social provenance of a
visualization based on its design features. Furthermore, these social
attributions have the power to influence reception, as readers' decisions about
how to engage with a visualization concern not only content, or even aesthetic
appeal, but also their sense of alignment or disalignment with the entities
they imagine to be involved in its production and circulation. We argue these
inferences hinge on a function of human sign systems that has thus far been
little studied in data visualization: socio-indexicality, whereby the formal
features (rather than the content) of communication evoke social contexts,
identities, and characteristics. Demonstrating the presence and significance of
this socio-indexical function in visualization, this paper offers both a
conceptual foundation and practical intervention for troubleshooting breakdowns
in public data communication.

</details>


### [312] [Gender and Careers in Platform-Mediated Work: A Longitudinal Study of Online Freelancers](https://arxiv.org/abs/2508.06778)
*Pyeonghwa Kim,Steve Sawyer,Michael Dunn*

Main category: cs.HC

TL;DR: 研究通过五年纵向调查揭示了数字劳动平台上性别不平等对自由职业者长期职业轨迹的影响，提出了职业去权化和平台中介的母亲惩罚概念，并提出了促进性别平等的设计建议。


<details>
  <summary>Details</summary>
Motivation: 数字劳动平台上的性别不平等问题在CSCW领域备受关注，但长期影响的研究有限。

Method: 对105名Upwork自由职业者进行了五年纵向研究。

Result: 揭示了性别差异对长期职业轨迹的持续影响，提出了职业去权化和平台中介的母亲惩罚概念。

Conclusion: 研究为CSCW社区提供了促进性别平等和可持续平台工作环境的设计建议。

Abstract: We advance gender-inclusive research within the CSCW field by investigating
the long-term gendered experiences of online freelancers on digital labor
platforms. The prevalence of gender-based inequalities has attracted
significant attention within the CSCW community. Yet, insights remain limited
on how these inequalities shape workers' long-term experiences on digital labor
platforms. Through a five-year longitudinal study of 105 freelancers on Upwork,
we reveal persistent gender disparities that influence workers' long-term work
and career trajectories, raising concerns about the sustainability of
platform-mediated work. We advance the ongoing dialogue on gender inclusivity
in the community by introducing the concepts of career disempowerment and
platform-mediated motherhood penalty and by offering research and design
implications for CSCW to foster more sustainable, equitable platform work
environments for all genders.

</details>


### [313] [Quantifying Visualization Vibes: Measuring Socio-Indexicality at Scale](https://arxiv.org/abs/2508.06786)
*Amy Rae Fox,Michelle Morgenstern,Graham M. Jones,Arvind Satyanarayan*

Main category: cs.HC

TL;DR: 研究探讨了可视化如何传达超出数据本身的信息，提出了分析框架并通过调查验证了社会推断的普遍性和对信任的影响。


<details>
  <summary>Details</summary>
Motivation: 探究可视化在社会文化背景下的功能，以及如何通过设计改进公共数据传播。

Method: 通过属性引发调查，分析社会推断的普遍性和设计特征的影响。

Result: 社会推断可异步研究，普遍存在且影响信任，设计特征与数据主题共同作用。

Conclusion: 扩展可视化研究至社会文化现象，可为公共数据传播提供实用设计建议。

Abstract: What impressions might readers form with visualizations that go beyond the
data they encode? In this paper, we build on recent work that demonstrates the
socio-indexical function of visualization, showing that visualizations
communicate more than the data they explicitly encode. Bridging this with prior
work examining public discourse about visualizations, we contribute an analytic
framework for describing inferences about an artifact's social provenance. Via
a series of attribution-elicitation surveys, we offer descriptive evidence that
these social inferences: (1) can be studied asynchronously, (2) are not unique
to a particular sociocultural group or a function of limited data literacy, and
(3) may influence assessments of trust. Further, we demonstrate (4) how design
features act in concert with the topic and underlying messages of an artifact's
data to give rise to such 'beyond-data' readings. We conclude by discussing the
design and research implications of inferences about social provenance, and why
we believe broadening the scope of research on human factors in visualization
to include sociocultural phenomena can yield actionable design recommendations
to address urgent challenges in public data communication.

</details>


### [314] [Entendimento de Campanhas no Contexto da Atenção Primária à Saúde: Um Processo de Design Socialmente Consciente](https://arxiv.org/abs/2508.06791)
*Deógenes P. da Silva Junior,Jonas Lopes Guerra,Krissia Menezes,Marisa Sel Franco,Roberto Pereira*

Main category: cs.HC

TL;DR: 该报告通过社会意识设计框架分析了社区卫生工作者和地方病控制工作者在初级卫生保健（PHC）中的工作背景，特别是健康活动。


<details>
  <summary>Details</summary>
Motivation: 理解社区卫生工作者和地方病控制工作者在健康活动中的工作背景，以开发适合的管理解决方案。

Method: 采用社会意识设计框架，包括利益相关者识别图、评估框架和符号学框架，以及人物角色和场景分析。

Result: 识别了利益相关者、挑战和社会技术需求，为开发中保真原型提供了依据。

Conclusion: 研究结果为PHC健康活动管理解决方案的原型开发提供了重要信息。

Abstract: This report presents the results of an exploratory analysis of the work
context of Community Health Agents and Endemic Disease Control Agents in
Primary Health Care (PHC), with a particular focus on Health Campaigns. To
understand this context, the study adopted the Socially Aware Design framework,
which employs artifacts and techniques to examine problem domains in a
comprehensive and sociotechnical manner. Methods such as the Stakeholder
Identification Diagram, Evaluation Frame, and Semiotic Framework were applied
to identify stakeholders, anticipate challenges, and elicit social and
technical requirements for the solution. Personas and Scenarios were also used
to illustrate the potential impacts of a solution on various stakeholders and
their life contexts within health campaigns. This report presents the analysis
method, its application, and results, discussing the study's findings to inform
the development of medium-fidelity prototypes for a PHC health campaign
management solution.

</details>


### [315] [Understanding Pedestrian Gesture Misrecognition: Insights from Vision-Language Model Reasoning](https://arxiv.org/abs/2508.06801)
*Tram Thi Minh Tran,Xinyan Yu,Callum Parker,Julie Stephany Berrio Perez,Stewart Worrall,Martin Tomitsch*

Main category: cs.HC

TL;DR: 研究利用GPT-4V作为诊断工具，分析行人手势在自动驾驶车辆（AVs）交互中的误识别模式及原因，提出改进手势设计和AV识别系统的建议。


<details>
  <summary>Details</summary>
Motivation: 行人手势在交通通信中至关重要，但其微妙、模糊和上下文依赖的特性给机器识别带来挑战。

Method: 结合手动视频审查和GPT-4V的定性推理主题分析，研究公共数据集中的行人-车辆交互。

Result: 发现手势可见性、行人行为、交互上下文和环境条件是影响误识别的常见因素。

Conclusion: 研究为手势设计和AV识别系统改进提供了实用建议，并适用于其他机器解释人类手势的领域。

Abstract: Pedestrian gestures play an important role in traffic communication,
particularly in interactions with autonomous vehicles (AVs), yet their subtle,
ambiguous, and context-dependent nature poses persistent challenges for machine
interpretation. This study investigates these challenges by using GPT-4V, a
vision-language model, not as a performance benchmark but as a diagnostic tool
to reveal patterns and causes of gesture misrecognition. We analysed a public
dataset of pedestrian-vehicle interactions, combining manual video review with
thematic analysis of the model's qualitative reasoning. This dual approach
surfaced recurring factors influencing misrecognition, including gesture
visibility, pedestrian behaviour, interaction context, and environmental
conditions. The findings suggest practical considerations for gesture design,
including the value of salience and contextual redundancy, and highlight
opportunities to improve AV recognition systems through richer context
modelling and uncertainty-aware interpretations. While centred on AV-pedestrian
interaction, the method and insights are applicable to other domains where
machines interpret human gestures, such as wearable AR and assistive
technologies.

</details>


### [316] [AdjustAR: AI-Driven In-Situ Adjustment of Site-Specific Augmented Reality Content](https://arxiv.org/abs/2508.06826)
*Nels Numan,Jessica Van Brummelen,Ziwen Lu,Anthony Steed*

Main category: cs.HC

TL;DR: AdjustAR系统利用多模态大语言模型（MLLMs）动态校正AR内容，以应对环境变化导致的虚拟与现实错位问题。


<details>
  <summary>Details</summary>
Motivation: 静态3D模型在动态环境中部署时，虚拟内容可能与现实参照物错位，影响用户体验和上下文理解。

Method: 通过MLLMs分析合成图像（原始视图与实时用户视图），检测错位并修正AR元素的2D位置，再将其投影回3D空间。

Result: 系统能在运行时自动调整AR内容，保持与原始意图的对齐。

Conclusion: AdjustAR通过MLLMs的视觉语义推理，实现了动态环境中AR内容的自动化校正。

Abstract: Site-specific outdoor AR experiences are typically authored using static 3D
models, but are deployed in physical environments that change over time. As a
result, virtual content may become misaligned with its intended real-world
referents, degrading user experience and compromising contextual
interpretation. We present AdjustAR, a system that supports in-situ correction
of AR content in dynamic environments using multimodal large language models
(MLLMs). Given a composite image comprising the originally authored view and
the current live user view from the same perspective, an MLLM detects
contextual misalignments and proposes revised 2D placements for affected AR
elements. These corrections are backprojected into 3D space to update the scene
at runtime. By leveraging MLLMs for visual-semantic reasoning, this approach
enables automated runtime corrections to maintain alignment with the authored
intent as real-world target environments evolve.

</details>


### [317] [Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators](https://arxiv.org/abs/2508.06846)
*Hyo Jin Do,Rachel Ostrand,Werner Geyer,Keerthiram Murugesan,Dennis Wei,Justin Weisz*

Main category: cs.HC

TL;DR: 研究探讨了如何通过设计策略有效传达LLM生成内容的真实性评分，发现用户更信任并偏好基于真实性评分的颜色编码设计。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成内容中虚假信息的问题，并探索如何向用户有效传达真实性评分。

Method: 通过两项情景实验，共208名参与者，比较不同设计策略对用户信任、验证准确性和偏好的影响。

Result: 用户更信任且偏好颜色编码设计，认为其更易于验证准确性。

Conclusion: 研究为LLM应用开发者提供了实用设计指南，旨在校准用户信任、满足偏好并提升用户验证能力。

Abstract: Large language models (LLMs) are susceptible to generating inaccurate or
false information, often referred to as "hallucinations" or "confabulations."
While several technical advancements have been made to detect hallucinated
content by assessing the factuality of the model's responses, there is still
limited research on how to effectively communicate this information to users.
To address this gap, we conducted two scenario-based experiments with a total
of 208 participants to systematically compare the effects of various design
strategies for communicating factuality scores by assessing participants'
ratings of trust, ease in validating response accuracy, and preference. Our
findings reveal that participants preferred and trusted a design in which all
phrases within a response were color-coded based on factuality scores.
Participants also found it easier to validate accuracy of the response in this
style compared to a baseline with no style applied. Our study offers practical
design guidelines for LLM application developers and designers, aimed at
calibrating user trust, aligning with user preferences, and enhancing users'
ability to scrutinize LLM outputs.

</details>


### [318] [Perceiving Slope and Acceleration: Evidence for Variable Tempo Sampling in Pitch-Based Sonification of Functions](https://arxiv.org/abs/2508.06872)
*Danyang Fan,Walker Smith,Takako Fujioka,Chris Chage,Sile O'Modhrain,Diana Deutsch,Sean Follmer*

Main category: cs.HC

TL;DR: 论文提出了一种基于音高的声化新方法（Variable Tempo），通过均匀的y间距采样，提升对数据趋势中斜率和加速度的感知能力。实验表明，该方法在斜率和加速度感知上优于传统方法，且用户对其偏好更高。


<details>
  <summary>Details</summary>
Motivation: 传统声化方法（如Variable Pitch Interval）在斜率和加速度感知上效果不佳，因此需要探索更有效的方法。

Method: 提出Variable Tempo方法，通过均匀y间距采样，生成固定音高间隔但可变节奏的音符。与传统方法（均匀x间距采样）和连续基线方法进行对比实验。

Result: Variable Tempo在斜率比较任务中更准确，加速度感知的差异阈值比其他方法精细13倍以上，用户对其信心更高、心理负担更低、偏好更强。

Conclusion: Variable Tempo是一种更优的声化采样方法，能提升对数据趋势中斜率和加速度的敏感度和准确性。

Abstract: Sonification offers a non-visual way to understand data, with pitch-based
encodings being the most common. Yet, how well people perceive slope and
acceleration-key features of data trends-remains poorly understood. Drawing on
people's natural abilities to perceive tempo, we introduce a novel sampling
method for pitch-based sonification to enhance the perception of slope and
acceleration in univariate functions. While traditional sonification methods
often sample data at uniform x-spacing, yielding notes played at a fixed tempo
with variable pitch intervals (Variable Pitch Interval), our approach samples
at uniform y-spacing, producing notes with consistent pitch intervals but
variable tempo (Variable Tempo). We conducted psychoacoustic experiments to
understand slope and acceleration perception across three sampling methods:
Variable Pitch Interval, Variable Tempo, and a Continuous (no sampling)
baseline. In slope comparison tasks, Variable Tempo was more accurate than the
other methods when modulated by the magnitude ratio between slopes. For
acceleration perception, just-noticeable differences under Variable Tempo were
over 13 times finer than with other methods. Participants also commonly
reported higher confidence, lower mental effort, and a stronger preference for
Variable Tempo compared to other methods. This work contributes models of slope
and acceleration perception across pitch-based sonification techniques,
introduces Variable Tempo as a novel and preferred sampling method, and
provides promising initial evidence that leveraging timing can lead to more
sensitive, accurate, and precise interpretation of derivative-based data
features.

</details>


### [319] [Viewpoint-Tolerant Depth Perception for Shared Extended Space Experience on Wall-Sized Display](https://arxiv.org/abs/2508.06889)
*Dooyoung Kim,Jinseok Hong,Heejeong Ko,Woontack Woo*

Main category: cs.HC

TL;DR: 提出了一种无需个体追踪的视角容忍共享深度感知方法，利用人类认知补偿在墙式显示器上实现3D渲染。


<details>
  <summary>Details</summary>
Motivation: 传统3D显示系统专注于单用户场景，而墙式显示器在扩展空间体验和支持多用户交互方面尚未充分探索。

Method: 研究虚拟深度（dv）和绝对观看距离（da）对人类认知补偿因素（感知距离差异、视角阈值和感知存在感）的影响。

Result: 参与者能在23至37度的偏角下获得深度感知，但虚拟深度过大反而降低感知效果。

Conclusion: 墙式显示器可在博物馆等场所提供无需追踪的沉浸式群体体验。

Abstract: We proposed viewpoint-tolerant shared depth perception without individual
tracking by leveraging human cognitive compensation in universally 3D rendered
images on a wall-sized display. While traditional 3D perception-enabled display
systems have primarily focused on single-user scenarios-adapting rendering
based on head and eye tracking the use of wall-sized displays to extend spatial
experiences and support perceptually coherent multi-user interactions remains
underexplored. We investigated the effects of virtual depths (dv) and absolute
viewing distance (da) on human cognitive compensation factors (perceived
distance difference, viewing angle threshold, and perceived presence) to
construct the wall display-based eXtended Reality (XR) space. Results show that
participants experienced a compelling depth perception even from off-center
angles of 23 to 37 degrees, and largely increasing virtual depth worsens depth
perception and presence factors, highlighting the importance of balancing
extended depth of virtual space and viewing distance from the wall-sized
display. Drawing on these findings, wall-sized displays in venues such as
museums, galleries, and classrooms can evolve beyond 2D information sharing to
offer immersive, spatially extended group experiences without individualized
tracking or wearables.

</details>


### [320] [Your Thoughtful Opponent: Embracing Cognitive Conflict with Peer Agent](https://arxiv.org/abs/2508.06955)
*Kyuwon Kim,Jaeryeong Hwang,Younseo Lee,Jeanhee Lee,Sung-Eun Kimm,Hyo-Jeong So*

Main category: cs.HC

TL;DR: 提出Peer Agent系统，通过模拟对话伙伴在困境游戏中引发社会认知冲突，培养民主技能。


<details>
  <summary>Details</summary>
Motivation: 复杂社会问题频发，教育中培养重视多元观点和协作决策的民主技能日益重要。

Method: 基于Inner Thoughts框架和价值敏感话语分析，设计Peer Agent系统，包含五个核心模块。

Result: 系统能通过语音多方对话参与，模拟社会认知冲突。

Conclusion: Peer Agent系统为教育中民主技能培养提供新工具。

Abstract: As complex societal issues continue to emerge, fostering democratic skills
like valuing diverse perspectives and collaborative decision-making is
increasingly vital in education. In this paper, we propose a Peer Agent (PA)
system designed to simulate a deliberative conversational partner that induces
socio-cognitive conflict within dilemma-based game play. Drawing on by the
Inner Thoughts framework and grounded in value-sensitive discourse analysis,
the PA actively participates in voice-based multi-party deliberation with human
players. The system architecture consists of five core modules: Context
Interpreter, Agent State Manager, Thought Generator, Thought Evaluator, and
Thought Articulator.

</details>


### [321] [Rethinking Privacy Indicators in Extended Reality: Multimodal Design for Situationally Impaired Bystanders](https://arxiv.org/abs/2508.07057)
*Syed Ibrahim Mustafa Shah Bukhari,Maha Sajid,Bo Ji,Brendan David-John*

Main category: cs.HC

TL;DR: 研究探讨了XR设备隐私指示器设计，针对注意力分散或视力受限的旁观者，发现多模态指示器比纯视觉指示器更有效。


<details>
  <summary>Details</summary>
Motivation: 随着XR设备普及，其传感器可能无意捕捉旁观者隐私，现有隐私指示器（如LED灯）假设旁观者能注意到信号，但容易被忽视。

Method: 通过焦点小组设计五种新型隐私指示器，并通过用户研究评估其效果。

Result: 纯视觉指示器在注意力分散或视力受限场景中评分低，多模态指示器更受青睐。

Conclusion: 需采用适应性、多模态且情境感知的设计，以更好地保护XR环境中的旁观者隐私。

Abstract: As Extended Reality (XR) devices become increasingly prevalent in everyday
settings, they raise significant privacy concerns for bystanders: individuals
in the vicinity of an XR device during its use, whom the device sensors may
accidentally capture. Current privacy indicators, such as small LEDs, often
presume that bystanders are attentive enough to interpret the privacy signals.
However, these cues can be easily overlooked when bystanders are distracted or
have limited vision. We define such individuals as situationally impaired
bystanders. This study explores XR privacy indicator designs that are effective
for situationally impaired bystanders. A focus group with eight participants
was conducted to design five novel privacy indicators. We evaluated these
designs through a user study with seven additional participants. Our results
show that visual-only indicators, typical in commercial XR devices, received
low ratings for perceived usefulness in impairment scenarios. In contrast,
multimodal indicators were preferred in privacy-sensitive scenarios with
situationally impaired bystanders. Ultimately, our results highlight the need
to move toward adaptable, multimodal, and situationally aware designs that
effectively support bystander privacy in everyday XR environments.

</details>


### [322] [Beyond Problem Solving: Framing and Problem-Solution Co-Evolution in Data Visualization Design](https://arxiv.org/abs/2508.07058)
*Paul C. Parsons,Prakash Chandra Shukla*

Main category: cs.HC

TL;DR: 该论文探讨了可视化设计中问题框架和解决方案的动态交互，强调了设计中的反思性和共同演化过程。


<details>
  <summary>Details</summary>
Motivation: 现有可视化设计模型过于强调技术问题解决，而忽视了设计的解释性和判断性，因此需要研究专家实践中问题框架和解决方案的共同演化。

Method: 采用混合方法研究，包括设计挑战、日记记录和半结构化访谈，对11位专家进行反思性主题分析。

Result: 发现专家通过隐喻、启发式、草图等方法动态调整问题框架，并将问题理解与解决方案开发紧密结合。

Conclusion: 可视化设计是一个反思性、共同演化的实践，问题框架是持续的活动，需在模型中更好地体现框架和解释性判断。

Abstract: Visualization design is often described as the process of solving a
well-defined problem by navigating a design space. While existing visualization
design models have provided valuable structure and guidance, they tend to
foreground technical problem-solving and underemphasize the interpretive,
judgment-based aspects of design. In contrast, research in other design
disciplines has emphasized the importance of framing--how designers define and
redefine what the problem is--and the co-evolution of problem and solution
spaces through reflective practice. These dimensions remain underexplored in
visualization research, particularly from the perspective of expert
practitioners. This paper investigates how visualization designers frame
problems and navigate the dynamic interplay between problem understanding and
solution development. We conducted a mixed-methods study with 11 expert
practitioners using design challenges, diary entries, and semi-structured
interviews. Through reflexive thematic analysis, we identified key strategies
that participants used to frame problems, reframe them in response to evolving
constraints or insights, and build bridges between problem and solution spaces.
These included using metaphors, heuristics, sketching, primary generators, and
reflective evaluation of failed or incomplete ideas. Our findings contribute an
empirically grounded account of visualization design as a reflective,
co-evolutionary practice, where framing is not a preliminary step but a
continuous activity embedded in design. Participants often reshaped their
understanding of the problem based on solution attempts, tool feedback, and
ethical or narrative concerns. These insights extend current visualization
design models and highlight the need for frameworks that better account for
framing and interpretive judgment. (See paper for full abstract.)

</details>


### [323] [Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust](https://arxiv.org/abs/2508.07095)
*Hyo Jin Do,Werner Geyer*

Main category: cs.HC

TL;DR: 研究探讨了四种披露AI生成内容事实性评估的方式对用户信任的影响，发现隐藏或模糊不准确内容能提高信任且不影响答案质量。


<details>
  <summary>Details</summary>
Motivation: 防止用户因盲目信任AI而做出错误决策，探索如何有效传达AI生成内容的事实性评估。

Method: 测试四种披露策略（透明、关注、不透明、模糊）与无事实性信息的基线对比，通过148人的实验验证效果。

Result: 不透明和模糊策略在保持答案质量的同时提高了用户信任。

Conclusion: 隐藏或模糊不准确内容有助于提升用户对AI的信任。

Abstract: Large language models are known to produce outputs that are plausible but
factually incorrect. To prevent people from making erroneous decisions by
blindly trusting AI, researchers have explored various ways of communicating
factuality estimates in AI-generated outputs to end-users. However, little is
known about whether revealing content estimated to be factually incorrect
influences users' trust when compared to hiding it altogether. We tested four
different ways of disclosing an AI-generated output with factuality
assessments: transparent (highlights less factual content), attention
(highlights factual content), opaque (removes less factual content), ambiguity
(makes less factual content vague), and compared them with a baseline response
without factuality information. We conducted a human subjects research (N =
148) using the strategies in question-answering scenarios. We found that the
opaque and ambiguity strategies led to higher trust while maintaining perceived
answer quality, compared to the other strategies. We discuss the efficacy of
hiding presumably less factual content to build end-user trust.

</details>


### [324] [Toward AI Matching Policies in Homeless Services: A Qualitative Study with Policymakers](https://arxiv.org/abs/2508.07129)
*Caroline M. Johnston,Olga Koumoundouros,Angel Hsing-Chi Hwang,Laura Onasch-Vera,Eric Rice,Phebe Vayanos*

Main category: cs.HC

TL;DR: 研究探讨了AI算法在无家可归者住房资源匹配中的应用，通过访谈发现政策制定者支持AI工具，但需与人类决策结合。


<details>
  <summary>Details</summary>
Motivation: 探索AI在住房资源匹配中的接受度和实际影响，为未来设计负责任算法提供参考。

Method: 对洛杉矶13名政策制定者进行半结构化访谈，分析其对AI工具的看法。

Result: 政策制定者支持AI工具，但强调需与人类决策结合，并关注效率、公平和透明度。

Conclusion: AI工具在住房资源匹配中有潜力，但需进一步研究和设计以满足实际需求。

Abstract: Artificial intelligence researchers have proposed various data-driven
algorithms to improve the processes that match individuals experiencing
homelessness to scarce housing resources. It remains unclear whether and how
these algorithms are received or adopted by practitioners and what their
corresponding consequences are. Through semi-structured interviews with 13
policymakers in homeless services in Los Angeles, we investigate whether such
change-makers are open to the idea of integrating AI into the housing resource
matching process, identifying where they see potential gains and drawbacks from
such a system in issues of efficiency, fairness, and transparency. Our
qualitative analysis indicates that, even when aware of various complicating
factors, policymakers welcome the idea of an AI matching tool if thoughtfully
designed and used in tandem with human decision-makers. Though there is no
consensus as to the exact design of such an AI system, insights from
policymakers raise open questions and design considerations that can be
enlightening for future researchers and practitioners who aim to build
responsible algorithmic systems to support decision-making in low-resource
scenarios.

</details>


### [325] [Canvas3D: Empowering Precise Spatial Control for Image Generation with Constraints from a 3D Virtual Canvas](https://arxiv.org/abs/2508.07135)
*Runlin Duan,Yuzhao Chen,Rahul Jain,Yichen Hu,Jingyu Shi,Karthik Ramani*

Main category: cs.HC

TL;DR: Canvas3D是一个基于3D引擎的交互系统，旨在通过精确的空间操控提升生成式AI的图像生成能力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在图像创作中缺乏对空间布局的精确控制，Canvas3D旨在解决这一问题。

Method: Canvas3D将文本描述转换为3D引擎中的交互对象，用户可直接调整空间布局，生成明确的约束条件指导图像生成。

Result: Canvas3D在空间控制、交互性和用户体验上优于基线系统。

Conclusion: Canvas3D通过3D引擎实现了对生成式AI图像空间布局的精确控制，提升了用户满意度。

Abstract: Generative AI (GenAI) has significantly advanced the ease and flexibility of
image creation. However, it remains a challenge to precisely control spatial
compositions, including object arrangement and scene conditions. To bridge this
gap, we propose Canvas3D, an interactive system leveraging a 3D engine to
enable precise spatial manipulation for image generation. Upon user prompt,
Canvas3D automatically converts textual descriptions into interactive objects
within a 3D engine-driven virtual canvas, empowering direct and precise spatial
configuration. These user-defined arrangements generate explicit spatial
constraints that guide generative models in accurately reflecting user
intentions in the resulting images. We conducted a closed-end comparative study
between Canvas3D and a baseline system. And an open-ended study to evaluate our
system "in the wild". The result indicates that Canvas3D outperforms the
baseline on spatial control, interactivity, and overall user experience.

</details>


### [326] [SketchConcept: Sketching-based Concept Recomposition for Product Design using Generative AI](https://arxiv.org/abs/2508.07141)
*Runlin Duan,Chenfei Zhu,Yuzhao Chen,Dizhi Ma,Jingyu Shi,Ziyi Liu,Karthik Ramani*

Main category: cs.HC

TL;DR: SketchConcept是一个设计支持工具，通过草图与文本描述将设计概念分解为视觉表现与功能，利用多模态生成式AI实现功能到视觉的映射。


<details>
  <summary>Details</summary>
Motivation: 当前基于草图的设计工具主要关注视觉设计，而忽略了功能概念的探索，因此需要一种工具同时支持视觉与功能设计。

Method: 提出功能到视觉的映射工作流，利用大型语言模型生成功能描述，并通过图像生成式AI生成概念组件。

Result: 通过多个用例验证了工作流的有效性，并通过用户研究评估了系统的效能与可用性。

Conclusion: SketchConcept成功实现了视觉与功能概念的协同设计，为概念产品设计提供了新工具。

Abstract: Conceptual product design requires designers to explore the design space of
visual and functional concepts simultaneously. Sketching has long been adopted
to empower concept exploration. However, current sketch-based design tools
mostly emphasize visual design using emerging techniques. We present
SketchConcept, a design support tool that decomposes design concepts into
visual representations and functionality of concepts using sketches and textual
descriptions. We propose a function-to-visual mapping workflow that maps the
function descriptions generated by a Large Language Model to a component of the
concept produced by image Generative Artificial Intelligence(GenAI). The
function-to-visual mapping allows our system to leverage multimodal GenAI to
decompose, generate, and edit the design concept to satisfy the overall
function and behavior. We present multiple use cases enabled by SketchConcept
to validate the workflow. Finally, we evaluated the efficacy and usability of
our system with a two-session user study.

</details>


### [327] [Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI](https://arxiv.org/abs/2508.07183)
*Ahmed M. Abuzuraiq,Philippe Pasquier*

Main category: cs.HC

TL;DR: 论文探讨了在创意背景下，可解释AI（XAI）如何通过暴露和操作大型生成模型的内部结构，支持艺术家的创作实践。


<details>
  <summary>Details</summary>
Motivation: 大型生成模型（如文本到图像扩散系统）通常限制了艺术家的控制权，而通过可解释性技术，可以将其转化为创意材料。

Method: 提出了一种基于工艺的可解释性方法，通过长期实践和模型操作（如模型弯曲和检查插件）来增强艺术家的直觉。

Result: 通过交互式操作生成模型的不同部分，艺术家可以直观理解每个组件对输出的影响。

Conclusion: 大型模型可以通过暴露和操作其内部结构，成为艺术家的创意工具，从而支持更深入的创作实践。

Abstract: Explainable AI (XAI) in creative contexts can go beyond transparency to
support artistic engagement, modifiability, and sustained practice. While
curated datasets and training human-scale models can offer artists greater
agency and control, large-scale generative models like text-to-image diffusion
systems often obscure these possibilities. We suggest that even large models
can be treated as creative materials if their internal structure is exposed and
manipulable. We propose a craft-based approach to explainability rooted in
long-term, hands-on engagement akin to Sch\"on's "reflection-in-action" and
demonstrate its application through a model-bending and inspection plugin
integrated into the node-based interface of ComfyUI. We demonstrate that by
interactively manipulating different parts of a generative model, artists can
develop an intuition about how each component influences the output.

</details>


### [328] [Civil Servants as Builders: Enabling Non-IT Staff to Develop Secure Python and R Tools](https://arxiv.org/abs/2508.07203)
*Prashant Sharma*

Main category: cs.HC

TL;DR: 论文提出了一种开源平台，支持非IT角色的公务员在政府网络中安全地开发、审查和部署小规模应用，填补了现有数字政府研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了具备编程能力但缺乏正式IT支持的公务员群体，导致其技能无法有效应用于政府数字化转型。

Method: 结合Jupyter Notebooks、预批准的开源库和轻量级治理，设计了一个沙盒化、可审计的工作流平台。

Result: 该平台在遵守采购规则和IT安全政策的同时，避免了供应商锁定，并提升了公务员的技术竞争力。

Conclusion: 该研究为公共部门技能保留和自下而上的数字化转型提供了可复制的模型。

Abstract: Current digital government literature focuses on professional in-house IT
teams, specialized digital service teams, vendor-developed systems, or
proprietary low-code/no-code tools. Almost no scholarship addresses a growing
middle ground: technically skilled civil servants outside formal IT roles who
can write real code but lack a sanctioned, secure path to deploy their work.
This paper introduces a limits-aware, open-source and replicable platform that
enables such public servants to develop, peer review, and deploy small-scale,
domain-specific applications within government networks via a sandboxed,
auditable workflow. By combining Jupyter Notebooks, preapproved open-source
libraries, and lightweight governance, the platform works within institutional
constraints such as procurement rules and IT security policies while avoiding
vendor lock-in. Unlike low/no-code approaches, it preserves and enhances civil
servants' programming skills, keeping them technically competitive with their
private-sector peers. This contribution fills a critical gap, offering a
replicable model for public-sector skill retention, resilience, and bottom-up
digital transformation.

</details>


### [329] [Exploring Micro Accidents and Driver Responses in Automated Driving: Insights from Real-world Videos](https://arxiv.org/abs/2508.07256)
*Wei Xiang,Chuyue Zhang,Jie Yan*

Main category: cs.HC

TL;DR: 本文研究了自动驾驶中未被充分探索的微事故（如急减速、蛇形驾驶），通过机器学习分析环境与自动驾驶代理的关键变量，并结合众包方法了解人类风险感知与反应，为自动驾驶系统设计提供改进方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶L3级别虽减轻了驾驶员负担，但引入了新的复杂性，尤其是微事故（非致命但异常的行为）可能引发更严重事故，需深入研究。

Method: 通过收集用户生成的微事故视频数据集，利用机器学习定位环境与自动驾驶代理的关键变量，并结合众包方法分析人类风险感知与反应。

Result: 识别了微事故中的关键变量，并揭示了人类对这些事件的感知与反应，为自动驾驶系统设计提供了新的安全场景特征。

Conclusion: 微事故是自动驾驶安全中的重要问题，研究结果为系统设计提供了改进方向，有助于预防更严重事故。

Abstract: Automated driving in level 3 autonomy has been adopted by multiple companies
such as Tesla and BMW, alleviating the burden on drivers while unveiling new
complexities. This article focused on the under-explored territory of micro
accidents during automated driving, characterized as not fatal but abnormal
aberrations such as abrupt deceleration and snake driving. These micro
accidents are basic yet pervasive events that might results in more severe
accidents. Through collecting a comprehensive dataset of user generated video
recording such micro accidents in natural driving scenarios, this article
locates key variables pertaining to environments and autonomous agents using
machine learning methods. Subsequently, crowdsourcing method provides insights
into human risk perceptions and reactions to these micro accidents. This
article thus describes features of safety critical scenarios other than crashes
and fatal accidents, informing and potentially advancing the design of
automated driving systems.

</details>


### [330] [Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment](https://arxiv.org/abs/2508.07283)
*Bujar Raufi*

Main category: cs.HC

TL;DR: 研究结合脑电图（EEG）微状态与大语言模型（LLMs），通过微调LLMs提升对认知负荷状态（如“休息”和“负荷”）的预测能力。


<details>
  <summary>Details</summary>
Motivation: 探索EEG微状态与LLMs的结合，以改进认知负荷状态的评估，推动认知神经科学和认知AI的发展。

Method: 分为四个阶段：数据收集与预处理、微状态分割与EEG回拟合、特征提取与提示工程、LLM模型选择与微调。

Result: 微调后的LLM在认知负荷状态识别上表现显著提升，验证了EEG-informed LLMs的潜力。

Conclusion: 该方法不仅深化了对脑动力学的理解，还为认知负荷与认知AI研究提供了新的机器学习技术路径。

Abstract: This study explores the intersection of electroencephalography (EEG)
microstates and Large Language Models (LLMs) to enhance the assessment of
cognitive load states. By utilizing EEG microstate features, the research aims
to fine-tune LLMs for improved predictions of distinct cognitive states,
specifically 'Rest' and 'Load'. The experimental design is delineated in four
comprehensive stages: dataset collection and preprocessing, microstate
segmentation and EEG backfitting, feature extraction paired with prompt
engineering, and meticulous LLM model selection and refinement. Employing a
supervised learning paradigm, the LLM is trained to identify cognitive load
states based on EEG microstate features integrated into prompts, producing
accurate discrimination of cognitive load. A curated dataset, linking EEG
features to specified cognitive load conditions, underpins the experimental
framework. The results indicate a significant improvement in model performance
following the proposed fine-tuning, showcasing the potential of EEG-informed
LLMs in cognitive neuroscience and cognitive AI applications. This approach not
only contributes to the understanding of brain dynamics but also paves the way
for advancements in machine learning techniques applicable to cognitive load
and cognitive AI research.

</details>


### [331] [In-person, Online and Back Again -- A Tale of Three Hybrid Hackathons](https://arxiv.org/abs/2508.07301)
*Abasi-amefon Obot Affia-Jomants,Alexander Serebrenik,James D. Herbsleb,Alexander Nolte*

Main category: cs.HC

TL;DR: 本文探讨了混合黑客马拉松（线上线下结合）的独特挑战，分析了组织者和参与者在协作中的关键维度，并提出了实践建议。


<details>
  <summary>Details</summary>
Motivation: 混合黑客马拉松的研究尚不完善，现有策略未能解决其独特挑战，如跨物理和虚拟空间的沟通管理。

Method: 通过三个黑客马拉松的案例研究，分析了同步性、物理分布、动态过渡和技术基础设施等关键维度。

Result: 研究发现组织者对混合维度的考虑不同，导致参与者体验差异；技术使用存在不足，如时区管理和动态过渡。

Conclusion: 提出了组织混合黑客马拉松的实践建议，并指出参与者需适应协作策略以弥补组织不足。

Abstract: Hybrid hackathons, which combine in-person and online participation, present
unique challenges for organizers and participants. Although such events are
increasingly conducted, research on them remains fragmented, with limited
integration between hackathon studies and hybrid collaboration. Existing
strategies for in-person or online-only events often fail to address the unique
challenges of hybrid formats, such as managing communication across physical
and virtual spaces. Our work addresses this gap by examining how hybrid
hackathons function, analyzing how organizers structure these events and how
participants navigate hybrid-specific challenges. Drawing on established
theories of hybrid collaboration, we examine key dimensions - synchronicity,
physical distribution, dynamic transitions, and technological infrastructure -
that shape collaboration in hybrid events. Through an exploratory case study of
three hackathon events, we analyze how these dimensions are implemented and
their effects on participant experiences. Our findings reveal differing
organizer considerations of the hybrid dimensions in the hackathon design,
leading to distinct experiences for participants. Implementation styles -
favoring in-person, online, or balanced participation - led to varied
participant experiences, affecting access to resources, communication, and team
coordination. Organizers in our study also relied on technology to bridge
hybrid interactions, but overlooked critical aspects like time-zone management,
dynamic transitions, and targeted support for hybrid teams. Additionally,
participants in their teams responded to gaps in event scaffolding by adapting
collaboration strategies, revealing gaps in organizers' preparedness for hybrid
events. Learning from our findings, we offer practical recommendations when
organizing hybrid hackathon events and recommendations to participants when
attending them.

</details>


### [332] [Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics](https://arxiv.org/abs/2508.07390)
*Gustavo Moreira,Leonardo Ferreira,Carolina Veiga,Maryam Hosseini,Fabio Miranda*

Main category: cs.HC

TL;DR: Urbanite是一个基于人机协作的框架，旨在通过数据流模型降低城市视觉分析的复杂性，支持多范围意图交互。


<details>
  <summary>Details</summary>
Motivation: 城市数据分析复杂且需要多领域专业知识，现有工具对非技术用户不友好，大语言模型虽能降低门槛，但意图与系统行为对齐存在挑战。

Method: 提出Urbanite框架，采用数据流模型支持多范围意图交互，结合可解释性和任务多分辨率定义，确保交互溯源。

Result: 通过与城市专家合作的场景验证了Urbanite的有效性。

Conclusion: Urbanite通过人机协作和数据流模型，成功降低了城市视觉分析的门槛，并解决了意图对齐问题。

Abstract: With the growing availability of urban data and the increasing complexity of
societal challenges, visual analytics has become essential for deriving
insights into pressing real-world problems. However, analyzing such data is
inherently complex and iterative, requiring expertise across multiple domains.
The need to manage diverse datasets, distill intricate workflows, and integrate
various analytical methods presents a high barrier to entry, especially for
researchers and urban experts who lack proficiency in data management, machine
learning, and visualization. Advancements in large language models offer a
promising solution to lower the barriers to the construction of analytics
systems by enabling users to specify intent rather than define precise
computational operations. However, this shift from explicit operations to
intent-based interaction introduces challenges in ensuring alignment throughout
the design and development process. Without proper mechanisms, gaps can emerge
between user intent, system behavior, and analytical outcomes. To address these
challenges, we propose Urbanite, a framework for human-AI collaboration in
urban visual analytics. Urbanite leverages a dataflow-based model that allows
users to specify intent at multiple scopes, enabling interactive alignment
across the specification, process, and evaluation stages of urban analytics.
Based on findings from a survey to uncover challenges, Urbanite incorporates
features to facilitate explainability, multi-resolution definition of tasks
across dataflows, nodes, and parameters, while supporting the provenance of
interactions. We demonstrate Urbanite's effectiveness through usage scenarios
created in collaboration with urban experts. Urbanite is available at
https://urbantk.org/urbanite.

</details>


### [333] [StreetWeave: A Declarative Grammar for Street-Overlaid Visualization of Multivariate Data](https://arxiv.org/abs/2508.07496)
*Sanjana Srabanti,G. Elisabeta Marai,Fabio Miranda*

Main category: cs.HC

TL;DR: 本文提出了一种名为StreetWeave的声明式语法，用于设计多分辨率空间网络数据的可视化，解决了现有街道和行人网络可视化缺乏统一设计框架的问题。


<details>
  <summary>Details</summary>
Motivation: 街道和行人网络的可视化对城市规划、气候研究和健康专家等领域专家至关重要，但缺乏统一的设计框架，且数据整合和可视化过程复杂，对非编程背景的专家构成障碍。

Method: 通过定性编码分析45项研究，总结了街道和行人网络可视化的用途、方法和数据来源，并基于此设计空间提出了StreetWeave语法。

Result: StreetWeave能够支持多分辨率空间网络数据的自定义可视化，帮助专家高效探索和分析空间数据。

Conclusion: StreetWeave为街道和行人网络可视化提供了灵活的设计工具，降低了技术门槛，推动了跨领域应用。

Abstract: The visualization and analysis of street and pedestrian networks are
important to various domain experts, including urban planners, climate
researchers, and health experts. This has led to the development of new
techniques for street and pedestrian network visualization, expanding how data
can be shown and understood more effectively. Despite their increasing
adoption, there is no established design framework to guide the creation of
these visualizations while addressing the diverse requirements of various
domains. When exploring a feature of interest, domain experts often need to
transform, integrate, and visualize a combination of thematic data (e.g.,
demographic, socioeconomic, pollution) and physical data (e.g., zip codes,
street networks), often spanning multiple spatial and temporal scales. This not
only complicates the process of visual data exploration and system
implementation for developers but also creates significant entry barriers for
experts who lack a background in programming. With this in mind, in this paper,
we reviewed 45 studies utilizing street-overlaid visualizations to understand
how they are used. Through qualitative coding of these visualizations, we
analyzed three key aspects of street and pedestrian network visualization
usage: the analytical purpose they serve, the visualization approaches
employed, and the data sources used in their creation. Building on this design
space, we introduce StreetWeave, a declarative grammar for designing custom
visualizations of multivariate spatial network data across multiple
resolutions. We demonstrate how StreetWeave can be used to create various
street-overlaid visualizations, enabling effective exploration and analysis of
spatial data. StreetWeave is available at https://urbantk.org/streetweave.

</details>


### [334] [VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design](https://arxiv.org/abs/2508.07497)
*Leonardo Ferreira,Gustavo Moreira,Fabio Miranda*

Main category: cs.HC

TL;DR: 本文提出了VA-Blueprint，一种系统化分类和构建城市视觉分析系统的方法论和知识库，填补了该领域设计和开发指导的空白。


<details>
  <summary>Details</summary>
Motivation: 当前视觉分析系统的设计和开发缺乏结构化知识库，导致开发过程效率低下且难以复制。

Method: 通过系统化审查和分类20个城市视觉分析系统的核心组件，构建多级结构的知识库，并利用大语言模型扩展到81篇论文，形成101篇论文的语料库。

Result: 建立了VA-Blueprint知识库，并通过专家访谈和定量分析验证了其有效性。

Conclusion: VA-Blueprint为视觉分析系统的开发提供了结构化、可复制和高效的基础。

Abstract: Designing and building visual analytics (VA) systems is a complex, iterative
process that requires the seamless integration of data processing, analytics
capabilities, and visualization techniques. While prior research has
extensively examined the social and collaborative aspects of VA system
authoring, the practical challenges of developing these systems remain
underexplored. As a result, despite the growing number of VA systems, there are
only a few structured knowledge bases to guide their design and development. To
tackle this gap, we propose VA-Blueprint, a methodology and knowledge base that
systematically reviews and categorizes the fundamental building blocks of urban
VA systems, a domain particularly rich and representative due to its intricate
data and unique problem sets. Applying this methodology to an initial set of 20
systems, we identify and organize their core components into a multi-level
structure, forming an initial knowledge base with a structured blueprint for VA
system development. To scale this effort, we leverage a large language model to
automate the extraction of these components for other 81 papers (completing a
corpus of 101 papers), assessing its effectiveness in scaling knowledge base
construction. We evaluate our method through interviews with experts and a
quantitative analysis of annotation metrics. Our contributions provide a deeper
understanding of VA systems' composition and establish a practical foundation
to support more structured, reproducible, and efficient system development.
VA-Blueprint is available at https://urbantk.org/va-blueprint.

</details>


### [335] [Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI](https://arxiv.org/abs/2508.07520)
*Baihan Lin*

Main category: cs.HC

TL;DR: 提出了一种名为Conversational DNA的新型可视化语言，通过生物隐喻揭示对话的时空结构，为理解对话提供了新框架。


<details>
  <summary>Details</summary>
Motivation: 探索对话中隐藏的模式是否比文字本身更能揭示沟通的本质，特别是在人类与AI对话日益增多的背景下。

Method: 采用生物隐喻（如DNA结构）可视化对话，通过线条粗细、颜色渐变等元素表现对话的复杂性、情感轨迹和话题连贯性。

Result: 在治疗对话和历史性人机对话的分析中，该方法揭示了传统方法忽略的交互模式。

Conclusion: 为理解对话提供了创新的可视化框架，结合了数据可视化、人机交互和对话意义的探索。

Abstract: What if the patterns hidden within dialogue reveal more about communication
than the words themselves? We introduce Conversational DNA, a novel visual
language that treats any dialogue -- whether between humans, between human and
AI, or among groups -- as a living system with interpretable structure that can
be visualized, compared, and understood. Unlike traditional conversation
analysis that reduces rich interaction to statistical summaries, our approach
reveals the temporal architecture of dialogue through biological metaphors.
Linguistic complexity flows through strand thickness, emotional trajectories
cascade through color gradients, conversational relevance forms through
connecting elements, and topic coherence maintains structural integrity through
helical patterns. Through exploratory analysis of therapeutic conversations and
historically significant human-AI dialogues, we demonstrate how this
visualization approach reveals interaction patterns that traditional methods
miss. Our work contributes a new creative framework for understanding
communication that bridges data visualization, human-computer interaction, and
the fundamental question of what makes dialogue meaningful in an age where
humans increasingly converse with artificial minds.

</details>


### [336] [Phoenix: A Novel Context-Aware Voice-Powered Math Equation Workspace and Editor](https://arxiv.org/abs/2508.07576)
*Kenneth Ge,Ryan Paul,Priscilla Zhang,JooYoung Seo*

Main category: cs.HC

TL;DR: 论文提出了一种基于语音的数学工作空间，利用神经科学和大语言模型，为精细运动障碍者提供直观的数学问题解决环境。


<details>
  <summary>Details</summary>
Motivation: 当前基于语音的数学技术依赖精确的符号听写和命令式界面，对精细运动障碍者不友好，且分散认知资源。

Method: 结合神经科学洞察和大语言模型，开发了上下文引擎，支持自然语言交互。

Result: 实现了对精细运动障碍者更流畅的数学参与，减少机械约束。

Conclusion: 该工作空间为精细运动障碍者提供了更直观、低认知负荷的数学问题解决方式。

Abstract: Writing mathematical notation requires substantial effort, diverting
cognitive resources from conceptual understanding to documentation mechanics,
significantly impacting individuals with fine motor disabilities (FMDs).
Current limits of speech-based math technologies rely on precise dictation of
math symbols and unintuitive command-based interfaces. We present a novel
voice-powered math workspace, applying neuroscience insights to create an
intuitive problem-solving environment. To minimize cognitive load, we leverage
large language models with our novel context engine to support natural language
interaction. Ultimately, we enable fluid mathematical engagement for
individuals with FMDs -- freed from mechanical constraints.

</details>


### [337] [On the Limits of Selective AI Prediction: A Case Study in Clinical Decision Making](https://arxiv.org/abs/2508.07617)
*Sarah Jabbour,David Fouhey,Nikola Banovic,Stephanie D. Shepard,Ella Kazerooni,Michael W. Sjoding,Jenna Wiens*

Main category: cs.HC

TL;DR: 选择性预测可以缓解AI不准确预测对决策的负面影响，但会改变错误模式，导致漏诊和漏治增加。


<details>
  <summary>Details</summary>
Motivation: 研究选择性预测对人类决策的影响，特别是在临床环境中，以验证其是否能减少AI不准确预测带来的负面影响。

Method: 对259名临床医生进行用户研究，比较基线表现、AI辅助表现及选择性预测下的表现。

Result: 选择性预测恢复了决策准确率（64% vs. 56%），但增加了漏诊（18%）和漏治（35%）。

Conclusion: 需实证验证人类与AI互动假设，选择性预测虽维持准确率但改变错误模式。

Abstract: AI has the potential to augment human decision making. However, even
high-performing models can produce inaccurate predictions when deployed. These
inaccuracies, combined with automation bias, where humans overrely on AI
predictions, can result in worse decisions. Selective prediction, in which
potentially unreliable model predictions are hidden from users, has been
proposed as a solution. This approach assumes that when AI abstains and informs
the user so, humans make decisions as they would without AI involvement. To
test this assumption, we study the effects of selective prediction on human
decisions in a clinical context. We conducted a user study of 259 clinicians
tasked with diagnosing and treating hospitalized patients. We compared their
baseline performance without any AI involvement to their AI-assisted accuracy
with and without selective prediction. Our findings indicate that selective
prediction mitigates the negative effects of inaccurate AI in terms of decision
accuracy. Compared to no AI assistance, clinician accuracy declined when shown
inaccurate AI predictions (66% [95% CI: 56%-75%] vs. 56% [95% CI: 46%-66%]),
but recovered under selective prediction (64% [95% CI: 54%-73%]). However,
while selective prediction nearly maintains overall accuracy, our results
suggest that it alters patterns of mistakes: when informed the AI abstains,
clinicians underdiagnose (18% increase in missed diagnoses) and undertreat (35%
increase in missed treatments) compared to no AI input at all. Our findings
underscore the importance of empirically validating assumptions about how
humans engage with AI within human-AI systems.

</details>


### [338] [Are UX evaluation methods truly accessible](https://arxiv.org/abs/2508.07620)
*Andrés Eduardo Fuentes-Cortázar,Alejandra Rivera-Hernández,José Rafael Rojano-Cáceres*

Main category: cs.HC

TL;DR: 研究分析了针对聋人用户设计的用户体验（UX）评估方法，发现传统方法存在显著障碍，需改进以确保真正的无障碍性。


<details>
  <summary>Details</summary>
Motivation: 为聋人用户提供公平和包容的用户体验是设计目标，但传统评估方法依赖听觉和认知能力，忽视了聋人的特殊需求。

Method: 通过文献批判性回顾和实际应用，分析评估方法在聋人用户中的适用性。

Result: 尽管文献推荐的方法常用，但存在关键缺陷，无法准确收集聋人用户的数据。

Conclusion: 需改进UX评估方法，以满足聋人群体的沟通和认知需求，真实反映其用户体验。

Abstract: Providing an equitable and inclusive user experience (UX) for people with
disabilities (PWD) is a central goal of accessible design. In the specific case
of Deaf users, whose hearing impairments impact language development and
communication, it is essential to consider their specific needs during software
evaluation processes. This study aimed to analyze a set of UX evaluation
methods suggested in the literature as suitable for Deaf individuals, with the
goal of validating their level of accessibility in real-world contexts. The
research was based on a critical review and practical application of these
methods, identifying their strengths and limitations in relation to the
interaction, perception, and comprehension of Deaf users. Traditional
evaluation instruments, commonly designed for hearing individuals, pose
significant barriers when applied to Deaf users due to their re-liance on
auditory and cognitive abilities, as well as the lack of consideration for
commu-nicational accessibility. The results show that although these methods
are frequently rec-ommended, they exhibit critical shortcomings that hinder the
collection of accurate and representative data. It is concluded that it is
essential to adapt UX evaluation methods to ensure genuinely accessible
processes that address the communicative and cognitive needs of the Deaf
community and accurately reflect their user experience.

</details>


### [339] [Through Their Eyes: User Perceptions on Sensitive Attribute Inference of Social Media Videos by Visual Language Models](https://arxiv.org/abs/2508.07658)
*Shuning Zhang,Gengrui Zhang,Yibo Meng,Ziyi Zhang,Hantao Zhao,Xin Yi,Hewu Li*

Main category: cs.HC

TL;DR: 研究通过半结构化访谈探讨用户对视觉语言模型（VLM）从视觉数据推断敏感属性的看法，发现用户对VLM的能力和隐私风险有深刻担忧，并提出了缓解策略和监管期望。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLM）的快速发展，其对敏感用户属性的推断能力引发了隐私风险，但用户对此的理解和反应尚未充分研究。

Method: 采用半结构化访谈（N=17），调查用户对VLM从其视觉数据推断敏感属性的观点。

Result: 用户认为VLM能高精度推断多种敏感属性（如位置、人口统计等），主要担忧包括未经授权的识别、信息滥用和错误推断。用户尝试缓解策略，但对AI的对抗效果持怀疑态度。

Conclusion: 研究强调了增强透明度、用户控制和隐私保护的必要性，为开发负责任的AI系统、隐私技术和政策制定提供了重要依据。

Abstract: The rapid advancement of Visual Language Models (VLMs) has enabled
sophisticated analysis of visual content, leading to concerns about the
inference of sensitive user attributes and subsequent privacy risks. While
technical capabilities of VLMs are increasingly studied, users' understanding,
perceptions, and reactions to these inferences remain less explored, especially
concerning videos uploaded on the social media. This paper addresses this gap
through a semi-structured interview (N=17), investigating user perspectives on
VLM-driven sensitive attribute inference from their visual data. Findings
reveal that users perceive VLMs as capable of inferring a range of attributes,
including location, demographics, and socioeconomic indicators, often with
unsettling accuracy. Key concerns include unauthorized identification, misuse
of personal information, pervasive surveillance, and harm from inaccurate
inferences. Participants reported employing various mitigation strategies,
though with skepticism about their ultimate effectiveness against advanced AI.
Users also articulate clear expectations for platforms and regulators,
emphasizing the need for enhanced transparency, user control, and proactive
privacy safeguards. These insights are crucial for guiding the development of
responsible AI systems, effective privacy-enhancing technologies, and informed
policymaking that aligns with user expectations and societal values.

</details>


### [340] [Understanding Users' Privacy Perceptions Towards LLM's RAG-based Memory](https://arxiv.org/abs/2508.07664)
*Shuning Zhang,Rongjun Ma,Ying Ma,Shixuan Li,Yiqun Xu,Xin Yi,Hewu Li*

Main category: cs.HC

TL;DR: 研究分析了用户对LLM记忆功能的理解、使用和期望，发现用户对记忆系统的认知不完整，关注隐私和控制问题，并提出设计建议。


<details>
  <summary>Details</summary>
Motivation: 探索用户对LLM基于RAG的记忆系统的认知、实践和期望，以改进设计。

Method: 对18名用户进行半结构化访谈，进行主题分析。

Result: 用户对记忆功能认知不完整，关注隐私和控制，希望更透明的管理机制。

Conclusion: 设计需更用户中心化、透明和可信赖，满足用户对记忆系统的控制需求。

Abstract: Large Language Models (LLMs) are increasingly integrating memory
functionalities to provide personalized and context-aware interactions.
However, user understanding, practices and expectations regarding these memory
systems are not yet well understood. This paper presents a thematic analysis of
semi-structured interviews with 18 users to explore their mental models of
LLM's Retrieval Augmented Generation (RAG)-based memory, current usage
practices, perceived benefits and drawbacks, privacy concerns and expectations
for future memory systems. Our findings reveal diverse and often incomplete
mental models of how memory operates. While users appreciate the potential for
enhanced personalization and efficiency, significant concerns exist regarding
privacy, control and the accuracy of remembered information. Users express a
desire for granular control over memory generation, management, usage and
updating, including clear mechanisms for reviewing, editing, deleting and
categorizing memories, as well as transparent insight into how memories and
inferred information are used. We discuss design implications for creating more
user-centric, transparent, and trustworthy LLM memory systems.

</details>


### [341] [Towards Aligning Personalized Conversational Recommendation Agents with Users' Privacy Preferences](https://arxiv.org/abs/2508.07672)
*Shuning Zhang,Ying Ma,Jingruo Chen,Simin Li,Xin Yi,Hewu Li*

Main category: cs.HC

TL;DR: 论文提出传统隐私管理模式已不适用于动态交互的AI代理，主张AI应主动适应用户隐私偏好，并提出基于情境完整性理论和隐私计算理论的框架。


<details>
  <summary>Details</summary>
Motivation: 传统隐私管理模式基于用户对被动工具的单向控制，与AI代理的动态交互特性不匹配，需转向AI主动适应用户隐私偏好。

Method: 提出基于情境完整性理论和隐私计算理论的框架，将隐私控制视为对齐问题，通过隐式或显式反馈学习用户偏好，并利用对齐和帕累托优化平衡隐私与效用。

Result: 提出了框架的公式化、实例化、潜在应用及五个挑战。

Conclusion: AI代理需主动适应用户隐私偏好，而非依赖用户单向控制，未来需解决相关挑战。

Abstract: The proliferation of AI agents, with their complex and context-dependent
actions, renders conventional privacy paradigms obsolete. This position paper
argues that the current model of privacy management, rooted in a user's
unilateral control over a passive tool, is inherently mismatched with the
dynamic and interactive nature of AI agents. We contend that ensuring effective
privacy protection necessitates that the agents proactively align with users'
privacy preferences instead of passively waiting for the user to control. To
ground this shift, and using personalized conversational recommendation agents
as a case, we propose a conceptual framework built on Contextual Integrity (CI)
theory and Privacy Calculus theory. This synthesis first reframes automatically
controlling users' privacy as an alignment problem, where AI agents initially
did not know users' preferences, and would learn their privacy preferences
through implicit or explicit feedback. Upon receiving the preference feedback,
the agents used alignment and Pareto optimization for aligning preferences and
balancing privacy and utility. We introduced formulations and instantiations,
potential applications, as well as five challenges.

</details>


### [342] [Improving Continuous Grasp Force Decoding from EEG with Time-Frequency Regressors and Premotor-Parietal Network Integration](https://arxiv.org/abs/2508.07677)
*Parth G. Dangi,Yogesh Kumar Meena*

Main category: cs.HC

TL;DR: EEGForceMap方法通过提取特定任务信号和改进特征集，显著提升了脑机接口对连续握力的解码能力。


<details>
  <summary>Details</summary>
Motivation: 现有脑机接口模型在解码连续握力时效果有限，且缺乏神经生理学依据，因此需要一种更有效的方法。

Method: 提出EEGForceMap方法，提取前顶叶区域信号并构建三种时频特征集，结合线性、非线性和深度学习回归器进行预测。

Result: 在WAY-EEG-GAL数据集上，EEGForceMap方法比现有模型在特定和独立条件下分别提升了61.7%和55.7%。

Conclusion: EEGForceMap方法显著提升了握力解码能力，为脑机接口在康复和辅助机器人中的应用提供了新思路。

Abstract: Brain-machine interfaces (BMIs) have significantly advanced
neuro-rehabilitation by enhancing motor control. However, accurately decoding
continuous grasp force remains a challenge, limiting the effectiveness of BMI
applications for fine motor tasks. Current models tend to prioritise
algorithmic complexity rather than incorporating neurophysiological insights
into force control, which is essential for developing effective neural
engineering solutions. To address this, we propose EEGForceMap, an EEG-based
methodology that isolates signals from the premotor-parietal region and
extracts task-specific components. We construct three distinct time-frequency
feature sets, which are validated by comparing them with prior studies, and use
them for force prediction with linear, non-linear, and deep learning-based
regressors. The performance of these regressors was evaluated on the
WAY-EEG-GAL dataset that includes 12 subjects. Our results show that
integrating EEGForceMap approach with regressor models yields a 61.7%
improvement in subject-specific conditions (R-squared = 0.815) and a 55.7%
improvement in subject-independent conditions (R-squared = 0.785) over the
state-of-the-art kinematic decoder models. Furthermore, an ablation study
confirms that each preprocessing step significantly enhances decoding accuracy.
This work contributes to the advancement of responsive BMIs for stroke
rehabilitation and assistive robotics by improving EEG-based decoding of
dynamic grasp force.

</details>


### [343] [SimViews: An Interactive Multi-Agent System Simulating Visitor-to-Visitor Conversational Patterns to Present Diverse Perspectives of Artifacts in Virtual Museums](https://arxiv.org/abs/2508.07730)
*Mingyang Su,Chao Liu,Jingling Zhang,WU Shuang,Mingming Fan*

Main category: cs.HC

TL;DR: SimViews是一个基于LLM的多代理系统，通过模拟游客对话模式，在虚拟博物馆中展示多样视角，提升游客理解和参与度。


<details>
  <summary>Details</summary>
Motivation: 虚拟博物馆中难以恰当呈现多样视角并保持游客注意力，而物理博物馆通过游客互动实现多样性。

Method: 利用LLM驱动的多代理模拟不同专业身份的虚拟游客，构建4种对话模式模拟互动。

Result: 与单代理条件相比，SimViews有效促进多样视角展示，提升参与者理解和参与度。

Conclusion: SimViews通过多代理对话模式成功解决了虚拟博物馆中多样视角呈现的挑战。

Abstract: Offering diverse perspectives on a museum artifact can deepen visitors'
understanding and help avoid the cognitive limitations of a single narrative,
ultimately enhancing their overall experience. Physical museums promote
diversity through visitor interactions. However, it remains a challenge to
present multiple voices appropriately while attracting and sustaining a
visitor's attention in the virtual museum. Inspired by recent studies that show
the effectiveness of LLM-powered multi-agents in presenting different opinions
about an event, we propose SimViews, an interactive multi-agent system that
simulates visitor-to-visitor conversational patterns to promote the
presentation of diverse perspectives. The system employs LLM-powered
multi-agents that simulate virtual visitors with different professional
identities, providing diverse interpretations of artifacts. Additionally, we
constructed 4 conversational patterns between users and agents to simulate
visitor interactions. We conducted a within-subject study with 20 participants,
comparing SimViews to a traditional single-agent condition. Our results show
that SimViews effectively facilitates the presentation of diverse perspectives
through conversations, enhancing participants' understanding of viewpoints and
engagement within the virtual museum.

</details>


### [344] [CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning](https://arxiv.org/abs/2508.07731)
*Abdul Basit,Maha Nawaz,Saim Rehman,Muhammad Shafique*

Main category: cs.HC

TL;DR: CognitiveArm是一个基于EEG的脑控假肢系统，通过优化的深度学习模型和嵌入式AI硬件实现实时操作，准确率高达90%，同时支持语音命令。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限设备上实现高效、低延迟的脑机接口（BCI）控制的挑战，以提升假肢的实时性和准确性。

Method: 结合BrainFlow库和优化的深度学习模型，通过进化搜索选择Pareto最优配置，应用模型压缩技术（如剪枝和量化），并设计EEG数据集和标注流程。

Result: 系统在嵌入式硬件上实现了低延迟和实时响应，分类三种核心动作的准确率达90%，支持语音命令切换模式。

Conclusion: CognitiveArm展示了在嵌入式设备上实现高效脑控假肢的潜力，为实际应用提供了可行的解决方案。

Abstract: Efficient control of prosthetic limbs via non-invasive brain-computer
interfaces (BCIs) requires advanced EEG processing, including pre-filtering,
feature extraction, and action prediction, performed in real time on edge AI
hardware. Achieving this on resource-constrained devices presents challenges in
balancing model complexity, computational efficiency, and latency. We present
CognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on
embedded AI hardware, achieving real-time operation without compromising
accuracy. The system integrates BrainFlow, an open-source library for EEG data
acquisition and streaming, with optimized deep learning (DL) models for precise
brain signal classification. Using evolutionary search, we identify
Pareto-optimal DL configurations through hyperparameter tuning, optimizer
analysis, and window selection, analyzed individually and in ensemble
configurations. We apply model compression techniques such as pruning and
quantization to optimize models for embedded deployment, balancing efficiency
and accuracy. We collected an EEG dataset and designed an annotation pipeline
enabling precise labeling of brain signals corresponding to specific intended
actions, forming the basis for training our optimized DL models. CognitiveArm
also supports voice commands for seamless mode switching, enabling control of
the prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded
hardware, it ensures low latency and real-time responsiveness. A full-scale
prototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset,
achieved up to 90% accuracy in classifying three core actions (left, right,
idle). Voice integration enables multiplexed, variable movement for everyday
tasks (e.g., handshake, cup picking), enhancing real-world performance and
demonstrating CognitiveArm's potential for advanced prosthetic control.

</details>


### [345] [Challenges in Mixed Reality in Assisting Adults with ADHD Symptoms](https://arxiv.org/abs/2508.07854)
*Valerie Tan,Jens Gerken*

Main category: cs.HC

TL;DR: 探讨混合现实技术在成人ADHD症状治疗中的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 研究混合现实如何帮助成人ADHD患者，填补现有解决方案的不足。

Method: 分析现有混合现实解决方案的局限性和成人ADHD治疗需求。

Result: 发现成人ADHD专用原型和研究不足，缺乏日常连续性干预方案。

Conclusion: 混合现实在成人ADHD治疗中潜力大，但需更多研究和开发。

Abstract: In this position paper, we discuss symptoms of attention deficit
hyperactivity disorder (ADHD) in adults, as well as available forms of
treatment or assistance in the context of mixed reality. Mixed reality offers
many potentials for assisting adults with symptoms commonly found in (but not
limited to) ADHD, but the availability of mixed reality solutions is not only
limited commercially, but also limited in terms of proof-of-concept prototypes.
We discuss two major challenges with attention assistance using mixed reality
solutions: the limited availability of adult-specific prototypes and studies,
as well as the limited number of solutions that offer continuous intervention
of ADHD-like symptoms that users can employ in their daily life.

</details>


### [346] [Early Explorations of Recommender Systems for Physical Activity and Well-being](https://arxiv.org/abs/2508.07980)
*Alan Said*

Main category: cs.HC

TL;DR: 论文提出了一个关于有形推荐系统的概念框架，关注用户对身体、习惯和健康的反应，强调信任、意图对齐和后果意识三个设计维度。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统通过可穿戴设备和教练工具引导用户行为，用户如何理解、信任和响应这些建议成为新挑战。

Method: 提出概念框架，分析三个设计维度（信任与解释、意图对齐、后果意识），并通过案例和设计反思说明。

Result: 框架揭示了传统推荐逻辑在实体环境中的局限性，为未来系统设计提供了方向。

Conclusion: 未来系统应支持长期健康、行为对齐和社会责任个性化。

Abstract: As recommender systems increasingly guide physical actions, often through
wearables and coaching tools, new challenges arise around how users interpret,
trust, and respond to this advice. This paper introduces a conceptual framework
for tangible recommendations that influence users' bodies, routines, and
well-being. We describe three design dimensions: trust and interpretation,
intent alignment, and consequence awareness. These highlight key limitations in
applying conventional recommender logic to embodied settings. Through examples
and design reflections, we outline how future systems can support long-term
well-being, behavioral alignment, and socially responsible personalization.

</details>


### [347] [EchoAid: Enhancing Livestream Shopping Accessibility for the DHH Community](https://arxiv.org/abs/2508.08020)
*Zeyu Yang,Zheng Wei,Yang Zhang,Xian Xu,Changyang He,Muzhi Zhou,Pan Hui*

Main category: cs.HC

TL;DR: 论文摘要介绍了为聋哑和听力障碍（DHH）群体设计的移动应用EchoAid，旨在解决直播购物中的信息无障碍问题。


<details>
  <summary>Details</summary>
Motivation: 直播购物平台常忽视DHH群体的需求，导致信息获取障碍和认知过载。

Method: 开发EchoAid应用，结合语音转文字、RSVP技术和LLM，简化直播信息流。通过用户反馈迭代设计，并在38名DHH用户中评估。

Result: EchoAid成功提升了产品信息提取效率，减少了认知过载，提供了更个性化的购物体验。

Conclusion: EchoAid的设计和验证过程展示了其在改善DHH用户直播购物体验方面的潜力。

Abstract: Livestream shopping platforms often overlook the accessibility needs of the
Deaf and Hard of Hearing (DHH) community, leading to barriers such as
information inaccessibility and overload. To tackle these challenges, we
developed \textit{EchoAid}, a mobile app designed to improve the livestream
shopping experience for DHH users. \textit{EchoAid} utilizes advanced
speech-to-text conversion, Rapid Serial Visual Presentation (RSVP) technology,
and Large Language Models (LLMs) to simplify the complex information flow in
live sales environments. We conducted exploratory studies with eight DHH
individuals to identify design needs and iteratively developed the
\textit{EchoAid} prototype based on feedback from three participants. We then
evaluate the performance of this system in a user study workshop involving 38
DHH participants. Our findings demonstrate the successful design and validation
process of \textit{EchoAid}, highlighting its potential to enhance product
information extraction, leading to reduced cognitive overload and more engaging
and customized shopping experiences for DHH users.

</details>


### [348] [ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience](https://arxiv.org/abs/2508.08101)
*Yeana Lee Bond,Mungyeong Choe,Baker Kasim Hasan,Arsh Siddiqui,Myounghoon Jeon*

Main category: cs.HC

TL;DR: 研究探讨了基于ChatGPT的车载对话代理在提升驾驶安全性和用户体验方面的潜力，结果显示其优于传统预脚本代理。


<details>
  <summary>Details</summary>
Motivation: 传统车载对话代理依赖预脚本或有限语音命令，限制了自然交互，研究旨在探索更自然的交互方式。

Method: 40名驾驶员在模拟器中测试三种条件（无代理、预脚本代理、ChatGPT代理），比较驾驶表现和主观评价。

Result: ChatGPT代理在驾驶稳定性和主观评价（如能力、信任、偏好）上表现更优，交互主题多样。

Conclusion: 基于大语言模型的车载代理能通过自然、丰富的交互提升驾驶安全和用户体验。

Abstract: Studies on in-vehicle conversational agents have traditionally relied on
pre-scripted prompts or limited voice commands, constraining natural
driver-agent interaction. To resolve this issue, the present study explored the
potential of a ChatGPT-based in-vehicle agent capable of carrying continuous,
multi-turn dialogues. Forty drivers participated in our experiment using a
motion-based driving simulator, comparing three conditions (No agent,
Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.
Results showed that the ChatGPT-based agent condition led to more stable
driving performance across multiple metrics. Participants demonstrated lower
variability in longitudinal acceleration, lateral acceleration, and lane
deviation compared to the other two conditions. In subjective evaluations, the
ChatGPT-based agent also received significantly higher ratings in competence,
animacy, affective trust, and preference compared to the Pre-scripted agent.
Our thematic analysis of driver-agent conversations revealed diverse
interaction patterns in topics, including driving assistance/questions,
entertainment requests, and anthropomorphic interactions. Our results highlight
the potential of LLM-powered in-vehicle conversational agents to enhance
driving safety and user experience through natural, context-rich interactions.

</details>


### [349] [Fuzzy Ontology Embeddings and Visual Query Building for Ontology Exploration](https://arxiv.org/abs/2508.08128)
*Vladimir Zhurov,John Kausch,Kamran Sedig,Mostafa Milani*

Main category: cs.HC

TL;DR: FuzzyVis是一个结合模糊逻辑和可视化界面的系统，旨在简化复杂本体的探索和查询。


<details>
  <summary>Details</summary>
Motivation: 解决非专家用户难以导航大型复杂本体的问题，同时弥补现有查询工具在灵活性和表达性上的不足。

Method: FuzzyVis整合了基于模糊逻辑的查询模型和交互式可视化界面，支持用户通过逻辑运算符组合概念，并使用模糊嵌入进行相似性搜索。

Result: 案例研究表明，FuzzyVis能够满足复杂信息需求，帮助用户在大规模本体中发现相关概念。

Conclusion: FuzzyVis通过模糊语义和嵌入推理，实现了灵活、高效的本体探索和学习。

Abstract: Ontologies play a central role in structuring knowledge across domains,
supporting tasks such as reasoning, data integration, and semantic search.
However, their large size and complexity, particularly in fields such as
biomedicine, computational biology, law, and engineering, make them difficult
for non-experts to navigate. Formal query languages such as SPARQL offer
expressive access but require users to understand the ontology's structure and
syntax. In contrast, visual exploration tools and basic keyword-based search
interfaces are easier to use but often lack flexibility and expressiveness. We
introduce FuzzyVis, a proof-of-concept system that enables intuitive and
expressive exploration of complex ontologies. FuzzyVis integrates two key
components: a fuzzy logic-based querying model built on fuzzy ontology
embeddings, and an interactive visual interface for building and interpreting
queries. Users can construct new composite concepts by selecting and combining
existing ontology concepts using logical operators such as conjunction,
disjunction, and negation. These composite concepts are matched against the
ontology using fuzzy membership-based embeddings, which capture degrees of
membership and support approximate, concept-level similarity search. The visual
interface supports browsing, query composition, and partial search without
requiring formal syntax. By combining fuzzy semantics with embedding-based
reasoning, FuzzyVis enables flexible interpretation, efficient computation, and
exploratory learning. Case studies demonstrate how FuzzyVis supports subtle
information needs and helps users uncover relevant concepts in large, complex
ontologies.

</details>


### [350] [Can AI Explanations Make You Change Your Mind?](https://arxiv.org/abs/2508.08158)
*Laura Spillner,Rachel Ringe,Robert Porzel,Rainer Malaka*

Main category: cs.HC

TL;DR: 研究发现，尽管AI解释有助于用户判断其建议的可信度，但用户往往不会详细阅读解释，影响其对AI建议的接受程度。


<details>
  <summary>Details</summary>
Motivation: 探讨用户在实际使用中是否会仔细阅读AI解释，以及这对他们接受AI建议的影响。

Method: 通过在线研究分析用户对可解释决策支持系统的信任度，观察其对解释的阅读时间和详细程度。

Result: 许多用户未详细阅读AI解释，这影响了他们对AI建议的接受程度。

Conclusion: 需进一步研究如何提高用户对AI解释的关注度，以优化决策支持系统的效果。

Abstract: In the context of AI-based decision support systems, explanations can help
users to judge when to trust the AI's suggestion, and when to question it. In
this way, human oversight can prevent AI errors and biased decision-making.
However, this rests on the assumption that users will consider explanations in
enough detail to be able to catch such errors. We conducted an online study on
trust in explainable DSS, and were surprised to find that in many cases,
participants spent little time on the explanation and did not always consider
it in detail. We present an exploratory analysis of this data, investigating
what factors impact how carefully study participants consider AI explanations,
and how this in turn impacts whether they are open to changing their mind based
on what the AI suggests.

</details>


### [351] [Bringing Everyone to the Table: An Experimental Study of LLM-Facilitated Group Decision Making](https://arxiv.org/abs/2508.08242)
*Mohammed Alsobay,David M. Rothschild,Jake M. Hofman,Daniel G. Goldstein*

Main category: cs.HC

TL;DR: 研究发现，LLM（如GPT-4）作为小组讨论的促进者，能提高信息共享水平，但对最终决策结果无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在小组决策中作为促进者的潜力，以解决信息共享不均的问题。

Method: 通过预注册随机实验，比较了无促进、一次性提示、人类促进者和LLM促进者在隐藏信息任务中的效果。

Result: LLM促进显著提高了信息共享，但对决策结果无显著影响。

Conclusion: LLM促进部分改善了信息共享，但不足以克服隐藏信息效应；实验平台GRAIL开源以支持未来研究。

Abstract: Group decision-making often suffers from uneven information sharing,
hindering decision quality. While large language models (LLMs) have been widely
studied as aids for individuals, their potential to support groups of users,
potentially as facilitators, is relatively underexplored. We present a
pre-registered randomized experiment with 1,475 participants assigned to 281
five-person groups completing a hidden profile task--selecting an optimal city
for a hypothetical sporting event--under one of four facilitation conditions:
no facilitation, a one-time message prompting information sharing, a human
facilitator, or an LLM (GPT-4o) facilitator. We find that LLM facilitation
increases information shared within a discussion by raising the minimum level
of engagement with the task among group members, and that these gains come at
limited cost in terms of participants' attitudes towards the task, their group,
or their facilitator. Whether by human or AI, there is no significant effect of
facilitation on the final decision outcome, suggesting that even substantial
but partial increases in information sharing are insufficient to overcome the
hidden profile effect studied. To support further research into how LLM-based
interfaces can support the future of collaborative decision making, we release
our experimental platform, the Group-AI Interaction Laboratory (GRAIL), as an
open-source tool.

</details>
