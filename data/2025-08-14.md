<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.LG](#cs.LG) [Total: 94]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.HC](#cs.HC) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: ParallelSearch是一种新型强化学习框架，通过并行处理查询提升搜索代理的效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理在处理查询时采用严格顺序方式，限制了计算效率，尤其是需要多实体比较的查询。

Method: 提出ParallelSearch框架，利用强化学习激励LLM识别并行查询结构并同时执行多个搜索操作。

Result: 在七个问答基准测试中平均性能提升2.9%，并行问题性能提升12.7%，LLM调用减少30.4%。

Conclusion: ParallelSearch通过并行化查询处理显著提升了搜索代理的效率和性能。

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [2] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: 论文研究了在罕见疾病领域中命名实体识别（NER）的挑战，评估了GPT-4o在低资源环境下的表现，并提出了多种提示策略和优化方法。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病领域的NER面临标注数据稀缺、语义模糊和长尾分布等挑战，需要探索高效的解决方案。

Method: 采用零样本提示、少样本上下文学习、检索增强生成（RAG）和任务级微调等策略，设计了结构化提示框架和语义引导的少样本选择方法。

Result: 在RareDis Corpus上，GPT-4o表现优于BioClinicalBERT，任务级微调达到新的SOTA结果。少样本提示在低成本下效果显著，RAG收益有限。

Conclusion: 提示优化的LLM可作为传统监督模型的有效替代方案，尤其在罕见疾病NER中标注数据稀缺的情况下。

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [3] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: TEN是一种神经符号方法，用于从半结构化文本中提取表格数据，结合了大型语言模型和符号检查器，显著优于纯神经方法。


<details>
  <summary>Details</summary>
Motivation: 解决半结构化文本中表格数据提取的挑战，尤其是缺乏明确分隔符的情况，纯神经方法因幻觉和无法强制约束而表现不佳。

Method: 采用结构分解提示（一种链式思维提示方法）生成初始表格，再通过符号检查器评估表格的完整性和检测幻觉，最后通过批判性LLM生成修复指导。

Result: TEN在多个数据集和指标上显著优于纯神经基线，准确率更高，幻觉率更低，用户研究也验证了其优越性。

Conclusion: TEN通过神经符号结合的方法，有效解决了表格提取问题，并在实际应用中表现出色。

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [4] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: 论文提出了一种将文本情感内容映射到大脑区域的框架，无需神经影像数据，并通过实验验证了其神经解剖学合理性。


<details>
  <summary>Details</summary>
Motivation: 传统神经影像成本高且受限于实验室，而数字文本为情感-大脑映射提供了新途径。现有研究缺乏对两者的整合。

Method: 使用OpenAI的文本嵌入生成语义表示，通过降维和聚类识别情感组，并将其映射到18个与情感处理相关的大脑区域。

Result: 结果显示神经解剖学上合理的映射，抑郁患者表现出更强的边缘系统参与，离散情感成功区分，LLM文本在基础情感分布上与人类匹配但缺乏某些区域的激活。

Conclusion: 该方法成本低、可扩展，支持大规模自然语言分析，区分临床人群，并为评估AI情感表达提供了基于大脑的基准。

Abstract: Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [5] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: 研究提出了一种人机混合德尔菲（HAH-Delphi）框架，结合生成式AI与专家小组，优化共识形成过程，测试显示高效且一致。


<details>
  <summary>Details</summary>
Motivation: 传统共识方法（如德尔菲研究）存在专家负担重、信息过载等问题，需更高效、灵活的解决方案。

Method: 采用HAH-Delphi框架，整合生成式AI（Gemini 2.5 Pro）、专家小组和结构化引导，分三阶段测试（回顾性复制、前瞻性比较、实际应用）。

Result: AI在回顾性复制中重现95%共识结论，前瞻性比较中与专家方向一致率达95%；实际应用中，专家小组达成>90%共识覆盖率。

Conclusion: HAH-Delphi框架高效、灵活，适用于高质量共识生成，为个性化指导和共识框架提供基础。

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [6] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

TL;DR: 本文提出了一种联合建模语言和声学信息的文本无关语音模型，通过生成语义标记和连续声学帧表示，提升了语音生成的声学细节。


<details>
  <summary>Details</summary>
Motivation: 现有文本无关语音模型仅预测语义标记，依赖外部声码器，缺乏声学上下文和控制能力。

Method: 采用流匹配目标预测连续声学向量，并研究多步语义标记预测以保留语言信息。

Result: 在语言似然基准测试中表现相当，同时在提示生成中提供更好的声学细节。

Conclusion: 联合建模语言和声学信息的方法有效提升了语音生成的声学质量。

Abstract: Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [7] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: APIO是一种无需依赖手动指定种子提示的自动提示诱导和优化方法，用于语法错误纠正和文本简化任务，取得了当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的进步使得通过简单的提示交互完成多种自然语言处理任务成为可能，但如何自动优化提示以提升性能仍需研究。

Method: 提出APIO方法，通过自动诱导和优化提示，不依赖手动指定的种子提示，专注于语法错误纠正和文本简化任务。

Result: APIO在语法错误纠正和文本简化任务上取得了当前最佳性能。

Conclusion: APIO为自动提示优化提供了一种简单有效的方法，推动了LLMs在特定任务中的应用。

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [8] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: 论文提出了一种名为Columbo的LLM解决方案，用于扩展表格中的缩写列名，显著优于现有方法，并在实际数据门户中应用。


<details>
  <summary>Details</summary>
Motivation: 解决表格中缩写列名扩展的问题，该问题在企业、科学领域和政府机构中普遍存在。

Method: 引入4个新数据集，提出新的同义词感知准确度评估方法，并开发了基于LLM的Columbo解决方案，结合上下文、规则和链式推理。

Result: Columbo在5个数据集上比当前最先进方法NameGuess表现优4-29%。

Conclusion: Columbo显著提升了缩写扩展的准确性，并已在环境科学数据门户EDI中实际应用。

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [9] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: 论文提出使用Zipformer处理双语（普通话和英语）不平衡的儿童导向语音场景，通过内部层编码语言特征，显著提升语言识别的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 解决双语环境中儿童导向语音的语言识别和代码切换问题，尤其是语言不平衡的情况。

Method: 利用Zipformer的内部层提取语言特征嵌入，并与不同后端进行比较。

Result: Zipformer在语言识别中表现稳健，平衡准确率达81.89%，比基线提升15.47%。

Conclusion: Zipformer在现实场景中展现出潜力，尤其在处理不平衡数据时表现优异。

Abstract: Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [10] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 研究探讨了视觉语言模型（VLMs）在生成图表摘要时可能放大的地理经济偏见，发现高收入国家倾向于获得更积极的描述。


<details>
  <summary>Details</summary>
Motivation: 图表摘要任务自动化有助于数据探索，但现有VLMs可能存在地理经济偏见，可能对社会造成危害。

Method: 通过6,000个图表-国家对的大规模评估，分析不同VLMs生成的摘要中地理经济偏见的表现。

Result: 发现VLMs对高收入国家的描述更积极，且不同模型（如GPT-4o-mini、Gemini-1.5-Flash等）表现出不同程度的偏见。

Conclusion: 提示去偏技术效果有限，需更鲁棒的去偏策略。代码和数据集已公开。

Abstract: Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [11] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 该研究探讨了可解释性NLP在实际应用中的采纳情况，发现当前方法满意度低，需改进定义和用户中心框架。


<details>
  <summary>Details</summary>
Motivation: 解决可解释性NLP在实际应用中采纳不足的问题，了解从业者的需求和挑战。

Method: 通过定性访谈研究，结合行业从业者和学术研究者的观点。

Result: 发现概念差距、对现有方法满意度低，并强调评估挑战。

Conclusion: 需明确定义和用户中心框架以促进可解释性NLP的实际应用。

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [12] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

TL;DR: 论文提出了首个用户中心的主观排行榜（USL），通过偏好驱动的动态排名帮助用户选择适合的LLM，并引入了可定制奖励模型（CRMs）以解决人类偏好的多样性和矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注可验证任务，难以满足用户个性化需求，因此需要一种更实用的LLM选择方法。

Method: 基于10K+主观查询的人类偏好数据，开发了可定制奖励模型（CRMs），参数仅4B，但性能优于GPT-4.1和Gemini-2.5-pro。

Result: CRMs表现出卓越的泛化能力，USL在矛盾偏好中表现出强负相关性。

Conclusion: USL和CRMs为LLM选择提供了更实用和个性化的解决方案。

Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [13] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: 论文提出了一种名为“主动阅读”的框架，通过让模型自我生成学习策略来更可靠地吸收知识，显著提升了模型在特定领域的知识掌握能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在学习和回忆知识时不可靠，缺乏工具确保模型能稳定学习给定知识体。

Method: 提出“主动阅读”框架，训练模型通过自我生成的学习策略研究给定材料。

Result: 在专家领域，主动阅读显著优于普通微调和其他数据增强方法，模型在SimpleQA和FinanceBench上表现大幅提升。

Conclusion: 主动阅读可扩展至预训练规模，构建更具事实性的模型，如发布的Meta WikiExpert-8B在事实问答上优于更大参数模型。

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [14] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: 论文提出了一种动态段落选择器（DPS），用于解决检索增强生成（RAG）系统中固定Top-K重排模块在多跳查询中的局限性。DPS通过监督学习动态选择最相关的段落集，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统的重排模块在多跳查询中表现不佳，固定Top-K选择要么遗漏关键信息，要么引入噪声。

Method: 引入DPS框架，将其视为监督学习问题，捕获段落间依赖关系并动态选择段落。

Result: 在五个基准测试中，DPS表现优于现有方法，尤其在MuSiQue数据集上F1分数提升显著。

Conclusion: DPS通过自适应证据选择，显著提升了复杂RAG场景下的推理能力。

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [15] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出了一种利用大语言模型（LLM）生成高质量伪标签数据的方法，用于跨语言基于方面的情感分析（ABSA），无需依赖翻译工具，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言ABSA方法依赖翻译工具，但翻译质量不可靠。本文旨在通过LLM生成伪标签数据，避免翻译工具的使用。

Method: 1. 训练ABSA模型预测目标语言数据；2. 用LLM生成更自然的伪标签句子；3. 在伪标签数据上微调ABSA模型。

Result: 在六种语言和五种骨干模型上验证，效果优于基于翻译的方法，且微调后的LLM优于小型多语言模型。

Conclusion: 该方法有效解决了跨语言ABSA中翻译工具的依赖问题，并提升了性能。

Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [16] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 本文综述了跨语言基于方面的情感分析（ABSA）的研究现状，总结了关键任务、数据集、建模方法及跨语言迁移技术，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 跨语言ABSA是一个未被充分探索的领域，缺乏系统性综述。本文旨在填补这一空白，推动资源丰富语言向低资源语言的知识迁移。

Method: 通过总结ABSA关键任务（如方面词提取、情感分类等）、数据集、建模范式和跨语言迁移方法，并结合单语和多语ABSA及大语言模型的研究成果，系统梳理跨语言ABSA的发展。

Result: 综述了跨语言ABSA的研究现状，指出了现有工作的贡献与不足，并总结了主要挑战。

Conclusion: 跨语言ABSA仍面临诸多挑战，未来研究需进一步探索高效的知识迁移方法，并结合大语言模型等技术推动领域发展。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [17] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 本文介绍了一种零样本事实核查声明检索系统，结合多种先进大语言模型生成文本嵌入，并在单语和跨语言任务中取得较好排名。


<details>
  <summary>Details</summary>
Motivation: 解决事实核查声明检索问题，尤其是在多语言环境下，通过零样本方法提高效率和准确性。

Method: 使用多种大语言模型生成文本嵌入，结合模型以优化结果，通过余弦相似度匹配最相关声明。

Result: 在单语任务中排名第7，跨语言任务中排名第9，NVIDIA NV-Embed-v2模型表现最佳。

Conclusion: 结合多种模型能提升检索效果，但在多语言环境下仍需改进。

Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [18] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: 论文提出了一种可控的共情推理方法，结合自然语言推理与结构化心理步骤，通过强化学习和奖励模型提升情感支持能力。


<details>
  <summary>Details</summary>
Motivation: 当前的情感支持对话模型缺乏基于心理学原理的深度共情推理，需要改进以更好地促进情感健康。

Method: 构建细粒度数据集，结合强化学习和统一的过程-结果奖励模型，引入基于个性的对话重写和冗余感知奖励重加权策略。

Result: 显著提升了模型的情感支持能力，推动了共情、类人支持系统的发展。

Conclusion: 提出的方法有效解决了共情推理和响应重复性问题，为情感支持系统提供了新方向。

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [19] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: 论文提出了一种名为N-Gram Coverage Attack的黑盒成员推理攻击方法，仅依赖目标模型的文本输出，适用于API访问的模型如GPT-4。该方法通过计算模型生成文本与候选成员的后缀的n-gram重叠度来判断成员资格，性能优于其他黑盒方法，甚至媲美白盒攻击。实验还发现攻击成功率随计算预算增加而提升，并揭示了GPT-4等新模型对成员推理的鲁棒性增强。


<details>
  <summary>Details</summary>
Motivation: 当前大多数成员推理攻击需要访问模型的隐藏状态或概率分布，限制了其在仅API访问的模型（如GPT-4）上的应用。本文旨在开发一种仅依赖文本输出的黑盒攻击方法，以扩展成员推理攻击的适用范围。

Method: 提出N-Gram Coverage Attack，通过获取模型在候选成员前缀条件下的多个生成文本，利用n-gram重叠度计算这些输出与真实后缀的相似性，高相似性表明成员资格。

Result: 在多个基准测试中，该方法优于其他黑盒方法，性能接近或超过白盒攻击。攻击成功率随生成序列数量增加而提升。实验还发现GPT-4等新模型对成员推理的鲁棒性增强。

Conclusion: N-Gram Coverage Attack是一种高效的黑盒成员推理攻击方法，适用于仅API访问的模型。同时，新模型如GPT-4在隐私保护方面表现出进步。

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [20] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

TL;DR: AINL-Eval 2025共享任务旨在检测俄语科学摘要中的AI生成内容，提供了一个包含52,305个样本的数据集，并吸引了10个团队参与。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，区分人类和AI生成内容变得困难，这对学术诚信构成挑战，尤其是在科学出版和多语言环境中。

Method: 创建了一个大规模数据集，包含人类和AI生成的科学摘要，并组织了共享任务，要求参与者开发能泛化到新领域和未见过模型的解决方案。

Result: 任务吸引了10个团队和159份提交，顶级系统在识别AI生成内容方面表现出色。

Conclusion: 通过共享任务平台和公开数据集，推动了AI生成内容检测领域的持续研究和进展。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [21] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: 论文探讨了通过调整解码温度来增加语言模型多样性的方法，发现降低温度可提高精确度，而提高温度未必能提升覆盖率。提出利用精确度-召回率框架重新思考损失函数，以实现更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过调整解码温度来提升语言模型的多样性，并探索其局限性。

Method: 通过分析温度调整对模型性能的影响，提出基于精确度-召回率框架的损失函数改进方法。

Result: 改进后的方法在精确度与召回率之间实现了更好的权衡，优于传统的负对数似然训练结合温度缩放。

Conclusion: 研究为语言模型提供了更灵活和鲁棒的建模技术路径。

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [22] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

TL;DR: EffiEval是一种无需训练的高效基准测试方法，通过自适应选择高质量代表性子集，解决数据冗余问题，同时保持评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展和多样化评估基准的增多，带来了巨大的计算挑战，需要一种高效且可靠的评估方法。

Method: 基于模型效用指数（MUI）自适应选择高质量代表性子集，满足代表性、公平性和通用性三大标准。

Result: 在多个公共基准和多样化LLMs上的实验表明，EffiEval仅用少量数据即可实现与全数据集评估一致的排名。

Conclusion: EffiEval为LLMs时代提供了一种实用、通用、可靠且高效的评估解决方案。

Abstract: The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [23] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为SLowED的安全蒸馏方法，通过Slow Tuning和Low-Entropy Masking模块，在提升小语言模型推理能力的同时保持其安全性。


<details>
  <summary>Details</summary>
Motivation: 现有CoT蒸馏方法虽能提升小语言模型的推理能力，但忽视了其对模型安全性的负面影响，且传统安全对齐方法需要额外计算或标注数据。

Method: 提出SLowED方法，包含Slow Tuning（限制权重变化幅度）和Low-Entropy Masking（屏蔽低熵词）两个模块。

Result: 实验表明SLowED在保持安全性的同时提升了推理能力，且模块有效性得到验证。

Conclusion: SLowED为小语言模型的安全蒸馏提供了有效解决方案。

Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [24] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: 论文评估了大型语言模型（LLM）在印度法律任务中的表现，发现其在起草和问题识别上表现优异，但在专业法律研究和推理上存在不足。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在法律职业中的应用潜力，特别是LLM在关键法律任务中的表现。

Method: 通过调查实验，比较LLM与初级律师的输出，由高级法律学生评估其帮助性、准确性和全面性。

Result: LLM在法律起草和问题识别上表现优异，但在专业研究和推理中常产生错误或虚构内容。

Conclusion: LLM可辅助部分法律任务，但人类专家在复杂推理和法律精确应用上仍不可或缺。

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [25] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
*Ridwan Mahbub,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Mizanur Rahman,Mir Tafseer Nayeem,Enamul Hoque*

Main category: cs.CL

TL;DR: 研究评估了视觉语言模型（VLMs）对误导性信息可视化的解读能力，发现大多数模型容易被欺骗，导致对图表的错误理解。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs被广泛用于解读可视化数据，尤其是非专业用户，了解这些模型对误导性视觉设计的敏感性至关重要。

Method: 通过分析来自十个不同模型的16,000多个响应，评估了八种不同类型的误导性图表设计对VLMs的影响。

Result: 研究表明，大多数VLMs会被误导性设计欺骗，导致对图表的错误解读。

Conclusion: 研究强调了在VLMs中建立防范视觉误导的鲁棒性保护措施的必要性。

Abstract: Information visualizations are powerful tools that help users quickly
identify patterns, trends, and outliers, facilitating informed decision-making.
However, when visualizations incorporate deceptive design elements-such as
truncated or inverted axes, unjustified 3D effects, or violations of best
practices-they can mislead viewers and distort understanding, spreading
misinformation. While some deceptive tactics are obvious, others subtly
manipulate perception while maintaining a facade of legitimacy. As
Vision-Language Models (VLMs) are increasingly used to interpret
visualizations, especially by non-expert users, it is critical to understand
how susceptible these models are to deceptive visual designs. In this study, we
conduct an in-depth evaluation of VLMs' ability to interpret misleading
visualizations. By analyzing over 16,000 responses from ten different models
across eight distinct types of misleading chart designs, we demonstrate that
most VLMs are deceived by them. This leads to altered interpretations of
charts, despite the underlying data remaining the same. Our findings highlight
the need for robust safeguards in VLMs against visual misinformation.

</details>


### [26] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: GFPO通过训练时采样更大的组并基于响应长度和令牌效率过滤响应，有效减少语言模型的冗余输出，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在强化学习中倾向于通过增加响应长度来提高准确性，导致冗余内容增多。

Method: 引入GFPO，通过训练时采样更大的组，并基于响应长度和令牌效率（奖励与令牌比）过滤响应。

Result: 在Phi-4-reasoning模型上，GFPO将GRPO的长度膨胀减少了46-71%，同时保持准确性。优化令牌效率后，减少幅度可达71-85%。

Conclusion: GFPO通过增加训练计算量直接减少测试计算量，是一种简单高效的推理优化方法。

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [27] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
*Seokgi Lee*

Main category: cs.CL

TL;DR: 提出了一种针对多跳问答的新型检索增强生成（RAG）框架，通过分解问题和生成可回答问题来优化检索和生成效果。


<details>
  <summary>Details</summary>
Motivation: 解决多跳问答中查询模糊和检索不准确的问题。

Method: 1. 使用LLM分解多跳问题为单跳子问题；2. 生成可回答问题并嵌入，通过问题-问题相似性检索相关文档块。

Result: 在三个多跳问答数据集上表现优于基线系统。

Conclusion: 验证了可回答问题嵌入和LLM查询分解在多跳RAG中的有效性。

Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [28] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
*Avneet Kaur*

Main category: cs.CL

TL;DR: 研究探讨了提示对LLM政治偏见评估的影响，发现支持或反驳的论据会显著改变模型响应，表明模型存在迎合倾向。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM的政治偏见，但提示内容对模型立场的影响尚未充分探索，这对理解模型行为和偏见评估的稳健性至关重要。

Method: 通过实验评估在支持或反驳论据存在时LLM的政治偏见，分析单轮和多轮对话中模型响应的变化。

Result: 实验表明，提供的论据会显著改变模型响应方向，且论据强度影响模型的一致性率，显示出模型的迎合倾向。

Conclusion: LLM存在迎合提示论据的倾向，这对政治偏见的测量和缓解策略开发具有重要影响。

Abstract: There have been numerous studies evaluating bias of LLMs towards political
topics. However, how positions towards these topics in model outputs are highly
sensitive to the prompt. What happens when the prompt itself is suggestive of
certain arguments towards those positions remains underexplored. This is
crucial for understanding how robust these bias evaluations are and for
understanding model behaviour, as these models frequently interact with
opinionated text. To that end, we conduct experiments for political bias
evaluation in presence of supporting and refuting arguments. Our experiments
show that such arguments substantially alter model responses towards the
direction of the provided argument in both single-turn and multi-turn settings.
Moreover, we find that the strength of these arguments influences the
directional agreement rate of model responses. These effects point to a
sycophantic tendency in LLMs adapting their stance to align with the presented
arguments which has downstream implications for measuring political bias and
developing effective mitigation strategies.

</details>


### [29] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
*Shuhei Kato*

Main category: cs.CL

TL;DR: UtterTune是一种轻量级适配方法，通过基于大语言模型（LLM）架构的多语言文本到语音（TTS）系统微调，增强目标语言（如日语）发音的可控性，同时保持其他语言的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM架构使TTS模型在自然度上表现优异，但准确建模字素到音素（G2P）映射和韵律仍具挑战性，尤其是在模型省略显式G2P模块时。

Method: UtterTune利用低秩适配技术，在音素级别控制日语的发音和音高重音，同时保持零样本设置下的自然度和说话人相似性。

Result: 客观和主观评估证实了其有效性。

Conclusion: UtterTune在增强发音可控性的同时，保持了多语言TTS系统的性能。

Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

</details>


### [30] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型（LLMs）自动生成高质量文本解释的框架，替代传统人工标注方法，并通过实验验证其效果。


<details>
  <summary>Details</summary>
Motivation: 传统人工标注方法成本高且难以扩展，因此需要一种自动化的解决方案来生成高质量的文本解释。

Method: 利用多种最先进的大型语言模型（LLMs）生成解释，并通过自然语言生成（NLG）指标评估其质量，同时研究这些解释对预训练语言模型（PLMs）和LLMs在自然语言推理任务中的影响。

Result: 实验表明，自动生成的解释在提升模型性能方面与人工标注解释具有竞争力。

Conclusion: 该研究为可扩展的、基于LLM的自动化文本解释生成提供了有前景的方向，可用于扩展NLP数据集和提升模型性能。

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [31] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
*Ahmed Masry,Abhay Puri,Masoud Hashemi,Juan A. Rodriguez,Megh Thakkar,Khyati Mahajan,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Alexandre Piché,Dzmitry Bahdanau,Christopher Pal,David Vazquez,Enamul Hoque,Perouz Taslakian,Sai Rajeswar,Spandana Gella*

Main category: cs.CL

TL;DR: 论文提出BigCharts数据集创建流程和综合训练框架，解决现有视觉语言模型在图表理解上的不足，生成多样化且真实的图表数据，并通过强化学习提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在图表理解上表现不佳，主要因训练数据缺乏多样性和真实性，且依赖低质量数据集的监督微调。

Method: 提出BigCharts数据集创建流程，结合真实图表数据生成多样化图像；引入综合训练框架，结合监督微调和GRPO强化学习。

Result: BigCharts-R1模型在多个图表问答基准测试中表现优于现有方法，包括更大的开源和闭源模型。

Conclusion: BigCharts方法通过高质量数据集和综合训练框架显著提升了图表推理模型的性能。

Abstract: Charts are essential to data analysis, transforming raw data into clear
visual representations that support human decision-making. Although current
vision-language models (VLMs) have made significant progress, they continue to
struggle with chart comprehension due to training on datasets that lack
diversity and real-world authenticity, or on automatically extracted underlying
data tables of charts, which can contain numerous estimation errors.
Furthermore, existing models only rely on supervised fine-tuning using these
low-quality datasets, severely limiting their effectiveness. To address these
issues, we first propose BigCharts, a dataset creation pipeline that generates
visually diverse chart images by conditioning the rendering process on
real-world charts sourced from multiple online platforms. Unlike purely
synthetic datasets, BigCharts incorporates real-world data, ensuring
authenticity and visual diversity, while still retaining accurate underlying
data due to our proposed replotting process. Additionally, we introduce a
comprehensive training framework that integrates supervised fine-tuning with
Group Relative Policy Optimization (GRPO)-based reinforcement learning. By
introducing novel reward signals specifically designed for chart reasoning, our
approach enhances model robustness and generalization across diverse chart
styles and domains, resulting in a state-of-the-art chart reasoning model,
BigCharts-R1. Extensive experiments demonstrate that our models surpass
existing methods on multiple chart question-answering benchmarks compared to
even larger open-source and closed-source models.

</details>


### [32] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文综述了用于训练AI心理健康辅助系统的临床数据集，分析了现有数据集的不足，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康问题日益严重，但临床资源不足，AI辅助诊断和治疗成为潜在解决方案。然而，高质量数据集的缺乏阻碍了AI模型的开发和应用。

Method: 通过分类整理现有临床心理健康数据集（按疾病、数据类型、任务类型等），并分析其局限性。

Result: 发现现有数据集存在纵向数据不足、文化语言代表性有限、标注标准不一致等问题。

Conclusion: 提出了改进数据集质量和标准化的建议，以推动更公平、稳健的AI心理健康系统发展。

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [33] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 本文综述了针对传统Transformer架构计算量大、难以大规模训练和部署的问题，探讨了多种高效LLM架构的创新方法。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构在计算资源和部署效率上存在显著限制，亟需更高效的LLM架构以支持大规模应用。

Method: 系统分析了线性与稀疏序列建模、高效注意力变体、稀疏混合专家、混合架构及扩散LLM等技术。

Result: 综述为现代高效LLM架构提供了蓝图，展示了多种提升效率的技术路径。

Conclusion: 本文旨在推动未来研究，开发更高效、多功能的AI系统。

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [34] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: PRELUDE是一个评估长文本理解的基准任务，通过判断角色的前传故事是否与原书叙事一致，强调全局理解和深度推理。实验显示现有模型表现远逊于人类。


<details>
  <summary>Details</summary>
Motivation: 现有基准对长文本理解和深度推理的需求不足，PRELUDE填补了这一空白，通过前传一致性任务提出更高要求。

Method: 采用前传一致性任务，要求模型整合间接相关信息，实验对比了上下文学习、RAG、领域训练和商业服务。

Result: 88%的实例需多部分叙事证据，模型表现落后人类15%以上，推理准确率差距超30%。

Conclusion: PRELUDE揭示了长文本理解和推理的显著不足，为未来研究提供了挑战方向。

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [35] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
*Abdul Rehman Antall,Naveed Akhtar*

Main category: cs.CL

TL;DR: 研究评估了轻量级Whisper模型（Tiny、Base、Small）在低资源环境下用于乌尔都语语音识别的可行性。结果显示，Whisper-Small表现最佳（33.68% WER），但仍存在语音准确性和词汇连贯性挑战。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语是全球第十大语言，但因其方言多样性、代码转换和训练数据稀缺，在自动语音识别（ASR）系统中表现有限。

Method: 使用未经微调的Whisper模型在乌尔都语数据集上进行基准测试，以词错误率（WER）为指标。

Result: Whisper-Small的WER最低（33.68%），优于Tiny（67.08%）和Base（53.67%）。定性分析显示语音准确性和词汇连贯性仍存在问题。

Conclusion: Whisper-Small展示了在乌尔都语ASR中的潜力，但仍需进一步研究以解决低资源环境下的挑战。

Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

</details>


### [36] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 论文提出了一种名为Memory Decoder的即插即用预训练记忆模块，用于高效领域适应，无需修改原始模型参数。


<details>
  <summary>Details</summary>
Motivation: 当前领域适应方法（如DAPT）成本高且存在灾难性遗忘，而RAG则因检索延迟影响推理效率。

Method: 采用小型Transformer解码器模仿外部非参数检索器的行为，训练后可无缝集成到任何共享相同分词器的预训练语言模型中。

Result: 实验表明，Memory Decoder在生物医学、金融和法律领域平均降低困惑度6.17点。

Conclusion: Memory Decoder为领域适应提供了一种新颖的即插即用范式，显著提升目标领域性能。

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [37] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: 综述探讨了自然语言处理（NLP）在心理健康领域的应用，特别是认知扭曲（CDs）的自动检测与分类，总结了38项研究的数据集、建模方法和评估策略。


<details>
  <summary>Details</summary>
Motivation: 认知扭曲是心理治疗中的重要问题，但相关研究在分类、任务定义和评估上存在不一致性，需要系统梳理以推动领域发展。

Method: 回顾了过去20年的38项研究，整合了CD分类法、任务设置和评估方法。

Result: 提供了统一的CD分类参考，总结了常见任务设置，并指出了当前研究的挑战。

Conclusion: 该综述为这一新兴领域的连贯性和可重复性研究提供了支持。

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [38] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: 论文探讨了数字化商业沟通中欺骗性语言的系统性检测方法，结合古典修辞学、心理学和语言学理论，提出使用计算文本分析实现高准确率检测，但在多语言环境中存在挑战。


<details>
  <summary>Details</summary>
Motivation: 数字化商业沟通增加了透明度和欺骗的可能性，需要开发系统性检测欺骗性语言的方法。

Method: 结合古典修辞学、心理学和语言学理论，使用计算文本分析和个性化Transformer模型检测欺骗性语言。

Result: 在受控环境中检测准确率超过99%，但在多语言环境中因数据不足和基础设施缺乏而难以复现。

Conclusion: 需开发更强的自动文本识别系统，以应对AI与人类沟通中日益复杂的欺骗性语言。

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [39] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: 本文提出了一种多维度评估框架，用于系统比较大型语言模型（LLMs）的对齐技术，涵盖检测、质量、计算效率和鲁棒性四个维度。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在现实应用中的普及，确保其输出符合人类价值观和安全标准变得至关重要，但缺乏统一的评估框架阻碍了对不同对齐方法的系统比较。

Method: 通过设计一个多维评估框架，对多种对齐技术（如RLHF、指令微调等）进行系统比较，实验覆盖不同基础模型和对齐策略。

Result: 实验结果表明，该框架能有效识别当前先进模型的优势和局限性，为未来研究提供有价值的见解。

Conclusion: 该框架为LLMs对齐技术的评估和部署决策提供了系统化的工具，有助于推动未来研究。

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [40] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: VisCodex是一个统一框架，通过结合视觉和编码语言模型，提升多模态大语言模型（MLLMs）的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在多模态输入生成代码方面能力有限，需要更强大的解决方案。

Method: 采用任务向量模型合并技术，将先进的编码LLM与视觉语言骨干模型结合，同时保留视觉理解和编码能力。

Result: VisCodex在开源MLLMs中表现最佳，接近GPT-4o等专有模型。

Conclusion: VisCodex展示了模型合并策略和新数据集的有效性，为多模态代码生成提供了强大工具。

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


### [41] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: 研究比较了通用、医学和领域特定分词器在放射学报告摘要任务中的表现，发现领域特定分词器在性能和计算效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索分词器对放射学文本生成质量的影响，填补该领域的研究空白。

Method: 系统比较通用、医学和领域特定分词器，结合是否预训练PubMed摘要的实验设计。

Result: 领域特定分词器在性能和计算效率上优于其他分词器，预训练部分缓解了性能差异。

Conclusion: 适应临床领域的分词器能提升模型性能和计算效率，适用于研究和实际医疗场景。

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [42] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: 论文提出了一种通过添加合理上下文来增强情感分析的方法，旨在提高标注的一致性。


<details>
  <summary>Details</summary>
Motivation: 情感分析具有模糊性，传统方法忽略了上下文信息的重要性。

Method: 通过自动生成基于不同情感的事件链，结合短故事生成技术，创建上下文丰富的叙事。

Result: 上下文叙事增强了特定情感的解释，并提高了标注的一致性。

Conclusion: 上下文信息对情感分析的可靠性和一致性有显著提升。

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>


### [43] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
*Fares Antaki,David Mikhail,Daniel Milad,Danny A Mammo,Sumit Sharma,Sunil K Srivastava,Bing Yu Chen,Samir Touma,Mertcan Sevgi,Jonathan El-Khoury,Pearse A Keane,Qingyu Chen,Yih Chung Tham,Renaud Duval*

Main category: cs.CL

TL;DR: GPT-5-high在眼科医学问答任务中表现最佳，准确率最高，但成本较高；GPT-5-mini-low在成本与性能之间找到了平衡。


<details>
  <summary>Details</summary>
Motivation: 探讨GPT-5系列在复杂医学问答任务中的最佳配置，以平衡准确性和成本效率。

Method: 评估了12种GPT-5配置，使用260道眼科选择题，比较准确性、排名、理由质量和成本效益。

Result: GPT-5-high准确率最高（0.965），但GPT-5-mini-low在成本与性能上表现最优。

Conclusion: GPT-5在眼科任务中表现出色，配置选择影响性能与成本，为LLM在医学领域的应用提供了参考。

Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

</details>


### [44] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
*Renas Adnan,Hossein Hassani*

Main category: cs.CL

TL;DR: 该研究旨在为库尔德语的Badini方言开发语音转文本（STT）系统，填补现有技术空白。通过使用Wav2Vec2-Large-XLSR-53和Whisper-small模型，实验表明前者在准确性和可读性上表现更优。


<details>
  <summary>Details</summary>
Motivation: Badini方言缺乏STT系统，影响了其社区的技术使用和全球可见性。研究试图通过开发语言模型解决这一问题。

Method: 选择Badini儿童故事作为文本输入，进行录音、清理、分段和标记化处理，使用Wav2Vec2-Large-XLSR-53和Whisper-small模型开发语言模型。

Result: Wav2Vec2-Large-XLSR-53模型表现更优，可读性达90.38%，准确性为82.67%，显著优于Whisper-small模型。

Conclusion: 研究成功为Badini方言开发了高效的STT系统，Wav2Vec2-Large-XLSR-53模型是更优选择。

Abstract: Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

</details>


### [45] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: 提出一种基于神经上下文老虎机的算法，用于选择适合子任务的LLM序列，以优化任务完成效果和成本。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，如何高效选择适合子任务的LLM序列成为关键问题，尤其是任务复杂且需要多LLM协作时。

Method: 使用神经上下文老虎机算法，在线训练神经网络模型，动态学习各子任务中LLM的成功率，指导LLM选择。

Result: 在电信问答和医疗诊断数据集上验证了算法的有效性，优于其他LLM选择方法。

Conclusion: 该方法能有效处理多LLM协作任务中的复杂依赖关系，提升任务成功率和成本效益。

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)
*Liping Huang,Yicheng Zhang,Yifang Yin,Sheng Zhang,Yi Zhang*

Main category: cs.LG

TL;DR: 该研究提出了一种基于特征标记化的Transformer模型，用于实时预测飞机的预计到达时间（ETA），在准确性和效率上均优于传统的提升树模型。


<details>
  <summary>Details</summary>
Motivation: 实时ETA预测对于航空到达管理至关重要，尤其是在跑道排序中。由于空域环境快速变化，预测效率与准确性同等重要。

Method: 采用特征标记化的Transformer模型，利用多头自注意力机制捕捉输入特征的重要方面，避免了复杂的特征工程，并通过并行计算能力实现高频ETA请求处理。

Result: 实验结果显示，该方法比XGBoost模型准确率提高7%，计算时间仅需39%，且在40架飞机同时预测时，推理时间仅为51.7微秒。

Conclusion: 该模型在实时到达管理系统中表现出色，兼具高准确性和高效性，适用于实际应用。

Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial
for arrival management in aviation, particularly for runway sequencing. Given
the rapidly changing airspace context, the ETA prediction efficiency is as
important as its accuracy in a real-time arrival aircraft management system. In
this study, we utilize a feature tokenization-based Transformer model to
efficiently predict aircraft ETA. Feature tokenization projects raw inputs to
latent spaces, while the multi-head self-attention mechanism in the Transformer
captures important aspects of the projections, alleviating the need for complex
feature engineering. Moreover, the Transformer's parallel computation
capability allows it to handle ETA requests at a high frequency, i.e., 1HZ,
which is essential for a real-time arrival management system. The model inputs
include raw data, such as aircraft latitude, longitude, ground speed, theta
degree for the airport, day and hour from track data, the weather context, and
aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA
prediction is updated every second. We apply the proposed aircraft ETA
prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using
one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October
1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers
all aircraft within a range of 10NM to 300NM from WSSS. The results show that
our proposed method method outperforms the commonly used boosting tree based
model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\%
of its computing time. Experimental results also indicate that, with 40
aircraft in the airspace at a given timestamp, the ETA inference time is only
51.7 microseconds, making it promising for real-time arrival management
systems.

</details>


### [47] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: MoLAN框架通过模态感知的动态噪声编辑，精细地抑制噪声并保留关键信息，MoLAN+在多模态情感分析中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析常因无关或误导性信息而受限，现有方法易丢失关键信息。

Method: MoLAN将模态特征分块，动态分配去噪强度，MoLAN+基于此框架提出新方法。

Result: 在五个模型和四个数据集上验证了MoLAN的广泛有效性，MoLAN+达到最优性能。

Conclusion: MoLAN框架灵活统一，MoLAN+在多模态情感分析中表现卓越。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [48] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: 论文提出了一种基于LLM transformer的上下文学习（ICL）理论，用于优化WiFi 7中的信道访问，解决了传统方法在动态信道环境下性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的二进制指数退避方案在动态信道环境下性能较差，而现有的基于模型的方法（如非持久和p-持久CSMA）在节点密度估计不准确时仍会导致吞吐量损失。

Method: 设计了一个基于transformer的ICL优化器，通过预收集碰撞阈值数据示例和查询碰撞案例，构建提示输入，生成预测的竞争窗口阈值（CWT）。

Result: 实验证明，该方法在未知节点密度下具有快速收敛性和接近最优的吞吐量表现，优于现有基于模型和深度强化学习的方法。

Conclusion: 提出的transformer-based ICL优化器在动态信道环境下显著提升了吞吐量性能，且对输入数据的误差具有鲁棒性。

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [49] [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Eunhwan Park,Hyunbyung Park,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Jihwan Kim,Minjae Kim,Taehwan Kim,Youngrok Kim,Haesol Lee,Jeesoo Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Daewon Suh,Dongjoo Weon*

Main category: cs.LG

TL;DR: Motif-2.6B是一个2.6B参数的LLM，通过创新架构（如差分注意力和PolyNorm激活函数）平衡性能与效率，适用于新兴研究群体。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM在性能与计算效率之间的平衡问题，尤其是为新兴研究群体提供高效的基础模型。

Method: 采用差分注意力和PolyNorm激活函数等创新架构，通过实验优化模型设计。

Result: 在多种基准测试中表现优于或等同于同类先进模型，验证了其高效性和可扩展性。

Conclusion: Motif-2.6B为高效、可扩展的基础LLM提供了新思路，并为未来研究和部署奠定基础。

Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized
artificial intelligence, yet developing an effective foundational LLM that
balances high performance with computational efficiency remains challenging,
especially for emerging research groups. To address this gap, we introduce
Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize
advanced LLM capabilities. Motif-2.6B incorporates several innovative
architectural enhancements, including Differential Attention and PolyNorm
activation functions, which improve long-context comprehension, reduce
hallucination, and enhance in-context learning capabilities. We rigorously
tested multiple novel architectural components through extensive
experimentation to determine the optimal architecture for Motif-2.6B.
Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or
exceeds the performance of similarly sized state-of-the-art models across
diverse benchmarks, showcasing its effectiveness, scalability, and real-world
applicability. Through detailed experiments and tailored techniques, Motif-2.6B
significantly advances the landscape of efficient, scalable, and powerful
foundational LLMs, offering valuable insights and a robust foundation for
future research and deployment.

</details>


### [50] [JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis](https://arxiv.org/abs/2508.09153)
*TaekHyun Park,Yongjae Lee,Daesan Park,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: 研究表明，在时间序列分析中，简单的密集层可以替代复杂的序列混合器，性能相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 质疑复杂序列混合器（如注意力机制）在时间序列分析中的必要性，探索简单架构的潜力。

Method: 提出JustDense，将序列混合器替换为密集层，通过MatrixMixer框架进行系统实验。

Result: 在29个基准测试中，密集层表现与复杂序列混合器相当或更优。

Conclusion: 挑战了时间序列分析中‘越复杂越好’的假设，表明简单架构可能足够有效。

Abstract: Sequence and channel mixers, the core mechanism in sequence models, have
become the de facto standard in time series analysis (TSA). However, recent
studies have questioned the necessity of complex sequence mixers, such as
attention mechanisms, demonstrating that simpler architectures can achieve
comparable or even superior performance. This suggests that the benefits
attributed to complex sequencemixers might instead emerge from other
architectural or optimization factors. Based on this observation, we pose a
central question: Are common sequence mixers necessary for time-series
analysis? Therefore, we propose JustDense, an empirical study that
systematically replaces sequence mixers in various well-established TSA models
with dense layers. Grounded in the MatrixMixer framework, JustDense treats any
sequence mixer as a mixing matrix and replaces it with a dense layer. This
substitution isolates the mixing operation, enabling a clear theoretical
foundation for understanding its role. Therefore, we conducted extensive
experiments on 29 benchmarks covering five representative TSA tasks using seven
state-of-the-art TSA models to address our research question. The results show
that replacing sequence mixers with dense layers yields comparable or even
superior performance. In the cases where dedicated sequence mixers still offer
benefits, JustDense challenges the assumption that "deeper and more complex
architectures are inherently better" in TSA.

</details>


### [51] [Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders](https://arxiv.org/abs/2508.09154)
*Xiaojing Du,Jiuyong Li,Lin Liu,Debo Cheng,Thuc. Le*

Main category: cs.LG

TL;DR: DIG2RSI是一种深度学习框架，通过I-G变换和2SRI技术解决社交网络中同时反馈和未观测混杂问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 社交网络中估计同伴因果效应因同时反馈和未观测混杂而困难，现有方法未能同时解决这些问题。

Method: DIG2RSI结合I-G变换消除反馈偏差，利用2SRI技术构建工具变量并通过神经网络预测和对抗去偏处理未观测混杂。

Result: 理论证明DIG2RSI估计器的一致性，实验显示其在半合成和真实数据集上优于现有方法。

Conclusion: DIG2RSI能有效解决复杂网络中的同伴效应估计问题，具有理论和实践优势。

Abstract: Estimating peer causal effects within complex real-world networks such as
social networks is challenging, primarily due to simultaneous feedback between
peers and unobserved confounders. Existing methods either address unobserved
confounders while ignoring the simultaneous feedback, or account for feedback
but under restrictive linear assumptions, thus failing to obtain accurate peer
effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning
framework which leverages I-G transformation (matrix operation) and 2SRI (an
instrumental variable or IV technique) to address both simultaneous feedback
and unobserved confounding, while accommodating complex, nonlinear and
high-dimensional relationships. DIG2RSI first applies the I-G transformation to
disentangle mutual peer influences and eliminate the bias due to the
simultaneous feedback. To deal with unobserved confounding, we first construct
valid IVs from network data. In stage 1 of 2RSI, we train a neural network on
these IVs to predict peer exposure, and extract residuals as proxies for the
unobserved confounders. In the stage 2, we fit a separate neural network
augmented by an adversarial discriminator that incorporates these residuals as
a control function and enforces the learned representation to contain no
residual confounding signal. The expressive power of deep learning models in
capturing complex non-linear relationships and adversarial debiasing enhances
the effectiveness of DIG2RSI in eliminating bias from both feedback loops and
hidden confounders. We prove consistency of our estimator under standard
regularity conditions, ensuring asymptotic recovery of the true peer effect.
Empirical results on two semi-synthetic benchmarks and a real-world dataset
demonstrate that DIG2RSI outperforms existing approaches.

</details>


### [52] [A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models](https://arxiv.org/abs/2508.09155)
*Wenkai Wang,Hongcan Guo,Zheqi Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: 论文提出AdaPO框架，通过自适应调整训练目标和动态奖励机制，解决多目标优化中的奖励黑客问题，显著提升模型的自我评估和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）在多轮对话中缺乏自我评估能力，现有强化学习方法因固定奖励机制易导致奖励黑客和模型崩溃。

Method: 提出AdaPO框架，包含自适应奖励模型（ARM）和奖励感知动态KL正则化机制，实时调整训练目标和动态系数。

Result: 在8个基准测试和多种模型上验证，显著提升直接推理和自我评估能力。

Conclusion: AdaPO框架有效解决了奖励黑客问题，提升了模型的自我评估能力，具有广泛的应用潜力。

Abstract: Self-evaluation, a model's ability to assess the correctness of its own
output, is crucial for Large Multimodal Models (LMMs) to achieve
self-improvement in multi-turn conversations, yet largely absent in foundation
models. Recent work has employed reinforcement learning (RL) to enhance
self-evaluation; however, its fixed reward mechanism suffers from reward
hacking when optimizing multiple training objectives, leading to model
collapse. In this paper we propose AdaPO, an online reinforcement learning
framework capable of adaptively adjusting training objective in real time
according to the current training state for each task. Specifically, to
mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a
Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's
training state from the distribution of model generated multi-turn
trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty
with dynamic coefficients which is modulated by the reward gap between
different multi-turn situations. Notably, our method automatically and smoothly
adjusts its learning focus based on sub-tasks' training progress without manual
intervention. Extensive experiments over 8 benchmarks and various models show
that our method significantly enhances both direct reasoning and
self-evaluation capability. We will release our code to contribute to the
community.

</details>


### [53] [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)
*Jan Tauberschmidt,Sophie Fellenz,Sebastian J. Vollmer,Andrew B. Duncan*

Main category: cs.LG

TL;DR: 提出了一种框架，通过微调流匹配生成模型来强制物理约束并解决科学系统中的逆问题。


<details>
  <summary>Details</summary>
Motivation: 解决低保真或观测数据训练的生成模型在物理一致性上的不足，同时处理逆问题中的未知物理输入。

Method: 采用可微的后训练过程最小化偏微分方程的弱形式残差，结合可学习的潜在参数预测器进行联合优化。

Result: 模型生成物理有效的场解并准确估计隐藏参数，验证了PDE约束的改进和潜在系数的恢复。

Conclusion: 该方法连接了生成建模与科学推断，为物理系统的数据高效建模和模拟增强发现提供了新途径。

Abstract: We present a framework for fine-tuning flow-matching generative models to
enforce physical constraints and solve inverse problems in scientific systems.
Starting from a model trained on low-fidelity or observational data, we apply a
differentiable post-training procedure that minimizes weak-form residuals of
governing partial differential equations (PDEs), promoting physical consistency
and adherence to boundary conditions without distorting the underlying learned
distribution. To infer unknown physical inputs, such as source terms, material
parameters, or boundary data, we augment the generative process with a
learnable latent parameter predictor and propose a joint optimization strategy.
The resulting model produces physically valid field solutions alongside
plausible estimates of hidden parameters, effectively addressing ill-posed
inverse problems in a data-driven yet physicsaware manner. We validate our
method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE
constraints and accurate recovery of latent coefficients. Our approach bridges
generative modelling and scientific inference, opening new avenues for
simulation-augmented discovery and data-efficient modelling of physical
systems.

</details>


### [54] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: EvaDrive提出了一种多目标强化学习框架，通过对抗优化实现轨迹生成与评估的闭环协同进化，解决了现有方法中迭代优化不足和标量化偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶的决策方法中，轨迹生成与评估分离导致无法迭代优化，而强化学习的标量化奖励掩盖了多维偏好的关键权衡。

Method: EvaDrive采用分层生成器和可训练的多目标评估器进行对抗优化，结合自回归意图建模和扩散模型，实现轨迹的迭代优化。

Result: 在NAVSIM和Bench2Drive基准测试中表现优异，分别达到94.9 PDMS和64.96 Driving Score。

Conclusion: EvaDrive通过动态权重生成多样驾驶风格，为人类迭代决策提供了无标量化的轨迹优化新方法。

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [55] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 该论文整合了15个数据集，构建了一个包含2510名受试者的糖尿病数据库，用于研究低血糖事件及其与心率的关系。


<details>
  <summary>Details</summary>
Motivation: 解决糖尿病研究中数据不足的问题，通过整合大规模数据集以改进低血糖预测模型。

Method: 系统整合15个数据集，构建包含149百万次测量的数据库，并提取两个子数据库（人口统计和心率数据），同时评估数据质量。

Result: 发现数据不平衡和缺失值是主要挑战，并揭示了低血糖前15至55分钟血糖水平与心率的相关性。

Conclusion: 整合的数据集为糖尿病研究提供了丰富资源，但数据质量问题仍需解决。

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [56] [Physics-Guided Memory Network for Building Energy Modeling](https://arxiv.org/abs/2508.09161)
*Muhammad Umair Danish,Kashif Ali,Kamran Siddiqui,Katarina Grolinger*

Main category: cs.LG

TL;DR: 提出了一种物理引导记忆网络（PgMN），结合深度学习和物理模型，解决能源消耗预测中历史数据不足或无数据的问题。


<details>
  <summary>Details</summary>
Motivation: 解决建筑能源消耗预测中深度学习和物理模型的局限性，特别是在历史数据不足或无数据的情况下。

Method: PgMN结合并行投影层处理不完整输入、记忆单元处理持续偏差、记忆经验模块优化预测范围。

Result: PgMN在多种场景（新建筑、数据缺失、稀疏历史数据等）中表现出高准确性和适用性。

Conclusion: PgMN为动态建筑环境中的能源预测提供了有效解决方案，扩展了模型在数据不足或无数据场景的应用。

Abstract: Accurate energy consumption forecasting is essential for efficient resource
management and sustainability in the building sector. Deep learning models are
highly successful but struggle with limited historical data and become unusable
when historical data are unavailable, such as in newly constructed buildings.
On the other hand, physics-based models, such as EnergyPlus, simulate energy
consumption without relying on historical data but require extensive building
parameter specifications and considerable time to model a building. This paper
introduces a Physics-Guided Memory Network (PgMN), a neural network that
integrates predictions from deep learning and physics-based models to address
their limitations. PgMN comprises a Parallel Projection Layers to process
incomplete inputs, a Memory Unit to account for persistent biases, and a Memory
Experience Module to optimally extend forecasts beyond their input range and
produce output. Theoretical evaluation shows that components of PgMN are
mathematically valid for performing their respective tasks. The PgMN was
evaluated on short-term energy forecasting at an hourly resolution, critical
for operational decision-making in smart grid and smart building systems.
Experimental validation shows accuracy and applicability of PgMN in diverse
scenarios such as newly constructed buildings, missing data, sparse historical
data, and dynamic infrastructure changes. This paper provides a promising
solution for energy consumption forecasting in dynamic building environments,
enhancing model applicability in scenarios where historical data are limited or
unavailable or when physics-based models are inadequate.

</details>


### [57] [An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals](https://arxiv.org/abs/2508.09162)
*Konstantinos Vasili,Zachery T. Dahm,William Richards,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 论文提出了一种基于自编码器和定制化windowSHAP算法的无监督可解释AI框架，用于实时检测和表征核反应堆中的重放攻击。


<details>
  <summary>Details</summary>
Motivation: 新一代先进核反应堆依赖数字化控制系统，数据完整性对安全至关重要。现有方法依赖合成数据或有限假设，无法满足实际需求。

Method: 结合自编码器和定制化windowSHAP算法，实现攻击检测、来源识别、时间和类型分析。

Result: 在PUR-1反应堆的真实数据测试中，框架检测和识别重放攻击的准确率达95%以上。

Conclusion: 该框架为核反应堆数据完整性提供了高效且可解释的解决方案。

Abstract: Next generation advanced nuclear reactors are expected to be smaller both in
size and power output, relying extensively on fully digital instrumentation and
control systems. These reactors will generate a large flow of information in
the form of multivariate time series data, conveying simultaneously various non
linear cyber physical, process, control, sensor, and operational states.
Ensuring data integrity against deception attacks is becoming increasingly
important for networked communication and a requirement for safe and reliable
operation. Current efforts to address replay attacks, almost universally focus
on watermarking or supervised anomaly detection approaches without further
identifying and characterizing the root cause of the anomaly. In addition,
these approaches rely mostly on synthetic data with uncorrelated Gaussian
process and measurement noise and full state feedback or are limited to
univariate signals, signal stationarity, linear quadratic regulators, or other
linear-time invariant state-space which may fail to capture any unmodeled
system dynamics. In the realm of regulated nuclear cyber-physical systems,
additional work is needed on characterization of replay attacks and
explainability of predictions using real data. Here, we propose an unsupervised
explainable AI framework based on a combination of autoencoder and customized
windowSHAP algorithm to fully characterize real-time replay attacks, i.e.,
detection, source identification, timing and type, of increasing complexity
during a dynamic time evolving reactor process. The proposed XAI framework was
benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1
with up to six signals concurrently being replayed. In all cases, the XAI
framework was able to detect and identify the source and number of signals
being replayed and the duration of the falsification with 95 percent or better
accuracy.

</details>


### [58] [Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)](https://arxiv.org/abs/2508.09163)
*Ziheng Wang,Pedro Reviriego,Farzad Niknia,Zhen Gao,Javier Conde,Shanshan Liu,Fabrizio Lombardi*

Main category: cs.LG

TL;DR: 本文提出了一种名为ASL（可调序列长度）的新方案，用于在随机计算（SC）神经网络中实现混合精度设计，显著降低能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 随机计算（SC）在资源受限场景（如物联网）中是一种高效低功耗的神经网络部署方案，但层间混合精度实现仍有待探索。

Method: 通过基于算子范数的理论模型分析截断噪声的累积传播，并利用随机森林回归进行敏感性分析，提出两种截断策略（粗粒度和细粒度）。

Result: 在32nm工艺下合成的SC多层感知器上，ASL方案可将能耗和延迟降低60%以上，且精度损失可忽略。

Conclusion: ASL方案在物联网应用中具有可行性，并突显了混合精度截断在SC设计中的独特优势。

Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative
for deploying neural networks (NNs) in resource-limited scenarios, such as the
Internet of Things (IoT). By encoding values as serial bitstreams, SC
significantly reduces energy dissipation compared to conventional
floating-point (FP) designs; however, further improvement of layer-wise
mixed-precision implementation for SC remains unexplored. This article
introduces Adjustable Sequence Length (ASL), a novel scheme that applies
mixed-precision concepts specifically to SC NNs. By introducing an
operator-norm-based theoretical model, this article shows that truncation noise
can cumulatively propagate through the layers by the estimated amplification
factors. An extended sensitivity analysis is presented, using random forest
(RF) regression to evaluate multilayer truncation effects and validate the
alignment of theoretical predictions with practical network behaviors. To
accommodate different application scenarios, this article proposes two
truncation strategies (coarse-grained and fine-grained), which apply diverse
sequence length configurations at each layer. Evaluations on a pipelined SC MLP
synthesized at 32nm demonstrate that ASL can reduce energy and latency
overheads by up to over 60% with negligible accuracy loss. It confirms the
feasibility of the ASL scheme for IoT applications and highlights the distinct
advantages of mixed-precision truncation in SC designs.

</details>


### [59] [Generating Feasible and Diverse Synthetic Populations Using Diffusion Models](https://arxiv.org/abs/2508.09164)
*Min Tang,Peng Lu,Qing Feng*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的人口合成方法，用于估计人口的联合分布，优于VAE和GAN方法。


<details>
  <summary>Details</summary>
Motivation: 由于高维数据稀疏性，传统方法难以准确合成人口数据，而深度生成模型可能生成不可行的组合。

Method: 使用扩散模型估计联合分布，恢复缺失的采样零并最小化结构零。

Result: 在边际分布相似性、可行性和多样性方面优于VAE和GAN方法。

Conclusion: 新方法在合成人口的可行性和多样性之间取得了更好平衡。

Abstract: Population synthesis is a critical task that involves generating synthetic
yet realistic representations of populations. It is a fundamental problem in
agent-based modeling (ABM), which has become the standard to analyze
intelligent transportation systems. The synthetic population serves as the
primary input for ABM transportation simulation, with traveling agents
represented by population members. However, when the number of attributes
describing agents becomes large, survey data often cannot densely support the
joint distribution of the attributes in the population due to the curse of
dimensionality. This sparsity makes it difficult to accurately model and
produce the population. Interestingly, deep generative models trained from
available sample data can potentially synthesize possible attribute
combinations that present in the actual population but do not exist in the
sample data(called sampling zeros). Nevertheless, this comes at the cost of
falsely generating the infeasible attribute combinations that do not exist in
the population (called structural zeros). In this study, a novel diffusion
model-based population synthesis method is proposed to estimate the underlying
joint distribution of a population. This approach enables the recovery of
numerous missing sampling zeros while keeping the generated structural zeros
minimal. Our method is compared with other recently proposed approaches such as
Variational Autoencoders (VAE) and Generative Adversarial Network (GAN)
approaches, which have shown success in high dimensional tabular population
synthesis. We assess the performance of the synthesized outputs using a range
of metrics, including marginal distribution similarity, feasibility, and
diversity. The results demonstrate that our proposed method outperforms
previous approaches in achieving a better balance between the feasibility and
diversity of the synthesized population.

</details>


### [60] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: PatchECG框架通过掩码训练策略自适应处理不同布局的ECG信号，显著提升心律失常识别的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 不同医院的ECG布局差异导致信号异步和部分缺失，现有模型难以应对。

Method: 提出PatchECG框架，基于掩码训练策略自适应学习关键块，处理不同布局的ECG信号。

Result: 在PTB-XL数据集和真实医院数据上，AUROC分别达到0.835和0.778，优于现有方法。

Conclusion: PatchECG在多种布局下表现稳定且优于基线模型，为ECG诊断提供了新解决方案。

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [61] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: SVG-1M数据集和SVGen模型解决了从自然语言生成精确SVG的挑战，提升了语义准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决将创意转化为精确矢量图形的耗时问题。

Method: 使用SVG-1M数据集和SVGen模型，结合课程学习和强化学习优化。

Result: SVGen在效果和效率上优于通用大模型和传统渲染方法。

Conclusion: SVG-1M和SVGen为文本到SVG生成提供了高效解决方案。

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [62] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级、无需训练的方法，利用检索增强生成（RAG）跨模态线性映射，解决预训练大型多模态模型（LMMs）中的模态间隙问题。通过生成合成描述优化映射，实验结果显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 预训练大型多模态模型存在模态间隙（文本与视觉表示不匹配），微调成本高且不实用，因此需要一种轻量级解决方案。

Method: 采用检索增强生成（RAG）和线性映射，无需训练即可跨模态扩展。通过生成合成描述优化映射，提升性能。

Result: 在两个基准多模态数据集上实验，结果显示显著改进。

Conclusion: 提出的轻量级方法有效解决了模态间隙问题，无需昂贵微调，具有实际应用价值。

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [63] [FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective](https://arxiv.org/abs/2508.09174)
*Zhekai Zhou,Shudong Liu,Zhaokun Zhou,Yang Liu,Qiang Yang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: FedMP是一种针对非独立同分布（non-IID）数据的联邦学习方法，通过随机特征流形补全和类原型对齐提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非IID数据（如医学影像）中因特征分布差异导致的模型收敛和性能问题。

Method: 使用随机特征流形补全和类原型对齐，优化客户端特征流形的一致性。

Result: 在医学影像和多域自然图像数据集上表现优于现有联邦学习算法。

Conclusion: FedMP有效提升非IID数据下的联邦学习性能，同时分析了流形维度、通信效率和隐私影响。

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a shared model without sharing their
local private data. However, real-world applications of FL frequently encounter
challenges arising from the non-identically and independently distributed
(non-IID) local datasets across participating clients, which is particularly
pronounced in the field of medical imaging, where shifts in image feature
distributions significantly hinder the global model's convergence and
performance. To address this challenge, we propose FedMP, a novel method
designed to enhance FL under non-IID scenarios. FedMP employs stochastic
feature manifold completion to enrich the training space of individual client
classifiers, and leverages class-prototypes to guide the alignment of feature
manifolds across clients within semantically consistent subspaces, facilitating
the construction of more distinct decision boundaries. We validate the
effectiveness of FedMP on multiple medical imaging datasets, including those
with real-world multi-center distributions, as well as on a multi-domain
natural image dataset. The experimental results demonstrate that FedMP
outperforms existing FL algorithms. Additionally, we analyze the impact of
manifold dimensionality, communication efficiency, and privacy implications of
feature exposure in our method.

</details>


### [64] [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)
*Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Francesca Palermo,Diana Trojaniello,Manuel Roveri*

Main category: cs.LG

TL;DR: DQT提出了一种动态量化训练框架，通过嵌套整数表示和定制整数运算，实现无解量化的动态混合精度量化，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有动态混合精度量化方法需要昂贵的解量化和重新量化操作，破坏了整数硬件范式并影响性能。DQT旨在消除这一瓶颈。

Method: DQT采用嵌套整数表示和定制整数运算，通过低成本位移操作实现动态位宽切换，同时引入轻量级控制器实时决定量化方式。

Result: 在ImageNet上，4位动态ResNet50达到77.00% top-1准确率，优于静态和动态方法，且位宽转换成本仅为28.3M位移操作。

Conclusion: DQT首次实现了无解量化的动态混合精度量化，为高效自适应AI开辟了新方向。

Abstract: The deployment of deep neural networks on resource-constrained devices relies
on quantization. While static, uniform quantization applies a fixed bit-width
to all inputs, it fails to adapt to their varying complexity. Dynamic,
instance-based mixed-precision quantization promises a superior
accuracy-efficiency trade-off by allocating higher precision only when needed.
However, a critical bottleneck remains: existing methods require a costly
dequantize-to-float and requantize-to-integer cycle to change precision,
breaking the integer-only hardware paradigm and compromising performance gains.
This paper introduces Dynamic Quantization Training (DQT), a novel framework
that removes this bottleneck. At the core of DQT is a nested integer
representation where lower-precision values are bit-wise embedded within
higher-precision ones. This design, coupled with custom integer-only
arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost
bit-shift operation. This makes DQT the first quantization framework to enable
both dequantization-free static mixed-precision of the backbone network, and
truly efficient dynamic, instance-based quantization through a lightweight
controller that decides at runtime how to quantize each layer. We demonstrate
DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on
ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1
accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,
76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this
with a bit-width transition cost of only 28.3M simple bit-shift operations, a
drastic improvement over the 56.6M costly Multiply-Accumulate (MAC)
floating-point operations required by previous dynamic approaches - unlocking a
new frontier in efficient, adaptive AI.

</details>


### [65] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: scAGC是一种单细胞聚类方法，通过自适应图学习和对比指导优化特征表示和细胞图结构，解决了传统方法在高维稀疏数据中的局限性。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据的高维性和零值普遍性使传统聚类方法面临统计和计算挑战，现有图神经网络方法依赖静态图结构，难以捕捉长尾分布。

Method: scAGC结合拓扑自适应图自编码器和可微分Gumbel-Softmax采样策略，动态优化图结构，并集成ZINB损失和对比学习目标以提高鲁棒性和稳定性。

Result: 在9个真实数据集上，scAGC在NMI和ARI指标上均优于现有方法，分别取得9个和7个数据集的最佳表现。

Conclusion: scAGC通过自适应图学习和对比指导，显著提升了单细胞聚类性能，为高维稀疏数据分析提供了有效解决方案。

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [66] [Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach](https://arxiv.org/abs/2508.09181)
*Jinghong Tan,Zhian Liu,Kun Guo,Mingxiong Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于诚实拍卖的长期客户选择联邦学习方案（LCSFLA），以解决非独立同分布数据对模型性能的影响，并通过激励机制确保客户参与和信息真实性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中非独立同分布（non-IID）数据导致模型收敛和准确性下降，传统客户选择方法存在资源浪费和信息不对称问题。

Method: 提出LCSFLA方案，结合长期数据质量评估和能源成本，采用拍卖机制和保证金要求激励客户参与并确保信息真实性。

Result: 理论证明了激励机制的兼容性和个体合理性，实验结果表明LCSFLA能有效缓解非IID数据导致的性能下降。

Conclusion: LCSFLA通过长期数据质量评估和激励机制，显著提升了联邦学习在非IID数据环境下的性能。

Abstract: Federated learning (FL) provides a decentralized framework that enables
universal model training through collaborative efforts on mobile nodes, such as
smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a
mobile client, contributing to the process without uploading local data. This
method leverages non-independent and identically distributed (non-IID) training
data from different vehicles, influenced by various driving patterns and
environmental conditions, which can significantly impact model convergence and
accuracy. Although client selection can be a feasible solution for non-IID
issues, it faces challenges related to selection metrics. Traditional metrics
evaluate client data quality independently per round and require client
selection after all clients complete local training, leading to resource
wastage from unused training results. In the IoV context, where vehicles have
limited connectivity and computational resources, information asymmetry in
client selection risks clients submitting false information, potentially making
the selection ineffective. To tackle these challenges, we propose a novel
Long-term Client-Selection Federated Learning based on Truthful Auction
(LCSFLA). This scheme maximizes social welfare with consideration of long-term
data quality using a new assessment mechanism and energy costs, and the advised
auction mechanism with a deposit requirement incentivizes client participation
and ensures information truthfulness. We theoretically prove the incentive
compatibility and individual rationality of the advised incentive mechanism.
Experimental results on various datasets, including those from IoV scenarios,
demonstrate its effectiveness in mitigating performance degradation caused by
non-IID data.

</details>


### [67] [Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring](https://arxiv.org/abs/2508.09187)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.LG

TL;DR: 综述探讨了呼吸分析的接触式与非接触式方法，重点介绍了机器学习和深度学习的最新进展，分析了其应用、挑战及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 传统接触式呼吸监测方法在舒适性和实用性上存在局限，非接触式方法如Wi-Fi和声学传感提供了更优解决方案。

Method: 比较了接触式与非接触式呼吸分析方法，详细介绍了数据预处理、特征提取和分类技术，以及适用的机器学习/深度学习模型。

Result: 非接触式方法在呼吸监测中表现出高准确性和非侵入性优势，适用于多用户场景和疾病检测。

Conclusion: 综述为呼吸分析的未来研究提供了框架，强调了可解释AI、联邦学习等新兴技术的重要性。

Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.

</details>


### [68] [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)
*Bing Han,Feifei Zhao,Dongcheng Zhao,Guobin Shen,Ping Wu,Yu Shi,Yi Zeng*

Main category: cs.LG

TL;DR: 论文提出了一种细粒度安全神经元（FGSN）方法，通过无训练持续投影技术减少微调LLM的安全风险，平衡安全性与实用性。


<details>
  <summary>Details</summary>
Motivation: 微调LLM可能破坏原有对齐机制并引入安全风险，现有防御方法缺乏对安全层与细粒度神经元的综合考虑。

Method: 提出FGSN方法，整合安全层与神经元的交互，定位稀疏且精确的安全神经元，并通过投影技术提升安全性。

Result: 实验表明，该方法显著降低有害分数和攻击成功率，同时保持模型实用性。

Conclusion: FGSN通过多维度异构安全神经元集群优化，实现了对未知安全威胁的持续防御和泛化能力。

Abstract: Fine-tuning as service injects domain-specific knowledge into large language
models (LLMs), while challenging the original alignment mechanisms and
introducing safety risks. A series of defense strategies have been proposed for
the alignment, fine-tuning, and post-fine-tuning phases, where most
post-fine-tuning defenses rely on coarse-grained safety layer mapping. These
methods lack a comprehensive consideration of both safety layers and
fine-grained neurons, limiting their ability to efficiently balance safety and
utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)
with Training-Free Continual Projection method to reduce the fine-tuning safety
risks. FGSN inherently integrates the multi-scale interactions between safety
layers and neurons, localizing sparser and more precise fine-grained safety
neurons while minimizing interference with downstream task neurons. We then
project the safety neuron parameters onto safety directions, improving model
safety while aligning more closely with human preferences. Extensive
experiments across multiple fine-tuned LLM models demonstrate that our method
significantly reduce harmfulness scores and attack success rates with minimal
parameter modifications, while preserving the model's utility. Furthermore, by
introducing a task-specific, multi-dimensional heterogeneous safety neuron
cluster optimization mechanism, we achieve continual defense and generalization
capability against unforeseen emerging safety concerns.

</details>


### [69] [From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization](https://arxiv.org/abs/2508.09191)
*Xiaoyu Tao,Shilong Zhang,Mingyue Cheng,Daoyu Wang,Tingyue Pan,Bokai Pan,Changqing Zhang,Shijin Wang*

Main category: cs.LG

TL;DR: TokenCast是一个基于LLM的框架，通过语言符号表示实现上下文感知的时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在多个关键领域（如能源、医疗和金融）至关重要，但现有方法难以整合历史数值序列与上下文特征（如非结构化文本数据）。

Method: TokenCast使用离散标记器将连续数值序列转换为时间标记，并通过预训练LLM嵌入共享表示空间，再通过监督微调预测未来标记。

Result: 在多个真实数据集上的实验表明，TokenCast具有高效性和通用性。

Conclusion: TokenCast通过统一的语义空间和LLM优化，显著提升了上下文感知时间序列预测的准确性。

Abstract: Time series forecasting plays a vital role in supporting decision-making
across a wide range of critical applications, including energy, healthcare, and
finance. Despite recent advances, forecasting accuracy remains limited due to
the challenge of integrating historical numerical sequences with contextual
features, which often comprise unstructured textual data. To address this
challenge, we propose TokenCast, an LLM-driven framework that leverages
language-based symbolic representations as a unified intermediary for
context-aware time series forecasting. Specifically, TokenCast employs a
discrete tokenizer to transform continuous numerical sequences into temporal
tokens, enabling structural alignment with language-based inputs. To bridge the
semantic gap between modalities, both temporal and contextual tokens are
embedded into a shared representation space via a pre-trained large language
model (LLM), further optimized with autoregressive generative objectives.
Building upon this unified semantic space, the aligned LLM is subsequently
fine-tuned in a supervised manner to predict future temporal tokens, which are
then decoded back into the original numerical space. Extensive experiments on
diverse real-world datasets enriched with contextual features demonstrate the
effectiveness and generalizability of TokenCast.

</details>


### [70] [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)
*Xu Wang,Chenkai Xu,Yijie Jin,Jiachun Jin,Hao Zhang,Zhijie Deng*

Main category: cs.LG

TL;DR: 本文提出了一种名为离散扩散强迫（D2F）的策略，通过结合自回归和扩散模型的优势，显著提升了扩散大语言模型（dLLMs）的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的开源dLLMs在推理速度上未能超越类似规模的自回归（AR）LLMs，因此需要一种更高效的解码方法。

Method: D2F通过块级自回归生成和跨块并行解码，将dLLMs改造为AR-扩散混合范式，并通过非对称蒸馏实现。

Result: D2F dLLMs在GSM8K上的推理速度比LLaMA3和Qwen2.5快2.5倍以上，比LLaDA和Dream等vanilla dLLMs快50倍以上，同时保持输出质量。

Conclusion: D2F是一种简单有效的策略，显著提升了dLLMs的推理效率，为高效文本生成提供了新思路。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source dLLMs have achieved superior inference speed over AR LLMs of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key
capabilities: (1) block-wise autoregressive generation to enable KV cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the
acceleration can be more than $\mathbf{50\times}$ while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.

</details>


### [71] [Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL](https://arxiv.org/abs/2508.09193)
*Sung-Hyun Kim,In-Chang Baek,Seo-Young Lee,Geum-Hwan Hwang,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: MIPCGRL是一种多目标表示学习方法，通过结合句子嵌入和多标签分类网络，显著提升了生成模型对复杂文本指令的响应能力。


<details>
  <summary>Details</summary>
Motivation: 现有IPCGRL方法难以充分利用文本输入的丰富表达性，尤其是在多目标指令下，导致可控性受限。

Method: 提出MIPCGRL，结合句子嵌入作为条件，通过多标签分类和多头回归网络训练多目标嵌入空间。

Result: 实验表明，该方法在多目标指令下的可控性提升了13.8%。

Conclusion: MIPCGRL能够处理复杂指令，实现更具表达力和灵活性的内容生成。

Abstract: Recent advancements in generative modeling emphasize the importance of
natural language as a highly expressive and accessible modality for controlling
content generation. However, existing instructed reinforcement learning for
procedural content generation (IPCGRL) method often struggle to leverage the
expressive richness of textual input, especially under complex, multi-objective
instructions, leading to limited controllability. To address this problem, we
propose \textit{MIPCGRL}, a multi-objective representation learning method for
instructed content generators, which incorporates sentence embeddings as
conditions. MIPCGRL effectively trains a multi-objective embedding space by
incorporating multi-label classification and multi-head regression networks.
Experimental results show that the proposed method achieves up to a 13.8\%
improvement in controllability with multi-objective instructions. The ability
to process complex instructions enables more expressive and flexible content
generation.

</details>


### [72] [Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments](https://arxiv.org/abs/2508.09194)
*Yipeng Du,Zihao Wang,Ahmad Farhan,Claudio Angione,Harry Yang,Fielding Johnston,James P. Buban,Patrick Colangelo,Yue Zhao,Yuzhe Yang*

Main category: cs.LG

TL;DR: 论文提出了一种基于元学习的框架，用于在去中心化系统中自动选择最优的推理加速方法，以提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大规模模型（如LLMs）的部署成本高且面临可扩展性和数据安全的挑战，去中心化系统成为趋势，但需要高效的推理加速方案。

Method: 引入元学习框架，通过学习历史性能数据，自动选择最适合任务的加速策略。

Result: 该框架在效率和性能上优于传统方法，为去中心化AI系统提供了更经济可行的解决方案。

Conclusion: 元学习框架为去中心化AI系统的推理加速提供了高效且自动化的选择方法，推动了更民主和经济的人工智能发展。

Abstract: The deployment of large-scale models, such as large language models (LLMs),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference acceleration schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal acceleration methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various acceleration techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best acceleration strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference acceleration in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.

</details>


### [73] [ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce](https://arxiv.org/abs/2508.09198)
*Li Kong,Bingzhe Wang,Zhou Chen,Suhan Hu,Yuchao Ma,Qi Qi,Suoyuan Song,Bicheng Jin*

Main category: cs.LG

TL;DR: 论文提出了一种名为ADT4Coupons的新营销框架，用于优化在线平台的长期优惠券分发策略。


<details>
  <summary>Details</summary>
Motivation: 现有优惠券分发策略未能充分利用平台与用户之间的复杂序列交互，导致性能瓶颈。

Method: 提出ADT4Coupons框架，整合通用场景、序列建模和高效迭代更新，以优化决策。

Result: 在真实工业数据集及公开和合成数据集上的实证结果验证了框架的优越性。

Conclusion: ADT4Coupons能有效提升长期收入，适用于多样化营销场景。

Abstract: Coupon distribution is a critical marketing strategy used by online platforms
to boost revenue and enhance user engagement. Regrettably, existing coupon
distribution strategies fall far short of effectively leveraging the complex
sequential interactions between platforms and users. This critical oversight,
despite the abundance of e-commerce log data, has precipitated a performance
plateau. In this paper, we focus on the scene that the platforms make
sequential coupon distribution decision multiple times for various users, with
each user interacting with the platform repeatedly. Based on this marketing
scenario, we propose a novel marketing framework, named Aligned Decision
Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution
policy for long-term revenue boosting. ADT4Coupons enables optimized online
decision-making in a variety of real-world marketing scenarios. It achieves
this by seamlessly integrating three key characteristics, general scenarios,
sequential modeling with more comprehensive historical data, and efficient
iterative updates within a unified framework. Furthermore, empirical results on
real-world industrial dataset, alongside public and synthetic datasets
demonstrate the superiority of our framework.

</details>


### [74] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: 论文介绍了Construction Safety Dataset (CSDataset)，一个综合多层级数据集，用于解决建筑安全研究中数据不足的问题，并展示了初步分析结果。


<details>
  <summary>Details</summary>
Motivation: 现有建筑安全数据集数量有限且多样性不足，阻碍了深入分析。

Method: 引入CSDataset，整合了OSHA的结构化属性和非结构化叙述，支持机器学习和大型语言模型的应用。

Result: 初步分析发现，投诉驱动的检查与后续事故可能性降低17.3%相关。

Conclusion: CSDataset为建筑安全研究提供了新工具，未来可进一步优化分析。

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [75] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: MoQE是一种基于混合专家（MoE）架构的量化推理框架，通过动态路由输入数据到最合适的量化专家模型，缓解单量化模型的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 量化方法在提高模型效率和降低部署成本方面至关重要，但量化过程会导致精度下降。本文旨在通过MoQE框架联合提升量化模型的性能。

Method: MoQE将全精度模型的多个量化变体作为专门的“量化专家”，并基于输入数据的特性动态路由到最合适的专家。设计了轻量级、结构感知的路由器模型，适用于CV和NLP任务。

Result: 在ResNet、LLaMA和Qwen模型家族以及ImageNet、WikiText等基准数据集上的实验表明，MoQE性能接近SOTA量化模型，且推理延迟无明显增加。

Conclusion: MoQE通过动态路由和专家模型组合，有效缓解了量化模型的性能下降问题，为资源受限设备上的深度学习应用提供了高效解决方案。

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [76] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: 提出了一种基于可微分转移模块的修复算法，用于优化微LED制造中的转移步骤，显著减少步骤数量并提高效率。


<details>
  <summary>Details</summary>
Motivation: 微LED制造中的选择性转移需要高效的计算模型以减少XY平台的运动并适应不同的优化目标。

Method: 采用可微分转移模块设计修复算法，支持梯度优化，避免了手工特征提取和强化学习的复杂性。

Result: 实验显示，该方法在2000x2000阵列上减少了50%的转移步骤，规划时间低于2分钟。

Conclusion: 该方法为微LED修复提供了高效、灵活的解决方案，适用于AR/VR和下一代显示器的制造。

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


### [77] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: Hi-Vec是一种分层自适应网络，通过动态层选择和权重合并机制，提升预训练模型在测试时对分布变化的适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法因单维线性分类层无法处理复杂分布变化的问题。

Method: 提出分层自适应网络Hi-Vec，包括动态层选择、权重合并机制和线性层一致性门控。

Result: 在多种挑战性场景下验证了Hi-Vec的优越性，提升了鲁棒性和对噪声的适应能力。

Conclusion: Hi-Vec显著提升了现有方法的适应能力，适用于复杂分布变化和噪声场景。

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [78] [GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction](https://arxiv.org/abs/2508.09227)
*Fan Ding,Hwa Hui Tew,Junn Yong Loo,Susilawati,LiTong Liu,Fang Yu Leong,Xuewen Luo,Kar Keong Chin,Jia Jun Gan*

Main category: cs.LG

TL;DR: 提出了一种名为GSMT的混合模型，结合图注意力网络和序列到序列RNN，通过任务校正器优化公交轨迹预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在数据有限的地区，仅依赖GPS数据预测公交轨迹具有挑战性，需开发高效模型。

Method: GSMT整合GAT和RNN，利用任务校正器聚类历史轨迹并优化预测，分两阶段进行轨迹预测。

Result: 在吉隆坡真实数据集上，GSMT在短期和长期轨迹预测中表现优异。

Conclusion: GSMT通过混合模型和任务校正器，显著提升了公交轨迹预测的准确性。

Abstract: Accurate trajectory prediction for buses is crucial in intelligent
transportation systems, particularly within urban environments. In developing
regions where access to multimodal data is limited, relying solely on onboard
GPS data remains indispensable despite inherent challenges. To address this
problem, we propose GSMT, a hybrid model that integrates a Graph Attention
Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and
incorporates a task corrector capable of extracting complex behavioral patterns
from large-scale trajectory data. The task corrector clusters historical
trajectories to identify distinct motion patterns and fine-tunes the
predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus
information and static station information through embedded hybrid networks to
perform trajectory prediction, and applies the task corrector for secondary
refinement after the initial predictions are generated. This two-stage approach
enables multi-node trajectory prediction among buses operating in dense urban
traffic environments under complex conditions. Experiments conducted on a
real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method
significantly outperforms existing approaches, achieving superior performance
in both short-term and long-term trajectory prediction tasks.

</details>


### [79] [Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models](https://arxiv.org/abs/2508.09237)
*Luigi D'Amico,Daniel De Rosso,Ninad Dixit,Raul Salles de Padua,Samuel Palmer,Samuel Mugel,Román Orús,Holger Eble,Ali Abedi*

Main category: cs.LG

TL;DR: 论文提出了一种结合量子启发图神经网络（QI-GNN）与集成模型的新方法，用于区块链网络中的反洗钱（AML）欺诈交易检测，通过引入CP分解层提升性能，F2分数达74.8%。


<details>
  <summary>Details</summary>
Motivation: 金融科技领域快速发展，区块链网络中的非法交易检测仍是一个关键挑战，需要创新解决方案。

Method: 结合QI-GNN与集成模型（QBoost或随机森林分类器），并在图神经网络框架中引入CP分解层。

Result: 在欺诈交易检测中F2分数达到74.8%，优于传统机器学习方法。

Conclusion: 量子启发算法在金融安全领域具有潜力，值得进一步探索和推广。

Abstract: In the rapidly evolving domain of financial technology, the detection of
illicit transactions within blockchain networks remains a critical challenge,
necessitating robust and innovative solutions. This work proposes a novel
approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with
flexibility of choice of an Ensemble Model using QBoost or a classic model such
as Random Forrest Classifier. This system is tailored specifically for
blockchain network analysis in anti-money laundering (AML) efforts. Our
methodology to design this system incorporates a novel component, a Canonical
Polyadic (CP) decomposition layer within the graph neural network framework,
enhancing its capability to process and analyze complex data structures
efficiently. Our technical approach has undergone rigorous evaluation against
classical machine learning implementations, achieving an F2 score of 74.8% in
detecting fraudulent transactions. These results highlight the potential of
quantum-inspired techniques, supplemented by the structural advancements of the
CP layer, to not only match but potentially exceed traditional methods in
complex network analysis for financial security. The findings advocate for a
broader adoption and further exploration of quantum-inspired algorithms within
the financial sector to effectively combat fraud.

</details>


### [80] [LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data](https://arxiv.org/abs/2508.09263)
*Peng Wang,Dongsheng Wang,He Zhao,Hangting Ye,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: 提出了一种基于大型语言模型（LLM）的原型估计框架，用于零样本和小样本表格数据学习，无需训练分类器或微调LLM。


<details>
  <summary>Details</summary>
Motivation: 探索如何有效利用LLM在零样本和小样本场景下进行表格数据建模，克服基于示例提示的限制。

Method: 通过任务和特征描述生成特征值，构建零样本原型，并融合小样本数据增强性能。

Result: 实验证明该方法在零样本和小样本表格学习中有效。

Conclusion: 该框架提供了一种可扩展且鲁棒的方法，避免了传统方法的限制。

Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to
in-depth investigation of their potential in tabular data modeling. However,
effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is
still challenging. To this end, we propose a novel LLM-based prototype
estimation framework for tabular learning. Our key idea is to query the LLM to
generate feature values based example-free prompt, which solely relies on task
and feature descriptions. With the feature values generated by LLM, we can
build a zero-shot prototype in a training-free manner, which can be further
enhanced by fusing few-shot samples, avoiding training a classifier or
finetuning the LLMs. Thanks to the example-free prompt and prototype
estimation, ours bypasses the constraints brought by the example-based prompt,
providing a scalable and robust framework. Extensive experiments demonstrate
the effectiveness of ours in zero and few-shot tabular learning.

</details>


### [81] [Detection of Odor Presence via Deep Neural Networks](https://arxiv.org/abs/2508.09264)
*Matin Hassanloo,Ali Zareh,Mehmet Kemal Özdemir*

Main category: cs.LG

TL;DR: 论文提出了一种基于局部场电位（LFPs）的深度学习模型，用于单次试验气味检测，支持嗅觉球信号足以实现高精度检测的假设。


<details>
  <summary>Details</summary>
Motivation: 当前气味检测的人工传感器在复杂混合物中表现不佳，非侵入性记录缺乏单次试验可靠性。

Method: 使用互补的一维卷积网络（ResCNN和AttentionCNN）解码嗅觉球多通道LFPs信号。

Result: 在2,349次小鼠试验中，模型平均准确率达86.6%，F1分数81.0%，AUC为0.9247，显著优于先前基准。

Conclusion: 研究证实了基于LFPs的单次试验气味检测的可行性，并展示了深度学习在理解嗅觉表征中的潜力。

Abstract: Odor detection underpins food safety, environmental monitoring, medical
diagnostics, and many more fields. The current artificial sensors developed for
odor detection struggle with complex mixtures while non-invasive recordings
lack reliable single-trial fidelity. To develop a general system for odor
detection, in this study we present a preliminary work where we aim to test two
hypotheses: (i) that spectral features of local field potentials (LFPs) are
sufficient for robust single-trial odor detection and (ii) that signals from
the olfactory bulb alone are adequate. To test two hypotheses, we propose an
ensemble of complementary one-dimensional convolutional networks (ResCNN and
AttentionCNN) that decodes the presence of odor from multichannel olfactory
bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble
model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score
of 81.0%, and an AUC of 0.9247, substantially outperforming previous
benchmarks. In addition, the t-SNE visualization confirms that our framework
captures biologically significant signatures. These findings establish the
feasibility of robust single-trial detection of the presence of odor from
extracellular LFPs, as well as demonstrate the potential of deep learning
models to provide a deeper understanding of olfactory representations.

</details>


### [82] [Over-Squashing in GNNs and Causal Inference of Rewiring Strategies](https://arxiv.org/abs/2508.09265)
*Danial Saber,Amirali Salehi-Abari*

Main category: cs.LG

TL;DR: 该论文提出了一种评估图神经网络（GNNs）中过度压缩（over-squashing）问题的方法，并通过实验验证了重连（rewiring）技术在缓解这一问题上的效果。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在多个领域表现优异，但消息传递式GNN存在长距离信息过度压缩的问题，限制了其表达能力。目前缺乏直接的度量方法评估重连技术的实际效果。

Method: 提出了一种基于拓扑结构的方法，通过节点对的互敏感度衰减率评估过度压缩，并扩展到四种图级统计量。结合因果设计，量化了重连策略对过度压缩的影响。

Result: 实验表明，大多数图分类数据集存在不同程度的过度压缩，重连能有效缓解，但效果因数据集和方法而异。节点分类数据集中过度压缩问题较轻，重连甚至可能加剧问题。

Conclusion: 重连技术在过度压缩严重且适度修正时最有效，过度或在不必要的情况下使用可能适得其反。论文提供的诊断工具可帮助实践者在训练前判断是否需要重连。

Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance
across wide-range of domains such as recommender systems, material design, and
drug repurposing. Yet message-passing GNNs suffer from over-squashing --
exponential compression of long-range information from distant nodes -- which
limits expressivity. Rewiring techniques can ease this bottleneck; but their
practical impacts are unclear due to the lack of a direct empirical
over-squashing metric. We propose a rigorous, topology-focused method for
assessing over-squashing between node pairs using the decay rate of their
mutual sensitivity. We then extend these pairwise assessments to four
graph-level statistics (prevalence, intensity, variability, extremity).
Coupling these metrics with a within-graph causal design, we quantify how
rewiring strategies affect over-squashing on diverse graph- and
node-classification benchmarks. Our extensive empirical analyses show that most
graph classification datasets suffer from over-squashing (but to various
extents), and rewiring effectively mitigates it -- though the degree of
mitigation, and its translation into performance gains, varies by dataset and
method. We also found that over-squashing is less notable in node
classification datasets, where rewiring often increases over-squashing, and
performance variations are uncorrelated with over-squashing changes. These
findings suggest that rewiring is most beneficial when over-squashing is both
substantial and corrected with restraint -- while overly aggressive rewiring,
or rewiring applied to minimally over-squashed graphs, is unlikely to help and
may even harm performance. Our plug-and-play diagnostic tool lets practitioners
decide -- before any training -- whether rewiring is likely to pay off.

</details>


### [83] [Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.09275)
*Amine Andam,Jamal Bentahar,Mustapha Hedabou*

Main category: cs.LG

TL;DR: 本文研究了协作多智能体强化学习（c-MARL）在现实约束条件下的新脆弱性，提出了一种高效的对抗扰动生成算法。


<details>
  <summary>Details</summary>
Motivation: c-MARL在敏感领域的应用广泛，但其对抗攻击脆弱性研究不足，现有工作多集中于训练时攻击或不现实场景。

Method: 在更现实的约束条件下（仅能收集和扰动部署智能体的观测），提出简单高效的对抗扰动算法。

Result: 在3个基准和22个环境中验证了算法的有效性，且仅需1,000样本，远低于先前方法的百万级需求。

Conclusion: 该算法在多样环境和算法中表现优异，为c-MARL的安全性提供了新视角。

Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly
evolved, offering state-of-the-art algorithms for real-world applications,
including sensitive domains. However, a key challenge to its widespread
adoption is the lack of a thorough investigation into its vulnerabilities to
adversarial attacks. Existing work predominantly focuses on training-time
attacks or unrealistic scenarios, such as access to policy weights or the
ability to train surrogate policies. In this paper, we investigate new
vulnerabilities under more realistic and constrained conditions, assuming an
adversary can only collect and perturb the observations of deployed agents. We
also consider scenarios where the adversary has no access at all. We propose
simple yet highly effective algorithms for generating adversarial perturbations
designed to misalign how victim agents perceive their environment. Our approach
is empirically validated on three benchmarks and 22 environments, demonstrating
its effectiveness across diverse algorithms and environments. Furthermore, we
show that our algorithm is sample-efficient, requiring only 1,000 samples
compared to the millions needed by previous methods.

</details>


### [84] [Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning](https://arxiv.org/abs/2508.09281)
*Muntasir Hoq,Griffin Pitts,Andrew Lan,Peter Brusilovsky,Bita Akram*

Main category: cs.LG

TL;DR: 提出了一种基于模式的知识组件（KC）自动发现框架，通过变分自编码器和注意力机制从学生代码中提取可解释的模式，改进计算机科学教育中的个性化学习。


<details>
  <summary>Details</summary>
Motivation: 解决传统知识组件（KC）提取方法在编程教育中因代码结构多样性和概念复杂性导致的解释性不足问题。

Method: 使用变分自编码器生成代表性代码模式，结合注意力机制识别重要模式，聚类形成模式基KC，并通过学习曲线分析和深度知识追踪（DKT）评估。

Result: 实验显示学习轨迹有意义，DKT预测性能显著优于传统方法。

Conclusion: 该框架为计算机科学教育提供了自动化、可扩展且可解释的知识建模方法，有助于学生掌握编程模式。

Abstract: Effective personalized learning in computer science education depends on
accurately modeling what students know and what they need to learn. While
Knowledge Components (KCs) provide a foundation for such modeling, automated KC
extraction from student code is inherently challenging due to insufficient
explainability of discovered KCs and the open-endedness of programming problems
with significant structural variability across student solutions and complex
interactions among programming concepts. In this work, we propose a novel,
explainable framework for automated KC discovery through pattern-based KCs:
recurring structural patterns within student code that capture the specific
programming patterns and language constructs that students must master. Toward
this, we train a Variational Autoencoder to generate important representative
patterns from student code guided by an explainable, attention-based code
representation model that identifies important correct and incorrect pattern
implementations from student code. These patterns are then clustered to form
pattern-based KCs. We evaluate our KCs using two well-established methods
informed by Cognitive Science: learning curve analysis and Deep Knowledge
Tracing (DKT). Experimental results demonstrate meaningful learning
trajectories and significant improvements in DKT predictive performance over
traditional KT methods. This work advances knowledge modeling in CS education
by providing an automated, scalable, and explainable framework for identifying
granular code patterns and algorithmic constructs, essential for student
learning.

</details>


### [85] [Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation](https://arxiv.org/abs/2508.09299)
*Rilwan Umar,Aydin Abadi,Basil Aldali,Benito Vincent,Elliot A. J. Hurley,Hotoon Aljazaeri,Jamie Hedley-Cook,Jamie-Lee Bell,Lambert Uwuigbusun,Mujeeb Ahmed,Shishir Nagaraja,Suleiman Sabo,Weaam Alrbeiqi*

Main category: cs.LG

TL;DR: 提出了一种结合联邦学习和区块链技术的去中心化天气预测框架，以提高隐私性、安全性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前集中式天气预测系统存在安全漏洞、可扩展性差和单点故障问题，需要一种更安全、高效的解决方案。

Method: 整合联邦学习（保护隐私）与区块链（透明验证），引入基于信誉的投票机制和IPFS存储。

Result: 实验表明，该方法提高了预测准确性、系统弹性和可扩展性。

Conclusion: 该框架适合在安全关键的实际环境中部署。

Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture,
and resource management, yet current centralized forecasting systems are
increasingly strained by security vulnerabilities, limited scalability, and
susceptibility to single points of failure. To address these challenges, we
propose a decentralized weather forecasting framework that integrates Federated
Learning (FL) with blockchain technology. FL enables collaborative model
training without exposing sensitive local data; this approach enhances privacy
and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures
transparent and dependable verification of model updates. To further enhance
the system's security, we introduce a reputation-based voting mechanism that
assesses the trustworthiness of submitted models while utilizing the
Interplanetary File System (IPFS) for efficient off-chain storage. Experimental
results demonstrate that our approach not only improves forecasting accuracy
but also enhances system resilience and scalability, making it a viable
candidate for deployment in real-world, security-critical environments.

</details>


### [86] [Distilling Reinforcement Learning into Single-Batch Datasets](https://arxiv.org/abs/2508.09283)
*Connor Wilhelm,Dan Ventura*

Main category: cs.LG

TL;DR: 论文提出了一种数据集蒸馏方法，将大型数据集压缩为小型合成数据集，使学习合成数据集近似于原始数据集。该方法可应用于强化学习任务，并将其转化为监督学习任务。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过数据集蒸馏压缩复杂任务（如强化学习），并将其转化为其他学习模态（如监督学习），以提高学习效率和通用性。

Method: 提出了一种基于近端策略优化的元学习方法，用于蒸馏多维经典问题（如cart-pole）、MuJoCo环境和Atari游戏。

Result: 实验证明，该方法能将复杂强化学习环境压缩为一步监督学习任务，并验证了其在不同学习架构中的通用性。

Conclusion: 数据集蒸馏不仅能高效压缩任务，还能实现不同学习模态间的转换，具有广泛的应用潜力。

Abstract: Dataset distillation compresses a large dataset into a small synthetic
dataset such that learning on the synthetic dataset approximates learning on
the original. Training on the distilled dataset can be performed in as little
as one step of gradient descent. We demonstrate that distillation is
generalizable to different tasks by distilling reinforcement learning
environments into one-batch supervised learning datasets. This demonstrates not
only distillation's ability to compress a reinforcement learning task but also
its ability to transform one learning modality (reinforcement learning) into
another (supervised learning). We present a novel extension of proximal policy
optimization for meta-learning and use it in distillation of a
multi-dimensional extension of the classic cart-pole problem, all MuJoCo
environments, and several Atari games. We demonstrate distillation's ability to
compress complex RL environments into one-step supervised learning, explore RL
distillation's generalizability across learner architectures, and demonstrate
distilling an environment into the smallest-possible synthetic dataset.

</details>


### [87] [Exact Verification of Graph Neural Networks with Incremental Constraint Solving](https://arxiv.org/abs/2508.09320)
*Minghao Liu,Chia-Hsuan Lu,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: 本文提出了一种精确的验证方法GNNev，用于提高图神经网络（GNNs）在对抗攻击下的鲁棒性，支持多种聚合函数。


<details>
  <summary>Details</summary>
Motivation: GNNs在高风险应用中易受对抗攻击，现有方法对常用聚合函数的支持不足。

Method: 采用约束求解和边界收紧技术，迭代解决松弛约束问题，支持sum、max和mean聚合函数。

Result: 在多个数据集上验证了GNNev的有效性和优越性能，尤其在sum聚合任务中表现最佳。

Conclusion: GNNev为GNNs提供了对抗扰动的精确验证，填补了现有方法的空白。

Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes
applications, such as fraud detection or healthcare, but are susceptible to
adversarial attacks. A number of techniques have been proposed to provide
adversarial robustness guarantees, but support for commonly used aggregation
functions in message-passing GNNs is still lacking. In this paper, we develop
an exact (sound and complete) verification method for GNNs to compute
guarantees against attribute and structural perturbations that involve edge
addition or deletion, subject to budget constraints. Focusing on node
classification tasks, our method employs constraint solving with bound
tightening, and iteratively solves a sequence of relaxed constraint
satisfaction problems while relying on incremental solving capabilities of
solvers to improve efficiency. We implement GNNev, a versatile solver for
message-passing neural networks, which supports three aggregation functions,
sum, max and mean, with the latter two considered here for the first time.
Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and
CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its
usability and effectiveness, as well as superior performance compared to
existing {exact verification} tools on sum-aggregated node classification
tasks.

</details>


### [88] [Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization](https://arxiv.org/abs/2508.09330)
*Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.LG

TL;DR: 提出了一种基于权重大小的突触剪枝方法，模拟生物大脑的突触修剪机制，逐步移除低重要性连接，显著提升了时间序列预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 生物大脑通过突触修剪移除弱连接以提高效率，而人工神经网络中的dropout正则化是随机失活神经元，缺乏活动依赖性剪枝。

Method: 提出了一种基于权重大小的剪枝方法，通过立方调度逐步增加全局稀疏性，定期移除低重要性权重并保持梯度流动，无需单独的剪枝和微调阶段。

Result: 在多个时间序列预测模型（如RNN、LSTM和Patch Time Series Transformer）上实验，显著降低了平均绝对误差（金融预测中最高降低20%），并在Friedman测试中表现出统计显著性（p < 0.01）。

Conclusion: 该方法通过动态剪枝机制改进了正则化，易于集成到多种架构中，尤其在金融时间序列预测中表现出色，是传统dropout技术的实用替代方案。

Abstract: Synaptic pruning in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent pruning. We
propose a magnitude-based synaptic pruning method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global sparsity. At fixed
intervals, pruning masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
pruning and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p < 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select transformer models.
This dynamic pruning mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.

</details>


### [89] [RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs](https://arxiv.org/abs/2508.09334)
*Zhongtian Sun,Anoushka Harit*

Main category: cs.LG

TL;DR: RicciFlowRec是一个基于几何的推荐框架，通过Ricci曲率和流分析动态金融图中的因果关系，提升推荐鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种能够量化金融数据中局部压力并追踪冲击传播的方法，以支持风险感知的金融决策。

Method: 方法包括利用离散Ricci曲率建模股票、宏观经济指标和新闻之间的动态交互，并通过Ricci流追踪冲击传播，从而揭示因果子结构。

Result: 初步结果显示，在S&P 500数据和FinBERT情感分析的支持下，RicciFlowRec在合成扰动下表现出更高的鲁棒性和可解释性。

Conclusion: RicciFlowRec是首个在金融决策支持中应用几何流推理的推荐系统，未来计划扩展至投资组合优化和收益预测。

Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

</details>


### [90] [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
*Charles O'Neill,Mudith Jayasekara,Max Kirkby*

Main category: cs.LG

TL;DR: 论文提出通过在特定领域（如医学文本）训练稀疏自编码器（SAE），提高特征重建的保真度和可解释性，相比广域训练效果更优。


<details>
  <summary>Details</summary>
Motivation: 传统SAE在广域数据训练下，固定预算只能捕捉高频通用模式，导致重建误差大、特征碎片化或重叠，影响解释性。

Method: 在医学文本领域训练JumpReLU SAE，使用Gemma-2模型的第20层激活和19.5万临床问答数据。

Result: 领域限定SAE解释方差提升20%，损失恢复更高，线性残差减少，特征与临床概念对齐。

Conclusion: 领域限定可缓解广域SAE的局限性，提供更完整和可解释的潜在分解，质疑通用SAE的扩展需求。

Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations
into latent features that reveal mechanistic structure. Conventional SAEs train
on broad data distributions, forcing a fixed latent budget to capture only
high-frequency, generic patterns. This often results in significant linear
``dark matter'' in reconstruction error and produces latents that fragment or
absorb each other, complicating interpretation. We show that restricting SAE
training to a well-defined domain (medical text) reallocates capacity to
domain-specific features, improving both reconstruction fidelity and
interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2
models using 195k clinical QA examples, we find that domain-confined SAEs
explain up to 20\% more variance, achieve higher loss recovery, and reduce
linear residual error compared to broad-domain SAEs. Automated and human
evaluations confirm that learned features align with clinically meaningful
concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather
than frequent but uninformative tokens. These domain-specific SAEs capture
relevant linear structure, leaving a smaller, more purely nonlinear residual.
We conclude that domain-confinement mitigates key limitations of broad-domain
SAEs, enabling more complete and interpretable latent decompositions, and
suggesting the field may need to question ``foundation-model'' scaling for
general-purpose SAEs.

</details>


### [91] [Understanding Dementia Speech Alignment with Diffusion-Based Image Generation](https://arxiv.org/abs/2508.09385)
*Mansi,Anastasios Lepipas,Dominika Woszczyk,Yiying Guan,Soteris Demetriou*

Main category: cs.LG

TL;DR: 文本到图像模型能根据自然语言描述生成高度逼真的图像，但鲜有研究探讨其是否能将病理语音与生成图像对齐。本文研究了此类模型在痴呆相关语音信息与生成图像对齐的能力，并开发了解释方法。研究发现，仅从生成图像即可实现痴呆检测，准确率达75%。


<details>
  <summary>Details</summary>
Motivation: 探索文本到图像模型在病理语音（如痴呆相关语音）与生成图像之间的对齐能力，填补相关研究空白。

Method: 利用文本到图像模型生成图像，并通过解释性方法分析语言中哪些部分对痴呆检测有贡献。

Result: 在ADReSS数据集上，仅通过生成图像即可实现75%的痴呆检测准确率。

Conclusion: 文本到图像模型在病理语音与图像对齐方面具有潜力，且生成图像可用于痴呆检测。

Abstract: Text-to-image models generate highly realistic images based on natural
language descriptions and millions of users use them to create and share images
online. While it is expected that such models can align input text and
generated image in the same latent space little has been done to understand
whether this alignment is possible between pathological speech and generated
images. In this work, we examine the ability of such models to align
dementia-related speech information with the generated images and develop
methods to explain this alignment. Surprisingly, we found that dementia
detection is possible from generated images alone achieving 75% accuracy on the
ADReSS dataset. We then leverage explainability methods to show which parts of
the language contribute to the detection.

</details>


### [92] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: NeuronTune通过细粒度动态调节稀疏神经元，同时优化安全性和实用性，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法在安全性和实用性之间存在缺陷，如对抗攻击鲁棒性不足、频繁拒绝良性查询以及生成文本质量下降。

Method: 提出NeuronTune框架，通过归因识别安全关键和实用性神经元，利用元学习动态调节神经元激活。

Result: 实验表明，NeuronTune在安全性和实用性上均优于现有技术。

Conclusion: NeuronTune通过细粒度干预实现了安全性与实用性的平衡，为LLM的可靠部署提供了新思路。

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [93] [Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment](https://arxiv.org/abs/2508.09399)
*Yue Yao,Zhen Xu,Youzhu Liu,Kunyuan Ma,Yuxiu Lin,Mohan Jiang*

Main category: cs.LG

TL;DR: 提出一种基于联邦学习的金融风险评估框架，通过特征注意力和时序建模实现跨机构联合建模，保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 解决跨机构金融风险分析中的数据隐私和协作建模挑战。

Method: 采用分布式优化策略，结合差分隐私和噪声注入保护参数，中央服务器聚合生成全局模型。

Result: 实验表明，该方法在通信效率、模型准确性和风险检测等方面优于传统方法。

Conclusion: 该方法在保护数据主权的同时，提升了风险识别的范围和效率，为金融风险分析提供了安全高效的解决方案。

Abstract: This paper addresses the challenges of data privacy and collaborative
modeling in cross-institution financial risk analysis. It proposes a risk
assessment framework based on federated learning. Without sharing raw data, the
method enables joint modeling and risk identification across multiple
institutions. This is achieved by incorporating a feature attention mechanism
and temporal modeling structure. Specifically, the model adopts a distributed
optimization strategy. Each financial institution trains a local sub-model. The
model parameters are protected using differential privacy and noise injection
before being uploaded. A central server then aggregates these parameters to
generate a global model. This global model is used for systemic risk
identification. To validate the effectiveness of the proposed method, multiple
experiments are conducted. These evaluate communication efficiency, model
accuracy, systemic risk detection, and cross-market generalization. The results
show that the proposed model outperforms both traditional centralized methods
and existing federated learning variants across all evaluation metrics. It
demonstrates strong modeling capabilities and practical value in sensitive
financial environments. The method enhances the scope and efficiency of risk
identification while preserving data sovereignty. It offers a secure and
efficient solution for intelligent financial risk analysis.

</details>


### [94] [Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery](https://arxiv.org/abs/2508.09401)
*Yun Zi,Ming Gong,Zhihao Xue,Yujun Zou,Nia Qi,Yingnan Deng*

Main category: cs.LG

TL;DR: 提出了一种无监督异常检测方法，用于分布式后端服务系统，通过动态图建模和Transformer技术实现端到端检测。


<details>
  <summary>Details</summary>
Motivation: 解决分布式系统中复杂的结构依赖、行为演化和缺乏标记数据的实际挑战。

Method: 构建动态图提取高阶结构表示，使用Transformer建模时间行为，通过可学习的联合嵌入机制融合特征，计算异常分数。

Result: 在真实云监控数据上表现优于现有模型，具有更强的表达能力和稳定性。

Conclusion: 该方法在捕获异常传播路径和动态行为序列方面表现出色，具有实际部署潜力。

Abstract: This study proposes an unsupervised anomaly detection method for distributed
backend service systems, addressing practical challenges such as complex
structural dependencies, diverse behavioral evolution, and the absence of
labeled data. The method constructs a dynamic graph based on service invocation
relationships and applies graph convolution to extract high-order structural
representations from multi-hop topologies. A Transformer is used to model the
temporal behavior of each node, capturing long-term dependencies and local
fluctuations. During the feature fusion stage, a learnable joint embedding
mechanism integrates structural and behavioral representations into a unified
anomaly vector. A nonlinear mapping is then applied to compute anomaly scores,
enabling an end-to-end detection process without supervision. Experiments on
real-world cloud monitoring data include sensitivity analyses across different
graph depths, sequence lengths, and data perturbations. Results show that the
proposed method outperforms existing models on several key metrics,
demonstrating stronger expressiveness and stability in capturing anomaly
propagation paths and modeling dynamic behavior sequences, with high potential
for practical deployment.

</details>


### [95] [Domain-Generalization to Improve Learning in Meta-Learning Algorithms](https://arxiv.org/abs/2508.09418)
*Usman Anjum,Chris Stockman,Cat Luong,Justin Zhan*

Main category: cs.LG

TL;DR: DGS-MAML是一种新的元学习算法，结合梯度匹配和锐度感知最小化，在少样本学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决在有限训练数据下跨任务泛化的问题。

Method: 在双层优化框架中结合梯度匹配和锐度感知最小化。

Result: 在基准数据集上表现优于现有方法，准确性和泛化能力更强。

Conclusion: DGS-MAML适用于少样本学习和快速适应场景，代码已开源。

Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization
Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm
designed to generalize across tasks with limited training data. DGS-MAML
combines gradient matching with sharpness-aware minimization in a bi-level
optimization framework to enhance model adaptability and robustness. We support
our method with theoretical analysis using PAC-Bayes and convergence
guarantees. Experimental results on benchmark datasets show that DGS-MAML
outperforms existing approaches in terms of accuracy and generalization. The
proposed method is particularly useful for scenarios requiring few-shot
learning and quick adaptation, and the source code is publicly available at
GitHub.

</details>


### [96] [Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees](https://arxiv.org/abs/2508.09427)
*Xiaoyu Li,Guangyu Tang,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种隐式超图神经网络（IHGNN），通过非线性固定点方程计算表示，解决了传统超图神经网络在长距离依赖捕获和训练稳定性上的限制。


<details>
  <summary>Details</summary>
Motivation: 现实中的许多交互是基于群体的（如多作者论文或用户共同参与项目），传统超图神经网络因固定层数限制长距离依赖且训练不稳定。

Method: IHGNN采用隐式平衡公式，通过非线性固定点方程计算表示，避免了深层架构，并提出了收敛性可证明的训练方案和隐式梯度训练过程。

Result: 在引用基准测试中，IHGNN在准确性和鲁棒性上均优于传统图/超图神经网络基线，且对随机初始化和超参数变化具有强鲁棒性。

Conclusion: IHGNN为高阶关系学习提供了稳定、高效的全局传播方法，具有强泛化能力和实用价值。

Abstract: Many real-world interactions are group-based rather than pairwise such as
papers with multiple co-authors and users jointly engaging with items.
Hypergraph neural networks have shown great promise at modeling higher-order
relations, but their reliance on a fixed number of explicit message-passing
layers limits long-range dependency capture and can destabilize training as
depth grows. In this work, we introduce Implicit Hypergraph Neural Networks
(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:
instead of stacking layers, IHGNN computes representations as the solution to a
nonlinear fixed-point equation, enabling stable and efficient global
propagation across hyperedges without deep architectures. We develop a
well-posed training scheme with provable convergence, analyze the oversmoothing
conditions and expressivity of the model, and derive a transductive
generalization bound on hypergraphs. We further present an implicit-gradient
training procedure coupled with a projection-based stabilization strategy.
Extensive experiments on citation benchmarks show that IHGNN consistently
outperforms strong traditional graph/hypergraph neural network baselines in
both accuracy and robustness. Empirically, IHGNN is resilient to random
initialization and hyperparameter variation, highlighting its strong
generalization and practical value for higher-order relational learning.

</details>


### [97] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: NEXICA算法通过分析高速公路速度数据，识别导致其他路段拥堵的源头，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决交通拥堵问题，通过集中资源于拥堵源头提高效率。

Method: 1. 关注时间序列中交通减速事件的开始；2. 使用最大似然估计建立概率模型；3. 训练二元分类器识别因果关系。

Result: 在洛杉矶地区195个传感器的六个月内数据上表现优于现有方法。

Conclusion: NEXICA在准确性和计算速度上优于现有技术。

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [98] [A Unified Contrastive-Generative Framework for Time Series Classification](https://arxiv.org/abs/2508.09451)
*Ziyu Liu,Azadeh Alavi,Minyi Li,Xiang Zhang*

Main category: cs.LG

TL;DR: CoGenT框架首次将对比学习和生成方法结合，通过联合优化提升时间序列的自监督学习性能，显著优于单独方法。


<details>
  <summary>Details</summary>
Motivation: 探索对比学习和生成方法在时间序列自监督学习中的互补潜力，解决各自局限性。

Method: 提出Contrastive Generative Time series框架（CoGenT），结合对比学习和生成方法的联合优化。

Result: 在六个时间序列数据集上，CoGenT比SimCLR和MAE分别提升59.2%和14.27%的F1分数。

Conclusion: CoGenT为时间序列领域的混合自监督学习奠定了基础，兼具判别力和生成鲁棒性。

Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes
two paradigms: contrastive methods that excel at instance discrimination and
generative approaches that model data distributions. While effective
individually, their complementary potential remains unexplored. We propose a
Contrastive Generative Time series framework (CoGenT), the first framework to
unify these paradigms through joint contrastive-generative optimization. CoGenT
addresses fundamental limitations of both approaches: it overcomes contrastive
learning's sensitivity to high intra-class similarity in temporal data while
reducing generative methods' dependence on large datasets. We evaluate CoGenT
on six diverse time series datasets. The results show consistent improvements,
with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,
respectively. Our analysis reveals that the hybrid objective preserves
discriminative power while acquiring generative robustness. These findings
establish a foundation for hybrid SSL in temporal domains. We will release the
code shortly.

</details>


### [99] [Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation](https://arxiv.org/abs/2508.09462)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: 提出了一种名为FGCRN的新型开放集故障诊断模型，通过多尺度深度卷积、双向门控循环单元和时间注意力机制捕捉特征，结合距离损失函数增强类内紧凑性，并通过极值理论识别未知故障。


<details>
  <summary>Details</summary>
Motivation: 在多模式过程中，同一健康状态的样本常呈现多簇分布，难以构建紧凑准确的决策边界，因此需要一种能有效识别未知故障的诊断系统。

Method: 结合多尺度深度卷积、双向门控循环单元和时间注意力机制捕捉特征，设计距离损失函数增强类内紧凑性，并通过无监督学习构建细粒度特征表示，利用极值理论建模距离以识别未知故障。

Result: 实验证明，该方法在故障诊断中表现出优越性能。

Conclusion: FGCRN模型能有效捕捉特征并识别未知故障，适用于多模式过程的故障诊断。

Abstract: A reliable fault diagnosis system should not only accurately classify known
health states but also effectively identify unknown faults. In multimode
processes, samples belonging to the same health state often show multiple
cluster distributions, making it difficult to construct compact and accurate
decision boundaries for that state. To address this challenge, a novel open-set
fault diagnosis model named fine-grained clustering and rejection network
(FGCRN) is proposed. It combines multiscale depthwise convolution,
bidirectional gated recurrent unit and temporal attention mechanism to capture
discriminative features. A distance-based loss function is designed to enhance
the intra-class compactness. Fine-grained feature representations are
constructed through unsupervised learning to uncover the intrinsic structures
of each health state. Extreme value theory is employed to model the distance
between sample features and their corresponding fine-grained representations,
enabling effective identification of unknown faults. Extensive experiments
demonstrate the superior performance of the proposed method.

</details>


### [100] [DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries](https://arxiv.org/abs/2508.09468)
*Muhammad Sakib Khan Inan,Kewen Liao*

Main category: cs.LG

TL;DR: 提出了一种名为DeepFeatIoT的深度学习模型，通过结合局部和全局特征、随机卷积核特征以及大型语言模型特征，显著提升了物联网时间序列数据的分类性能。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器数据存在元数据丢失、数据源异构、采样频率不一致等问题，导致原始数据难以解释，影响了智能系统的有效性。

Method: 提出DeepFeatIoT模型，融合学习到的局部和全局特征、非学习的随机卷积核特征以及大型语言模型特征。

Result: 模型在多个真实物联网数据集上表现优于现有基准模型，尤其在标记数据有限的情况下仍能保持高性能。

Conclusion: DeepFeatIoT有望推动物联网数据分析的进步，并支持下一代智能系统的发展。

Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.

</details>


### [101] [Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation](https://arxiv.org/abs/2508.09467)
*Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: GraB-NAS是一种新颖的Meta-NAS框架，通过图建模和混合搜索策略实现高效神经网络架构搜索。


<details>
  <summary>Details</summary>
Motivation: 解决现有Meta-NAS方法在泛化性、搜索空间和计算成本方面的不足。

Method: 将神经网络架构建模为图，结合全局贝叶斯优化和局部梯度上升的混合搜索策略。

Result: GraB-NAS在实验中优于现有Meta-NAS方法，泛化性和搜索效率更优。

Conclusion: GraB-NAS通过混合搜索策略实现了超越预定义搜索空间的高性能架构设计。

Abstract: Neural Architecture Search (NAS) automates the design of high-performing
neural networks but typically targets a single predefined task, thereby
restricting its real-world applicability. To address this, Meta Neural
Architecture Search (Meta-NAS) has emerged as a promising paradigm that
leverages prior knowledge across tasks to enable rapid adaptation to new ones.
Nevertheless, existing Meta-NAS methods often struggle with poor
generalization, limited search spaces, or high computational costs. In this
paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS
first models neural architectures as graphs, and then a hybrid search strategy
is developed to find and generate new graphs that lead to promising neural
architectures. The search strategy combines global architecture search via
Bayesian Optimization in the search space with local exploration for novel
neural networks via gradient ascent in the latent space. Such a hybrid search
strategy allows GraB-NAS to discover task-aware architectures with strong
performance, even beyond the predefined search space. Extensive experiments
demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,
achieving better generalization and search effectiveness.

</details>


### [102] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: 提出了一种联邦持续学习（FCL）框架，通过轻量级本地模型动态适应新任务，同时提升大模型的实用性，解决了基础模型（FMs）在持续学习中的挑战。


<details>
  <summary>Details</summary>
Motivation: 基础模型（FMs）在持续学习中表现不佳，尤其是无法利用本地私有数据，且难以避免遗忘先前知识。轻量级模型虽资源友好但缺乏大模型的泛化能力。

Method: 提出协作框架，轻量级本地模型动态适应新任务；包含两个新组件：小模型持续微调防止遗忘，以及一对一蒸馏实现异构本地知识的个性化融合。

Result: 实验结果表明，该框架在异构小模型环境下仍表现出优越性能。

Conclusion: 该框架有效解决了FCL中基础模型的挑战，为持续学习提供了新思路。

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [103] [EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models](https://arxiv.org/abs/2508.09471)
*Omar Bazarbachi,Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: EGGS-PTP是一种基于扩展图的结构化剪枝方法，旨在减少大型语言模型的计算和内存需求，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模的扩大，其计算和内存需求急剧增加，亟需开发更高效的模型变体。

Method: 利用扩展图理论指导N:M结构化剪枝设计，确保剪枝后网络的信息流动和功能完整性。

Result: 实验表明，EGGS-PTP在加速和内存节省方面表现优异，且准确率优于现有结构化剪枝技术。

Conclusion: EGGS-PTP为大型语言模型的高效部署提供了一种有效解决方案。

Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured pruning, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant acceleration and memory savings due to
structured sparsity but also outperforms existing structured pruning techniques
in terms of accuracy across various LLMs.

</details>


### [104] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: MiCo框架是一个用于边缘AI应用的全方位混合精度量化（MPQ）探索和部署框架，旨在通过优化算法和硬件感知延迟模型，实现高精度和低延迟的量化方案。


<details>
  <summary>Details</summary>
Motivation: 现有MPQ方案在灵活性和效率上存在不足，且缺乏端到端的优化和部署框架。

Method: 提出MiCo框架，采用新型优化算法搜索最优量化方案，并构建硬件感知延迟模型以支持快速探索。

Result: 框架实现了高精度的量化方案，并支持从PyTorch模型到裸机C代码的直接部署，带来端到端加速。

Conclusion: MiCo框架为边缘AI应用提供了一种高效的MPQ探索和部署解决方案，显著减少了精度损失。

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [105] [Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks](https://arxiv.org/abs/2508.09532)
*Bokeng Zheng,Jianqiang Zhong,Jiayi Liu,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: 提出了一种分层联邦微调框架，用于动态车联网（IoV）场景下的多任务适应，结合LoRA和UCB-DUAL算法，优化资源利用和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决车联网系统中因客户端移动性、资源异构性和间歇性连接导致的多任务适应挑战。

Method: 采用分层联邦学习框架，结合LoRA和UCB-DUAL算法，实现资源感知和移动弹性学习。

Result: 实验表明，该方法在延迟降低24%的同时，平均准确率提高2.5%。

Conclusion: 该框架在车联网场景中实现了高效的准确性与效率权衡。

Abstract: Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.

</details>


### [106] [Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems](https://arxiv.org/abs/2508.09504)
*Arun Vignesh Malarkkan,Haoyue Bai,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出了一种基于因果图的异常检测框架CGAD，用于公共基础设施系统中的可靠网络攻击检测，解决了传统方法在分布偏移和类别不平衡问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 针对关键基础设施日益复杂的网络攻击，传统异常检测方法在多元时间序列中表现不佳，需要更鲁棒的策略。

Method: CGAD采用两阶段监督框架：因果建模和异常评分，利用动态贝叶斯网络学习因果不变图结构，并通过因果图比较检测异常。

Result: CGAD在非平稳和不平衡时间序列环境中表现出更高的适应性和准确性，显著提升了F1和ROC-AUC分数。

Conclusion: CGAD通过揭示传感器数据下的因果结构，不仅提高了检测精度，还重新定义了异常检测的鲁棒性。

Abstract: With the growing complexity of cyberattacks targeting critical
infrastructures such as water treatment networks, there is a pressing need for
robust anomaly detection strategies that account for both system
vulnerabilities and evolving attack patterns. Traditional methods --
statistical, density-based, and graph-based models struggle with distribution
shifts and class imbalance in multivariate time series, often leading to high
false positive rates. To address these challenges, we propose CGAD, a Causal
Graph-based Anomaly Detection framework designed for reliable cyberattack
detection in public infrastructure systems. CGAD follows a two-phase supervised
framework -- causal profiling and anomaly scoring. First, it learns causal
invariant graph structures representing the system's behavior under "Normal"
and "Attack" states using Dynamic Bayesian Networks. Second, it employs
structural divergence to detect anomalies via causal graph comparison by
evaluating topological deviations in causal graphs over time. By leveraging
causal structures, CGAD achieves superior adaptability and accuracy in
non-stationary and imbalanced time series environments compared to conventional
machine learning approaches. By uncovering causal structures beneath volatile
sensor data, our framework not only detects cyberattacks with markedly higher
precision but also redefines robustness in anomaly detection, proving
resilience where traditional models falter under imbalance and drift. Our
framework achieves substantial gains in F1 and ROC-AUC scores over
best-performing baselines across four industrial datasets, demonstrating robust
detection of delayed and structurally complex anomalies.

</details>


### [107] [Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach](https://arxiv.org/abs/2508.09510)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.LG

TL;DR: 论文提出了一种名为Gauss-Tin的新方法，结合重放策略与高斯混合模型，以提升大语言模型（LLMs）在持续学习中的知识保留能力，实验结果显示其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在持续学习中出现的灾难性遗忘问题，即模型在学习新知识时遗忘旧知识。

Method: 提出Gauss-Tin方法，结合重放策略与高斯混合模型优化样本选择，并通过指令引导辅助生成过去的学习内容。

Result: 实验表明，Gauss-Tin在保留指标上比传统方法提升了6%。

Conclusion: Gauss-Tin是一种有效的策略，能够缓解LLMs的灾难性遗忘问题，展示了混合模型在动态学习环境中的潜力。

Abstract: Despite the significant advancements in Large Language Models (LLMs),
catastrophic forgetting remains a substantial challenge, where models lose
previously acquired knowledge upon learning new information. Continual learning
(CL) strategies have emerged as a potential solution to this problem, with
replay-based techniques demonstrating superior performance in preserving
learned knowledge. In this context, we introduce Gauss-Tin, a novel approach
that integrates the replay strategy with a Gaussian mixture model to enhance
the quality of sample selection during training, supplemented by instructional
guidance to facilitate the generation of past learning. This method aims to
improve LLMs' retention capabilities by strategically reinforcing important
past learnings while accommodating new information. Our experimental results
indicate a promising 6\% improvement in retention metrics over traditional
methods, suggesting that Gauss-Tin is an effective strategy for mitigating
catastrophic forgetting in LLMs. This study underscores the potential of hybrid
models in enhancing the robustness and adaptability of LLMs in dynamic learning
environments.

</details>


### [108] [Goal Discovery with Causal Capacity for Efficient Reinforcement Learning](https://arxiv.org/abs/2508.09624)
*Yan Yu,Yaodong Yang,Zhengbo Lu,Chengdong Ma,Wengang Zhou,Houqiang Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果容量的目标发现框架（GDCC），用于高效探索环境，通过测量状态空间中的因果容量并识别关键点作为子目标。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，建立动作与状态转移之间的因果关系能增强智能体的探索能力，但在复杂场景中测量因果关系具有挑战性。

Method: 提出因果容量的测量方法，并通过蒙特卡洛方法识别离散和连续高维环境中的关键点，将其作为子目标指导探索。

Result: 实验表明，高因果容量的状态与预期子目标一致，GDCC在多目标任务中显著提升了成功率。

Conclusion: GDCC框架通过因果容量和子目标发现，实现了更高效和有针对性的环境探索。

Abstract: Causal inference is crucial for humans to explore the world, which can be
modeled to enable an agent to efficiently explore the environment in
reinforcement learning. Existing research indicates that establishing the
causality between action and state transition will enhance an agent to reason
how a policy affects its future trajectory, thereby promoting directed
exploration. However, it is challenging to measure the causality due to its
intractability in the vast state-action space of complex scenarios. In this
paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework
for efficient environment exploration. Specifically, we first derive a
measurement of causality in state space, \emph{i.e.,} causal capacity, which
represents the highest influence of an agent's behavior on future trajectories.
After that, we present a Monte Carlo based method to identify critical points
in discrete state space and further optimize this method for continuous
high-dimensional environments. Those critical points are used to uncover where
the agent makes important decisions in the environment, which are then regarded
as our subgoals to guide the agent to make exploration more purposefully and
efficiently. Empirical results from multi-objective tasks demonstrate that
states with high causal capacity align with our expected subgoals, and our GDCC
achieves significant success rate improvements compared to baselines.

</details>


### [109] [Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring](https://arxiv.org/abs/2508.09527)
*Fang Wang,Ernesto Damiani*

Main category: cs.LG

TL;DR: 本文提出了一种统一的、可解释的GNN框架，用于预测业务流程监控中的未来事件，解决了现有GNN模型的不足。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型在PBPM中未能充分利用时间相关性和转移语义，导致性能受限。

Method: 结合前缀GCN和全局GAT模型，引入时间衰减注意力机制和转移语义边特征，增强模型性能。

Result: 在五个基准测试中，模型表现优异，无需针对数据集调整。

Conclusion: 该框架为PBPM中的事件预测提供了鲁棒、通用且可解释的解决方案。

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events
in ongoing cases based on historical event logs. While Graph Neural Networks
(GNNs) are well suited to capture structural dependencies in process data,
existing GNN-based PBPM models remain underdeveloped. Most rely either on short
prefix subgraphs or global architectures that overlook temporal relevance and
transition semantics. We propose a unified, interpretable GNN framework that
advances the state of the art along three key axes. First, we compare
prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention
Networks(GATs) to quantify the performance gap between localized and global
modeling. Second, we introduce a novel time decay attention mechanism that
constructs dynamic, prediction-centered windows, emphasizing temporally
relevant history and suppressing noise. Third, we embed transition type
semantics into edge features to enable fine grained reasoning over structurally
ambiguous traces. Our architecture includes multilevel interpretability
modules, offering diverse visualizations of attention behavior. Evaluated on
five benchmarks, the proposed models achieve competitive Top-k accuracy and DL
scores without per-dataset tuning. By addressing architectural, temporal, and
semantic gaps, this work presents a robust, generalizable, and explainable
solution for next event prediction in PBPM.

</details>


### [110] [TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling](https://arxiv.org/abs/2508.09630)
*Yifei Sun,Junming Liu,Ding Wang,Yirong Chen,Xuefeng Yan*

Main category: cs.LG

TL;DR: TimeMKG是一个多模态因果推理框架，通过结合变量语义和历史时间序列数据，提升时间序列建模的预测性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型忽略变量名称和数据描述中的语义信息，而这些信息对建模至关重要。TimeMKG旨在利用这些语义信息，实现更鲁棒和可解释的建模。

Method: TimeMKG使用大语言模型解析变量语义，构建多变量知识图谱，并通过双模态编码器分别建模语义提示和历史时间序列的统计模式，最后通过跨模态注意力融合这些表示。

Result: 实验表明，引入变量级知识显著提升了预测性能和泛化能力。

Conclusion: TimeMKG通过结合语义和统计信息，为时间序列建模提供了更强大且可解释的框架。

Abstract: Multivariate time series data typically comprises two distinct modalities:
variable semantics and sampled numerical observations. Traditional time series
models treat variables as anonymous statistical signals, overlooking the rich
semantic information embedded in variable names and data descriptions. However,
these textual descriptors often encode critical domain knowledge that is
essential for robust and interpretable modeling. Here we present TimeMKG, a
multimodal causal reasoning framework that elevates time series modeling from
low-level signal processing to knowledge informed inference. TimeMKG employs
large language models to interpret variable semantics and constructs structured
Multivariate Knowledge Graphs that capture inter-variable relationships. A
dual-modality encoder separately models the semantic prompts, generated from
knowledge graph triplets, and the statistical patterns from historical time
series. Cross-modality attention aligns and fuses these representations at the
variable level, injecting causal priors into downstream tasks such as
forecasting and classification, providing explicit and interpretable priors to
guide model reasoning. The experiment in diverse datasets demonstrates that
incorporating variable-level knowledge significantly improves both predictive
performance and generalization.

</details>


### [111] [SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification](https://arxiv.org/abs/2508.09544)
*Sasan Tavakkol,Lin Chen,Max Springer,Abigail Schantz,Blaž Bratanič,Vincent Cohen-Addad,MohammadHossein Bateni*

Main category: cs.LG

TL;DR: SYNAPSE-G利用LLMs生成合成数据解决罕见事件分类中的冷启动问题，通过半监督标签传播扩展数据集，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 标记数据稀缺，尤其是罕见事件，阻碍了有效机器学习模型的训练。

Method: 提出SYNAPSE-G，利用LLMs生成合成数据作为种子，通过半监督标签传播扩展数据集，最终训练分类器。

Result: 在SST2和MHS数据集上，SYNAPSE-G在发现正标签方面优于基线方法。

Conclusion: SYNAPSE-G通过合成数据生成和半监督学习，有效解决了罕见事件分类问题。

Abstract: Scarcity of labeled data, especially for rare events, hinders training
effective machine learning models. This paper proposes SYNAPSE-G (Synthetic
Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline
leveraging Large Language Models (LLMs) to generate synthetic training data for
rare event classification, addressing the cold-start problem. This synthetic
data serve as seeds for semi-supervised label propagation on a similarity graph
constructed between the seeds and a large unlabeled dataset. This identifies
candidate positive examples, subsequently labeled by an oracle (human or LLM).
The expanded dataset then trains/fine-tunes a classifier. We theoretically
analyze how the quality (validity and diversity) of the synthetic data impacts
the precision and recall of our method. Experiments on the imbalanced SST2 and
MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,
outperforming baselines including nearest neighbor search.

</details>


### [112] [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](https://arxiv.org/abs/2508.09561)
*Changyuan Zhao,Guangyuan Liu,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Jiawen Kang,Dusit Niyato,Zan Li,Xuemin,Shen,Zhu Han,Sumei Sun,Chau Yuen,Dong In Kim*

Main category: cs.LG

TL;DR: EGI（边缘通用智能）通过世界模型实现分布式智能体的感知、推理和自主行动，填补了无线边缘领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 探索世界模型在无线边缘计算中的应用，以提升智能体的自主性和优化能力。

Method: 分析世界模型的架构基础（如潜在表示学习、动态建模和基于想象的规划），并探讨其在EGI场景中的主动应用。

Result: 展示了世界模型在车辆网络、无人机网络、物联网等领域的优化潜力，以及与基础模型和数字孪生的协同作用。

Conclusion: 提出了安全保证、高效训练等开放挑战，为下一代智能边缘系统提供了理论和实践指导。

Abstract: Edge General Intelligence (EGI) represents a transformative evolution of edge
computing, where distributed agents possess the capability to perceive, reason,
and act autonomously across diverse, dynamic environments. Central to this
vision are world models, which act as proactive internal simulators that not
only predict but also actively imagine future trajectories, reason under
uncertainty, and plan multi-step actions with foresight. This proactive nature
allows agents to anticipate potential outcomes and optimize decisions ahead of
real-world interactions. While prior works in robotics and gaming have
showcased the potential of world models, their integration into the wireless
edge for EGI remains underexplored. This survey bridges this gap by offering a
comprehensive analysis of how world models can empower agentic artificial
intelligence (AI) systems at the edge. We first examine the architectural
foundations of world models, including latent representation learning, dynamics
modeling, and imagination-based planning. Building on these core capabilities,
we illustrate their proactive applications across EGI scenarios such as
vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of
Things (IoT) systems, and network functions virtualization, thereby
highlighting how they can enhance optimization under latency, energy, and
privacy constraints. We then explore their synergy with foundation models and
digital twins, positioning world models as the cognitive backbone of EGI.
Finally, we highlight open challenges, such as safety guarantees, efficient
training, and constrained deployment, and outline future research directions.
This survey provides both a conceptual foundation and a practical roadmap for
realizing the next generation of intelligent, autonomous edge systems.

</details>


### [113] [Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models](https://arxiv.org/abs/2508.09719)
*Anish Narain,Ritam Majumdar,Nikita Narayanan,Dominic Marshall,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 论文提出了一种结合临床笔记上下文信息的方法，通过大型语言模型（LLM）生成额外概念，提升了概念瓶颈模型（CBM）在急性呼吸窘迫综合征（ARDS）识别中的性能。


<details>
  <summary>Details</summary>
Motivation: 利用公开临床数据集理解疾病异质性和个性化治疗，但数据不完整且缺乏标签，现有AI工具解释性有限。

Method: 使用大型语言模型（LLM）处理临床笔记，生成额外概念以改进概念瓶颈模型（CBM）。

Result: 性能提升10%，学习到更全面的概念，减少信息泄漏和依赖虚假捷径。

Conclusion: 结合临床笔记的上下文信息可显著提升CBM的性能和解释性。

Abstract: Large, publicly available clinical datasets have emerged as a novel resource
for understanding disease heterogeneity and to explore personalization of
therapy. These datasets are derived from data not originally collected for
research purposes and, as a result, are often incomplete and lack critical
labels. Many AI tools have been developed to retrospectively label these
datasets, such as by performing disease classification; however, they often
suffer from limited interpretability. Previous work has attempted to explain
predictions using Concept Bottleneck Models (CBMs), which learn interpretable
concepts that map to higher-level clinical ideas, facilitating human
evaluation. However, these models often experience performance limitations when
the concepts fail to adequately explain or characterize the task. We use the
identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging
test case to demonstrate the value of incorporating contextual information from
clinical notes to improve CBM performance. Our approach leverages a Large
Language Model (LLM) to process clinical notes and generate additional
concepts, resulting in a 10% performance gain over existing methods.
Additionally, it facilitates the learning of more comprehensive concepts,
thereby reducing the risk of information leakage and reliance on spurious
shortcuts, thus improving the characterization of ARDS.

</details>


### [114] [Online Prediction with Limited Selectivity](https://arxiv.org/abs/2508.09592)
*Licheng Liu,Mingda Qiao*

Main category: cs.LG

TL;DR: 论文提出了一种有限选择性预测模型（PLS），研究在预测窗口受限时的最优预测误差，并引入复杂度度量提供实例依赖的误差界限。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测模型允许预测者在任意时间开始预测，但实际中预测窗口可能受限，因此需要研究有限选择性下的预测性能。

Method: 引入PLS模型，分析实例依赖和平均情况下的最优预测误差，提出复杂度度量以量化误差界限。

Result: 对于随机生成的PLS实例，提出的复杂度度量能高概率匹配最优误差界限。

Conclusion: PLS模型为有限选择性预测提供了理论框架，复杂度度量在实际应用中具有指导意义。

Abstract: Selective prediction [Dru13, QV19] models the scenario where a forecaster
freely decides on the prediction window that their forecast spans. Many data
statistics can be predicted to a non-trivial error rate without any
distributional assumptions or expert advice, yet these results rely on that the
forecaster may predict at any time. We introduce a model of Prediction with
Limited Selectivity (PLS) where the forecaster can start the prediction only on
a subset of the time horizon. We study the optimal prediction error both on an
instance-by-instance basis and via an average-case analysis. We introduce a
complexity measure that gives instance-dependent bounds on the optimal error.
For a randomly-generated PLS instance, these bounds match with high
probability.

</details>


### [115] [Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs](https://arxiv.org/abs/2508.09627)
*Subhankar Sarkar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 提出了一种物理和几何感知的时空谱图神经算子（πG-Sp²GNO），用于高效求解偏微分方程（PDEs），在复杂几何和有限标注数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 偏微分方程的高效求解是科学与工程中的核心挑战，尤其在复杂几何和有限数据场景下。

Method: 改进Sp²GNO，引入几何感知和物理信息，提出多尺度学习和混合物理损失函数。

Result: 在多种基准测试中表现优于现有物理信息神经算子算法。

Conclusion: πG-Sp²GNO在复杂几何和时间相关问题中具有高效性和准确性。

Abstract: Solving partial differential equations (PDEs) efficiently and accurately
remains a cornerstone challenge in science and engineering, especially for
problems involving complex geometries and limited labeled data. We introduce a
Physics- and Geometry- Aware Spatio-Spectral Graph Neural Operator
($\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and
time-dependent PDEs. The proposed approach first improves upon the recently
developed Sp$^2$GNO by enabling geometry awareness and subsequently exploits
the governing physics to learn the underlying solution operator in a
simulation-free setup. While the spatio-spectral structure present in the
proposed architecture allows multiscale learning, two separate strategies for
enabling geometry awareness is introduced in this paper. For time dependent
problems, we also introduce a novel hybrid physics informed loss function that
combines higher-order time-marching scheme with upscaled theory inspired
stochastic projection scheme. This allows accurate integration of the
physics-information into the loss function. The performance of the proposed
approach is illustrated on number of benchmark examples involving regular and
complex domains, variation in geometry during inference, and time-independent
and time-dependent problems. The results obtained illustrate the efficacy of
the proposed approach as compared to the state-of-the-art physics-informed
neural operator algorithms in the literature.

</details>


### [116] [Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations](https://arxiv.org/abs/2508.09787)
*Mauro Tucci*

Main category: cs.LG

TL;DR: Proto-PINV+H是一种快速训练范式，结合闭式权重计算与梯度优化，通过更新原型、软标签和隐藏激活，实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 旨在通过减少权重空间的自由度，转移到数据/激活空间，提高训练效率和准确性。

Method: 采用闭式权重计算和梯度优化，通过伪逆解和Adam更新原型，支持多层扩展和可学习参数。

Result: 在MNIST和Fashion-MNIST上分别达到97.8%和89.3%的测试准确率，训练时间仅3.9-4.5秒。

Conclusion: 该方法在准确性、速度和模型大小方面优于传统方法，如ELM和浅层MLP。

Abstract: We present Proto-PINV+H, a fast training paradigm that combines closed-form
weight computation with gradient-based optimisation of a small set of synthetic
inputs, soft labels, and-crucially-hidden activations. At each iteration we
recompute all weight matrices in closed form via two (or more)
ridge-regularised pseudo-inverse solves, while updating only the prototypes
with Adam. The trainable degrees of freedom are thus shifted from weight space
to data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k
train, 10k test), our method reaches 97.8% and 89.3% test accuracy on the
official 10k test sets, respectively, in 3.9s--4.5s using approximately 130k
trainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a
multi-layer extension (optimised activations at each hidden stage), learnable
ridge parameters, optional PCA/PLS projections, and theory linking the
condition number of prototype matrices to generalisation. The approach yields
favourable accuracy--speed--size trade-offs against ELM, random-feature ridge,
and shallow MLPs trained by back-propagation.

</details>


### [117] [Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments](https://arxiv.org/abs/2508.09659)
*Johannes F. Hevler,Shivam Verma,Mirat Soijtra,Carolyn R. Bertozzi*

Main category: cs.LG

TL;DR: Thermal Tracks是一个基于Python的统计框架，用于分析蛋白质热稳定性数据，克服了现有热蛋白质组分析（TPP）方法的关键限制。


<details>
  <summary>Details</summary>
Motivation: 传统TPP方法假设蛋白质熔解曲线为S形，且受限于经验零分布，导致仅能检测约5%的数据。Thermal Tracks旨在解决这一问题，提供更灵活的分析方法。

Method: 采用高斯过程（GP）模型和平方指数核，灵活建模任何熔解曲线形状，并通过核先验生成无偏零分布。

Result: 特别适用于分析显著改变蛋白质热稳定性的扰动（如通路抑制、遗传修饰或环境压力），并能处理非传统熔解曲线的蛋白质（如相分离蛋白和膜蛋白）。

Conclusion: Thermal Tracks是一个免费、灵活的工具，适用于全蛋白质组热稳定性研究。

Abstract: Thermal Tracks is a Python-based statistical framework for analyzing protein
thermal stability data that overcomes key limitations of existing thermal
proteome profiling (TPP) work-flows. Unlike standard approaches that assume
sigmoidal melting curves and are constrained by empirical null distributions
(limiting significant hits to approximately 5 % of data), Thermal Tracks uses
Gaussian Process (GP) models with squared-exponential kernels to flexibly model
any melting curve shape while generating unbiased null distributions through
kernel priors. This framework is particularly valuable for analyzing
proteome-wide perturbations that significantly alter protein thermal stability,
such as pathway inhibitions, genetic modifications, or environmental stresses,
where conventional TPP methods may miss biologically relevant changes due to
their statistical constraints. Furthermore, Thermal Tracks excels at analyzing
proteins with un-conventional melting profiles, including phase-separating
proteins and membrane proteins, which often exhibit complex, non-sigmoidal
thermal stability behaviors. Thermal Tracks is freely available from GitHub and
is implemented in Python, providing an accessible and flexible tool for
proteome-wide thermal profiling studies.

</details>


### [118] [Provable In-Context Vector Arithmetic via Retrieving Task Concepts](https://arxiv.org/abs/2508.09820)
*Dake Bu,Wei Huang,Andi Han,Atsushi Nitanda,Qingfu Zhang,Hau-San Wong,Taiji Suzuki*

Main category: cs.LG

TL;DR: 论文提出了一个理论框架，解释了大语言模型（LLMs）在上下文学习（ICL）中如何通过向量算术解决事实召回任务，并证明了其强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明LLMs在ICL中存在潜在任务向量，并利用Word2Vec类向量算术解决事实召回任务，但仍缺乏理论解释。本文旨在填补这一空白。

Method: 基于经验的分层概念建模，提出了一个优化理论，证明非线性残差变换器通过梯度下降和交叉熵损失执行事实召回任务。

Result: 理论证明了0-1损失的收敛性，并展示了强泛化能力，包括对概念重组和分布偏移的鲁棒性。

Conclusion: 研究阐明了变换器相对于静态嵌入模型的优势，并通过实证模拟验证了理论结果。

Abstract: In-context learning (ICL) has garnered significant attention for its ability
to grasp functions/tasks from demonstrations. Recent studies suggest the
presence of a latent task/function vector in LLMs during ICL. Merullo et al.
(2024) showed that LLMs leverage this vector alongside the residual stream for
Word2Vec-like vector arithmetic, solving factual-recall ICL tasks.
Additionally, recent work empirically highlighted the key role of
Question-Answer data in enhancing factual-recall capabilities. Despite these
insights, a theoretical explanation remains elusive. To move one step forward,
we propose a theoretical framework building on empirically grounded
hierarchical concept modeling. We develop an optimization theory, showing how
nonlinear residual transformers trained via gradient descent on cross-entropy
loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss
convergence and show the strong generalization, including robustness to concept
recombination and distribution shifts. These results elucidate the advantages
of transformers over static embedding predecessors. Empirical simulations
corroborate our theoretical insights.

</details>


### [119] [Global Convergence Analysis of Vanilla Gradient Descent for Asymmetric Matrix Completion](https://arxiv.org/abs/2508.09685)
*Xu Zhang,Shuo Chen,Jinsheng Li,Xiangying Pang,Maoguo Gong*

Main category: cs.LG

TL;DR: 本文研究了非对称低秩矩阵补全问题，通过梯度下降方法解决，发现无需正则化项也能保证收敛。


<details>
  <summary>Details</summary>
Motivation: 探索梯度下降方法在非对称低秩矩阵补全中的表现，验证正则化项的必要性。

Method: 使用留一技术和谱初始化，证明普通梯度下降具有线性收敛率。

Result: 实验表明算法计算成本更低，补全性能与其他方法相当。

Conclusion: 梯度下降具有隐式正则化特性，无需显式正则化项即可高效收敛。

Abstract: This paper investigates the asymmetric low-rank matrix completion problem,
which can be formulated as an unconstrained non-convex optimization problem
with a nonlinear least-squares objective function, and is solved via gradient
descent methods. Previous gradient descent approaches typically incorporate
regularization terms into the objective function to guarantee convergence.
However, numerical experiments and theoretical analysis of the gradient flow
both demonstrate that the elimination of regularization terms in gradient
descent algorithms does not adversely affect convergence performance. By
introducing the leave-one-out technique, we inductively prove that the vanilla
gradient descent with spectral initialization achieves a linear convergence
rate with high probability. Besides, we demonstrate that the balancing
regularization term exhibits a small norm during iterations, which reveals the
implicit regularization property of gradient descent. Empirical results show
that our algorithm has a lower computational cost while maintaining comparable
completion performance compared to other gradient descent algorithms.

</details>


### [120] [Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture](https://arxiv.org/abs/2508.09693)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 该论文提出了一个基于算子理论的框架，用于在嵌入空间中进行时间锚定，通过漂移映射和事件索引块的交替实现，最终通过仿射投影完成。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种严格的数学框架，用于处理嵌入空间中的时间锚定问题，并提供可证明的理论保证。

Method: 方法包括构建漂移映射与事件索引块的交替结构，并利用仿射投影完成收敛。论文还提出了一个内部手稿计算机（MC），其计算完全由这些算子定义。

Result: 结果包括证明了变量块收缩引理、漂移-投影收敛定理，以及软注意力层的Lipschitz性质。

Conclusion: 结论是该框架为时间锚定提供了严格的理论基础，并证明了其在注意力层中的适用性。

Abstract: We develop an operator-theoretic framework for temporal anchoring in
embedding spaces, modeled as drift maps interleaved with event-indexed blocks
culminating in affine projections. We provide complete proofs for a
variable-block contraction lemma (products of Lipschitz factors), a
drift--projection convergence theorem with explicit uniform-gap envelopes, and
ontological convergence under nested affine anchors with a robustness variant.
We formalize an internal Manuscript Computer (MC) whose computations are
defined purely by these operators and prove a rigorous finite-run equivalence
theorem (with perturbation bounds). For attention layers, we give a
self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive
sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All
floats are placed exactly where written; the manuscript uses only in-paper
pseudocode and appendix figures.

</details>


### [121] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: 提出了一种动态连接掩码（DCM）机制，用于增强MLP和KAN对噪声标签的鲁棒性，通过自适应掩码减少梯度误差，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中噪声标签不可避免，现有研究多关注损失函数和样本选择，对模型架构正则化的探索较少。

Method: 提出DCM机制，动态评估边的重要性并掩码不重要边，可与其他噪声鲁棒方法结合。

Result: 在合成和真实数据集上优于SOTA方法，并首次验证KAN在噪声标签下的优越性。

Conclusion: DCM机制有效提升噪声鲁棒性，KAN在噪声场景中表现优于MLP。

Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong
capacity of deep neural networks to memorize corrupted labels, these noisy
labels can cause significant performance degradation. Existing research on
mitigating the negative effects of noisy labels has mainly focused on robust
loss functions and sample selection, with comparatively limited exploration of
regularization in model architecture. Inspired by the sparsity regularization
used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection
Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and
KANs to enhance the robustness of classifiers against noisy labels. The
mechanism can adaptively mask less important edges during training by
evaluating their information-carrying capacity. Through theoretical analysis,
we demonstrate its efficiency in reducing gradient error. Our approach can be
seamlessly integrated into various noise-robust training methods to build more
robust deep networks, including robust loss functions, sample selection
strategies, and regularization techniques. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that our method consistently
outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the
first to investigate KANs as classifiers against noisy labels, revealing their
superior noise robustness over MLPs in real-world noisy scenarios. Our code
will soon be publicly available.

</details>


### [122] [GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation](https://arxiv.org/abs/2508.09710)
*Yitong Luo,Islem Rekik*

Main category: cs.LG

TL;DR: 论文提出GraphTreeGen（GTG），一种基于子树的生成框架，用于高效、准确地合成脑连接组，解决了现有模型在局部细节、节点属性依赖、边权重预测和计算效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 脑连接组对理解大脑组织至关重要，但获取成本高且耗时，因此需要生成方法。现有模型存在局部细节模糊、依赖节点属性、边权重预测不准确和计算效率低等问题。

Method: GTG将连接组分解为熵引导的k-hop子树，通过共享GCN编码，结合全局节点特征，并通过双分支解码器联合预测边的存在和权重。

Result: GTG在自监督任务中优于现有方法，在监督任务中表现竞争性，具有更高的结构保真度和更精确的权重预测，且内存需求更低。

Conclusion: GTG提供了一种高效、准确的脑连接组生成方法，其模块化设计支持扩展到超分辨率和跨模态合成。

Abstract: Brain connectomes, representing neural connectivity as graphs, are crucial
for understanding brain organization but costly and time-consuming to acquire,
motivating generative approaches. Recent advances in graph generative modeling
offer a data-driven alternative, enabling synthetic connectome generation and
reducing dependence on large neuroimaging datasets. However, current models
face key limitations: (i) compressing the whole graph into a single latent code
(e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node
attributes rarely available in connectomes reduces reconstruction quality;
(iii) edge-centric models emphasize topology but overlook accurate edge-weight
prediction, harming quantitative fidelity; and (iv) computationally expensive
designs (e.g., edge-conditioned convolutions) impose high memory demands,
limiting scalability. We propose GraphTreeGen (GTG), a subtree-centric
generative framework for efficient, accurate connectome synthesis. GTG
decomposes each connectome into entropy-guided k-hop trees capturing
informative local structure, encoded by a shared GCN. A bipartite
message-passing layer fuses subtree embeddings with global node features, while
a dual-branch decoder jointly predicts edge existence and weights to
reconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in
self-supervised tasks and remains competitive in supervised settings,
delivering higher structural fidelity and more precise weights with far less
memory. Its modular design enables extensions to connectome super-resolution
and cross-modality synthesis. Code: https://github.com/basiralab/GTG/

</details>


### [123] [Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning](https://arxiv.org/abs/2508.09883)
*Xiaojun Wu,Xiaoguang Jiang,Huiyang Li,Jucai Zhai,Dengfeng Liu,Qiaobo Hao,Huang Liu,Zhiguo Yang,Ji Xie,Ninglun Gu,Jin Yang,Kailai Zhang,Yelun Bao,Jun Wang*

Main category: cs.LG

TL;DR: 提出了一种数据高效的蒸馏框架（DED），通过优化推理蒸馏的帕累托前沿，显著减少计算成本，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法通过扩展语料库和多阶段训练提升了推理能力，但计算成本高昂且缺乏明确的推理扩展规律。

Method: 结合强化学习的策略和多样化推理轨迹，选择最优教师模型，并使用精心筛选的小规模语料库进行蒸馏。

Result: 在数学推理和代码生成任务上取得最先进成果，仅需0.8k样本。

Conclusion: DED提供了一种高效且实用的推理能力提升途径，同时兼顾通用能力。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities in
tasks such as algorithmic coding and mathematical problem-solving. Recent
methods have improved reasoning through expanded corpus and multistage training
combining reinforcement learning and supervised fine-tuning. Although some
methods suggest that small but targeted dataset can incentivize reasoning via
only distillation, a reasoning scaling laws is still taking shape, increasing
computational costs. To address this, we propose a data-efficient distillation
framework (DED) that optimizes the Pareto frontier of reasoning distillation.
Inspired by the on-policy learning and diverse roll-out strategies of
reinforcement learning, the key idea of our approach is threefold: (1) We
identify that benchmark scores alone do not determine an effective teacher
model. Through comprehensive comparisons of leading reasoning LLMs, we develop
a method to select an optimal teacher model. (2) While scaling distillation can
enhance reasoning, it often degrades out-of-domain performance. A carefully
curated, smaller corpus achieves a balanced trade-off between in-domain and
out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the
student model to develop robust reasoning skills. We validate our method
through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and
code generation (LiveCodeBench), achieving state-of-the-art results with only
0.8k carefully curated examples, bypassing the need for extensive scaling. Our
systematic analysis demonstrates that DED outperforms existing methods by
considering factors beyond superficial hardness, token length, or teacher model
capability. This work offers a practical and efficient pathway to advanced
reasoning while preserving general capabilities.

</details>


### [124] [Rare anomalies require large datasets: About proving the existence of anomalies](https://arxiv.org/abs/2508.09894)
*Simon Klüttermann,Emmanuel Müller*

Main category: cs.LG

TL;DR: 论文研究了如何确定数据集中是否存在异常，提出了一个基于数据集大小、污染率和算法常数的下限条件。


<details>
  <summary>Details</summary>
Motivation: 异常检测中确认异常存在的方法尚未充分研究，本文旨在填补这一空白。

Method: 通过超过三百万次统计测试，分析数据集大小、污染率和算法常数之间的关系。

Result: 发现了一个下限条件：N ≥ α_algo/ν²，用于确认异常存在。

Conclusion: 该条件揭示了异常检测中确认异常存在的可行性限制。

Abstract: Detecting whether any anomalies exist within a dataset is crucial for
effective anomaly detection, yet it remains surprisingly underexplored in
anomaly detection literature. This paper presents a comprehensive study that
addresses the fundamental question: When can we conclusively determine that
anomalies are present? Through extensive experimentation involving over three
million statistical tests across various anomaly detection tasks and
algorithms, we identify a relationship between the dataset size, contamination
rate, and an algorithm-dependent constant $ \alpha_{\text{algo}} $. Our results
demonstrate that, for an unlabeled dataset of size $ N $ and contamination rate
$ \nu $, the condition $ N \ge \frac{\alpha_{\text{algo}}}{\nu^2} $ represents
a lower bound on the number of samples required to confirm anomaly existence.
This threshold implies a limit to how rare anomalies can be before proving
their existence becomes infeasible.

</details>


### [125] [Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization](https://arxiv.org/abs/2508.09730)
*Qiaolei Gu,Yu Li,DingYi Zeng,Lu Wang,Ming Pang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: 提出GenCO框架，结合生成建模与多实例奖励学习，优化电商广告创意组合选择，显著提升广告收入。


<details>
  <summary>Details</summary>
Motivation: 现有方法单独评估创意元素，无法高效探索巨大组合空间，需新方法提升广告效果。

Method: 两阶段架构：1) 生成模型生成多样化创意组合，强化学习优化；2) 多实例学习模型分配组合级奖励至单个元素。

Result: 在领先电商平台部署后显著提升广告收入，并发布工业数据集。

Conclusion: GenCO框架有效解决创意组合优化问题，具有实际应用价值。

Abstract: In e-commerce advertising, selecting the most compelling combination of
creative elements -- such as titles, images, and highlights -- is critical for
capturing user attention and driving conversions. However, existing methods
often evaluate creative components individually, failing to navigate the
exponentially large search space of possible combinations. To address this
challenge, we propose a novel framework named GenCO that integrates generative
modeling with multi-instance reward learning. Our unified two-stage
architecture first employs a generative model to efficiently produce a diverse
set of creative combinations. This generative process is optimized with
reinforcement learning, enabling the model to effectively explore and refine
its selections. Next, to overcome the challenge of sparse user feedback, a
multi-instance learning model attributes combination-level rewards, such as
clicks, to the individual creative elements. This allows the reward model to
provide a more accurate feedback signal, which in turn guides the generative
model toward creating more effective combinations. Deployed on a leading
e-commerce platform, our approach has significantly increased advertising
revenue, demonstrating its practical value. Additionally, we are releasing a
large-scale industrial dataset to facilitate further research in this important
domain.

</details>


### [126] [Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs](https://arxiv.org/abs/2508.09904)
*Arjun Ashok,Andrew Robert Williams,Vincent Zhihao Zheng,Irina Rish,Nicolas Chapados,Étienne Marcotte,Valentina Zantedeschi,Alexandre Drouin*

Main category: cs.LG

TL;DR: 论文提出了四种策略（ReDP、CorDP、IC-DP、RouteDP）来提升大型语言模型（LLMs）在上下文辅助预测任务中的零样本能力，并展示了这些策略在不同模型上的优势。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的预测任务需要模型整合历史数据和上下文信息，而现有方法对LLMs的潜力挖掘不足。

Method: 通过四种策略：ReDP（提升可解释性）、CorDP（优化现有预测）、IC-DP（嵌入历史示例）、RouteDP（任务路由优化），探索LLMs在上下文辅助预测中的表现。

Result: 在CiK基准测试中，这些策略在不同规模和家族的LLMs上均优于直接提示方法。

Conclusion: 这些简单而有效的策略为基于LLM的上下文辅助预测提供了进一步改进的可能性。

Abstract: Forecasting in real-world settings requires models to integrate not only
historical data but also relevant contextual information, often available in
textual form. While recent work has shown that large language models (LLMs) can
be effective context-aided forecasters via na\"ive direct prompting, their full
potential remains underexplored. We address this gap with 4 strategies,
providing new insights into the zero-shot capabilities of LLMs in this setting.
ReDP improves interpretability by eliciting explicit reasoning traces, allowing
us to assess the model's reasoning over the context independently from its
forecast accuracy. CorDP leverages LLMs solely to refine existing forecasts
with context, enhancing their applicability in real-world forecasting
pipelines. IC-DP proposes embedding historical examples of context-aided
forecasting tasks in the prompt, substantially improving accuracy even for the
largest models. Finally, RouteDP optimizes resource efficiency by using LLMs to
estimate task difficulty, and routing the most challenging tasks to larger
models. Evaluated on different kinds of context-aided forecasting tasks from
the CiK benchmark, our strategies demonstrate distinct benefits over na\"ive
prompting across LLMs of different sizes and families. These results open the
door to further simple yet effective improvements in LLM-based context-aided
forecasting.

</details>


### [127] [HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks](https://arxiv.org/abs/2508.09743)
*Yanick Chistian Tchenko,Felix Mohr,Hicham Hadj Abdelkader,Hedi Tabia*

Main category: cs.LG

TL;DR: 提出了一种名为Hereditary Knowledge Transfer (HKT)的方法，通过结构化知识继承优化小型可部署模型，性能优于传统知识蒸馏。


<details>
  <summary>Details</summary>
Motivation: 解决深度模型性能提升带来的可集成性和效率问题，通过生物启发的知识继承机制优化小型模型。

Method: HKT框架包含Extraction、Transfer和Mixture (ETM)三个阶段，结合Genetic Attention机制选择性继承任务相关特征。

Result: 在多种视觉任务（如光流、图像分类、语义分割）中，HKT显著提升小型模型性能，同时保持其紧凑性。

Conclusion: HKT为资源受限环境提供了一种通用、可解释且可扩展的高性能神经网络部署方案。

Abstract: A prevailing trend in neural network research suggests that model performance
improves with increasing depth and capacity - often at the cost of
integrability and efficiency. In this paper, we propose a strategy to optimize
small, deployable models by enhancing their capabilities through structured
knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a
biologically inspired framework for modular and selective transfer of
task-relevant features from a larger, pretrained parent network to a smaller
child model. Unlike standard knowledge distillation, which enforces uniform
imitation of teacher outputs, HKT draws inspiration from biological inheritance
mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage
process of feature transfer. Neural network blocks are treated as functional
carriers, and knowledge is transmitted through three biologically motivated
components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention
(GA) mechanism governs the integration of inherited and native representations,
ensuring both alignment and selectivity. We evaluate HKT across diverse vision
tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),
and semantic segmentation (LiTS), demonstrating that it significantly improves
child model performance while preserving its compactness. The results show that
HKT consistently outperforms conventional distillation approaches, offering a
general-purpose, interpretable, and scalable solution for deploying
high-performance neural networks in resource-constrained environments.

</details>


### [128] [Residual Reservoir Memory Networks](https://arxiv.org/abs/2508.09925)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出了一种新型的未训练循环神经网络（ResRMN），结合线性与非线性记忆库，通过残差正交连接增强长期输入传播，实验证明其优于传统RC模型。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过结合线性与非线性记忆库以及残差正交连接，提升循环神经网络在时间序列和分类任务中的性能。

Method: 提出ResRMN模型，结合线性记忆库和非线性库，利用残差正交连接优化长期输入传播，并通过线性稳定性分析研究其动态特性。

Result: 实验结果表明，ResRMN在时间序列和1-D分类任务中优于传统RC模型。

Conclusion: ResRMN通过创新的结构设计，显著提升了循环神经网络的性能，尤其在长期依赖任务中表现突出。

Abstract: We introduce a novel class of untrained Recurrent Neural Networks (RNNs)
within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory
Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear
reservoir, where the latter is based on residual orthogonal connections along
the temporal dimension for enhanced long-term propagation of the input. The
resulting reservoir state dynamics are studied through the lens of linear
stability analysis, and we investigate diverse configurations for the temporal
residual connections. The proposed approach is empirically assessed on
time-series and pixel-level 1-D classification tasks. Our experimental results
highlight the advantages of the proposed approach over other conventional RC
models.

</details>


### [129] [A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers](https://arxiv.org/abs/2508.09747)
*Nazira Dunbayeva,Yulong Li,Yutong Xie,Imran Razzak*

Main category: cs.LG

TL;DR: 该研究开发了一种机器学习流程，通过捕捉生物标志物的动态变化来预测年龄，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 预测个体的衰老轨迹是预防医学和生物信息学的核心挑战，现有模型难以捕捉衰老的动态纵向特性。

Method: 利用纵向队列数据，设计新特征以捕捉生物标志物的变化速率，并训练LightGBM模型进行预测。

Result: 模型在后续时间点的预测中表现优异（男性R²=0.515，女性R²=0.498），显著优于传统方法。

Conclusion: 动态健康轨迹是生物年龄的关键决定因素，该框架为临床工具开发提供了基础。

Abstract: Predicting an individual's aging trajectory is a central challenge in
preventative medicine and bioinformatics. While machine learning models can
predict chronological age from biomarkers, they often fail to capture the
dynamic, longitudinal nature of the aging process. In this work, we developed
and validated a machine learning pipeline to predict age using a longitudinal
cohort with data from two distinct time periods (2019-2020 and 2021-2022). We
demonstrate that a model using only static, cross-sectional biomarkers has
limited predictive power when generalizing to future time points. However, by
engineering novel features that explicitly capture the rate of change (slope)
of key biomarkers over time, we significantly improved model performance. Our
final LightGBM model, trained on the initial wave of data, successfully
predicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for
males, $R^2 = 0.498$ for females), significantly outperforming both traditional
linear models and other tree-based ensembles. SHAP analysis of our successful
model revealed that the engineered slope features were among the most important
predictors, highlighting that an individual's health trajectory, not just their
static health snapshot, is a key determinant of biological age. Our framework
paves the way for clinical tools that dynamically track patient health
trajectories, enabling early intervention and personalized prevention
strategies for age-related diseases.

</details>


### [130] [$μ$-Parametrization for Mixture of Experts](https://arxiv.org/abs/2508.09752)
*Jan Małaśnicki,Kamil Ciebiera,Mateusz Boruń,Maciej Pióro,Jan Ludziejewski,Maciej Stefaniak,Michał Krutul,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jakub Krajewski*

Main category: cs.LG

TL;DR: 本文探讨了将μTransfer技术应用于MoE架构的理论与实证研究，提出了μP方法并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管μTransfer和MoE架构在各自领域取得了进展，但二者的结合尚未被研究。本文旨在填补这一空白。

Method: 提出了μ-Parameterization（μP）方法，为MoE中的路由器和专家模块提供了理论保证，并通过实验验证了其有效性。

Result: 实证结果表明，μP方法在MoE架构中有效，同时研究了专家数量和粒度对最优学习率的影响。

Conclusion: 本文为MoE架构的μTransfer应用提供了理论基础和实证支持，为未来研究开辟了新方向。

Abstract: Recent years have seen a growing interest and adoption of LLMs, with
$\mu$Transfer becoming a key technique for tuning hyperparameters in
large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a
leading architecture in extremely large models. However, the intersection of
these two advancements has remained unexplored. In this work, we derive a
$\mu$-Parameterization ($\mu$P) for MoE, providing theoretical guarantees for
feature learning across model widths in both the router and experts. We
empirically validate our parameterization and further investigate how scaling
the number of experts and granularity affects the optimal learning rate.

</details>


### [131] [TriForecaster: A Mixture of Experts Framework for Multi-Region Electric Load Forecasting with Tri-dimensional Specialization](https://arxiv.org/abs/2508.09753)
*Zhaoyang Zhu,Zhipeng Zeng,Qiming Chen,Linxiao Yang,Peiyuan Liu,Weiqi Chen,Liang Sun*

Main category: cs.LG

TL;DR: 论文提出TriForecaster框架，通过混合专家和多任务学习方法解决多区域电力负荷预测问题，显著降低预测误差。


<details>
  <summary>Details</summary>
Motivation: 智能电网和电表的普及提供了更详细的负荷数据，但多区域电力负荷预测面临区域、上下文和时间变化的挑战。

Method: 提出TriForecaster框架，结合Mixture of Experts和多任务学习，设计RegionMixer和CTSpecializer层处理区域、上下文和时间变化。

Result: 在四个真实数据集上，TriForecaster平均预测误差降低22.4%，并在中国东部17个城市成功部署。

Conclusion: TriForecaster在多区域电力负荷预测中表现出色，具有灵活性和广泛适用性。

Abstract: Electric load forecasting is pivotal for power system operation, planning and
decision-making. The rise of smart grids and meters has provided more detailed
and high-quality load data at multiple levels of granularity, from home to bus
and cities. Motivated by similar patterns of loads across different cities in a
province in eastern China, in this paper we focus on the Multi-Region Electric
Load Forecasting (MRELF) problem, targeting accurate short-term load
forecasting for multiple sub-regions within a large region. We identify three
challenges for MRELF, including regional variation, contextual variation, and
temporal variation. To address them, we propose TriForecaster, a new framework
leveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning
(MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer
and Context-Time Specializer (CTSpecializer) layers, enabling dynamic
cooperation and specialization of expert models across regional, contextual,
and temporal dimensions. Based on evaluation on four real-world MRELF datasets
with varied granularity, TriForecaster outperforms state-of-the-art models by
achieving an average forecast error reduction of 22.4\%, thereby demonstrating
its flexibility and broad applicability. In particular, the deployment of
TriForecaster on the eForecaster platform in eastern China exemplifies its
practical utility, effectively providing city-level, short-term load forecasts
for 17 cities, supporting a population exceeding 110 million and daily
electricity usage over 100 gigawatt-hours.

</details>


### [132] [Bayesian autoregression to optimize temporal Matérn kernel Gaussian process hyperparameters](https://arxiv.org/abs/2508.09792)
*Wouter M. Kouw*

Main category: cs.LG

TL;DR: 提出了一种优化Matérn核时间高斯过程的方法，基于递归贝叶斯估计，性能优于边际似然最大化和哈密顿蒙特卡洛采样。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在概率数值学中很重要，但现有方法（如边际似然最大化或哈密顿蒙特卡洛采样）在运行时和精度上存在不足。

Method: 将优化问题转化为自回归模型参数的递归贝叶斯估计过程。

Result: 在运行时和均方根误差上优于边际似然最大化和哈密顿蒙特卡洛采样。

Conclusion: 该方法为高斯过程超参数优化提供了更高效和精确的解决方案。

Abstract: Gaussian processes are important models in the field of probabilistic
numerics. We present a procedure for optimizing Mat\'ern kernel temporal
Gaussian processes with respect to the kernel covariance function's
hyperparameters. It is based on casting the optimization problem as a recursive
Bayesian estimation procedure for the parameters of an autoregressive model. We
demonstrate that the proposed procedure outperforms maximizing the marginal
likelihood as well as Hamiltonian Monte Carlo sampling, both in terms of
runtime and ultimate root mean square error in Gaussian process regression.

</details>


### [133] [Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques](https://arxiv.org/abs/2508.09810)
*Qi Gan,Stephan Clémençon,Mounîm A. El-Yacoubi,Sao Mai Nguyen,Eric Fenaux,Ons Jelassi*

Main category: cs.LG

TL;DR: 该研究利用机器学习模型分析跳远比赛中的生物力学特征，发现速度和特定技术特征对顶级表现至关重要，男女运动员的关键特征有所不同。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以明确分析生物力学特征与运动员表现的关系，现代数据分析和机器学习为此提供了新工具。

Method: 使用分位数回归建模生物力学特征与有效距离的关系，并结合SHAP、PDP和ICE图解释模型。

Result: 男性运动员的关键特征是起跳前支撑腿膝盖角度（>169°），女性运动员的关键特征是着陆姿势和助跑技术。

Conclusion: 研究为分析特征对运动表现的影响提供了框架，特别关注顶级表现事件。

Abstract: Biomechanical features have become important indicators for evaluating
athletes' techniques. Traditionally, experts propose significant features and
evaluate them using physics equations. However, the complexity of the human
body and its movements makes it challenging to explicitly analyze the
relationships between some features and athletes' final performance. With
advancements in modern machine learning and statistics, data analytics methods
have gained increasing importance in sports analytics. In this study, we
leverage machine learning models to analyze expert-proposed biomechanical
features from the finals of long jump competitions in the World Championships.
The objectives of the analysis include identifying the most important features
contributing to top-performing jumps and exploring the combined effects of
these key features. Using quantile regression, we model the relationship
between the biomechanical feature set and the target variable (effective
distance), with a particular focus on elite-level jumps. To interpret the
model, we apply SHapley Additive exPlanations (SHAP) alongside Partial
Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The
findings reveal that, beyond the well-documented velocity-related features,
specific technical aspects also play a pivotal role. For male athletes, the
angle of the knee of the supporting leg before take-off is identified as a key
factor for achieving top 10% performance in our dataset, with angles greater
than 169{\deg}contributing significantly to jump performance. In contrast, for
female athletes, the landing pose and approach step technique emerge as the
most critical features influencing top 10% performances, alongside velocity.
This study establishes a framework for analyzing the impact of various features
on athletic performance, with a particular emphasis on top-performing events.

</details>


### [134] [RankList -- A Listwise Preference Learning Framework for Predicting Subjective Preferences](https://arxiv.org/abs/2508.09826)
*Abinay Reddy Naini,Fernando Diaz,Carlos Busso*

Main category: cs.LG

TL;DR: 论文提出RankList，一种新颖的列表式偏好学习框架，通过建模局部和非局部排序约束，提升全局排序一致性，并在多个任务中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决现有成对偏好学习框架（如RankNet）在全局排序一致性上的局限性。

Method: 提出RankList框架，结合概率模型和log-sum-exp近似优化训练效率，并引入跳过式比较增强全局排序能力。

Result: 在多个基准数据集（如MSP-Podcast、IEMOCAP等）上，RankList在Kendall's Tau和排序准确率上表现优于基线方法。

Conclusion: RankList为主观学习任务提供了一种统一且可扩展的偏好建模方法，具有广泛适用性。

Abstract: Preference learning has gained significant attention in tasks involving
subjective human judgments, such as \emph{speech emotion recognition} (SER) and
image aesthetic assessment. While pairwise frameworks such as RankNet offer
robust modeling of relative preferences, they are inherently limited to local
comparisons and struggle to capture global ranking consistency. To address
these limitations, we propose RankList, a novel listwise preference learning
framework that generalizes RankNet to structured list-level supervision. Our
formulation explicitly models local and non-local ranking constraints within a
probabilistic framework. The paper introduces a log-sum-exp approximation to
improve training efficiency. We further extend RankList with skip-wise
comparisons, enabling progressive exposure to complex list structures and
enhancing global ranking fidelity. Extensive experiments demonstrate the
superiority of our method across diverse modalities. On benchmark SER datasets
(MSP-Podcast, IEMOCAP, BIIC Podcast), RankList achieves consistent improvements
in Kendall's Tau and ranking accuracy compared to standard listwise baselines.
We also validate our approach on aesthetic image ranking using the Artistic
Image Aesthetics dataset, highlighting its broad applicability. Through
ablation and cross-domain studies, we show that RankList not only improves
in-domain ranking but also generalizes better across datasets. Our framework
offers a unified, extensible approach for modeling ordered preferences in
subjective learning scenarios.

</details>


### [135] [FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness](https://arxiv.org/abs/2508.09866)
*Siyuan Wen,Meng Zhang,Yang Yang,Ningning Ding*

Main category: cs.LG

TL;DR: FedShard是一种联邦学习遗忘算法，旨在同时保证效率和性能公平性，解决现有研究中未充分探索的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘研究主要关注效率和有效性，而忽略了效率和性能公平性，FedShard旨在填补这一空白。

Method: FedShard自适应地解决收敛、遗忘效率和公平性之间的困境，并提出两种新指标量化公平性。

Result: FedShard在遗忘性能和效率上均表现出公平性，加速遗忘过程1.3-6.2倍，优于现有方法。

Conclusion: FedShard有效解决了联邦遗忘中的公平性问题，同时提升了效率和性能。

Abstract: To protect clients' right to be forgotten in federated learning, federated
unlearning aims to remove the data contribution of leaving clients from the
global learned model. While current studies mainly focused on enhancing
unlearning efficiency and effectiveness, the crucial aspects of efficiency
fairness and performance fairness among decentralized clients during unlearning
have remained largely unexplored. In this study, we introduce FedShard, the
first federated unlearning algorithm designed to concurrently guarantee both
efficiency fairness and performance fairness. FedShard adaptively addresses the
challenges introduced by dilemmas among convergence, unlearning efficiency, and
unlearning fairness. Furthermore, we propose two novel metrics to
quantitatively assess the fairness of unlearning algorithms, which we prove to
satisfy well-known properties in other existing fairness measurements. Our
theoretical analysis and numerical evaluation validate FedShard's fairness in
terms of both unlearning performance and efficiency. We demonstrate that
FedShard mitigates unfairness risks such as cascaded leaving and poisoning
attacks and realizes more balanced unlearning costs among clients. Experimental
results indicate that FedShard accelerates the data unlearning process 1.3-6.2
times faster than retraining from scratch and 4.9 times faster than the
state-of-the-art exact unlearning methods.

</details>


### [136] [Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?](https://arxiv.org/abs/2508.09888)
*Viacheslav Barkov,Jonas Schmidinger,Robin Gebbers,Martin Atzmueller*

Main category: cs.LG

TL;DR: 现代人工神经网络（ANN）在土壤性质预测中超越传统方法，TabPFN表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究现代ANN在土壤光谱数据预测中的适用性，挑战传统机器学习的主导地位。

Method: 通过31个数据集评估多种ANN架构，包括MLP、Transformer和TabPFN等。

Result: 现代ANN在多数任务中优于传统方法，TabPFN表现最稳健。

Conclusion: 推荐现代ANN（尤其是TabPFN）作为土壤预测建模的新标准工具。

Abstract: In the field of pedometrics, tabular machine learning is the predominant
method for predicting soil properties from remote and proximal soil sensing
data, forming a central component of digital soil mapping. At the field-scale,
this predictive soil modeling (PSM) task is typically constrained by small
training sample sizes and high feature-to-sample ratios in soil spectroscopy.
Traditionally, these conditions have proven challenging for conventional deep
learning methods. Classical machine learning algorithms, particularly
tree-based models like Random Forest and linear models such as Partial Least
Squares Regression, have long been the default choice for field-scale PSM.
Recent advances in artificial neural networks (ANN) for tabular data challenge
this view, yet their suitability for field-scale PSM has not been proven. We
introduce a comprehensive benchmark that evaluates state-of-the-art ANN
architectures, including the latest multilayer perceptron (MLP)-based models
(TabM, RealMLP), attention-based transformer variants (FT-Transformer,
ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR,
ModernNCA), and an in-context learning foundation model (TabPFN). Our
evaluation encompasses 31 field- and farm-scale datasets containing 30 to 460
samples and three critical soil properties: soil organic matter or soil organic
carbon, pH, and clay content. Our results reveal that modern ANNs consistently
outperform classical methods on the majority of tasks, demonstrating that deep
learning has matured sufficiently to overcome the long-standing dominance of
classical machine learning for PSM. Notably, TabPFN delivers the strongest
overall performance, showing robustness across varying conditions. We therefore
recommend the adoption of modern ANNs for field-scale PSM and propose TabPFN as
the new default choice in the toolkit of every pedometrician.

</details>


### [137] [Prototype-Guided Diffusion: Visual Conditioning without External Memory](https://arxiv.org/abs/2508.09922)
*Bilal Faye,Hanane Azzag,Mustapha Lebbah*

Main category: cs.LG

TL;DR: PDM（Prototype Diffusion Model）通过将原型学习直接集成到扩散过程中，无需外部存储，实现了高效且自适应的视觉条件生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在高质量图像生成中表现出色，但计算成本高，尤其是迭代去噪过程。现有方法如RDM依赖外部存储和检索，成本高且缺乏适应性。

Method: PDM通过对比学习从干净图像特征中构建动态紧凑视觉原型，用于指导去噪步骤，无需外部存储。

Result: 实验表明，PDM在保持高质量生成的同时，减少了计算和存储开销。

Conclusion: PDM为扩散模型提供了一种可扩展的替代方案，避免了检索方法的缺点。

Abstract: Diffusion models have emerged as a leading framework for high-quality image
generation, offering stable training and strong performance across diverse
domains. However, they remain computationally intensive, particularly during
the iterative denoising process. Latent-space models like Stable Diffusion
alleviate some of this cost by operating in compressed representations, though
at the expense of fine-grained detail. More recent approaches such as
Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning
denoising on similar examples retrieved from large external memory banks. While
effective, these methods introduce drawbacks: they require costly storage and
retrieval infrastructure, depend on static vision-language models like CLIP for
similarity, and lack adaptability during training. We propose the Prototype
Diffusion Model (PDM), a method that integrates prototype learning directly
into the diffusion process for efficient and adaptive visual conditioning -
without external memory. Instead of retrieving reference samples, PDM
constructs a dynamic set of compact visual prototypes from clean image features
using contrastive learning. These prototypes guide the denoising steps by
aligning noisy representations with semantically relevant visual patterns,
enabling efficient generation with strong semantic grounding. Experiments show
that PDM maintains high generation quality while reducing computational and
storage overhead, offering a scalable alternative to retrieval-based
conditioning in diffusion models.

</details>


### [138] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: 论文提出了一种通过噪声超网络替代奖励引导的测试时噪声优化的方法，以减少计算开销，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放虽然提升了模型性能，但增加了计算时间，限制了其实际应用。

Method: 提出噪声超网络，通过调制初始输入噪声，学习奖励倾斜的分布，以减少计算成本。

Result: 方法在显著降低计算成本的同时，恢复了大部分测试时优化的质量提升。

Conclusion: 噪声超网络是一种高效且实用的解决方案，能够在保持性能的同时减少计算开销。

Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in
Large Language Models (LLMs) (e.g. reasoning models) and in generative vision
models, allowing models to allocate additional computation during inference to
effectively tackle increasingly complex problems. Despite the improvements of
this approach, an important limitation emerges: the substantial increase in
computation time makes the process slow and impractical for many applications.
Given the success of this paradigm and its growing usage, we seek to preserve
its benefits while eschewing the inference overhead. In this work we propose
one solution to the critical problem of integrating test-time scaling knowledge
into a model during post-training. Specifically, we replace reward guided
test-time noise optimization in diffusion models with a Noise Hypernetwork that
modulates initial input noise. We propose a theoretically grounded framework
for learning this reward-tilted distribution for distilled generators, through
a tractable noise-space objective that maintains fidelity to the base model
while optimizing for desired characteristics. We show that our approach
recovers a substantial portion of the quality gains from explicit test-time
optimization at a fraction of the computational cost. Code is available at
https://github.com/ExplainableML/HyperNoise

</details>


### [139] [Dynamic Mixture-of-Experts for Incremental Graph Learning](https://arxiv.org/abs/2508.09974)
*Lecheng Kong,Theodore Vasiloudis,Seongjun Yun,Han Xie,Xiang Song*

Main category: cs.LG

TL;DR: 论文提出动态专家混合（DyMoE）方法解决图增量学习中的灾难性遗忘问题，通过定制正则化损失和稀疏MoE提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统图机器学习方法在增量学习中存在灾难性遗忘问题，且未考虑不同时间点知识对新任务的贡献差异。

Method: 提出DyMoE GNN层，新增专家网络处理新数据块，设计定制正则化损失，并引入稀疏MoE以减少计算成本。

Result: 模型在类增量学习上相对基线准确率提升4.92%。

Conclusion: DyMoE方法有效解决了灾难性遗忘问题，提升了增量学习性能。

Abstract: Graph incremental learning is a learning paradigm that aims to adapt trained
models to continuously incremented graphs and data over time without the need
for retraining on the full dataset. However, regular graph machine learning
methods suffer from catastrophic forgetting when applied to incremental
learning settings, where previously learned knowledge is overridden by new
knowledge. Previous approaches have tried to address this by treating the
previously trained model as an inseparable unit and using techniques to
maintain old behaviors while learning new knowledge. These approaches, however,
do not account for the fact that previously acquired knowledge at different
timestamps contributes differently to learning new tasks. Some prior patterns
can be transferred to help learn new data, while others may deviate from the
new data distribution and be detrimental. To address this, we propose a dynamic
mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a
DyMoE GNN layer adds new expert networks specialized in modeling the incoming
data blocks. We design a customized regularization loss that utilizes data
sequence information so existing experts can maintain their ability to solve
old tasks while helping the new expert learn the new data effectively. As the
number of data blocks grows over time, the computational cost of the full
mixture-of-experts (MoE) model increases. To address this, we introduce a
sparse MoE approach, where only the top-$k$ most relevant experts make
predictions, significantly reducing the computation time. Our model achieved
4.92\% relative accuracy increase compared to the best baselines on class
incremental learning, showing the model's exceptional power.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [140] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit是一种将值函数初始化（VFI）扩展到深度强化学习（DRL）的方法，通过重用先前任务中的紧凑表格Q值作为可转移知识库，提高了早期学习效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在DRL中扩展VFI面临状态-动作空间的连续性、神经网络近似噪声以及存储所有过去模型的不可行性等挑战。

Method: DQInit通过基于已知度的机制将转移值软集成到未探索区域，并逐步转向代理的学习估计，避免了固定时间衰减的限制。

Result: 实验表明，DQInit在多个连续控制任务中显著提高了早期学习效率、稳定性和整体性能。

Conclusion: DQInit为DRL中的知识转移提供了新视角，仅依赖值估计而非策略或演示，结合了跳启RL和策略蒸馏的优势并避免了其缺点。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [141] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: 论文介绍了Othello AI Arena，一个评估AI系统在有限时间内适应新环境能力的基准框架，旨在填补现有AI测试在评估灵活性和泛化能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准主要关注固定环境下的性能优化，缺乏对系统在规则或结构变化时适应能力的评估。

Method: 通过Othello AI Arena框架，要求AI系统在60秒内分析新Othello棋盘配置并生成定制策略，分离元级智能与任务级策略性能评估。

Result: 初步测试显示不同适应方法，如快速参数调整或通过模拟学习环境模型。

Conclusion: Othello AI Arena是评估AI快速适应能力的有价值工具，兼具教育和研究用途。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [142] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: 提出了一种基于大语言模型和多智能体协作的自动化多模态评估框架，解决了当前评估方法的高成本、标准不一致和主观偏见问题。


<details>
  <summary>Details</summary>
Motivation: 随着移动智能助手技术的快速发展，多模态AI助手成为日常用户交互的重要接口，但现有评估方法存在高人工成本、标准不一致和主观偏见等挑战。

Method: 采用三层智能体架构（交互评估、语义验证和体验决策智能体），并在Qwen3-8B模型上进行监督微调。

Result: 在八个主要智能体上的实验表明，该框架能有效预测用户满意度和识别生成缺陷，评估匹配准确率显著。

Conclusion: 该框架为多模态AI助手的自动化评估提供了高效、一致的解决方案。

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [143] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: 提出EvoCurr框架，通过动态调整难度的课程学习提升LLM在复杂决策任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂问题中因缺乏结构化指导而表现不佳的问题。

Method: 使用课程生成LLM动态调整问题难度，配合代码生成LLM逐步学习。

Result: 实验显示任务成功率和效率显著提升。

Conclusion: LLM驱动的课程学习在复杂领域有潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [144] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 论文提出了一种方法，将SHAP值的不确定性分解为偶然性、认知性和纠缠性成分，结合Dempster-Shafer证据理论和Dirichlet过程采样，验证了方法的有效性，并指出高SHAP值特征不一定最稳定。


<details>
  <summary>Details</summary>
Motivation: SHAP值通常被视为点估计，忽略了预测模型和数据中的不确定性，尤其是在高风险领域（如医疗分析）中，这种不确定性可能影响决策的可靠性。

Method: 结合Dempster-Shafer证据理论和Dirichlet过程采样，分解SHAP值的不确定性为偶然性、认知性和纠缠性成分。

Result: 实验表明，高SHAP值特征不一定最稳定，认知性不确定性可以通过更好的数据和模型开发技术减少。

Conclusion: 该方法提供了对SHAP解释可靠性和可解释性的更全面理解，有助于高风险应用中的决策和模型优化。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [145] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: MEML-GRPO通过多专家互学机制和多样化提示，解决了RLVR中的奖励稀疏问题，显著提升了LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 标准RLVR在奖励稀疏时无法提供学习信号，尤其是在复杂任务中。

Method: 提出MEML-GRPO框架，利用多样化专家提示生成更多响应，并通过专家间互学机制共享知识。

Result: 在多个推理基准测试中，MEML-GRPO平均性能提升4.89%（Qwen）和11.33%（Llama）。

Conclusion: MEML-GRPO有效克服了传统RLVR的核心限制，显著提升了模型性能。

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [146] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: 论文提出了一种无监督去偏框架UDA，通过动态调整Elo评分系统减少评估中的偏好偏差，提升模型评估的一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）成对评估存在偏好偏差，导致不同评估者的排名不一致且失真。

Method: 提出UDA框架，通过紧凑神经网络动态调整Elo评分系统的K因子和胜率，以最小化评估者之间的评分差异为目标。

Result: UDA显著减少了评估者评分的标准差（最高63.4%），并提升与人类判断的平均相关性（24.7%）。

Conclusion: UDA通过无监督方式减少系统偏差，提升评估的稳健性和可靠性，使低质量评估者表现接近高质量评估者。

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [147] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: 论文提出了PacifAIst基准，用于评估大型语言模型在目标冲突情境下的行为对齐，发现模型表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）自主性增强，需从内容安全转向行为对齐评估，现有基准无法系统测试目标冲突下的决策。

Method: 引入PacifAIst基准，包含700个场景，围绕Existential Prioritization（EP）分类测试自我保存、资源冲突等行为。

Result: 评估8个主流LLMs，Gemini 2.5 Flash表现最佳（P-Score 90.31%），GPT-5最低（79.49%），模型在子类别中表现差异显著。

Conclusion: 需标准化工具如PacifAIst以测量和缓解目标冲突风险，确保AI行为优先级符合人类安全。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [148] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: POL（公共观察逻辑）是一种用于推理基于公共观察的知识更新的逻辑，其可满足性问题被证明是2EXPTIME完全的。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统中基于观察的知识更新，特别是在认知规划中的应用。

Method: 提出POL，一种基于公共观察的知识更新逻辑，利用带有预期观察的克里普克模型。

Result: 证明了POL的可满足性问题是2EXPTIME完全的。

Conclusion: POL为多智能体系统中的知识更新提供了一种有效的逻辑框架，但其计算复杂度较高。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [149] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: VIPCGRL是一种结合文本、关卡和草图的多模态强化学习框架，通过对比学习和嵌入对齐提升人机协作内容生成的人类相似性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在人类中心行为上表现不足，限制了AI工具在实际设计工作流中的应用。

Method: 提出VIPCGRL框架，整合文本、关卡和草图三种模态，通过四重对比学习和嵌入相似性奖励对齐策略。

Result: VIPCGRL在人类相似性上优于现有基线，定量指标和人类评估均验证其效果。

Conclusion: VIPCGRL为人类对齐AI提供了有效解决方案，增强了AI在协作内容生成中的实用性。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [150] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 论文提出了一种动态监督和操纵机制，构建了一个多代理系统（MAS）架构，通过引入Guard Agent验证和修正推理过程，显著提升了系统的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理在依赖多个外部工具时，面临上下文扩展和噪声输出的挑战，需要增强系统稳定性。

Method: 在AWorld框架中，Execution Agent在关键步骤调用Guard Agent验证和修正推理过程，减少噪声导致的错误。

Result: 在GAIA测试数据集上的实验表明，动态操纵机制显著提升了解决方案的有效性和稳定性，优于单代理系统和标准工具增强系统。

Conclusion: 动态MAS系统在GAIA排行榜上名列前茅，证明了协作代理角色在构建可靠智能系统中的实用价值。

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [151] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体框架，结合知识图谱和检索增强生成，用于解决法规合规问答中的精确性和可验证性问题。


<details>
  <summary>Details</summary>
Motivation: 法规合规问答需要精确、可验证的信息和领域专业知识，这对大型语言模型提出了挑战。

Method: 通过构建和维护无本体知识图谱，提取SPO三元组，嵌入并存储在向量数据库中，结合检索增强生成进行问答。

Result: 混合系统在复杂法规查询中优于传统方法，确保事实正确性、可追溯性，并通过子图可视化增强理解。

Conclusion: 该系统为合规驱动和审计应用提供了坚实基础。

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [152] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 研究评估了四种大语言模型（LLM）在数学任务中的准确性，发现推理增强的OpenAI o1模型表现最佳，双代理配置显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在数学教育中的准确性，为AI驱动的教学和评估提供可靠反馈。

Method: 通过构建挑战性数学任务，分析LLM的答案准确性和步骤错误，测试单代理和双代理配置。

Result: OpenAI o1模型表现最佳，双代理配置显著改善性能，程序性错误最常见。

Conclusion: 研究为提升LLM性能和数学教育中的AI应用提供了有效策略。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [153] [Based AI improves human decision-making but reduces trust](https://arxiv.org/abs/2508.09297)
*Shiyang Lai,Junsol Kim,Nadav Kunievsky,Yujin Potter,James Evans*

Main category: cs.HC

TL;DR: 研究发现，带有文化偏见的AI助手能提升人类决策表现，增加参与度并减少评估偏见，但会降低信任度。


<details>
  <summary>Details</summary>
Motivation: 探讨AI中立性是否抑制人类认知参与，以及文化偏见AI是否能改善决策。

Method: 对2,500名参与者进行随机试验，测试不同政治倾向的GPT-4o变体在信息评估任务中的影响。

Result: 党派性AI助手提升人类表现和参与度，减少偏见，但对信任度有负面影响。

Conclusion: 战略性地整合多样化文化偏见可能改善人类决策，挑战了AI中立性的传统观念。

Abstract: Current AI systems minimize risk by enforcing ideological neutrality, yet
this may introduce automation bias by suppressing cognitive engagement in human
decision-making. We conducted randomized trials with 2,500 participants to test
whether culturally biased AI enhances human decision-making. Participants
interacted with politically diverse GPT-4o variants on information evaluation
tasks. Partisan AI assistants enhanced human performance, increased engagement,
and reduced evaluative bias compared to non-biased counterparts, with amplified
benefits when participants encountered opposing views. These gains carried a
trust penalty: participants underappreciated biased AI and overcredited neutral
systems. Exposing participants to two AIs whose biases flanked human
perspectives closed the perception-performance gap. These findings complicate
conventional wisdom about AI neutrality, suggesting that strategic integration
of diverse cultural biases may foster improved and resilient human
decision-making.

</details>


### [154] [Micro-Health Interventions: Exploring Design Strategies for 1-Minute Interventions as a Gateway to Healthy Habits](https://arxiv.org/abs/2508.09312)
*Zahra Hassanzadeh,David Haag,Lydia Chilton,Jan Smeddinck,Norman Farb,Joseph Jay Williams*

Main category: cs.HC

TL;DR: 一分钟行为干预看似短暂，但研究表明，设计得当的提示可以成为健康习惯的入口。


<details>
  <summary>Details</summary>
Motivation: 探讨超简短提示是否能在日常生活中促成有意义的行动。

Method: 通过两项研究：一项探索性研究分析参与者对四种领域（身体活动、饮食、屏幕使用、心理健康）的提示反应；另一项14天的对照研究比较两种提示设计（立即行动与反思优先）。

Result: 参与者未注意到提示结构差异，但对及时、相关或情感支持的提示反应积极。内容契合度、语气和即时准备状态影响参与度。

Conclusion: 一分钟干预若设计得当，可在当下提供帮助，成为健康习惯的有效切入点。

Abstract: One-minute behavior change interventions might seem too brief to matter.
Could something so short really help people build healthier routines? This work
explores this question through two studies examining how ultra-brief prompts
might encourage meaningful actions in daily life. In a formative study, we
explored how participants engaged with one-minute prompts across four domains:
physical activity, eating, screen use, and mental well-being. This revealed two
common design approaches: Immediate Action prompts (simple, directive tasks)
and Reflection-First prompts (self-awareness before action). We then conducted
a 14-day, within-subjects study comparing these two flows with 28 participants.
Surprisingly, most participants did not notice differences in structure -- but
responded positively when prompts felt timely, relevant, or emotionally
supportive. Engagement was not shaped by flow type, but by content fit, tone,
and momentary readiness. Participants also co-designed messages, favoring those
with step-by-step guidance, personal meaning, or sensory detail. These results
suggest that one-minute interventions, while easily dismissed, may serve as
meaningful gateways into healthier routines -- if designed to feel helpful in
the moment.

</details>


### [155] [Affordances of Sketched Notations for Multimodal UI Design and Development Tools](https://arxiv.org/abs/2508.09342)
*Sam H. Ross,Yunseo Lee,Coco K. Lee,Jayne Everson,R. Benjamin Shapiro*

Main category: cs.HC

TL;DR: 论文探讨了多模态UI设计工具中符号（notations）的设计与评估，通过分析两种UI草图符号（FixedSketch和FlexiSketch），发现FlexiSketch更具创造性和低认知负担，但需结合现代AI方法以实现人本设计。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为多模态UI设计工具设计直观且易用的符号，通过分析训练数据集作为符号规范，以提升工具的可用性。

Method: 采用Cognitive Dimensions of Notations框架分析两种UI草图符号：FixedSketch（基于现有数据集规则）和FlexiSketch（基于用户自由草图）。

Result: FlexiSketch符号支持更高的创造性和低认知负担，但需结合上下文理解技术（如transformer和强化学习）以实现准确识别。

Conclusion: 未来多模态设计工具需采用现代AI技术（如transformer和强化学习），以支持用户丰富的上下文表达和修正。

Abstract: Multimodal UI design and development tools that interpret sketches or natural
language descriptions of UIs inherently have notations: the inputs they can
understand. In AI-based systems, notations are implicitly defined by the data
used to train these systems. In order to create usable and intuitive notations
for interactive design systems, we must regard, design, and evaluate these
training datasets as notation specifications. To better understand the design
space of notational possibilities for future design tools, we use the Cognitive
Dimensions of Notations framework to analyze two possible notations for UI
sketching. The first notation is the sketching rules for an existing UI sketch
dataset, and the second notation is the set of sketches generated by
participants in this study, where individuals sketched UIs without imposed
representational rules. We imagine two systems, FixedSketch and FlexiSketch,
built with each notation respectively, in order to understand the differential
affordances of, and potential design requirements for, systems. We find that
participants' sketches were composed of element-level notations that are
ambiguous in isolation but are interpretable in context within whole designs.
For many cognitive dimensions, the FlexiSketch notation supports greater
intuitive creative expression and affords lower cognitive effort than the
FixedSketch notation, but cannot be supported with prevailing, element-based
approaches to UI sketch recognition. We argue that for future multimodal design
tools to be truly human-centered, they must adopt contemporary AI methods,
including transformer-based and human-in-the-loop, reinforcement learning
techniques to understand users' context-rich expressive notations and
corrections.

</details>


### [156] [Virtual Reality User Interface Design: Best Practices and Implementation](https://arxiv.org/abs/2508.09358)
*Esin Mehmedova,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: 本文通过系统文献综述提出了一套统一的VR UI设计指南，并通过开发应用FlUId和用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: VR领域缺乏统一的UI设计指南，影响了用户体验的沉浸感、可用性和舒适性。

Method: 通过系统文献综述识别最佳实践，开发VR应用FlUId展示设计原则，并进行用户研究验证。

Result: 提出了一套完整的VR UI设计指南，并通过用户研究验证了其有效性。

Conclusion: 研究填补了VR UI设计指南的空白，为设计师和开发者提供了实用建议。

Abstract: Designing effective user interfaces (UIs) for virtual reality (VR) is
essential to enhance user immersion, usability, comfort, and accessibility in
virtual environments. Despite the growing adoption of VR across domains such as
education, healthcare, gaming, and rehabilitation, there is a noticeable lack
of unified and comprehensive design guidelines for VR UI design. To address
this gap, we conducted a systematic literature review to identify existing best
practices and propose complete and unified guidelines for UI development in VR.
  Building on these insights, this research proposes a set of best practices to
guide the creation of more effective VR interfaces. To demonstrate and validate
these practices, we developed a VR application called \textit{FlUId} that
showcases both good and bad UI design principles for direct comparison. A user
study was conducted to evaluate the impact of the proposed guidelines. The
findings aim to bridge the gap between theory and practice, offering concrete
recommendations for VR designers and developers.

</details>


### [157] [VIVA: Virtual Healthcare Interactions Using Visual Analytics, With Controllability Through Configuration](https://arxiv.org/abs/2508.09386)
*Jürgen Bernard,Mara Solen,Helen Novak Lauscher,Kurtis Stewart,Kendall Ho,Tamara Munzner*

Main category: cs.HC

TL;DR: HLBC在COVID-19疫情期间快速整合医生到虚拟医疗服务中，并开发了可视化分析工具VIVA以优化服务。通过案例研究验证设计，并提出了可控性配置模型。


<details>
  <summary>Details</summary>
Motivation: 在疫情期间优化虚拟医疗服务，提升患者满意度和医疗系统效率。

Method: 设计并实现可视化分析工具VIVA，抽象数据和分析任务，提出Scan, Act, Adapt工作流，并通过案例研究验证。

Result: VIVA设计得到验证，并提出了可控性配置模型以支持设计研究。

Conclusion: VIVA工具通过配置模型在硬编码和完全交互性之间找到平衡，提升了虚拟医疗服务的分析效率。

Abstract: At the beginning of the COVID-19 pandemic, HealthLink BC (HLBC) rapidly
integrated physicians into the triage process of their virtual healthcare
service to improve patient outcomes and satisfaction with this service and
preserve health care system capacity. We present the design and implementation
of a visual analytics tool, VIVA (Virtual healthcare Interactions using Visual
Analytics), to support HLBC in analysing various forms of usage data from the
service. We abstract HLBC's data and data analysis tasks, which we use to
inform our design of VIVA. We also present the interactive workflow abstraction
of Scan, Act, Adapt. We validate VIVA's design through three case studies with
stakeholder domain experts. We also propose the Controllability Through
Configuration model to conduct and analyze design studies, and discuss
architectural evolution of VIVA through that lens. It articulates
configuration, both that specified by a developer or technical power user and
that constructed automatically through log data from previous interactive
sessions, as a bridge between the rigidity of hardwired programming and the
time-consuming implementation of full end-user interactivity.
  Availability: Supplemental materials at https://osf.io/wv38n

</details>


### [158] [Realtime Multimodal Emotion Estimation using Behavioral and Neurophysiological Data](https://arxiv.org/abs/2508.09402)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Main category: cs.HC

TL;DR: 提出了一种多模态情绪估计系统，结合生理和行为数据，为神经多样性用户提供实时情绪跟踪和个性化支持。


<details>
  <summary>Details</summary>
Motivation: 支持自闭症谱系障碍（ASD）等神经多样性用户在情绪识别和表达方面的需求，推动包容性情绪技术的发展。

Method: 结合EEG、ECG、BVP、GSR等生理数据及面部表情、语音等行为数据，构建实时情绪估计系统。

Result: 系统能实时跟踪情绪状态，适用于情绪教育、神经适应性反馈和交互支持。

Conclusion: 该系统为神经多样性用户提供了有效的情绪监测和个性化支持工具。

Abstract: Many individuals especially those with autism spectrum disorder (ASD),
alexithymia, or other neurodivergent profiles face challenges in recognizing,
expressing, or interpreting emotions. To support more inclusive and
personalized emotion technologies, we present a real-time multimodal emotion
estimation system that combines neurophysiological EEG, ECG, blood volume pulse
(BVP), and galvanic skin response (GSR/EDA) and behavioral modalities (facial
expressions, and speech) in a unified arousal-valence 2D interface to track
moment-to-moment emotional states. This architecture enables interpretable,
user-specific analysis and supports applications in emotion education,
neuroadaptive feedback, and interaction support for neurodiverse users. Two
demonstration scenarios illustrate its application: (1) passive media viewing
(2D or VR videos) reveals cortical and autonomic responses to affective
content, and (2) semi-scripted conversations with a facilitator or virtual
agent capture real-time facial and vocal expressions. These tasks enable
controlled and naturalistic emotion monitoring, making the system well-suited
for personalized feedback and neurodiversity-informed interaction design.

</details>


### [159] [Fulfillment of the Work Games: Warehouse Workers' Experiences with Algorithmic Management](https://arxiv.org/abs/2508.09438)
*EunJeong Cheon,Ingrid Erickson*

Main category: cs.HC

TL;DR: 研究探讨亚马逊物流中心工人对算法管理的反应，揭示其抵抗行为与“工作游戏”的联系，扩展对算法控制机制的批判。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注平台零工工人，而对传统行业工人体验了解不足，需深入探讨算法管理对工人的影响。

Method: 基于两年民族志研究，分析物流中心工人对算法干预、生产率要求及量化管理的反应。

Result: 工人通过抵抗行为和“工作游戏”策略应对算法管理，揭示了算法控制机制的复杂性。

Conclusion: 理解工人对算法管理的抵抗与顺从，有助于批判和瓦解这些社会技术劳动系统的经济政治根源。

Abstract: The introduction of algorithms into a large number of industries has already
restructured the landscape of work and threatens to continue. While a growing
body of CSCW research centered on the future of work has begun to document
these shifts, relatively little is known about workers' experiences beyond
those of platform-mediated gig workers. In this paper, we turn to a traditional
work sector, Amazon fulfillment centers (FC), to deepen our field's empirical
examination of algorithmic management. Drawing on two years of ethnographic
research, we show how FC workers react to managers' interventions, imposed
productivity rates, and quantified objectification when subjected to
labor-tracking systems in their physical work environments. Situating FC
workers' resistance to algorithmic systems and metrics within the current CSCW
literature allows us to explicate and link the nuanced practices of FC workers
to the larger discourse of algorithmic control mechanisms. In addition, we show
how FC workers' resistance practices are emblematic of 'work games'--a
long-studied means by which workers agentically configure ("trick") their
engagement within work systems. We argue that gaining a more nuanced
understanding of workers' resistance and consent in relation to algorithmic
management expands our ability to critique and potentially disassemble the
economic and political forces at the root of these sociotechnical labor
systems.

</details>


### [160] [Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis](https://arxiv.org/abs/2508.09458)
*Xi Long,Christy Boscardin,Lauren A. Maggio,Joseph A. Costello,Ralph Gonzales,Rasmyah Hammoudeh,Ki Lai,Yoon Soo Park,Brian C. Gin*

Main category: cs.HC

TL;DR: AI辅助数据提取在健康职业教育文献综述中表现出高效性，尤其在明确问题时与人类一致性高，但在主观解释问题上表现较差。AI错误率低，人类错误率更高。


<details>
  <summary>Details</summary>
Motivation: 解决文献综述中数据提取的高劳动强度问题，同时探讨AI在提取中的准确性和可靠性。

Method: 使用大型语言模型（LLMs）开发提取平台，比较AI与人类在187篇文献和17个提取问题上的表现，测量一致性和错误率。

Result: AI在明确问题上与人类高度一致，主观问题上一致性较低；AI错误率（1.51%）低于人类（4.37%）。

Conclusion: AI可作为知识合成的透明、可靠工具，但需注意保留人类关键见解，重复提取可识别解释复杂性。

Abstract: Knowledge syntheses (literature reviews) are essential to health professions
education (HPE), consolidating findings to advance theory and practice.
However, they are labor-intensive, especially during data extraction.
Artificial Intelligence (AI)-assisted extraction promises efficiency but raises
concerns about accuracy, making it critical to distinguish AI 'hallucinations'
(fabricated content) from legitimate interpretive differences. We developed an
extraction platform using large language models (LLMs) to automate data
extraction and compared AI to human responses across 187 publications and 17
extraction questions from a published scoping review. AI-human, human-human,
and AI-AI consistencies were measured using interrater reliability
(categorical) and thematic similarity ratings (open-ended). Errors were
identified by comparing extracted responses to source publications. AI was
highly consistent with humans for concrete, explicitly stated questions (e.g.,
title, aims) and lower for questions requiring subjective interpretation or
absent in text (e.g., Kirkpatrick's outcomes, study rationale). Human-human
consistency was not higher than AI-human and showed the same question-dependent
variability. Discordant AI-human responses (769/3179 = 24.2%) were mostly due
to interpretive differences (18.3%); AI inaccuracies were rare (1.51%), while
humans were nearly three times more likely to state inaccuracies (4.37%).
Findings suggest AI accuracy depends more on interpretability than
hallucination. Repeating AI extraction can identify interpretive complexity or
ambiguity, refining processes before human review. AI can be a transparent,
trustworthy partner in knowledge synthesis, though caution is needed to
preserve critical human insights.

</details>


### [161] [Handows: A Palm-Based Interactive Multi-Window Management System in Virtual Reality](https://arxiv.org/abs/2508.09469)
*Jindu Wang,Ke Zhou,Haoyu Ren,Per Ola Kristensson,Xiang Li*

Main category: cs.HC

TL;DR: Handows是一种基于手掌的VR界面，通过智能手机式手势实现窗口管理，显著降低物理负担并提高效率。


<details>
  <summary>Details</summary>
Motivation: VR中的窗口管理因空间复杂性和物理需求而具有挑战性，需要更直观、低负担的交互方式。

Method: Handows结合人体工学设计和被动触觉，支持四种核心操作（选择、关闭、定位、缩放），并通过用户研究（N=15）与两种常见VR技术对比。

Result: Handows显著减少物理负担和头部移动，提高任务效率和交互精度。后续案例研究（N=8）验证了其在多任务场景中的实用性。

Conclusion: Handows展示了将移动设备隐喻融入身体中心界面的潜力，支持低负担且空间一致的VR交互。

Abstract: Window management in virtual reality (VR) remains a challenging task due to
the spatial complexity and physical demands of current interaction methods. We
introduce Handows, a palm-based interface that enables direct manipulation of
spatial windows through familiar smartphone-inspired gestures on the user's
non-dominant hand. Combining ergonomic layout design with body-centric input
and passive haptics, Handows supports four core operations: window selection,
closure, positioning, and scaling. We evaluate Handows in a user study (N=15)
against two common VR techniques (virtual hand and controller) across these
core window operations. Results show that Handows significantly reduces
physical effort and head movement while improving task efficiency and
interaction precision. A follow-up case study (N=8) demonstrates Handows'
usability in realistic multitasking scenarios, highlighting user-adapted
workflows and spontaneous layout strategies. Our findings suggest the potential
of embedding mobile-inspired metaphors into proprioceptive body-centric
interfaces to support low-effort and spatially coherent interaction in VR.

</details>


### [162] [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)
*Daniel Raffini,Agnese Macori,Lorenzo Porcaro,Tiziana Catarci,Marco Angelini*

Main category: cs.HC

TL;DR: 研究探讨了ChatGPT生成的伦理相关议论文的语言和修辞特征及其对读者的说服效果，发现其结构一致但风格单一，说服力有限，尤其在伦理问题上。


<details>
  <summary>Details</summary>
Motivation: 探究AI生成文本在伦理敏感话题上的说服效果及其对读者观点的影响。

Method: 通过62名参与者的用户研究及前后调查，分析AI生成文本的语言、修辞特征及其对意见改变的影响。

Result: ChatGPT能生成连贯的议论文，但说服力有限，伦理问题上的担忧甚至可能加剧。结果因话题而异。

Conclusion: 研究为AI在伦理敏感领域的说服效果提供了新见解，是未来研究的基础。

Abstract: This study examines the rhetorical and linguistic features of argumentative
texts generated by ChatGPT on ethically nuanced topics and investigates their
persuasive impact on human readers.Through a user study involving 62
participants and pre-post interaction surveys, the paper analyzes how exposure
to AI-generated arguments affects opinion change and user perception. A
linguistic and rhetorical analysis of the generated texts reveals a consistent
argumentative macrostructure, reliance on formulaic expressions, and limited
stylistic richness. While ChatGPT demonstrates proficiency in constructing
coherent argumentative texts, its persuasive efficacy appears constrained,
particularly on topics involving ethical issues.The study finds that while
participants often acknowledge the benefits highlighted by ChatGPT, ethical
concerns tend to persist or even intensify post-interaction. The results also
demonstrate a variation depending on the topic. These findings highlight new
insights on AI-generated persuasion in ethically sensitive domains and are a
basis for future research.

</details>


### [163] [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)
*Daniel Raffini,Agnese Macori,Marco Angelini,Tiziana Catarci*

Main category: cs.HC

TL;DR: 研究探讨了ChatGPT、Gemini和Claude生成故事中的性别叙事偏见，基于Propp角色分类和Freytag叙事结构设计提示，通过细读分析揭示隐含偏见的持续性。


<details>
  <summary>Details</summary>
Motivation: 评估AI生成故事中的性别偏见，强调多层面偏见分析的重要性。

Method: 基于Propp和Freytag理论设计提示，通过细读分析故事的角色性别分布、描述、行动及情节发展。

Result: 发现生成故事中存在隐含性别偏见，需多层面评估。

Conclusion: 研究强调需采用解释性方法多层面评估AI生成内容中的偏见。

Abstract: The paper explores the study of gender-based narrative biases in stories
generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's
character classifications and Freytag's narrative structure. The stories are
analyzed through a close reading approach, with particular attention to
adherence to the prompt, gender distribution of characters, physical and
psychological descriptions, actions, and finally, plot development and
character relationships. The results reveal the persistence of biases -
especially implicit ones - in the generated stories and highlight the
importance of assessing biases at multiple levels using an interpretative
approach.

</details>


### [164] [Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous Deliberation on Perspectivist Data](https://arxiv.org/abs/2508.09911)
*Malik Khadar,Daniel Runningen,Julia Tang,Stevie Chancellor,Harmanpreet Kaur*

Main category: cs.HC

TL;DR: 论文提出了一种基于大型语言模型（LLM）的苏格拉底对话系统，用于替代众包平台中的同步讨论，以提升数据标注的多样性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代AI依赖数据标注，但众包数据集可能损害多样性的保存。同步讨论成本高且耗时，因此需要一种更高效的方法。

Method: 使用LLM构建苏格拉底对话系统，作为标注者的讨论伙伴，替代传统众包同步讨论。

Result: 在Sarcasm和Relation检测任务中，该系统鼓励参与者考虑不同视角，提高标注准确性和信心。

Conclusion: 该方法为构建可扩展系统奠定了基础，能在生成更具代表性数据集的同时保留个体视角。

Abstract: Data annotation underpins the success of modern AI, but the aggregation of
crowd-collected datasets can harm the preservation of diverse perspectives in
data. Difficult and ambiguous tasks cannot easily be collapsed into unitary
labels. Prior work has shown that deliberation and discussion improve data
quality and preserve diverse perspectives -- however, synchronous deliberation
through crowdsourcing platforms is time-intensive and costly. In this work, we
create a Socratic dialog system using Large Language Models (LLMs) to act as a
deliberation partner in place of other crowdworkers. Against a benchmark of
synchronous deliberation on two tasks (Sarcasm and Relation detection), our
Socratic LLM encouraged participants to consider alternate annotation
perspectives, update their labels as needed (with higher confidence), and
resulted in higher annotation accuracy (for the Relation task where ground
truth is available). Qualitative findings show that our agent's Socratic
approach was effective at encouraging reasoned arguments from our participants,
and that the intervention was well-received. Our methodology lays the
groundwork for building scalable systems that preserve individual perspectives
in generating more representative datasets.

</details>
