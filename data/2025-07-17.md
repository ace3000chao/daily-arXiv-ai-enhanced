<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.HC](#cs.HC) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)
*Kazuyoshi Otsuka*

Main category: cs.CL

TL;DR: 研究将大语言模型（LLMs）作为“主观文学评论家”，探索其在文学评估中的审美偏好和评价模式。通过分析十篇日本科幻短篇小说的翻译文本，发现LLMs在评价一致性和词汇使用上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在文学评价中是否表现出类似人类评论家的个体化特征，而非中立基准。

Method: 将十篇日本科幻小说翻译成英文，由六种先进LLMs进行七次独立评估，使用主成分分析和聚类技术分析数据。

Result: 发现评价一致性差异显著（α范围1.00至0.35），识别出五种评价模式，且不同模型使用独特的评价词汇。

Conclusion: LLMs可能具有类似人类评论家的个体化评价特征，而非单纯的中立工具。

Abstract: This study positions large language models (LLMs) as "subjective literary
critics" to explore aesthetic preferences and evaluation patterns in literary
assessment. Ten Japanese science fiction short stories were translated into
English and evaluated by six state-of-the-art LLMs across seven independent
sessions. Principal component analysis and clustering techniques revealed
significant variations in evaluation consistency ({\alpha} ranging from 1.00 to
0.35) and five distinct evaluation patterns. Additionally, evaluation variance
across stories differed by up to 4.5-fold, with TF-IDF analysis confirming
distinctive evaluation vocabularies for each model. Our seven-session
within-day protocol using an original Science Fiction corpus strategically
minimizes external biases, allowing us to observe implicit value systems shaped
by RLHF and their influence on literary judgment. These findings suggest that
LLMs may possess individual evaluation characteristics similar to human
critical schools, rather than functioning as neutral benchmarkers.

</details>


### [2] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

TL;DR: 论文介绍了MapIQ基准数据集，用于评估多模态大语言模型（MLLMs）在地图视觉问答（Map-VQA）中的表现，涵盖多种地图类型和主题，并分析了模型对地图设计变化的敏感性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有Map-VQA研究主要局限于等值区域图，覆盖的主题和任务有限，需要更全面的评估框架。

Method: 构建MapIQ数据集（14,706个问答对），涵盖三种地图类型和六个主题，评估多个MLLMs在六项视觉分析任务中的表现，并测试地图设计变化对模型的影响。

Result: MLLMs的表现与人类基线相比存在差距，模型对地图设计变化敏感，依赖内部地理知识。

Conclusion: MapIQ为Map-VQA提供了更全面的评估基准，揭示了MLLMs的局限性，并提出了改进方向。

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [3] [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)
*Farideh Majidi,Ziaeddin Beheshtifard*

Main category: cs.CL

TL;DR: 研究探讨了在波斯语中使用少样本学习和增量学习方法进行跨语言情感分析，目标是利用高资源语言的知识开发适用于低资源语言的模型。


<details>
  <summary>Details</summary>
Motivation: 解决波斯语情感分析中数据有限的问题，同时利用高资源语言的知识。

Method: 使用XLM-RoBERTa、mDeBERTa和DistilBERT三种预训练多语言模型，通过少样本和增量学习方法在小样本波斯语数据上进行微调。

Result: mDeBERTa和XLM-RoBERTa在波斯语情感分析中达到96%的准确率。

Conclusion: 结合少样本学习、增量学习和多语言预训练模型是有效的。

Abstract: This research examines cross-lingual sentiment analysis using few-shot
learning and incremental learning methods in Persian. The main objective is to
develop a model capable of performing sentiment analysis in Persian using
limited data, while getting prior knowledge from high-resource languages. To
achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and
DistilBERT) were employed, which were fine-tuned using few-shot and incremental
learning approaches on small samples of Persian data from diverse sources,
including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled
the models to learn from a broad range of contexts. Experimental results show
that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%
accuracy on Persian sentiment analysis. These findings highlight the
effectiveness of combining few-shot learning and incremental learning with
multilingual pre-trained models.

</details>


### [4] [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)
*Guimin Hu,Yi Xin,Lijie Hu,Zhihong Zhu,Hasti Seifi*

Main category: cs.CL

TL;DR: PgM框架通过模态分割器将多模态表示分为单模态和配对模态特征，结合专用学习组件和解码器，实现灵活的特征学习和调整。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中，单模态和配对模态特征的区分与学习对任务性能至关重要。

Method: 提出PgM框架，包含模态分割器、单模态学习器、配对模态学习器和解码器，支持特征分割与灵活调整。

Result: 实验证明PgM在四种多模态任务中有效，并具有迁移性。特征分布可视化揭示了其贡献。

Conclusion: PgM通过明确区分和学习单模态与配对模态特征，提升了多模态任务的性能与灵活性。

Abstract: Multimodal learning benefits from multiple modal information, and each
learned modal representations can be divided into uni-modal that can be learned
from uni-modal training and paired-modal features that can be learned from
cross-modal interaction. Building on this perspective, we propose a
partitioner-guided modal learning framework, PgM, which consists of the modal
partitioner, uni-modal learner, paired-modal learner, and uni-paired modal
decoder. Modal partitioner segments the learned modal representation into
uni-modal and paired-modal features. Modal learner incorporates two dedicated
components for uni-modal and paired-modal learning. Uni-paired modal decoder
reconstructs modal representation based on uni-modal and paired-modal features.
PgM offers three key benefits: 1) thorough learning of uni-modal and
paired-modal features, 2) flexible distribution adjustment for uni-modal and
paired-modal representations to suit diverse downstream tasks, and 3) different
learning rates across modalities and partitions. Extensive experiments
demonstrate the effectiveness of PgM across four multimodal tasks and further
highlight its transferability to existing models. Additionally, we visualize
the distribution of uni-modal and paired-modal features across modalities and
tasks, offering insights into their respective contributions.

</details>


### [5] [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Pedro Alonso Doval,Jorge Alcalde Vesteiro,Héctor Cerezo-Costas*

Main category: cs.CL

TL;DR: ExpliCIT-QA是一个多模态表格问答系统，通过模块化设计提供可解释的答案，包括表格理解、语言推理、代码生成与执行，以及自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 解决端到端表格问答系统中可解释性不足的问题，适用于金融和医疗等敏感领域。

Method: 采用模块化设计，包括多模态表格理解、语言推理、自动代码生成、代码执行和自然语言解释。

Result: 在TableVQA-Bench基准测试中表现优于现有基线，提高了可解释性和透明度。

Conclusion: ExpliCIT-QA通过模块化设计填补了表格问答系统的可解释性空白，适用于需要审计的敏感领域。

Abstract: We present ExpliCIT-QA, a system that extends our previous MRT approach for
tabular question answering into a multimodal pipeline capable of handling
complex table images and providing explainable answers. ExpliCIT-QA follows a
modular design, consisting of: (1) Multimodal Table Understanding, which uses a
Chain-of-Thought approach to extract and transform content from table images;
(2) Language-based Reasoning, where a step-by-step explanation in natural
language is generated to solve the problem; (3) Automatic Code Generation,
where Python/Pandas scripts are created based on the reasoning steps, with
feedback for handling errors; (4) Code Execution to compute the final answer;
and (5) Natural Language Explanation that describes how the answer was
computed. The system is built for transparency and auditability: all
intermediate outputs, parsed tables, reasoning steps, generated code, and final
answers are available for inspection. This strategy works towards closing the
explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on
the TableVQA-Bench benchmark, comparing it with existing baselines. We
demonstrated improvements in interpretability and transparency, which open the
door for applications in sensitive domains like finance and healthcare where
auditing results are critical.

</details>


### [6] [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
*Meng Li,Timothy M. McPhillips,Dingmin Wang,Shin-Rong Tsai,Bertram Ludäscher*

Main category: cs.CL

TL;DR: 论文提出了一种结合有限语法分析和LLM的方法（CRABS），用于理解Python笔记本的信息流和执行依赖关系，解决了LLM在长上下文和幻觉问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 由于重新执行笔记本以理解其信息流和操作不切实际，且现有LLM在处理复杂笔记本时存在幻觉和长上下文问题，需要一种更可靠的方法。

Method: 提出CRABS策略，结合浅层语法分析（AST）和LLM的零样本学习，捕获笔记本的输入输出集并解决剩余歧义。

Result: 在50个代表性Kaggle笔记本上测试，LLM解决了98%的歧义，CRABS在信息流和执行依赖识别上分别达到98%和99%的F1分数。

Conclusion: CRABS通过结合语法分析和LLM，有效解决了笔记本理解问题，为评估和重用笔记本提供了可靠工具。

Abstract: Recognizing the information flows and operations comprising data science and
machine learning Python notebooks is critical for evaluating, reusing, and
adapting notebooks for new tasks. Investigating a notebook via re-execution
often is impractical due to the challenges of resolving data and software
dependencies. While Large Language Models (LLMs) pre-trained on large codebases
have demonstrated effectiveness in understanding code without running it, we
observe that they fail to understand some realistic notebooks due to
hallucinations and long-context challenges. To address these issues, we propose
a notebook understanding task yielding an information flow graph and
corresponding cell execution dependency graph for a notebook, and demonstrate
the effectiveness of a pincer strategy that uses limited syntactic analysis to
assist full comprehension of the notebook using an LLM. Our Capture and Resolve
Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and
analysis of the abstract syntax tree (AST) to capture the correct
interpretation of a notebook between lower and upper estimates of the
inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via
cell-by-cell zero-shot learning, thereby identifying the true data inputs and
outputs of each cell. We evaluate and demonstrate the effectiveness of our
approach using an annotated dataset of 50 representative, highly up-voted
Kaggle notebooks that together represent 3454 actual cell inputs and outputs.
The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the
syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves
average F1 scores of 98% identifying cell-to-cell information flows and 99%
identifying transitive cell execution dependencies.

</details>


### [7] [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)
*Matteo Fasulo,Luca Babboni,Luca Tedeschini*

Main category: cs.CL

TL;DR: AI Wizards参与CLEF 2025 CheckThat! Lab任务1，通过结合情感分数增强Transformer模型，在多语言和零样本设置中提升主观性检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决新闻文章中句子主观性检测的挑战，特别是在多语言和零样本场景下的泛化能力。

Method: 使用mDeBERTaV3-base、ModernBERT-base和Llama3.2-1B等模型，结合情感分数增强句子表示，并通过决策阈值校准处理类别不平衡。

Result: 情感特征显著提升性能，尤其在主观F1分数上，最终在希腊语任务中排名第一（Macro F1 = 0.51）。

Conclusion: 结合情感特征的Transformer模型在多语言主观性检测任务中表现优异，验证了方法的有效性。

Abstract: This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab
Task 1: Subjectivity Detection in News Articles, classifying sentences as
subjective/objective in monolingual, multilingual, and zero-shot settings.
Training/development datasets were provided for Arabic, German, English,
Italian, and Bulgarian; final evaluation included additional unseen languages
(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our
primary strategy enhanced transformer-based classifiers by integrating
sentiment scores, derived from an auxiliary model, with sentence
representations, aiming to improve upon standard fine-tuning. We explored this
sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base
(English), and Llama3.2-1B. To address class imbalance, prevalent across
languages, we employed decision threshold calibration optimized on the
development set. Our experiments show sentiment feature integration
significantly boosts performance, especially subjective F1 score. This
framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

</details>


### [8] [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
*Dante Campregher,Yanxu Chen,Sander Hoffman,Maria Heuss*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）如何处理事实与反事实信息的竞争，重点关注注意力头的作用。


<details>
  <summary>Details</summary>
Motivation: 旨在复现并整合近期三项研究（Ortu等人、Yu等人、Pavlick和McDougall等人）的发现，探讨模型学习的事实与矛盾上下文信息之间的竞争机制。

Method: 通过机制解释工具分析注意力头强度与事实输出比例的关系，评估关于注意力头抑制机制的竞争假设，并研究这些注意力模式的领域特异性。

Result: 研究发现，促进事实输出的注意力头通过通用的复制抑制而非选择性反事实抑制实现，且其行为具有领域依赖性，更大模型表现出更专业和类别敏感的模式。

Conclusion: 注意力头的作用机制是通用的，但其行为受领域和模型规模影响，为理解LLMs的信息处理提供了新视角。

Abstract: This paper presents a reproducibility study examining how Large Language
Models (LLMs) manage competing factual and counterfactual information, focusing
on the role of attention heads in this process. We attempt to reproduce and
reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and
Pavlick and McDougall et al. that investigate the competition between
model-learned facts and contradictory context information through Mechanistic
Interpretability tools. Our study specifically examines the relationship
between attention head strength and factual output ratios, evaluates competing
hypotheses about attention heads' suppression mechanisms, and investigates the
domain specificity of these attention patterns. Our findings suggest that
attention heads promoting factual output do so via general copy suppression
rather than selective counterfactual suppression, as strengthening them can
also inhibit correct facts. Additionally, we show that attention head behavior
is domain-dependent, with larger models exhibiting more specialized and
category-sensitive patterns.

</details>


### [9] [ILID: Native Script Language Identification for Indian Languages](https://arxiv.org/abs/2507.11832)
*Yash Ingle,Pruthwik Mishra*

Main category: cs.CL

TL;DR: 论文发布了一个包含23万句英语和22种印度官方语言的数据集，并开发了基于机器学习和深度学习的基线模型，用于语言识别任务，尤其在嘈杂、简短和混合代码的环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 语言识别是NLP的基础任务，尤其在多语言环境中面临挑战。印度语言因共享脚本和相似性而增加了识别难度，因此需要新的数据集和模型来解决这一问题。

Method: 创建了一个包含英语和22种印度官方语言的23万句数据集，并利用机器学习和深度学习的最新方法开发了基线模型。

Result: 基线模型在语言识别任务中表现优异，与现有最先进模型相当。

Conclusion: 论文通过发布新数据集和基线模型，为印度语言识别研究提供了重要资源，并展示了模型在多语言环境中的有效性。

Abstract: The language identification task is a crucial fundamental step in NLP. Often
it serves as a pre-processing step for widely used NLP applications such as
multilingual machine translation, information retrieval, question and
answering, and text summarization. The core challenge of language
identification lies in distinguishing languages in noisy, short, and code-mixed
environments. This becomes even harder in case of diverse Indian languages that
exhibit lexical and phonetic similarities, but have distinct differences. Many
Indian languages share the same script making the task even more challenging.
In this paper, we release a dataset of 230K sentences consisting of English and
all 22 official Indian languages labeled with their language identifiers where
data in most languages are newly created. We also develop and release robust
baseline models using state-of-the-art approaches in machine learning and deep
learning that can aid the research in this field. Our baseline models are
comparable to the state-of-the-art models for the language identification task.

</details>


### [10] [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
*Mohammad Samragh,Arnav Kundu,David Harrison,Kumari Nishu,Devang Naik,Minsik Cho,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 提出了一种新框架，通过多令牌预测提升自回归语言模型的推理速度和并行性。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型因逐令牌生成的顺序性限制了推理速度和并行性，尤其在生成后期方向明确时。

Method: 结合掩码输入、门控LoRA、轻量采样器、辅助训练损失和推测生成策略，实现多令牌预测。

Result: 代码和数学生成速度提升近5倍，通用聊天和知识任务提升约2.5倍，且质量无损失。

Conclusion: 新框架显著提升了自回归模型的效率，为未来研究提供了方向。

Abstract: Autoregressive language models are constrained by their inherently sequential
nature, generating one token at a time. This paradigm limits inference speed
and parallelism, especially during later stages of generation when the
direction and semantics of text are relatively certain. In this work, we
propose a novel framework that leverages the inherent knowledge of vanilla
autoregressive language models about future tokens, combining techniques to
realize this potential and enable simultaneous prediction of multiple
subsequent tokens. Our approach introduces several key innovations: (1) a
masked-input formulation where multiple future tokens are jointly predicted
from a common prefix; (2) a gated LoRA formulation that preserves the original
LLM's functionality, while equipping it for multi-token prediction; (3) a
lightweight, learnable sampler module that generates coherent sequences from
the predicted future tokens; (4) a set of auxiliary training losses, including
a consistency loss, to enhance the coherence and accuracy of jointly generated
tokens; and (5) a speculative generation strategy that expands tokens
quadratically in the future while maintaining high fidelity. Our method
achieves significant speedups through supervised fine-tuning on pretrained
models. For example, it generates code and math nearly 5x faster, and improves
general chat and knowledge tasks by almost 2.5x. These gains come without any
loss in quality.

</details>


### [11] [Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition](https://arxiv.org/abs/2507.11862)
*Junhong Ye,Xu Yuan,Xinying Qiu*

Main category: cs.CL

TL;DR: 研究了跨领域模型迁移、多领域数据融合和样本高效学习对PII识别的效果，发现法律领域数据对传记文本迁移效果好，医疗领域迁移效果差，融合效果因领域而异，低专业化领域仅需10%训练数据即可实现高质量识别。


<details>
  <summary>Details</summary>
Motivation: 探索PII识别的跨领域模型迁移、数据融合和样本高效学习方法，以提高文本匿名化的准确性。

Method: 使用医疗（I2B2）、法律（TAB）和传记（Wikipedia）领域的标注语料库，评估模型在领域内性能、跨领域迁移性、数据融合和少样本学习四个维度上的表现。

Result: 法律领域数据对传记文本迁移效果好，医疗领域迁移效果差；数据融合效果因领域而异；低专业化领域仅需10%训练数据即可实现高质量识别。

Conclusion: 跨领域迁移、数据融合和样本高效学习对PII识别效果显著，但效果因领域不同而异，需根据领域特点选择合适方法。

Abstract: Accurate recognition of personally identifiable information (PII) is central
to automated text anonymization. This paper investigates the effectiveness of
cross-domain model transfer, multi-domain data fusion, and sample-efficient
learning for PII recognition. Using annotated corpora from healthcare (I2B2),
legal (TAB), and biography (Wikipedia), we evaluate models across four
dimensions: in-domain performance, cross-domain transferability, fusion, and
few-shot learning. Results show legal-domain data transfers well to
biographical texts, while medical domains resist incoming transfer. Fusion
benefits are domain-specific, and high-quality recognition is achievable with
only 10% of training data in low-specialization domains.

</details>


### [12] [COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction](https://arxiv.org/abs/2507.11867)
*Xiangyu Yang,Xinying Qiu*

Main category: cs.CL

TL;DR: COLA-GEC框架通过双向知识转移提升语法错误纠正（GEC）和语法可接受性判断（COLA）任务，实现多语言基准上的最优表现。


<details>
  <summary>Details</summary>
Motivation: GEC和COLA任务共享语法知识但独立发展，通过双向框架实现知识互补。

Method: 1. 利用GEC数据集增强语法可接受性模型；2. 通过动态损失函数将语法可接受性信号整合到GEC训练中。

Result: 在多语言基准上取得最优结果，但标点错误纠正仍有挑战。

Conclusion: COLA-GEC框架有效提升任务性能，未来需进一步优化语法建模。

Abstract: Grammatical Error Correction (GEC) and grammatical acceptability judgment
(COLA) are core tasks in natural language processing, sharing foundational
grammatical knowledge yet typically evolving independently. This paper
introduces COLA-GEC, a novel bidirectional framework that enhances both tasks
through mutual knowledge transfer. First, we augment grammatical acceptability
models using GEC datasets, significantly improving their performance across
multiple languages. Second, we integrate grammatical acceptability signals into
GEC model training via a dynamic loss function, effectively guiding corrections
toward grammatically acceptable outputs. Our approach achieves state-of-the-art
results on several multilingual benchmarks. Comprehensive error analysis
highlights remaining challenges, particularly in punctuation error correction,
providing insights for future improvements in grammatical modeling.

</details>


### [13] [DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation](https://arxiv.org/abs/2507.11875)
*Tianyou Huang,Xinglu Chen,Jingshen Zhang,Xinying Qiu,Ruiying Niu*

Main category: cs.CL

TL;DR: DualReward是一种新颖的强化学习框架，用于自动生成完形填空测试的干扰项，通过双奖励结构和自适应缩放机制优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要依赖监督学习或静态生成模型，无法动态区分人类创建的干扰项和模型生成的候选干扰项。

Method: 采用双奖励结构和自适应缩放机制，动态调整奖励信号强度。

Result: 在CLOTH-F和MCQ数据集上表现优于现有方法，尤其在跨域数据上提升显著（P@1提高3.48-3.86%）。

Conclusion: DualReward框架能有效平衡从可靠人类示例中学习和探索高质量干扰项，适用于自动化测试生成。

Abstract: This paper introduces DualReward, a novel reinforcement learning framework
for automatic distractor generation in cloze tests. Unlike conventional
approaches that rely primarily on supervised learning or static generative
models, our method employs a dual reward structure with adaptive scaling that
differentiates between human-created gold standard distractors and
model-generated candidates. The framework dynamically adjusts reward signal
intensity based on model performance and confidence. We evaluate our approach
on both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,
demonstrating consistent improvements over state-of-the-art baselines.
Experimental results show that our adaptive reward scaling mechanism provides
modest but consistent benefits on homogeneous datasets (CLOTH-F) and more
substantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data
(MCQ), suggesting its particular effectiveness for handling varied question
types and domains. Our work offers a flexible framework that effectively
balances learning from reliable human examples while exploring novel,
high-quality distractors for automated test generation.

</details>


### [14] [LLMs Encode Harmfulness and Refusal Separately](https://arxiv.org/abs/2507.11878)
*Jiachen Zhao,Jing Huang,Zhengxuan Wu,David Bau,Weiyan Shi*

Main category: cs.CL

TL;DR: 研究发现LLMs内部对“有害性”和“拒绝”是两个独立的概念，通过操纵“有害性方向”可以改变模型对指令的判断，而“拒绝方向”仅影响拒绝行为。基于此，提出了一种新的安全机制（Latent Guard）。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否真正理解“有害性”，而不仅仅是机械地拒绝有害指令。

Method: 通过识别和分析LLMs内部的“有害性方向”和“拒绝方向”，验证其独立性和影响。进一步提出Latent Guard作为安全应用。

Result: 发现“有害性”和“拒绝”是两个独立的概念，Latent Guard在检测不安全输入和减少过度拒绝方面表现优异。

Conclusion: LLMs对“有害性”的理解比拒绝行为更稳健，为AI安全研究提供了新视角。

Abstract: LLMs are trained to refuse harmful instructions, but do they truly understand
harmfulness beyond just refusing? Prior work has shown that LLMs' refusal
behaviors can be mediated by a one-dimensional subspace, i.e., a refusal
direction. In this work, we identify a new dimension to analyze safety
mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a
separate concept from refusal. There exists a harmfulness direction that is
distinct from the refusal direction. As causal evidence, steering along the
harmfulness direction can lead LLMs to interpret harmless instructions as
harmful, but steering along the refusal direction tends to elicit refusal
responses directly without reversing the model's judgment on harmfulness.
Furthermore, using our identified harmfulness concept, we find that certain
jailbreak methods work by reducing the refusal signals without reversing the
model's internal belief of harmfulness. We also find that adversarially
finetuning models to accept harmful instructions has minimal impact on the
model's internal belief of harmfulness. These insights lead to a practical
safety application: The model's latent harmfulness representation can serve as
an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing
over-refusals that is robust to finetuning attacks. For instance, our Latent
Guard achieves performance comparable to or better than Llama Guard 3 8B, a
dedicated finetuned safeguard model, across different jailbreak methods. Our
findings suggest that LLMs' internal understanding of harmfulness is more
robust than their refusal decision to diverse input instructions, offering a
new perspective to study AI safety

</details>


### [15] [Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models](https://arxiv.org/abs/2507.11882)
*Bo Zeng,Chenyang Lyu,Sinuo Liu,Mingyan Zeng,Minghao Wu,Xuanfan Ni,Tianqi Shi,Yu Zhao,Yefeng Liu,Chenyu Zhu,Ruizhe Li,Jiahui Geng,Qing Li,Yu Tong,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 论文提出了一个多语言版本的指令跟随评测基准Marco-Bench-MIF，覆盖30种语言，解决了现有数据集在语言和文化适应性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有指令跟随评测数据集多为英语或机器翻译，限制了多语言场景下的适用性。

Method: 通过结合翻译与验证的混合流程，创建了本地化的多语言评测基准Marco-Bench-MIF。

Result: 评测发现高低资源语言间存在25-35%的准确率差距，模型规模影响性能45-60%，机器翻译数据低估准确率7-22%。

Conclusion: Marco-Bench-MIF揭示了多语言指令跟随的挑战，如关键词一致性和跨语言约束遵循，并提供了开源基准。

Abstract: Instruction-following capability has become a major ability to be evaluated
for Large Language Models (LLMs). However, existing datasets, such as IFEval,
are either predominantly monolingual and centered on English or simply machine
translated to other languages, limiting their applicability in multilingual
contexts. In this paper, we present an carefully-curated extension of IFEval to
a localized multilingual version named Marco-Bench-MIF, covering 30 languages
with varying levels of localization. Our benchmark addresses linguistic
constraints (e.g., modifying capitalization requirements for Chinese) and
cultural references (e.g., substituting region-specific company names in
prompts) via a hybrid pipeline combining translation with verification. Through
comprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)
25-35% accuracy gap between high/low-resource languages, (2) model scales
largely impact performance by 45-60% yet persists script-specific challenges,
and (3) machine-translated data underestimates accuracy by7-22% versus
localized data. Our analysis identifies challenges in multilingual instruction
following, including keyword consistency preservation and compositional
constraint adherence across languages. Our Marco-Bench-MIF is available at
https://github.com/AIDC-AI/Marco-Bench-MIF.

</details>


### [16] [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
*Jianzhe Ma,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文综述了深度学习在几何问题解决中的应用，包括任务总结、方法回顾、评估指标分析及未来挑战讨论。


<details>
  <summary>Details</summary>
Motivation: 几何问题解决是数学推理的关键领域，涉及教育和人工智能评估等多个重要领域，深度学习技术的发展推动了相关研究。

Method: 通过综述几何问题解决的相关任务、深度学习方法、评估指标和方法，以及讨论当前挑战和未来方向。

Result: 提供了深度学习在几何问题解决中的全面参考，促进该领域的进一步发展。

Conclusion: 本文为几何问题解决的深度学习研究提供了实用指南，并指出了未来研究方向。

Abstract: Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.

</details>


### [17] [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)
*Yichen Xu,Liangyu Chen,Liang Zhang,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: PolyChartQA是首个大规模多语言图表问答基准，覆盖10种语言，旨在解决现有图表理解基准的英语中心问题。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准主要针对英语，限制了全球用户的访问和适用性。

Method: 采用解耦管道，分离图表数据和渲染代码，利用LLM翻译和严格质量控制生成多语言图表。

Result: 实验显示英语与其他语言（尤其是非拉丁文字的低资源语言）之间存在显著性能差距。

Conclusion: PolyChartQA为推进全球包容性视觉语言模型奠定了基础。

Abstract: Charts are a universally adopted medium for interpreting and communicating
data. However, existing chart understanding benchmarks are predominantly
English-centric, limiting their accessibility and applicability to global
audiences. In this paper, we present PolyChartQA, the first large-scale
multilingual chart question answering benchmark covering 22,606 charts and
26,151 question-answering pairs across 10 diverse languages. PolyChartQA is
built using a decoupled pipeline that separates chart data from rendering code,
allowing multilingual charts to be flexibly generated by simply translating the
data and reusing the code. We leverage state-of-the-art LLM-based translation
and enforce rigorous quality control in the pipeline to ensure the linguistic
and semantic consistency of the generated multilingual charts. PolyChartQA
facilitates systematic evaluation of multilingual chart understanding.
Experiments on both open- and closed-source large vision-language models reveal
a significant performance gap between English and other languages, especially
low-resource ones with non-Latin scripts. This benchmark lays a foundation for
advancing globally inclusive vision-language models.

</details>


### [18] [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)
*Amos You*

Main category: cs.CL

TL;DR: BlockBPE是一种并行GPU实现的BPE算法，优化了批处理推理，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有CPU绑定的BPE实现在大规模批处理推理中效率低下，需优化GPU性能。

Method: BlockBPE通过消除Regex预分词，实现高度并行化的标记合并，复杂度降至O(nd)。

Result: 在高批处理推理任务中，BlockBPE吞吐量比tiktoken高2倍，比HuggingFace Tokenizers高2.5倍。

Conclusion: BlockBPE显著提升了BPE在GPU上的效率，适用于高吞吐量场景。

Abstract: Tokenization is a critical preprocessing step in large language model
pipelines, yet widely-used implementations remain CPU-bound and suboptimal for
batch inference workflows on GPU. We present BlockBPE, a parallel GPU
implementation of byte-pair encoding (BPE) that achieves near linear-time
complexity under realistic assumptions and is optimized for high-throughput,
batch inference. Unlike existing Rust-based tokenizers such as HuggingFace
Tokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex
pre-tokenization and exhibit $O(n \log n)$ runtime-BlockBPE eliminates the
Regex pre-tokenization which leads to small loss in generation quality, but
enables highly parallelized token merges within thread blocks, reducing overall
complexity to $O(nd)$ where $d \ll n$. On high-batch inference workloads,
BlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over
HuggingFace Tokenizers.

</details>


### [19] [DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression](https://arxiv.org/abs/2507.11942)
*Yi Zhao,Zuchao Li,Hai Zhao,Baoyuan Qi,Guoming Liu*

Main category: cs.CL

TL;DR: 论文提出了一种动态注意力感知的任务无关提示压缩方法（DAC），通过结合熵和注意力信息，动态感知压缩过程中的熵变化，实现细粒度压缩。实验证明DAC在多个任务和模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖信息熵压缩词汇单元，但忽略了注意力关键令牌的重要性以及压缩过程中熵的变化。

Method: 提出动态注意力感知方法（DAC），整合熵和注意力信息，动态感知熵变化。

Result: 在LongBench、GSM8K和BBH等多个领域的实验中，DAC表现稳健且显著优于现有方法。

Conclusion: DAC通过动态整合熵和注意力信息，实现了更高效的提示压缩，适用于多种任务和模型。

Abstract: Task-agnostic prompt compression leverages the redundancy in natural language
to reduce computational overhead and enhance information density within
prompts, especially in long-context scenarios. Existing methods predominantly
rely on information entropy as the metric to compress lexical units, aiming to
achieve minimal information loss. However, these approaches overlook two
critical aspects: (i) the importance of attention-critical tokens at the
algorithmic level, and (ii) shifts in information entropy during the
compression process. Motivated by these challenges, we propose a dynamic
attention-aware approach for task-agnostic prompt compression (DAC). This
approach effectively integrates entropy and attention information, dynamically
sensing entropy shifts during compression to achieve fine-grained prompt
compression. Extensive experiments across various domains, including LongBench,
GSM8K, and BBH, show that DAC consistently yields robust and substantial
improvements across a diverse range of tasks and LLMs, offering compelling
evidence of its efficacy.

</details>


### [20] [IAM: Efficient Inference through Attention Mapping between Different-scale LLMs](https://arxiv.org/abs/2507.11953)
*Yi Zhao,Zuchao Li,Hai Zhao*

Main category: cs.CL

TL;DR: IAM框架通过利用不同规模LLM间注意力矩阵的高相似性，优化注意力计算和KV缓存使用，提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在长上下文场景下资源消耗大，现有方法仅利用模型内部稀疏性，未利用外部信息优化。

Method: 提出IAM框架，通过分析注意力矩阵相似性、选择映射层及验证一致性，实现小规模与大规模LLM间的注意力映射。

Result: IAM加速预填充15%，减少KV缓存使用22.1%，且不影响性能。实验证明其通用性。

Conclusion: IAM是一种通用且与现有优化方法正交的LLM效率提升工具。

Abstract: LLMs encounter significant challenges in resource consumption nowadays,
especially with long contexts. Despite extensive efforts dedicate to enhancing
inference efficiency, these methods primarily exploit internal sparsity within
the models, without leveraging external information for optimization. We
identify the high similarity of attention matrices across different-scale LLMs,
which offers a novel perspective for optimization. We first conduct a
comprehensive analysis of how to measure similarity, how to select mapping
Layers and whether mapping is consistency. Based on these insights, we
introduce the IAM framework, which achieves dual benefits of accelerated
attention computation and reduced KV cache usage by performing attention
mapping between small and large LLMs. Our experimental results demonstrate that
IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without
appreciably sacrificing performance. Experiments on different series of models
show the generalizability of IAM. Importantly, it is also orthogonal to many
existing KV cache optimization methods, making it a versatile addition to the
current toolkit for enhancing LLM efficiency.

</details>


### [21] [The benefits of query-based KGQA systems for complex and temporal questions in LLM era](https://arxiv.org/abs/2507.11954)
*Artem Alekseev,Mikhail Chaichuk,Miron Butko,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 论文提出了一种基于查询的多阶段知识图谱问答框架，用于提升多跳和时间问题的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多跳推理和时间问题上表现不佳，而基于查询的知识图谱问答提供了一种模块化替代方案。

Method: 采用多阶段查询框架，结合新颖的实体链接和谓词匹配方法，利用CoT推理。

Result: 在多跳和时间问答数据集上表现出色，展示了小型语言模型的潜力。

Conclusion: 基于查询的多阶段知识图谱问答框架能有效提升多跳和时间问题的性能。

Abstract: Large language models excel in question-answering (QA) yet still struggle
with multi-hop reasoning and temporal questions. Query-based knowledge graph QA
(KGQA) offers a modular alternative by generating executable queries instead of
direct answers. We explore multi-stage query-based framework for WikiData QA,
proposing multi-stage approach that enhances performance on challenging
multi-hop and temporal benchmarks. Through generalization and rejection
studies, we evaluate robustness across multi-hop and temporal QA datasets.
Additionally, we introduce a novel entity linking and predicate matching method
using CoT reasoning. Our results demonstrate the potential of query-based
multi-stage KGQA framework for improving multi-hop and temporal QA with small
language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System

</details>


### [22] [PoTPTQ: A Two-step Power-of-Two Post-training for LLMs](https://arxiv.org/abs/2507.11959)
*Xinyu Wang,Vahid Partovi Nia,Peng Lu,Jerry Huang,Xiao-Wen Chang,Boxing Chen,Yufei Cui*

Main category: cs.CL

TL;DR: 提出了一种新的PoT量化框架，用于LLM权重，在极低精度格式下优于现有方法，并实现更高效的反量化。


<details>
  <summary>Details</summary>
Motivation: LLMs部署需要大量计算资源，PoT量化是解决这一问题的工具，但现有方法在GPU上效果不佳。

Method: 采用两步后训练算法：初始化量化尺度并使用校准集优化。

Result: 在2-和3-bit格式下优于现有整数量化方法，反量化速度显著提升。

Conclusion: 新PoT量化框架在低精度和高效反量化方面表现优异。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing (NLP) tasks. However, their deployment is
challenging due to the substantial computational resources required.
Power-of-two (PoT) quantization is a general tool to counteract this
difficulty. Albeit previous works on PoT quantization can be efficiently
dequantized on CPUs using fixed-point addition, it showed less effectiveness on
GPUs. The reason is entanglement of the sign bit and sequential bit
manipulations needed for dequantization. We propose a novel POT quantization
framework for LLM weights that (i) outperforms state-of-the-art accuracy in
extremely low-precision number formats, and (ii) enables faster inference
through more efficient dequantization. To maintain the accuracy of the
quantized model, we introduce a two-step post-training algorithm: (i)
initialize the quantization scales with a robust starting point, and (ii)
refine these scales using a minimal calibration set. The performance of our PoT
post-training algorithm surpasses the current state-of-the-art in integer
quantization, particularly at low precisions such as 2- and 3-bit formats. Our
PoT quantization accelerates the dequantization step required for the floating
point inference and leads to $3.67\times$ speed up on a NVIDIA V100, and
$1.63\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.

</details>


### [23] [Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](https://arxiv.org/abs/2507.11966)
*Ziyu Ge,Gabriel Chua,Leanne Tan,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 提出了一种两阶段框架，用于在低资源语言对中保留毒性内容的翻译，以新加坡英语为例，通过人工验证的少样本提示工程和模型优化，提升翻译质量和安全性。


<details>
  <summary>Details</summary>
Motivation: 在线交流中，低资源语言和方言的翻译系统常无法保留本地俚语、混合语和文化嵌入的有害言论标记，且缺乏平行数据和安全过滤器。

Method: 1. 人工验证的少样本提示工程，筛选和排序新加坡英语示例；2. 通过直接和回译优化模型-提示对。

Result: 定量人工评估证实了框架的有效性和效率。

Conclusion: 该框架不仅提升翻译质量，还支持低资源环境下的文化敏感内容审核，强调在现实应用中保留社会语言细微差别的重要性。

Abstract: As online communication increasingly incorporates under-represented languages
and colloquial dialects, standard translation systems often fail to preserve
local slang, code-mixing, and culturally embedded markers of harmful speech.
Translating toxic content between low-resource language pairs poses additional
challenges due to scarce parallel data and safety filters that sanitize
offensive expressions. In this work, we propose a reproducible, two-stage
framework for toxicity-preserving translation, demonstrated on a code-mixed
Singlish safety corpus. First, we perform human-verified few-shot prompt
engineering: we iteratively curate and rank annotator-selected Singlish-target
examples to capture nuanced slang, tone, and toxicity. Second, we optimize
model-prompt pairs by benchmarking several large language models using semantic
similarity via direct and back-translation. Quantitative human evaluation
confirms the effectiveness and efficiency of our pipeline. Beyond improving
translation quality, our framework contributes to the safety of multicultural
LLMs by supporting culturally sensitive moderation and benchmarking in
low-resource contexts. By positioning Singlish as a testbed for inclusive NLP,
we underscore the importance of preserving sociolinguistic nuance in real-world
applications such as content moderation and regional platform governance.

</details>


### [24] [Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception](https://arxiv.org/abs/2507.12356)
*Liu He,Yuanchao Li,Rui Feng,XinRan Han,Yin-Long Liu,Yuwei Yang,Zude Zhu,Jiahong Yuan*

Main category: cs.CL

TL;DR: 研究发现性别偏见影响阿尔茨海默病（AD）语音感知，男性语音更易被识别为AD，尤其在中文语音中。声学分析显示男性语音的shimmer值与AD感知显著相关。


<details>
  <summary>Details</summary>
Motivation: 探讨性别偏见在AD语音感知中的作用，揭示其对语音识别的影响。

Method: 通过16名中文听众对中文和希腊语语音的感知实验，结合声学分析（如shimmer值和语音部分）。

Result: 男性语音更频繁被识别为AD，中文语音中性别偏见更明显；shimmer值与AD感知显著相关。

Conclusion: 性别偏见在AD语音感知中起关键作用，需在AD检测模型中加以解决，并进一步验证跨语言性能。

Abstract: Gender bias has been widely observed in speech perception tasks, influenced
by the fundamental voicing differences between genders. This study reveals a
gender bias in the perception of Alzheimer's Disease (AD) speech. In a
perception experiment involving 16 Chinese listeners evaluating both Chinese
and Greek speech, we identified that male speech was more frequently identified
as AD, with this bias being particularly pronounced in Chinese speech. Acoustic
analysis showed that shimmer values in male speech were significantly
associated with AD perception, while speech portion exhibited a significant
negative correlation with AD identification. Although language did not have a
significant impact on AD perception, our findings underscore the critical role
of gender bias in AD speech perception. This work highlights the necessity of
addressing gender bias when developing AD detection models and calls for
further research to validate model performance across different linguistic
contexts.

</details>


### [25] [Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker](https://arxiv.org/abs/2507.11972)
*Yuhong Zhang,Jialu Li,Shilai Yang,Yuchen Xu,Gert Cauwenberghs,Tzyy-Ping Jung*

Main category: cs.CL

TL;DR: 论文探讨了人类与大型语言模型（LLMs）在阅读理解中的差异，通过图结构分析揭示了LLMs在语言理解上的高度一致性。


<details>
  <summary>Details</summary>
Motivation: 研究人类与LLMs在阅读理解中的差异，以改进人类与AI的协作学习策略。

Method: 使用LLM将阅读材料中的单词分组为节点和边，构建基于语义和问题导向的图结构，并与人类眼动数据对比。

Result: LLMs在图拓扑结构层面表现出高度一致的语言理解能力。

Conclusion: 研究为人类与AI的协作学习提供了新见解，扩展了先前的研究成果。

Abstract: Reading comprehension is a fundamental skill in human cognitive development.
With the advancement of Large Language Models (LLMs), there is a growing need
to compare how humans and LLMs understand language across different contexts
and apply this understanding to functional tasks such as inference, emotion
interpretation, and information retrieval. Our previous work used LLMs and
human biomarkers to study the reading comprehension process. The results showed
that the biomarkers corresponding to words with high and low relevance to the
inference target, as labeled by the LLMs, exhibited distinct patterns,
particularly when validated using eye-tracking data. However, focusing solely
on individual words limited the depth of understanding, which made the
conclusions somewhat simplistic despite their potential significance. This
study used an LLM-based AI agent to group words from a reading passage into
nodes and edges, forming a graph-based text representation based on semantic
meaning and question-oriented prompts. We then compare the distribution of eye
fixations on important nodes and edges. Our findings indicate that LLMs exhibit
high consistency in language understanding at the level of graph topological
structure. These results build on our previous findings and offer insights into
effective human-AI co-learning strategies.

</details>


### [26] [Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate](https://arxiv.org/abs/2507.12370)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CL

TL;DR: 论文提出了一种多智能体辩论框架，用于提升大语言模型（LLMs）对用户请求中模糊性的检测和解决能力，显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理用户请求时存在模糊性问题，需要一种方法增强其检测和解决能力。

Method: 采用多智能体辩论框架，结合三种LLM架构（Llama3-8B、Gemma2-9B和Mistral-7B变体）和包含多样模糊性的数据集。

Result: 辩论框架显著提升了Llama3-8B和Mistral-7B变体的性能，其中Mistral-7B主导的辩论成功率高达76.7%，尤其擅长复杂模糊性和高效共识。

Conclusion: 多智能体辩论框架是增强LLM能力的有效方法，为开发更鲁棒和自适应的语言理解系统提供了重要见解。

Abstract: Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and generating human language, contributing to more natural
interactions with complex systems. However, they face challenges such as
ambiguity in user requests processed by LLMs. To address these challenges, this
paper introduces and evaluates a multi-agent debate framework designed to
enhance detection and resolution capabilities beyond single models. The
framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and
Mistral-7B variants) and a dataset with diverse ambiguities. The debate
framework markedly enhanced the performance of Llama3-8B and Mistral-7B
variants over their individual baselines, with Mistral-7B-led debates achieving
a notable 76.7% success rate and proving particularly effective for complex
ambiguities and efficient consensus. While acknowledging varying model
responses to collaborative strategies, these findings underscore the debate
framework's value as a targeted method for augmenting LLM capabilities. This
work offers important insights for developing more robust and adaptive language
understanding systems by showing how structured debates can lead to improved
clarity in interactive systems.

</details>


### [27] [Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness](https://arxiv.org/abs/2507.11979)
*Yuki Sakamoto,Takahisa Uchida,Hiroshi Ishiguro*

Main category: cs.CL

TL;DR: 研究探讨了价值相似性对LLM代理之间关系建立的影响，发现价值相似性高的代理表现出更强的互信和人际亲近。


<details>
  <summary>Details</summary>
Motivation: 探索价值相似性在人工社会中是否与人类社会中一样对关系建立有重要影响。

Method: 通过两个实验：初步实验评估LLM中价值的可控性，主实验生成具有特定价值的代理对并分析其互评。

Result: 价值相似性高的代理对表现出更强的互信和人际亲近。

Conclusion: LLM代理模拟可作为社会科学理论的有效测试平台，并为新理论提供基础。

Abstract: Large language models (LLMs) have emerged as powerful tools for simulating
complex social phenomena using human-like agents with specific traits. In human
societies, value similarity is important for building trust and close
relationships; however, it remains unexplored whether this principle holds true
in artificial societies comprising LLM agents. Therefore, this study
investigates the influence of value similarity on relationship-building among
LLM agents through two experiments. First, in a preliminary experiment, we
evaluated the controllability of values in LLMs to identify the most effective
model and prompt design for controlling the values. Subsequently, in the main
experiment, we generated pairs of LLM agents imbued with specific values and
analyzed their mutual evaluations of trust and interpersonal closeness
following a dialogue. The experiments were conducted in English and Japanese to
investigate language dependence. The results confirmed that pairs of agents
with higher value similarity exhibited greater mutual trust and interpersonal
closeness. Our findings demonstrate that the LLM agent simulation serves as a
valid testbed for social science theories, contributes to elucidating the
mechanisms by which values influence relationship building, and provides a
foundation for inspiring new theories and insights into the social sciences.

</details>


### [28] [Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions](https://arxiv.org/abs/2507.11981)
*Lukas Ellinger,Miriam Anschütz,Georg Groh*

Main category: cs.CL

TL;DR: 研究发现，简化大型语言模型（LLM）对多义词的定义会降低完整性，增加误解风险。通过微调模型可显著改善定义质量。


<details>
  <summary>Details</summary>
Motivation: 探讨不同目标群体（如儿童或语言学习者）对多义词定义的需求，避免因简化导致信息丢失或误导。

Method: 使用两个多语言评估数据集，测试多个LLM模型（如DeepSeek v3、GPT-4o mini等），结合LLM-as-Judge和人工标注。

Result: 简化显著降低定义完整性，忽略多义性；微调Llama 3.1 8B可提升定义质量。

Conclusion: 教育NLP需平衡简洁性与完整性，确保为所有学习者提供可靠、上下文感知的定义。

Abstract: Large Language Models (LLMs) can provide accurate word definitions and
explanations for any context. However, the scope of the definition changes for
different target groups, like children or language learners. This is especially
relevant for homonyms, words with multiple meanings, where oversimplification
might risk information loss by omitting key senses, potentially misleading
users who trust LLM outputs. We investigate how simplification impacts homonym
definition quality across three target groups: Normal, Simple, and ELI5. Using
two novel evaluation datasets spanning multiple languages, we test DeepSeek v3,
Llama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge
and human annotations. Our results show that simplification drastically
degrades definition completeness by neglecting polysemy, increasing the risk of
misunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization
substantially improves homonym response quality across all prompt types. These
findings highlight the need to balance simplicity and completeness in
educational NLP to ensure reliable, context-aware definitions for all learners.

</details>


### [29] [Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis](https://arxiv.org/abs/2507.12004)
*Josip Jukić*

Main category: cs.CL

TL;DR: 该论文提出了一种结合表示平滑分析和优化技术的方法，以提高神经语言模型的数据和参数效率，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决神经语言模型在数据和参数效率方面的挑战，增强模型的鲁棒性和泛化能力。

Method: 1. 分析语言表示的动态特性并提出基于平滑性的正则化策略；2. 结合主动学习和参数高效微调；3. 利用上下文学习增强弱监督。

Result: 实验表明，这些方法在性能、稳定性和效率上显著优于传统方法。

Conclusion: 提出的方法在低资源环境和动态数据场景中表现出更高的准确性和适应性。

Abstract: This thesis addresses challenges related to data and parameter efficiency in
neural language models, with a focus on representation analysis and the
introduction of new optimization techniques. The first part examines the
properties and dynamics of language representations within neural models,
emphasizing their significance in enhancing robustness and generalization. It
proposes innovative approaches based on representation smoothness, including
regularization strategies that utilize Jacobian and Hessian matrices to
stabilize training and mitigate sensitivity to input perturbations. The second
part focuses on methods to significantly enhance data and parameter efficiency
by integrating active learning strategies with parameter-efficient fine-tuning,
guided by insights from representation smoothness analysis. It presents
smoothness-informed early-stopping techniques designed to eliminate the need
for labeled validation sets and proposes innovative combinations of active
learning and parameter-efficient fine-tuning to reduce labeling efforts and
computational resources. Extensive experimental evaluations across various NLP
tasks demonstrate that these combined approaches substantially outperform
traditional methods in terms of performance, stability, and efficiency. The
third part explores weak supervision techniques enhanced by in-context learning
to effectively utilize unlabeled data, further reducing dependence on extensive
labeling. It shows that using in-context learning as a mechanism for weak
supervision enables models to better generalize from limited labeled data by
leveraging unlabeled examples more effectively during training. Comprehensive
empirical evaluations confirm significant gains in model accuracy,
adaptability, and robustness, especially in low-resource settings and dynamic
data environments.

</details>


### [30] [A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans](https://arxiv.org/abs/2507.12039)
*Anca Dinu,Andra-Maria Florescu,Alina Resceanu*

Main category: cs.CL

TL;DR: 论文介绍了一种通用的语言创造力测试，用于评估人类和大型语言模型（LLMs）生成新词和短语的能力。测试结果显示，LLMs在所有评估标准上均优于人类，并在六项任务中表现更佳。人类更倾向于扩展创造力（E-creativity），而LLMs则偏向固定创造力（F-creativity）。


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较人类和LLMs在语言创造力方面的表现，探索两者在生成新词和短语时的差异。

Method: 设计了基于词形变化（派生和复合）和隐喻语言使用的任务，对24名人类和24个LLMs进行测试，并使用OCSAI工具自动评估答案的原创性、精细度和灵活性。

Result: LLMs在所有评估标准（原创性、精细度和灵活性）上优于人类，并在八项任务中的六项中表现更佳。人类更倾向于E-creativity，而LLMs偏向F-creativity。

Conclusion: 研究表明LLMs在语言创造力方面具有优势，但人类和LLMs在创造力类型上存在差异，人类更倾向于扩展性创造力，而LLMs更偏向固定模式。

Abstract: The following paper introduces a general linguistic creativity test for
humans and Large Language Models (LLMs). The test consists of various tasks
aimed at assessing their ability to generate new original words and phrases
based on word formation processes (derivation and compounding) and on
metaphorical language use. We administered the test to 24 humans and to an
equal number of LLMs, and we automatically evaluated their answers using OCSAI
tool for three criteria: Originality, Elaboration, and Flexibility. The results
show that LLMs not only outperformed humans in all the assessed criteria, but
did better in six out of the eight test tasks. We then computed the uniqueness
of the individual answers, which showed some minor differences between humans
and LLMs. Finally, we performed a short manual analysis of the dataset, which
revealed that humans are more inclined towards E(extending)-creativity, while
LLMs favor F(ixed)-creativity.

</details>


### [31] [Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited](https://arxiv.org/abs/2507.12059)
*Anthony G Cohn,Robert E Blackwell*

Main category: cs.CL

TL;DR: 研究评估了28种大型语言模型（LLMs）在方向推理任务上的表现，发现即使是新型大型推理模型也无法完全正确完成任务。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在方向推理任务中的能力，通过模板生成的基准测试其表现。

Method: 使用模板生成的基准测试28种LLMs，测试其在不同场景下确定正确方向的能力。

Result: 即使是最新的大型推理模型也无法在所有问题上可靠地确定正确方向。

Conclusion: LLMs在方向推理任务上仍有改进空间，需进一步研究提升其能力。

Abstract: We investigate the abilities of 28 Large language Models (LLMs) to reason
about cardinal directions (CDs) using a benchmark generated from a set of
templates, extensively testing an LLM's ability to determine the correct CD
given a particular scenario. The templates allow for a number of degrees of
variation such as means of locomotion of the agent involved, and whether set in
the first, second or third person. Even the newer Large Reasoning Models are
unable to reliably determine the correct CD for all questions. This paper
summarises and extends earlier work presented at COSIT-24.

</details>


### [32] [StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features](https://arxiv.org/abs/2507.12064)
*Jeremi K. Ochab,Mateusz Matias,Tymoteusz Boba,Tomasz Walkowiak*

Main category: cs.CL

TL;DR: 论文提出了一种基于模块化风格测量管道的二进制AI检测方法，使用spaCy模型预处理文本并提取特征，采用轻量级梯度提升机作为分类器，训练集包含50万条机器生成文本。


<details>
  <summary>Details</summary>
Motivation: 探索一种非神经网络的、计算成本低但可解释的AI检测方法，利用大规模训练数据提升分类器性能。

Method: 使用spaCy模型进行文本预处理和特征提取，采用轻量级梯度提升机作为分类器，并通过参数优化提升模型能力。

Result: 训练了一个包含50万条机器生成文本的大规模数据集，并优化了分类器参数。

Conclusion: 该方法延续了非神经网络、低成本且可解释的AI检测策略，并在大规模数据集上验证了其有效性。

Abstract: This submission to the binary AI detection task is based on a modular
stylometric pipeline, where: public spaCy models are used for text
preprocessing (including tokenisation, named entity recognition, dependency
parsing, part-of-speech tagging, and morphology annotation) and extracting
several thousand features (frequencies of n-grams of the above linguistic
annotations); light-gradient boosting machines are used as the classifier. We
collect a large corpus of more than 500 000 machine-generated texts for the
classifier's training. We explore several parameter options to increase the
classifier's capacity and take advantage of that training set. Our approach
follows the non-neural, computationally inexpensive but explainable approach
found effective previously.

</details>


### [33] [BOOKCOREF: Coreference Resolution at Book Scale](https://arxiv.org/abs/2507.12075)
*Giuliano Martinelli,Tommaso Bonomo,Pere-Lluís Huguet Cabot,Roberto Navigli*

Main category: cs.CL

TL;DR: 论文提出了一个自动标注管道，创建了首个书籍规模的共指消解基准BOOKCOREF，并展示了其在提升系统性能方面的价值。


<details>
  <summary>Details</summary>
Motivation: 现有共指消解基准在长文本评估上存在局限，无法充分评估系统在书籍规模（数十万token）下的表现。

Method: 开发了一个自动标注管道，用于生成高质量的全叙事文本共指消解标注，并创建了BOOKCOREF基准。

Result: 实验表明，该自动标注方法稳健，BOOKCOREF使当前长文档共指消解系统性能提升高达20 CoNLL-F1分。

Conclusion: 书籍规模共指消解带来了新挑战，当前模型表现不如短文档，研究鼓励开发新系统。

Abstract: Coreference Resolution systems are typically evaluated on benchmarks
containing small- to medium-scale documents. When it comes to evaluating long
texts, however, existing benchmarks, such as LitBank, remain limited in length
and do not adequately assess system capabilities at the book scale, i.e., when
co-referring mentions span hundreds of thousands of tokens. To fill this gap,
we first put forward a novel automatic pipeline that produces high-quality
Coreference Resolution annotations on full narrative texts. Then, we adopt this
pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with
an average document length of more than 200,000 tokens. We carry out a series
of experiments showing the robustness of our automatic procedure and
demonstrating the value of our resource, which enables current long-document
coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full
books. Moreover, we report on the new challenges introduced by this
unprecedented book-scale setting, highlighting that current models fail to
deliver the same performance they achieve on smaller documents. We release our
data and code to encourage research and development of new book-scale
Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.

</details>


### [34] [Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning](https://arxiv.org/abs/2507.12079)
*Tosin Adewumi,Foteini Simistira Liwicki,Marcus Liwicki,Viktor Gardelli,Lama Alkhaled,Hamam Mokayed*

Main category: cs.CL

TL;DR: 该研究探讨了结合苏格拉底法、思维链推理、简化游戏化和形成性反馈的MEGA方法对大学生数学学习的影响，发现其优于传统逐步法。


<details>
  <summary>Details</summary>
Motivation: 许多学生在数学学习中遇到困难，导致他们回避数学相关学科，而传统教学方法可能不够有效。

Method: 采用组内设计，随机分配问题，比较MEGA方法与传统的逐步法（CoT），使用GSM8K和MATH数据集评估两种大型语言模型（GPT4o和Claude 3.5 Sonnet）。

Result: 结果显示，MEGA方法在两种数据集上均被学生认为更有利于学习，尤其在较难的MATH数据集中表现更优（47.5% vs 26.67%）。

Conclusion: MEGA方法在解释复杂数学问题时表现更佳，是一种更有效的数学教学工具。

Abstract: This paper presents an intervention study on the effects of the combined
methods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)
simplified gamification and (4) formative feedback on university students'
Maths learning driven by large language models (LLMs). We call our approach
Mathematics Explanations through Games by AI LLMs (MEGA). Some students
struggle with Maths and as a result avoid Math-related discipline or subjects
despite the importance of Maths across many fields, including signal
processing. Oftentimes, students' Maths difficulties stem from suboptimal
pedagogy. We compared the MEGA method to the traditional step-by-step (CoT)
method to ascertain which is better by using a within-group design after
randomly assigning questions for the participants, who are university students.
Samples (n=60) were randomly drawn from each of the two test sets of the Grade
School Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)
datasets, based on the error margin of 11%, the confidence level of 90%, and a
manageable number of samples for the student evaluators. These samples were
used to evaluate two capable LLMs at length (Generative Pretrained Transformer
4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for
capability. The results showed that students agree in more instances that the
MEGA method is experienced as better for learning for both datasets. It is even
much better than the CoT (47.5% compared to 26.67%) in the more difficult MATH
dataset, indicating that MEGA is better at explaining difficult Maths problems.

</details>


### [35] [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](https://arxiv.org/abs/2507.12126)
*Payal Bhattad,Sai Manoj Pudukotai Dinakarrao,Anju Gupta*

Main category: cs.CL

TL;DR: 本文提出了一种评估大型语言模型（LLM）文本增强的框架，包括可扩展性分析和迭代增强与摘要细化（IASR），验证了GPT-3.5 Turbo在语义保真度、多样性和生成效率上的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本增强技术在语义保存方面的不足，特别是在大规模或迭代生成时导致的冗余和不稳定性。

Method: 提出两个评估组件：可扩展性分析和IASR，用于测量语义一致性和递归释义中的语义漂移。

Result: GPT-3.5 Turbo表现最佳，应用于BERTopic任务时，主题粒度增加400%且完全消除主题重叠。

Conclusion: 验证了所提框架在实际NLP流程中评估LLM增强的实用性。

Abstract: Text data augmentation is a widely used strategy for mitigating data sparsity
in natural language processing (NLP), particularly in low-resource settings
where limited samples hinder effective semantic modeling. While augmentation
can improve input diversity and downstream interpretability, existing
techniques often lack mechanisms to ensure semantic preservation during
large-scale or iterative generation, leading to redundancy and instability.
This work introduces a principled evaluation framework for large language model
(LLM) based text augmentation, comprising two components: (1) Scalability
Analysis, which measures semantic consistency as augmentation volume increases,
and (2) Iterative Augmentation with Summarization Refinement (IASR), which
evaluates semantic drift across recursive paraphrasing cycles. Empirical
evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the
best balance of semantic fidelity, diversity, and generation efficiency.
Applied to a real-world topic modeling task using BERTopic with GPT-enhanced
few-shot labeling, the proposed approach results in a 400% increase in topic
granularity and complete elimination of topic overlaps. These findings
validated the utility of the proposed frameworks for structured evaluation of
LLM-based augmentation in practical NLP pipelines.

</details>


### [36] [Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators](https://arxiv.org/abs/2507.12143)
*Pavel Šindelář,Ondřej Bojar*

Main category: cs.CL

TL;DR: ELOQUENT的Sensemaking任务旨在通过教师、学生和评估者系统的三步测试评估生成语言模型的文本理解能力，发现现有方法在问题生成、答案限制和评估准确性方面存在问题。


<details>
  <summary>Details</summary>
Motivation: 为生成语言模型建立可测试的高标准评估框架，通过模拟课堂考试的三步流程（问题生成、回答和评分）来评估模型的文本理解能力。

Method: 使用7种来源的测试材料（如事实核查、教科书等），4个团队提交了教师、学生和评估者系统，并对比了自动与人工评估结果。

Result: 发现现有方法在问题生成质量评估、答案限制和LLM评估准确性方面存在问题，尤其是LLM-as-a-Judge范式易误判。

Conclusion: 需要改进问题生成和答案限制的评估策略，并优化LLM评估的准确性。

Abstract: ELOQUENT is a set of shared tasks that aims to create easily testable
high-level criteria for evaluating generative language models. Sensemaking is
one such shared task.
  In Sensemaking, we try to assess how well generative models ``make sense out
of a given text'' in three steps inspired by exams in a classroom setting: (1)
Teacher systems should prepare a set of questions, (2) Student systems should
answer these questions, and (3) Evaluator systems should score these answers,
all adhering rather strictly to a given set of input materials.
  We report on the 2025 edition of Sensemaking, where we had 7 sources of test
materials (fact-checking analyses of statements, textbooks, transcribed
recordings of a lecture, and educational videos) spanning English, German,
Ukrainian, and Czech languages.
  This year, 4 teams participated, providing us with 2 Teacher submissions, 2
Student submissions, and 2 Evaluator submissions. We added baselines for
Teacher and Student using commercial large language model systems. We devised a
fully automatic evaluation procedure, which we compare to a minimalistic manual
evaluation.
  We were able to make some interesting observations. For the first task, the
creation of questions, better evaluation strategies will still have to be
devised because it is difficult to discern the quality of the various candidate
question sets. In the second task, question answering, the LLMs examined
overall perform acceptably, but restricting their answers to the given input
texts remains problematic. In the third task, evaluation of question answers,
our adversarial tests reveal that systems using the LLM-as-a-Judge paradigm
erroneously rate both garbled question-answer pairs and answers to mixed-up
questions as acceptable.

</details>


### [37] [Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production](https://arxiv.org/abs/2507.12208)
*Michael Carl,Takanori Mizowaki,Aishvarya Ray,Masaru Yamada,Devi Sri Bandaru,Xinyue Ren*

Main category: cs.CL

TL;DR: 论文提出了一种行为翻译风格空间（BTSS），用于描述可能的翻译行为模式，并通过分析击键和注视数据揭示隐藏的认知过程。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过观察翻译行为（如眼动和手指动作）来理解背后的高阶认知和情感状态。

Method: 通过分析击键和注视数据，构建多层嵌入的BTSS，作为计算翻译代理的基础。

Result: BTSS能够模拟翻译过程中情感、自动行为和认知的时间动态。

Conclusion: BTSS为理解翻译行为及其背后的认知机制提供了新视角，并为计算翻译代理的开发奠定了基础。

Abstract: The paper introduces a Behavioural Translation Style Space (BTSS) that
describes possible behavioural translation patterns. The suggested BTSS is
organized as a hierarchical structure that entails various embedded processing
layers. We posit that observable translation behaviour - i.e., eye and finger
movements - is fundamental when executing the physical act of translation but
it is caused and shaped by higher-order cognitive processes and affective
translation states. We analyse records of keystrokes and gaze data as
indicators of the hidden mental processing structure and organize the
behavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the
basis for a computational translation agent to simulate the temporal dynamics
of affect, automatized behaviour and cognition during human translation
production.

</details>


### [38] [Towards few-shot isolated word reading assessment](https://arxiv.org/abs/2507.12217)
*Reuben Smit,Retief Louw,Herman Kamper*

Main category: cs.CL

TL;DR: 研究探索了一种在低资源环境下无需自动语音识别（ASR）的孤立词阅读评估方法，采用少量样本对比儿童语音与成人参考模板，发现自监督学习（SSL）表示在处理儿童数据时存在局限性。


<details>
  <summary>Details</summary>
Motivation: 在低资源环境中，传统ASR方法可能不适用，因此需要探索替代方法，尤其是针对儿童语音的评估。

Method: 使用少量样本对比儿童语音与成人参考模板，利用自监督学习（SSL）模型的中间层编码，并研究离散化SSL特征和模板重心平均等设计选项。

Result: 实验显示，成人语音表现良好，但儿童语音输入即使使用儿童模板，性能显著下降。

Conclusion: 尽管SSL表示在低资源语音任务中表现良好，但在少量样本分类系统中处理儿童数据时存在局限性。

Abstract: We explore an ASR-free method for isolated word reading assessment in
low-resource settings. Our few-shot approach compares input child speech to a
small set of adult-provided reference templates. Inputs and templates are
encoded using intermediate layers from large self-supervised learned (SSL)
models. Using an Afrikaans child speech benchmark, we investigate design
options such as discretising SSL features and barycentre averaging of the
templates. Idealised experiments show reasonable performance for adults, but a
substantial drop for child speech input, even with child templates. Despite the
success of employing SSL representations in low-resource speech tasks, our work
highlights the limitations of SSL representations for processing child data
when used in a few-shot classification system.

</details>


### [39] [Improving Contextual ASR via Multi-grained Fusion with Large Language Models](https://arxiv.org/abs/2507.12252)
*Shilin Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: 提出了一种多粒度融合方法，结合了词级和短语级融合的优势，利用大语言模型（LLMs）提升自动语音识别（ASR）中关键词识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管端到端ASR模型在通用语音转录中表现优异，但在识别上下文相关的关键词（如专有名词或用户特定实体）时仍存在困难。

Method: 提出了一种多粒度融合方法，结合了词级和短语级融合，并采用后融合策略将ASR的声学信息与LLM的上下文知识相结合。

Result: 在中文和英文数据集上的实验表明，该方法在关键词相关指标上达到最优性能，同时保持了非关键词文本的高准确率。

Conclusion: 消融研究证实，词级和短语级组件均对性能提升有显著贡献，二者在多粒度框架中互补。代码和模型将公开。

Abstract: While end-to-end Automatic Speech Recognition (ASR) models have shown
impressive performance in transcribing general speech, they often struggle to
accurately recognize contextually relevant keywords, such as proper nouns or
user-specific entities.
  Previous approaches have explored leveraging keyword dictionaries in the
textual modality to improve keyword recognition, either through token-level
fusion that guides token-by-token generation or phrase-level fusion that
enables direct copying of keyword phrases.
  However, these methods operate at different granularities and have their own
limitations.
  In this paper, we propose a novel multi-grained fusion approach that jointly
leverages the strengths of both token-level and phrase-level fusion with Large
Language Models (LLMs).
  Our approach incorporates a late-fusion strategy that elegantly combines
ASR's acoustic information with LLM's rich contextual knowledge, balancing
fine-grained token precision with holistic phrase-level understanding.
  Experiments on Chinese and English datasets demonstrate that our approach
achieves state-of-the-art performance on keyword-related metrics while
preserving high accuracy on non-keyword text.
  Ablation studies further confirm that the token-level and phrase-level
components both contribute significantly to the performance gains,
complementing each other in our joint multi-grained framework.
  The code and models will be publicly available at https://github.com/.

</details>


### [40] [Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese](https://arxiv.org/abs/2507.12260)
*Yikang Liu,Wanyang Zhang,Yiming Wang,Jialong Tang,Pei Zhang,Baosong Yang,Fei Huang,Rui Wang,Hai Hu*

Main category: cs.CL

TL;DR: 提出了一种新的量化指标T-index，用于测量翻译文本的特性（translationese），并通过实验验证其鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对翻译文本特性的量化测量，T-index旨在填补这一空白。

Method: 通过对比微调的语言模型计算T-index，并在合成数据和真实翻译数据上评估其泛化性和有效性。

Result: T-index能够有效捕捉翻译特性，与人工标注相关性高（Pearson's r = 0.568），且与现有MT质量评估指标相关性低。

Conclusion: T-index是一种高效且鲁棒的翻译特性测量指标，可作为MT质量评估的补充工具。

Abstract: In this paper, we propose the first quantitative measure for translationese
-- the translationese-index (T-index) for graded and generalizable measurement
of translationese, computed from the likelihood ratios of two contrastively
fine-tuned language models (LMs). We use a synthesized dataset and a dataset
with translations in the wild to evaluate T-index's generalizability in
cross-domain settings and its validity against human judgments. Our results
show that T-index is both robust and efficient. T-index scored by two 0.5B LMs
fine-tuned on only 1-5k pairs of synthetic data can well capture translationese
in the wild. We find that the relative differences in T-indices between
translations can well predict pairwise translationese annotations obtained from
human annotators; and the absolute values of T-indices correlate well with
human ratings of degrees of translationese (Pearson's $r = 0.568$).
Additionally, the correlation between T-index and existing machine translation
(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting
that T-index is not covered by these metrics and can serve as a complementary
metric in MT QE.

</details>


### [41] [Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes](https://arxiv.org/abs/2507.12261)
*Johann Frei,Nils Feldhus,Lisa Raithel,Roland Roller,Alexander Meyer,Frank Kramer*

Main category: cs.CL

TL;DR: 提出了一种基于LLM代理、代码执行和医学术语数据库的端到端框架Infherno，用于将自由形式的临床笔记转换为结构化FHIR资源，解决了现有方法的泛化性和结构一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如模块化规则系统或指令调优的LLM）在将临床笔记转换为FHIR资源时存在泛化性不足和结构不一致的问题。

Method: 采用LLM代理、代码执行和医学术语数据库工具构建端到端框架Infherno，确保符合FHIR文档模式。

Result: Infherno在从非结构化文本预测FHIR资源方面表现优异，接近人类基线水平。

Conclusion: Infherno支持临床数据集成和跨机构互操作性，提供自定义和合成数据的前端，适用于本地和专有模型。

Abstract: For clinical data integration and healthcare services, the HL7 FHIR standard
has established itself as a desirable format for interoperability between
complex health data. Previous attempts at automating the translation from
free-form clinical notes into structured FHIR resources rely on modular,
rule-based systems or LLMs with instruction tuning and constrained decoding.
Since they frequently suffer from limited generalizability and structural
inconformity, we propose an end-to-end framework powered by LLM agents, code
execution, and healthcare terminology database tools to address these issues.
Our solution, called Infherno, is designed to adhere to the FHIR document
schema and competes well with a human baseline in predicting FHIR resources
from unstructured text. The implementation features a front end for custom and
synthetic data and both local and proprietary models, supporting clinical data
integration processes and interoperability across institutions.

</details>


### [42] [Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding](https://arxiv.org/abs/2507.12295)
*Feng Xiao,Jicong Fan*

Main category: cs.CL

TL;DR: 该论文提出了一个文本异常检测的基准测试，通过多种预训练语言模型的嵌入和多领域数据集，系统评估了嵌入质量对异常检测效果的影响，并开源了工具包。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏标准化的文本异常检测评估基准，限制了方法的比较和创新。

Method: 利用多种预训练语言模型（如BERT、LLaMa-2等）的嵌入和多领域数据集，结合传统和深度学习算法进行系统评估。

Result: 嵌入质量对异常检测效果至关重要，深度学习算法在LLM嵌入下未表现出优势；跨模型性能矩阵具有低秩特性。

Conclusion: 该基准为未来文本异常检测研究提供了基础，并开源了工具包以促进发展。

Abstract: Text anomaly detection is a critical task in natural language processing
(NLP), with applications spanning fraud detection, misinformation
identification, spam detection and content moderation, etc. Despite significant
advances in large language models (LLMs) and anomaly detection algorithms, the
absence of standardized and comprehensive benchmarks for evaluating the
existing anomaly detection methods on text data limits rigorous comparison and
development of innovative approaches. This work performs a comprehensive
empirical study and introduces a benchmark for text anomaly detection,
leveraging embeddings from diverse pre-trained language models across a wide
array of text datasets. Our work systematically evaluates the effectiveness of
embedding-based text anomaly detection by incorporating (1) early language
models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI
(small, ada, large)); (3) multi-domain text datasets (news, social media,
scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).
Our experiments reveal a critical empirical insight: embedding quality
significantly governs anomaly detection efficacy, and deep learning-based
approaches demonstrate no performance advantage over conventional shallow
algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived
embeddings.In addition, we observe strongly low-rank characteristics in
cross-model performance matrices, which enables an efficient strategy for rapid
model evaluation (or embedding evaluation) and selection in practical
applications. Furthermore, by open-sourcing our benchmark toolkit that includes
all embeddings from different models and code at
https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work
provides a foundation for future research in robust and scalable text anomaly
detection systems.

</details>


### [43] [Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization](https://arxiv.org/abs/2507.12308)
*Prashanth Vijayaraghavan,Apoorva Nitsure,Charles Mackin,Luyao Shi,Stefano Ambrogio,Arvind Haran,Viresh Paruthi,Ali Elzein,Dan Coops,David Beymer,Tyler Baldwin,Ehsan Degan*

Main category: cs.CL

TL;DR: 该论文评估了现有代码大语言模型（LLMs）在VHDL代码生成和摘要任务中的表现，发现其性能不足，并提出了一种名为Chain-of-Descriptions（CoDes）的新方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在通用代码任务中表现优异，但在硬件描述语言（如VHDL）领域的研究和优化不足，亟需填补这一空白。

Method: 提出CoDes方法，通过生成一系列中间描述步骤（基于问题陈述或VHDL代码）并结合原始输入提示，以提升LLMs的性能。

Result: 实验表明，CoDes方法在VHDL-Eval和VHDL-Xform数据集上显著优于标准提示策略。

Conclusion: CoDes不仅提升了VHDL代码生成和摘要的质量，还为未来优化代码LLMs提供了框架。

Abstract: Large Language Models (LLMs) have become widely used across diverse NLP tasks
and domains, demonstrating their adaptability and effectiveness. In the realm
of Electronic Design Automation (EDA), LLMs show promise for tasks like
Register-Transfer Level (RTL) code generation and summarization. However,
despite the proliferation of LLMs for general code-related tasks, there's a
dearth of research focused on evaluating and refining these models for hardware
description languages (HDLs), notably VHDL. In this study, we evaluate the
performance of existing code LLMs for VHDL code generation and summarization
using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,
an in-house dataset, aims to gauge LLMs' understanding of functionally
equivalent code. Our findings reveal consistent underperformance of these
models across different metrics, underscoring a significant gap in their
suitability for this domain. To address this challenge, we propose
Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of
LLMs for VHDL code generation and summarization tasks. CoDes involves
generating a series of intermediate descriptive steps based on: (i) the problem
statement for code generation, and (ii) the VHDL code for summarization. These
steps are then integrated with the original input prompt (problem statement or
code) and provided as input to the LLMs to generate the final output. Our
experiments demonstrate that the CoDes approach significantly surpasses the
standard prompting strategy across various metrics on both datasets. This
method not only improves the quality of VHDL code generation and summarization
but also serves as a framework for future research aimed at enhancing code LLMs
for VHDL.

</details>


### [44] [Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics](https://arxiv.org/abs/2507.12372)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei,Mohsen Mosleh*

Main category: cs.CL

TL;DR: 论文探讨了具备网页浏览能力的大型语言模型（LLMs）是否能通过用户名推断社交媒体用户的 demographics，发现其具备一定准确性，但也存在偏见和滥用风险。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLMs在实时信息检索方面的潜力，尤其是在社交媒体数据分析中的应用，填补了此前研究的空白。

Method: 方法包括使用合成数据集（48个X/Twitter账户）和调查数据集（1,384名国际参与者），评估LLMs访问社交媒体内容并预测用户 demographics 的能力。

Result: 结果显示LLMs能够以合理准确性预测用户 demographics，但也揭示了模型在解析社交媒体资料时可能引入性别和政治偏见。

Conclusion: 结论指出这种能力对计算社会科学有潜在价值，但也存在滥用风险，建议限制公共应用中的功能，同时保留研究用途的受控访问。

Abstract: Large language models (LLMs) have traditionally relied on static training
data, limiting their knowledge to fixed snapshots. Recent advancements,
however, have equipped LLMs with web browsing capabilities, enabling real time
information retrieval and multi step reasoning over live web content. While
prior studies have demonstrated LLMs ability to access and analyze websites,
their capacity to directly retrieve and analyze social media data remains
unexplored. Here, we evaluate whether web browsing LLMs can infer demographic
attributes of social media users given only their usernames. Using a synthetic
dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international
participants, we show that these models can access social media content and
predict user demographics with reasonable accuracy. Analysis of the synthetic
dataset further reveals how LLMs parse and interpret social media profiles,
which may introduce gender and political biases against accounts with minimal
activity. While this capability holds promise for computational social science
in the post API era, it also raises risks of misuse particularly in information
operations and targeted advertising underscoring the need for safeguards. We
recommend that LLM providers restrict this capability in public facing
applications, while preserving controlled access for verified research
purposes.

</details>


### [45] [Probing for Arithmetic Errors in Language Models](https://arxiv.org/abs/2507.12379)
*Yucheng Sun,Alessandro Stolfo,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 研究发现语言模型内部激活可用于检测算术错误，简单探针能高精度解码预测输出和正确答案，轻量级错误检测器准确率超90%，并可推广到复杂任务中。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型内部激活是否可用于检测算术错误，以提升模型自我修正能力。

Method: 从3位数加法任务开始，训练简单探针解码隐藏状态；扩展到结构化思维链任务，验证探针泛化能力；利用探针指导选择性重新提示。

Result: 探针能高精度检测错误，并在复杂任务中表现良好；选择性重新提示可提升任务准确率。

Conclusion: 算术错误可通过内部激活预测，简单探针为轻量级模型自我修正提供可行路径。

Abstract: We investigate whether internal activations in language models can be used to
detect arithmetic errors. Starting with a controlled setting of 3-digit
addition, we show that simple probes can accurately decode both the model's
predicted output and the correct answer from hidden states, regardless of
whether the model's output is correct. Building on this, we train lightweight
error detectors that predict model correctness with over 90% accuracy. We then
extend our analysis to structured chain-of-thought traces on addition-only
GSM8K problems and find that probes trained on simple arithmetic generalize
well to this more complex setting, revealing consistent internal
representations. Finally, we demonstrate that these probes can guide selective
re-prompting of erroneous reasoning steps, improving task accuracy with minimal
disruption to correct outputs. Our findings suggest that arithmetic errors can
be anticipated from internal activations alone, and that simple probes offer a
viable path toward lightweight model self-correction.

</details>


### [46] [Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data](https://arxiv.org/abs/2507.12425)
*Chandana Cheerla*

Main category: cs.CL

TL;DR: 该论文提出了一种改进的RAG框架，结合混合检索策略和元数据感知过滤，显著提升了企业数据处理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 企业依赖专有数据决策，但现有LLMs和RAG框架在处理异构数据时存在局限。

Method: 采用混合检索（密集嵌入和BM25）、元数据过滤、语义分块和量化索引，结合人工反馈优化。

Result: 实验显示Precision@5提升15%，Recall@5提升13%，定性评估中Faithfulness和Relevance显著提高。

Conclusion: 框架有效提升企业任务响应质量，未来将扩展至多模态数据和基于代理的检索。

Abstract: Organizations increasingly rely on proprietary enterprise data, including HR
records, structured reports, and tabular documents, for critical
decision-making. While Large Language Models (LLMs) have strong generative
capabilities, they are limited by static pretraining, short context windows,
and challenges in processing heterogeneous data formats. Conventional
Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but
often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval
strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by
metadata-aware filtering with SpaCy NER and cross-encoder reranking. The
framework applies semantic chunking to maintain textual coherence and retains
tabular data structures to preserve row-column integrity. Quantized indexing
optimizes retrieval efficiency, while human-in-the-loop feedback and
conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5
increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),
and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative
evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness
(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.
These results demonstrate the framework's effectiveness in delivering accurate,
comprehensive, and contextually relevant responses for enterprise tasks. Future
work includes extending to multimodal data and integrating agent-based
retrieval. The source code will be released at
https://github.com/CheerlaChandana/Enterprise-Chatbot

</details>


### [47] [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)
*Yik Siu Chan,Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CL

TL;DR: 研究探讨了如何利用推理语言模型的思维链（CoTs）预测最终输出的对齐风险，发现基于CoT激活的线性探针优于文本方法，且能提前预测。


<details>
  <summary>Details</summary>
Motivation: Open-weights推理语言模型的思维链可能包含有害内容，研究旨在探索是否可以通过CoTs预测最终输出的对齐风险。

Method: 评估了多种监测方法（人类、大语言模型、文本分类器），比较了基于CoT文本和激活的方法。

Result: 基于CoT激活的线性探针显著优于文本方法，且能在推理完成前准确预测。

Conclusion: 轻量级探针可实现实时安全监测和早期干预，适用于不同模型和任务。

Abstract: Open-weights reasoning language models generate long chains-of-thought (CoTs)
before producing a final response, which improves performance but introduces
additional alignment risks, with harmful content often appearing in both the
CoTs and the final outputs. In this work, we investigate if we can use CoTs to
predict final response misalignment. We evaluate a range of monitoring
approaches, including humans, highly-capable large language models, and text
classifiers, using either CoT text or activations. First, we find that a simple
linear probe trained on CoT activations can significantly outperform all
text-based methods in predicting whether a final response will be safe or
unsafe. CoT texts are often unfaithful and can mislead humans and classifiers,
while model latents (i.e., CoT activations) offer a more reliable predictive
signal. Second, the probe makes accurate predictions before reasoning
completes, achieving strong performance even when applied to early CoT
segments. These findings generalize across model sizes, families, and safety
benchmarks, suggesting that lightweight probes could enable real-time safety
monitoring and early intervention during generation.

</details>


### [48] [S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling](https://arxiv.org/abs/2507.12451)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: S2WTM提出了一种新的主题建模方法，通过球面切片Wasserstein距离解决VAE-NTMs中的后验坍塌问题，生成更一致和多样的主题。


<details>
  <summary>Details</summary>
Motivation: VAE-NTMs在建模高维文本数据的超球面潜在表示时容易出现后验坍塌，导致潜在表示无效。

Method: S2WTM采用单位超球面上的先验分布，并利用球面切片Wasserstein距离对齐后验分布与先验。

Result: 实验表明S2WTM优于现有主题模型，生成更一致和多样的主题，并在下游任务中表现更好。

Conclusion: S2WTM有效解决了后验坍塌问题，提升了主题建模的性能。

Abstract: Modeling latent representations in a hyperspherical space has proven
effective for capturing directional similarities in high-dimensional text data,
benefiting topic modeling. Variational autoencoder-based neural topic models
(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical
structure. However, VAE-NTMs often suffer from posterior collapse, where the KL
divergence term in the objective function highly diminishes, leading to
ineffective latent representations. To mitigate this issue while modeling
hyperspherical structure in the latent space, we propose the Spherical Sliced
Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior
distribution supported on the unit hypersphere and leverages the Spherical
Sliced-Wasserstein distance to align the aggregated posterior distribution with
the prior. Experimental results demonstrate that S2WTM outperforms
state-of-the-art topic models, generating more coherent and diverse topics
while improving performance on downstream tasks.

</details>


### [49] [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/abs/2507.12466)
*David Mizrahi,Anders Boesen Lindbo Larsen,Jesse Allardice,Suzie Petryk,Yuri Gorokhov,Jeffrey Li,Alex Fang,Josh Gardner,Tom Gunter,Afshin Dehghan*

Main category: cs.CL

TL;DR: 论文提出了一种名为BETR的数据选择方法，通过显式优化预训练数据与评估基准的相似性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索显式优化预训练数据选择对模型性能的影响，而非依赖隐式的基准驱动迭代。

Method: BETR方法通过嵌入基准示例和预训练文档到共享空间，基于相似性评分选择数据，并训练轻量级分类器预测全量数据。

Result: 实验表明，BETR在10项任务中的9项上表现优于基线，计算效率提升2.1倍，且适用于不同规模的模型。

Conclusion: 研究结论表明，直接匹配预训练数据与目标任务能精确塑造模型能力，且数据选择策略需适应模型规模。

Abstract: Every data selection method inherently has a target. In practice, these
targets often emerge implicitly through benchmark-driven iteration: researchers
develop selection strategies, train models, measure benchmark performance, then
refine accordingly. This raises a natural question: what happens when we make
this optimization explicit? To explore this, we propose benchmark-targeted
ranking (BETR), a simple method that selects pretraining documents based on
similarity to benchmark training examples. BETR embeds benchmark examples and a
sample of pretraining documents in a shared space, scores this sample by
similarity to benchmarks, then trains a lightweight classifier to predict these
scores for the full corpus. We compare data selection methods by training over
500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to
them. From this, we find that simply aligning pretraining data to evaluation
benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline
(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks
across all scales. BETR also generalizes well: when targeting a diverse set of
benchmarks disjoint from our evaluation suite, it still matches or outperforms
baselines. Our scaling analysis further reveals a clear trend: larger models
require less aggressive filtering. Overall, our findings show that directly
matching pretraining data to target tasks precisely shapes model capabilities
and highlight that optimal selection strategies must adapt to model scale.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming](https://arxiv.org/abs/2507.11547)
*Yingxue Zhao,Qianyi Chen,Haoran Li,Haosu Zhou,Hamid Reza Attar,Tobias Pfaff,Tailin Wu,Nan Li*

Main category: cs.LG

TL;DR: 提出了一种名为RUGNN的图神经网络替代模型，用于预测材料成形过程中的变形场，解决了传统AI模型在捕捉3D空间关系和置换不变性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统基于AI的替代模型在捕捉复杂3D空间关系和置换不变性方面存在局限性，需要更高效的模型来支持材料成形设计。

Method: 开发了RUGNN模型，结合门控循环单元（GRUs）建模时间动态，并采用U-Net启发的图上下采样机制处理空间长程依赖，提出了一种新的'节点到表面'接触表示方法。

Result: RUGNN在冷成形和热成形案例中表现出色，预测结果与有限元模拟接近，并优于其他基线GNN架构。

Conclusion: RUGNN是一种可靠的方法，能够通过准确的成形性预测支持材料成形设计。

Abstract: In recent years, various artificial intelligence-based surrogate models have
been proposed to provide rapid manufacturability predictions of material
forming processes. However, traditional AI-based surrogate models, typically
built with scalar or image-based neural networks, are limited in their ability
to capture complex 3D spatial relationships and to operate in a
permutation-invariant manner. To overcome these issues, emerging graph-based
surrogate models are developed using graph neural networks. This study
developed a new graph neural network surrogate model named Recurrent U
Net-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate
predictions of sheet material deformation fields across multiple forming
timesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model
temporal dynamics and a U-Net inspired graph-based downsample/upsample
mechanism to handle spatial long-range dependencies. A novel 'node-to-surface'
contact representation method was proposed, offering significant improvements
in computational efficiency for large-scale contact interactions. The RUGNN
model was validated using a cold forming case study and a more complex hot
forming case study using aluminium alloys. Results demonstrate that the RUGNN
model provides accurate deformation predictions closely matching ground truth
FE simulations and outperforming several baseline GNN architectures. Model
tuning was also performed to identify suitable hyperparameters, training
strategies, and input feature representations. These results demonstrate that
RUGNN is a reliable approach to support sheet material forming design by
enabling accurate manufacturability predictions.

</details>


### [51] [SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery](https://arxiv.org/abs/2507.11570)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.LG

TL;DR: 论文提出了一种名为SurgeryLSTM的模型，用于预测脊柱手术住院时长（LOS），结合了时间建模和可解释性，性能优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种能够准确预测脊柱手术住院时长的机器学习模型，同时注重模型的可解释性，以支持临床决策。

Method: 方法包括比较传统机器学习模型（如线性回归、随机森林、SVM和XGBoost）与提出的SurgeryLSTM模型（基于掩码双向LSTM和注意力机制），使用结构化电子健康记录（EHR）数据。

Result: SurgeryLSTM在预测准确性上表现最佳（R2=0.86），优于XGBoost（R2=0.85）和其他基线模型。注意力机制提高了模型的可解释性，识别出关键预测因素如骨病、慢性肾病和腰椎融合术。

Conclusion: 结论是SurgeryLSTM是一种高效且可解释的AI解决方案，支持将时间建模和可解释性机器学习方法整合到临床决策支持系统中，以优化出院准备和个性化患者护理。

Abstract: Objective: To develop and evaluate machine learning (ML) models for
predicting length of stay (LOS) in elective spine surgery, with a focus on the
benefits of temporal modeling and model interpretability. Materials and
Methods: We compared traditional ML models (e.g., linear regression, random
forest, support vector machine (SVM), and XGBoost) with our developed model,
SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an
attention, using structured perioperative electronic health records (EHR) data.
Performance was evaluated using the coefficient of determination (R2), and key
predictors were identified using explainable AI. Results: SurgeryLSTM achieved
the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)
and baseline models. The attention mechanism improved interpretability by
dynamically identifying influential temporal segments within preoperative
clinical sequences, allowing clinicians to trace which events or features most
contributed to each LOS prediction. Key predictors of LOS included bone
disorder, chronic kidney disease, and lumbar fusion identified as the most
impactful predictors of LOS. Discussion: Temporal modeling with attention
mechanisms significantly improves LOS prediction by capturing the sequential
nature of patient data. Unlike static models, SurgeryLSTM provides both higher
accuracy and greater interpretability, which are critical for clinical
adoption. These results highlight the potential of integrating attention-based
temporal models into hospital planning workflows. Conclusion: SurgeryLSTM
presents an effective and interpretable AI solution for LOS prediction in
elective spine surgery. Our findings support the integration of temporal,
explainable ML approaches into clinical decision support systems to enhance
discharge readiness and individualized patient care.

</details>


### [52] [Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators](https://arxiv.org/abs/2507.11574)
*Kazuma Kobayashi,Shailesh Garg,Farid Ahmed,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: CMCO框架结合蒙特卡洛dropout和分形预测，为神经算子提供高效、分布自由的预测区间，解决了实时虚拟传感中的不确定性量化问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在实时虚拟传感中因稀疏、噪声或非共位传感器数据导致的不确定性量化难题。

Method: 通过将蒙特卡洛dropout与分形预测结合在DeepONet架构中，实现无需重新训练或定制损失设计的空间解析不确定性估计。

Result: 在湍流、弹塑性变形和全球宇宙辐射剂量估计三个应用中，CMCO实现了接近名义覆盖率的性能。

Conclusion: CMCO为神经算子提供了一种通用、即插即用的不确定性量化解决方案，推动了科学机器学习的可扩展性和泛化性。

Abstract: Robust uncertainty quantification (UQ) remains a critical barrier to the safe
deployment of deep learning in real-time virtual sensing, particularly in
high-stakes domains where sparse, noisy, or non-collocated sensor data are the
norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework
that transforms neural operator-based virtual sensing with calibrated,
distribution-free prediction intervals. By unifying Monte Carlo dropout with
split conformal prediction in a single DeepONet architecture, CMCO achieves
spatially resolved uncertainty estimates without retraining, ensembling, or
custom loss design. Our method addresses a longstanding challenge: how to endow
operator learning with efficient and reliable UQ across heterogeneous domains.
Through rigorous evaluation on three distinct applications: turbulent flow,
elastoplastic deformation, and global cosmic radiation dose estimation-CMCO
consistently attains near-nominal empirical coverage, even in settings with
strong spatial gradients and proxy-based sensing. This breakthrough offers a
general-purpose, plug-and-play UQ solution for neural operators, unlocking
real-time, trustworthy inference in digital twins, sensor fusion, and
safety-critical monitoring. By bridging theory and deployment with minimal
computational overhead, CMCO establishes a new foundation for scalable,
generalizable, and uncertainty-aware scientific machine learning.

</details>


### [53] [Einstein Fields: A Neural Perspective To Computational General Relativity](https://arxiv.org/abs/2507.11589)
*Sandeep Suresh Cranganore,Andrei Bodnar,Arturs Berzins,Johannes Brandstetter*

Main category: cs.LG

TL;DR: Einstein Fields是一种神经表示方法，用于压缩计算密集型四维数值相对论模拟为紧凑的隐式神经网络权重。


<details>
  <summary>Details</summary>
Motivation: 传统数值相对论模拟计算成本高，Einstein Fields旨在通过神经表示提高效率和易用性。

Method: 采用Neural Tensor Fields建模广义相对论的核心张量场（metric），通过自动微分推导物理量。

Result: Einstein Fields在4D时空连续建模、存储效率、导数准确性等方面表现出潜力。

Conclusion: Einstein Fields为数值相对论提供了可扩展且高效的新方法，并开源了相关代码。

Abstract: We introduce Einstein Fields, a neural representation that is designed to
compress computationally intensive four-dimensional numerical relativity
simulations into compact implicit neural network weights. By modeling the
\emph{metric}, which is the core tensor field of general relativity, Einstein
Fields enable the derivation of physical quantities via automatic
differentiation. However, unlike conventional neural fields (e.g., signed
distance, occupancy, or radiance fields), Einstein Fields are \emph{Neural
Tensor Fields} with the key difference that when encoding the spacetime
geometry of general relativity into neural field representations, dynamics
emerge naturally as a byproduct. Einstein Fields show remarkable potential,
including continuum modeling of 4D spacetime, mesh-agnosticity, storage
efficiency, derivative accuracy, and ease of use. We address these challenges
across several canonical test beds of general relativity and release an open
source JAX-based library, paving the way for more scalable and expressive
approaches to numerical relativity. Code is made available at
https://github.com/AndreiB137/EinFields

</details>


### [54] [Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques](https://arxiv.org/abs/2507.11590)
*Raju Challagundla,Mohsen Dorodchi,Pu Wang,Minwoo Lee*

Main category: cs.LG

TL;DR: 本文综述了合成表格数据生成的最新进展，重点介绍了保留复杂特征关系、统计保真度和隐私要求的方法，并提出了一种基于实际生成目标的新分类法。


<details>
  <summary>Details</summary>
Motivation: 随着隐私法规的严格化和真实数据获取受限，合成数据生成成为解决隐私问题的关键方案，特别是在金融、医疗和社会科学等领域。

Method: 提出了一种基于下游应用、隐私保证和数据效用的分类法，并设计了一个基准框架以评估方法的实用性。

Result: 综述为未来研究提供了路线图，并为隐私敏感环境中实施合成表格数据提供了指导。

Conclusion: 本文通过连接理论基础与实际需求，推动了合成数据生成领域的发展，并为其在隐私关键环境中的应用提供了实用指南。

Abstract: As privacy regulations become more stringent and access to real-world data
becomes increasingly constrained, synthetic data generation has emerged as a
vital solution, especially for tabular datasets, which are central to domains
like finance, healthcare and the social sciences. This survey presents a
comprehensive and focused review of recent advances in synthetic tabular data
generation, emphasizing methods that preserve complex feature relationships,
maintain statistical fidelity, and satisfy privacy requirements. A key
contribution of this work is the introduction of a novel taxonomy based on
practical generation objectives, including intended downstream applications,
privacy guarantees, and data utility, directly informing methodological design
and evaluation strategies. Therefore, this review prioritizes the actionable
goals that drive synthetic data creation, including conditional generation and
risk-sensitive modeling. Additionally, the survey proposes a benchmark
framework to align technical innovation with real-world demands. By bridging
theoretical foundations with practical deployment, this work serves as both a
roadmap for future research and a guide for implementing synthetic tabular data
in privacy-critical environments.

</details>


### [55] [Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification](https://arxiv.org/abs/2507.11620)
*Steven Dillmann,Juan Rafael Martínez-Galarza*

Main category: cs.LG

TL;DR: 论文提出了针对不规则事件时间序列的二维和三维张量表示方法，结合稀疏自编码器学习物理意义明确的潜在表示，支持多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 事件时间序列的不规则性和非结构化特性使得传统方法难以提取有意义模式，需要新的表示和学习方法。

Method: 采用二维和三维张量表示事件时间序列，结合稀疏自编码器学习潜在表示。

Result: 在X射线天文学数据集中，该方法成功捕捉了时间和光谱特征，并分离了多种X射线瞬变类别。

Conclusion: 该框架为跨科学和工业领域的复杂不规则事件时间序列分析提供了灵活、可扩展和通用的解决方案。

Abstract: Event time series are sequences of discrete events occurring at irregular
time intervals, each associated with a domain-specific observational modality.
They are common in domains such as high-energy astrophysics, computational
social science, cybersecurity, finance, healthcare, neuroscience, and
seismology. Their unstructured and irregular structure poses significant
challenges for extracting meaningful patterns and identifying salient phenomena
using conventional techniques. We propose novel two- and three-dimensional
tensor representations for event time series, coupled with sparse autoencoders
that learn physically meaningful latent representations. These embeddings
support a variety of downstream tasks, including anomaly detection,
similarity-based retrieval, semantic clustering, and unsupervised
classification. We demonstrate our approach on a real-world dataset from X-ray
astronomy, showing that these representations successfully capture temporal and
spectral signatures and isolate diverse classes of X-ray transients. Our
framework offers a flexible, scalable, and generalizable solution for analyzing
complex, irregular event time series across scientific and industrial domains.

</details>


### [56] [Deep Generative Methods and Tire Architecture Design](https://arxiv.org/abs/2507.11639)
*Fouad Oubari,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: 本文研究了五种深度生成模型在工业轮胎架构生成任务中的表现，发现扩散模型整体表现最佳，并提出了一种无需额外训练的类别修复方法。


<details>
  <summary>Details</summary>
Motivation: 工业实践中缺乏关于哪种深度生成模型最适合复杂制造设计任务的明确指导，本文旨在填补这一空白。

Method: 通过评估五种模型（VAE、GAN、MMVAE、DDPM、MDM）在三种工业场景下的表现，并引入类别修复方法处理条件生成。

Result: 扩散模型整体表现最佳，其中MDM在分布内表现最好，DDPM在分布外维度约束下泛化能力更强。

Conclusion: 扩散模型是工业轮胎架构生成任务的最佳选择，同时类别修复方法为条件生成提供了有效解决方案。

Abstract: As deep generative models proliferate across the AI landscape, industrial
practitioners still face critical yet unanswered questions about which deep
generative models best suit complex manufacturing design tasks. This work
addresses this question through a complete study of five representative models
(Variational Autoencoder, Generative Adversarial Network, multimodal
Variational Autoencoder, Denoising Diffusion Probabilistic Model, and
Multinomial Diffusion Model) on industrial tire architecture generation. Our
evaluation spans three key industrial scenarios: (i) unconditional generation
of complete multi-component designs, (ii) component-conditioned generation
(reconstructing architectures from partial observations), and (iii)
dimension-constrained generation (creating designs that satisfy specific
dimensional requirements). To enable discrete diffusion models to handle
conditional scenarios, we introduce categorical inpainting, a mask-aware
reverse diffusion process that preserves known labels without requiring
additional training. Our evaluation employs geometry-aware metrics specifically
calibrated for industrial requirements, quantifying spatial coherence,
component interaction, structural connectivity, and perceptual fidelity. Our
findings reveal that diffusion models achieve the strongest overall
performance; a masking-trained VAE nonetheless outperforms the multimodal
variant MMVAE\textsuperscript{+} on nearly all component-conditioned metrics,
and within the diffusion family MDM leads in-distribution whereas DDPM
generalises better to out-of-distribution dimensional constraints.

</details>


### [57] [Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation](https://arxiv.org/abs/2507.11645)
*Ahmed Salah,David Yevick*

Main category: cs.LG

TL;DR: 本文提出了几种实用指标（如Dropout鲁棒性曲线、稀疏性等）来预测神经网络中的“grokking”行为，并揭示了其起源和动态。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络中“grokking”（延迟泛化）现象的预测方法，以理解其行为机制。

Method: 通过Dropout鲁棒性曲线（DRC）、测试精度方差、神经元稀疏性和嵌入相似性等指标分析模型从记忆到泛化的转变。

Result: 发现DRC、精度方差和神经元活动性等指标能有效预测grokking，且嵌入分布与数据集对称性相关。

Conclusion: 提出的指标不仅预测了grokking行为，还为其起源和动态提供了新见解。

Abstract: Grokking refers to delayed generalization in which the increase in test
accuracy of a neural network occurs appreciably after the improvement in
training accuracy This paper introduces several practical metrics including
variance under dropout, robustness, embedding similarity, and sparsity
measures, that can forecast grokking behavior. Specifically, the resilience of
neural networks to noise during inference is estimated from a Dropout
Robustness Curve (DRC) obtained from the variation of the accuracy with the
dropout rate as the model transitions from memorization to generalization. The
variance of the test accuracy under stochastic dropout across training
checkpoints further exhibits a local maximum during the grokking. Additionally,
the percentage of inactive neurons decreases during generalization, while the
embeddings tend to a bimodal distribution independent of initialization that
correlates with the observed cosine similarity patterns and dataset symmetries.
These metrics additionally provide valuable insight into the origin and
behaviour of grokking.

</details>


### [58] [ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs](https://arxiv.org/abs/2507.11649)
*Daniel Commey,Benjamin Appiah,Griffith S. Klogo,Garth V. Crosby*

Main category: cs.LG

TL;DR: 提出了一种基于零知识证明（ZKP）的联邦学习隐私保护评估协议，避免通过性能指标泄露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的评估阶段可能通过共享性能指标泄露敏感信息，需要一种隐私保护且可验证的评估方法。

Method: 使用零知识证明生成简洁证明，断言本地损失低于预定义阈值，无需依赖外部API，实现自包含模块。

Result: 在MNIST和HAR数据集上验证了方法的计算开销、通信成本和可验证性。

Conclusion: 提出的协议有效实现了隐私保护和可验证的联邦学习评估。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data without exposing raw data. However, the evaluation phase in FL may leak
sensitive information through shared performance metrics. In this paper, we
propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to
enable privacy-preserving and verifiable evaluation for FL. Instead of
revealing raw loss values, clients generate a succinct proof asserting that
their local loss is below a predefined threshold. Our approach is implemented
without reliance on external APIs, using self-contained modules for federated
learning simulation, ZKP circuit design, and experimental evaluation on both
the MNIST and Human Activity Recognition (HAR) datasets. We focus on a
threshold-based proof for a simple Convolutional Neural Network (CNN) model
(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate
the approach in terms of computational overhead, communication cost, and
verifiability.

</details>


### [59] [STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics](https://arxiv.org/abs/2507.11660)
*Joao F. Rocha,Ke Xu,Xingzhi Sun,Ananya Krishna,Dhananjay Bhaskar,Blanche Mongeon,Morgan Craig,Mark Gerstein,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习和基于代理的建模（ABM）的方法STAGED，用于模拟细胞间通信及其对细胞内基因调控网络的影响。


<details>
  <summary>Details</summary>
Motivation: 单细胞技术虽然能揭示细胞状态和亚群，但传统方法将细胞视为独立数据点，缺乏对细胞间动态交互的建模。

Method: 结合ABM与深度学习，使用图ODE网络（GDEs）和注意力机制动态学习基因间交互强度。

Result: 模型能够捕捉细胞间和细胞内交互，更准确地表示细胞动态。

Conclusion: STAGED为数据驱动的复杂细胞动态建模提供了新方法。

Abstract: The advent of single-cell technology has significantly improved our
understanding of cellular states and subpopulations in various tissues under
normal and diseased conditions by employing data-driven approaches such as
clustering and trajectory inference. However, these methods consider cells as
independent data points of population distributions. With spatial
transcriptomics, we can represent cellular organization, along with dynamic
cell-cell interactions that lead to changes in cell state. Still, key
computational advances are necessary to enable the data-driven learning of such
complex interactive cellular dynamics. While agent-based modeling (ABM)
provides a powerful framework, traditional approaches rely on handcrafted rules
derived from domain knowledge rather than data-driven approaches. To address
this, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)
integrating ABM with deep learning to model intercellular communication, and
its effect on the intracellular gene regulatory network. Using graph ODE
networks (GDEs) with shared weights per cell type, our approach represents
genes as vertices and interactions as directed edges, dynamically learning
their strengths through a designed attention mechanism. Trained to match
continuous trajectories of simulated as well as inferred trajectories from
spatial transcriptomics data, the model captures both intercellular and
intracellular interactions, enabling a more adaptive and accurate
representation of cellular dynamics.

</details>


### [60] [Composing Linear Layers from Irreducibles](https://arxiv.org/abs/2507.11688)
*Travis Pence,Daisuke Yamada,Vikas Singh*

Main category: cs.LG

TL;DR: 论文研究了线性层中的几何基元（如双向量）如何组合成更高层次的功能，并提出了一种基于Clifford代数的可微分算法，用O(log²d)参数实现与密集矩阵相当的性能。


<details>
  <summary>Details</summary>
Motivation: 探索大型模型中低层次基元如何组合成功能更丰富的模块，尤其是线性层中的几何基元。

Method: 使用Clifford代数，将线性层表示为双向量的组合，并设计可微分算法将其分解为转子乘积。

Result: 提出的转子基线性层在LLM注意力层中表现与块Hadamard和低秩近似等基线相当，且参数更少。

Conclusion: 研究为理解几何基元在深度模型中的组合提供了代数视角。

Abstract: Contemporary large models often exhibit behaviors suggesting the presence of
low-level primitives that compose into modules with richer functionality, but
these fundamental building blocks remain poorly understood. We investigate this
compositional structure in linear layers by asking: can we identify/synthesize
linear transformations from a minimal set of geometric primitives? Using
Clifford algebra, we show that linear layers can be expressed as compositions
of bivectors -- geometric objects encoding oriented planes -- and introduce a
differentiable algorithm that decomposes them into products of rotors. This
construction uses only O(log^2 d) parameters, versus O(d^2) required by dense
matrices. Applied to the key, query, and value projections in LLM attention
layers, our rotor-based layers match the performance of strong baselines such
as block-Hadamard and low-rank approximations. Our findings provide an
algebraic perspective on how these geometric primitives can compose into
higher-level functions within deep models.

</details>


### [61] [The Impact of Coreset Selection on Spurious Correlations and Group Robustness](https://arxiv.org/abs/2507.11690)
*Amaya Dharmasiri,William Yang,Polina Kirichenko,Lydia Liu,Olga Russakovsky*

Main category: cs.LG

TL;DR: 本文分析了数据选择方法对核心集偏差水平及下游模型鲁棒性的影响，发现嵌入表征方法比基于学习动态的方法更能减少偏差，但难样本优先策略未必能保证模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究数据选择方法是否及如何影响核心集的偏差水平，以及下游模型的鲁棒性。

Method: 在十个不同的虚假相关性基准上，结合五种样本重要性/难度评分和五种数据选择策略，进行广泛实验。

Result: 嵌入表征方法比基于学习动态的方法更不易加剧偏差，但难样本优先策略未必能提升模型鲁棒性。

Conclusion: 数据选择方法需谨慎设计，以避免偏差放大，且难样本优先策略需结合其他因素以确保模型鲁棒性。

Abstract: Coreset selection methods have shown promise in reducing the training data
size while maintaining model performance for data-efficient machine learning.
However, as many datasets suffer from biases that cause models to learn
spurious correlations instead of causal features, it is important to understand
whether and how dataset reduction methods may perpetuate, amplify, or mitigate
these biases. In this work, we conduct the first comprehensive analysis of the
implications of data selection on the spurious bias levels of the selected
coresets and the robustness of downstream models trained on them. We use an
extensive experimental setting spanning ten different spurious correlations
benchmarks, five score metrics to characterize sample importance/ difficulty,
and five data selection policies across a broad range of coreset sizes.
Thereby, we unravel a series of nontrivial nuances in interactions between
sample difficulty and bias alignment, as well as dataset bias and resultant
model robustness. For example, we find that selecting coresets using
embedding-based sample characterization scores runs a comparatively lower risk
of inadvertently exacerbating bias than selecting using characterizations based
on learning dynamics. Most importantly, our analysis reveals that although some
coreset selection methods could achieve lower bias levels by prioritizing
difficult samples, they do not reliably guarantee downstream robustness.

</details>


### [62] [Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption](https://arxiv.org/abs/2507.11702)
*Hein de Wilde,Ali Mohammed Mansoor Alsahag,Pierre Blanchet*

Main category: cs.LG

TL;DR: 该研究开发了一种基于LSTM网络和卫星数据的叶落预测系统，显著提高了预测准确性，有助于优化铁路行业的叶落缓解措施。


<details>
  <summary>Details</summary>
Motivation: 英国铁路因叶落每年损失超3亿英镑，现有预测方法在可扩展性和可靠性上存在局限，亟需改进。

Method: 结合地面真实叶落数据、多光谱和气象卫星数据，训练LSTM网络预测叶落时间。

Result: 模型预测叶落开始和结束的均方根误差分别为6.32天和9.31天，优于先前研究。

Conclusion: 该模型为铁路行业提供了高效的叶落预测工具，并有助于理解复杂生态系统。

Abstract: Railroad traffic disruption as a result of leaf-fall cost the UK rail
industry over 300 million per year and measures to mitigate such disruptions
are employed on a large scale, with 1.67 million kilometers of track being
treated in the UK in 2021 alone. Therefore, the ability to anticipate the
timing of leaf-fall would offer substantial benefits for rail network
operators, enabling the efficient scheduling of such mitigation measures.
However, current methodologies for predicting leaf-fall exhibit considerable
limitations in terms of scalability and reliability. This study endeavors to
devise a prediction system that leverages specialized prediction methods and
the latest satellite data sources to generate both scalable and reliable
insights into leaf-fall timings. An LSTM network trained on ground-truth
leaf-falling data combined with multispectral and meteorological satellite data
demonstrated a root-mean-square error of 6.32 days for predicting the start of
leaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which
improves upon previous work on the topic, offers promising opportunities for
the optimization of leaf mitigation measures in the railway industry and the
improvement of our understanding of complex ecological systems.

</details>


### [63] [Reinforcement Learning from Adversarial Preferences in Tabular MDPs](https://arxiv.org/abs/2507.11706)
*Taira Tsuchiya,Shinji Ito,Haipeng Luo*

Main category: cs.LG

TL;DR: 论文提出了一种基于偏好的马尔可夫决策过程（PbMDPs）框架，研究了Borda分数下的遗憾下界，并提出了两种算法实现T^{2/3}的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究在偏好而非数值损失下学习的MDP问题，填补了标准MDP与偏好学习之间的空白。

Method: 1. 建立PbMDPs的遗憾下界；2. 提出基于全局优化的算法和策略优化算法。

Result: 证明了PbMDPs的遗憾下界为Ω((H^2SK)^{1/3}T^{2/3})，并提出了两种算法分别实现O~((H^2S^2K)^{1/3}T^{2/3})和O~((H^6SK^5)^{1/3}T^{2/3})的遗憾界。

Conclusion: 论文为偏好学习的MDP提供了理论框架和算法支持，未来可优化算法效率。

Abstract: We introduce a new framework of episodic tabular Markov decision processes
(MDPs) with adversarial preferences, which we refer to as preference-based MDPs
(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the
numerical value of the loss is directly observed, in PbMDPs the learner instead
observes preferences between two candidate arms, which represent the choices
being compared. In this work, we focus specifically on the setting where the
reward functions are determined by Borda scores. We begin by establishing a
regret lower bound for PbMDPs with Borda scores. As a preliminary step, we
present a simple instance to prove a lower bound of $\Omega(\sqrt{HSAT})$ for
episodic MDPs with adversarial losses, where $H$ is the number of steps per
episode, $S$ is the number of states, $A$ is the number of actions, and $T$ is
the number of episodes. Leveraging this construction, we then derive a regret
lower bound of $\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda
scores, where $K$ is the number of arms. Next, we develop algorithms that
achieve a regret bound of order $T^{2/3}$. We first propose a global
optimization approach based on online linear optimization over the set of all
occupancy measures, achieving a regret bound of $\tilde{O}((H^2 S^2 K)^{1/3}
T^{2/3} )$ under known transitions. However, this approach suffers from
suboptimal dependence on the potentially large number of states $S$ and
computational inefficiency. To address this, we propose a policy optimization
algorithm whose regret is roughly bounded by $\tilde{O}( (H^6 S K^5)^{1/3}
T^{2/3} )$ under known transitions, and further extend the result to the
unknown-transition setting.

</details>


### [64] [Subgraph Generation for Generalizing on Out-of-Distribution Links](https://arxiv.org/abs/2507.11710)
*Jay Revolinsky,Harry Shomer,Jiliang Tang*

Main category: cs.LG

TL;DR: FLEX是一个图生成模型框架，通过结构条件生成和对抗性协同训练，提升图神经网络在分布外场景下的链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在分布外数据上性能下降的问题，同时扩展图生成模型的应用范围。

Method: 结合结构条件生成和对抗性协同训练（自动编码器与图神经网络），确保样本分布的结构对齐。

Result: 在合成和真实世界的分布外场景中，FLEX显著提升了链接预测性能。

Conclusion: FLEX无需专家知识即可适应不同分布外场景，并通过图数据增强优化链接结构。

Abstract: Graphs Neural Networks (GNNs) demonstrate high-performance on the link
prediction (LP) task. However, these models often rely on all dataset samples
being drawn from the same distribution. In addition, graph generative models
(GGMs) show a pronounced ability to generate novel output graphs. Despite this,
GGM applications remain largely limited to domain-specific tasks. To bridge
this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)
structurally-conditioned graph generation, and (2) adversarial co-training
between an auto-encoder and GNN. As such, FLEX ensures structural-alignment
between sample distributions to enhance link-prediction performance in
out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert
knowledge to function in different OOD scenarios. Numerous experiments are
conducted in synthetic and real-world OOD settings to demonstrate FLEX's
performance-enhancing ability, with further analysis for understanding the
effects of graph data augmentation on link structures. The source code is
available here: https://github.com/revolins/FlexOOD.

</details>


### [65] [Globalization for Scalable Short-term Load Forecasting](https://arxiv.org/abs/2507.11729)
*Amirhossein Ahmadi,Hamidreza Zareipour,Henry Leung*

Main category: cs.LG

TL;DR: 本文研究了电力传输网络中的全局负载预测方法，探讨了数据漂移、建模技术和数据异质性的影响，提出了基于时间序列聚类的解决方案，并通过实验验证了全局目标转换模型的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统局部预测模型在通用性、过拟合、数据漂移和冷启动问题上存在局限性，且难以扩展。全局预测模型通过全局化和交叉学习提供了更优的预测能力。

Method: 研究了特征转换和目标转换模型，提出了基于时间序列聚类的方法（模型基于特征转换和加权实例基于目标转换），并分析了全球化对峰值负载预测和分层预测的影响。

Result: 全局目标转换模型在全局特征和聚类技术的支持下表现优于局部模型，而全局特征转换模型需通过时间序列聚类平衡局部和全局动态。

Conclusion: 全局目标转换模型在负载预测中具有显著优势，而时间序列聚类是处理数据异质性的有效手段。

Abstract: Forecasting load in power transmission networks is essential across various
hierarchical levels, from the system level down to individual points of
delivery (PoD). While intuitive and locally accurate, traditional local
forecasting models (LFMs) face significant limitations, particularly in
handling generalizability, overfitting, data drift, and the cold start problem.
These methods also struggle with scalability, becoming computationally
expensive and less efficient as the network's size and data volume grow. In
contrast, global forecasting models (GFMs) offer a new approach to enhance
prediction generalizability, scalability, accuracy, and robustness through
globalization and cross-learning. This paper investigates global load
forecasting in the presence of data drifts, highlighting the impact of
different modeling techniques and data heterogeneity. We explore
feature-transforming and target-transforming models, demonstrating how
globalization, data heterogeneity, and data drift affect each differently. In
addition, we examine the role of globalization in peak load forecasting and its
potential for hierarchical forecasting. To address data heterogeneity and the
balance between globality and locality, we propose separate time series
clustering (TSC) methods, introducing model-based TSC for feature-transforming
models and new weighted instance-based TSC for target-transforming models.
Through extensive experiments on a real-world dataset of Alberta's electricity
load, we demonstrate that global target-transforming models consistently
outperform their local counterparts, especially when enriched with global
features and clustering techniques. In contrast, global feature-transforming
models face challenges in balancing local and global dynamics, often requiring
TSC to manage data heterogeneity effectively.

</details>


### [66] [Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning](https://arxiv.org/abs/2507.11732)
*Shiyu Chen,Cencheng Shen,Youngser Park,Carey E. Priebe*

Main category: cs.LG

TL;DR: 本文提出了一种基于统计方法的图编码嵌入（GEE）来生成高质量的初始节点特征，以提升图神经网络（GNN）的性能。通过实验验证，该方法在节点聚类和分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统GNN的性能受限于随机或低信息量的初始特征表示，导致收敛慢和次优解。本文旨在通过结构感知的特征初始化提升GNN性能。

Method: 提出GEE-powered GNN（GG）框架，利用GEE生成高质量初始特征。在节点分类任务中进一步提出GG-C变体，结合GG和GEE的输出。

Result: 在节点聚类任务中，GG在所有评估的真实数据集上表现最佳，且收敛更快。GG-C在节点分类任务中优于基线方法。

Conclusion: 结构感知的特征初始化对发挥GNN的潜力至关重要，GG和GG-C在多个任务中验证了其有效性。

Abstract: Graph neural networks (GNNs) have emerged as a powerful framework for a wide
range of node-level graph learning tasks. However, their performance is often
constrained by reliance on random or minimally informed initial feature
representations, which can lead to slow convergence and suboptimal solutions.
In this paper, we leverage a statistically grounded method, one-hot graph
encoder embedding (GEE), to generate high-quality initial node features that
enhance the end-to-end training of GNNs. We refer to this integrated framework
as the GEE-powered GNN (GG), and demonstrate its effectiveness through
extensive simulations and real-world experiments across both unsupervised and
supervised settings. In node clustering, GG consistently achieves
state-of-the-art performance, ranking first across all evaluated real-world
datasets, while exhibiting faster convergence compared to the standard GNN. For
node classification, we further propose an enhanced variant, GG-C, which
concatenates the outputs of GG and GEE and outperforms competing baselines.
These results confirm the importance of principled, structure-aware feature
initialization in realizing the full potential of GNNs.

</details>


### [67] [Sparse Identification of Nonlinear Dynamics with Conformal Prediction](https://arxiv.org/abs/2507.11739)
*Urban Fasel*

Main category: cs.LG

TL;DR: 本文探讨了如何将Conformal Prediction框架与Ensemble-SINDy（E-SINDy）结合，以量化SINDy模型中的不确定性，并展示了其在时间序列预测、模型选择和系数不确定性量化中的应用。


<details>
  <summary>Details</summary>
Motivation: 量化SINDy模型的不确定性对于评估其可靠性至关重要，尤其是在安全关键应用中。

Method: 通过Conformal Prediction框架与E-SINDy结合，提出了三种应用：时间序列预测的不确定性量化、基于特征重要性的模型选择，以及模型系数的不确定性量化。

Result: 实验表明，该方法在时间序列预测中能可靠地达到目标覆盖范围，有效量化特征重要性，并在非高斯噪声下生成更稳健的系数不确定性区间。

Conclusion: Conformal Prediction与E-SINDy的结合能够显著提升模型的不确定性量化能力，适用于复杂动态系统。

Abstract: The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for
discovering nonlinear dynamical system models from data. Quantifying
uncertainty in SINDy models is essential for assessing their reliability,
particularly in safety-critical applications. While various uncertainty
quantification methods exist for SINDy, including Bayesian and ensemble
approaches, this work explores the integration of Conformal Prediction, a
framework that can provide valid prediction intervals with coverage guarantees
based on minimal assumptions like data exchangeability. We introduce three
applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)
quantifying uncertainty in time series prediction, (2) model selection based on
library feature importance, and (3) quantifying the uncertainty of identified
model coefficients using feature conformal prediction. We demonstrate the three
applications on stochastic predator-prey dynamics and several chaotic dynamical
systems. We show that conformal prediction methods integrated with E-SINDy can
reliably achieve desired target coverage for time series forecasting,
effectively quantify feature importance, and produce more robust uncertainty
intervals for model coefficients, even under non-Gaussian noise, compared to
standard E-SINDy coefficient estimates.

</details>


### [68] [MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory](https://arxiv.org/abs/2507.11821)
*Pouya Shaeri,Arash Karimi,Ariane Middel*

Main category: cs.LG

TL;DR: MNIST-Gen是一个自动化框架，用于生成特定领域的MNIST风格数据集，结合了语义理解和强化学习，显著节省时间和提高分类准确性。


<details>
  <summary>Details</summary>
Motivation: 标准数据集（如MNIST）对领域特定任务不适用，而创建自定义数据集耗时且复杂。

Method: 结合CLIP语义理解、强化学习和人工反馈，采用分层语义分类和模块化设计。

Result: 生成Tree-MNIST和Food-MNIST数据集，自动分类准确率达85%，节省80%时间。

Conclusion: MNIST-Gen为领域特定任务提供了高效的数据集生成解决方案。

Abstract: Neural networks are often benchmarked using standard datasets such as MNIST,
FashionMNIST, or other variants of MNIST, which, while accessible, are limited
to generic classes such as digits or clothing items. For researchers working on
domain-specific tasks, such as classifying trees, food items, or other
real-world objects, these data sets are insufficient and irrelevant.
Additionally, creating and publishing a custom dataset can be time consuming,
legally constrained, or beyond the scope of individual projects. We present
MNIST-Gen, an automated, modular, and adaptive framework for generating
MNIST-style image datasets tailored to user-specified categories using
hierarchical semantic categorization. The system combines CLIP-based semantic
understanding with reinforcement learning and human feedback to achieve
intelligent categorization with minimal manual intervention. Our hierarchical
approach supports complex category structures with semantic characteristics,
enabling fine-grained subcategorization and multiple processing modes:
individual review for maximum control, smart batch processing for large
datasets, and fast batch processing for rapid creation. Inspired by category
theory, MNIST-Gen models each data transformation stage as a composable
morphism, enhancing clarity, modularity, and extensibility. As proof of
concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and
\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing
task-specific evaluation data while achieving 85\% automatic categorization
accuracy and 80\% time savings compared to manual approaches.

</details>


### [69] [A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction](https://arxiv.org/abs/2507.11757)
*Yuehua Song,Yong Gao*

Main category: cs.LG

TL;DR: 提出了一种名为Graph-in-Graph（GiG）的新框架，结合转导学习和归纳学习，有效整合药物和靶标的分子特征及其相互作用，显著提升了药物-靶标相互作用（DTI）预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的DTI预测方法难以有效整合药物、靶标及其相互作用的多样化特征，因此需要一种新方法来解决这一局限性。

Method: 提出GiG模型，将药物和靶标分子结构图表示为药物-靶标相互作用图中的元节点，结合转导学习和归纳学习，探索其复杂关系。

Result: GiG模型在多个评估指标上显著优于现有方法，证明了整合不同学习范式和交互数据的优势。

Conclusion: GiG框架通过整合分子水平和网络水平的特征，为DTI预测提供了更高效和准确的解决方案。

Abstract: Accurately predicting drug-target interactions (DTIs) is pivotal for
advancing drug discovery and target validation techniques. While machine
learning approaches including those that are based on Graph Neural Networks
(GNN) have achieved notable success in DTI prediction, many of them have
difficulties in effectively integrating the diverse features of drugs, targets
and their interactions. To address this limitation, we introduce a novel
framework to take advantage of the power of both transductive learning and
inductive learning so that features at molecular level and drug-target
interaction network level can be exploited. Within this framework is a
GNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and
target molecular structures as meta-nodes in a drug-target interaction graph,
enabling a detailed exploration of their intricate relationships. To evaluate
the proposed model, we have compiled a special benchmark comprising drug
SMILES, protein sequences, and their interaction data, which is interesting in
its own right. Our experimental results demonstrate that the GiG model
significantly outperforms existing approaches across all evaluation metrics,
highlighting the benefits of integrating different learning paradigms and
interaction data.

</details>


### [70] [Torsional-GFN: a conditional conformation generator for small molecules](https://arxiv.org/abs/2507.11759)
*Alexandra Volokhova,Léna Néhale Ezzine,Piotr Gaiński,Luca Scimeca,Emmanuel Bengio,Prudencio Tossou,Yoshua Bengio,Alex Hernandez-Garcia*

Main category: cs.LG

TL;DR: Torsional-GFN是一种基于GFlowNet的条件生成模型，用于从玻尔兹曼分布中采样分子构象，仅需奖励函数作为训练信号。


<details>
  <summary>Details</summary>
Motivation: 在药物发现中，生成稳定的分子构象对估计分子与靶标的结合亲和力至关重要。传统方法如分子动力学效率较低，而生成式机器学习方法提供了更高效的解决方案。

Method: Torsional-GFN通过条件化分子图及其局部结构（键长和键角），采样扭转角的旋转，从而生成分子构象。

Result: 实验表明，Torsional-GFN能够近似玻尔兹曼分布采样分子构象，并实现零样本泛化到未见过的键长和键角。

Conclusion: 该工作为扩展方法至更大分子系统、实现零样本泛化到新分子以及将局部结构生成纳入模型提供了有前景的途径。

Abstract: Generating stable molecular conformations is crucial in several drug
discovery applications, such as estimating the binding affinity of a molecule
to a target. Recently, generative machine learning methods have emerged as a
promising, more efficient method than molecular dynamics for sampling of
conformations from the Boltzmann distribution. In this paper, we introduce
Torsional-GFN, a conditional GFlowNet specifically designed to sample
conformations of molecules proportionally to their Boltzmann distribution,
using only a reward function as training signal. Conditioned on a molecular
graph and its local structure (bond lengths and angles), Torsional-GFN samples
rotations of its torsion angles. Our results demonstrate that Torsional-GFN is
able to sample conformations approximately proportional to the Boltzmann
distribution for multiple molecules with a single model, and allows for
zero-shot generalization to unseen bond lengths and angles coming from the MD
simulations for such molecules. Our work presents a promising avenue for
scaling the proposed approach to larger molecular systems, achieving zero-shot
generalization to unseen molecules, and including the generation of the local
structure into the GFlowNet model.

</details>


### [71] [Scaling laws for activation steering with Llama 2 models and refusal mechanisms](https://arxiv.org/abs/2507.11771)
*Sheikh Abdur Raheem Ali,Justin Xu,Ivory Yang,Jasmine Xinze Li,Ayse Arslan,Clark Benham*

Main category: cs.LG

TL;DR: 本文探讨了对比激活加法（CAA）在不同规模的Llama 2模型（7B、13B、70B）中的有效性，发现其效果随模型规模增大而减弱，且在早期至中层应用时效果最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的复杂性和能力提升，较少广泛部署的对齐技术效果尚不明确。本文旨在验证CAA在不同规模模型中的表现。

Method: 通过对比对（如仇恨到爱）在模型的残差流向量空间中寻找理想方向，并在前向传播时将该方向添加到残差流中，直接操控模型输出。

Result: 1) CAA在早期至中层应用时效果最佳；2) CAA效果随模型规模增大而减弱；3) 负面导向在所有模型规模中效果更显著。

Conclusion: CAA的效果受模型规模和应用层影响，负面导向更具优势，为模型对齐技术提供了新见解。

Abstract: As large language models (LLMs) evolve in complexity and capability, the
efficacy of less widely deployed alignment techniques are uncertain. Building
on previous work on activation steering and contrastive activation addition
(CAA), this paper explores the effectiveness of CAA with model scale using the
family of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable
'directions' in the model's residual stream vector space using contrastive
pairs (for example, hate to love) and adding this direction to the residual
stream during the forward pass. It directly manipulates the residual stream and
aims to extract features from language models to better control their outputs.
Using answer matching questions centered around the refusal behavior, we found
that 1) CAA is most effective when applied at early-mid layers. 2) The
effectiveness of CAA diminishes with model size. 3) Negative steering has more
pronounced effects than positive steering across all model sizes.

</details>


### [72] [Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network](https://arxiv.org/abs/2507.11776)
*Merel Kampere,Ali Mohammed Mansoor Alsahag*

Main category: cs.LG

TL;DR: 研究使用XGBoost分类器结合拓扑特征预测荷兰铁路网络延误，发现现有方法在非同步测试中表现有限，需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 荷兰铁路网络繁忙且延误问题突出，现有研究多关注短期预测，缺乏对网络整体模式的分析，本研究旨在填补这一空白。

Method: 采用XGBoost分类器，结合节点中心性度量，并与多种分类器（如随机森林、决策树等）进行比较，预测延误轨迹。

Result: 结果显示模型性能有限，尤其在非同步测试中表现不佳，表明需要更多针对性的优化。

Conclusion: 研究为交通网络评估提供了新见解，并提出了未来开发更鲁棒预测模型的方向。

Abstract: The Dutch railway network is one of the busiest in the world, with delays
being a prominent concern for the principal passenger railway operator NS. This
research addresses a gap in delay prediction studies within the Dutch railway
network by employing an XGBoost Classifier with a focus on topological
features. Current research predominantly emphasizes short-term predictions and
neglects the broader network-wide patterns essential for mitigating ripple
effects. This research implements and improves an existing methodology,
originally designed to forecast the evolution of the fast-changing US air
network, to predict delays in the Dutch Railways. By integrating Node
Centrality Measures and comparing multiple classifiers like RandomForest,
DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is
to predict delayed trajectories. However, the results reveal limited
performance, especially in non-simultaneous testing scenarios, suggesting the
necessity for more context-specific adaptations. Regardless, this research
contributes to the understanding of transportation network evaluation and
proposes future directions for developing more robust predictive models for
delays.

</details>


### [73] [Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation](https://arxiv.org/abs/2507.11789)
*Alessandro Palma,Sergei Rybakov,Leon Hetzel,Stephan Günnemann,Fabian J. Theis*

Main category: cs.LG

TL;DR: FlatVI是一种新的训练框架，通过正则化离散似然变分自编码器的潜在流形，使其更适合单细胞计数数据的建模，提升下游方法的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单细胞RNA测序中假设线性转移和欧几里得几何，但线性插值可能不符合数据流形上的测地线路径，限制了方法的有效性。

Method: FlatVI通过正则化潜在流形，使其更接近欧几里得几何，从而在潜在空间中直线插值近似于解码后的单细胞流形上的测地线插值。

Result: 实验表明，FlatVI在合成数据和单细胞RNA测序数据中均表现出更好的轨迹重建和流形插值效果。

Conclusion: FlatVI通过优化潜在流形的几何特性，显著提升了单细胞数据建模的准确性和下游应用的兼容性。

Abstract: Latent space interpolations are a powerful tool for navigating deep
generative models in applied settings. An example is single-cell RNA
sequencing, where existing methods model cellular state transitions as latent
space interpolations with variational autoencoders, often assuming linear
shifts and Euclidean geometry. However, unless explicitly enforced, linear
interpolations in the latent space may not correspond to geodesic paths on the
data manifold, limiting methods that assume Euclidean geometry in the data
representations. We introduce FlatVI, a novel training framework that
regularises the latent manifold of discrete-likelihood variational autoencoders
towards Euclidean geometry, specifically tailored for modelling single-cell
count data. By encouraging straight lines in the latent space to approximate
geodesic interpolations on the decoded single-cell manifold, FlatVI enhances
compatibility with downstream approaches that assume Euclidean latent geometry.
Experiments on synthetic data support the theoretical soundness of our
approach, while applications to time-resolved single-cell RNA sequencing data
demonstrate improved trajectory reconstruction and manifold interpolation.

</details>


### [74] [CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels](https://arxiv.org/abs/2507.11807)
*Ruofan Hu,Dongyu Zhang,Huayi Zhang,Elke Rundensteiner*

Main category: cs.LG

TL;DR: 提出了一种不依赖干净标签数据的元学习方法CLID-MU，通过跨层信息差异评估模型性能，在噪声标签场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统元学习方法依赖干净标签数据集的问题，提出一种无需干净标签的噪声标签学习方法。

Method: 利用干净样本在隐藏层和最终层的数据结构一致性，设计跨层信息差异策略（CLID-MU）指导训练。

Result: 在合成和真实噪声标签的基准数据集上，CLID-MU优于现有方法。

Conclusion: CLID-MU为噪声标签学习提供了一种有效且不依赖干净标签的方法。

Abstract: Learning with noisy labels (LNL) is essential for training deep neural
networks with imperfect data. Meta-learning approaches have achieved success by
using a clean unbiased labeled set to train a robust model. However, this
approach heavily depends on the availability of a clean labeled meta-dataset,
which is difficult to obtain in practice. In this work, we thus tackle the
challenge of meta-learning for noisy label scenarios without relying on a clean
labeled dataset. Our approach leverages the data itself while bypassing the
need for labels. Building on the insight that clean samples effectively
preserve the consistency of related data structures across the last hidden and
the final layer, whereas noisy samples disrupt this consistency, we design the
Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU).
CLID-MU leverages the alignment of data structures across these diverse feature
spaces to evaluate model performance and use this alignment to guide training.
Experiments on benchmark datasets with varying amounts of labels under both
synthetic and real-world noise demonstrate that CLID-MU outperforms
state-of-the-art methods. The code is released at
https://github.com/ruofanhu/CLID-MU.

</details>


### [75] [SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling](https://arxiv.org/abs/2507.11818)
*Andrei Rekesh,Miruna Cretu,Dmytro Shevchuk,Vignesh Ram Somnath,Pietro Liò,Robert A. Batey,Mike Tyers,Michał Koziarski,Cheng-Hao Liu*

Main category: cs.LG

TL;DR: SynCoGen是一个结合掩码图扩散和流匹配的框架，用于生成可合成的3D分子，解决了现有方法局限于2D分子图的局限性。


<details>
  <summary>Details</summary>
Motivation: 生成可合成的小分子是药物发现中的关键挑战，现有方法多局限于2D分子图，无法实现基于几何的条件生成。

Method: SynCoGen通过联合采样分子构建块、化学反应和原子坐标的分布，结合掩码图扩散和流匹配技术，生成3D分子。

Result: SynCoGen在无条件小分子图和构象生成中达到最优性能，并在零射击分子连接设计中表现优异。

Conclusion: SynCoGen为未来非自回归分子生成应用（如类似物扩展和直接结构条件化）奠定了基础。

Abstract: Ensuring synthesizability in generative small molecule design remains a major
challenge. While recent developments in synthesizable molecule generation have
demonstrated promising results, these efforts have been largely confined to 2D
molecular graph representations, limiting the ability to perform geometry-based
conditional generation. In this work, we present SynCoGen (Synthesizable
Co-Generation), a single framework that combines simultaneous masked graph
diffusion and flow matching for synthesizable 3D molecule generation. SynCoGen
samples from the joint distribution of molecular building blocks, chemical
reactions, and atomic coordinates. To train the model, we curated SynSpace, a
dataset containing over 600K synthesis-aware building block graphs and 3.3M
conformers. SynCoGen achieves state-of-the-art performance in unconditional
small molecule graph and conformer generation, and the model delivers
competitive performance in zero-shot molecular linker design for protein ligand
generation in drug discovery. Overall, this multimodal formulation represents a
foundation for future applications enabled by non-autoregressive molecular
generation, including analog expansion, lead optimization, and direct structure
conditioning.

</details>


### [76] [HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction](https://arxiv.org/abs/2507.11836)
*Jian Gao,Jianshe Wu,JingYi Ding*

Main category: cs.LG

TL;DR: 论文提出HyperEvent框架，通过将动态链接预测重构为超事件识别，利用事件相关性向量动态构建关联序列，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能捕捉复合超事件的结构凝聚力，限制了动态图建模的准确性。

Method: 提出HyperEvent框架，通过事件相关性向量动态构建关联序列，评估查询事件与历史事件是否形成有效超事件。

Result: 在5个数据集中4个表现优于现有方法，大规模Flight数据集上MRR提升6.95%，训练时间仅需10.17%。

Conclusion: HyperEvent在准确性和效率上均优于现有方法，适用于大规模动态图建模。

Abstract: Dynamic link prediction in continuous-time dynamic graphs is a fundamental
task for modeling evolving complex systems. Existing node-centric and
event-centric methods focus on individual interactions or atomic states,
failing to capture the structural cohesion of composite hyper-events, groups of
causally related events. To address this, we propose HyperEvent, a framework
reframing dynamic link prediction as hyper-event recognition. Central to
HyperEvent is the dynamic construction of an association sequence using event
correlation vectors. These vectors quantify pairwise dependencies between the
query event and relevant historical events, thereby characterizing the
structural cohesion of a potential hyper-event. The framework predicts the
occurrence of the query event by evaluating whether it collectively forms a
valid hyper-event with these historical events. Notably, HyperEvent outperforms
state-of-the-art methods on 4 out of 5 datasets in the official leaderboard.
For scalability, we further introduce an efficient parallel training algorithm
that segments large event streams to enable concurrent training. Experiments
validate HyperEvent's superior accuracy and efficiency on large-scale graphs.
Among which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank
over state-of-the-art baseline on the large-scale Flight dataset while
utilizing only 10.17% of the training time.

</details>


### [77] [Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM](https://arxiv.org/abs/2507.11839)
*Chengyue Gong,Xinshi Chen,Yuxuan Zhang,Yuxuan Song,Hao Zhou,Wenzhi Xiao*

Main category: cs.LG

TL;DR: Protenix-Mini通过优化架构和采样策略，实现了高效的蛋白质结构预测，计算开销显著降低，性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 解决生物分子结构预测中模型效率与预测精度平衡的挑战。

Method: 1) 用少步ODE采样器替代多步AF3采样器；2) 剪枝冗余Transformer模块；3) 用ESM模块替代传统MSA模块。

Result: Protenix-Mini在基准数据集上性能仅下降1-5%，计算开销显著降低。

Conclusion: Protenix-Mini是计算资源有限但需高精度预测的理想选择。

Abstract: Lightweight inference is critical for biomolecular structure prediction and
other downstream tasks, enabling efficient real-world deployment and
inference-time scaling for large-scale applications. In this work, we address
the challenge of balancing model efficiency and prediction accuracy by making
several key modifications, 1) Multi-step AF3 sampler is replaced by a few-step
ODE sampler, significantly reducing computational overhead for the diffusion
module part during inference; 2) In the open-source Protenix framework, a
subset of pairformer or diffusion transformer blocks doesn't make contributions
to the final structure prediction, presenting opportunities for architectural
pruning and lightweight redesign; 3) A model incorporating an ESM module is
trained to substitute the conventional MSA module, reducing MSA preprocessing
time. Building on these key insights, we present Protenix-Mini, a compact and
optimized model designed for efficient protein structure prediction. This
streamlined version incorporates a more efficient architectural design with a
two-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating
redundant Transformer components and refining the sampling process,
Protenix-Mini significantly reduces model complexity with slight accuracy drop.
Evaluations on benchmark datasets demonstrate that it achieves high-fidelity
predictions, with only a negligible 1 to 5 percent decrease in performance on
benchmark datasets compared to its full-scale counterpart. This makes
Protenix-Mini an ideal choice for applications where computational resources
are limited but accurate structure prediction remains crucial.

</details>


### [78] [Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update](https://arxiv.org/abs/2507.11847)
*Yu-Jie Zhang,Sheng-An Xu,Peng Zhao,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出了一种广义线性老虎机问题的联合高效算法，实现了近乎最优的遗憾边界，并具有每轮O(1)的时间和空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 广义线性老虎机问题因其非线性特性在计算和统计效率上存在挑战，现有方法通常需要在两者之间权衡。

Method: 通过在线镜像下降（OMD）估计器构建紧密置信集，利用混合损失概念进行新颖分析。

Result: 算法在统计效率上接近最大似然估计，实现了联合高效优化。

Conclusion: 提出的方法在计算和统计效率上均表现优异，适用于广泛的实际场景。

Abstract: We study the generalized linear bandit (GLB) problem, a contextual
multi-armed bandit framework that extends the classical linear model by
incorporating a non-linear link function, thereby modeling a broad class of
reward distributions such as Bernoulli and Poisson. While GLBs are widely
applicable to real-world scenarios, their non-linear nature introduces
significant challenges in achieving both computational and statistical
efficiency. Existing methods typically trade off between two objectives, either
incurring high per-round costs for optimal regret guarantees or compromising
statistical efficiency to enable constant-time updates. In this paper, we
propose a jointly efficient algorithm that attains a nearly optimal regret
bound with $\mathcal{O}(1)$ time and space complexities per round. The core of
our method is a tight confidence set for the online mirror descent (OMD)
estimator, which is derived through a novel analysis that leverages the notion
of mix loss from online prediction. The analysis shows that our OMD estimator,
even with its one-pass updates, achieves statistical efficiency comparable to
maximum likelihood estimation, thereby leading to a jointly efficient
optimistic method.

</details>


### [79] [OrdShap: Feature Position Importance for Sequential Black-Box Models](https://arxiv.org/abs/2507.11855)
*Davin Hill,Brian L. Hill,Aria Masoomi,Vijay S. Nori,Robert E. Tillman,Jennifer Dy*

Main category: cs.LG

TL;DR: OrdShap是一种新的特征归因方法，通过量化特征位置对模型预测的影响，解决了现有方法混淆特征值和位置的问题。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法假设特征顺序固定，无法区分特征值和位置对预测的影响，限制了模型的可解释性。

Method: 提出OrdShap方法，通过置换特征位置量化其对预测的影响，并与Sanchez-Bergantiños值建立博弈论联系。

Result: 在健康、自然语言和合成数据集上的实验表明，OrdShap能有效捕捉特征值和位置的影响，提供更深入的模型行为洞察。

Conclusion: OrdShap为位置敏感的特征归因提供了理论基础和实践工具，提升了模型的可解释性。

Abstract: Sequential deep learning models excel in domains with temporal or sequential
dependencies, but their complexity necessitates post-hoc feature attribution
methods for understanding their predictions. While existing techniques quantify
feature importance, they inherently assume fixed feature ordering - conflating
the effects of (1) feature values and (2) their positions within input
sequences. To address this gap, we introduce OrdShap, a novel attribution
method that disentangles these effects by quantifying how a model's predictions
change in response to permuting feature position. We establish a game-theoretic
connection between OrdShap and Sanchez-Berganti\~nos values, providing a
theoretically grounded approach to position-sensitive attribution. Empirical
results from health, natural language, and synthetic datasets highlight
OrdShap's effectiveness in capturing feature value and feature position
attributions, and provide deeper insight into model behavior.

</details>


### [80] [A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers](https://arxiv.org/abs/2507.11865)
*Hanwen Dai,Chang Gao,Fang He,Congyuan Ji,Yanni Yang*

Main category: cs.LG

TL;DR: 研究提出了一种动态管理司机接受折扣快车服务的方法，通过pi-DDPG框架优化决策，提升早期学习效率和匹配性能。


<details>
  <summary>Details</summary>
Motivation: 平台整合中，司机参与折扣快车服务虽能扩大需求池和提高匹配效率，但会降低利润。缺乏历史数据和高随机性增加了管理难度。

Method: 采用pi-DDPG框架，包含改进模块、卷积LSTM网络和优先经验回放机制，以动态管理司机接受行为。

Result: pi-DDPG在模拟实验中表现出更高的学习效率，显著减少了早期训练损失。

Conclusion: pi-DDPG框架有效解决了平台在缺乏历史数据和高随机性下的动态管理问题，具有实际应用潜力。

Abstract: The rapid expansion of platform integration has emerged as an effective
solution to mitigate market fragmentation by consolidating multiple
ride-hailing platforms into a single application. To address heterogeneous
passenger preferences, third-party integrators provide Discount Express service
delivered by express drivers at lower trip fares. For the individual platform,
encouraging broader participation of drivers in Discount Express services has
the potential to expand the accessible demand pool and improve matching
efficiency, but often at the cost of reduced profit margins. This study aims to
dynamically manage drivers' acceptance of Discount Express from the perspective
of individual platforms. The lack of historical data under the new business
model necessitates online learning. However, early-stage exploration through
trial and error can be costly in practice, highlighting the need for reliable
early-stage performance in real-world deployment. To address these challenges,
this study formulates the decision regarding the proportion of drivers'
acceptance behavior as a continuous control task. In response to the high
stochasticity, the opaque matching mechanisms employed by third-party
integrator, and the limited availability of historical data, we propose a
policy-improved deep deterministic policy gradient (pi-DDPG) framework. The
proposed framework incorporates a refiner module to boost policy performance
during the early training phase, leverages a convolutional long short-term
memory network to effectively capture complex spatiotemporal patterns, and
adopts a prioritized experience replay mechanism to enhance learning
efficiency. A simulator based on a real-world dataset is developed to validate
the effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate
that pi-DDPG achieves superior learning efficiency and significantly reduces
early-stage training losses.

</details>


### [81] [Imbalanced Regression Pipeline Recommendation](https://arxiv.org/abs/2507.11901)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 论文提出了一种名为Meta-IR的元学习框架，用于解决回归任务中的不平衡问题，通过训练元分类器推荐最佳的数据预处理和模型组合。


<details>
  <summary>Details</summary>
Motivation: 回归任务中目标值的不平衡问题缺乏有效解决方案，现有方法需要大量组合测试。

Method: 提出独立和链式两种元分类器训练方法，分别独立或顺序推荐最佳学习算法和重采样策略。

Result: 链式方法表现更优，Meta-IR在实验中优于42种基线配置和AutoML框架。

Conclusion: Meta-IR通过元学习有效解决了回归任务中的不平衡问题，性能优于现有方法。

Abstract: Imbalanced problems are prevalent in various real-world scenarios and are
extensively explored in classification tasks. However, they also present
challenges for regression tasks due to the rarity of certain target values. A
common alternative is to employ balancing algorithms in preprocessing to
address dataset imbalance. However, due to the variety of resampling methods
and learning models, determining the optimal solution requires testing many
combinations. Furthermore, the learning model, dataset, and evaluation metric
affect the best strategies. This work proposes the Meta-learning for Imbalanced
Regression (Meta-IR) framework, which diverges from existing literature by
training meta-classifiers to recommend the best pipeline composed of the
resampling strategy and learning model per task in a zero-shot fashion. The
meta-classifiers are trained using a set of meta-features to learn how to map
the meta-features to the classes indicating the best pipeline. We propose two
formulations: Independent and Chained. Independent trains the meta-classifiers
to separately indicate the best learning algorithm and resampling strategy.
Chained involves a sequential procedure where the output of one meta-classifier
is used as input for another to model intrinsic relationship factors. The
Chained scenario showed superior performance, suggesting a relationship between
the learning algorithm and the resampling strategy per task. Compared with
AutoML frameworks, Meta-IR obtained better results. Moreover, compared with
baselines of six learning algorithms and six resampling algorithms plus no
resampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of
them. The code, data, and further information of the experiments can be found
on GitHub: https://github.com/JusciAvelino/Meta-IR.

</details>


### [82] [Kevin: Multi-Turn RL for Generating CUDA Kernels](https://arxiv.org/abs/2507.11948)
*Carlo Baronio,Pietro Marsella,Ben Pan,Simon Guo,Silas Alberti*

Main category: cs.LG

TL;DR: 论文提出了一种多轮强化学习（RL）方法Kevin，用于生成和优化CUDA内核，显著提升了正确性和性能。


<details>
  <summary>Details</summary>
Motivation: GPU内核编写对AI系统效率至关重要，但具有挑战性且需要迭代优化，因此适合应用强化学习。

Method: 开发了一种多轮RL方法，解决了长轨迹学习和跨轮奖励分配等现实挑战。

Result: Kevin在正确性（56%到82%）和性能（0.53x到1.10x）上显著优于基础模型和前沿模型。

Conclusion: 多轮RL在CUDA内核优化中有效，且串行细化比并行采样更具优势。

Abstract: Writing GPU kernels is a challenging task and critical for AI systems'
efficiency. It is also highly iterative: domain experts write code and improve
performance through execution feedback. Moreover, it presents verifiable
rewards like correctness and speedup, making it a natural environment to apply
Reinforcement Learning (RL). To explicitly incorporate the iterative nature of
this process into training, we develop a flexible multi-turn RL recipe that
addresses unique challenges encountered in real-world settings, such as
learning from long trajectories and effective reward attribution across turns.
We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL
for CUDA kernel generation and optimization. In our evaluation setup, Kevin
shows significant gains over its base model (QwQ-32B), improving correctness of
generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to
1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini
(0.78x). Finally, we study its behavior across test-time scaling axes: we found
scaling serial refinement more beneficial than parallel sampling. In
particular, when given more refinement turns, Kevin shows a higher rate of
improvement.

</details>


### [83] [Resampling strategies for imbalanced regression: a survey and empirical analysis](https://arxiv.org/abs/2507.11902)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 该论文研究了不平衡回归问题，提出了基于回归模型、学习过程和评估指标的分类法，并通过实验展示了平衡策略在不同模型中的优势。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中不平衡回归任务的挑战，填补分类任务中不平衡问题研究之外的空白。

Method: 通过广泛的实验研究，结合多种平衡和预测模型，并使用特定指标评估不平衡回归数据。

Result: 研究提供了不平衡回归策略的新见解，并展示了其对模型学习过程的优势。

Conclusion: 论文提出了不平衡回归的分类法，并指出了未来研究的方向，相关代码和数据已公开。

Abstract: Imbalanced problems can arise in different real-world situations, and to
address this, certain strategies in the form of resampling or balancing
algorithms are proposed. This issue has largely been studied in the context of
classification, and yet, the same problem features in regression tasks, where
target values are continuous. This work presents an extensive experimental
study comprising various balancing and predictive models, and wich uses metrics
to capture important elements for the user and to evaluate the predictive model
in an imbalanced regression data context. It also proposes a taxonomy for
imbalanced regression approaches based on three crucial criteria: regression
model, learning process, and evaluation metrics. The study offers new insights
into the use of such strategies, highlighting the advantages they bring to each
model's learning process, and indicating directions for further studies. The
code, data and further information related to the experiments performed herein
can be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.

</details>


### [84] [From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning](https://arxiv.org/abs/2507.11926)
*Max Hopkins,Sihan Liu,Christopher Ye,Yuichi Yoshida*

Main category: cs.LG

TL;DR: 论文研究了可复制学习算法在强化学习（RL）中的样本效率问题，提出了一种在低范围表格MDP中样本高效的算法，填补了生成模型与交互环境之间的样本成本差距。


<details>
  <summary>Details</summary>
Motivation: 解决可复制学习在强化学习中的探索成本问题，填补现有理论与实际应用之间的差距。

Method: 提出了一种可复制的RL算法，适用于低范围表格MDP，样本复杂度为$	ilde{O}(S^2A)$。

Result: 算法在生成模型和交互环境中的样本成本接近最优，并提供了匹配的下界。

Conclusion: 探索并非可复制学习的显著障碍，样本高效的RL算法是可行的。

Abstract: The epidemic failure of replicability across empirical science and machine
learning has recently motivated the formal study of replicable learning
algorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from
a fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the
design of data-efficient replicable algorithms is now more or less understood.
In contrast, there remain significant gaps in our knowledge for control
settings like reinforcement learning where an agent must interact directly with
a shifting environment. Karbasi et. al show that with access to a generative
model of an environment with $S$ states and $A$ actions (the RL 'batch
setting'), replicably learning a near-optimal policy costs only
$\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a
generative model jumps to $\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the
substantial difficulty of environment exploration. This gap raises a key
question in the broader theory of replicability: Is replicable exploration
inherently more expensive than batch learning? Is sample-efficient replicable
RL even possible?
  In this work, we (nearly) resolve this problem (for low-horizon tabular
MDPs): exploration is not a significant barrier to replicable learning! Our
main result is a replicable RL algorithm on $\tilde{O}(S^2A)$ samples, bridging
the gap between the generative and episodic settings. We complement this with a
matching $\tilde{\Omega}(S^2A)$ lower bound in the generative setting (under
the common parallel sampling assumption) and an unconditional lower bound in
the episodic setting of $\tilde{\Omega}(S^2)$ showcasing the near-optimality of
our algorithm with respect to the state space $S$.

</details>


### [85] [Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning](https://arxiv.org/abs/2507.11928)
*Abhishek Sriram,Neal Tuffy*

Main category: cs.LG

TL;DR: 本文提出了一种机器学习加速的优化框架，用于RF功率放大器设计，减少65%的仿真需求，同时保持±0.3至±0.4 dBm的精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要穷举所有参数组合以实现目标P2dB压缩规格，仿真成本高。本文旨在通过智能采样和机器学习减少仿真需求，提高设计效率。

Method: 结合MaxMin拉丁超立方采样和CatBoost梯度提升，智能探索多维参数空间，仅选择35%的关键仿真点。框架处理ADS网表，执行谐波平衡仿真，并训练CatBoost模型预测P2dB性能。

Result: 在15种PA工作模式下验证，平均R²为0.901，仿真时间减少58.24%至77.78%。

Conclusion: 该框架通过自动化GUI工作流实现快速设计迭代，同时满足生产RF电路的精度要求。

Abstract: This paper presents a machine learning-accelerated optimization framework for
RF power amplifier design that reduces simulation requirements by 65% while
maintaining $\pm0.3$ to $\pm0.4$ dBm accuracy. The proposed method combines
MaxMin Latin Hypercube Sampling with CatBoost gradient boosting to
intelligently explore multidimensional parameter spaces. Instead of
exhaustively simulating all parameter combinations to achieve target P2dB
compression specifications, our approach strategically selects approximately
35% of critical simulation points. The framework processes ADS netlists,
executes harmonic balance simulations on the reduced dataset, and trains a
CatBoost model to predict P2dB performance across the entire design space.
Validation across 15 PA operating modes yields an average $R^2$ of 0.901, with
the system ranking parameter combinations by their likelihood of meeting target
specifications. The integrated solution delivers 58.24% to 77.78% reduction in
simulation time through automated GUI-based workflows, enabling rapid design
iterations without compromising accuracy standards required for production RF
circuits.

</details>


### [86] [Online Training and Pruning of Deep Reinforcement Learning Networks](https://arxiv.org/abs/2507.11975)
*Valentin Frank Ingmar Guenter,Athanasios Sideris*

Main category: cs.LG

TL;DR: 提出了一种在强化学习（RL）中同时训练和剪枝神经网络的方法，结合了Online Feature Extractor Network（OFENet），显著降低了计算和内存复杂度。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络（NN）在强化学习中表现优异，但其计算和内存复杂度高。剪枝方法在监督学习中已成功应用，但在RL中尚未充分探索。

Method: 提出XiNet，通过变分伯努利分布和随机变量ξ对网络单元进行剪枝，结合成本感知的稀疏正则化方案，自动选择超参数。

Result: 在MuJoCo连续控制基准测试中，OFENet剪枝后性能损失极小，且训练时剪枝比从头训练小网络更高效。

Conclusion: 该方法有效结合了RL目标和网络压缩，提升了RL代理的效率和性能。

Abstract: Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms
has been shown to enhance performance when feature extraction networks are used
but the gained performance comes at the significant expense of increased
computational and memory complexity. Neural network pruning methods have
successfully addressed this challenge in supervised learning. However, their
application to RL is underexplored. We propose an approach to integrate
simultaneous training and pruning within advanced RL methods, in particular to
RL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our
networks (XiNet) are trained to solve stochastic optimization problems over the
RL networks' weights and the parameters of variational Bernoulli distributions
for 0/1 Random Variables $\xi$ scaling each unit in the networks. The
stochastic problem formulation induces regularization terms that promote
convergence of the variational parameters to 0 when a unit contributes little
to the performance. In this case, the corresponding structure is rendered
permanently inactive and pruned from its network. We propose a cost-aware,
sparsity-promoting regularization scheme, tailored to the DenseNet architecture
of OFENets expressing the parameter complexity of involved networks in terms of
the parameters of the RVs in these networks. Then, when matching this cost with
the regularization terms, the many hyperparameters associated with them are
automatically selected, effectively combining the RL objectives and network
compression. We evaluate our method on continuous control benchmarks (MuJoCo)
and the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned
considerably with minimal loss in performance. Furthermore, our results confirm
that pruning large networks during training produces more efficient and higher
performing RL agents rather than training smaller networks from scratch.

</details>


### [87] [Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection](https://arxiv.org/abs/2507.11997)
*Tairan Huang,Yili Wang*

Main category: cs.LG

TL;DR: MLED框架通过多级LLM增强，结合文本信息和图结构，提升图欺诈检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略原始文本信息的语义线索，且难以融合LLM处理的文本嵌入与图结构。

Method: 设计多级LLM增强框架（类型级和关系级增强器），融合LLM提取的外部知识与图结构。

Result: 在四个真实数据集上，MLED作为通用框架达到最先进的性能。

Conclusion: MLED有效整合LLM与图结构，显著提升欺诈检测能力。

Abstract: Graph fraud detection has garnered significant attention as Graph Neural
Networks (GNNs) have proven effective in modeling complex relationships within
multimodal data. However, existing graph fraud detection methods typically use
preprocessed node embeddings and predefined graph structures to reveal
fraudsters, which ignore the rich semantic cues contained in raw textual
information. Although Large Language Models (LLMs) exhibit powerful
capabilities in processing textual information, it remains a significant
challenge to perform multimodal fusion of processed textual embeddings with
graph structures. In this paper, we propose a \textbf{M}ulti-level \textbf{L}LM
\textbf{E}nhanced Graph Fraud \textbf{D}etection framework called MLED. In
MLED, we utilize LLMs to extract external knowledge from textual information to
enhance graph fraud detection methods. To integrate LLMs with graph structure
information and enhance the ability to distinguish fraudsters, we design a
multi-level LLM enhanced framework including type-level enhancer and
relation-level enhancer. One is to enhance the difference between the
fraudsters and the benign entities, the other is to enhance the importance of
the fraudsters in different relations. The experiments on four real-world
datasets show that MLED achieves state-of-the-art performance in graph fraud
detection as a generalized framework that can be applied to existing methods.

</details>


### [88] [DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning](https://arxiv.org/abs/2507.12011)
*Yao Lu,Hongyu Gao,Zhuangzhi Chen,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: 论文提出了一种名为DUSE的数据扩展框架，通过不确定性评分函数从相关数据集中筛选有用样本，并结合主动学习策略优化评分器，解决了自动调制识别（AMR）中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在AMR领域表现优异，但需要大量标注数据。实际场景中目标域数据稀缺，手动收集或数据增强方法无法根本解决问题。

Method: 提出DUSE框架，利用不确定性评分函数筛选样本，并通过主动学习策略持续优化评分器。

Result: DUSE在类平衡和类不平衡设置下均优于8种核心集选择基线方法，并展现出对未见模型的强泛化能力。

Conclusion: DUSE有效解决了AMR中的数据稀缺问题，具有实际应用潜力。

Abstract: Although deep neural networks have made remarkable achievements in the field
of automatic modulation recognition (AMR), these models often require a large
amount of labeled data for training. However, in many practical scenarios, the
available target domain data is scarce and difficult to meet the needs of model
training. The most direct way is to collect data manually and perform expert
annotation, but the high time and labor costs are unbearable. Another common
method is data augmentation. Although it can enrich training samples to a
certain extent, it does not introduce new data and therefore cannot
fundamentally solve the problem of data scarcity. To address these challenges,
we introduce a data expansion framework called Dynamic Uncertainty-driven
Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring
function to filter out useful samples from relevant AMR datasets and employs an
active learning strategy to continuously refine the scorer. Extensive
experiments demonstrate that DUSE consistently outperforms 8 coreset selection
baselines in both class-balance and class-imbalance settings. Besides, DUSE
exhibits strong cross-architecture generalization for unseen models.

</details>


### [89] [Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing](https://arxiv.org/abs/2507.12002)
*Alice Zhang,Callihan Bertley,Dawei Liang,Edison Thomaz*

Main category: cs.LG

TL;DR: 该论文提出了一种利用智能手表的多模态数据（音频和惯性数据）检测面对面对话的计算方法，并在实验室和半自然环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 社交互动对人类行为和社会关系至关重要，但现有方法在复杂声学环境中检测对话的能力有限。

Method: 结合机器学习和深度学习方法，通过三种融合方式分析音频和惯性数据，同时考虑语言和非语言线索。

Result: 在实验室和半自然环境中，模型的宏F1分数分别达到82.0±3.0%和77.2±1.8%。

Conclusion: 多模态传感在特定情境下显著提升了对话检测的准确性，证明了其潜力。

Abstract: Social interactions play a crucial role in shaping human behavior,
relationships, and societies. It encompasses various forms of communication,
such as verbal conversation, non-verbal gestures, facial expressions, and body
language. In this work, we develop a novel computational approach to detect a
foundational aspect of human social interactions, in-person verbal
conversations, by leveraging audio and inertial data captured with a commodity
smartwatch in acoustically-challenging scenarios. To evaluate our approach, we
conducted a lab study with 11 participants and a semi-naturalistic study with
24 participants. We analyzed machine learning and deep learning models with 3
different fusion methods, showing the advantages of fusing audio and inertial
data to consider not only verbal cues but also non-verbal gestures in
conversations. Furthermore, we perform a comprehensive set of evaluations
across activities and sampling rates to demonstrate the benefits of multimodal
sensing in specific contexts. Overall, our framework achieved 82.0$\pm$3.0%
macro F1-score when detecting conversations in the lab and 77.2$\pm$1.8% in the
semi-naturalistic setting.

</details>


### [90] [PRISM: Distributed Inference for Foundation Models at Edge](https://arxiv.org/abs/2507.12145)
*Muhammad Azlan Qazi,Alexandros Iosifidis,Qi Zhang*

Main category: cs.LG

TL;DR: PRISM是一种高效、通信优化的分布式Transformer推理策略，适用于边缘设备，显著减少通信和计算开销。


<details>
  <summary>Details</summary>
Motivation: 基础模型在边缘部署面临通信和计算挑战，需要高效策略。

Method: 采用Segment Means表示近似中间特征，重构自注意力机制以减少冗余计算，并设计分区感知的因果掩码。

Result: 在ViT、BERT和GPT-2上测试，通信开销减少99.2%，计算减少51.24%，精度损失小。

Conclusion: PRISM为资源受限的边缘环境提供了可扩展的解决方案。

Abstract: Foundation models (FMs) have achieved remarkable success across a wide range
of applications, from image classification to natural langurage processing, but
pose significant challenges for deployment at edge. This has sparked growing
interest in developing practical and efficient strategies for bringing
foundation models to edge environments. In this work, we propose PRISM, a
communication-efficient and compute-aware strategy for distributed Transformer
inference on edge devices. Our method leverages a Segment Means representation
to approximate intermediate output features, drastically reducing inter-device
communication. Additionally, we restructure the self-attention mechanism to
eliminate redundant computations caused by per-device Key/Value calculation in
position-wise partitioning and design a partition-aware causal masking scheme
tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2
across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and
CBT. Our results demonstrate substantial reductions in communication overhead
(up to 99.2% for BERT at compression rate CR = 128) and per-device computation
(51.24% for BERT at the same setting), with only minor accuracy degradation.
This method offers a scalable and practical solution for deploying foundation
models in distributed resource-constrained environments.

</details>


### [91] [Granular feedback merits sophisticated aggregation](https://arxiv.org/abs/2507.12041)
*Anmol Kagrecha,Henrik Marklund,Potsawee Manakul,Richard Zeckhauser,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 论文探讨了在有限个体反馈下预测群体反馈分布的方法，发现反馈粒度越高，复杂方法比正则化平均更有效。


<details>
  <summary>Details</summary>
Motivation: 研究如何在成本限制下，利用有限个体反馈更准确地预测群体反馈分布，尤其是高粒度反馈的情况。

Method: 比较正则化平均与更复杂的方法在不同反馈粒度下的表现，通过社会态度问题的实证分析验证。

Result: 高粒度反馈（如五点）下，复杂方法所需个体数量约为正则化平均的一半即可达到相同性能。

Conclusion: 反馈粒度影响预测方法的有效性，高粒度反馈更适合复杂方法以节省成本。

Abstract: Human feedback is increasingly used across diverse applications like training
AI models, developing recommender systems, and measuring public opinion -- with
granular feedback often being preferred over binary feedback for its greater
informativeness. While it is easy to accurately estimate a population's
distribution of feedback given feedback from a large number of individuals,
cost constraints typically necessitate using smaller groups. A simple method to
approximate the population distribution is regularized averaging: compute the
empirical distribution and regularize it toward a prior. Can we do better? As
we will discuss, the answer to this question depends on feedback granularity.
  Suppose one wants to predict a population's distribution of feedback using
feedback from a limited number of individuals. We show that, as feedback
granularity increases, one can substantially improve upon predictions of
regularized averaging by combining individuals' feedback in ways more
sophisticated than regularized averaging.
  Our empirical analysis using questions on social attitudes confirms this
pattern. In particular, with binary feedback, sophistication barely reduces the
number of individuals required to attain a fixed level of performance. By
contrast, with five-point feedback, sophisticated methods match the performance
of regularized averaging with about half as many individuals.

</details>


### [92] [Selective Quantization Tuning for ONNX Models](https://arxiv.org/abs/2507.12196)
*Nikolaos Louloudakis,Ajitha Rajan*

Main category: cs.LG

TL;DR: TuneQn是一个工具套件，用于选择性量化、部署和执行ONNX模型，通过多目标优化和性能分析，显著减少精度损失和模型大小。


<details>
  <summary>Details</summary>
Motivation: 解决全量化模型精度下降和低端硬件部署问题，通过选择性量化优化模型性能。

Method: 提出TuneQn套件，结合选择性量化、部署、性能分析和多目标优化，生成优化的ONNX模型。

Result: 在四种ONNX模型上测试，TuneQn显著减少精度损失（最高54.14%）和模型大小（最高72.9%）。

Conclusion: TuneQn有效实现选择性量化和优化，为模型部署提供高效解决方案。

Abstract: Quantization is a process that reduces the precision of deep neural network
models to lower model size and computational demands, often at the cost of
accuracy. However, fully quantized models may exhibit sub-optimal performance
below acceptable levels and face deployment challenges on low-end hardware
accelerators due to practical constraints. To address these issues,
quantization can be selectively applied to only a subset of layers, but
selecting which layers to exclude is non-trivial. To this direction, we propose
TuneQn, a suite enabling selective quantization, deployment and execution of
ONNX models across various CPU and GPU devices, combined with profiling and
multi-objective optimization. TuneQn generates selectively quantized ONNX
models, deploys them on different hardware, measures performance on metrics
like accuracy and size, performs Pareto Front minimization to identify the best
model candidate and visualizes the results. To demonstrate the effectiveness of
TuneQn, we evaluated TuneQn on four ONNX models with two quantization settings
across CPU and GPU devices. As a result, we demonstrated that our utility
effectively performs selective quantization and tuning, selecting ONNX model
candidates with up to a $54.14$% reduction in accuracy loss compared to the
fully quantized model, and up to a $72.9$% model size reduction compared to the
original model.

</details>


### [93] [Information-Theoretic Generalization Bounds of Replay-based Continual Learning](https://arxiv.org/abs/2507.12043)
*Wen Wen,Tieliang Gong,Yunjiao Zhang,Zeyu Gao,Weizhan Zhang,Yong-Jin Liu*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，用于分析基于回放的持续学习方法，揭示了内存缓冲区如何影响泛化性能，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 持续学习（CL）在避免灾难性遗忘的同时从顺序任务中获取知识，但其泛化行为的理论理解有限，尤其是基于回放的方法。

Method: 建立了一个统一的理论框架，推导了一系列信息论界限，分析了内存缓冲区与当前任务的交互如何影响泛化。

Result: 实验证明，利用有限的历史任务样本与当前任务数据结合，可以改善泛化并减少遗忘。

Conclusion: 本文的理论框架和界限为基于回放的持续学习提供了新的理论支持，并展示了其实际应用的有效性。

Abstract: Continual learning (CL) has emerged as a dominant paradigm for acquiring
knowledge from sequential tasks while avoiding catastrophic forgetting.
Although many CL methods have been proposed to show impressive empirical
performance, the theoretical understanding of their generalization behavior
remains limited, particularly for replay-based approaches. In this paper, we
establish a unified theoretical framework for replay-based CL, deriving a
series of information-theoretic bounds that explicitly characterize how the
memory buffer interacts with the current task to affect generalization.
Specifically, our hypothesis-based bounds reveal that utilizing the limited
exemplars of previous tasks alongside the current task data, rather than
exhaustive replay, facilitates improved generalization while effectively
mitigating catastrophic forgetting. Furthermore, our prediction-based bounds
yield tighter and computationally tractable upper bounds of the generalization
gap through the use of low-dimensional variables. Our analysis is general and
broadly applicable to a wide range of learning algorithms, exemplified by
stochastic gradient Langevin dynamics (SGLD) as a representative method.
Comprehensive experimental evaluations demonstrate the effectiveness of our
derived bounds in capturing the generalization dynamics in replay-based CL
settings.

</details>


### [94] [FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling](https://arxiv.org/abs/2507.12053)
*Seanglidet Yean,Jiazu Zhou,Bu-Sung Lee,Markus Schläpfer*

Main category: cs.LG

TL;DR: 提出一种基于条件生成对抗网络（cGANs）的数据驱动方法，用于生成适应动态城市情景的出行流，解决了传统方法依赖静态假设或历史数据的问题。


<details>
  <summary>Details</summary>
Motivation: 城市规划和交通优化需要模拟动态的人类出行模式，但现有方法或依赖历史数据，或假设静态情景，无法适应未来变化。

Method: 结合动态区域大小和土地利用类型，利用cGANs融合历史数据与自适应参数，快速生成可调空间粒度的出行流。

Result: 在新加坡手机数据上的应用表明，该方法性能优于现有方法，且无需大量校准数据或复杂行为建模。

Conclusion: 该方法为动态城市情景下的出行流生成提供了高效、灵活的解决方案。

Abstract: The mobility patterns of people in cities evolve alongside changes in land
use and population. This makes it crucial for urban planners to simulate and
analyze human mobility patterns for purposes such as transportation
optimization and sustainable urban development. Existing generative models
borrowed from machine learning rely heavily on historical trajectories and
often overlook evolving factors like changes in population density and land
use. Mechanistic approaches incorporate population density and facility
distribution but assume static scenarios, limiting their utility for future
projections where historical data for calibration is unavailable. This study
introduces a novel, data-driven approach for generating origin-destination
mobility flows tailored to simulated urban scenarios. Our method leverages
adaptive factors such as dynamic region sizes and land use archetypes, and it
utilizes conditional generative adversarial networks (cGANs) to blend
historical data with these adaptive parameters. The approach facilitates rapid
mobility flow generation with adjustable spatial granularity based on regions
of interest, without requiring extensive calibration data or complex behavior
modeling. The promising performance of our approach is demonstrated by its
application to mobile phone data from Singapore, and by its comparison with
existing methods.

</details>


### [95] [Emergence of Quantised Representations Isolated to Anisotropic Functions](https://arxiv.org/abs/2507.12070)
*George Bird*

Main category: cs.LG

TL;DR: 本文提出了一种基于Spotlight Resonance方法的新方法，用于确定表示对齐，发现网络原语的代数对称性是任务无关表示结构的强预测因子。通过改变激活函数的研究，揭示了离散表示的形成机制。


<details>
  <summary>Details</summary>
Motivation: 研究功能形式选择如何无意中引入归纳偏差，导致表示中出现任务无关的结构，特别是当代形式如何导致连续结构的离散化（量化效应）。

Method: 使用改进的Spotlight Resonance方法，通过改变激活函数进行消融研究，分析离散表示的形成和排列。

Result: 发现离散代数置换等变对称性下表示倾向于离散化，而连续代数正交等变定义下表示保持连续。量化表示与重建误差增加相关。

Conclusion: 功能形式对表示的影响机制为解释性研究提供了新视角，支持离散表示形成的因果模型，并可能解释下游解释性现象。

Abstract: This paper describes a novel methodology for determining representational
alignment, developed upon the existing Spotlight Resonance method. Using this,
it is found that algebraic symmetries of network primitives are a strong
predictor for task-agnostic structure in representations. Particularly, this
new tool is used to gain insight into how discrete representations can form and
arrange in autoencoder models, through an ablation study where only the
activation function is altered. Representations are found to tend to discretise
when the activation functions are defined through a discrete algebraic
permutation-equivariant symmetry. In contrast, they remain continuous under a
continuous algebraic orthogonal-equivariant definition. These findings
corroborate the hypothesis that functional form choices can carry unintended
inductive biases which produce task-independent artefactual structures in
representations, particularly that contemporary forms induce discretisation of
otherwise continuous structure -- a quantisation effect. Moreover, this
supports a general causal model for one mode in which discrete representations
may form, and could constitute a prerequisite for downstream interpretability
phenomena, including grandmother neurons, discrete coding schemes, general
linear features and possibly Superposition. Hence, this tool and proposed
mechanism for the influence of functional form on representations may provide
several insights into emergent interpretability research. Finally, preliminary
results indicate that quantisation of representations appears to correlate with
a measurable increase in reconstruction error, reinforcing previous conjectures
that this collapse can be detrimental.

</details>


### [96] [Measuring Informativeness Gap of (Mis)Calibrated Predictors](https://arxiv.org/abs/2507.12094)
*Yiding Feng,Wei Tang*

Main category: cs.LG

TL;DR: 论文提出了一种衡量预测模型在决策任务中‘有用性’的新框架，称为‘信息差距’，并证明了其优越性和可计算性。


<details>
  <summary>Details</summary>
Motivation: 解决在多预测模型中选择最优模型的问题，尤其是在模型可能都存在校准误差的情况下。

Method: 引入‘信息差距’概念，通过最大标准化收益优势比较预测模型，并给出其对偶表征和一种类似EMD的度量方法。

Result: 新框架统一了现有概念，如U-Calibration和Blackwell信息性，并证明了其样本高效可估计性。

Conclusion: 提出的信息差距框架为模型选择提供了理论基础，并展示了其在实际应用中的潜力。

Abstract: In many applications, decision-makers must choose between multiple predictive
models that may all be miscalibrated. Which model (i.e., predictor) is more
"useful" in downstream decision tasks? To answer this, our first contribution
introduces the notion of the informativeness gap between any two predictors,
defined as the maximum normalized payoff advantage one predictor offers over
the other across all decision-making tasks. Our framework strictly generalizes
several existing notions: it subsumes U-Calibration [KLST-23] and Calibration
Decision Loss [HW-24], which compare a miscalibrated predictor to its
calibrated counterpart, and it recovers Blackwell informativeness [Bla-51,
Bla-53] as a special case when both predictors are perfectly calibrated. Our
second contribution is a dual characterization of the informativeness gap,
which gives rise to a natural informativeness measure that can be viewed as a
relaxed variant of the earth mover's distance (EMD) between two prediction
distributions. We show that this measure satisfies natural desiderata: it is
complete and sound, and it can be estimated sample-efficiently in the
prediction-only access setting. Along the way, we also obtain novel
combinatorial structural results when applying this measure to perfectly
calibrated predictors.

</details>


### [97] [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](https://arxiv.org/abs/2507.12262)
*Zachary James,Joseph Guinness*

Main category: cs.LG

TL;DR: 提出一种使用非平稳核的高斯过程框架，通过神经网络动态调整核参数，提升模型表达能力。


<details>
  <summary>Details</summary>
Motivation: 高斯过程的平稳核限制了模型的表达能力，无法适应复杂数据集。

Method: 将非平稳核参数建模为神经网络的输出，联合训练神经网络和高斯过程。

Result: 在多个数据集上表现优于平稳模型和变分推断的层次模型，且能恢复空间数据的非平稳参数。

Conclusion: 该方法灵活、可扩展，适用于多种非平稳核，无需重新设计优化过程。

Abstract: Gaussian processes have become a popular tool for nonparametric regression
because of their flexibility and uncertainty quantification. However, they
often use stationary kernels, which limit the expressiveness of the model and
may be unsuitable for many datasets. We propose a framework that uses
nonstationary kernels whose parameters vary across the feature space, modeling
these parameters as the output of a neural network that takes the features as
input. The neural network and Gaussian process are trained jointly using the
chain rule to calculate derivatives. Our method clearly describes the behavior
of the nonstationary parameters and is compatible with approximation methods
for scaling to large datasets. It is flexible and easily adapts to different
nonstationary kernels without needing to redesign the optimization procedure.
Our methods are implemented with the GPyTorch library and can be readily
modified. We test a nonstationary variance and noise variant of our method on
several machine learning datasets and find that it achieves better accuracy and
log-score than both a stationary model and a hierarchical model approximated
with variational inference. Similar results are observed for a model with only
nonstationary variance. We also demonstrate our approach's ability to recover
the nonstationary parameters of a spatial dataset.

</details>


### [98] [Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks](https://arxiv.org/abs/2507.12127)
*Ngoc Duy Pham,Thusitha Dayaratne,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.LG

TL;DR: 论文提出了一种基于联邦学习的半监督频谱感知方法，解决了标记数据稀缺和数据投毒攻击的安全问题。


<details>
  <summary>Details</summary>
Motivation: 无线设备数量激增导致频谱稀缺，动态频谱分配（DSA）成为关键解决方案，但传统机器学习方法面临隐私和带宽限制。

Method: 采用半监督联邦学习结合能量检测，解决标记数据不足问题；提出基于疫苗接种的防御机制应对数据投毒攻击。

Result: 实验验证了方法在未标记数据集上的高准确性，并能抵御恶意参与者的数据投毒攻击。

Conclusion: 联邦学习频谱感知（FLSS）在隐私保护和安全性方面表现出色，为未来无线网络提供了可行解决方案。

Abstract: Advancements in wireless and mobile technologies, including 5G advanced and
the envisioned 6G, are driving exponential growth in wireless devices. However,
this rapid expansion exacerbates spectrum scarcity, posing a critical
challenge. Dynamic spectrum allocation (DSA)--which relies on sensing and
dynamically sharing spectrum--has emerged as an essential solution to address
this issue. While machine learning (ML) models hold significant potential for
improving spectrum sensing, their adoption in centralized ML-based DSA systems
is limited by privacy concerns, bandwidth constraints, and regulatory
challenges. To overcome these limitations, distributed ML-based approaches such
as Federated Learning (FL) offer promising alternatives. This work addresses
two key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of
labeled data for training FL models in practical spectrum sensing scenarios is
tackled with a semi-supervised FL approach, combined with energy detection,
enabling model training on unlabeled datasets. Second, we examine the security
vulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our
analysis highlights the shortcomings of existing majority-based defenses in
countering such attacks. To address these vulnerabilities, we propose a novel
defense mechanism inspired by vaccination, which effectively mitigates data
poisoning attacks without relying on majority-based assumptions. Extensive
experiments on both synthetic and real-world datasets validate our solutions,
demonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets
and maintain Byzantine robustness against both targeted and untargeted data
poisoning attacks, even when a significant proportion of participants are
malicious.

</details>


### [99] [HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD](https://arxiv.org/abs/2507.12133)
*Hanwen Liu,Yuhe Huang,Yifeng Gong,Yanjie Zhai,Jiaxuan Lu*

Main category: cs.LG

TL;DR: HyDRA是一种混合双模射频架构，结合优化的VMD和新型CNN、Transformer与Mamba融合架构，支持闭集和开集分类任务，在无线设备识别中实现高效实时认证。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统中的设备识别对安全至关重要，尤其是访问控制等应用。RFFI通过硬件引起的信号失真提供非加密解决方案。

Method: HyDRA结合优化的VMD预处理和CNN、Transformer、Mamba融合架构，使用TDSE建模全局依赖，MLFE实现线性复杂度处理。

Result: 在公开数据集上，HyDRA在闭集场景中达到SOTA准确率，开集分类方法表现稳健，可有效识别未授权设备。

Conclusion: HyDRA在NVIDIA Jetson Xavier NX上实现毫秒级推理速度和低功耗，为实时无线认证提供实用解决方案。

Abstract: Device recognition is vital for security in wireless communication systems,
particularly for applications like access control. Radio Frequency Fingerprint
Identification (RFFI) offers a non-cryptographic solution by exploiting
hardware-induced signal distortions. This paper proposes HyDRA, a Hybrid
Dual-mode RF Architecture that integrates an optimized Variational Mode
Decomposition (VMD) with a novel architecture based on the fusion of
Convolutional Neural Networks (CNNs), Transformers, and Mamba components,
designed to support both closed-set and open-set classification tasks. The
optimized VMD enhances preprocessing efficiency and classification accuracy by
fixing center frequencies and using closed-form solutions. HyDRA employs the
Transformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and
the Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting
to varying conditions. Evaluation on public datasets demonstrates
state-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance
in our proposed open-set classification method, effectively identifying
unauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves
millisecond-level inference speed with low power consumption, providing a
practical solution for real-time wireless authentication in real-world
environments.

</details>


### [100] [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
*Vladimir Bogachev,Vladimir Aletov,Alexander Molozhavenko,Denis Bobkov,Vera Soboleva,Aibek Alanov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出了一种名为RiemannLoRA的新方法，通过将LoRA矩阵视为光滑流形上的元素，解决了初始化策略和低秩矩阵分解中的过参数化问题。


<details>
  <summary>Details</summary>
Motivation: LoRA在参数高效微调中广泛应用，但仍面临初始化策略和过参数化的挑战。

Method: 将固定秩的LoRA矩阵视为光滑流形上的元素，通过流形上的最快损失下降方向提供初始化，并采用数值稳定和计算高效的实现。

Result: 在LLM和扩散模型架构上的实验表明，RiemannLoRA在收敛速度和最终性能上均优于标准LoRA及其先进改进。

Conclusion: RiemannLoRA为LoRA的优化提供了统一框架，显著提升了性能。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

</details>


### [101] [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/abs/2507.12305)
*M. Anwar Ma'sum,Mahardhika Pratama,Savitha Ramasamy,Lin Liu,Habibullah Habibullah,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 提出了一种基于提示的在线持续学习方法，解决了数据隐私和参数增长问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线持续学习中数据隐私约束加剧了灾难性遗忘问题，现有方法存在数据开放政策或参数增长问题。

Method: 包含轻量级提示生成器、可训练缩放移位器、预训练模型泛化保持和硬软更新机制。

Result: 在多个数据集上性能显著优于现有方法，参数较少且训练和推理时间适中。

Conclusion: 该方法在性能和效率上均优于现有技术，代码已开源。

Abstract: The data privacy constraint in online continual learning (OCL), where the
data can be seen only once, complicates the catastrophic forgetting problem in
streaming data. A common approach applied by the current SOTAs in OCL is with
the use of memory saving exemplars or features from previous classes to be
replayed in the current task. On the other hand, the prompt-based approach
performs excellently in continual learning but with the cost of a growing
number of trainable parameters. The first approach may not be applicable in
practice due to data openness policy, while the second approach has the issue
of throughput associated with the streaming data. In this study, we propose a
novel prompt-based method for online continual learning that includes 4 main
components: (1) single light-weight prompt generator as a general knowledge,
(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model
(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our
proposed method achieves significantly higher performance than the current
SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity
analysis shows that our method requires a relatively smaller number of
parameters and achieves moderate training time, inference time, and throughput.
For further study, the source code of our method is available at
https://github.com/anwarmaxsum/PROL.

</details>


### [102] [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
*Antoine Saillenfest,Pirmin Lemberger*

Main category: cs.LG

TL;DR: 论文提出了一种名为LEOPARD的概念擦除方法，通过正交投影从文本表示中移除敏感信息，同时保留其他语义信息，以促进公平性。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，确保神经模型无法从文本表示中推断敏感信息（如性别或种族）是一个关键挑战，尤其是在关注公平性时。

Method: 采用概念擦除方法，学习嵌入空间中的正交投影，使离散概念的条件特征分布在投影后无法区分，同时通过调整投影秩控制信息移除程度。

Result: LEOPARD在经典自然语言处理任务中实现了离散属性的非线性擦除，并在深度非线性分类器中有效减少了偏见。

Conclusion: LEOPARD方法在移除敏感信息和促进公平性方面表现出色，达到了最先进的性能。

Abstract: Ensuring that neural models used in real-world applications cannot infer
sensitive information, such as demographic attributes like gender or race, from
text representations is a critical challenge when fairness is a concern. We
address this issue through concept erasure, a process that removes information
related to a specific concept from distributed representations while preserving
as much of the remaining semantic information as possible. Our approach
involves learning an orthogonal projection in the embedding space, designed to
make the class-conditional feature distributions of the discrete concept to
erase indistinguishable after projection. By adjusting the rank of the
projector, we control the extent of information removal, while its
orthogonality ensures strict preservation of the local structure of the
embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves
state-of-the-art performance in nonlinear erasure of a discrete attribute on
classic natural language processing benchmarks. Furthermore, we demonstrate
that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear
classifiers, thereby promoting fairness.

</details>


### [103] [FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale](https://arxiv.org/abs/2507.12144)
*Boris Bonev,Thorsten Kurth,Ankur Mahesh,Mauro Bisson,Jean Kossaifi,Karthik Kashinath,Anima Anandkumar,William D. Collins,Michael S. Pritchard,Alexander Keller*

Main category: cs.LG

TL;DR: FourCastNet 3通过几何机器学习方法提升全球天气建模，实现高效、准确的概率集合预报，超越传统模型和扩散方法。


<details>
  <summary>Details</summary>
Motivation: 改进全球天气建模，提供更快速、准确的概率预报，同时保持光谱真实性和动态稳定性。

Method: 采用纯卷积神经网络架构，结合模型和数据并行训练范式，适应球面几何特性。

Result: 预报速度提升8至60倍，精度超越传统模型，60天内保持良好概率校准和光谱真实性。

Conclusion: FourCastNet 3在计算效率、概率技能和光谱保真度方面表现优异，有望改进气象预报和预警系统。

Abstract: FourCastNet 3 advances global weather modeling by implementing a scalable,
geometric machine learning (ML) approach to probabilistic ensemble forecasting.
The approach is designed to respect spherical geometry and to accurately model
the spatially correlated probabilistic nature of the problem, resulting in
stable spectra and realistic dynamics across multiple scales. FourCastNet 3
delivers forecasting accuracy that surpasses leading conventional ensemble
models and rivals the best diffusion-based methods, while producing forecasts 8
to 60 times faster than these approaches. In contrast to other ML approaches,
FourCastNet 3 demonstrates excellent probabilistic calibration and retains
realistic spectra, even at extended lead times of up to 60 days. All of these
advances are realized using a purely convolutional neural network architecture
tailored for spherical geometry. Scalable and efficient large-scale training on
1024 GPUs and more is enabled by a novel training paradigm for combined model-
and data-parallelism, inspired by domain decomposition methods in classical
numerical models. Additionally, FourCastNet 3 enables rapid inference on a
single GPU, producing a 90-day global forecast at 0.25{\deg}, 6-hourly
resolution in under 20 seconds. Its computational efficiency, medium-range
probabilistic skill, spectral fidelity, and rollout stability at subseasonal
timescales make it a strong candidate for improving meteorological forecasting
and early warning systems through large ensemble predictions.

</details>


### [104] [Thought Purity: Defense Paradigm For Chain-of-Thought Attack](https://arxiv.org/abs/2507.12314)
*Zihao Xue,Zhen Bi,Long Ma,Zhenlin Hu,Yan Wang,Zhenfang Liu,Qing Sheng,Jie Xiao,Jungang Lou*

Main category: cs.LG

TL;DR: 论文提出了一种防御机制Thought Purity (TP)，用于保护强化学习训练的大型推理模型（LRMs）免受Chain-of-Thought Attack (CoTA)的攻击，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理能力上表现优异，但其在Chain-of-Thought生成过程中易受安全威胁（如后门提示攻击）的影响，导致推理机制被破坏。

Method: 提出Thought Purity (TP)防御范式，包含三个协同组件：安全优化的数据处理流程、强化学习增强的规则约束和自适应监控指标。

Result: TP机制显著提升了模型对CoTA攻击的抵抗力，同时保持了操作效能，为下一代AI架构的安全性提供了解决方案。

Conclusion: TP是首个针对强化学习对齐推理系统中CoTA漏洞的全面防御机制，平衡了安全性与功能性。

Abstract: While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,
Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large
Language Models (LLMs) domain, their susceptibility to security threats remains
a critical vulnerability. This weakness is particularly evident in
Chain-of-Thought (CoT) generation processes, where adversarial methods like
backdoor prompt attacks can systematically subvert the model's core reasoning
mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this
vulnerability through exploiting prompt controllability, simultaneously
degrading both CoT safety and task performance with low-cost interventions. To
address this compounded security-performance vulnerability, we propose Thought
Purity (TP): a defense paradigm that systematically strengthens resistance to
malicious content while preserving operational efficacy. Our solution achieves
this through three synergistic components: (1) a safety-optimized data
processing pipeline (2) reinforcement learning-enhanced rule constraints (3)
adaptive monitoring metrics. Our approach establishes the first comprehensive
defense mechanism against CoTA vulnerabilities in reinforcement
learning-aligned reasoning systems, significantly advancing the
security-functionality equilibrium for next-generation AI architectures.

</details>


### [105] [Multi-Component VAE with Gaussian Markov Random Field](https://arxiv.org/abs/2507.12165)
*Fouad Oubari,Mohamed El-Baha,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: 提出了一种新的生成模型GMRF MCVAE，通过嵌入高斯马尔可夫随机场来显式建模多组件关系，提升了生成的结构一致性。


<details>
  <summary>Details</summary>
Motivation: 当前多组件变分自编码器依赖简化聚合策略，忽略了关键细节，导致生成组件间结构一致性不足。

Method: 在变分自编码器的先验和后验分布中嵌入高斯马尔可夫随机场，显式建模跨组件关系。

Result: 在合成Copula数据集、PolyMNIST基准和真实BIKED数据集上表现优异，显著提升结构一致性。

Conclusion: GMRF MCVAE适用于需要多组件一致性的实际应用，表现优于现有方法。

Abstract: Multi-component datasets with intricate dependencies, like industrial
assemblies or multi-modal imaging, challenge current generative modeling
techniques. Existing Multi-component Variational AutoEncoders typically rely on
simplified aggregation strategies, neglecting critical nuances and consequently
compromising structural coherence across generated components. To explicitly
address this gap, we introduce the Gaussian Markov Random Field Multi-Component
Variational AutoEncoder , a novel generative framework embedding Gaussian
Markov Random Fields into both prior and posterior distributions. This design
choice explicitly models cross-component relationships, enabling richer
representation and faithful reproduction of complex interactions. Empirically,
our GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula
dataset specifically constructed to evaluate intricate component relationships,
demonstrates competitive results on the PolyMNIST benchmark, and significantly
enhances structural coherence on the real-world BIKED dataset. Our results
indicate that the GMRF MCVAE is especially suited for practical applications
demanding robust and realistic modeling of multi-component coherence

</details>


### [106] [NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data](https://arxiv.org/abs/2507.12412)
*Dzung Dinh,Boqi Chen,Marc Niethammer,Junier Oliva*

Main category: cs.LG

TL;DR: NOCTA是一种非贪婪的目标成本权衡获取方法，用于在资源受限的预测任务中动态选择最具信息量的特征。


<details>
  <summary>Details</summary>
Motivation: 在医疗等资源受限的应用中，动态选择最具信息量的特征以平衡成本和时间是关键挑战。

Method: 提出了NOCTA方法，包括非参数（NOCTA-NP）和参数（NOCTA-P）两种互补的估计器。

Result: 实验表明，NOCTA在合成和真实医疗数据集上优于现有基线。

Conclusion: NOCTA能有效解决动态特征获取问题，适用于资源受限的预测任务。

Abstract: In many critical applications, resource constraints limit the amount of
information that can be gathered to make predictions. For example, in
healthcare, patient data often spans diverse features ranging from lab tests to
imaging studies. Each feature may carry different information and must be
acquired at a respective cost of time, money, or risk to the patient. Moreover,
temporal prediction tasks, where both instance features and labels evolve over
time, introduce additional complexity in deciding when or what information is
important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff
Acquisition method that sequentially acquires the most informative features at
inference time while accounting for both temporal dynamics and acquisition
cost. We first introduce a cohesive estimation target for our NOCTA setting,
and then develop two complementary estimators: 1) a non-parametric method based
on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric
method that directly predicts the utility of potential acquisitions (NOCTA-P).
Experiments on synthetic and real-world medical datasets demonstrate that both
NOCTA variants outperform existing baselines.

</details>


### [107] [RadioDiff-3D: A 3D$\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication](https://arxiv.org/abs/2507.12166)
*Xiucheng Wang,Qiming Zhang,Nan Cheng,Junting Chen,Zezhong Zhang,Zan Li,Shuguang Cui,Xuemin Shen*

Main category: cs.LG

TL;DR: 论文提出了UrbanRadio3D数据集和RadioDiff-3D方法，用于构建高分辨率3D无线电地图，解决了现有方法在方向、时间和垂直空间变化上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有无线电地图构建方法仅关注固定2D平面的路径损耗预测，忽略了方向、时间和垂直空间变化等关键参数，限制了其泛化能力。

Method: 通过射线追踪构建UrbanRadio3D数据集，并提出基于3D卷积的UNet和扩散模型RadioDiff-3D，支持辐射感知和非感知场景。

Result: RadioDiff-3D在UrbanRadio3D数据集上表现优异，能够构建丰富的高维无线电地图。

Conclusion: 该工作为未来3D环境感知通信研究提供了基础数据集和基准方法。

Abstract: Radio maps (RMs) serve as a critical foundation for enabling
environment-aware wireless communication, as they provide the spatial
distribution of wireless channel characteristics. Despite recent progress in RM
construction using data-driven approaches, most existing methods focus solely
on pathloss prediction in a fixed 2D plane, neglecting key parameters such as
direction of arrival (DoA), time of arrival (ToA), and vertical spatial
variations. Such a limitation is primarily due to the reliance on static
learning paradigms, which hinder generalization beyond the training data
distribution. To address these challenges, we propose UrbanRadio3D, a
large-scale, high-resolution 3D RM dataset constructed via ray tracing in
realistic urban environments. UrbanRadio3D is over 37$\times$3 larger than
previous datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,
forming a novel 3D$\times$33D dataset with 7$\times$3 more height layers than
prior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet
with 3D convolutional operators is proposed. Moreover, we further introduce
RadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D
convolutional architecture. RadioDiff-3D supports both radiation-aware
scenarios with known transmitter locations and radiation-unaware settings based
on sparse spatial observations. Extensive evaluations on UrbanRadio3D validate
that RadioDiff-3D achieves superior performance in constructing rich,
high-dimensional radio maps under diverse environmental dynamics. This work
provides a foundational dataset and benchmark for future research in 3D
environment-aware communication. The dataset is available at
https://github.com/UNIC-Lab/UrbanRadio3D.

</details>


### [108] [Mixture of Raytraced Experts](https://arxiv.org/abs/2507.12419)
*Andrea Perin,Giacomo Lagomarsini,Claudio Gallicchio,Giuseppe Nuti*

Main category: cs.LG

TL;DR: 提出了一种动态选择专家序列的混合专家架构（MoE），通过可变计算图提升预测精度，无需负载均衡机制，实验显示训练周期减少10%-40%。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构对每个样本的计算量固定，限制了灵活性和效率。本文旨在设计一种动态调整计算深度和宽度的MoE架构。

Method: 采用类似RNN的序列展开方法训练模型，动态选择专家序列，生成可变计算图。

Result: 实验表明，该方法在保持或提高精度的同时，训练周期减少10%-40%。

Conclusion: 该方法为MoE领域提供了新方向，可能设计出更快、表达能力更强的模型。

Abstract: We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts
(MoE) architecture which can dynamically select sequences of experts, producing
computational graphs of variable width and depth. Existing MoE architectures
generally require a fixed amount of computation for a given sample. Our
approach, in contrast, yields predictions with increasing accuracy as the
computation cycles through the experts' sequence. We train our model by
iteratively sampling from a set of candidate experts, unfolding the sequence
akin to how Recurrent Neural Networks are trained. Our method does not require
load-balancing mechanisms, and preliminary experiments show a reduction in
training epochs of 10\% to 40\% with a comparable/higher accuracy. These
results point to new research directions in the field of MoEs, allowing the
design of potentially faster and more expressive models. The code is available
at https://github.com/nutig/RayTracing

</details>


### [109] [Explainable Evidential Clustering](https://arxiv.org/abs/2507.12192)
*Victor F. Lopes de Souza,Karima Bakhti,Sofiane Ramdani,Denis Mottet,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 本文探讨了基于Dempster-Shafer理论的证据聚类方法，并提出了一种解释其结果的框架。通过引入代表性和效用函数，定义了证据错误性，并提出了IEMM算法，为高风险领域提供可解释的决策树解释。


<details>
  <summary>Details</summary>
Motivation: 现实数据常存在不确定性和不精确性，传统方法难以处理。证据聚类能解决这些问题，但其结果解释问题尚未充分研究，尤其是在医疗等高风险领域。

Method: 基于代表性条件，通过效用函数扩展部分标签，定义证据错误性作为解释成本，并提出IEMM算法构建解释器。

Result: 在合成和真实数据上验证，IEMM算法能提供高达93%满意度的解释。

Conclusion: IEMM算法为证据聚类提供了可解释且谨慎的决策树解释，适用于高风险领域。

Abstract: Unsupervised classification is a fundamental machine learning problem.
Real-world data often contain imperfections, characterized by uncertainty and
imprecision, which are not well handled by traditional methods. Evidential
clustering, based on Dempster-Shafer theory, addresses these challenges. This
paper explores the underexplored problem of explaining evidential clustering
results, which is crucial for high-stakes domains such as healthcare. Our
analysis shows that, in the general case, representativity is a necessary and
sufficient condition for decision trees to serve as abductive explainers.
Building on the concept of representativity, we generalize this idea to
accommodate partial labeling through utility functions. These functions enable
the representation of "tolerable" mistakes, leading to the definition of
evidential mistakeness as explanation cost and the construction of explainers
tailored to evidential classifiers. Finally, we propose the Iterative
Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable
and cautious decision tree explanations for evidential clustering functions. We
validate the proposed algorithm on synthetic and real-world data. Taking into
account the decision-maker's preferences, we were able to provide an
explanation that was satisfactory up to 93% of the time.

</details>


### [110] [Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation](https://arxiv.org/abs/2507.12218)
*Tomohisa Okazaki*

Main category: cs.LG

TL;DR: 论文提出了一种基于线性基函数组合的物理信息线性模型（PILM），用于解决偏微分方程（PDEs）的正反问题，并通过地壳应变率估计验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法解决PDEs和估计参数或边界条件时面临计算复杂性和数值不稳定性问题，PILM通过线性模型提供解析解框架，简化了求解过程。

Method: PILM使用线性基函数组合表示解，通过最小化PDEs、边界条件和数据的残差和来求解问题，并比较了物理正则化和数学正则化的效果。

Result: PILM在正反问题中表现良好，数学正则化在贝叶斯视角下优于物理正则化。

Conclusion: PILM为线性正反问题、欠定系统和物理正则化提供了可解析求解的框架，具有广泛的应用潜力。

Abstract: Many physical systems are described by partial differential equations (PDEs),
and solving these equations and estimating their coefficients or boundary
conditions (BCs) from observational data play a crucial role in understanding
the associated phenomena. Recently, a machine learning approach known as
physics-informed neural network, which solves PDEs using neural networks by
minimizing the sum of residuals from the PDEs, BCs, and data, has gained
significant attention in the scientific community. In this study, we
investigate a physics-informed linear model (PILM) that uses linear
combinations of basis functions to represent solutions, thereby enabling an
analytical representation of optimal solutions. The PILM was formulated and
verified for illustrative forward and inverse problems including cases with
uncertain BCs. Furthermore, the PILM was applied to estimate crustal strain
rates using geodetic data. Specifically, physical regularization that enforces
elastic equilibrium on the velocity fields was compared with mathematical
regularization that imposes smoothness constraints. From a Bayesian
perspective, mathematical regularization exhibited superior performance. The
PILM provides an analytically solvable framework applicable to linear forward
and inverse problems, underdetermined systems, and physical regularization.

</details>


### [111] [Optimizers Qualitatively Alter Solutions And We Should Leverage This](https://arxiv.org/abs/2507.12224)
*Razvan Pascanu,Clare Lyle,Ionut-Vlad Modoranu,Naima Elosegui Borras,Dan Alistarh,Petar Velickovic,Sarath Chandar,Soham De,James Martens*

Main category: cs.LG

TL;DR: 论文探讨了深度神经网络（DNNs）优化器的角色，指出其不仅影响收敛速度，还影响学习解的性质，呼吁社区关注优化器的设计以引导模型特性。


<details>
  <summary>Details</summary>
Motivation: 早期对DNNs可行性的质疑源于其非线性特性导致无法保证收敛到唯一全局最小。尽管实证表明DNNs优化表现良好，但社区过于关注效率而忽视了优化器对解性质的影响。

Method: 通过分析现有优化器的行为，提出优化器不仅影响收敛速度，还能编码归纳偏置并改变模型的有效表达能力。

Result: 优化器可以成为学习过程中编码期望特性的有效工具，而不仅仅是提高效率的手段。

Conclusion: 呼吁社区更深入地理解优化器的偏置，并设计新的优化器以引导特定解性质，而不仅关注收敛速度。

Abstract: Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not
guarantee convergence to a unique global minimum of the loss when using
optimizers relying only on local information, such as SGD. Indeed, this was a
primary source of skepticism regarding the feasibility of DNNs in the early
days of the field. The past decades of progress in deep learning have revealed
this skepticism to be misplaced, and a large body of empirical evidence shows
that sufficiently large DNNs following standard training protocols exhibit
well-behaved optimization dynamics that converge to performant solutions. This
success has biased the community to use convex optimization as a mental model
for learning, leading to a focus on training efficiency, either in terms of
required iteration, FLOPs or wall-clock time, when improving optimizers. We
argue that, while this perspective has proven extremely fruitful, another
perspective specific to DNNs has received considerably less attention: the
optimizer not only influences the rate of convergence, but also the qualitative
properties of the learned solutions. Restated, the optimizer can and will
encode inductive biases and change the effective expressivity of a given class
of models. Furthermore, we believe the optimizer can be an effective way of
encoding desiderata in the learning process. We contend that the community
should aim at understanding the biases of already existing methods, as well as
aim to build new optimizers with the explicit intent of inducing certain
properties of the solution, rather than solely judging them based on their
convergence rates. We hope our arguments will inspire research to improve our
understanding of how the learning process can impact the type of solution we
converge to, and lead to a greater recognition of optimizers design as a
critical lever that complements the roles of architecture and data in shaping
model outcomes.

</details>


### [112] [Robust Causal Discovery in Real-World Time Series with Power-Laws](https://arxiv.org/abs/2507.12257)
*Matteo Tusoni,Giuseppe Masi,Andrea Coletta,Aldo Glielmo,Viviana Arrigoni,Novella Bartolini*

Main category: cs.LG

TL;DR: 提出了一种基于功率谱特征的鲁棒因果发现方法，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索随机时间序列中的因果关系具有广泛应用，但现有方法对噪声敏感，容易产生误导性结果。

Method: 利用时间序列的功率谱分布特征，提取因果信号，构建鲁棒的因果发现方法。

Result: 在合成基准和真实数据集上优于现有方法，验证了其鲁棒性和实用性。

Conclusion: 该方法通过功率谱特征有效提升了因果发现的准确性和鲁棒性。

Abstract: Exploring causal relationships in stochastic time series is a challenging yet
crucial task with a vast range of applications, including finance, economics,
neuroscience, and climate science. Many algorithms for Causal Discovery (CD)
have been proposed, but they often exhibit a high sensitivity to noise,
resulting in misleading causal inferences when applied to real data. In this
paper, we observe that the frequency spectra of typical real-world time series
follow a power-law distribution, notably due to an inherent self-organizing
behavior. Leveraging this insight, we build a robust CD method based on the
extraction of power -law spectral features that amplify genuine causal signals.
Our method consistently outperforms state-of-the-art alternatives on both
synthetic benchmarks and real-world datasets with known causal structures,
demonstrating its robustness and practical relevance.

</details>


### [113] [RegCL: Continual Adaptation of Segment Anything Model via Model Merging](https://arxiv.org/abs/2507.12297)
*Yuan-Chen Shu,Zhiwei Lin,Yongtao Wang*

Main category: cs.LG

TL;DR: 提出了一种名为RegCL的新型非回放持续学习框架，通过模型合并实现多领域知识的高效整合，解决了Segment Anything Model (SAM)在特定领域中的性能限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用基于适配器的一步适应范式，但这些方法可能因领域特定性导致性能下降，且存在灾难性遗忘问题，限制了模型的可扩展性。

Method: RegCL将模型合并算法引入持续学习范式，通过合并不同领域训练的SAM适配模块（如LoRA模块）参数，并以权重优化为指导，最小化预测差异。

Result: 实验结果表明，RegCL在多个下游数据集上实现了良好的持续学习性能，验证了其在动态场景中的有效性。

Conclusion: RegCL通过模型合并有效整合多领域知识，同时保持参数效率，无需历史数据存储，模型大小恒定。

Abstract: To address the performance limitations of the Segment Anything Model (SAM) in
specific domains, existing works primarily adopt adapter-based one-step
adaptation paradigms. However, some of these methods are specific developed for
specific domains. If used on other domains may lead to performance degradation.
This issue of catastrophic forgetting severely limits the model's scalability.
To address this issue, this paper proposes RegCL, a novel non-replay continual
learning (CL) framework designed for efficient multi-domain knowledge
integration through model merging. Specifically, RegCL incorporates the model
merging algorithm into the continual learning paradigm by merging the
parameters of SAM's adaptation modules (e.g., LoRA modules) trained on
different domains. The merging process is guided by weight optimization, which
minimizes prediction discrepancies between the merged model and each of the
domain-specific models. RegCL effectively consolidates multi-domain knowledge
while maintaining parameter efficiency, i.e., the model size remains constant
regardless of the number of tasks, and no historical data storage is required.
Experimental results demonstrate that RegCL achieves favorable continual
learning performance across multiple downstream datasets, validating its
effectiveness in dynamic scenarios.

</details>


### [114] [Heat Kernel Goes Topological](https://arxiv.org/abs/2507.12380)
*Maximilian Krahn,Vikas Garg*

Main category: cs.LG

TL;DR: 提出了一种基于组合复形（CCs）的新型拓扑框架，通过引入拉普拉斯算子计算热核，解决了高阶消息传递的计算开销问题，提升了计算效率和表达能力。


<details>
  <summary>Details</summary>
Motivation: 拓扑神经网络虽然强大，但高阶消息传递导致计算开销大，需要一种更高效的方法来捕捉多尺度信息并实现置换等变表示。

Method: 引入组合复形上的拉普拉斯算子，计算热核作为节点描述符，支持多尺度信息捕捉和置换等变表示，易于与现代Transformer架构集成。

Result: 理论证明该方法能区分任意非同构组合复形；实验显示其在计算效率和拓扑结构区分能力上优于现有方法，并在分子数据集上表现优异。

Conclusion: 该工作通过提供表达力强且可扩展的表示，推动了拓扑深度学习的发展，为分子分类和性质预测任务开辟了新途径。

Abstract: Topological neural networks have emerged as powerful successors of graph
neural networks. However, they typically involve higher-order message passing,
which incurs significant computational expense. We circumvent this issue with a
novel topological framework that introduces a Laplacian operator on
combinatorial complexes (CCs), enabling efficient computation of heat kernels
that serve as node descriptors. Our approach captures multiscale information
and enables permutation-equivariant representations, allowing easy integration
into modern transformer-based architectures.
  Theoretically, the proposed method is maximally expressive because it can
distinguish arbitrary non-isomorphic CCs. Empirically, it significantly
outperforms existing topological methods in terms of computational efficiency.
Besides demonstrating competitive performance with the state-of-the-art
descriptors on standard molecular datasets, it exhibits superior capability in
distinguishing complex topological structures and avoiding blind spots on
topological benchmarks. Overall, this work advances topological deep learning
by providing expressive yet scalable representations, thereby opening up
exciting avenues for molecular classification and property prediction tasks.

</details>


### [115] [Improving Reinforcement Learning Sample-Efficiency using Local Approximation](https://arxiv.org/abs/2507.12383)
*Mohit Prashant,Arvind Easwaran*

Main category: cs.LG

TL;DR: 本文提出了在无限时间马尔可夫决策过程（MDP）中更严格的样本复杂度PAC界限，通过近似原始MDP减少样本复杂度，并扩展到无模型设置。


<details>
  <summary>Details</summary>
Motivation: 现有文献中的样本复杂度界限不够精确，本文旨在通过分析状态间的相关性，提出更优的样本复杂度界限。

Method: 通过构建原始MDP的子集近似，减少样本复杂度，并设计PAC-MDP算法扩展到无模型设置。

Result: 样本复杂度降低到O(SA log A)时间步，实验表明显著优于现有方法。

Conclusion: 提出的方法显著提升了样本复杂度界限，适用于无限时间MDP和无模型设置。

Abstract: In this study, we derive Probably Approximately Correct (PAC) bounds on the
asymptotic sample-complexity for RL within the infinite-horizon Markov Decision
Process (MDP) setting that are sharper than those in existing literature. The
premise of our study is twofold: firstly, the further two states are from each
other, transition-wise, the less relevant the value of the first state is when
learning the $\epsilon$-optimal value of the second; secondly, the amount of
'effort', sample-complexity-wise, expended in learning the $\epsilon$-optimal
value of a state is independent of the number of samples required to learn the
$\epsilon$-optimal value of a second state that is a sufficient number of
transitions away from the first. Inversely, states within each other's vicinity
have values that are dependent on each other and will require a similar number
of samples to learn. By approximating the original MDP using smaller MDPs
constructed using subsets of the original's state-space, we are able to reduce
the sample-complexity by a logarithmic factor to $O(SA \log A)$ timesteps,
where $S$ and $A$ are the state and action space sizes. We are able to extend
these results to an infinite-horizon, model-free setting by constructing a
PAC-MDP algorithm with the aforementioned sample-complexity. We conclude with
showing how significant the improvement is by comparing our algorithm against
prior work in an experimental setting.

</details>


### [116] [Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries](https://arxiv.org/abs/2507.12384)
*Bo Wen,Guoyun Gao,Zhicheng Xu,Ruibin Mao,Xiaojuan Qi,X. Sharon Hu,Xunzhao Yin,Can Li*

Main category: cs.LG

TL;DR: 提出了一种基于$MoS_2$闪存模拟CAM的软树模型硬件-软件协同设计方法，显著提升了树模型的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 人工智能的快速发展引发了对模型可信度的担忧，尤其是在可解释性和鲁棒性方面。树模型虽在可解释性和准确性上表现优异，但传统硬件实现存在计算成本高、易受设备变化和对抗攻击影响的问题。

Method: 采用$MoS_2$闪存模拟CAM硬件，结合软树模型，利用其固有的软边界特性，实现了高效的推理。

Result: 实验表明，该方法在WDBC数据集上达到96%的准确率，且在设备阈值变化10%时，MNIST数据集的准确率仅下降0.6%，远优于传统决策树的45.3%下降。

Conclusion: 该研究为提升AI的可信度和效率提供了专用硬件设计的新思路。

Abstract: The rapid advancement of artificial intelligence has raised concerns
regarding its trustworthiness, especially in terms of interpretability and
robustness. Tree-based models like Random Forest and XGBoost excel in
interpretability and accuracy for tabular data, but scaling them remains
computationally expensive due to poor data locality and high data dependence.
Previous efforts to accelerate these models with analog content addressable
memory (CAM) have struggled, due to the fact that the difficult-to-implement
sharp decision boundaries are highly susceptible to device variations, which
leads to poor hardware performance and vulnerability to adversarial attacks.
This work presents a novel hardware-software co-design approach using $MoS_2$
Flash-based analog CAM with inherent soft boundaries, enabling efficient
inference with soft tree-based models. Our soft tree model inference
experiments on $MoS_2$ analog CAM arrays show this method achieves exceptional
robustness against device variation and adversarial attacks while achieving
state-of-the-art accuracy. Specifically, our fabricated analog CAM arrays
achieve $96\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,
while maintaining decision explainability. Our experimentally calibrated model
validated only a $0.6\%$ accuracy drop on the MNIST dataset under $10\%$ device
threshold variation, compared to a $45.3\%$ drop for traditional decision
trees. This work paves the way for specialized hardware that enhances AI's
trustworthiness and efficiency.

</details>


### [117] [ROC-n-reroll: How verifier imperfection affects test-time scaling](https://arxiv.org/abs/2507.12399)
*Florian E. Dorner,Yatong Chen,André F. Cruz,Fanny Yang*

Main category: cs.LG

TL;DR: 论文研究了测试时扩展（test-time scaling）中验证器不完美对性能的影响，通过ROC曲线的几何特性分析了Best-of-N和拒绝采样的实例级准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对验证器不完美如何影响测试时扩展性能的理论理解，本文旨在填补这一空白。

Method: 通过分析验证器ROC曲线的几何特性，比较Best-of-N和拒绝采样在不同计算条件下的性能。

Result: 拒绝采样在固定计算下优于Best-of-N，但在无限计算下两者性能相同，由ROC曲线原点附近的斜率决定。

Conclusion: 验证器ROC曲线的几何特性决定了测试时扩展的性能，实验结果支持了理论分析。

Abstract: Test-time scaling aims to improve language model performance by leveraging
additional compute during inference. While many works have empirically studied
techniques like Best-of-N (BoN) and rejection sampling that make use of a
verifier to enable test-time scaling, there is little theoretical understanding
of how verifier imperfection affects performance. In this work, we address this
gap. Specifically, we prove how instance-level accuracy of these methods is
precisely characterized by the geometry of the verifier's ROC curve.
Interestingly, while scaling is determined by the local geometry of the ROC
curve for rejection sampling, it depends on global properties of the ROC curve
for BoN. As a consequence when the ROC curve is unknown, it is impossible to
extrapolate the performance of rejection sampling based on the low-compute
regime. Furthermore, while rejection sampling outperforms BoN for fixed
compute, in the infinite-compute limit both methods converge to the same level
of accuracy, determined by the slope of the ROC curve near the origin. Our
theoretical results are confirmed by experiments on GSM8K using different
versions of Llama and Qwen to generate and verify solutions.

</details>


### [118] [Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks](https://arxiv.org/abs/2507.12435)
*Yi Li,David Mccoy,Nolan Gunter,Kaitlyn Lee,Alejandro Schuler,Mark van der Laan*

Main category: cs.LG

TL;DR: 论文提出了一种名为TDA的新框架，将TMLE直接嵌入神经网络参数空间，解决了现有方法在因果推断中的偏差问题，并支持多参数目标。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络在预测方面强大，但在因果参数推断（如治疗效果或生存曲线）上缺乏有效性。现有方法存在局限性，如目标损失无法保证解决高效影响函数方程或计算成本高。

Method: TDA通过分区模型参数，冻结大部分参数，仅更新一小部分“目标”子集，并沿目标梯度迭代更新，从而消除一阶偏差并生成渐近有效的置信区间。

Result: 在IHDP数据集和模拟生存数据上，TDA相比标准神经网络估计器和现有后处理方法，减少了偏差并提高了覆盖率。

Conclusion: TDA为复杂多参数目标的深度架构提供了一种直接、可扩展的严格因果推断途径。

Abstract: Modern deep neural networks are powerful predictive tools yet often lack
valid inference for causal parameters, such as treatment effects or entire
survival curves. While frameworks like Double Machine Learning (DML) and
Targeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,
existing neural implementations either rely on "targeted losses" that do not
guarantee solving the efficient influence function equation or computationally
expensive post-hoc "fluctuations" for multi-parameter settings. We propose
Targeted Deep Architectures (TDA), a new framework that embeds TMLE directly
into the network's parameter space with no restrictions on the backbone
architecture. Specifically, TDA partitions model parameters - freezing all but
a small "targeting" subset - and iteratively updates them along a targeting
gradient, derived from projecting the influence functions onto the span of the
gradients of the loss with respect to weights. This procedure yields plug-in
estimates that remove first-order bias and produce asymptotically valid
confidence intervals. Crucially, TDA easily extends to multi-dimensional causal
estimands (e.g., entire survival curves) by merging separate targeting
gradients into a single universal targeting update. Theoretically, TDA inherits
classical TMLE properties, including double robustness and semiparametric
efficiency. Empirically, on the benchmark IHDP dataset (average treatment
effects) and simulated survival data with informative censoring, TDA reduces
bias and improves coverage relative to both standard neural-network estimators
and prior post-hoc approaches. In doing so, TDA establishes a direct, scalable
pathway toward rigorous causal inference within modern deep architectures for
complex multi-parameter targets.

</details>


### [119] [A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning](https://arxiv.org/abs/2507.12439)
*Daniel Commey,Rebecca A. Sarpong,Griffith S. Klogo,Winful Bagyl-Bac,Garth V. Crosby*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的贝叶斯激励机制，通过经济手段主动防御联邦学习中的数据投毒攻击，确保恶意行为在经济上不划算。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的开放参与性使其容易受到数据投毒攻击，现有防御方法多为被动且计算成本高，假设诚实参与者占多数。

Method: 设计了一个贝叶斯博弈模型，服务器使用小型私有验证数据集验证更新质量后发放奖励，满足个体理性（IR）和激励相容性（IC）。

Result: 在非独立同分布的MNIST和FashionMNIST数据集上，即使面对50%的标签翻转攻击，模型准确率仍保持96.7%，显著优于标准FedAvg。

Conclusion: 该机制计算轻量、预算可控，易于集成到现有联邦学习框架中，为构建经济上稳健且可持续的联邦学习生态系统提供了实用方案。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy. However, its
open-participation nature exposes it to data-poisoning attacks, in which
malicious actors submit corrupted model updates to degrade the global model.
Existing defenses are often reactive, relying on statistical aggregation rules
that can be computationally expensive and that typically assume an honest
majority. This paper introduces a proactive, economic defense: a lightweight
Bayesian incentive mechanism that makes malicious behavior economically
irrational. Each training round is modeled as a Bayesian game of incomplete
information in which the server, acting as the principal, uses a small, private
validation dataset to verify update quality before issuing payments. The design
satisfies Individual Rationality (IR) for benevolent clients, ensuring their
participation is profitable, and Incentive Compatibility (IC), making poisoning
an economically dominated strategy. Extensive experiments on non-IID partitions
of MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping
adversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3
percentage points lower than in a scenario with 30% label-flipping adversaries.
This outcome is 51.7 percentage points better than standard FedAvg, which
collapses under the same 50% attack. The mechanism is computationally light,
budget-bounded, and readily integrates into existing FL frameworks, offering a
practical route to economically robust and sustainable FL ecosystems.

</details>


### [120] [Cost-aware Stopping for Bayesian Optimization](https://arxiv.org/abs/2507.12453)
*Qian Xie,Linda Cai,Alexander Terenin,Peter I. Frazier,Ziv Scully*

Main category: cs.LG

TL;DR: 提出了一种成本感知的停止规则，用于贝叶斯优化，无需启发式调整，并在理论和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在贝叶斯优化中，如何在高成本的黑盒函数评估中适时停止是一个重要问题，现有方法缺乏成本保证。

Method: 基于Pandora's Box Gittins Index (PBGI)和log expected improvement per cost的理论联系，提出了一种自适应停止规则。

Result: 实验表明，该规则与PBGI结合时，在成本调整的简单遗憾指标上优于其他方法。

Conclusion: 提出的停止规则在理论和实践中均表现出色，适用于高成本优化任务。

Abstract: In automated machine learning, scientific discovery, and other applications
of Bayesian optimization, deciding when to stop evaluating expensive black-box
functions is an important practical consideration. While several adaptive
stopping rules have been proposed, in the cost-aware setting they lack
guarantees ensuring they stop before incurring excessive function evaluation
costs. We propose a cost-aware stopping rule for Bayesian optimization that
adapts to varying evaluation costs and is free of heuristic tuning. Our rule is
grounded in a theoretical connection to state-of-the-art cost-aware acquisition
functions, namely the Pandora's Box Gittins Index (PBGI) and log expected
improvement per cost. We prove a theoretical guarantee bounding the expected
cumulative evaluation cost incurred by our stopping rule when paired with these
two acquisition functions. In experiments on synthetic and empirical tasks,
including hyperparameter optimization and neural architecture size search, we
show that combining our stopping rule with the PBGI acquisition function
consistently matches or outperforms other acquisition-function--stopping-rule
pairs in terms of cost-adjusted simple regret, a metric capturing trade-offs
between solution quality and cumulative evaluation cost.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [121] [A Study on the Application of Artificial Intelligence in Ecological Design](https://arxiv.org/abs/2507.11595)
*Hengyue Zhao*

Main category: cs.AI

TL;DR: 探讨AI如何促进人与自然从支配关系转向相互依存，并通过案例研究展示AI在生态设计中的应用与潜力。


<details>
  <summary>Details</summary>
Motivation: 研究AI是否能作为媒介，推动人与自然关系的转变，实现生态设计的创新。

Method: 通过案例研究分析AI在数据、图像识别和生态修复中的应用，并结合原型设计提出AI与植物修复结合的方法。

Result: AI不仅扩展了创意方法，还重构了生态设计的理论与实践，展示了其在科学、艺术和环保中的潜力。

Conclusion: AI为可持续技术生态系统提供了研究路径，未来可进一步探索其在生态设计中的应用。

Abstract: This paper asks whether our relationship with nature can move from human
dominance to genuine interdependence, and whether artificial intelligence (AI)
can mediate that shift. We examine a new ecological-design paradigm in which AI
interacts with non-human life forms. Through case studies we show how artists
and designers apply AI for data analysis, image recognition, and ecological
restoration, producing results that differ from conventional media. We argue
that AI not only expands creative methods but also reframes the theory and
practice of ecological design. Building on the author's prototype for
AI-assisted water remediation, the study proposes design pathways that couple
reinforcement learning with plant-based phytoremediation. The findings
highlight AI's potential to link scientific insight, artistic practice, and
environmental stewardship, offering a roadmap for future research on
sustainable, technology-enabled ecosystems.

</details>


### [122] [General Modular Harness for LLM Agents in Multi-Turn Gaming Environments](https://arxiv.org/abs/2507.11633)
*Yuxuan Zhang,Haoyang Yu,Lanxiang Hu,Haojian Jin,Hao Zhang*

Main category: cs.AI

TL;DR: 提出了一种模块化设计，结合感知、记忆和推理组件，使LLM/VLM能适应多轮游戏环境，无需领域特定工程。


<details>
  <summary>Details</summary>
Motivation: 通过游戏环境测试模块化设计对通用智能体的性能提升效果。

Method: 使用经典和现代游戏套件作为测试平台，分析各模块在动态交互中的表现。

Result: 实验表明，模块化设计显著提升游戏表现，不同模块在不同场景中贡献各异。

Conclusion: 模块化设计有效推进通用智能体发展，游戏环境验证了其潜力。

Abstract: We introduce a modular harness design for LLM agents that composes of
perception, memory, and reasoning components, enabling a single LLM or VLM
backbone to tackle a wide spectrum of multi turn gaming environments without
domain-specific engineering. Using classic and modern game suites as
low-barrier, high-diversity testbeds, our framework provides a unified workflow
for analyzing how each module affects performance across dynamic interactive
settings. Extensive experiments demonstrate that the harness lifts gameplay
performance consistently over un-harnessed baselines and reveals distinct
contribution patterns, for example, memory dominates in long-horizon puzzles
while perception is critical in vision noisy arcades. These findings highlight
the effectiveness of our modular harness design in advancing general-purpose
agent, given the familiarity and ubiquity of games in everyday human
experience.

</details>


### [123] [Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification](https://arxiv.org/abs/2507.11662)
*Moises Andrade,Joonhyuk Cha,Brandon Ho,Vriksha Srihari,Karmesh Yadav,Zsolt Kira*

Main category: cs.AI

TL;DR: MLLMs作为验证器在复杂任务中存在一致性偏差，提出自我基础验证（SGV）方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在缺乏明确成功标准的领域（如计算机使用）扩展AI验证器的应用。

Method: 提出自我基础验证（SGV），利用MLLMs的采样机制，分两步：先获取任务完成的广泛先验，再评估候选轨迹。

Result: SGV使MLLMs验证器准确率提升20点，任务完成率提升48%，在多个领域超越现有最佳。

Conclusion: SGV有效解决了MLLMs作为验证器的一致性偏差问题，显著提升了性能。

Abstract: Verifiers -- functions assigning rewards to agent behavior -- have been key
for AI progress in domains like math and board games. However, extending these
gains to domains without clear-cut success criteria (e.g.,computer use) remains
a challenge: while humans can recognize suitable outcomes, translating this
intuition into scalable rules is non-trivial. Multimodal Large Language
Models(MLLMs) emerge as a promising solution, given their world knowledge,
human-preference alignment, and reasoning skills. We evaluate MLLMs as
verifiers of agent trajectories across web navigation, computer use, and
robotic manipulation, and identify a critical limitation: agreement bias, a
strong tendency for MLLMs to favor information in their context window, often
generating chains of thought to rationalize flawed behavior. This bias is
pervasive across models, resilient to test-time scaling, and can impact several
methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs
despite MLLMs showing strong, human-aligned priors on desired behavior. To
address this, we propose Self-Grounded Verification (SGV), a lightweight method
that enables more effective use of MLLMs' knowledge and reasoning by harnessing
their own sampling mechanisms via unconditional and conditional generation. SGV
operates in two steps: first, the MLLM is elicited to retrieve broad priors
about task completion, independent of the data under evaluation. Then,
conditioned on self-generated priors, it reasons over and evaluates a candidate
trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in
accuracy and failure detection rates, and can perform real-time supervision of
heterogeneous agents, boosting task completion of a GUI specialist in OSWorld,
a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting
a new state of the art on the benchmark, surpassing the previous best by 48%.

</details>


### [124] [ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making](https://arxiv.org/abs/2507.11733)
*Srikanth Vemula*

Main category: cs.AI

TL;DR: ClarifAI结合案例推理和本体驱动方法，提升AI透明度和可解释性，适用于高风险的决策场景。


<details>
  <summary>Details</summary>
Motivation: 解决AI在决策过程中缺乏透明度和可解释性的问题，满足不同利益相关者的需求。

Method: 结合案例推理（CBR）和本体驱动方法，设计理论框架和架构蓝图。

Result: ClarifAI能显著提升AI系统的可解释性，适用于多领域和高风险环境。

Conclusion: ClarifAI为AI系统在关键决策中的应用提供了透明和可解释的解决方案。

Abstract: This Study introduces Clarity and Reasoning Interface for Artificial
Intelligence(ClarifAI), a novel approach designed to augment the transparency
and interpretability of artificial intelligence (AI) in the realm of improved
decision making. Leveraging the Case-Based Reasoning (CBR) methodology and
integrating an ontology-driven approach, ClarifAI aims to meet the intricate
explanatory demands of various stakeholders involved in AI-powered
applications. The paper elaborates on ClarifAI's theoretical foundations,
combining CBR and ontologies to furnish exhaustive explanation mechanisms. It
further elaborates on the design principles and architectural blueprint,
highlighting ClarifAI's potential to enhance AI interpretability across
different sectors and its applicability in high-stake environments. This
research delineates the significant role of ClariAI in advancing the
interpretability of AI systems, paving the way for its deployment in critical
decision-making processes.

</details>


### [125] [Auto-Formulating Dynamic Programming Problems with Large Language Models](https://arxiv.org/abs/2507.11737)
*Chenyu Zhou,Jingyuan Yang,Linwei Xin,Yitian Chen,Ziyan He,Dongdong Ge*

Main category: cs.AI

TL;DR: 论文提出DP-Bench基准和DPLM模型，通过DualReflect数据生成方法解决动态规划问题中LLM应用的挑战。


<details>
  <summary>Details</summary>
Motivation: 动态规划模型构建传统上依赖专家知识，LLM有潜力自动化这一过程，但面临数据稀缺和随机性挑战。

Method: 引入DP-Bench基准和DPLM模型，采用DualReflect合成数据生成方法，结合前向和后向生成策略。

Result: DPLM在性能上媲美主流LLM，并在难题上表现更优；DualReflect在低数据和高数据场景下各有优势。

Conclusion: 结合前向和后向生成策略的DualReflect方法有效解决了动态规划问题中LLM的数据挑战。

Abstract: Dynamic programming (DP) is a fundamental method in operations research, but
formulating DP models has traditionally required expert knowledge of both the
problem context and DP techniques. Large Language Models (LLMs) offer the
potential to automate this process. However, DP problems pose unique challenges
due to their inherently stochastic transitions and the limited availability of
training data. These factors make it difficult to directly apply existing
LLM-based models or frameworks developed for other optimization problems, such
as linear or integer programming. We introduce DP-Bench, the first benchmark
covering a wide range of textbook-level DP problems to enable systematic
evaluation. We present Dynamic Programming Language Model (DPLM), a
7B-parameter specialized model that achieves performance comparable to
state-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on
hard problems. Central to DPLM's effectiveness is DualReflect, our novel
synthetic data generation pipeline, designed to scale up training data from a
limited set of initial examples. DualReflect combines forward generation for
diversity and backward generation for reliability. Our results reveal a key
insight: backward generation is favored in low-data regimes for its strong
correctness guarantees, while forward generation, though lacking such
guarantees, becomes increasingly valuable at scale for introducing diverse
formulations. This trade-off highlights the complementary strengths of both
approaches and the importance of combining them.

</details>


### [126] [Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity](https://arxiv.org/abs/2507.11787)
*Chandrashekar Muniyappa,Eunjin Kim*

Main category: cs.AI

TL;DR: 本文综述了基于群体智能算法的语义相似性文档搜索的最新进展，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 群体智能（SI）因其高效性在人工智能领域广受欢迎，通过模拟自然界动物和昆虫行为解决实际问题。本文旨在总结其在语义相似性文档搜索中的应用。

Method: 通过文献综述，分析群体智能算法在文档搜索中的最新应用和发展。

Result: 总结了群体智能在语义相似性文档搜索中的有效性，并识别了当前研究的局限性。

Conclusion: 群体智能在文档搜索中具有潜力，未来研究应进一步优化算法并拓展应用场景。

Abstract: Swarm Intelligence (SI) is gaining a lot of popularity in artificial
intelligence, where the natural behavior of animals and insects is observed and
translated into computer algorithms called swarm computing to solve real-world
problems. Due to their effectiveness, they are applied in solving various
computer optimization problems. This survey will review all the latest
developments in Searching for documents based on semantic similarity using
Swarm Intelligence algorithms and recommend future research directions.

</details>


### [127] [A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS](https://arxiv.org/abs/2507.11916)
*Ehsan Futuhi,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: 论文提出了一种利用GPU并行计算能力优化深度优先搜索（DFS）的方法，特别是成本受限的深度优先搜索（CB-DFS），并扩展了Batch IDA*和Batch BTS算法。


<details>
  <summary>Details</summary>
Motivation: GPU技术的快速发展为经典搜索算法提供了新的优化机会，但目前很少有算法在搜索过程中充分利用GPU。

Method: 提出了一种批处理GPU计算的方法，结合现代CPU和GPU的并行能力，设计了CB-DFS算法，并扩展了Batch IDA*和Batch BTS。

Result: 在3x3魔方和4x4滑动拼图（STP）上验证了方法的有效性，并分析了超参数、神经网络启发式大小和硬件资源对性能的影响。

Conclusion: 研究表明，GPU操作可以在DFS中高效批处理，同时保持最优性保证。

Abstract: The rapid advancement of GPU technology has unlocked powerful parallel
processing capabilities, creating new opportunities to enhance classic search
algorithms. A recent successful application of GPUs is in compressing large
pattern database (PDB) heuristics using neural networks while preserving
heuristic admissibility. However, very few algorithms have been designed to
exploit GPUs during search. Several variants of A* exist that batch GPU
computations. In this paper we introduce a method for batching GPU computations
in depth first search. In particular, we describe a new cost-bounded
depth-first search (CB-DFS) method that leverages the combined parallelism of
modern CPUs and GPUs. This is used to create algorithms like \emph{Batch IDA*},
an extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an
extensions of Budgeted Tree Search. Our approach builds on the general approach
used by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality
guarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding
tile puzzle (STP), showing that GPU operations can be efficiently batched in
DFS. Additionally, we conduct extensive experiments to analyze the effects of
hyperparameters, neural network heuristic size, and hardware resources on
performance.

</details>


### [128] [Aime: Towards Fully-Autonomous Multi-Agent Framework](https://arxiv.org/abs/2507.11988)
*Yexuan Shi,Mingyu Wang,Yunxiang Cao,Hongjie Lai,Junjian Lan,Xin Han,Yu Wang,Jie Geng,Zhenan Li,Zihao Xia,Xiang Chen,Chen Li,Jian Xu,Wenbo Duan,Yuanshuo Zhu*

Main category: cs.AI

TL;DR: Aime是一个新型多智能体框架，通过动态反应式规划和执行解决了传统静态框架的局限性，显著提升了多智能体系统的适应性和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体系统（MAS）采用静态规划与执行框架，存在执行僵化、能力静态和通信低效等问题，限制了其在动态环境中的表现。

Method: Aime框架包含动态规划器、动态角色工厂和进度管理模块，实现了实时策略调整、按需角色创建和全局状态管理。

Result: 在多个基准测试（GAIA、SWE-bench Verified、WebVoyager）中，Aime表现优于现有最先进的专用智能体。

Conclusion: Aime为多智能体协作提供了更具弹性和高效性的基础，显著提升了动态环境中的适应性和任务成功率。

Abstract: Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are
emerging as a powerful paradigm for solving complex, multifaceted problems.
However, the potential of these systems is often constrained by the prevalent
plan-and-execute framework, which suffers from critical limitations: rigid plan
execution, static agent capabilities, and inefficient communication. These
weaknesses hinder their adaptability and robustness in dynamic environments.
This paper introduces Aime, a novel multi-agent framework designed to overcome
these challenges through dynamic, reactive planning and execution. Aime
replaces the conventional static workflow with a fluid and adaptive
architecture. Its core innovations include: (1) a Dynamic Planner that
continuously refines the overall strategy based on real-time execution
feedback; (2) an Actor Factory that implements Dynamic Actor instantiation,
assembling specialized agents on-demand with tailored tools and knowledge; and
(3) a centralized Progress Management Module that serves as a single source of
truth for coherent, system-wide state awareness. We empirically evaluated Aime
on a diverse suite of benchmarks spanning general reasoning (GAIA), software
engineering (SWE-bench Verified), and live web navigation (WebVoyager). The
results demonstrate that Aime consistently outperforms even highly specialized
state-of-the-art agents in their respective domains. Its superior adaptability
and task success rate establish Aime as a more resilient and effective
foundation for multi-agent collaboration.

</details>


### [129] [Understanding visual attention beehind bee-inspired UAV navigation](https://arxiv.org/abs/2507.11992)
*Pranav Rajbhandari,Abhi Veda,Matthew Garratt,Mandayam Srinivasan,Sridhar Ravi*

Main category: cs.AI

TL;DR: 论文研究了基于光流的强化学习代理在无人机导航中的应用，发现代理主要关注光流的不连续区域和大光流区域，行为类似昆虫飞行。


<details>
  <summary>Details</summary>
Motivation: 生物系统（如蜜蜂）利用光流在复杂环境中导航的能力激发了研究，希望通过强化学习模拟这种行为。

Method: 训练强化学习代理仅使用光流作为输入在障碍物隧道中导航，并分析其注意力模式。

Result: 代理主要关注光流的不连续区域和大光流区域，行为类似昆虫飞行，且在不同代理中表现一致。

Conclusion: 这种策略可能适用于开发简单的无人机显式控制法则。

Abstract: Bio-inspired design is often used in autonomous UAV navigation due to the
capacity of biological systems for flight and obstacle avoidance despite
limited sensory and computational capabilities. In particular, honeybees mainly
use the sensory input of optic flow, the apparent motion of objects in their
visual field, to navigate cluttered environments. In our work, we train a
Reinforcement Learning agent to navigate a tunnel with obstacles using only
optic flow as sensory input. We inspect the attention patterns of trained
agents to determine the regions of optic flow on which they primarily base
their motor decisions. We find that agents trained in this way pay most
attention to regions of discontinuity in optic flow, as well as regions with
large optic flow magnitude. The trained agents appear to navigate a cluttered
tunnel by avoiding the obstacles that produce large optic flow, while
maintaining a centered position in their environment, which resembles the
behavior seen in flying insects. This pattern persists across independently
trained agents, which suggests that this could be a good strategy for
developing a simple explicit control law for physical UAVs.

</details>


### [130] [Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs](https://arxiv.org/abs/2507.12110)
*Ye Han,Lijun Zhang,Dejian Meng,Zhuang Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种拓扑增强的多智能体强化学习方法（TPE-MARL），用于优化混合交通中联网自动驾驶车辆（CAVs）的协同决策。通过构建游戏拓扑张量和结合QMIX算法，该方法在探索与利用之间取得了平衡，显著提升了交通效率、安全性和决策合理性。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）在混合交通中面临状态-动作空间指数增长的挑战，探索与利用的平衡尤为困难。本文旨在通过拓扑结构优化MARL，提升CAVs的协同决策能力。

Method: 1. 构建动态交通流的游戏拓扑张量，压缩高维状态信息；2. 基于QMIX算法，结合访问计数和智能体互信息，建立拓扑增强的MARL框架。

Result: 实验表明，TPE-MARL在多种交通密度和CAV渗透率下表现优异，平衡了探索与利用，显著提升了交通效率、安全性和决策平滑性，且决策合理性接近或超过人类驾驶员。

Conclusion: TPE-MARL通过拓扑结构优化MARL，有效解决了探索与利用的平衡问题，为混合交通中的CAV协同决策提供了高效解决方案。

Abstract: The exploration-exploitation trade-off constitutes one of the fundamental
challenges in reinforcement learning (RL), which is exacerbated in multi-agent
reinforcement learning (MARL) due to the exponential growth of joint
state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)
method for optimizing cooperative decision-making of connected and autonomous
vehicles (CAVs) in mixed traffic. This work presents two primary contributions:
First, we construct a game topology tensor for dynamic traffic flow,
effectively compressing high-dimensional traffic state information and decrease
the search space for MARL algorithms. Second, building upon the designed game
topology tensor and using QMIX as the backbone RL algorithm, we establish a
topology-enhanced MARL framework incorporating visit counts and agent mutual
information. Extensive simulations across varying traffic densities and CAV
penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations
encompassing training dynamics, exploration patterns, macroscopic traffic
performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL
successfully balances exploration and exploitation. Consequently, it exhibits
superior performance in terms of traffic efficiency, safety, decision
smoothness, and task completion. Furthermore, the algorithm demonstrates
decision-making rationality comparable to or exceeding that of human drivers in
both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is
available at
\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.

</details>


### [131] [Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation](https://arxiv.org/abs/2507.12186)
*Edward Kim,Hanna Kurniawati*

Main category: cs.AI

TL;DR: 提出了一种新的在线近似POMDP求解器，通过深度采样未来历史并逐步更新策略，性能损失由采样误差的平均值而非最大值决定。


<details>
  <summary>Details</summary>
Motivation: 解决在线规划中采样稀疏性问题，提升动态环境下的决策性能。

Method: 提出部分可观测参考策略编程（PORPP），结合深度采样和逐步策略更新。

Result: 理论证明性能损失受采样误差平均值限制，实验验证在大规模动态环境中优于现有方法。

Conclusion: PORPP在理论和实践中均表现出色，适用于复杂动态环境。

Abstract: This paper proposes Partially Observable Reference Policy Programming, a
novel anytime online approximate POMDP solver which samples meaningful future
histories very deeply while simultaneously forcing a gradual policy update. We
provide theoretical guarantees for the algorithm's underlying scheme which say
that the performance loss is bounded by the average of the sampling
approximation errors rather than the usual maximum, a crucial requirement given
the sampling sparsity of online planning. Empirical evaluations on two
large-scale problems with dynamically evolving environments -- including a
helicopter emergency scenario in the Corsica region requiring approximately 150
planning steps -- corroborate the theoretical results and indicate that our
solver considerably outperforms current online benchmarks.

</details>


### [132] [BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2507.12207)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: BuildEvo利用大型语言模型自动设计高效且可解释的建筑能耗预测启发式方法，结合物理原理，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法精度不足，而高级模型缺乏透明性且忽视物理原理，难以泛化。

Method: 通过进化过程引导LLMs构建和优化启发式方法，结合建筑特性和操作数据的物理洞察。

Result: 在基准测试中达到最优性能，泛化能力更强且预测逻辑透明。

Conclusion: BuildEvo推动了自动化设计稳健、基于物理的启发式方法，为复杂能源系统提供可信模型。

Abstract: Accurate building energy forecasting is essential, yet traditional heuristics
often lack precision, while advanced models can be opaque and struggle with
generalization by neglecting physical principles. This paper introduces
BuildEvo, a novel framework that uses Large Language Models (LLMs) to
automatically design effective and interpretable energy prediction heuristics.
Within an evolutionary process, BuildEvo guides LLMs to construct and enhance
heuristics by systematically incorporating physical insights from building
characteristics and operational data (e.g., from the Building Data Genome
Project 2). Evaluations show BuildEvo achieves state-of-the-art performance on
benchmarks, offering improved generalization and transparent prediction logic.
This work advances the automated design of robust, physically grounded
heuristics, promoting trustworthy models for complex energy systems.

</details>


### [133] [Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning](https://arxiv.org/abs/2507.12215)
*Yuhao Chen,Shuochen Liu,Yuanjie Lyu,Chao Zhang,Jiayao Shi,Tong Xu*

Main category: cs.AI

TL;DR: 论文提出了一种针对中国象棋（Xiangqi）的LLM训练框架Xiangqi-R1，通过多阶段训练显著提升了模型在空间战略推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在空间战略推理中的不足，尤其是复杂且完全可观察的棋盘游戏（如中国象棋）中的表现。

Method: 采用多阶段训练：1）微调合法移动预测；2）引入战略注释；3）通过GRPO强化学习提升推理稳定性。

Result: Xiangqi-R1在移动合法性和分析准确性上分别提升了18%和22%，优于通用LLMs。

Conclusion: 该研究为在空间复杂领域构建通用战略智能提供了可行路径。

Abstract: Game playing has long served as a fundamental benchmark for evaluating
Artificial General Intelligence (AGI). While Large Language Models (LLMs) have
demonstrated impressive capabilities in general reasoning, their effectiveness
in spatial strategic reasoning, which is critical for complex and fully
observable board games, remains insufficiently explored. In this work, we adopt
Chinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate
rules and spatial complexity. To advance LLMs' strategic competence in such
environments, we propose a training framework tailored to Xiangqi, built upon a
large-scale dataset of five million board-move pairs enhanced with expert
annotations and engine evaluations. Building on this foundation, we introduce
Xiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning
for legal move prediction to capture basic spatial rules, (2) incorporating
strategic annotations to improve decision-making, and (3) applying
reinforcement learning via Group Relative Policy Optimization (GRPO) with
multi-dimensional reward signals to enhance reasoning stability. Our
Experimental results indicate that, despite their size and power,
general-purpose LLMs struggle to achieve satisfactory performance in these
tasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an
18% rise in move legality and a 22% boost in analysis accuracy. Our results
point to a promising path for creating general strategic intelligence in
spatially complex areas.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [134] [Neuroaesthetics and the Science of Visual Experience](https://arxiv.org/abs/2507.11599)
*Harish Vijayakumar*

Main category: cs.HC

TL;DR: 神经美学研究大脑如何感知美，揭示设计如何通过神经机制影响情感与认知。


<details>
  <summary>Details</summary>
Motivation: 探索视觉美感的神经机制，解释为何某些设计或艺术品能引发情感共鸣。

Method: 结合神经科学、心理学和艺术，分析感知、情感与认知的相互作用。

Result: 发现设计不仅是表面吸引力，还能通过神经机制实现深层次的情感连接。

Conclusion: 神经美学为设计领域提供了科学依据，强调视觉体验的深层价值。

Abstract: Neuroaesthetics is an interdisciplinary field that brings together
neuroscience, psychology, and the arts to explore how the human brain perceives
and responds to visual beauty. This paper examines the neural mechanisms behind
aesthetic experiences, aiming to explain why certain designs or artworks feel
emotionally or cognitively "right." By analyzing the interaction between
perception, emotion, and cognition, neuroaesthetics reveals how beauty is
constructed in the brain and how this understanding can inform fields such as
graphic and interface design. This paper offers a clear and accessible overview
of core neuroaesthetic principles, making the subject approachable to a wide
audience. The findings suggest that impactful design is more than surface-level
appeal: well-crafted visual experiences can engage, support, and connect people
in meaningful ways.

</details>


### [135] [DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday Storytelling](https://arxiv.org/abs/2507.11628)
*Jiangnan Xu,Haeseul Cha,Gosu Choi,Gyu-cheol Lee,Yeo-Jin Yoon,Zucheul Lee,Konstantinos Papangelis,Dae Hyun Kim,Juho Kim*

Main category: cs.HC

TL;DR: DiaryPlay是一个AI辅助的交互式小故事创作系统，通过自然语言输入简化创作流程，并利用LLM生成分支叙事结构。


<details>
  <summary>Details</summary>
Motivation: 解决交互式小故事创作复杂度高、不适合日常讲故事的问题。

Method: 输入自然语言故事，提取核心元素（环境、角色、事件），利用LLM生成分支叙事结构。

Result: 技术评估显示生成的角色活动可信度与人工创作相当；用户研究表明系统能有效支持创作并保持作者意图。

Conclusion: DiaryPlay为日常讲故事提供了一种简单高效的交互式小故事创作工具。

Abstract: An interactive vignette is a popular and immersive visual storytelling
approach that invites viewers to role-play a character and influences the
narrative in an interactive environment. However, it has not been widely used
by everyday storytellers yet due to authoring complexity, which conflicts with
the immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted
authoring system for interactive vignette creation in everyday storytelling. It
takes a natural language story as input and extracts the three core elements of
an interactive vignette (environment, characters, and events), enabling authors
to focus on refining these elements instead of constructing them from scratch.
Then, it automatically transforms the single-branch story input into a
branch-and-bottleneck structure using an LLM-powered narrative planner, which
enables flexible viewer interactions while freeing the author from
multi-branching. A technical evaluation (N=16) shows that DiaryPlay-generated
character activities are on par with human-authored ones regarding
believability. A user study (N=16) shows that DiaryPlay effectively supports
authors in creating interactive vignette elements, maintains authorial intent
while reacting to viewer interactions, and provides engaging viewing
experiences.

</details>


### [136] [CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations](https://arxiv.org/abs/2507.11677)
*Mashrur Rashik,Jean-Daniel Fekete,Narges Mahyar*

Main category: cs.HC

TL;DR: CLAImate是一个AI原型工具，通过个性化对话和本地化可视化改善气候变化的沟通效果。


<details>
  <summary>Details</summary>
Motivation: 气候变化报告通常过于抽象或技术化，难以引起公众共鸣，而现有工具缺乏个性化叙事和可视化。

Method: CLAImate根据用户的气候知识和地理位置个性化对话叙事和可视化，并通过内部验证、专家研究及英国居民试点评估效果。

Result: CLAImate在SNLI准确率达到66%，FACTSCORE为70%，专家认可其清晰度和个性化，70%的英国参与者表示对气候风险的理解和本地相关性有所提升。

Conclusion: CLAImate展示了在个性化沟通中的潜力，但也面临准确性、可扩展性等设计挑战，未来可进一步整合可视化与对话界面。

Abstract: Communicating climate change remains challenging, as climate reports, though
rich in data and visualizations, often feel too abstract or technical for the
public. Although personalization can enhance communication, most tools still
lack the narrative and visualization tailoring needed to connect with
individual experiences. We present CLAImate, an AI-enabled prototype that
personalizes conversation narratives and localizes visualizations based on
users' climate knowledge and geographic location. We evaluated CLAImate through
internal verification of factual correctness, a formative study with experts,
and a pilot with UK residents. CLAImate achieved 66% SNLI accuracy and 70%
FACTSCORE. Visualization experts appreciated its clarity and personalization,
and seven out of ten UK participants reported better understanding and local
relevance of climate risks with CLAImate. We also discuss design challenges in
personalization, accuracy, and scalability, and outline future directions for
integrating visualizations in personalized conversational interfaces.

</details>


### [137] [GIST: Group Interaction Sensing Toolkit for Mixed Reality](https://arxiv.org/abs/2507.11797)
*Diana Romero,Yasra Chandio,Fatima Anwar,Salma Elmalaki*

Main category: cs.HC

TL;DR: 论文提出了一种名为GIST的群组交互感知工具包，用于在混合现实（MR）环境中被动捕获多模态交互数据，并通过传感器数据识别动态行为模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部摄像头或仅关注单一模态，限制了其有效性和适用性。

Method: 开发了GIST工具包，利用MR头显传感器捕获语音、注视和空间接近等多模态数据，并自动生成静态交互网络和动态行为模式。

Result: 通过48名参与者的实验验证，GIST能够准确识别行为模式与交互网络结构的变化。

Conclusion: GIST通过传感器数据成功捕捉了交互中的瞬时变化，为MR协作应用设计提供了有效工具。

Abstract: Understanding how teams coordinate, share work, and negotiate roles in
immersive environments is critical for designing effective mixed-reality (MR)
applications that support real-time collaboration. However, existing methods
either rely on external cameras and offline annotation or focus narrowly on
single modalities, limiting their validity and applicability. To address this,
we present a novel group interaction sensing toolkit (GIST), a deployable
system that passively captures multi-modal interaction data, such as speech,
gaze, and spatial proximity from commodity MR headset's sensors and
automatically derives both overall static interaction networks and dynamic
moment-by-moment behavior patterns. We evaluate GIST with a human subject study
with 48 participants across 12 four-person groups performing an open-ended
image-sorting task in MR. Our analysis shows strong alignment between the
identified behavior modes and shifts in interaction network structure,
confirming that momentary changes in speech, gaze, and proximity data are
observable through the sensor data.

</details>


### [138] ["Mapping What I Feel": Understanding Affective Geovisualization Design Through the Lens of People-Place Relationships](https://arxiv.org/abs/2507.11841)
*Xingyu Lan,Yutong Yang,Yifan Wang*

Main category: cs.HC

TL;DR: 本文分析了情感地理可视化设计，通过PPP模型分类方法，提出了四种设计范式，为这一新兴领域提供了具体指导和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 情感可视化设计是一个跨学科的新兴领域，需要更细粒度的分析。本文聚焦于情感地理可视化设计，以提供更具体的领域见解。

Method: 使用地理理论中的PPP模型分析情感地理可视化设计，并分类设计方法。

Result: 提出了四种高层次设计范式（如计算型、拟人化型），扩展了现有情感可视化设计框架。

Conclusion: 通过地理特异性扩展框架，为未来研究和实践提供了具体指导和创新方向。

Abstract: Affective visualization design is an emerging research direction focused on
communicating and influencing emotion through visualization. However, as
revealed by previous research, this area is highly interdisciplinary and
involves theories and practices from diverse fields and disciplines, thus
awaiting analysis from more fine-grained angles. To address this need, this
work focuses on a pioneering and relatively mature sub-area, affective
geovisualization design, to further the research in this direction and provide
more domain-specific insights. Through an analysis of a curated corpus of
affective geovisualization designs using the Person-Process-Place (PPP) model
from geographic theory, we derived a design taxonomy that characterizes a
variety of methods for eliciting and enhancing emotions through geographic
visualization. We also identified four underlying high-level design paradigms
of affective geovisualization design (e.g., computational, anthropomorphic)
that guide distinct approaches to linking geographic information with human
experience. By extending existing affective visualization design frameworks
with geographic specificity, we provide additional design examples,
domain-specific analyses, and insights to guide future research and practices
in this underexplored yet highly innovative domain.

</details>


### [139] [Interactive Hybrid Rice Breeding with Parametric Dual Projection](https://arxiv.org/abs/2507.11848)
*Changjian Chen,Pengcheng Wang,Fei Lyu,Zhuo Tang,Li Yang,Long Wang,Yong Cai,Feng Yu,Kenli Li*

Main category: cs.HC

TL;DR: 提出了一种可视化分析方法，用于交互式杂交水稻育种，结合基因和杂交可视化，提高育种效率。


<details>
  <summary>Details</summary>
Motivation: 基因组选择虽能预测杂交水稻性状，但模型准确性有限，仍需结合经验筛选基因和杂交种，过程耗时。

Method: 开发了一种参数化双投影方法，支持交互式双分析，并基于此方法设计了基因和杂交可视化工具。

Result: 通过定量评估、案例研究和育种者反馈，验证了方法的有效性。

Conclusion: 该方法显著提升了杂交水稻育种的效率和准确性。

Abstract: Hybrid rice breeding crossbreeds different rice lines and cultivates the
resulting hybrids in fields to select those with desirable agronomic traits,
such as higher yields. Recently, genomic selection has emerged as an efficient
way for hybrid rice breeding. It predicts the traits of hybrids based on their
genes, which helps exclude many undesired hybrids, largely reducing the
workload of field cultivation. However, due to the limited accuracy of genomic
prediction models, breeders still need to combine their experience with the
models to identify regulatory genes that control traits and select hybrids,
which remains a time-consuming process. To ease this process, in this paper, we
proposed a visual analysis method to facilitate interactive hybrid rice
breeding. Regulatory gene identification and hybrid selection naturally
ensemble a dual-analysis task. Therefore, we developed a parametric dual
projection method with theoretical guarantees to facilitate interactive dual
analysis. Based on this dual projection method, we further developed a gene
visualization and a hybrid visualization to verify the identified regulatory
genes and hybrids. The effectiveness of our method is demonstrated through the
quantitative evaluation of the parametric dual projection method, identified
regulatory genes and desired hybrids in the case study, and positive feedback
from breeders.

</details>


### [140] [Unveiling the Visual Rhetoric of Persuasive Cartography: A Case Study of the Design of Octopus Maps](https://arxiv.org/abs/2507.11903)
*Daocheng Lin,Yifan Wang,Yutong Yang,Xingyu Lan*

Main category: cs.HC

TL;DR: 论文探讨了数据可视化如何通过修辞构造成为强大的说服工具，并以章鱼地图为例分析了其跨世纪的修辞策略和社会影响。


<details>
  <summary>Details</summary>
Motivation: 填补当前可视化研究中修辞构造视角的不足，揭示章鱼地图作为说服性可视化的持久影响力。

Method: 采用修辞图式理论，收集并分析了90个19世纪至今的章鱼地图，构建了设计空间并识别了常见的修辞策略。

Result: 发现章鱼地图在现代仍活跃，且其修辞策略因文化背景而异，同时揭示了说服性可视化的伦理问题。

Conclusion: 章鱼地图展示了修辞构造在说服性可视化中的重要性，并呼吁关注其伦理影响。

Abstract: When designed deliberately, data visualizations can become powerful
persuasive tools, influencing viewers' opinions, values, and actions. While
researchers have begun studying this issue (e.g., to evaluate the effects of
persuasive visualization), we argue that a fundamental mechanism of persuasion
resides in rhetorical construction, a perspective inadequately addressed in
current visualization research. To fill this gap, we present a focused analysis
of octopus maps, a visual genre that has maintained persuasive power across
centuries and achieved significant social impact. Employing rhetorical schema
theory, we collected and analyzed 90 octopus maps spanning from the 19th
century to contemporary times. We closely examined how octopus maps implement
their persuasive intents and constructed a design space that reveals how visual
metaphors are strategically constructed and what common rhetorical strategies
are applied to components such as maps, octopus imagery, and text. Through the
above analysis, we also uncover a set of interesting findings. For instance,
contrary to the common perception that octopus maps are primarily a historical
phenomenon, our research shows that they remain a lively design convention in
today's digital age. Additionally, while most octopus maps stem from Western
discourse that views the octopus as an evil symbol, some designs offer
alternative interpretations, highlighting the dynamic nature of rhetoric across
different sociocultural settings. Lastly, drawing from the lessons provided by
octopus maps, we discuss the associated ethical concerns of persuasive
visualization.

</details>


### [141] [AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding](https://arxiv.org/abs/2507.11911)
*Xiaoqing Chen,Siyang Li,Dongrui Wu*

Main category: cs.HC

TL;DR: 提出了一种无需校准的跨数据集EEG解码框架AFPM，通过空间对齐和帧-块编码提升BCI性能。


<details>
  <summary>Details</summary>
Motivation: 解决EEG解码模型在跨数据集学习和泛化中的问题，如通道布局不一致、信号分布非平稳和神经生理学先验整合不足。

Method: AFPM框架包括空间对齐（选择任务相关通道并统一布局）和帧-块编码（将信号建模为统一时空块）。

Result: 在运动想象和事件相关电位任务上分别提升4.40%和3.58%，优于17种现有方法。

Conclusion: AFPM是首个无需校准的跨数据集EEG解码框架，显著提升了BCI的实际应用价值。

Abstract: Electroencephalogram (EEG) decoding models for brain-computer interfaces
(BCIs) struggle with cross-dataset learning and generalization due to channel
layout inconsistencies, non-stationary signal distributions, and limited
neurophysiological prior integration. To address these issues, we propose a
plug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has
two main components: 1) Spatial Alignment, which selects task-relevant channels
based on brain-region priors, aligns EEG distributions across domains, and
remaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,
which models multi-dataset signals into unified spatiotemporal patches for EEG
decoding. Compared to 17 state-of-the-art approaches that need dataset-specific
tuning, the proposed calibration-free AFPM achieves performance gains of up to
4.40% on motor imagery and 3.58% on event-related potential tasks. To our
knowledge, this is the first calibration-free cross-dataset EEG decoding
framework, substantially enhancing the practicalness of BCIs in real-world
applications.

</details>


### [142] [d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement](https://arxiv.org/abs/2507.11960)
*Hyein Hong,Sangbong Yoo,SeokHwan Choi,Jisue Kim,Seongbum Seo,Haneol Cho,Chansoo Kim,Yun Jang*

Main category: cs.HC

TL;DR: 论文提出d-DQIVAR系统，结合数据驱动和流程驱动方法，通过可视化分析提升数据质量（DQ）以优化机器学习模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注数据预处理而非真正的数据质量改进（DQI），且数据驱动方法在批量预处理中常导致数据特性失真，影响模型性能。

Method: d-DQIVAR系统整合数据驱动（如填补、异常检测）和流程驱动（如DQ评估、K-S检验）技术，结合可视化分析。

Result: 通过案例研究和用户评估，系统证明能有效利用专家知识提升DQ和模型性能。

Conclusion: d-DQIVAR为数据质量改进提供了实用工具，结合两种方法优化了机器学习模型的性能。

Abstract: Approaches to enhancing data quality (DQ) are classified into two main
categories: data- and process-driven. However, prior research has predominantly
utilized batch data preprocessing within the data-driven framework, which often
proves insufficient for optimizing machine learning (ML) model performance and
frequently leads to distortions in data characteristics. Existing studies have
primarily focused on data preprocessing rather than genuine data quality
improvement (DQI). In this paper, we introduce d-DQIVAR, a novel visual
analytics system designed to facilitate DQI strategies aimed at improving ML
model performance. Our system integrates visual analytics techniques that
leverage both data-driven and process-driven approaches. Data-driven techniques
tackle DQ issues such as imputation, outlier detection, deletion, format
standardization, removal of duplicate records, and feature selection.
Process-driven strategies encompass evaluating DQ and DQI procedures by
considering DQ dimensions and ML model performance and applying the
Kolmogorov-Smirnov test. We illustrate how our system empowers users to harness
expert and domain knowledge effectively within a practical workflow through
case studies, evaluations, and user studies.

</details>


### [143] [Dataset-Adaptive Dimensionality Reduction](https://arxiv.org/abs/2507.11984)
*Hyeon Jeon,Jeongin Park,Soohyun Lee,Dae Hyun Kim,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: 提出了一种基于结构复杂性度量的数据集自适应降维优化方法，显著提高了降维效率且不损失精度。


<details>
  <summary>Details</summary>
Motivation: 降维技术选择和超参数优化通常需要大量试错，计算开销大，因此需要一种更高效的方法。

Method: 利用结构复杂性度量量化数据集内在复杂性，预测降维技术的最大可达到精度，避免冗余试错。

Result: 验证了度量能有效近似数据集真实复杂性，并证实其适用于指导自适应降维流程。

Conclusion: 数据集自适应流程显著提升了降维优化效率，同时保持了精度。

Abstract: Selecting the appropriate dimensionality reduction (DR) technique and
determining its optimal hyperparameter settings that maximize the accuracy of
the output projections typically involves extensive trial and error, often
resulting in unnecessary computational overhead. To address this challenge, we
propose a dataset-adaptive approach to DR optimization guided by structural
complexity metrics. These metrics quantify the intrinsic complexity of a
dataset, predicting whether higher-dimensional spaces are necessary to
represent it accurately. Since complex datasets are often inaccurately
represented in two-dimensional projections, leveraging these metrics enables us
to predict the maximum achievable accuracy of DR techniques for a given
dataset, eliminating redundant trials in optimizing DR. We introduce the design
and theoretical foundations of these structural complexity metrics. We
quantitatively verify that our metrics effectively approximate the ground truth
complexity of datasets and confirm their suitability for guiding
dataset-adaptive DR workflow. Finally, we empirically show that our
dataset-adaptive workflow significantly enhances the efficiency of DR
optimization without compromising accuracy.

</details>


### [144] [Envisage: Towards Expressive Visual Graph Querying](https://arxiv.org/abs/2507.11999)
*Xiaolin Wen,Qishuang Fu,Shuangyue Han,Yichen Guo,Joseph K. Liu,Yong Wang*

Main category: cs.HC

TL;DR: Envisage是一个交互式视觉图查询系统，通过支持直观的图结构构建和灵活的规则规范，提升复杂查询场景下的表达能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉图查询工具仅支持简单查询，限制了用户表达复杂查询意图的能力，尤其是模糊查询意图。

Method: Envisage包含四个阶段：查询表达、查询验证、渐进查询执行和结果分析。

Result: 通过案例研究和用户访谈，证明Envisage在构建、验证和执行复杂图查询方面的有效性和可用性。

Conclusion: Envisage显著提升了视觉图查询的表达能力，适用于复杂查询场景。

Abstract: Graph querying is the process of retrieving information from graph data using
specialized languages (e.g., Cypher), often requiring programming expertise.
Visual Graph Querying (VGQ) streamlines this process by enabling users to
construct and execute queries via an interactive interface without resorting to
complex coding. However, current VGQ tools only allow users to construct simple
and specific query graphs, limiting users' ability to interactively express
their query intent, especially for underspecified query intent. To address
these limitations, we propose Envisage, an interactive visual graph querying
system to enhance the expressiveness of VGQ in complex query scenarios by
supporting intuitive graph structure construction and flexible parameterized
rule specification. Specifically, Envisage comprises four stages: Query
Expression allows users to interactively construct graph queries through
intuitive operations; Query Verification enables the validation of constructed
queries via rule verification and query instantiation; Progressive Query
Execution can progressively execute queries to ensure meaningful querying
results; and Result Analysis facilitates result exploration and interpretation.
To evaluate Envisage, we conducted two case studies and in-depth user
interviews with 14 graph analysts. The results demonstrate its effectiveness
and usability in constructing, verifying, and executing complex graph queries.

</details>


### [145] [Tao-Technology for Teen Mobile Use: Harmonizing Adaptation, Autonomy, and Reflection](https://arxiv.org/abs/2507.12204)
*Pengyu Zhu,Janghee Cho*

Main category: cs.HC

TL;DR: 提出Tao-Technology框架，基于道家哲学，动态调节青少年移动技术使用，取代僵化控制。


<details>
  <summary>Details</summary>
Motivation: 现有青少年移动技术监管机制过于僵化，忽视自主性和自然使用模式。

Method: 结合道家哲学（无为、阴阳、自然）和反思信息学、信息生态学，提出动态自适应框架。

Result: Tao-Technology框架支持动态调整，促进自我反思和意义构建。

Conclusion: 从外部限制转向动态协同调节，帮助青少年与技术建立平衡关系。

Abstract: Adolescents' mobile technology use is often regulated through rigid control
mechanisms that fail to account for their autonomy and natural usage patterns.
Drawing on Taoist philosophy, particularly Wu Wei, Yin-Yang, and Zi Ran, this
position paper proposes Tao-Technology, a self-organizing, adaptive regulatory
framework. Integrating insights from Reflective Informatics and Information
Ecologies, we explore how mobile technology can dynamically adjust to context
while fostering self-reflection and meaning-making. This approach shifts from
external restrictions to dynamic co-adaptative regulation, ensuring technology
governance remains flexible yet structured, supporting adolescents in
cultivating a balanced and intentional relationship with digital technology.

</details>


### [146] [Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness](https://arxiv.org/abs/2507.12212)
*Garyoung Kim,Huisung Kwon,Seoju Yun,Yu-Won Youn*

Main category: cs.HC

TL;DR: 研究探讨了生成式AI如何理解和表达“丑陋”，发现其再现了文化偏见，尤其是将丑陋与老年白人男性相关联。


<details>
  <summary>Details</summary>
Motivation: 批判性分析生成式AI如何理解和表达“丑陋”，揭示其内在的文化偏见。

Method: 通过迭代提示提取13个形容词，生成624张图像，并进行编码和主题分析。

Result: AI模型将丑陋与老年白人男性关联，并表现出矛盾偏见，避免边缘化群体却负面投射多数群体。

Conclusion: 生成式AI仍延续偏见，需改进伦理训练范式以推动包容性发展。

Abstract: Generative AI does not only replicate human creativity but also reproduces
deep-seated cultural biases, making it crucial to critically examine how
concepts like ugliness are understood and expressed by these tools. This study
investigates how four different generative AI models understand and express
ugliness through text and image and explores the biases embedded within these
representations. We extracted 13 adjectives associated with ugliness through
iterative prompting of a large language model and generated 624 images across
four AI models and three prompts. Demographic and socioeconomic attributes
within the images were independently coded and thematically analyzed. Our
findings show that AI models disproportionately associate ugliness with old
white male figures, reflecting entrenched social biases as well as paradoxical
biases, where efforts to avoid stereotypical depictions of marginalized groups
inadvertently result in the disproportionate projection of negative attributes
onto majority groups. Qualitative analysis further reveals that, despite
supposed attempts to frame ugliness within social contexts, conventional
physical markers such as asymmetry and aging persist as central visual motifs.
These findings demonstrate that despite attempts to create more equal
representations, generative AI continues to perpetuate inherited and
paradoxical biases, underscoring the critical work being done to create ethical
AI training paradigms and advance methodologies for more inclusive AI
development.

</details>


### [147] [Humans are more gullible than LLMs in believing common psychological myths](https://arxiv.org/abs/2507.12296)
*Bevan Koopman,Guido Zuccon*

Main category: cs.HC

TL;DR: 研究探讨大型语言模型（LLMs）是否模仿人类对心理学神话的信念，并探索减少这种倾向的方法。结果显示LLMs的神话信念率显著低于人类，但用户提示会影响其回答。


<details>
  <summary>Details</summary>
Motivation: 尽管心理学神话已被广泛辟谣，但仍根深蒂固。研究旨在了解LLMs是否模仿人类的这种信念，并寻找减少其倾向的方法。

Method: 使用50个流行的心理学神话，评估不同提示策略下LLMs的神话信念率，包括检索增强生成（RAG）和引导提示。

Result: LLMs的神话信念率显著低于人类，RAG能有效减少其神话信念，并揭示LLMs潜在的纠偏能力。

Conclusion: 研究为机器心理学领域提供贡献，并展示认知科学方法如何指导LLM系统的评估与开发。

Abstract: Despite widespread debunking, many psychological myths remain deeply
entrenched. This paper investigates whether Large Language Models (LLMs) mimic
human behaviour of myth belief and explores methods to mitigate such
tendencies. Using 50 popular psychological myths, we evaluate myth belief
across multiple LLMs under different prompting strategies, including
retrieval-augmented generation and swaying prompts. Results show that LLMs
exhibit significantly lower myth belief rates than humans, though user
prompting can influence responses. RAG proves effective in reducing myth belief
and reveals latent debiasing potential within LLMs. Our findings contribute to
the emerging field of Machine Psychology and highlight how cognitive science
methods can inform the evaluation and development of LLM-based systems.

</details>


### [148] [TrialCompass: Visual Analytics for Enhancing the Eligibility Criteria Design of Clinical Trials](https://arxiv.org/abs/2507.12298)
*Rui Sheng,Xingbo Wang,Jiachen Wang,Xiaofu Jin,Zhonghua Sheng,Zhenxing Xu,Suraj Rajendran,Huamin Qu,Fei Wang*

Main category: cs.HC

TL;DR: TrialCompass是一个可视化分析系统，帮助临床医生通过知识驱动和结果驱动的方法迭代探索临床试验的资格标准，并结合电子健康记录数据进行细化。


<details>
  <summary>Details</summary>
Motivation: 现有资格标准设计方法无法支持交互式探索，且未充分利用电子健康记录数据的详细特征。

Method: 提出TrialCompass系统，整合知识驱动和结果驱动的工作流程，支持历史追踪以优化资格标准设计。

Result: 通过真实数据集验证，TrialCompass在脓毒症休克和急性肾损伤的资格标准设计中提供了有效见解。

Conclusion: TrialCompass展示了可视化分析在临床试验中的潜力，并提出了未来研究方向。

Abstract: Eligibility criteria play a critical role in clinical trials by determining
the target patient population, which significantly influences the outcomes of
medical interventions. However, current approaches for designing eligibility
criteria have limitations to support interactive exploration of the large space
of eligibility criteria. They also ignore incorporating detailed
characteristics from the original electronic health record (EHR) data for
criteria refinement. To address these limitations, we proposed TrialCompass, a
visual analytics system integrating a novel workflow, which can empower
clinicians to iteratively explore the vast space of eligibility criteria
through knowledge-driven and outcome-driven approaches. TrialCompass supports
history-tracking to help clinicians trace the evolution of their adjustments
and decisions when exploring various forms of data (i.e., eligibility criteria,
outcome metrics, and detailed characteristics of original EHR data) through
these two approaches. This feature can help clinicians comprehend the impact of
eligibility criteria on outcome metrics and patient characteristics, which
facilitates systematic refinement of eligibility criteria. Using a real-world
dataset, we demonstrated the effectiveness of TrialCompass in providing
insights into designing eligibility criteria for septic shock and
sepsis-associated acute kidney injury. We also discussed the research prospects
of applying visual analytics to clinical trials.

</details>


### [149] [An Analysis of Text Functions in Information Visualization](https://arxiv.org/abs/2507.12334)
*Chase Stokes,Anjana Arunkumar,Marti A. Hearst,Lace Padilla*

Main category: cs.HC

TL;DR: 本文提出了一个理解信息可视化中文本功能的框架，填补了现有分类的空白，并通过分析120个实际可视化案例和804个文本元素，识别了十种文本功能。


<details>
  <summary>Details</summary>
Motivation: 研究文本在可视化设计中的作用，填补现有分类的不足。

Method: 分析120个实际可视化案例和804个文本元素，识别文本功能并进行因子分析。

Result: 发现十种文本功能和四种文本驱动的设计策略，揭示了文本的多功能性和灵活性。

Conclusion: 该框架丰富了现有文本在可视化中作用的理解，强调了文本的多样性和多功能性。

Abstract: Text is an integral but understudied component of visualization design.
Although recent studies have examined how text elements (e.g., titles and
annotations) influence comprehension, preferences, and predictions, many
questions remain about textual design and use in practice. This paper
introduces a framework for understanding text functions in information
visualizations, building on and filling gaps in prior classifications and
taxonomies. Through an analysis of 120 real-world visualizations and 804 text
elements, we identified ten distinct text functions, ranging from identifying
data mappings to presenting valenced subtext. We further identify patterns in
text usage and conduct a factor analysis, revealing four overarching
text-informed design strategies: Attribution and Variables, Annotation-Centric
Design, Visual Embellishments, and Narrative Framing. In addition to these
factors, we explore features of title rhetoric and text multifunctionality,
while also uncovering previously unexamined text functions, such as text
replacing visual elements. Our findings highlight the flexibility of text,
demonstrating how different text elements in a given design can combine to
communicate, synthesize, and frame visual information. This framework adds
important nuance and detail to existing frameworks that analyze the diverse
roles of text in visualization.

</details>


### [150] [MExplore: an entity-based visual analytics approach for medical expertise acquisition](https://arxiv.org/abs/2507.12337)
*Xiao Pang,Yan Huang,Chang Liu,JiYuan Liu,MingYou Liu*

Main category: cs.HC

TL;DR: MExplore是一个交互式视觉分析系统，用于从非结构化医学文本中提取医学实体，并通过多级可视化框架支持医学专业知识的学习。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏从非结构化医学文本中提取专业知识的方法，而这些文本在医学文献中占很大比例且更具灵活性。

Method: 使用微调的BERT模型提取医学实体，并通过多级可视化框架进行交互式探索。

Result: 案例研究、用户研究和专家访谈表明，MExplore显著提升了医学专业知识的学习效果。

Conclusion: MExplore为从医学文本中获取和保留知识提供了一种有效的交互式方法。

Abstract: Acquiring medical expertise is a critical component of medical education and
professional development. While existing studies focus primarily on
constructing medical knowledge bases or developing learning tools based on the
structured, private healthcare data, they often lack methods for extracting
expertise from unstructured medical texts. These texts constitute a significant
portion of medical literature and offer greater flexibility and detail compared
to structured data formats. Furthermore, many studies fail to provide explicit
analytical and learning pathways in this context.
  This paper introduces MExplore, an interactive visual analytics system
designed to support the acquisition of medical expertise. To address the
challenges of the inconsistencies and confidentiality concerns inherent in
unstructured medical texts, we propose a workflow that employs a fine-tuned
BERT-based model to extract medical entities (MEs) from them. We then present a
novel multilevel visual analysis framework that integrates multiple coordinated
visualizations, enabling a progressive and interactive exploration of medical
knowledge.
  To assess the effectiveness of MExplore, we conducted three case studies, a
user study, and interviews with domain experts. The results indicate that the
system significantly enhances the medical expertise acquisition process,
providing an effective interactive approach for acquiring and retaining
knowledge from medical texts.

</details>


### [151] [Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight](https://arxiv.org/abs/2507.12377)
*Ke Er Amy Zhang,Jodie Jenkinson,Laura Garrison*

Main category: cs.HC

TL;DR: 通过对17位全球视觉数据记者的访谈进行解构性阅读，揭示了视觉数据新闻中隐含的对立信念（客观性/主观性、人文主义/机械主义），并通过谱系分析将其置于历史背景中。


<details>
  <summary>Details</summary>
Motivation: 探索语言意义的不稳定性，揭示视觉数据新闻中的隐含信念及其社会根源。

Method: 采用文学批评中的解构方法，结合谱系分析，对访谈进行解构性阅读。

Result: 发现视觉数据新闻中的对立信念是外部社会力量和范式变迁的产物。

Conclusion: 通过解构和谱系理论，重新定义视觉数据叙事的“成功”，并推动可视化研究的多样化。

Abstract: We conduct a deconstructive reading of a qualitative interview study with 17
visual data journalists from newsrooms across the globe. We borrow a
deconstruction approach from literary critique to explore the instability of
meaning in language and reveal implicit beliefs in words and ideas. Through our
analysis we surface two sets of opposing implicit beliefs in visual data
journalism: objectivity/subjectivity and humanism/mechanism. We contextualize
these beliefs through a genealogical analysis, which brings deconstruction
theory into practice by providing a historic backdrop for these opposing
perspectives. Our analysis shows that these beliefs held within visual data
journalism are not self-enclosed but rather a product of external societal
forces and paradigm shifts over time. Through this work, we demonstrate how
thinking with critical theories such as deconstruction and genealogy can
reframe "success" in visual data storytelling and diversify visualization
research outcomes. These efforts push the ways in which we as researchers
produce domain knowledge to examine the sociotechnical issues of today's values
towards datafication and data visualization.

</details>
