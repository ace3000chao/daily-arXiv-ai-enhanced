{"id": "2508.05722", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05722", "abs": "https://arxiv.org/abs/2508.05722", "authors": ["Rania Al-Sabbagh"], "title": "PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare", "comment": null, "summary": "This paper introduces PEACH, a sentence-aligned parallel English-Arabic\ncorpus of healthcare texts encompassing patient information leaflets and\neducational materials. The corpus contains 51,671 parallel sentences, totaling\napproximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths\nvary between 9.52 and 11.83 words on average. As a manually aligned corpus,\nPEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,\ntranslation studies, and natural language processing. It can be used to derive\nbilingual lexicons, adapt large language models for domain-specific machine\ntranslation, evaluate user perceptions of machine translation in healthcare,\nassess patient information leaflets and educational materials' readability and\nlay-friendliness, and as an educational resource in translation studies. PEACH\nis publicly accessible."}
{"id": "2508.05775", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05775", "abs": "https://arxiv.org/abs/2508.05775", "authors": ["Chi Zhang", "Changjia Zhu", "Junjie Xiong", "Xiaoran Xu", "Lingyao Li", "Yao Liu", "Zhuo Lu"], "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized content creation across\ndigital platforms, offering unprecedented capabilities in natural language\ngeneration and understanding. These models enable beneficial applications such\nas content generation, question and answering (Q&A), programming, and code\nreasoning. Meanwhile, they also pose serious risks by inadvertently or\nintentionally producing toxic, offensive, or biased content. This dual role of\nLLMs, both as powerful tools for solving real-world problems and as potential\nsources of harmful language, presents a pressing sociotechnical challenge. In\nthis survey, we systematically review recent studies spanning unintentional\ntoxicity, adversarial jailbreaking attacks, and content moderation techniques.\nWe propose a unified taxonomy of LLM-related harms and defenses, analyze\nemerging multimodal and LLM-assisted jailbreak strategies, and assess\nmitigation efforts, including reinforcement learning with human feedback\n(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the\nevolving landscape of LLM safety, identifies limitations in current evaluation\nmethodologies, and outlines future research directions to guide the development\nof robust and ethically aligned language technologies."}
{"id": "2508.05782", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05782", "abs": "https://arxiv.org/abs/2508.05782", "authors": ["Xiangyan Chen", "Yufeng Li", "Yujian Gan", "Arkaitz Zubiaga", "Matthew Purver"], "title": "FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification", "comment": null, "summary": "Large Language Models (LLMs) are known to produce hallucinations - factually\nincorrect or fabricated information - which poses significant challenges for\nmany Natural Language Processing (NLP) applications, such as dialogue systems.\nAs a result, detecting hallucinations has become a critical area of research.\nCurrent approaches to hallucination detection in dialogue systems primarily\nfocus on verifying the factual consistency of generated responses. However,\nthese responses often contain a mix of accurate, inaccurate or unverifiable\nfacts, making one factual label overly simplistic and coarse-grained. In this\npaper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact\nverification, which involves verifying atomic facts extracted from dialogue\nresponses. To support this, we construct a dataset based on publicly available\ndialogue datasets and evaluate it using various baseline methods. Experimental\nresults demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning\ncan enhance performance in dialogue fact verification. Despite this, the best\nF1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is\nonly 0.75, indicating that the benchmark remains a challenging task for future\nresearch. Our dataset and code will be public on GitHub."}
{"id": "2508.05803", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.05803", "abs": "https://arxiv.org/abs/2508.05803", "authors": ["Abishek Thamma", "Micha Heilbron"], "title": "Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models", "comment": null, "summary": "Human memory is fleeting. As words are processed, the exact wordforms that\nmake up incoming sentences are rapidly lost. Cognitive scientists have long\nbelieved that this limitation of memory may, paradoxically, help in learning\nlanguage - an idea supported by classic connectionist modelling work. The rise\nof Transformers appears to challenge this idea, as these models can learn\nlanguage effectively, despite lacking memory limitations or other architectural\nrecency biases. Here, we investigate the hypothesized benefit of fleeting\nmemory for language learning in tightly controlled experiments on transformer\nlanguage models. Training transformers with and without fleeting memory on a\ndevelopmentally realistic training set, we find that fleeting memory\nconsistently improves language learning (as quantified by both overall language\nmodelling performance and targeted syntactic evaluation) but, unexpectedly,\nimpairs surprisal-based prediction of human reading times. Interestingly,\nfollow up analyses revealed that this discrepancy - better language modeling,\nyet worse reading time prediction - could not be accounted for by prior\nexplanations of why better language models sometimes fit human reading time\nworse. Together, these results support a benefit of memory limitations on\nneural network language learning - but not on predicting behavior."}
{"id": "2508.05659", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.05659", "abs": "https://arxiv.org/abs/2508.05659", "authors": ["Jeroen F. Uleman", "Loes Crielaard", "Leonie K. Elsenburg", "Guido A. Veldhuis", "Karien Stronks", "Naja Hulvej Rod", "Rick Quax", "Vítor V. Vasconcelos"], "title": "Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty", "comment": "21 pages, 4 figures, 4 tables", "summary": "Causal loop diagrams (CLDs) are widely used in health and environmental\nresearch to represent hypothesized causal structures underlying complex\nproblems. However, as qualitative and static representations, CLDs are limited\nin their ability to support dynamic analysis and inform intervention\nstrategies. Additionally, quantitative CLD analysis methods like network\ncentrality analysis often lead to false inference. We propose\nDiagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory\nsystem dynamics models (SDMs) in the absence of empirical data. With minimal\nuser input - following a protocol to label variables as stocks,\nflows/auxiliaries, or constants - D2D leverages the structural information\nalready encoded in CLDs, namely, link existence and polarity, to simulate\nhypothetical interventions and explore potential leverage points under\nuncertainty. Results suggest that D2D helps distinguish between high- and\nlow-ranked leverage points. We compare D2D to a data-driven SDM constructed\nfrom the same CLD and variable labeling. D2D showed greater consistency with\nthe data-driven model than network centrality analysis, while providing\nuncertainty estimates and guidance for future data collection. The method is\nimplemented in an open-source Python package and a web-based application to\nsupport further testing and lower the barrier to dynamic modeling for\nresearchers working with CLDs. We expect additional validation will further\nestablish the approach's utility across a broad range of cases and domains."}
{"id": "2508.05731", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05731", "abs": "https://arxiv.org/abs/2508.05731", "authors": ["Yuhang Liu", "Zeyu Liu", "Shuanghe Zhu", "Pengxiang Li", "Congkai Xie", "Jiasheng Wang", "Xueyu Hu", "Xiaotian Han", "Jianbo Yuan", "Xinyao Wang", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization", "comment": "11 pages, 3 figures", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1."}
{"id": "2508.05637", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05637", "abs": "https://arxiv.org/abs/2508.05637", "authors": ["Siddharth Gangwar", "David A. Selby", "Sebastian J. Vollmer"], "title": "Automated Visualization Makeovers with LLMs", "comment": null, "summary": "Making a good graphic that accurately and efficiently conveys the desired\nmessage to the audience is both an art and a science, typically not taught in\nthe data science curriculum. Visualisation makeovers are exercises where the\ncommunity exchange feedback to improve charts and data visualizations. Can\nmulti-modal large language models (LLMs) emulate this task? Given a plot in the\nform of an image file, or the code used to generate it, an LLM, primed with a\nlist of visualization best practices, is employed to semi-automatically\ngenerate constructive criticism to produce a better plot. Our system is centred\naround prompt engineering of a pre-trained model, relying on a combination of\nuserspecified guidelines and any latent knowledge of data visualization\npractices that might lie within an LLMs training corpus. Unlike other works,\nthe focus is not on generating valid visualization scripts from raw data or\nprompts, but on educating the user how to improve their existing data\nvisualizations according to an interpretation of best practices. A quantitative\nevaluation is performed to measure the sensitivity of the LLM agent to various\nplotting issues across different chart types. We make the tool available as a\nsimple self-hosted applet with an accessible Web interface."}
{"id": "2508.05830", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05830", "abs": "https://arxiv.org/abs/2508.05830", "authors": ["Tong Li", "Rasiq Hussain", "Mehak Gupta", "Joshua R. Oltmanns"], "title": "\"Mirror\" Language AI Models of Depression are Criterion-Contaminated", "comment": "39 pages, 9 figures", "summary": "A growing number of studies show near-perfect LLM language-based prediction\nof depression assessment scores (up to R2 of .70). However, many develop these\nmodels directly from language responses to depression assessments. These\n\"Mirror models\" suffer from \"criterion contamination\", which arises when a\npredicted score depends in part on the predictors themselves. This causes\nartificial effect size inflation which reduces model generalizability. The\npresent study compares the performance of Mirror models versus \"Non-Mirror\nmodels\", which are developed from language that does not mirror the assessment\nthey are developed to predict. N = 110 research participants completed two\ndifferent interviews: structured diagnostic and life history interviews. GPT-4,\nGPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic\ninterview depression scores from the two transcripts separately. Mirror models\n(using structured diagnostic data) showed very large effect sizes (e.g., R2 =\n.80). As expected, NonMirror models (using life history data) demonstrated\nsmaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror\nand Non-Mirror model-predicted structured interview depression scores were\ncorrelated with self-reported depression symptoms, Mirror and NonMirror\nperformed the same (e.g., r = ~.54), indicating that Mirror models contain bias\nperhaps due to criterion contamination. Topic modeling identified clusters\nacross Mirror and Non-Mirror models, as well as between true-positive and\nfalse-positive predictions. In this head-to-head comparison study, Mirror\nlanguage AI models of depression showed artificially inflated effect sizes and\nless generalizability. As language AI models for depression continue to evolve,\nincorporating Non-Mirror models may identify interpretable, and generalizable\nsemantic features that have unique utility in real-world psychological\nassessment."}
{"id": "2508.05724", "categories": ["cs.LG", "physics.data-an", "68T07, 81-08, 05C90", "I.2.6; G.2.2; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.05724", "abs": "https://arxiv.org/abs/2508.05724", "authors": ["Massimiliano Romiti"], "title": "A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics", "comment": "14 pages, 9 figures", "summary": "This work introduces a novel framework for representing and analyzing\nphysical laws as a weighted knowledge graph. We constructed a database of 659\ndistinct physical equations, subjected to rigorous semantic cleaning to resolve\nnotational ambiguities, resulting in a corpus of 400 advanced physics\nequations. We developed an enhanced graph representation where both physical\nconcepts and equations are nodes, connected by weighted inter-equation bridges.\nThese weights are objectively defined using normalized metrics for variable\noverlap, physics-informed importance scores, and bibliometric data. A Graph\nAttention Network (GAT) was trained for link prediction, achieving a test AUC\nof 0.9742 +/- 0.0018 across five independent runs, significantly outperforming\nboth classical heuristics (best baseline AUC: 0.9487) and established GNN\narchitectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing\nconfirmed significance of all comparisons (p < 0.05), with 2.7% improvement\nover the best baseline. Our analysis reveals three key findings: (i) The model\nautonomously rediscovers the known macroscopic structure of physics,\nidentifying strong conceptual axes between Electromagnetism and Statistical\nMechanics. (ii) It identifies central hub equations that serve as critical\nbridges between multiple physical domains. (iii) The model generates stable,\ncomputationally-derived hypotheses for cross-domain relationships, identifying\nboth known principles and suggesting novel mathematical analogies for further\ntheoretical investigation. The framework can generate hundreds of such\nhypotheses, enabling the creation of specialized datasets for targeted analysis\nof specific physics subfields. Code and data available at\nhttps://github.com/kingelanci/graphysics"}
{"id": "2508.05766", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.05766", "abs": "https://arxiv.org/abs/2508.05766", "authors": ["Bo Wen"], "title": "A Framework for Inherently Safer AGI through Language-Mediated Active Inference", "comment": null, "summary": "This paper proposes a novel framework for developing safe Artificial General\nIntelligence (AGI) by combining Active Inference principles with Large Language\nModels (LLMs). We argue that traditional approaches to AI safety, focused on\npost-hoc interpretability and reward engineering, have fundamental limitations.\nWe present an architecture where safety guarantees are integrated into the\nsystem's core design through transparent belief representations and\nhierarchical value alignment. Our framework leverages natural language as a\nmedium for representing and manipulating beliefs, enabling direct human\noversight while maintaining computational tractability. The architecture\nimplements a multi-agent system where agents self-organize according to Active\nInference principles, with preferences and safety constraints flowing through\nhierarchical Markov blankets. We outline specific mechanisms for ensuring\nsafety, including: (1) explicit separation of beliefs and preferences in\nnatural language, (2) bounded rationality through resource-aware free energy\nminimization, and (3) compositional safety through modular agent structures.\nThe paper concludes with a research agenda centered on the Abstraction and\nReasoning Corpus (ARC) benchmark, proposing experiments to validate our\nframework's safety properties. Our approach offers a path toward AGI\ndevelopment that is inherently safer, rather than retrofitted with safety\nmeasures."}
{"id": "2508.05646", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05646", "abs": "https://arxiv.org/abs/2508.05646", "authors": ["Thomas Sievers"], "title": "A Humanoid Social Robot as a Teaching Assistant in the Classroom", "comment": null, "summary": "Although innovation and the support of new technologies are much needed to\nease the burden on the education system, social robots in schools to help\nteachers with educational tasks are rare. Child-Robot Interaction (CRI) could\nsupport teachers and add an embodied social component to modern multi-modal and\nmulti-sensory learning environments already in use. The social robot Pepper,\nconnected to the Large Language Model (LLM) ChatGPT, was used in a high school\nclassroom to teach new learning content to groups of students. I tested the\ntechnical possibilities with the robot on site and asked the students about\ntheir acceptance and perceived usefulness of teaching with the help of a social\nrobot. All participants felt that the robot's presentation of the learning\nmaterial was appropriate or at least partially appropriate and that its use\nmade sense."}
{"id": "2508.05843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05843", "abs": "https://arxiv.org/abs/2508.05843", "authors": ["Miles Gilberti", "Shane Storks", "Huteng Dai"], "title": "Discovering Properties of Inflectional Morphology in Neural Emergent Communication", "comment": null, "summary": "Emergent communication (EmCom) with deep neural network-based agents promises\nto yield insights into the nature of human language, but remains focused\nprimarily on a few subfield-specific goals and metrics that prioritize\ncommunication schemes which represent attributes with unique characters\none-to-one and compose them syntactically. We thus reinterpret a common EmCom\nsetting, the attribute-value reconstruction game, by imposing a\nsmall-vocabulary constraint to simulate double articulation, and formulating a\nnovel setting analogous to naturalistic inflectional morphology (enabling\nmeaningful comparison to natural language communication schemes). We develop\nnew metrics and explore variations of this game motivated by real properties of\ninflectional morphology: concatenativity and fusionality. Through our\nexperiments, we discover that simulated phonological constraints encourage\nconcatenative morphology, and emergent languages replicate the tendency of\nnatural languages to fuse grammatical attributes."}
{"id": "2508.05778", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.05778", "abs": "https://arxiv.org/abs/2508.05778", "authors": ["Jaemin Oh", "Jinsil Lee", "Youngjoon Hong"], "title": "Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems", "comment": "21 pages, 5 figures, 6 tables", "summary": "Nudging is an empirical data assimilation technique that incorporates an\nobservation-driven control term into the model dynamics. The trajectory of the\nnudged system approaches the true system trajectory over time, even when the\ninitial conditions differ. For linear state space models, such control terms\ncan be derived under mild assumptions. However, designing effective nudging\nterms becomes significantly more challenging in the nonlinear setting. In this\nwork, we propose neural network nudging, a data-driven method for learning\nnudging terms in nonlinear state space models. We establish a theoretical\nexistence result based on the Kazantzis--Kravaris--Luenberger observer theory.\nThe proposed approach is evaluated on three benchmark problems that exhibit\nchaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and\nthe Kolmogorov flow."}
{"id": "2508.05776", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05776", "abs": "https://arxiv.org/abs/2508.05776", "authors": ["Thomas L. Griffiths", "Brenden M. Lake", "R. Thomas McCoy", "Ellie Pavlick", "Taylor W. Webb"], "title": "Whither symbols in the era of advanced neural networks?", "comment": null, "summary": "Some of the strongest evidence that human minds should be thought about in\nterms of symbolic systems has been the way they combine ideas, produce novelty,\nand learn quickly. We argue that modern neural networks -- and the artificial\nintelligence systems built upon them -- exhibit similar abilities. This\nundermines the argument that the cognitive processes and representations used\nby human minds are symbolic, although the fact that these neural networks are\ntypically trained on data generated by symbolic systems illustrates that such\nsystems play an important role in characterizing the abstract problems that\nhuman minds have to solve. This argument leads us to offer a new agenda for\nresearch on the symbolic basis of human thought."}
{"id": "2508.05653", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05653", "abs": "https://arxiv.org/abs/2508.05653", "authors": ["Jules Clerc", "Domitile Lourdeaux", "Mohamed Sallak", "Johann Barbier", "Marc Ravaine"], "title": "Modeling Interactive Narrative Systems: A Formal Approach", "comment": null, "summary": "Interactive Narrative Systems (INS) have revolutionized digital experiences\nby empowering users to actively shape their stories, diverging from traditional\npassive storytelling. However, the field faces challenges due to fragmented\nresearch efforts and diverse system representations. This paper introduces a\nformal representation framework for INS, inspired by diverse approaches from\nthe state of the art. By providing a consistent vocabulary and modeling\nstructure, the framework facilitates the analysis, the description and\ncomparison of INS properties. Experimental validations on the \"Little Red\nRiding Hood\" scenario highlight the usefulness of the proposed formalism and\nits impact on improving the evaluation of INS. This work aims to foster\ncollaboration and coherence within the INS research community by proposing a\nmethodology for formally representing these systems."}
{"id": "2508.05880", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05880", "abs": "https://arxiv.org/abs/2508.05880", "authors": ["Sree Bhattacharyya", "Lucas Craig", "Tharun Dilliraj", "Jia Li", "James Z. Wang"], "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models", "comment": null, "summary": "Affective Computing has been established as a crucial field of inquiry to\nadvance the holistic development of Artificial Intelligence (AI) systems.\nFoundation models -- especially Large Language Models (LLMs) -- have been\nevaluated, trained, or instruction-tuned in several past works, to become\nbetter predictors or generators of emotion. Most of these studies, however,\napproach emotion-related tasks in a supervised manner, assessing or training\nthe capabilities of LLMs using discrete emotion labels associated with stimuli\n(e.g., text, images, video, audio). Evaluation studies, in particular, have\noften been limited to standard and superficial emotion-related tasks, such as\nthe recognition of evoked or expressed emotions. In this paper, we move beyond\nsurface-level emotion tasks to investigate how LLMs reason about emotions\nthrough cognitive dimensions. Drawing from cognitive appraisal theory, we\nexamine whether LLMs produce coherent and plausible cognitive reasoning when\nreasoning about emotionally charged stimuli. We introduce a large-scale\nbenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal\ncognitive structures implicitly used by LLMs for emotional reasoning. Through a\nplethora of evaluation experiments and analysis, we seek to answer: (a) Are\nmodels more likely to implicitly rely on specific cognitive appraisal\ndimensions?, (b) What cognitive dimensions are important for characterizing\nspecific emotions?, and, (c) Can the internal representations of different\nemotion categories in LLMs be interpreted through cognitive appraisal\ndimensions? Our results and analyses reveal diverse reasoning patterns across\ndifferent LLMs. Our benchmark and code will be made publicly available."}
{"id": "2508.05791", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05791", "abs": "https://arxiv.org/abs/2508.05791", "authors": ["Haoran Li", "Lihao Mai", "Muhao Guo", "Jiaqi Wu", "Yang Weng", "Yannan Sun", "Ce Jimmy Liu"], "title": "From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data", "comment": "10 pages", "summary": "Accurate distribution grid topology is essential for reliable modern grid\noperations. However, real-world utility data originates from multiple sources\nwith varying characteristics and levels of quality. In this work, developed in\ncollaboration with Oncor Electric Delivery, we propose a scalable framework\nthat reconstructs a trustworthy grid topology by systematically integrating\nheterogeneous data. We observe that distribution topology is fundamentally\ngoverned by two complementary dimensions: the spatial layout of physical\ninfrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the\nsystem in the signal domain (e.g., voltage time series). When jointly\nleveraged, these dimensions support a complete and physically coherent\nreconstruction of network connectivity. To address the challenge of uneven data\nquality without compromising observability, we introduce a confidence-aware\ninference mechanism that preserves structurally informative yet imperfect\ninputs, while quantifying the reliability of each inferred connection for\noperator interpretation. This soft handling of uncertainty is tightly coupled\nwith hard enforcement of physical feasibility: we embed operational\nconstraints, such as transformer capacity limits and radial topology\nrequirements, directly into the learning process. Together, these components\nensure that inference is both uncertainty-aware and structurally valid,\nenabling rapid convergence to actionable, trustworthy topologies under\nreal-world deployment conditions. The proposed framework is validated using\ndata from over 8000 meters across 3 feeders in Oncor's service territory,\ndemonstrating over 95% accuracy in topology reconstruction and substantial\nimprovements in confidence calibration and computational efficiency relative to\nbaseline methods."}
{"id": "2508.05792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05792", "abs": "https://arxiv.org/abs/2508.05792", "authors": ["Kausik Lakkaraju", "Siva Likitha Valluru", "Biplav Srivastava"], "title": "Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making", "comment": null, "summary": "Current eXplainable AI (XAI) methods largely serve developers, often focusing\non justifying model outputs rather than supporting diverse stakeholder needs. A\nrecent shift toward Evaluative AI reframes explanation as a tool for hypothesis\ntesting, but still focuses primarily on operational organizations. We introduce\nHolistic-XAI (H-XAI), a unified framework that integrates causal rating methods\nwith traditional XAI methods to support explanation as an interactive,\nmulti-method process. H-XAI allows stakeholders to ask a series of questions,\ntest hypotheses, and compare model behavior against automatically constructed\nrandom and biased baselines. It combines instance-level and global\nexplanations, adapting to each stakeholder's goals, whether understanding\nindividual decisions, assessing group-level bias, or evaluating robustness\nunder perturbations. We demonstrate the generality of our approach through two\ncase studies spanning six scenarios: binary credit risk classification and\nfinancial time-series forecasting. H-XAI fills critical gaps left by existing\nXAI methods by combining causal ratings and post-hoc explanations to answer\nstakeholder-specific questions at both the individual decision level and the\noverall model level."}
{"id": "2508.05913", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05913", "abs": "https://arxiv.org/abs/2508.05913", "authors": ["Stefan Pasch", "Min Chul Cha"], "title": "Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction", "comment": null, "summary": "As AI systems become increasingly embedded in organizational workflows and\nconsumer applications, ethical principles such as fairness, transparency, and\nrobustness have been widely endorsed in policy and industry guidelines.\nHowever, there is still scarce empirical evidence on whether these principles\nare recognized, valued, or impactful from the perspective of users. This study\ninvestigates the link between ethical AI and user satisfaction by analyzing\nover 100,000 user reviews of AI products from G2. Using transformer-based\nlanguage models, we measure sentiment across seven ethical dimensions defined\nby the EU Ethics Guidelines for Trustworthy AI. Our findings show that all\nseven dimensions are positively associated with user satisfaction. Yet, this\nrelationship varies systematically across user and product types. Technical\nusers and reviewers of AI development platforms more frequently discuss\nsystem-level concerns (e.g., transparency, data governance), while\nnon-technical users and reviewers of end-user applications emphasize\nhuman-centric dimensions (e.g., human agency, societal well-being). Moreover,\nthe association between ethical AI and user satisfaction is significantly\nstronger for non-technical users and end-user applications across all\ndimensions. Our results highlight the importance of ethical AI design from\nusers' perspectives and underscore the need to account for contextual\ndifferences across user roles and product types."}
{"id": "2508.05909", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05909", "abs": "https://arxiv.org/abs/2508.05909", "authors": ["Zhanghao Hu", "Qinglin Zhu", "Siya Qi", "Yulan He", "Hanqi Yan", "Lin Gui"], "title": "Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation", "comment": null, "summary": "Large Language Models (LLMs) have shown improved generation performance\nthrough retrieval-augmented generation (RAG) following the retriever-reader\nparadigm, which supplements model inputs with externally retrieved knowledge.\nHowever, prior work often evaluates RAG holistically, assessing the retriever\nand reader jointly, making it difficult to isolate the true contribution of\nretrieval, particularly given the prompt sensitivity of LLMs used as readers.\nWe introduce Spectrum Projection Score (SPS), a lightweight, supervision-free\nmetric that allows the reader to gauge the semantic alignment of a retrieved\nsummary with its hidden representation by comparing the area formed by\ngenerated tokens from the summary, and the principal directions of subspace in\nthe reader and to measure the relevance. Building on SPS we present xCompress,\nan inference time controller framework that dynamically samples, ranks, and\ncompresses retrieval summary candidates. Extensive experiments on five QA\nbenchmarks with four open source LLMs show that SPS not only enhances\nperformance across a range of tasks but also provides a principled perspective\non the interaction between retrieval and generation."}
{"id": "2508.05831", "categories": ["cs.LG", "cs.NA", "math.NA", "15A29 Inverse problems in linear algebra 65F22, 68T07, 65F05, 62C12", "G.1.3; F.2.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.05831", "abs": "https://arxiv.org/abs/2508.05831", "authors": ["Alexander DeLise", "Kyle Loh", "Krish Patel", "Meredith Teague", "Andrea Arnold", "Matthias Chung"], "title": "Optimal Linear Baseline Models for Scientific Machine Learning", "comment": "40 pages, 10 Figures, 9 Tables", "summary": "Across scientific domains, a fundamental challenge is to characterize and\ncompute the mappings from underlying physical processes to observed signals and\nmeasurements. While nonlinear neural networks have achieved considerable\nsuccess, they remain theoretically opaque, which hinders adoption in contexts\nwhere interpretability is paramount. In contrast, linear neural networks serve\nas a simple yet effective foundation for gaining insight into these complex\nrelationships. In this work, we develop a unified theoretical framework for\nanalyzing linear encoder-decoder architectures through the lens of Bayes risk\nminimization for solving data-driven scientific machine learning problems. We\nderive closed-form, rank-constrained linear and affine linear optimal mappings\nfor forward modeling and inverse recovery tasks. Our results generalize\nexisting formulations by accommodating rank-deficiencies in data, forward\noperators, and measurement processes. We validate our theoretical results by\nconducting numerical experiments on datasets from simple biomedical imaging,\nfinancial factor analysis, and simulations involving nonlinear fluid dynamics\nvia the shallow water equations. This work provides a robust baseline for\nunderstanding and benchmarking learned neural network models for scientific\nmachine learning problems."}
{"id": "2508.05855", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05855", "abs": "https://arxiv.org/abs/2508.05855", "authors": ["Zixia Wang", "Jia Hu", "Ronghui Mu"], "title": "Safety of Embodied Navigation: A Survey", "comment": null, "summary": "As large language models (LLMs) continue to advance and gain influence, the\ndevelopment of embodied AI has accelerated, drawing significant attention,\nparticularly in navigation scenarios. Embodied navigation requires an agent to\nperceive, interact with, and adapt to its environment while moving toward a\nspecified target in unfamiliar settings. However, the integration of embodied\nnavigation into critical applications raises substantial safety concerns. Given\ntheir deployment in dynamic, real-world environments, ensuring the safety of\nsuch systems is critical. This survey provides a comprehensive analysis of\nsafety in embodied navigation from multiple perspectives, encompassing attack\nstrategies, defense mechanisms, and evaluation methodologies. Beyond conducting\na comprehensive examination of existing safety challenges, mitigation\ntechnologies, and various datasets and metrics that assess effectiveness and\nrobustness, we explore unresolved issues and future research directions in\nembodied navigation safety. These include potential attack methods, mitigation\nstrategies, more reliable evaluation techniques, and the implementation of\nverification frameworks. By addressing these critical gaps, this survey aims to\nprovide valuable insights that can guide future research toward the development\nof safer and more reliable embodied navigation systems. Furthermore, the\nfindings of this study have broader implications for enhancing societal safety\nand increasing industrial efficiency."}
{"id": "2508.05933", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05933", "abs": "https://arxiv.org/abs/2508.05933", "authors": ["Xueyuan Xu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "title": "REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition", "comment": null, "summary": "The affective brain-computer interface is a crucial technology for affective\ninteraction and emotional intelligence, emerging as a significant area of\nresearch in the human-computer interaction. Compared to single-type features,\nmulti-type EEG features provide a multi-level representation for analyzing\nmulti-dimensional emotions. However, the high dimensionality of multi-type EEG\nfeatures, combined with the relatively small number of high-quality EEG\nsamples, poses challenges such as classifier overfitting and suboptimal\nreal-time performance in multi-dimensional emotion recognition. Moreover,\npractical applications of affective brain-computer interface frequently\nencounters partial absence of multi-dimensional emotional labels due to the\nopen nature of the acquisition environment, and ambiguity and variability in\nindividual emotion perception. To address these challenges, this study proposes\na novel EEG feature selection method for missing multi-dimensional emotion\nrecognition. The method leverages adaptive orthogonal non-negative matrix\nfactorization to reconstruct the multi-dimensional emotional label space\nthrough second-order and higher-order correlations, which could reduce the\nnegative impact of missing values and outliers on label reconstruction.\nSimultaneously, it employs least squares regression with graph-based manifold\nlearning regularization and global feature redundancy minimization\nregularization to enable EEG feature subset selection despite missing\ninformation, ultimately achieving robust EEG-based multi-dimensional emotion\nrecognition. Simulation experiments on three widely used multi-dimensional\nemotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method\noutperforms thirteen advanced feature selection methods in terms of robustness\nfor EEG emotional feature selection."}
{"id": "2508.05938", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "pdf": "https://arxiv.org/pdf/2508.05938", "abs": "https://arxiv.org/abs/2508.05938", "authors": ["Rafal Kocielnik", "Min Kim", "Penphob", "Boonyarungsrit", "Fereshteh Soltani", "Deshawn Sambrano", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale", "comment": "9 pages, 4 figures, 4 tables", "summary": "Detecting prosociality in text--communication intended to affirm, support, or\nimprove others' behavior--is a novel and increasingly important challenge for\ntrust and safety systems. Unlike toxic content detection, prosociality lacks\nwell-established definitions and labeled data, requiring new approaches to both\nannotation and deployment. We present a practical, three-stage pipeline that\nenables scalable, high-precision prosocial content classification while\nminimizing human labeling effort and inference costs. First, we identify the\nbest LLM-based labeling strategy using a small seed set of human-labeled\nexamples. We then introduce a human-AI refinement loop, where annotators review\nhigh-disagreement cases between GPT-4 and humans to iteratively clarify and\nexpand the task definition-a critical step for emerging annotation tasks like\nprosociality. This process results in improved label quality and definition\nalignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train\na two-stage inference system: a lightweight classifier handles high-confidence\npredictions, while only $\\sim$35\\% of ambiguous instances are escalated to\nGPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving\nhigh precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI\ninteraction, careful task formulation, and deployment-aware architecture design\ncan unlock scalable solutions for novel responsible AI tasks."}
{"id": "2508.05836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05836", "abs": "https://arxiv.org/abs/2508.05836", "authors": ["Rituparna Datta", "Nibir Chandra Mandal"], "title": "An Effective Approach for Node Classification in Textual Graphs", "comment": null, "summary": "Textual Attribute Graphs (TAGs) are critical for modeling complex networks\nlike citation networks, but effective node classification remains challenging\ndue to difficulties in integrating rich semantics from text with structural\ngraph information. Existing methods often struggle with capturing nuanced\ndomain-specific terminology, modeling long-range dependencies, adapting to\ntemporal evolution, and scaling to massive datasets. To address these issues,\nwe propose a novel framework that integrates TAPE (Text-Attributed Graph\nRepresentation Enhancement) with Graphormer. Our approach leverages a large\nlanguage model (LLM), specifically ChatGPT, within the TAPE framework to\ngenerate semantically rich explanations from paper content, which are then\nfused into enhanced node representations. These embeddings are combined with\nstructural features using a novel integration layer with learned attention\nweights. Graphormer's path-aware position encoding and multi-head attention\nmechanisms are employed to effectively capture long-range dependencies across\nthe citation network. We demonstrate the efficacy of our framework on the\nchallenging ogbn-arxiv dataset, achieving state-of-the-art performance with a\nclassification accuracy of 0.772, significantly surpassing the best GCN\nbaseline of 0.713. Our method also yields strong results in precision (0.671),\nrecall (0.577), and F1-score (0.610). We validate our approach through\ncomprehensive ablation studies that quantify the contribution of each\ncomponent, demonstrating the synergy between semantic and structural\ninformation. Our framework provides a scalable and robust solution for node\nclassification in dynamic TAGs, offering a promising direction for future\nresearch in knowledge systems and scientific discovery."}
{"id": "2508.05888", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.05888", "abs": "https://arxiv.org/abs/2508.05888", "authors": ["Sahil Bansal", "Sai Shruthi Sistla", "Aarti Arikatala", "Sebastian Schreiber"], "title": "Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning", "comment": null, "summary": "Effective tool retrieval is essential for AI agents to select from a vast\narray of tools when identifying and planning actions in the context of complex\nuser queries. Despite its central role in planning, this aspect remains\nunderexplored in the literature. Traditional approaches rely primarily on\nsimilarities between user queries and tool descriptions, which significantly\nlimits retrieval accuracy, specifically when handling multi-step user requests.\nTo address these limitations, we propose a Knowledge Graph (KG)-based tool\nretrieval framework that captures the semantic relationships between tools and\ntheir functional dependencies. Our retrieval algorithm leverages ensembles of\n1-hop ego tool graphs to model direct and indirect connections between tools,\nenabling more comprehensive and contextual tool selection for multi-step tasks.\nWe evaluate our approach on a synthetically generated internal dataset across\nsix defined user classes, extending previous work on coherent dialogue\nsynthesis and too retrieval benchmarks. Results demonstrate that our tool\ngraph-based method achieves 91.85% tool coverage on the micro-average Complete\nRecall metric, compared to 89.26% for re-ranked semantic-lexical hybrid\nretrieval, the strongest non-KG baseline in our experiments. These findings\nsupport our hypothesis that the structural information in the KG provides\ncomplementary signals to pure similarity matching, particularly for queries\nrequiring sequential tool composition."}
{"id": "2508.05934", "categories": ["cs.HC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05934", "abs": "https://arxiv.org/abs/2508.05934", "authors": ["Xueyuan Xu", "Tianze Yu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "title": "ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection", "comment": null, "summary": "Recently, multi-modal physiological signals based emotion recognition has\ngarnered increasing attention in the field of brain-computer interfaces.\nNevertheness, the associated multi-modal physiological features are often\nhigh-dimensional and inevitably include irrelevant, redundant, and noisy\nrepresentation, which can easily lead to overfitting, poor performance, and\nhigh computational complexity in emotion classifiers. Feature selection has\nbeen widely applied to address these challenges. However, previous studies\ngenerally assumed that multi-modal physiological data are complete, whereas in\nreality, the data are often incomplete due to the openness of the acquisition\nand operational environment. For example, a part of samples are available in\nseveral modalities but not in others. To address this issue, we propose a novel\nmethod for incomplete multi-modal physiological signal feature selection called\nadaptive shared latent structure learning (ASLSL). Based on the property that\nsimilar features share similar emotional labels, ASLSL employs adaptive shared\nlatent structure learning to explore a common latent space shared for\nincomplete multi-modal physiological signals and multi-dimensional emotional\nlabels, thereby mitigating the impact of missing information and mining\nconsensus information. Two most popular multi-modal physiological emotion\ndatasets (DEAP and DREAMER) with multi-dimensional emotional labels were\nutilized to compare the performance between compare ASLSL and seventeen feature\nselection methods. Comprehensive experimental results on these datasets\ndemonstrate the effectiveness of ASLSL."}
{"id": "2508.05987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05987", "abs": "https://arxiv.org/abs/2508.05987", "authors": ["Chunyun Zhang", "Hongyan Zhao", "Chaoran Cui", "Qilong Song", "Zhiqing Lu", "Shuai Gong", "Kailin Liu"], "title": "Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring", "comment": null, "summary": "Cross-topic automated essay scoring (AES) aims to develop a transferable\nmodel capable of effectively evaluating essays on a target topic. A significant\nchallenge in this domain arises from the inherent discrepancies between topics.\nWhile existing methods predominantly focus on extracting topic-shared features\nthrough distribution alignment of source and target topics, they often neglect\ntopic-specific features, limiting their ability to assess critical traits such\nas topic adherence. To address this limitation, we propose an Adversarial\nTOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns\ntopic-shared and topic-specific features to improve cross-topic AES. ATOP\nachieves this by optimizing a learnable topic-aware prompt--comprising both\nshared and specific components--to elicit relevant knowledge from pre-trained\nlanguage models (PLMs). To enhance the robustness of topic-shared prompt\nlearning and mitigate feature scale sensitivity introduced by topic alignment,\nwe incorporate adversarial training within a unified regression and\nclassification framework. In addition, we employ a neighbor-based classifier to\nmodel the local structure of essay representations and generate pseudo-labels\nfor target-topic essays. These pseudo-labels are then used to guide the\nsupervised learning of topic-specific prompts tailored to the target topic.\nExtensive experiments on the publicly available ASAP++ dataset demonstrate that\nATOP significantly outperforms existing state-of-the-art methods in both\nholistic and multi-trait essay scoring. The implementation of our method is\npublicly available at: https://anonymous.4open.science/r/ATOP-A271."}
{"id": "2508.05876", "categories": ["cs.LG", "astro-ph.EP", "astro-ph.IM", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.05876", "abs": "https://arxiv.org/abs/2508.05876", "authors": ["Francesca Ferrara", "Lander W. Schillinger Arana", "Florian Dörfler", "Sarah H. Q. Li"], "title": "A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance", "comment": "16 pages, 13 figures, submitted to the 2025 Astrodynamics Specialist\n  Conference", "summary": "This work presents a Markov decision process (MDP) framework to model\ndecision-making for collision avoidance maneuver (CAM) and a reinforcement\nlearning policy gradient (RL-PG) algorithm to train an autonomous guidance\npolicy using historic CAM data. In addition to maintaining acceptable collision\nrisks, this approach seeks to minimize the average fuel consumption of CAMs by\nmaking early maneuver decisions. We model CAM as a continuous state, discrete\naction and finite horizon MDP, where the critical decision is determining when\nto initiate the maneuver. The MDP model also incorporates analytical models for\nconjunction risk, propellant consumption, and transit orbit geometry. The\nMarkov policy effectively trades-off maneuver delay-which improves the\nreliability of conjunction risk indicators-with propellant consumption-which\nincreases with decreasing maneuver time. Using historical data of tracked\nconjunction events, we verify this framework and conduct an extensive ablation\nstudy on the hyper-parameters used within the MDP. On synthetic conjunction\nevents, the trained policy significantly minimizes both the overall and average\npropellant consumption per CAM when compared to a conventional cut-off policy\nthat initiates maneuvers 24 hours before the time of closest approach (TCA). On\nhistorical conjunction events, the trained policy consumes more propellant\noverall but reduces the average propellant consumption per CAM. For both\nhistorical and synthetic conjunction events, the trained policy achieves equal\nif not higher overall collision risk guarantees."}
{"id": "2508.05996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05996", "abs": "https://arxiv.org/abs/2508.05996", "authors": ["Kaitao Chen", "Mianxin Liu", "Daoming Zong", "Chaoyue Ding", "Shaohao Rui", "Yankai Jiang", "Mu Zhou", "Xiaosong Wang"], "title": "Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making", "comment": "14 pages, 4 figures", "summary": "Complex medical decision-making involves cooperative workflows operated by\ndifferent clinicians. Designing AI multi-agent systems can expedite and augment\nhuman-level clinical decision-making. Existing multi-agent researches primarily\nfocus on language-only tasks, yet their extension to multimodal scenarios\nremains challenging. A blind combination of diverse vision-language models\n(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are\nless capable in instruction following and importantly self-reflection, compared\nto large language models (LLMs) of comparable sizes. This disparity largely\nconstrains VLMs' ability in cooperative workflows. In this study, we propose\nMedOrch, a mediator-guided multi-agent collaboration framework for medical\nmultimodal decision-making. MedOrch employs an LLM-based mediator agent that\nenables multiple VLM-based expert agents to exchange and reflect on their\noutputs towards collaboration. We utilize multiple open-source general-purpose\nand domain-specific VLMs instead of costly GPT-series models, revealing the\nstrength of heterogeneous models. We show that the collaboration within\ndistinct VLM-based agents can surpass the capabilities of any individual agent.\nWe validate our approach on five medical vision question answering benchmarks,\ndemonstrating superior collaboration performance without model training. Our\nfindings underscore the value of mediator-guided multi-agent collaboration in\nadvancing medical multimodal intelligence. Our code will be made publicly\navailable."}
{"id": "2508.05940", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.05940", "abs": "https://arxiv.org/abs/2508.05940", "authors": ["Kathy Cheng", "Alison Olechowski", "Shurui Zhou"], "title": "It's a Complete Haystack: Understanding Dependency Management Needs in Computer-Aided Design", "comment": "To be published in the Proceedings of the ACM on Human-Computer\n  Interaction, Volume 9, Issue CSCW2", "summary": "In today's landscape, hardware development teams face increasing demands for\nbetter quality products, greater innovation, and shorter manufacturing lead\ntimes. Despite the need for more efficient and effective processes, hardware\ndesigners continue to struggle with a lack of awareness of design changes and\nother collaborators' actions, a persistent issue in decades of CSCW research.\nOne significant and unaddressed challenge is understanding and managing\ndependencies between 3D CAD (computer-aided design) models, especially when\nproducts can contain thousands of interconnected components. In this two-phase\nformative study, we explore designers' pain points of CAD dependency management\nthrough a thematic analysis of 100 online forum discussions and semi-structured\ninterviews with 10 designers. We identify nine key challenges related to the\ntraceability, navigation, and consistency of CAD dependencies, that harm the\neffective coordination of hardware development teams. To address these\nchallenges, we propose design goals and necessary features to enhance hardware\ndesigners' awareness and management of dependencies, ultimately with the goal\nof improving collaborative workflows."}
{"id": "2508.06016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06016", "abs": "https://arxiv.org/abs/2508.06016", "authors": ["Sagar Gandhi", "Vishal Gandhi"], "title": "Crisp Attention: Regularizing Transformers via Structured Sparsity", "comment": null, "summary": "The quadratic computational cost of the self-attention mechanism is a primary\nchallenge in scaling Transformer models. While attention sparsity is widely\nstudied as a technique to improve computational efficiency, it is almost\nuniversally assumed to come at the cost of model accuracy. In this paper, we\nreport a surprising counter-example to this common wisdom. By introducing\nstructured, post-hoc sparsity to the attention mechanism of a DistilBERT model\nduring fine-tuning on the SST-2 sentiment analysis task, we find that model\naccuracy improves significantly. Our model with 80\\% attention sparsity\nachieves a validation accuracy of 91.59\\%, a 0.97\\% absolute improvement over\nthe dense baseline. We hypothesize that this phenomenon is due to sparsity\nacting as a powerful implicit regularizer, preventing the model from\noverfitting by forcing it to make predictions with a more constrained and\nrobust set of features. Our work recasts attention sparsity not just as a tool\nfor computational efficiency, but as a potential method for improving the\ngeneralization and performance of Transformer models."}
{"id": "2508.05905", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05905", "abs": "https://arxiv.org/abs/2508.05905", "authors": ["Jeffrey Uhlmann"], "title": "The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)", "comment": null, "summary": "Quantization is usually regarded as a means to trade quality of performance\nfor reduced compute requirements, i.e., as a suboptimal approximation. However,\nif examined in terms of a fixed overall resource budget, a very different\nperspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit\nquantization that deterministically provides gradient information with no\nforward-path penalty. Our analysis provides evidence that it may improve\ninformation density compared to non-quantized alternatives."}
{"id": "2508.06042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06042", "abs": "https://arxiv.org/abs/2508.06042", "authors": ["Daechul Ahn", "San Kim", "Jonghyun Choi"], "title": "Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning", "comment": "COLM 2025", "summary": "Large Language Models (LLMs) have recently demonstrated impressive action\nsequence prediction capabilities but often struggle with dynamic, long-horizon\ntasks such as real-time strategic games. In a game such as StarCraftII (SC2),\nagents need to manage resource constraints and adapt to evolving battlefield\nsituations in a partially observable environment. This often overwhelms\nexisiting LLM-based approaches. To address these challenges, we propose a\nhierarchical multi-agent framework that employs specialized imitation learning\nagents under a meta-controller called Strategic Planner (SP). By expert\ndemonstrations, each specialized agent learns a distinctive strategy, such as\naerial support or defensive maneuvers, and produces coherent, structured\nmultistep action sequences. The SP then orchestrates these proposals into a\nsingle, environmentally adaptive plan that ensures local decisions aligning\nwith long-term strategies. We call this HIMA (Hierarchical Imitation\nMulti-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that\nencompasses all race match combinations in SC2. Our empirical results show that\nHIMA outperforms state of the arts in strategic clarity, adaptability, and\ncomputational efficiency, underscoring the potential of combining specialized\nimitation modules with meta-level orchestration to develop more robust,\ngeneral-purpose AI agents."}
{"id": "2508.06000", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06000", "abs": "https://arxiv.org/abs/2508.06000", "authors": ["Wei Xiang", "Ziyue Lei", "Haoyuan Che", "Fangyuan Ye", "Xueting Wu", "Lingyun Sun"], "title": "Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning", "comment": "Accepted by IJCAI 2025", "summary": "Operational skill learning, inherently physical and reliant on hands-on\npractice and kinesthetic feedback, has yet to be effectively replicated in\nlarge language model (LLM)-supported training. Current LLM training assistants\nprimarily generate customized textual feedback, neglecting the crucial\nkinesthetic modality. This gap derives from the textual and uncertain nature of\nLLMs, compounded by concerns on user acceptance of LLM driven body control. To\nbridge this gap and realize the potential of collaborative human-LLM action,\nthis work explores human experience of LLM driven kinesthetic assistance.\nSpecifically, we introduced an \"Align-Analyze-Adjust\" strategy and developed\nFlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS)\nfor flight skill acquisition, a representative operational skill domain.\nFlightAxis learns flight skills from manuals and guides forearm movements\nduring simulated flight tasks. Our results demonstrate high user acceptance of\nLLM-mediated body control and significantly reduced task completion times.\nCrucially, trainees reported that this kinesthetic assistance enhanced their\nawareness of operation flaws and fostered increased engagement in the training\nprocess, rather than relieving perceived load. This work demonstrated the\npotential of kinesthetic LLM training in operational skill acquisition."}
{"id": "2508.06026", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06026", "abs": "https://arxiv.org/abs/2508.06026", "authors": ["Yidong Wang", "Xin Wang", "Cunxiang Wang", "Junfeng Fang", "Qiufeng Wang", "Jianing Chu", "Xuran Meng", "Shuxun Yang", "Libo Qin", "Yue Zhang", "Wei Ye", "Shikun Zhang"], "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future", "comment": "12 pages, 5 figures", "summary": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose \\textbf{Temporal Self-Rewarding\nLanguage Models} that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) \\textit{Anchored Rejection} - fixing rejected responses using the past\ninitial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data."}
{"id": "2508.05915", "categories": ["cs.LG", "62M10"], "pdf": "https://arxiv.org/pdf/2508.05915", "abs": "https://arxiv.org/abs/2508.05915", "authors": ["Alex Glushkovsky"], "title": "Dual Signal Decomposition of Stochastic Time Series", "comment": "21 pages, 9 figures, 1 table", "summary": "The research paper addresses decomposition of a stochastic time series into\nthree time series representing a dual signal i.e., the mean and the dispersion,\nwith noise isolated. Decomposition is done by applying machine learning to fit\na dual signal. Machine learning minimizes the loss function which compromises\nbetween fitting the original time series and penalizing irregularities of the\ndual signal. The latter includes terms based on the first and second order\nderivatives along time. To preserve special patterns, weighting of the\nregularization components of the loss function has been introduced based on\nStatistical Process Control methodology. The proposed decomposition can be\napplied as a smoothing algorithm against the mean and dispersion of the time\nseries. By isolating noise, the proposed decomposition can be seen as a\ndenoising algorithm. Two approaches of the learning process have been\nconsidered: sequential and jointly. The former approach learns the mean signal\nfirst and then dispersion. The latter approach fits the dual signal jointly.\nJointly learning can uncover complex relationships for the time series with\nheteroskedasticity. Learning has been set by solving the direct non-linear\nunconstrained optimization problem or by applying neural networks that have\nsequential or twin output architectures. Tuning of the loss function\nhyperparameters focuses on the isolated noise to be a stationary stochastic\nprocess without autocorrelation properties. Depending on the applications, the\nhyperparameters of the learning can be tuned towards either the discrete states\nby stepped signal or smoothed series. The decomposed dual signal can be\nrepresented on the 2D space and used to learn inherent structures, to forecast\nboth mean and dispersion, or to analyze cross effects in case of multiple time\nseries."}
{"id": "2508.06060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06060", "abs": "https://arxiv.org/abs/2508.06060", "authors": ["Sankarshan Damle", "Boi Faltings"], "title": "LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences", "comment": "Published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence (ECAI 2025)", "summary": "Large Language Models (LLMs) are increasingly expected to handle complex\ndecision-making tasks, yet their ability to perform structured resource\nallocation remains underexplored. Evaluating their reasoning is also difficult\ndue to data contamination and the static nature of existing benchmarks. We\npresent a dual-purpose framework leveraging Participatory Budgeting (PB) both\nas (i) a practical setting for LLM-based resource allocation and (ii) an\nadaptive benchmark for evaluating their reasoning capabilities. We task LLMs\nwith selecting project subsets under feasibility (e.g., budget) constraints via\nthree prompting strategies: greedy selection, direct optimization, and a\nhill-climbing-inspired refinement. We benchmark LLMs' allocations against a\nutility-maximizing oracle. Interestingly, we also test whether LLMs can infer\nstructured preferences from natural-language voter input or metadata, without\nexplicit votes. By comparing allocations based on inferred preferences to those\nfrom ground-truth votes, we evaluate LLMs' ability to extract preferences from\nopen-ended input. Our results underscore the role of prompt design and show\nthat LLMs hold promise for mechanism design with unstructured inputs."}
{"id": "2508.06056", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06056", "abs": "https://arxiv.org/abs/2508.06056", "authors": ["Sizhe Cheng", "Jiaping Li", "Huanchen Wang", "Yuxin Ma"], "title": "RAGTrace: Understanding and Refining Retrieval-Generation Dynamics in Retrieval-Augmented Generation", "comment": "19 pages, 9 figures, Accepted by UIST 2025", "summary": "Retrieval-Augmented Generation (RAG) systems have emerged as a promising\nsolution to enhance large language models (LLMs) by integrating external\nknowledge retrieval with generative capabilities. While significant\nadvancements have been made in improving retrieval accuracy and response\nquality, a critical challenge remains that the internal knowledge integration\nand retrieval-generation interactions in RAG workflows are largely opaque. This\npaper introduces RAGTrace, an interactive evaluation system designed to analyze\nretrieval and generation dynamics in RAG-based workflows. Informed by a\ncomprehensive literature review and expert interviews, the system supports a\nmulti-level analysis approach, ranging from high-level performance evaluation\nto fine-grained examination of retrieval relevance, generation fidelity, and\ncross-component interactions. Unlike conventional evaluation practices that\nfocus on isolated retrieval or generation quality assessments, RAGTrace enables\nan integrated exploration of retrieval-generation relationships, allowing users\nto trace knowledge sources and identify potential failure cases. The system's\nworkflow allows users to build, evaluate, and iterate on retrieval processes\ntailored to their specific domains of interest. The effectiveness of the system\nis demonstrated through case studies and expert evaluations on real-world RAG\napplications."}
{"id": "2508.06030", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06030", "abs": "https://arxiv.org/abs/2508.06030", "authors": ["Kartik Sharma", "Yiqiao Jin", "Rakshit Trivedi", "Srijan Kumar"], "title": "Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings", "comment": null, "summary": "Large language models (LLMs) acquire knowledge across diverse domains such as\nscience, history, and geography encountered during generative pre-training.\nHowever, due to their stochasticity, it is difficult to predict what LLMs have\nacquired. Prior work has developed different ways to probe this knowledge by\ninvestigating the hidden representations, crafting specific task prompts,\ncurating representative samples, and estimating their uncertainty. However,\nthese methods require making forward passes through the underlying model to\nprobe the LLM's knowledge about a specific fact, making them computationally\nexpensive and time-consuming. To bridge this gap, we propose $\\textbf{PEEK}$ or\n$\\textbf{P}$roxy $\\textbf{E}$mbeddings to $\\textbf{E}$stimate\n$\\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models\nthat effectively encode factual knowledge as text or graphs as proxies for\nLLMs. First, we identify a training set of facts known by LLMs through various\nprobing strategies and then adapt embedding models to predict the LLM outputs\nwith a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived\ndatasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict\nLLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find\nthat sentence embedding models are more suitable than graph embeddings to\npredict LLM knowledge, shedding light on the underlying representation of the\nfactual landscape. Thus, we believe that knowledge-adapted embeddings can be\nused to identify knowledge gaps in LLMs at scale and can provide deeper\ninsights into LLMs' internal inductive bias. The code and data are made\navailable at https://github.com/claws-lab/peek."}
{"id": "2508.05921", "categories": ["cs.LG", "math.FA", "math.RT", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.05921", "abs": "https://arxiv.org/abs/2508.05921", "authors": ["Siddharth Rout"], "title": "Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations", "comment": null, "summary": "Accuracy in neural PDE solvers often breaks down not because of limited\nexpressivity, but due to poor optimisation caused by ill-conditioning,\nespecially in multi-fidelity and stiff problems. We study this issue in\nPhysics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural\nPDE solvers, and show that asymptotic components in governing equations can\nproduce highly ill-conditioned activation matrices, severely limiting\nconvergence. We introduce Shifted Gaussian Encoding, a simple yet effective\nactivation filtering step that increases matrix rank and expressivity while\npreserving convexity. Our method extends the solvable range of Peclet numbers\nin steady advection-diffusion equations by over two orders of magnitude,\nachieves up to six orders lower error on multi-frequency function learning, and\nfits high-fidelity image vectors more accurately and faster than deep networks\nwith over a million parameters. This work highlights that conditioning, not\ndepth, is often the bottleneck in scientific neural solvers and that simple\narchitectural changes can unlock substantial gains."}
{"id": "2508.06062", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 68T30"], "pdf": "https://arxiv.org/pdf/2508.06062", "abs": "https://arxiv.org/abs/2508.06062", "authors": ["Evgenii E. Vityaev", "Andrei Mantsivoda"], "title": "Don't Forget Imagination!", "comment": "14 pages, 2 figures", "summary": "Cognitive imagination is a type of imagination that plays a key role in human\nthinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to\nmentally visualize coherent and holistic systems of concepts and causal links\nthat serve as semantic contexts for reasoning, decision making and prediction.\nOur position is that the role of cognitive imagination is still greatly\nunderestimated, and this creates numerous problems and diminishes the current\ncapabilities of AI. For instance, when reasoning, humans rely on imaginary\ncontexts to retrieve background info. They also constantly return to the\ncontext for semantic verification that their reasoning is still reasonable.\nThus, reasoning without imagination is blind. This paper is a call for greater\nattention to cognitive imagination as the next promising breakthrough in\nartificial intelligence. As an instrument for simulating cognitive imagination,\nwe propose semantic models -- a new approach to mathematical models that can\nlearn, like neural networks, and are based on probabilistic causal\nrelationships. Semantic models can simulate cognitive imagination because they\nensure the consistency of imaginary contexts and implement a glass-box approach\nthat allows the context to be manipulated as a holistic and coherent system of\ninterrelated facts glued together with causal relations."}
{"id": "2508.06065", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV", "H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06065", "abs": "https://arxiv.org/abs/2508.06065", "authors": ["Daniel Lee", "Nikhil Sharma", "Donghoon Shin", "DaEun Choi", "Harsh Sharma", "Jeonghwan Kim", "Heng Ji"], "title": "ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation", "comment": null, "summary": "Generative AI has made image creation more accessible, yet aligning outputs\nwith nuanced creative intent remains challenging, particularly for non-experts.\nExisting tools often require users to externalize ideas through prompts or\nreferences, limiting fluid exploration. We introduce ThematicPlane, a system\nthat enables users to navigate and manipulate high-level semantic concepts\n(e.g., mood, style, or narrative tone) within an interactive thematic design\nplane. This interface bridges the gap between tacit creative intent and system\ncontrol. In our exploratory study (N=6), participants engaged in divergent and\nconvergent creative modes, often embracing unexpected results as inspiration or\niteration cues. While they grounded their exploration in familiar themes,\ndiffering expectations of how themes mapped to outputs revealed a need for more\nexplainable controls. Overall, ThematicPlane fosters expressive, iterative\nworkflows and highlights new directions for intuitive, semantics-driven\ninteraction in generative design tools."}
{"id": "2508.06046", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06046", "abs": "https://arxiv.org/abs/2508.06046", "authors": ["Xinda Wang", "Zhengxu Hou", "Yangshijie Zhang", "Bingren Yan", "Zhibo Yang", "Xingsheng Zhang", "Luxi Xing", "Qiang Zhou", "Chen Zhang"], "title": "EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation", "comment": null, "summary": "Although the effectiveness of Large Language Models (LLMs) as judges\n(LLM-as-a-judge) has been validated, their performance remains limited in\nopen-ended tasks, particularly in story evaluation. Accurate story evaluation\nis crucial not only for assisting human quality judgment but also for providing\nkey signals to guide story generation. However, existing methods face a\ndilemma: prompt engineering for closed-source models suffers from poor\nadaptability, while fine-tuning approaches for open-source models lack the\nrigorous reasoning capabilities essential for story evaluation. To address\nthis, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.\nGrounded in pairwise comparison, the framework first self-synthesizes\nscore-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To\nensure data quality, these raw CoTs undergo a self-filtering process, utilizing\nmulti-agents to guarantee their logical rigor and robustness. Finally, the\nevaluator trained on the refined data is deployed as a reward model to guide\nthe story generation task. Experimental results demonstrate that our framework\nachieves state-of-the-art (SOTA) performance on three evaluation benchmarks\nincluding StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward\nmodel, it significantly enhances the quality of generated stories, thereby\nfully validating the superiority of our self-evolving approach."}
{"id": "2508.05928", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05928", "abs": "https://arxiv.org/abs/2508.05928", "authors": ["Si Shen", "Peijun Shen", "Wenhua Zhao", "Danhao Zhu"], "title": "Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting", "comment": null, "summary": "Group-Relative Policy Optimization (GRPO) is a key technique for training\nlarge reasoning models, yet it suffers from a critical vulnerability: the\n\\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning\nprocess. This problem is most severe in unbalanced response groups,\nparadoxically degrading the signal precisely when it should be most\ninformative. To address this challenge, we propose Stable Group-Relative Policy\nOptimization (S-GRPO), a principled enhancement that derives optimal,\nnoise-aware advantage weights to stabilize training. Our comprehensive\nexperiments on mathematical reasoning benchmarks demonstrate S-GRPO's\neffectiveness and robustness. On various models, S-GRPO significantly\noutperforms DR. GRPO, achieving performance gains of +2.5% on\nQwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on\nQwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn\nunder 20% synthetic reward noise, S-GRPO maintains stable learning progress.\nThese results highlight S-GRPO's potential for more robust and effective\ntraining of large-scale reasoning models. \\footnote{Code and data are available\nat: https://github.com/shenpeijun0212/S-GRPO"}
{"id": "2508.06064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06064", "abs": "https://arxiv.org/abs/2508.06064", "authors": ["Harold Silvère Kiossou", "Siegfried Nijssen", "Pierre Schaus"], "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree", "comment": null, "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees."}
{"id": "2508.06117", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06117", "abs": "https://arxiv.org/abs/2508.06117", "authors": ["Maurice Koch", "Nelusa Pathmanathan", "Daniel Weiskopf", "Kuno Kurzhals"], "title": "A Multimodal Framework for Understanding Collaborative Design Processes", "comment": "Accepted to IEEE VIS 2025", "summary": "An essential task in analyzing collaborative design processes, such as those\nthat are part of workshops in design studies, is identifying design outcomes\nand understanding how the collaboration between participants formed the results\nand led to decision-making. However, findings are typically restricted to a\nconsolidated textual form based on notes from interviews or observations. A\nchallenge arises from integrating different sources of observations, leading to\nlarge amounts and heterogeneity of collected data. To address this challenge we\npropose a practical, modular, and adaptable framework of workshop setup,\nmultimodal data acquisition, AI-based artifact extraction, and visual analysis.\nOur interactive visual analysis system, reCAPit, allows the flexible\ncombination of different modalities, including video, audio, notes, or gaze, to\nanalyze and communicate important workshop findings. A multimodal streamgraph\ndisplays activity and attention in the working area, temporally aligned topic\ncards summarize participants' discussions, and drill-down techniques allow\ninspecting raw data of included sources. As part of our research, we conducted\nsix workshops across different themes ranging from social science research on\nurban planning to a design study on band-practice visualization. The latter two\nare examined in detail and described as case studies. Further, we present\nconsiderations for planning workshops and challenges that we derive from our\nown experience and the interviews we conducted with workshop experts. Our\nresearch extends existing methodology of collaborative design workshops by\npromoting data-rich acquisition of multimodal observations, combined AI-based\nextraction and interactive visual analysis, and transparent dissemination of\nresults."}
{"id": "2508.06094", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06094", "abs": "https://arxiv.org/abs/2508.06094", "authors": ["Morris Alper", "Moran Yanuka", "Raja Giryes", "Gašper Beguš"], "title": "ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline", "comment": "Project page: https://conlangcrafter.github.io", "summary": "Constructed languages (conlangs) such as Esperanto and Quenya have played\ndiverse roles in art, philosophy, and international communication. Meanwhile,\nlarge-scale foundation models have revolutionized creative generation in text,\nimages, and beyond. In this work, we leverage modern LLMs as computational\ncreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a\nmulti-hop pipeline that decomposes language design into modular stages --\nphonology, morphology, syntax, lexicon generation, and translation. At each\nstage, our method leverages LLMs' meta-linguistic reasoning capabilities,\ninjecting randomness to encourage diversity and leveraging self-refinement\nfeedback to encourage consistency in the emerging language description. We\nevaluate ConlangCrafter on metrics measuring coherence and typological\ndiversity, demonstrating its ability to produce coherent and varied conlangs\nwithout human linguistic expertise."}
{"id": "2508.05957", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05957", "abs": "https://arxiv.org/abs/2508.05957", "authors": ["Hasibul Karim Shanto", "Umme Ayman Koana", "Shadikur Rahman"], "title": "Multi-Armed Bandits-Based Optimization of Decision Trees", "comment": null, "summary": "Decision trees, without appropriate constraints, can easily become overly\ncomplex and prone to overfit, capturing noise rather than generalizable\npatterns. To resolve this problem,pruning operation is a crucial part in\noptimizing decision trees, as it not only reduces the complexity of trees but\nalso decreases the probability of generating overfit models. The conventional\npruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning\n(REP) are mostly based on greedy approaches that focus on immediate gains in\nperformance while pruning nodes of the decision tree. However, this might\nresult in a lower generalization in the long run, compromising the robust\nability of the tree model when introduced to unseen data samples, particularly\nwhen trained with small and complex datasets. To address this challenge, we are\nproposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement\nlearning (RL)-based technique, that will dynamically prune the tree to generate\nan optimal decision tree with better generalization. Our proposed approach\nassumes the pruning process as an exploration-exploitation problem, where we\nare utilizing the MAB algorithms to find optimal branch nodes to prune based on\nfeedback from each pruning actions. Experimental evaluation on several\nbenchmark datasets, demonstrated that our proposed approach results in better\npredictive performance compared to the traditional ones. This suggests the\npotential of utilizing MAB for a dynamic and probabilistic way of decision tree\npruning, in turn optimizing the decision tree-based model."}
{"id": "2508.06074", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06074", "abs": "https://arxiv.org/abs/2508.06074", "authors": ["Siyi Lu", "Run Liu", "Dongsheng Yang", "Lei He"], "title": "ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception", "comment": null, "summary": "Autonomous driving systems face significant challenges in perceiving complex\nenvironments and making real-time decisions. Traditional modular approaches,\nwhile offering interpretability, suffer from error propagation and coordination\nissues, whereas end-to-end learning systems can simplify the design but face\ncomputational bottlenecks. This paper presents a novel approach to autonomous\ndriving using deep reinforcement learning (DRL) that integrates bird's-eye view\n(BEV) perception for enhanced real-time decision-making. We introduce the\n\\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction\nnetwork that combines BEV-based perception with the Mamba framework for\ntemporal feature modeling. This integration allows the system to encode vehicle\nsurroundings and road features in a unified coordinate system and accurately\nmodel long-range dependencies. Building on this, we propose the\n\\texttt{ME$^3$-BEV} framework, which utilizes the \\texttt{Mamba-BEV} model as a\nfeature input for end-to-end DRL, achieving superior performance in dynamic\nurban driving scenarios. We further enhance the interpretability of the model\nby visualizing high-dimensional features through semantic segmentation,\nproviding insight into the learned representations. Extensive experiments on\nthe CARLA simulator demonstrate that \\texttt{ME$^3$-BEV} outperforms existing\nmodels across multiple metrics, including collision rate and trajectory\naccuracy, offering a promising solution for real-time autonomous driving."}
{"id": "2508.06300", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06300", "abs": "https://arxiv.org/abs/2508.06300", "authors": ["Weihan Zhang", "Jun Tao"], "title": "Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models", "comment": "Accepted by IEEE VIS 2025", "summary": "Explorative flow visualization allows domain experts to analyze complex flow\nstructures by interactively investigating flow patterns. However, traditional\nvisual interfaces often rely on specialized graphical representations and\ninteractions, which require additional effort to learn and use. Natural\nlanguage interaction offers a more intuitive alternative, but teaching machines\nto recognize diverse scientific concepts and extract corresponding structures\nfrom flow data poses a significant challenge. In this paper, we introduce an\nautomated framework that aligns flow pattern representations with the semantic\nspace of large language models (LLMs), eliminating the need for manual\nlabeling. Our approach encodes streamline segments using a denoising\nautoencoder and maps the generated flow pattern representations to LLM\nembeddings via a projector layer. This alignment empowers semantic matching\nbetween textual embeddings and flow representations through an attention\nmechanism, enabling the extraction of corresponding flow patterns based on\ntextual descriptions. To enhance accessibility, we develop an interactive\ninterface that allows users to query and visualize flow structures using\nnatural language. Through case studies, we demonstrate the effectiveness of our\nframework in enabling intuitive and intelligent flow exploration."}
{"id": "2508.06103", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06103", "abs": "https://arxiv.org/abs/2508.06103", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Ammar Mohammed"], "title": "Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs", "comment": "6 pages , 2 figures , Accepted in IMSA 2025,Egypt ,\n  https://imsa.msa.edu.eg/", "summary": "This paper presents two effective approaches for Extractive Question\nAnswering (QA) on the Quran. It addresses challenges related to complex\nlanguage, unique terminology, and deep meaning in the text. The second uses\nfew-shot prompting with instruction-tuned large language models such as Gemini\nand DeepSeek. A specialized Arabic prompt framework is developed for span\nextraction. A strong post-processing system integrates subword alignment,\noverlap suppression, and semantic filtering. This improves precision and\nreduces hallucinations. Evaluations show that large language models with Arabic\ninstructions outperform traditional fine-tuned models. The best configuration\nachieves a pAP10 score of 0.637. The results confirm that prompt-based\ninstruction tuning is effective for low-resource, semantically rich QA tasks."}
{"id": "2508.05960", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05960", "abs": "https://arxiv.org/abs/2508.05960", "authors": ["Haohui Chen", "Zhiyong Chen"], "title": "Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) seeks to learn optimal policies from\nstatic datasets without further environment interaction. A key challenge is the\ndistribution shift between the learned and behavior policies, leading to\nout-of-distribution (OOD) actions and overestimation. To prevent gross\noverestimation, the value function must remain conservative; however, excessive\nconservatism may hinder performance improvement. To address this, we propose\nthe mildly conservative regularized evaluation (MCRE) framework, which balances\nconservatism and performance by combining temporal difference (TD) error with a\nbehavior cloning term in the Bellman backup. Building on this, we develop the\nmildly conservative regularized Q-learning (MCRQ) algorithm, which integrates\nMCRE into an off-policy actor-critic framework. Experiments show that MCRQ\noutperforms strong baselines and state-of-the-art offline RL algorithms on\nbenchmark datasets."}
{"id": "2508.06091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06091", "abs": "https://arxiv.org/abs/2508.06091", "authors": ["Stan P Hauke", "Przemysław Andrzej Wałęga"], "title": "Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2", "comment": "18 pages", "summary": "In recent years, there has been growing interest in understanding the\nexpressive power of graph neural networks (GNNs) by relating them to logical\nlanguages. This research has been been initialised by an influential result of\nBarcel\\'o et al. (2020), who showed that the graded modal logic (or a guarded\nfragment of the logic C2), characterises the logical expressiveness of\naggregate-combine GNNs. As a ``challenging open problem'' they left the\nquestion whether full C2 characterises the logical expressiveness of\naggregate-combine-readout GNNs. This question has remained unresolved despite\nseveral attempts. In this paper, we solve the above open problem by proving\nthat the logical expressiveness of aggregate-combine-readout GNNs strictly\nexceeds that of C2. This result holds over both undirected and directed graphs.\nBeyond its implications for GNNs, our work also leads to purely logical\ninsights on the expressive power of infinitary logics."}
{"id": "2508.06349", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.06349", "abs": "https://arxiv.org/abs/2508.06349", "authors": ["Serena Tardelli", "Lorenzo Alvisi", "Lorenzo Cima", "Stefano Cresci", "Maurizio Tesconi"], "title": "Emoji Reactions on Telegram Often Reflect Social Approval Over Emotional Resonance", "comment": null, "summary": "Emoji reactions are a frequently used feature of messaging platforms. Prior\nwork mainly interpreted emojis as indicators of emotional resonance or user\nsentiment. However, emoji reactions may instead reflect broader social\ndynamics. Here, we investigate the communicative function of emoji reactions on\nTelegram by analyzing the relationship between the emotional and rhetorical\ncontent of messages and the emoji reactions they receive. We collect and\nanalyze over 650k Telegram messages that received at least one emoji reaction.\nWe annotate each message with sentiment, emotion, persuasion strategy, and\nspeech act labels, and infer the sentiment and emotion of emoji reactions using\nboth lexicons and large languages. We find a systematic mismatch between\nmessage sentiment and reaction sentiment, with positive reactions dominating\neven when the message is neutral or negative. We show that this pattern remains\nconsistent across rhetorical strategies and emotional tones, suggesting that\nemoji reactions may signal a degree of social approval rather than reflecting\nemotional resonance. Finally, we shed light on the communicative strategies\nthat predict greater emoji engagement. These findings have methodological\nimplications for sentiment analysis, as interpreting emoji reactions as direct\nproxies for emotional response may be misleading."}
{"id": "2508.06105", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06105", "abs": "https://arxiv.org/abs/2508.06105", "authors": ["Shengyuan Chen", "Chuang Zhou", "Zheng Yuan", "Qinggang Zhang", "Zeyang Cui", "Hao Chen", "Yilin Xiao", "Jiannong Cao", "Xiao Huang"], "title": "You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures", "comment": null, "summary": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect statements when handling questions beyond their knowledge\nand perception. Retrieval-augmented generation (RAG) addresses this by\nretrieving query-relevant contexts from knowledge bases to support LLM\nreasoning. Recent advances leverage pre-constructed graphs to capture the\nrelational connections among distributed documents, showing remarkable\nperformance in complex tasks. However, existing Graph-based RAG (GraphRAG)\nmethods rely on a costly process to transform the corpus into a graph,\nintroducing overwhelming token cost and update latency. Moreover, real-world\nqueries vary in type and complexity, requiring different logic structures for\naccurate reasoning. The pre-built graph may not align with these required\nstructures, resulting in ineffective knowledge retrieval. To this end, we\npropose a \\textbf{\\underline{Logic}}-aware\n\\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented\n\\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically\nextracts reasoning structures at inference time to guide adaptive retrieval\nwithout any pre-built graph. LogicRAG begins by decomposing the input query\ninto a set of subproblems and constructing a directed acyclic graph (DAG) to\nmodel the logical dependencies among them. To support coherent multi-step\nreasoning, LogicRAG then linearizes the graph using topological sort, so that\nsubproblems can be addressed in a logically consistent order. Besides, LogicRAG\napplies graph pruning to reduce redundant retrieval and uses context pruning to\nfilter irrelevant context, significantly reducing the overall token cost.\nExtensive experiments demonstrate that LogicRAG achieves both superior\nperformance and efficiency compared to state-of-the-art baselines."}
{"id": "2508.05977", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2508.05977", "abs": "https://arxiv.org/abs/2508.05977", "authors": ["Aoming Liang", "Chi Cheng", "Dashuai Chen", "Boai Sun", "Dixia Fan"], "title": "LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning", "comment": null, "summary": "In the domain of scientific machine learning, designing effective reward\nfunctions remains a challenge in reinforcement learning (RL), particularly in\nenvironments where task goals are difficult to specify numerically. Reward\nfunctions in existing work are predominantly based on heuristics, manual\nengineering, or task-specific tuning. In this work, we introduce a semantically\naligned reinforcement learning method where rewards are computed by aligning\nthe current state with a target semantic instruction using a\nSentence-Bidirectional Encoder Representations from Transformers (SBERT).\nInstead of relying on manually defined reward functions, the policy receives\nfeedback based on the reward, which is a cosine similarity between the goal\ntextual description and the statement description in the episode. We evaluated\nour approach in several environments and showed that semantic reward can guide\nlearning to achieve competitive control behavior, even in the absence of\nhand-crafted reward functions. Our study demonstrates a correlation between the\nlanguage embedding space and the conventional Euclidean space. This framework\nopens new horizons for aligning agent behavior with natural language goals and\nlays the groundwork for a more seamless integration of larger language models\n(LLMs) and fluid control applications."}
{"id": "2508.06110", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06110", "abs": "https://arxiv.org/abs/2508.06110", "authors": ["Yiran Rex Ma"], "title": "PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion", "comment": "Accepted at IJCNN 2025", "summary": "Table reasoning, including tabular QA and fact verification, often depends on\nannotated data or complex data augmentation, limiting flexibility and\ngeneralization. LLMs, despite their versatility, often underperform compared to\nsimple supervised models. To approach these issues, we introduce PanelTR, a\nframework utilizing LLM agent scientists for robust table reasoning through a\nstructured scientific approach. PanelTR's workflow involves agent scientists\nconducting individual investigations, engaging in self-review, and\nparticipating in collaborative peer-review discussions. This process, driven by\nfive scientist personas, enables semantic-level transfer without relying on\ndata augmentation or parametric optimization. Experiments across four\nbenchmarks show that PanelTR outperforms vanilla LLMs and rivals fully\nsupervised models, all while remaining independent of training data. Our\nfindings indicate that structured scientific methodology can effectively handle\ncomplex tasks beyond table reasoning with flexible semantic understanding in a\nzero-shot context."}
{"id": "2508.06354", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06354", "abs": "https://arxiv.org/abs/2508.06354", "authors": ["Clara Rigaud"], "title": "Zombitron: towards a toolbox for repurposing obsolete smartphones into new interactive systems", "comment": "Post-proceedings paper presented at LIMITS 2025: 11th Workshop on\n  Computing within Limits, 2025-06-26/27, Online", "summary": "This article explores the possibilities of reusing obsolete smartphones and\ntablets to build new interactive systems. Taking the case of a musical\ninstrument, I present my research into the design of a controller made from\nvarious of these obsolete smartphones. From the diagnostic stage to the\ncreation of a new autonomous electronic object, I document the process, the\nbarriers and the levers encountered. Based on these explorations and\ndiscussions with two professional musicians, I provide several insights into\nthe software and hardware aspects, with a view to continuing this work, towards\nthe creation of an open-source toolkit enabling anyone to build new interactive\nsystems with old devices. I discuss the implication of how a high-level\nweb-based approach could allow designers to enter the black box and foster\npermacomputing using smartphones."}
{"id": "2508.06124", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06124", "abs": "https://arxiv.org/abs/2508.06124", "authors": ["Sayantan Adak", "Pratyush Chatterjee", "Somnath Banerjee", "Rima Hazra", "Somak Aditya", "Animesh Mukherjee"], "title": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models", "comment": null, "summary": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications."}
{"id": "2508.05984", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05984", "abs": "https://arxiv.org/abs/2508.05984", "authors": ["Ankur Naskar", "Gugan Thoppe", "Vijay Gupta"], "title": "Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning", "comment": null, "summary": "Algorithms for solving \\textit{nonlinear} fixed-point equations -- such as\naverage-reward \\textit{$Q$-learning} and \\textit{TD-learning} -- often involve\nsemi-norm contractions. Achieving parameter-free optimal convergence rates for\nthese methods via Polyak--Ruppert averaging has remained elusive, largely due\nto the non-monotonicity of such semi-norms. We close this gap by (i.) recasting\nthe averaged error as a linear recursion involving a nonlinear perturbation,\nand (ii.) taming the nonlinearity by coupling the semi-norm's contraction with\nthe monotonicity of a suitably induced norm. Our main result yields the first\nparameter-free $\\tilde{O}(1/\\sqrt{t})$ optimal rates for $Q$-learning in both\naverage-reward and exponentially discounted settings, where $t$ denotes the\niteration index. The result applies within a broad framework that accommodates\nsynchronous and asynchronous updates, single-agent and distributed deployments,\nand data streams obtained either from simulators or along Markovian\ntrajectories."}
{"id": "2508.06111", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06111", "abs": "https://arxiv.org/abs/2508.06111", "authors": ["Dewi S. W. Gould", "Bruno Mlodozeniec", "Samuel F. Brown"], "title": "SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges", "comment": "7 pages and appendices", "summary": "Evaluating the capabilities and risks of foundation models is paramount, yet\ncurrent methods demand extensive domain expertise, hindering their scalability\nas these models rapidly evolve. We introduce SKATE: a novel evaluation\nframework in which large language models (LLMs) compete by generating and\nsolving verifiable tasks for one another. Our core insight is to treat\nevaluation as a game: models act as both task-setters and solvers, incentivized\nto create questions which highlight their own strengths while exposing others'\nweaknesses. SKATE offers several key advantages, balancing scalability,\nopen-endedness, and objectivity. It is fully automated, data-free, and\nscalable, requiring no human input or domain expertise. By using verifiable\ntasks rather than LLM judges, scoring is objective. Unlike domain-limited\nprogrammatically-generated benchmarks (e.g. chess-playing or spatial\nreasoning), having LLMs creatively pose challenges enables open-ended and\nscalable evaluation. As a proof of concept, we introduce LLM-set\ncode-output-prediction (COP) challenges as a verifiable and extensible\nframework in which to test our approach. Using a TrueSkill-based ranking\nsystem, we evaluate six frontier LLMs and find that: (1) weaker models can\nreliably differentiate and score stronger ones, (2) LLM-based systems are\ncapable of self-preferencing behavior, generating questions that align with\ntheir own capabilities, and (3) SKATE automatically surfaces fine-grained\ncapability differences between models. Our findings are an important step\ntowards general, scalable evaluation frameworks which can keep pace with LLM\nprogress."}
{"id": "2508.06484", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06484", "abs": "https://arxiv.org/abs/2508.06484", "authors": ["Yuvraj Virk", "Dongyu Liu"], "title": "Non-programmers Assessing AI-Generated Code: A Case Study of Business Users Analyzing Data", "comment": "Accepted by VL/HCC 2025", "summary": "Non-technical end-users increasingly rely on AI code generation to perform\ntechnical tasks like data analysis. However, large language models (LLMs)\nremain unreliable, and it is unclear whether end-users can effectively identify\nmodel errors $\\unicode{x2014}$ especially in realistic and domain-specific\nscenarios. We surveyed marketing and sales professionals to assess their\nability to critically evaluate LLM-generated analyses of marketing data.\nParticipants were shown natural language explanations of the AI's code,\nrepeatedly informed the AI often makes mistakes, and explicitly prompted to\nidentify them. Yet, participants frequently failed to detect critical flaws\nthat could compromise decision-making, many of which required no technical\nknowledge to recognize. To investigate why, we reformatted AI responses into\nclearly delineated steps and provided alternative approaches for each decision\nto support critical evaluation. While these changes had a positive effect,\nparticipants often struggled to reason through the AI's steps and alternatives.\nOur findings suggest that business professionals cannot reliably verify\nAI-generated data analyses on their own and explore reasons why to inform\nfuture designs. As non-programmers adopt code-generating AI for technical\ntasks, unreliable AI and insufficient human oversight poses risks of unsafe or\nlow-quality decisions."}
{"id": "2508.06135", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06135", "abs": "https://arxiv.org/abs/2508.06135", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "title": "Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models", "comment": null, "summary": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs."}
{"id": "2508.05988", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05988", "abs": "https://arxiv.org/abs/2508.05988", "authors": ["Wenhao Zeng", "Yaoning Wang", "Chao Hu", "Yuling Shi", "Chengcheng Wan", "Hongyu Zhang", "Xiaodong Gu"], "title": "Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal", "comment": "Code and model available at https://github.com/Zengwh02/ASAP", "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in code reasoning by scaling up the length of Chain-of-Thought\n(CoT). However, excessively long reasoning traces introduce substantial\nchallenges in terms of training cost, inference latency, and deployment\nfeasibility. While various CoT compression approaches have emerged to address\nthis challenge, they face inherent trade-offs: token-level methods often\ndisrupt syntactic and logical coherence, while step-level methods based on\nperplexity fail to reliably capture the logically critical reasoning steps. In\nthis paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel\ncoarse-to-fine framework for CoT compression. ASAP first performs anchor-guided\npruning to preserve the core reasoning structure, which efficiently reduces the\nsearch space for subsequent processing. It then enables a logic-aware pruning\nby selecting logically essential reasoning steps based on a novel first-token\nsurprisal metric. Finally, ASAP teaches models to autonomously generate and\nleverage these concise CoTs at inference time, enabling efficient reasoning in\ncoding tasks. Experiments show that ASAP achieves state-of-the-art accuracy\nacross multiple code generation benchmarks while substantially reducing\ntraining and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,\nour approach reduces token generation by 23.5% and inference latency by 43.5%\ncompared to the strongest baseline, while achieving a competitive accuracy of\n36.19% in Pass@1. Our results highlight a promising direction for building\npowerful and efficient LRMs."}
{"id": "2508.06129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06129", "abs": "https://arxiv.org/abs/2508.06129", "authors": ["Bachtiar Herdianto", "Romain Billot", "Flavien Lucas", "Marc Sevaux"], "title": "Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem", "comment": "22 pages, 14 figures", "summary": "The Vehicle Routing Problem (VRP) is a complex optimization problem with\nnumerous real-world applications, mostly solved using metaheuristic algorithms\ndue to its $\\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely\non human-crafted designs developed through empirical studies. However, recent\nresearch shows that machine learning methods can be used the structural\ncharacteristics of solutions in combinatorial optimization, thereby aiding in\ndesigning more efficient algorithms, particularly for solving VRP. Building on\nthis advancement, this study extends the previous research by conducting a\nsensitivity analysis using multiple classifier models that are capable of\npredicting the quality of VRP solutions. Hence, by leveraging explainable AI,\nthis research is able to extend the understanding of how these models make\ndecisions. Finally, our findings indicate that while feature importance varies,\ncertain features consistently emerge as strong predictors. Furthermore, we\npropose a unified framework able of ranking feature impact across different\nscenarios to illustrate this finding. These insights highlight the potential of\nfeature importance analysis as a foundation for developing a guidance mechanism\nof metaheuristic algorithms for solving the VRP."}
{"id": "2508.06167", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06167", "abs": "https://arxiv.org/abs/2508.06167", "authors": ["Vít Gvoždiak"], "title": "Pragmatics beyond humans: meaning, communication, and LLMs", "comment": null, "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI."}
{"id": "2508.06149", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06149", "abs": "https://arxiv.org/abs/2508.06149", "authors": ["Gunhee Cho", "Yun-Gyung Cheong"], "title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts", "comment": null, "summary": "We present Big5-Scaler, a prompt-based framework for conditioning large\nlanguage models (LLMs) with controllable Big Five personality traits. By\nembedding numeric trait values into natural language prompts, our method\nenables fine-grained personality control without additional training. We\nevaluate Big5-Scaler across trait expression, dialogue generation, and human\ntrait imitation tasks. Results show that it induces consistent and\ndistinguishable personality traits across models, with performance varying by\nprompt type and scale. Our analysis highlights the effectiveness of concise\nprompts and lower trait intensities, providing a efficient approach for\nbuilding personality-aware dialogue agents."}
{"id": "2508.05995", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05995", "abs": "https://arxiv.org/abs/2508.05995", "authors": ["Fei Xu Yu", "Gina Adam", "Nathaniel D. Bastian", "Tian Lan"], "title": "Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode generation and structured reasoning; however, their performance often\ndegrades on complex tasks that require consistent multi-step planning. Recent\nwork has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet\nexisting approaches primarily focus on generating heuristic-based code for\noptimization or target simpler tasks where correctness alone is sufficient. In\nthis work, we propose MCTS-OPS, a novel neural-symbolic framework that\nformulates prompt selection as a sequential decision process guided by MCTS.\nOur method explores and refines multi-step prompt sequences for the goal of\nimproving code generation quality and enhancing the problem-solving\ncapabilities of LLMs in general optimization. Experiments on network\noptimization show significant improvement over the baselines, both in the\nsuccess rate of executing the generated code and in the optimization results\nwith the specified objective and constraints (2$\\sim$4$\\times$ higher reward\nand 3$\\times$ lower standard deviation). Moreover, it improves the chance of\nattaining the optimal solution by about 10\\% of cases, compared to baseline\nmethods in hard problems. These results highlight the promise of combining\nsymbolic planning with LLMs for robust, high-quality code generation in complex\ndomains."}
{"id": "2508.06145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06145", "abs": "https://arxiv.org/abs/2508.06145", "authors": ["Byeonghun Bang", "Jongsuk Yoon", "Dong-Jin Chang", "Seho Park", "Yong Oh Lee"], "title": "Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications", "comment": null, "summary": "The versatility of large language models (LLMs) has been explored across\nvarious sectors, but their application in healthcare poses challenges,\nparticularly in the domain of pharmaceutical contraindications where accurate\nand reliable information is required. This study enhances the capability of\nLLMs to address contraindications effectively by implementing a Retrieval\nAugmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base\nmodel, and the text-embedding-3-small model for embeddings, our approach\nintegrates Langchain to orchestrate a hybrid retrieval system with re-ranking.\nThis system leverages Drug Utilization Review (DUR) data from public databases,\nfocusing on contraindications for specific age groups, pregnancy, and\nconcomitant drug use. The dataset includes 300 question-answer pairs across\nthree categories, with baseline model accuracy ranging from 0.49 to 0.57.\nPost-integration of the RAG pipeline, we observed a significant improvement in\nmodel accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications\nrelated to age groups, pregnancy, and concomitant drug use, respectively. The\nresults indicate that augmenting LLMs with a RAG framework can substantially\nreduce uncertainty in prescription and drug intake decisions by providing more\nprecise and reliable drug contraindication information."}
{"id": "2508.06196", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06196", "abs": "https://arxiv.org/abs/2508.06196", "authors": ["Nizi Nazar", "Ehsaneddin Asgari"], "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations", "comment": null, "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment."}
{"id": "2508.06155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06155", "abs": "https://arxiv.org/abs/2508.06155", "authors": ["Renhan Zhang", "Lian Lian", "Zhen Qi", "Guiran Liu"], "title": "Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach", "comment": null, "summary": "This paper addresses the issue of implicit stereotypes that may arise during\nthe generation process of large language models. It proposes an interpretable\nbias detection method aimed at identifying hidden social biases in model\noutputs, especially those semantic tendencies that are not easily captured\nthrough explicit linguistic features. The method combines nested semantic\nrepresentation with a contextual contrast mechanism. It extracts latent bias\nfeatures from the vector space structure of model outputs. Using attention\nweight perturbation, it analyzes the model's sensitivity to specific social\nattribute terms, thereby revealing the semantic pathways through which bias is\nformed. To validate the effectiveness of the method, this study uses the\nStereoSet dataset, which covers multiple stereotype dimensions including\ngender, profession, religion, and race. The evaluation focuses on several key\nmetrics, such as bias detection accuracy, semantic consistency, and contextual\nsensitivity. Experimental results show that the proposed method achieves strong\ndetection performance across various dimensions. It can accurately identify\nbias differences between semantically similar texts while maintaining high\nsemantic alignment and output stability. The method also demonstrates high\ninterpretability in its structural design. It helps uncover the internal bias\nassociation mechanisms within language models. This provides a more transparent\nand reliable technical foundation for bias detection. The approach is suitable\nfor real-world applications where high trustworthiness of generated content is\nrequired."}
{"id": "2508.06023", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06023", "abs": "https://arxiv.org/abs/2508.06023", "authors": ["Xiaobin Shen", "Jonathan Elmer", "George H. Chen"], "title": "Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients", "comment": null, "summary": "Prognostication for comatose post-cardiac arrest patients is a critical\nchallenge that directly impacts clinical decision-making in the ICU. Clinical\ninformation that informs prognostication is collected serially over time.\nShortly after cardiac arrest, various time-invariant baseline features are\ncollected (e.g., demographics, cardiac arrest characteristics). After ICU\nadmission, additional features are gathered, including time-varying hemodynamic\ndata (e.g., blood pressure, doses of vasopressor medications). We view these as\ntwo phases in which we collect new features. In this study, we propose a novel\nstepwise dynamic competing risks model that improves the prediction of\nneurological outcomes by automatically determining when to take advantage of\ntime-invariant features (first phase) and time-varying features (second phase).\nNotably, our model finds patients for whom this second phase (time-varying\nhemodynamic) information is beneficial for prognostication and also when this\ninformation is beneficial (as we collect more hemodynamic data for a patient\nover time, how important these data are for prognostication varies). Our\napproach extends the standard Fine and Gray model to explicitly model the two\nphases and to incorporate neural networks to flexibly capture complex nonlinear\nfeature relationships. Evaluated on a retrospective cohort of 2,278 comatose\npost-arrest patients, our model demonstrates robust discriminative performance\nfor the competing outcomes of awakening, withdrawal of life-sustaining therapy,\nand death despite maximal support. Our approach generalizes to more than two\nphases in which new features are collected and could be used in other dynamic\nprediction tasks, where it may be helpful to know when and for whom newly\ncollected features significantly improve prediction."}
{"id": "2508.06225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06225", "abs": "https://arxiv.org/abs/2508.06225", "authors": ["Zailong Tian", "Zhuoheng Han", "Yanzhe Chen", "Haozhe Xu", "Xi Yang", "richeng xuan", "Hongfeng Wang", "Lizi Liao"], "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution", "comment": null, "summary": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines."}
{"id": "2508.06336", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06336", "abs": "https://arxiv.org/abs/2508.06336", "authors": ["Constantin Ruhdorfer", "Matteo Bortoletto", "Victor Oei", "Anna Penzkofer", "Andreas Bulling"], "title": "Unsupervised Partner Design Enables Robust Ad-hoc Teamwork", "comment": "16 pages", "summary": "We introduce Unsupervised Partner Design (UPD) - a population-free,\nmulti-agent reinforcement learning framework for robust ad-hoc teamwork that\nadaptively generates training partners without requiring pretrained partners or\nmanual parameter tuning. UPD constructs diverse partners by stochastically\nmixing an ego agent's policy with biased random behaviours and scores them\nusing a variance-based learnability metric that prioritises partners near the\nego agent's current learning frontier. We show that UPD can be integrated with\nunsupervised environment design, resulting in the first method enabling fully\nunsupervised curricula over both level and partner distributions in a\ncooperative setting. Through extensive evaluations on Overcooked-AI and the\nOvercooked Generalisation Challenge, we demonstrate that this dynamic partner\ncurriculum is highly effective: UPD consistently outperforms both\npopulation-based and population-free baselines as well as ablations. In a user\nstudy, we further show that UPD achieves higher returns than all baselines and\nwas perceived as significantly more adaptive, more human-like, a better\ncollaborator, and less frustrating."}
{"id": "2508.06163", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06163", "abs": "https://arxiv.org/abs/2508.06163", "authors": ["Yingfeng Luo", "Dingyang Lin", "Junxin Wang", "Ziqiang Xu", "Kaiyan Chang", "Tong Zheng", "Bei Li", "Anxiang Ma", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging", "comment": "Under review", "summary": "Model merging has emerged as a compelling data-free paradigm for multi-task\nlearning, enabling the fusion of multiple fine-tuned models into a single,\npowerful entity. A key technique in merging methods is sparsification, which\nprunes redundant parameters from task vectors to mitigate interference.\nHowever, prevailing approaches employ a ``one-size-fits-all'' strategy,\napplying a uniform sparsity ratio that overlooks the inherent structural and\nstatistical heterogeneity of model parameters. This often leads to a suboptimal\ntrade-off, where critical parameters are inadvertently pruned while less useful\nones are retained. To address this limitation, we introduce \\textbf{TADrop}\n(\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive\nsparsification strategy that respects this heterogeneity. Instead of a global\nratio, TADrop assigns a tailored sparsity level to each parameter tensor based\non its distributional properties. The core intuition is that tensors with\ndenser, more redundant distributions can be pruned aggressively, while sparser,\nmore critical ones are preserved. As a simple and plug-and-play module, we\nvalidate TADrop by integrating it with foundational, classic, and SOTA merging\nmethods. Extensive experiments across diverse tasks (vision, language, and\nmultimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and\nsignificantly boosts their performance. For instance, when enhancing a leading\nmerging method, it achieves an average performance gain of 2.0\\% across 8\nViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter\ninterference by tailoring sparsification to the model's structure, offering a\nnew baseline for high-performance model merging."}
{"id": "2508.06034", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06034", "abs": "https://arxiv.org/abs/2508.06034", "authors": ["Qin Chen", "Guojie Song"], "title": "Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity", "comment": "Accepted tp CIKM 2025", "summary": "Heterogeneous graphs (HGs) are common in real-world scenarios and often\nexhibit heterophily. However, most existing studies focus on either\nheterogeneity or heterophily in isolation, overlooking the prevalence of\nheterophilic HGs in practical applications. Such ignorance leads to their\nperformance degradation. In this work, we first identify two main challenges in\nmodeling heterophily HGs: (1) varying heterophily distributions across hops and\nmeta-paths; (2) the intricate and often heterophily-driven diversity of\nsemantic information across different meta-paths. Then, we propose the Adaptive\nHeterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN\nemploys a heterophily-aware convolution that accounts for heterophily\ndistributions specific to both hops and meta-paths. It then integrates messages\nfrom diverse semantic spaces using a coarse-to-fine attention mechanism, which\nfilters out noise and emphasizes informative signals. Experiments on seven\nreal-world graphs and twenty baselines demonstrate the superior performance of\nAHGNN, particularly in high-heterophily situations."}
{"id": "2508.06226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06226", "abs": "https://arxiv.org/abs/2508.06226", "authors": ["Yumeng Fu", "Jiayin Zhu", "Lingling Zhang", "Bo Zhao", "Shaoxuan Ma", "Yushun Zhang", "Yanrui Wu", "Wenjun Wu"], "title": "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines", "comment": null, "summary": "Geometry problem solving (GPS) requires models to master diagram\ncomprehension, logical reasoning, knowledge application, numerical computation,\nand auxiliary line construction. This presents a significant challenge for\nMultimodal Large Language Models (MLLMs). However, existing benchmarks for\nevaluating MLLM geometry skills overlook auxiliary line construction and lack\nfine-grained process evaluation, making them insufficient for assessing MLLMs'\nlong-step reasoning abilities. To bridge these gaps, we present the GeoLaux\nbenchmark, comprising 2,186 geometry problems, incorporating both calculation\nand proving questions. Notably, the problems require an average of 6.51\nreasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary\nline construction. Building on the dataset, we design a novel five-dimensional\nevaluation strategy assessing answer correctness, process correctness, process\nquality, auxiliary line impact, and error causes. Extensive experiments on 13\nleading MLLMs (including thinking models and non-thinking models) yield three\npivotal findings: First, models exhibit substantial performance degradation in\nextended reasoning steps (nine models demonstrate over 50% performance drop).\nSecond, compared to calculation problems, MLLMs tend to take shortcuts when\nsolving proving problems. Third, models lack auxiliary line awareness, and\nenhancing this capability proves particularly beneficial for overall geometry\nreasoning improvement. These findings establish GeoLaux as both a benchmark for\nevaluating MLLMs' long-step geometric reasoning with auxiliary lines and a\nguide for capability advancement. Our dataset and code are included in\nsupplementary materials and will be released."}
{"id": "2508.06352", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06352", "abs": "https://arxiv.org/abs/2508.06352", "authors": ["Christian Meske", "Justin Brenne", "Erdi Uenal", "Sabahat Oelcer", "Ayseguel Doganguen"], "title": "From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI", "comment": null, "summary": "Current explainable AI (XAI) approaches prioritize algorithmic transparency\nand present explanations in abstract, non-adaptive formats that often fail to\nsupport meaningful end-user understanding. This paper introduces \"Explanatory\nAI\" as a complementary paradigm that leverages generative AI capabilities to\nserve as explanatory partners for human understanding rather than providers of\nalgorithmic transparency. While XAI reveals algorithmic decision processes for\nmodel validation, Explanatory AI addresses contextual reasoning to support\nhuman decision-making in sociotechnical contexts. We develop a definition and\nsystematic eight-dimensional conceptual model distinguishing Explanatory AI\nthrough narrative communication, adaptive personalization, and progressive\ndisclosure principles. Empirical validation through Rapid Contextual Design\nmethodology with healthcare professionals demonstrates that users consistently\nprefer context-sensitive, multimodal explanations over technical transparency.\nOur findings reveal the practical urgency for AI systems designed for human\ncomprehension rather than algorithmic introspection, establishing a\ncomprehensive research agenda for advancing user-centered AI explanation\napproaches across diverse domains and cultural contexts."}
{"id": "2508.06165", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06165", "abs": "https://arxiv.org/abs/2508.06165", "authors": ["Weitao Li", "Boran Xiang", "Xiaolong Wang", "Zhinan Gou", "Weizhi Ma", "Yang Liu"], "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2."}
{"id": "2508.06041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06041", "abs": "https://arxiv.org/abs/2508.06041", "authors": ["Sangwoo Kwon", "Seong Hoon Seo", "Jae W. Lee", "Yeonhong Park"], "title": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment", "comment": null, "summary": "How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding iterations. Building on this\ninsight, we introduce DP-LLM, a novel mechanism that dynamically assigns\nprecision to each layer based on input values. DP-LLM augments each linear\nlayer in an LLM with a precision selector that determines the bitwidth at\nruntime using a lightweight error estimator and threshold values learned\nthrough fine-tuning. Experimental results across multiple models and benchmarks\ndemonstrate that DP-LLM achieves a superior performance-latency trade-off,\noutperforming prior approaches."}
{"id": "2508.06230", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06230", "abs": "https://arxiv.org/abs/2508.06230", "authors": ["Ruben Sharma", "Sebastijan Dumančić", "Ross D. King", "Andrew Cropper"], "title": "Learning Logical Rules using Minimum Message Length", "comment": null, "summary": "Unifying probabilistic and logical learning is a key challenge in AI. We\nintroduce a Bayesian inductive logic programming approach that learns minimum\nmessage length programs from noisy data. Our approach balances hypothesis\ncomplexity and data fit through priors, which explicitly favour more general\nprograms, and a likelihood that favours accurate programs. Our experiments on\nseveral domains, including game playing and drug design, show that our method\nsignificantly outperforms previous methods, notably those that learn minimum\ndescription length programs. Our results also show that our approach is\ndata-efficient and insensitive to example balance, including the ability to\nlearn from exclusively positive examples."}
{"id": "2508.06167", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06167", "abs": "https://arxiv.org/abs/2508.06167", "authors": ["Vít Gvoždiak"], "title": "Pragmatics beyond humans: meaning, communication, and LLMs", "comment": null, "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI."}
{"id": "2508.06066", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06066", "abs": "https://arxiv.org/abs/2508.06066", "authors": ["Barak Gahtan", "Alex M. Bronstein"], "title": "Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology", "comment": null, "summary": "Deep temporal architectures such as Temporal Convolutional Networks (TCNs)\nachieve strong predictive performance on sequential data, yet theoretical\nunderstanding of their generalization remains limited. We address this gap by\nproviding both the first non-vacuous, architecture-aware generalization bounds\nfor deep temporal models and a principled evaluation methodology.\n  For exponentially $\\beta$-mixing sequences, we derive bounds scaling as $\nO\\!\\Bigl(R\\,\\sqrt{\\tfrac{D\\,p\\,n\\,\\log N}{N}}\\Bigr), $ where $D$ is network\ndepth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our\ndelayed-feedback blocking mechanism transforms dependent samples into\neffectively independent ones while discarding only $O(1/\\log N)$ of the data,\nyielding $\\sqrt{D}$ scaling instead of exponential, implying that doubling\ndepth requires approximately quadrupling the training data.\n  We also introduce a fair-comparison methodology that fixes the effective\nsample size to isolate the effect of temporal structure from information\ncontent. Under $N_{\\text{eff}}=2{,}000$, strongly dependent sequences\n($\\rho=0.8$) exhibit $\\approx76\\%$ smaller generalization gaps than weakly\ndependent ones ($\\rho=0.2$), challenging the intuition that dependence is\npurely detrimental. Yet convergence rates diverge from theory: weak\ndependencies follow $N_{\\text{eff}}^{-1.21}$ scaling and strong dependencies\nfollow $N_{\\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.\nThese findings reveal that temporal dependence can enhance learning under fixed\ninformation budgets, while highlighting gaps between theory and practice that\nmotivate future research."}
{"id": "2508.06263", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06263", "abs": "https://arxiv.org/abs/2508.06263", "authors": ["Andrew Cropper", "David M. Cerna", "Matti Järvisalo"], "title": "Symmetry breaking for inductive logic programming", "comment": null, "summary": "The goal of inductive logic programming is to search for a hypothesis that\ngeneralises training data and background knowledge. The challenge is searching\nvast hypothesis spaces, which is exacerbated because many logically equivalent\nhypotheses exist. To address this challenge, we introduce a method to break\nsymmetries in the hypothesis space. We implement our idea in answer set\nprogramming. Our experiments on multiple domains, including visual reasoning\nand game playing, show that our approach can reduce solving times from over an\nhour to just 17 seconds."}
{"id": "2508.06178", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06178", "abs": "https://arxiv.org/abs/2508.06178", "authors": ["Hugo Abonizio", "Thales Almeida", "Roberto Lotufo", "Rodrigo Nogueira"], "title": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime", "comment": null, "summary": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods."}
{"id": "2508.06097", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06097", "abs": "https://arxiv.org/abs/2508.06097", "authors": ["Simon Bührer", "Andreas Plesner", "Till Aczel", "Roger Wattenhofer"], "title": "Recurrent Deep Differentiable Logic Gate Networks", "comment": null, "summary": "While differentiable logic gates have shown promise in feedforward networks,\ntheir application to sequential modeling remains unexplored. This paper\npresents the first implementation of Recurrent Deep Differentiable Logic Gate\nNetworks (RDDLGN), combining Boolean operations with recurrent architectures\nfor sequence-to-sequence learning.\n  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and\n30.9\\% accuracy during training, approaching GRU performance (5.41 BLEU) and\ngraceful degradation (4.39 BLEU) during inference. This work establishes\nrecurrent logic-based neural computation as viable, opening research directions\nfor FPGA acceleration in sequential modeling and other recursive network\narchitectures."}
{"id": "2508.06296", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06296", "abs": "https://arxiv.org/abs/2508.06296", "authors": ["Pierre Peigné - Lefebvre", "Quentin Feuillade-Montixi", "Tom David", "Nicolas Miailhe"], "title": "LLM Robustness Leaderboard v1 --Technical report", "comment": null, "summary": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community."}
{"id": "2508.06186", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06186", "abs": "https://arxiv.org/abs/2508.06186", "authors": ["Ali Sarabadani", "Maryam Abdollahi Shamami", "Hamidreza Sadeghsalehi", "Borhan Asadi", "Saba Hesaraki"], "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration", "comment": null, "summary": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input."}
{"id": "2508.06108", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06108", "abs": "https://arxiv.org/abs/2508.06108", "authors": ["Xing Lei", "Wenyan Yang", "Kaiqiang Ke", "Shentao Yang", "Xuetao Zhang", "Joni Pajarinen", "Donglin Wang"], "title": "GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning", "comment": null, "summary": "Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a\nfundamental challenge in reinforcement learning. While hindsight experience\nreplay (HER) has shown promise by relabeling collected trajectories with\nachieved goals, we argue that trajectory relabeling alone does not fully\nexploit the available experiences in off-policy GCRL methods, resulting in\nlimited sample efficiency. In this paper, we propose Hindsight Goal-conditioned\nRegularization (HGR), a technique that generates action regularization priors\nbased on hindsight goals. When combined with hindsight self-imitation\nregularization (HSR), our approach enables off-policy RL algorithms to maximize\nexperience utilization. Compared to existing GCRL methods that employ HER and\nself-imitation techniques, our hindsight regularizations achieve substantially\nmore efficient sample reuse and the best performances, which we empirically\ndemonstrate on a suite of navigation and manipulation tasks."}
{"id": "2508.06326", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06326", "abs": "https://arxiv.org/abs/2508.06326", "authors": ["Nathaniel Virgo", "Martin Biehl", "Manuel Baltieri", "Matteo Capucci"], "title": "A \"good regulator theorem\" for embodied agents", "comment": "Accepted at the Artificial Life conference 2025 (ALife 2025). 10\n  pages, 1 figure", "summary": "In a classic paper, Conant and Ashby claimed that \"every good regulator of a\nsystem must be a model of that system.\" Artificial Life has produced many\nexamples of systems that perform tasks with apparently no model in sight; these\nsuggest Conant and Ashby's theorem doesn't easily generalise beyond its\nrestricted setup. Nevertheless, here we show that a similar intuition can be\nfleshed out in a different way: whenever an agent is able to perform a\nregulation task, it is possible for an observer to interpret it as having\n\"beliefs\" about its environment, which it \"updates\" in response to sensory\ninput. This notion of belief updating provides a notion of model that is more\nsophisticated than Conant and Ashby's, as well as a theorem that is more\nbroadly applicable. However, it necessitates a change in perspective, in that\nthe observer plays an essential role in the theory: models are not a mere\nproperty of the system but are imposed on it from outside. Our theorem holds\nregardless of whether the system is regulating its environment in a classic\ncontrol theory setup, or whether it's regulating its own internal state; the\nmodel is of its environment either way. The model might be trivial, however,\nand this is how the apparent counterexamples are resolved."}
{"id": "2508.06194", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06194", "abs": "https://arxiv.org/abs/2508.06194", "authors": ["Lai Jiang", "Yuekang Li", "Xiaohan Zhang", "Youtao Ding", "Li Pan"], "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation", "comment": null, "summary": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage."}
{"id": "2508.06151", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06151", "abs": "https://arxiv.org/abs/2508.06151", "authors": ["Yong Oh Lee", "JeeEun Kim", "Jung Woo Lee"], "title": "Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models", "comment": null, "summary": "In oral cancer diagnostics, the limited availability of annotated datasets\nfrequently constrains the performance of diagnostic models, particularly due to\nthe variability and insufficiency of training data. To address these\nchallenges, this study proposed a novel approach to enhance diagnostic accuracy\nby synthesizing realistic oral cancer lesions using an inpainting technique\nwith a fine-tuned diffusion model. We compiled a comprehensive dataset from\nmultiple sources, featuring a variety of oral cancer images. Our method\ngenerated synthetic lesions that exhibit a high degree of visual fidelity to\nactual lesions, thereby significantly enhancing the performance of diagnostic\nalgorithms. The results show that our classification model achieved a\ndiagnostic accuracy of 0.97 in differentiating between cancerous and\nnon-cancerous tissues, while our detection model accurately identified lesion\nlocations with 0.85 accuracy. This method validates the potential for synthetic\nimage generation in medical diagnostics and paves the way for further research\ninto extending these methods to other types of cancer diagnostics."}
{"id": "2508.06348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06348", "abs": "https://arxiv.org/abs/2508.06348", "authors": ["Mille Mei Zhen Loo", "Gert Luzkov", "Paolo Burelli"], "title": "AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games", "comment": null, "summary": "Cheating in online video games compromises the integrity of gaming\nexperiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face\nsignificant challenges in keeping pace with evolving cheating methods without\nimposing invasive measures on users' systems. This paper presents\nAntiCheatPT\\_256, a transformer-based machine learning model designed to detect\ncheating behaviour in Counter-Strike 2 using gameplay data. To support this, we\nintroduce and publicly release CS2CD: A labelled dataset of 795 matches. Using\nthis dataset, 90,707 context windows were created and subsequently augmented to\naddress class imbalance. The transformer model, trained on these windows,\nachieved an accuracy of 89.17\\% and an AUC of 93.36\\% on an unaugmented test\nset. This approach emphasizes reproducibility and real-world applicability,\noffering a robust baseline for future research in data-driven cheat detection."}
{"id": "2508.06196", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06196", "abs": "https://arxiv.org/abs/2508.06196", "authors": ["Nizi Nazar", "Ehsaneddin Asgari"], "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations", "comment": null, "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment."}
{"id": "2508.06183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06183", "abs": "https://arxiv.org/abs/2508.06183", "authors": ["Xiyuan Yang", "Shengyuan Hu", "Soyeon Kim", "Tian Li"], "title": "Differentially Private Federated Clustering with Random Rebalancing", "comment": "21 pages", "summary": "Federated clustering aims to group similar clients into clusters and produce\none model for each cluster. Such a personalization approach typically improves\nmodel performance compared with training a single model to serve all clients,\nbut can be more vulnerable to privacy leakage. Directly applying client-level\ndifferentially private (DP) mechanisms to federated clustering could degrade\nthe utilities significantly. We identify that such deficiencies are mainly due\nto the difficulties of averaging privacy noise within each cluster (following\nstandard privacy mechanisms), as the number of clients assigned to the same\nclusters is uncontrolled. To this end, we propose a simple and effective\ntechnique, named RR-Cluster, that can be viewed as a light-weight add-on to\nmany federated clustering algorithms. RR-Cluster achieves reduced privacy noise\nvia randomly rebalancing cluster assignments, guaranteeing a minimum number of\nclients assigned to each cluster. We analyze the tradeoffs between decreased\nprivacy noise variance and potentially increased bias from incorrect\nassignments and provide convergence bounds for RR-Clsuter. Empirically, we\ndemonstrate the RR-Cluster plugged into strong federated clustering algorithms\nresults in significantly improved privacy/utility tradeoffs across both\nsynthetic and real-world datasets."}
{"id": "2508.06352", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06352", "abs": "https://arxiv.org/abs/2508.06352", "authors": ["Christian Meske", "Justin Brenne", "Erdi Uenal", "Sabahat Oelcer", "Ayseguel Doganguen"], "title": "From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI", "comment": null, "summary": "Current explainable AI (XAI) approaches prioritize algorithmic transparency\nand present explanations in abstract, non-adaptive formats that often fail to\nsupport meaningful end-user understanding. This paper introduces \"Explanatory\nAI\" as a complementary paradigm that leverages generative AI capabilities to\nserve as explanatory partners for human understanding rather than providers of\nalgorithmic transparency. While XAI reveals algorithmic decision processes for\nmodel validation, Explanatory AI addresses contextual reasoning to support\nhuman decision-making in sociotechnical contexts. We develop a definition and\nsystematic eight-dimensional conceptual model distinguishing Explanatory AI\nthrough narrative communication, adaptive personalization, and progressive\ndisclosure principles. Empirical validation through Rapid Contextual Design\nmethodology with healthcare professionals demonstrates that users consistently\nprefer context-sensitive, multimodal explanations over technical transparency.\nOur findings reveal the practical urgency for AI systems designed for human\ncomprehension rather than algorithmic introspection, establishing a\ncomprehensive research agenda for advancing user-centered AI explanation\napproaches across diverse domains and cultural contexts."}
{"id": "2508.06204", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06204", "abs": "https://arxiv.org/abs/2508.06204", "authors": ["Richard Willats", "Josh Pennington", "Aravind Mohan", "Bertie Vidgen"], "title": "Classification is a RAG problem: A case study on hate speech detection", "comment": null, "summary": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems."}
{"id": "2508.06199", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06199", "abs": "https://arxiv.org/abs/2508.06199", "authors": ["Mateusz Praski", "Jakub Adamczyk", "Wojciech Czech"], "title": "Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning", "comment": null, "summary": "Pretrained neural networks have attracted significant interest in chemistry\nand small molecule drug design. Embeddings from these models are widely used\nfor molecular property prediction, virtual screening, and small data learning\nin molecular chemistry. This study presents the most extensive comparison of\nsuch models to date, evaluating 25 models across 25 datasets. Under a fair\ncomparison framework, we assess models spanning various modalities,\narchitectures, and pretraining strategies. Using a dedicated hierarchical\nBayesian statistical testing model, we arrive at a surprising result: nearly\nall neural models show negligible or no improvement over the baseline ECFP\nmolecular fingerprint. Only the CLAMP model, which is also based on molecular\nfingerprints, performs statistically significantly better than the\nalternatives. These findings raise concerns about the evaluation rigor in\nexisting studies. We discuss potential causes, propose solutions, and offer\npractical recommendations."}
{"id": "2508.06368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06368", "abs": "https://arxiv.org/abs/2508.06368", "authors": ["Claudia dAmato", "Giuseppe Rubini", "Francesco Didio", "Donato Francioso", "Fatima Zahra Amara", "Nicola Fanizzi"], "title": "Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned", "comment": null, "summary": "Legal decision-making process requires the availability of comprehensive and\ndetailed legislative background knowledge and up-to-date information on legal\ncases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a\nvaluable tool to facilitate access to legal information, to be queried and\nexploited for the purpose, and to enable advanced reasoning and machine\nlearning applications. Indeed, legal KGs may act as knowledge intensive\ncomponent to be used by pre-dictive machine learning solutions supporting the\ndecision process of the legal expert. Nevertheless, a few KGs can be found in\nthe legal domain. To fill this gap, we developed a legal KG targeting legal\ncases of violence against women, along with clear adopted methodologies.\nSpecifically, the paper introduces two complementary approaches for automated\nlegal KG construction; a systematic bottom-up approach, customized for the\nlegal domain, and a new solution leveraging Large Language Models. Starting\nfrom legal sentences publicly available from the European Court of Justice, the\nsolutions integrate structured data extraction, ontology development, and\nsemantic enrichment to produce KGs tailored for legal cases involving violence\nagainst women. After analyzing and comparing the results of the two approaches,\nthe developed KGs are validated via suitable competency questions. The obtained\nKG may be impactful for multiple purposes: can improve the accessibility to\nlegal information both to humans and machine, can enable complex queries and\nmay constitute an important knowledge component to be possibly exploited by\nmachine learning tools tailored for predictive justice."}
{"id": "2508.06220", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06220", "abs": "https://arxiv.org/abs/2508.06220", "authors": ["Keummin Ka", "Junhyeong Park", "Jahyun Jeon", "Youngjae Yu"], "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?", "comment": "14 pages, 9 figures", "summary": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems."}
{"id": "2508.06208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06208", "abs": "https://arxiv.org/abs/2508.06208", "authors": ["Ce Na", "Kai Yang", "Dengzhao Fang", "Yu Li", "Jingtong Gao", "Chengcheng Zhu", "Jiale Zhang", "Xiaobing Sun", "Yi Chang"], "title": "Graph Federated Learning for Personalized Privacy Recommendation", "comment": null, "summary": "Federated recommendation systems (FedRecs) have gained significant attention\nfor providing privacy-preserving recommendation services. However, existing\nFedRecs assume that all users have the same requirements for privacy\nprotection, i.e., they do not upload any data to the server. The approaches\noverlook the potential to enhance the recommendation service by utilizing\npublicly available user data. In real-world applications, users can choose to\nbe private or public. Private users' interaction data is not shared, while\npublic users' interaction data can be shared. Inspired by the issue, this paper\nproposes a novel Graph Federated Learning for Personalized Privacy\nRecommendation (GFed-PP) that adapts to different privacy requirements while\nimproving recommendation performance. GFed-PP incorporates the interaction data\nof public users to build a user-item interaction graph, which is then used to\nform a user relationship graph. A lightweight graph convolutional network (GCN)\nis employed to learn each user's user-specific personalized item embedding. To\nprotect user privacy, each client learns the user embedding and the scoring\nfunction locally. Additionally, GFed-PP achieves optimization of the federated\nrecommendation framework through the initialization of item embedding on\nclients and the aggregation of the user relationship graph on the server.\nExperimental results demonstrate that GFed-PP significantly outperforms\nexisting methods for five datasets, offering superior recommendation accuracy\nwithout compromising privacy. This framework provides a practical solution for\naccommodating varying privacy preferences in federated recommendation systems."}
{"id": "2508.06443", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06443", "abs": "https://arxiv.org/abs/2508.06443", "authors": ["Debabrota Basu", "Udvas Das"], "title": "The Fair Game: Auditing & Debiasing AI Algorithms Over Time", "comment": null, "summary": "An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify\ndifferent types of bias (also known as unfairness) exhibited in the predictions\nof ML algorithms, and to design new algorithms to mitigate them. Often, the\ndefinitions of bias used in the literature are observational, i.e. they use the\ninput and output of a pre-trained algorithm to quantify a bias under concern.\nIn reality,these definitions are often conflicting in nature and can only be\ndeployed if either the ground truth is known or only in retrospect after\ndeploying the algorithm. Thus,there is a gap between what we want Fair ML to\nachieve and what it does in a dynamic social environment. Hence, we propose an\nalternative dynamic mechanism,\"Fair Game\",to assure fairness in the predictions\nof an ML algorithm and to adapt its predictions as the society interacts with\nthe algorithm over time. \"Fair Game\" puts together an Auditor and a Debiasing\nalgorithm in a loop around an ML algorithm. The \"Fair Game\" puts these two\ncomponents in a loop by leveraging Reinforcement Learning (RL). RL algorithms\ninteract with an environment to take decisions, which yields new observations\n(also known as data/feedback) from the environment and in turn, adapts future\ndecisions. RL is already used in algorithms with pre-fixed long-term fairness\ngoals. \"Fair Game\" provides a unique framework where the fairness goals can be\nadapted over time by only modifying the auditor and the different biases it\nquantifies. Thus,\"Fair Game\" aims to simulate the evolution of ethical and\nlegal frameworks in the society by creating an auditor which sends feedback to\na debiasing algorithm deployed around an ML system. This allows us to develop a\nflexible and adaptive-over-time framework to build Fair ML systems pre- and\npost-deployment."}
{"id": "2508.06277", "categories": ["cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.06277", "abs": "https://arxiv.org/abs/2508.06277", "authors": ["Theresa Pekarek Rosin", "Burak Can Kaplan", "Stefan Wermter"], "title": "Large Language Model Data Generation for Enhanced Intent Recognition in German Speech", "comment": "11 pages, 3 figures, accepted at KONVENS 2025", "summary": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility."}
{"id": "2508.06214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06214", "abs": "https://arxiv.org/abs/2508.06214", "authors": ["Hai Zhong", "Xun Wang", "Zhuoran Li", "Longbo Huang"], "title": "Reparameterization Proximal Policy Optimization", "comment": null, "summary": "Reparameterization policy gradient (RPG) is promising for improving sample\nefficiency by leveraging differentiable dynamics. However, a critical barrier\nis its training instability, where high-variance gradients can destabilize the\nlearning process. To address this, we draw inspiration from Proximal Policy\nOptimization (PPO), which uses a surrogate objective to enable stable sample\nreuse in the model-free setting. We first establish a connection between this\nsurrogate objective and RPG, which has been largely unexplored and is\nnon-trivial. Then, we bridge this gap by demonstrating that the\nreparameterization gradient of a PPO-like surrogate objective can be computed\nefficiently using backpropagation through time. Based on this key insight, we\npropose Reparameterization Proximal Policy Optimization (RPO), a stable and\nsample-efficient RPG-based method. RPO enables multiple epochs of stable sample\nreuse by optimizing a clipped surrogate objective tailored for RPG, while being\nfurther stabilized by Kullback-Leibler (KL) divergence regularization and\nremaining fully compatible with existing variance reduction methods. We\nevaluate RPO on a suite of challenging locomotion and manipulation tasks, where\nexperiments demonstrate that our method achieves superior sample efficiency and\nstrong performance."}
{"id": "2508.06454", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06454", "abs": "https://arxiv.org/abs/2508.06454", "authors": ["Joshua Caiata", "Ben Armstrong", "Kate Larson"], "title": "What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting", "comment": "41 pages", "summary": "Committee-selection problems arise in many contexts and applications, and\nthere has been increasing interest within the social choice research community\non identifying which properties are satisfied by different multi-winner voting\nrules. In this work, we propose a data-driven framework to evaluate how\nfrequently voting rules violate axioms across diverse preference distributions\nin practice, shifting away from the binary perspective of axiom satisfaction\ngiven by worst-case analysis. Using this framework, we analyze the relationship\nbetween multi-winner voting rules and their axiomatic performance under several\npreference distributions. We then show that neural networks, acting as voting\nrules, can outperform traditional rules in minimizing axiom violations. Our\nresults suggest that data-driven approaches to social choice can inform the\ndesign of new voting systems and support the continuation of data-driven\nresearch in social choice."}
{"id": "2508.06309", "categories": ["cs.CL", "math.PR"], "pdf": "https://arxiv.org/pdf/2508.06309", "abs": "https://arxiv.org/abs/2508.06309", "authors": ["Ruichong Zhang"], "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC", "comment": null, "summary": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible."}
{"id": "2508.06243", "categories": ["cs.LG", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06243", "abs": "https://arxiv.org/abs/2508.06243", "authors": ["Ioan-Sorin Comsa", "Purav Shah", "Karthik Vaidhyanathan", "Deepak Gangadharan", "Christof Imhof", "Per Bergamin", "Aryan Kaushik", "Gabriel-Miro Muntean", "Ramona Trestian"], "title": "SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems", "comment": null, "summary": "The advent of 6G networks opens new possibilities for connected infotainment\nservices in vehicular environments. However, traditional Radio Resource\nManagement (RRM) techniques struggle with the increasing volume and complexity\nof data such as Channel Quality Indicators (CQI) from autonomous vehicles. To\naddress this, we propose SCAR (State-Space Compression for AI-Driven Resource\nManagement), an Edge AI-assisted framework that optimizes scheduling and\nfairness in vehicular infotainment. SCAR employs ML-based compression\ntechniques (e.g., clustering and RBF networks) to reduce CQI data size while\npreserving essential features. These compressed states are used to train\n6G-enabled Reinforcement Learning policies that maximize throughput while\nmeeting fairness objectives defined by the NGMN. Simulations show that SCAR\nincreases time in feasible scheduling regions by 14\\% and reduces unfair\nscheduling time by 15\\% compared to RL baselines without CQI compression.\nFurthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based\nclustering reduces CQI clustering distortion by 10\\%, confirming its\nefficiency. These results demonstrate SCAR's scalability and fairness benefits\nfor dynamic vehicular networks."}
{"id": "2304.04475", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2304.04475", "abs": "https://arxiv.org/abs/2304.04475", "authors": ["Gaurav Deshkar", "Jayanta Kshirsagar", "Harshal Hayatnagarkar", "Janani Venugopalan"], "title": "Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient", "comment": null, "summary": "To mitigate the impact of the pandemic, several measures include lockdowns,\nrapid vaccination programs, school closures, and economic stimulus. These\ninterventions can have positive or unintended negative consequences. Current\nresearch to model and determine an optimal intervention automatically through\nround-tripping is limited by the simulation objectives, scale (a few thousand\nindividuals), model types that are not suited for intervention studies, and the\nnumber of intervention strategies they can explore (discrete vs continuous). We\naddress these challenges using a Deep Deterministic Policy Gradient (DDPG)\nbased policy optimization framework on a large-scale (100,000 individual)\nepidemiological agent-based simulation where we perform multi-objective\noptimization. We determine the optimal policy for lockdown and vaccination in a\nminimalist age-stratified multi-vaccine scenario with a basic simulation for\neconomic activity. With no lockdown and vaccination (mid-age and elderly),\nresults show optimal economy (individuals below the poverty line) with balanced\nhealth objectives (infection, and hospitalization). An in-depth simulation is\nneeded to further validate our results and open-source our framework."}
{"id": "2508.06345", "categories": ["cs.CL", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06345", "abs": "https://arxiv.org/abs/2508.06345", "authors": ["Yanbin Wei", "Jiangyue Yan", "Chun Kang", "Yang Chen", "Hua Liu", "James T. Kwok", "Yu Zhang"], "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering", "comment": null, "summary": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy"}
{"id": "2508.06244", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06244", "abs": "https://arxiv.org/abs/2508.06244", "authors": ["Xurun Wang", "Guangrui Liu", "Xinjie Li", "Haoyu He", "Lin Yao", "Weizhe Zhang"], "title": "Membership Inference Attack with Partial Features", "comment": null, "summary": "Machine learning models have been shown to be susceptible to membership\ninference attack, which can be used to determine whether a given sample appears\nin the training data. Existing membership inference methods commonly assume\nthat the adversary has full access to the features of the target sample. This\nassumption, however, does not hold in many real-world scenarios where only\npartial features information is available, thereby limiting the applicability\nof these methods. In this work, we study an inference scenario where the\nadversary observes only partial features of each sample and aims to infer\nwhether this observed subset was present in the training set of the target\nmodel. We define this problem as Partial Feature Membership Inference (PFMI).\nTo address this problem, we propose MRAD (Memory-guided Reconstruction and\nAnomaly Detection), a two-stage attack framework. In the first stage, MRAD\noptimizes the unknown feature values to minimize the loss of the sample. In the\nsecond stage, it measures the deviation between the reconstructed sample and\nthe training distribution using anomaly detection. Empirical results\ndemonstrate that MRAD is effective across a range of datasets, and maintains\ncompatibility with various off-the-shelf anomaly detection techniques. For\nexample, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of\nthe missing features."}
{"id": "2508.06360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06360", "abs": "https://arxiv.org/abs/2508.06360", "authors": ["Aisha Saeid", "Anu Sabu", "Girish A. Koushik", "Ferrante Neri", "Diptesh Kanojia"], "title": "Cyberbullying Detection via Aggression-Enhanced Prompting", "comment": "Accepted to RANLP 2025", "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks."}
{"id": "2508.06247", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06247", "abs": "https://arxiv.org/abs/2508.06247", "authors": ["Zichun Ye", "Runqi Wang", "Xutong Liu", "Shuai Li"], "title": "Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits", "comment": null, "summary": "The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential\ndecision-making framework, dominated by two algorithmic families: UCB-based and\nadversarial methods such as follow the regularized leader (FTRL) and online\nmirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer\nfrom additional regret factor $\\log T$ that is detrimental over long horizons,\nwhile adversarial methods such as EXP3.M and HYBRID impose significant\ncomputational overhead. To resolve this trade-off, we introduce the\nCombinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS\nis a computationally efficient algorithm that achieves an instance-independent\nregret of $O\\big( (\\log k)^2\\sqrt{kmT}\\big )$ under semi-bandit feedback, where\n$m$ is the number of arms and $k$ is the maximum cardinality of a feasible\naction. Crucially, this result eliminates the dependency on $\\log T$ and\nmatches the established $\\Omega\\big( \\sqrt{kmT}\\big)$ lower bound up to\n$O\\big((\\log k)^2\\big)$. We then extend our analysis to show that CMOSS is also\napplicable to cascading feedback. Experiments on synthetic and real-world\ndatasets validate that CMOSS consistently outperforms benchmark algorithms in\nboth regret and runtime efficiency."}
{"id": "2508.05637", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05637", "abs": "https://arxiv.org/abs/2508.05637", "authors": ["Siddharth Gangwar", "David A. Selby", "Sebastian J. Vollmer"], "title": "Automated Visualization Makeovers with LLMs", "comment": null, "summary": "Making a good graphic that accurately and efficiently conveys the desired\nmessage to the audience is both an art and a science, typically not taught in\nthe data science curriculum. Visualisation makeovers are exercises where the\ncommunity exchange feedback to improve charts and data visualizations. Can\nmulti-modal large language models (LLMs) emulate this task? Given a plot in the\nform of an image file, or the code used to generate it, an LLM, primed with a\nlist of visualization best practices, is employed to semi-automatically\ngenerate constructive criticism to produce a better plot. Our system is centred\naround prompt engineering of a pre-trained model, relying on a combination of\nuserspecified guidelines and any latent knowledge of data visualization\npractices that might lie within an LLMs training corpus. Unlike other works,\nthe focus is not on generating valid visualization scripts from raw data or\nprompts, but on educating the user how to improve their existing data\nvisualizations according to an interpretation of best practices. A quantitative\nevaluation is performed to measure the sensitivity of the LLM agent to various\nplotting issues across different chart types. We make the tool available as a\nsimple self-hosted applet with an accessible Web interface."}
{"id": "2508.06374", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06374", "abs": "https://arxiv.org/abs/2508.06374", "authors": ["Anubhav Jangra", "Bahareh Sarrafzadeh", "Adrian de Wynter", "Silviu Cucerzan", "Sujay Kumar Jauhar"], "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions", "comment": null, "summary": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation."}
{"id": "2508.06249", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06249", "abs": "https://arxiv.org/abs/2508.06249", "authors": ["David Kaczér", "Magnus Jørgenvåg", "Clemens Vetter", "Lucie Flek", "Florian Mai"], "title": "In-Training Defenses against Emergent Misalignment in Language Models", "comment": "Under review", "summary": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research."}
{"id": "2508.05653", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05653", "abs": "https://arxiv.org/abs/2508.05653", "authors": ["Jules Clerc", "Domitile Lourdeaux", "Mohamed Sallak", "Johann Barbier", "Marc Ravaine"], "title": "Modeling Interactive Narrative Systems: A Formal Approach", "comment": null, "summary": "Interactive Narrative Systems (INS) have revolutionized digital experiences\nby empowering users to actively shape their stories, diverging from traditional\npassive storytelling. However, the field faces challenges due to fragmented\nresearch efforts and diverse system representations. This paper introduces a\nformal representation framework for INS, inspired by diverse approaches from\nthe state of the art. By providing a consistent vocabulary and modeling\nstructure, the framework facilitates the analysis, the description and\ncomparison of INS properties. Experimental validations on the \"Little Red\nRiding Hood\" scenario highlight the usefulness of the proposed formalism and\nits impact on improving the evaluation of INS. This work aims to foster\ncollaboration and coherence within the INS research community by proposing a\nmethodology for formally representing these systems."}
{"id": "2508.06388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06388", "abs": "https://arxiv.org/abs/2508.06388", "authors": ["Lanlan Qiu", "Xiao Pu", "Yeqi Feng", "Tianxing He"], "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing", "comment": "21 pages, 17 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime."}
{"id": "2508.06251", "categories": ["cs.LG", "cs.AI", "cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06251", "abs": "https://arxiv.org/abs/2508.06251", "authors": ["Alejandro Moreno R.", "Desale Fentaw", "Samuel Palmer", "Raúl Salles de Padua", "Ninad Dixit", "Samuel Mugel", "Roman Orús", "Manuel Radons", "Josef Menter", "Ali Abedi"], "title": "Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)", "comment": "10 pages", "summary": "Synthetic data generation is a key technique in modern artificial\nintelligence, addressing data scarcity, privacy constraints, and the need for\ndiverse datasets in training robust models. In this work, we propose a method\nfor generating privacy-preserving high-quality synthetic tabular data using\nTensor Networks, specifically Matrix Product States (MPS). We benchmark the\nMPS-based generative model against state-of-the-art models such as CTGAN, VAE,\nand PrivBayes, focusing on both fidelity and privacy-preserving capabilities.\nTo ensure differential privacy (DP), we integrate noise injection and gradient\nclipping during training, enabling privacy guarantees via R\\'enyi Differential\nPrivacy accounting. Across multiple metrics analyzing data fidelity and\ndownstream machine learning task performance, our results show that MPS\noutperforms classical models, particularly under strict privacy constraints.\nThis work highlights MPS as a promising tool for privacy-aware synthetic data\ngeneration. By combining the expressive power of tensor network representations\nwith formal privacy mechanisms, the proposed approach offers an interpretable\nand scalable alternative for secure data sharing. Its structured design\nfacilitates integration into sensitive domains where both data quality and\nconfidentiality are critical."}
{"id": "2508.05791", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05791", "abs": "https://arxiv.org/abs/2508.05791", "authors": ["Haoran Li", "Lihao Mai", "Muhao Guo", "Jiaqi Wu", "Yang Weng", "Yannan Sun", "Ce Jimmy Liu"], "title": "From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data", "comment": "10 pages", "summary": "Accurate distribution grid topology is essential for reliable modern grid\noperations. However, real-world utility data originates from multiple sources\nwith varying characteristics and levels of quality. In this work, developed in\ncollaboration with Oncor Electric Delivery, we propose a scalable framework\nthat reconstructs a trustworthy grid topology by systematically integrating\nheterogeneous data. We observe that distribution topology is fundamentally\ngoverned by two complementary dimensions: the spatial layout of physical\ninfrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the\nsystem in the signal domain (e.g., voltage time series). When jointly\nleveraged, these dimensions support a complete and physically coherent\nreconstruction of network connectivity. To address the challenge of uneven data\nquality without compromising observability, we introduce a confidence-aware\ninference mechanism that preserves structurally informative yet imperfect\ninputs, while quantifying the reliability of each inferred connection for\noperator interpretation. This soft handling of uncertainty is tightly coupled\nwith hard enforcement of physical feasibility: we embed operational\nconstraints, such as transformer capacity limits and radial topology\nrequirements, directly into the learning process. Together, these components\nensure that inference is both uncertainty-aware and structurally valid,\nenabling rapid convergence to actionable, trustworthy topologies under\nreal-world deployment conditions. The proposed framework is validated using\ndata from over 8000 meters across 3 feeders in Oncor's service territory,\ndemonstrating over 95% accuracy in topology reconstruction and substantial\nimprovements in confidence calibration and computational efficiency relative to\nbaseline methods."}
{"id": "2508.06418", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06418", "abs": "https://arxiv.org/abs/2508.06418", "authors": ["Haoran Shi", "Hongwei Yao", "Shuo Shao", "Shaopeng Jiao", "Ziqi Peng", "Zhan Qin", "Cong Wang"], "title": "Quantifying Conversation Drift in MCP via Latent Polytope", "comment": null, "summary": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy."}
{"id": "2508.06257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06257", "abs": "https://arxiv.org/abs/2508.06257", "authors": ["Jielong Lu", "Zhihao Wu", "Jiajun Yu", "Jiajun Bu", "Haishuai Wang"], "title": "Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors", "comment": null, "summary": "Integrating multi-omics datasets through data-driven analysis offers a\ncomprehensive understanding of the complex biological processes underlying\nvarious diseases, particularly cancer. Graph Neural Networks (GNNs) have\nrecently demonstrated remarkable ability to exploit relational structures in\nbiological data, enabling advances in multi-omics integration for cancer\nsubtype classification. Existing approaches often neglect the intricate\ncoupling between heterogeneous omics, limiting their capacity to resolve subtle\ncancer subtype heterogeneity critical for precision oncology. To address these\nlimitations, we propose a framework named Graph Transformer for Multi-omics\nCancer Subtype Classification (GTMancer). This framework builds upon the GNN\noptimization problem and extends its application to complex multi-omics data.\nSpecifically, our method leverages contrastive learning to embed multi-omics\ndata into a unified semantic space. We unroll the multiplex graph optimization\nproblem in that unified space and introduce dual sets of attention coefficients\nto capture structural graph priors both within and among multi-omics data. This\napproach enables global omics information to guide the refining of the\nrepresentations of individual omics. Empirical experiments on seven real-world\ncancer datasets demonstrate that GTMancer outperforms existing state-of-the-art\nalgorithms."}
{"id": "2508.05880", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05880", "abs": "https://arxiv.org/abs/2508.05880", "authors": ["Sree Bhattacharyya", "Lucas Craig", "Tharun Dilliraj", "Jia Li", "James Z. Wang"], "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models", "comment": null, "summary": "Affective Computing has been established as a crucial field of inquiry to\nadvance the holistic development of Artificial Intelligence (AI) systems.\nFoundation models -- especially Large Language Models (LLMs) -- have been\nevaluated, trained, or instruction-tuned in several past works, to become\nbetter predictors or generators of emotion. Most of these studies, however,\napproach emotion-related tasks in a supervised manner, assessing or training\nthe capabilities of LLMs using discrete emotion labels associated with stimuli\n(e.g., text, images, video, audio). Evaluation studies, in particular, have\noften been limited to standard and superficial emotion-related tasks, such as\nthe recognition of evoked or expressed emotions. In this paper, we move beyond\nsurface-level emotion tasks to investigate how LLMs reason about emotions\nthrough cognitive dimensions. Drawing from cognitive appraisal theory, we\nexamine whether LLMs produce coherent and plausible cognitive reasoning when\nreasoning about emotionally charged stimuli. We introduce a large-scale\nbenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal\ncognitive structures implicitly used by LLMs for emotional reasoning. Through a\nplethora of evaluation experiments and analysis, we seek to answer: (a) Are\nmodels more likely to implicitly rely on specific cognitive appraisal\ndimensions?, (b) What cognitive dimensions are important for characterizing\nspecific emotions?, and, (c) Can the internal representations of different\nemotion categories in LLMs be interpreted through cognitive appraisal\ndimensions? Our results and analyses reveal diverse reasoning patterns across\ndifferent LLMs. Our benchmark and code will be made publicly available."}
{"id": "2508.06433", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06433", "abs": "https://arxiv.org/abs/2508.06433", "authors": ["Runnan Fang", "Yuan Liang", "Xiaobin Wang", "Jialong Wu", "Shuofei Qiao", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "title": "Memp: Exploring Agent Procedural Memory", "comment": "Work in progress", "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains."}
{"id": "2508.06269", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06269", "abs": "https://arxiv.org/abs/2508.06269", "authors": ["Zhuoran Li", "Xun Wang", "Hai Zhong", "Longbo Huang"], "title": "OM2P: Offline Multi-Agent Mean-Flow Policy", "comment": null, "summary": "Generative models, especially diffusion and flow-based models, have been\npromising in offline multi-agent reinforcement learning. However, integrating\npowerful generative models into this framework poses unique challenges. In\nparticular, diffusion and flow-based policies suffer from low sampling\nefficiency due to their iterative generation processes, making them impractical\nin time-sensitive or resource-constrained settings. To tackle these\ndifficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel\noffline MARL algorithm to achieve efficient one-step action sampling. To\naddress the misalignment between generative objectives and reward maximization,\nwe introduce a reward-aware optimization scheme that integrates a\ncarefully-designed mean-flow matching loss with Q-function supervision.\nAdditionally, we design a generalized timestep distribution and a\nderivative-free estimation strategy to reduce memory overhead and improve\ntraining stability. Empirical evaluations on Multi-Agent Particle and MuJoCo\nbenchmarks demonstrate that OM2P achieves superior performance, with up to a\n3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.\nOur approach represents the first to successfully integrate mean-flow model\ninto offline MARL, paving the way for practical and scalable generative\npolicies in cooperative multi-agent settings."}
{"id": "2508.05913", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05913", "abs": "https://arxiv.org/abs/2508.05913", "authors": ["Stefan Pasch", "Min Chul Cha"], "title": "Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction", "comment": null, "summary": "As AI systems become increasingly embedded in organizational workflows and\nconsumer applications, ethical principles such as fairness, transparency, and\nrobustness have been widely endorsed in policy and industry guidelines.\nHowever, there is still scarce empirical evidence on whether these principles\nare recognized, valued, or impactful from the perspective of users. This study\ninvestigates the link between ethical AI and user satisfaction by analyzing\nover 100,000 user reviews of AI products from G2. Using transformer-based\nlanguage models, we measure sentiment across seven ethical dimensions defined\nby the EU Ethics Guidelines for Trustworthy AI. Our findings show that all\nseven dimensions are positively associated with user satisfaction. Yet, this\nrelationship varies systematically across user and product types. Technical\nusers and reviewers of AI development platforms more frequently discuss\nsystem-level concerns (e.g., transparency, data governance), while\nnon-technical users and reviewers of end-user applications emphasize\nhuman-centric dimensions (e.g., human agency, societal well-being). Moreover,\nthe association between ethical AI and user satisfaction is significantly\nstronger for non-technical users and end-user applications across all\ndimensions. Our results highlight the importance of ethical AI design from\nusers' perspectives and underscore the need to account for contextual\ndifferences across user roles and product types."}
{"id": "2508.06435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06435", "abs": "https://arxiv.org/abs/2508.06435", "authors": ["Andrea Nasuto", "Stefano Maria Iacus", "Francisco Rowe", "Devika Jain"], "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages", "comment": null, "summary": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research."}
{"id": "2508.06280", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06280", "abs": "https://arxiv.org/abs/2508.06280", "authors": ["Gokul Adethya T", "S. Jaya Nirmala"], "title": "A Study on Regularization-Based Continual Learning Methods for Indic ASR", "comment": null, "summary": "Indias linguistic diversity poses significant challenges for developing\ninclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual\nmodels, which require simultaneous access to all language data, are impractical\ndue to the sequential arrival of data and privacy constraints. Continual\nLearning (CL) offers a solution by enabling models to learn new languages\nsequentially without catastrophically forgetting previously learned knowledge.\nThis paper investigates CL for ASR on Indian languages using a subset of the\nIndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,\ninitially pretrained on Hindi, which is then incrementally trained on eight\nadditional Indian languages, for a total sequence of nine languages. We\nevaluate three prominent regularization- and distillation-based CL strategies:\nElastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning\nwithout Forgetting (LwF), selected for their suitability in no-replay,\nprivacy-conscious scenarios. Performance is analyzed using Word Error Rate\n(WER) for both RNN-T and CTC paths on clean and noisy data, as well as\nknowledge retention via Backward Transfer. We also explore the impact of\nvarying the number of training epochs (1, 2, 5, and 10) per task. Results,\ncompared against naive fine-tuning, demonstrate CLs effectiveness in mitigating\nforgetting, making it a promising approach for scalable ASR in diverse Indian\nlanguages under realistic constraints. The code is available at:\nhttps://github.com/FrozenWolf-Cyber/Indic-CL-ASR"}
{"id": "2508.05933", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05933", "abs": "https://arxiv.org/abs/2508.05933", "authors": ["Xueyuan Xu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "title": "REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition", "comment": null, "summary": "The affective brain-computer interface is a crucial technology for affective\ninteraction and emotional intelligence, emerging as a significant area of\nresearch in the human-computer interaction. Compared to single-type features,\nmulti-type EEG features provide a multi-level representation for analyzing\nmulti-dimensional emotions. However, the high dimensionality of multi-type EEG\nfeatures, combined with the relatively small number of high-quality EEG\nsamples, poses challenges such as classifier overfitting and suboptimal\nreal-time performance in multi-dimensional emotion recognition. Moreover,\npractical applications of affective brain-computer interface frequently\nencounters partial absence of multi-dimensional emotional labels due to the\nopen nature of the acquisition environment, and ambiguity and variability in\nindividual emotion perception. To address these challenges, this study proposes\na novel EEG feature selection method for missing multi-dimensional emotion\nrecognition. The method leverages adaptive orthogonal non-negative matrix\nfactorization to reconstruct the multi-dimensional emotional label space\nthrough second-order and higher-order correlations, which could reduce the\nnegative impact of missing values and outliers on label reconstruction.\nSimultaneously, it employs least squares regression with graph-based manifold\nlearning regularization and global feature redundancy minimization\nregularization to enable EEG feature subset selection despite missing\ninformation, ultimately achieving robust EEG-based multi-dimensional emotion\nrecognition. Simulation experiments on three widely used multi-dimensional\nemotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method\noutperforms thirteen advanced feature selection methods in terms of robustness\nfor EEG emotional feature selection."}
{"id": "2508.06445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06445", "abs": "https://arxiv.org/abs/2508.06445", "authors": ["Abolfazl Ansari", "Delvin Ce Zhang", "Nafis Irtiza Tripto", "Dongwon Lee"], "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking", "comment": "To appear in 18th International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, and to be published in the Springer LNCS series", "summary": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media."}
{"id": "2508.06292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06292", "abs": "https://arxiv.org/abs/2508.06292", "authors": ["Sanja Karilanova", "Subhrakanti Dey", "Ayça Özçelikkale"], "title": "Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback", "comment": "15 pages, 7 Tables, 6 Figures", "summary": "Neuromorphic computing is an emerging technology enabling low-latency and\nenergy-efficient signal processing. A key algorithmic tool in neuromorphic\ncomputing is spiking neural networks (SNNs). SNNs are biologically inspired\nneural networks which utilize stateful neurons, and provide low-bit data\nprocessing by encoding and decoding information using spikes. Similar to SNNs,\ndeep state-space models (SSMs) utilize stateful building blocks. However, deep\nSSMs, which recently achieved competitive performance in various temporal\nmodeling tasks, are typically designed with high-precision activation functions\nand no reset mechanisms. To bridge the gains offered by SNNs and the recent\ndeep SSM models, we propose a novel multiple-output spiking neuron model that\ncombines a linear, general SSM state transition with a non-linear feedback\nmechanism through reset. Compared to the existing neuron models for SNNs, our\nproposed model clearly conceptualizes the differences between the spiking\nfunction, the reset condition and the reset action. The experimental results on\nvarious tasks, i.e., a keyword spotting task, an event-based vision task and a\nsequential pattern recognition task, show that our proposed model achieves\nperformance comparable to existing benchmarks in the SNN literature. Our\nresults illustrate how the proposed reset mechanism can overcome instability\nand enable learning even when the linear part of neuron dynamics is unstable,\nallowing us to go beyond the strictly enforced stability of linear dynamics in\nrecent deep SSM models."}
{"id": "2508.05934", "categories": ["cs.HC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05934", "abs": "https://arxiv.org/abs/2508.05934", "authors": ["Xueyuan Xu", "Tianze Yu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "title": "ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection", "comment": null, "summary": "Recently, multi-modal physiological signals based emotion recognition has\ngarnered increasing attention in the field of brain-computer interfaces.\nNevertheness, the associated multi-modal physiological features are often\nhigh-dimensional and inevitably include irrelevant, redundant, and noisy\nrepresentation, which can easily lead to overfitting, poor performance, and\nhigh computational complexity in emotion classifiers. Feature selection has\nbeen widely applied to address these challenges. However, previous studies\ngenerally assumed that multi-modal physiological data are complete, whereas in\nreality, the data are often incomplete due to the openness of the acquisition\nand operational environment. For example, a part of samples are available in\nseveral modalities but not in others. To address this issue, we propose a novel\nmethod for incomplete multi-modal physiological signal feature selection called\nadaptive shared latent structure learning (ASLSL). Based on the property that\nsimilar features share similar emotional labels, ASLSL employs adaptive shared\nlatent structure learning to explore a common latent space shared for\nincomplete multi-modal physiological signals and multi-dimensional emotional\nlabels, thereby mitigating the impact of missing information and mining\nconsensus information. Two most popular multi-modal physiological emotion\ndatasets (DEAP and DREAMER) with multi-dimensional emotional labels were\nutilized to compare the performance between compare ASLSL and seventeen feature\nselection methods. Comprehensive experimental results on these datasets\ndemonstrate the effectiveness of ASLSL."}
{"id": "2508.06447", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06447", "abs": "https://arxiv.org/abs/2508.06447", "authors": ["Lingkun Long", "Rubing Yang", "Yushi Huang", "Desheng Hui", "Ao Zhou", "Jianlei Yang"], "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning", "comment": null, "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."}
{"id": "2508.06301", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.06301", "abs": "https://arxiv.org/abs/2508.06301", "authors": ["Junhyeog Yun", "Minui Hong", "Gunhee Kim"], "title": "FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields", "comment": "ICCV 2025", "summary": "Neural fields provide a memory-efficient representation of data, which can\neffectively handle diverse modalities and large-scale data. However, learning\nto map neural fields often requires large amounts of training data and\ncomputations, which can be limited to resource-constrained edge devices. One\napproach to tackle this limitation is to leverage Federated Meta-Learning\n(FML), but traditional FML approaches suffer from privacy leakage. To address\nthese issues, we introduce a novel FML approach called FedMeNF. FedMeNF\nutilizes a new privacy-preserving loss function that regulates privacy leakage\nin the local meta-optimization. This enables the local meta-learner to optimize\nquickly and efficiently without retaining the client's private data. Our\nexperiments demonstrate that FedMeNF achieves fast optimization speed and\nrobust reconstruction performance, even with few-shot or non-IID data across\ndiverse data modalities, while preserving client data privacy."}
{"id": "2508.05938", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "pdf": "https://arxiv.org/pdf/2508.05938", "abs": "https://arxiv.org/abs/2508.05938", "authors": ["Rafal Kocielnik", "Min Kim", "Penphob", "Boonyarungsrit", "Fereshteh Soltani", "Deshawn Sambrano", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale", "comment": "9 pages, 4 figures, 4 tables", "summary": "Detecting prosociality in text--communication intended to affirm, support, or\nimprove others' behavior--is a novel and increasingly important challenge for\ntrust and safety systems. Unlike toxic content detection, prosociality lacks\nwell-established definitions and labeled data, requiring new approaches to both\nannotation and deployment. We present a practical, three-stage pipeline that\nenables scalable, high-precision prosocial content classification while\nminimizing human labeling effort and inference costs. First, we identify the\nbest LLM-based labeling strategy using a small seed set of human-labeled\nexamples. We then introduce a human-AI refinement loop, where annotators review\nhigh-disagreement cases between GPT-4 and humans to iteratively clarify and\nexpand the task definition-a critical step for emerging annotation tasks like\nprosociality. This process results in improved label quality and definition\nalignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train\na two-stage inference system: a lightweight classifier handles high-confidence\npredictions, while only $\\sim$35\\% of ambiguous instances are escalated to\nGPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving\nhigh precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI\ninteraction, careful task formulation, and deployment-aware architecture design\ncan unlock scalable solutions for novel responsible AI tasks."}
{"id": "2508.06471", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06471", "abs": "https://arxiv.org/abs/2508.06471", "authors": ["GLM-4. 5 Team", ":", "Aohan Zeng", "Xin Lv", "Qinkai Zheng", "Zhenyu Hou", "Bin Chen", "Chengxing Xie", "Cunxiang Wang", "Da Yin", "Hao Zeng", "Jiajie Zhang", "Kedong Wang", "Lucen Zhong", "Mingdao Liu", "Rui Lu", "Shulin Cao", "Xiaohan Zhang", "Xuancheng Huang", "Yao Wei", "Yean Cheng", "Yifan An", "Yilin Niu", "Yuanhao Wen", "Yushi Bai", "Zhengxiao Du", "Zihan Wang", "Zilin Zhu", "Bohan Zhang", "Bosi Wen", "Bowen Wu", "Bowen Xu", "Can Huang", "Casey Zhao", "Changpeng Cai", "Chao Yu", "Chen Li", "Chendi Ge", "Chenghua Huang", "Chenhui Zhang", "Chenxi Xu", "Chenzheng Zhu", "Chuang Li", "Congfeng Yin", "Daoyan Lin", "Dayong Yang", "Dazhi Jiang", "Ding Ai", "Erle Zhu", "Fei Wang", "Gengzheng Pan", "Guo Wang", "Hailong Sun", "Haitao Li", "Haiyang Li", "Haiyi Hu", "Hanyu Zhang", "Hao Peng", "Hao Tai", "Haoke Zhang", "Haoran Wang", "Haoyu Yang", "He Liu", "He Zhao", "Hongwei Liu", "Hongxi Yan", "Huan Liu", "Huilong Chen", "Ji Li", "Jiajing Zhao", "Jiamin Ren", "Jian Jiao", "Jiani Zhao", "Jianyang Yan", "Jiaqi Wang", "Jiayi Gui", "Jiayue Zhao", "Jie Liu", "Jijie Li", "Jing Li", "Jing Lu", "Jingsen Wang", "Jingwei Yuan", "Jingxuan Li", "Jingzhao Du", "Jinhua Du", "Jinxin Liu", "Junkai Zhi", "Junli Gao", "Ke Wang", "Lekang Yang", "Liang Xu", "Lin Fan", "Lindong Wu", "Lintao Ding", "Lu Wang", "Man Zhang", "Minghao Li", "Minghuan Xu", "Mingming Zhao", "Mingshu Zhai", "Pengfan Du", "Qian Dong", "Shangde Lei", "Shangqing Tu", "Shangtong Yang", "Shaoyou Lu", "Shijie Li", "Shuang Li", "Shuang-Li", "Shuxun Yang", "Sibo Yi", "Tianshu Yu", "Wei Tian", "Weihan Wang", "Wenbo Yu", "Weng Lam Tam", "Wenjie Liang", "Wentao Liu", "Xiao Wang", "Xiaohan Jia", "Xiaotao Gu", "Xiaoying Ling", "Xin Wang", "Xing Fan", "Xingru Pan", "Xinyuan Zhang", "Xinze Zhang", "Xiuqing Fu", "Xunkai Zhang", "Yabo Xu", "Yandong Wu", "Yida Lu", "Yidong Wang", "Yilin Zhou", "Yiming Pan", "Ying Zhang", "Yingli Wang", "Yingru Li", "Yinpei Su", "Yipeng Geng", "Yitong Zhu", "Yongkun Yang", "Yuhang Li", "Yuhao Wu", "Yujiang Li", "Yunan Liu", "Yunqing Wang", "Yuntao Li", "Yuxuan Zhang", "Zezhen Liu", "Zhen Yang", "Zhengda Zhou", "Zhongpei Qiao", "Zhuoer Feng", "Zhuorui Liu", "Zichen Zhang", "Zihan Wang", "Zijun Yao", "Zikang Wang", "Ziqiang Liu", "Ziwei Chai", "Zixuan Li", "Zuodong Zhao", "Wenguang Chen", "Jidong Zhai", "Bin Xu", "Minlie Huang", "Hongning Wang", "Juanzi Li", "Yuxiao Dong", "Jie Tang"], "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models", "comment": null, "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5."}
{"id": "2508.06336", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06336", "abs": "https://arxiv.org/abs/2508.06336", "authors": ["Constantin Ruhdorfer", "Matteo Bortoletto", "Victor Oei", "Anna Penzkofer", "Andreas Bulling"], "title": "Unsupervised Partner Design Enables Robust Ad-hoc Teamwork", "comment": "16 pages", "summary": "We introduce Unsupervised Partner Design (UPD) - a population-free,\nmulti-agent reinforcement learning framework for robust ad-hoc teamwork that\nadaptively generates training partners without requiring pretrained partners or\nmanual parameter tuning. UPD constructs diverse partners by stochastically\nmixing an ego agent's policy with biased random behaviours and scores them\nusing a variance-based learnability metric that prioritises partners near the\nego agent's current learning frontier. We show that UPD can be integrated with\nunsupervised environment design, resulting in the first method enabling fully\nunsupervised curricula over both level and partner distributions in a\ncooperative setting. Through extensive evaluations on Overcooked-AI and the\nOvercooked Generalisation Challenge, we demonstrate that this dynamic partner\ncurriculum is highly effective: UPD consistently outperforms both\npopulation-based and population-free baselines as well as ablations. In a user\nstudy, we further show that UPD achieves higher returns than all baselines and\nwas perceived as significantly more adaptive, more human-like, a better\ncollaborator, and less frustrating."}
{"id": "2508.05957", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05957", "abs": "https://arxiv.org/abs/2508.05957", "authors": ["Hasibul Karim Shanto", "Umme Ayman Koana", "Shadikur Rahman"], "title": "Multi-Armed Bandits-Based Optimization of Decision Trees", "comment": null, "summary": "Decision trees, without appropriate constraints, can easily become overly\ncomplex and prone to overfit, capturing noise rather than generalizable\npatterns. To resolve this problem,pruning operation is a crucial part in\noptimizing decision trees, as it not only reduces the complexity of trees but\nalso decreases the probability of generating overfit models. The conventional\npruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning\n(REP) are mostly based on greedy approaches that focus on immediate gains in\nperformance while pruning nodes of the decision tree. However, this might\nresult in a lower generalization in the long run, compromising the robust\nability of the tree model when introduced to unseen data samples, particularly\nwhen trained with small and complex datasets. To address this challenge, we are\nproposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement\nlearning (RL)-based technique, that will dynamically prune the tree to generate\nan optimal decision tree with better generalization. Our proposed approach\nassumes the pruning process as an exploration-exploitation problem, where we\nare utilizing the MAB algorithms to find optimal branch nodes to prune based on\nfeedback from each pruning actions. Experimental evaluation on several\nbenchmark datasets, demonstrated that our proposed approach results in better\npredictive performance compared to the traditional ones. This suggests the\npotential of utilizing MAB for a dynamic and probabilistic way of decision tree\npruning, in turn optimizing the decision tree-based model."}
{"id": "2508.06475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06475", "abs": "https://arxiv.org/abs/2508.06475", "authors": ["Guimin Hu", "Daniel Hershcovich", "Hasti Seifi"], "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning", "comment": null, "summary": "Haptic captioning is the task of generating natural language descriptions\nfrom haptic signals, such as vibrations, for use in virtual reality,\naccessibility, and rehabilitation applications. While previous multimodal\nresearch has focused primarily on vision and audio, haptic signals for the\nsense of touch remain underexplored. To address this gap, we formalize the\nhaptic captioning task and propose HapticLLaMA, a multimodal sensory language\nmodel that interprets vibration signals into descriptions in a given sensory,\nemotional, or associative category. We investigate two types of haptic\ntokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that\nconvert haptic signals into sequences of discrete units, enabling their\nintegration with the LLaMA model. HapticLLaMA is trained in two stages: (1)\nsupervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,\nand (2) fine-tuning via reinforcement learning from human feedback (RLHF). We\nassess HapticLLaMA's captioning performance using both automated n-gram metrics\nand human evaluation. HapticLLaMA demonstrates strong capability in\ninterpreting haptic vibration signals, achieving a METEOR score of 59.98 and a\nBLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated\ncaptions received human ratings above 3.5 on a 7-point scale, with RLHF\nyielding a 10% improvement in the overall rating distribution, indicating\nstronger alignment with human haptic perception. These findings highlight the\npotential of large language models to process and adapt to sensory data."}
{"id": "2508.06346", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06346", "abs": "https://arxiv.org/abs/2508.06346", "authors": ["Mert Can Kurucu", "Tufan Kumbasar", "İbrahim Eksin", "Müjde Güzelkaya"], "title": "Introducing Fractional Classification Loss for Robust Learning with Noisy Labels", "comment": "25 pages, 6 figures, 2 table. Submitted to Pattern Recognition", "summary": "Robust loss functions are crucial for training deep neural networks in the\npresence of label noise, yet existing approaches require extensive,\ndataset-specific hyperparameter tuning. In this work, we introduce Fractional\nClassification Loss (FCL), an adaptive robust loss that automatically\ncalibrates its robustness to label noise during training. Built within the\nactive-passive loss framework, FCL employs the fractional derivative of the\nCross-Entropy (CE) loss as its active component and the Mean Absolute Error\n(MAE) as its passive loss component. With this formulation, we demonstrate that\nthe fractional derivative order $\\mu$ spans a family of loss functions that\ninterpolate between MAE-like robustness and CE-like fast convergence.\nFurthermore, we integrate $\\mu$ into the gradient-based optimization as a\nlearnable parameter and automatically adjust it to optimize the trade-off\nbetween robustness and convergence speed. We reveal that FCL's unique property\nestablishes a critical trade-off that enables the stable learning of $\\mu$:\nlower log penalties on difficult or mislabeled examples improve robustness but\nimpose higher penalties on easy or clean data, reducing model confidence in\nthem. Consequently, FCL can dynamically reshape its loss landscape to achieve\neffective classification performance under label noise. Extensive experiments\non benchmark datasets show that FCL achieves state-of-the-art results without\nthe need for manual hyperparameter tuning."}
{"id": "2508.05960", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05960", "abs": "https://arxiv.org/abs/2508.05960", "authors": ["Haohui Chen", "Zhiyong Chen"], "title": "Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) seeks to learn optimal policies from\nstatic datasets without further environment interaction. A key challenge is the\ndistribution shift between the learned and behavior policies, leading to\nout-of-distribution (OOD) actions and overestimation. To prevent gross\noverestimation, the value function must remain conservative; however, excessive\nconservatism may hinder performance improvement. To address this, we propose\nthe mildly conservative regularized evaluation (MCRE) framework, which balances\nconservatism and performance by combining temporal difference (TD) error with a\nbehavior cloning term in the Bellman backup. Building on this, we develop the\nmildly conservative regularized Q-learning (MCRQ) algorithm, which integrates\nMCRE into an off-policy actor-critic framework. Experiments show that MCRQ\noutperforms strong baselines and state-of-the-art offline RL algorithms on\nbenchmark datasets."}
{"id": "2508.06482", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06482", "abs": "https://arxiv.org/abs/2508.06482", "authors": ["Yilun Hua", "Evan Wang", "Yoav Artzi"], "title": "Post-training for Efficient Communication via Convention Formation", "comment": "Accepted to COLM 2025", "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods."}
{"id": "2508.06347", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.06347", "abs": "https://arxiv.org/abs/2508.06347", "authors": ["Ruiyu Zhang", "Ce Zhao", "Xin Zhao", "Lin Nie", "Wai-Fung Lam"], "title": "Structural Equation-VAE: Disentangled Latent Representations for Tabular Data", "comment": "10 pages, 2 figures", "summary": "Learning interpretable latent representations from tabular data remains a\nchallenge in deep generative modeling. We introduce SE-VAE (Structural\nEquation-Variational Autoencoder), a novel architecture that embeds measurement\nstructure directly into the design of a variational autoencoder. Inspired by\nstructural equation modeling, SE-VAE aligns latent subspaces with known\nindicator groupings and introduces a global nuisance latent to isolate\nconstruct-specific confounding variation. This modular architecture enables\ndisentanglement through design rather than through statistical regularizers\nalone. We evaluate SE-VAE on a suite of simulated tabular datasets and\nbenchmark its performance against a series of leading baselines using standard\ndisentanglement metrics. SE-VAE consistently outperforms alternatives in factor\nrecovery, interpretability, and robustness to nuisance variation. Ablation\nresults reveal that architectural structure, rather than regularization\nstrength, is the key driver of performance. SE-VAE offers a principled\nframework for white-box generative modeling in scientific and social domains\nwhere latent constructs are theory-driven and measurement validity is\nessential."}
{"id": "2508.06000", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06000", "abs": "https://arxiv.org/abs/2508.06000", "authors": ["Wei Xiang", "Ziyue Lei", "Haoyuan Che", "Fangyuan Ye", "Xueting Wu", "Lingyun Sun"], "title": "Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning", "comment": "Accepted by IJCAI 2025", "summary": "Operational skill learning, inherently physical and reliant on hands-on\npractice and kinesthetic feedback, has yet to be effectively replicated in\nlarge language model (LLM)-supported training. Current LLM training assistants\nprimarily generate customized textual feedback, neglecting the crucial\nkinesthetic modality. This gap derives from the textual and uncertain nature of\nLLMs, compounded by concerns on user acceptance of LLM driven body control. To\nbridge this gap and realize the potential of collaborative human-LLM action,\nthis work explores human experience of LLM driven kinesthetic assistance.\nSpecifically, we introduced an \"Align-Analyze-Adjust\" strategy and developed\nFlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS)\nfor flight skill acquisition, a representative operational skill domain.\nFlightAxis learns flight skills from manuals and guides forearm movements\nduring simulated flight tasks. Our results demonstrate high user acceptance of\nLLM-mediated body control and significantly reduced task completion times.\nCrucially, trainees reported that this kinesthetic assistance enhanced their\nawareness of operation flaws and fostered increased engagement in the training\nprocess, rather than relieving perceived load. This work demonstrated the\npotential of kinesthetic LLM training in operational skill acquisition."}
{"id": "2508.06353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06353", "abs": "https://arxiv.org/abs/2508.06353", "authors": ["Parichit Sharma", "Marcin Stanislaw", "Hasan Kurban", "Oguzhan Kulekci", "Mehmet Dalkilic"], "title": "Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means", "comment": null, "summary": "This paper introduces Geometric-k-means (or Gk-means for short), a novel\napproach that significantly enhances the efficiency and energy economy of the\nwidely utilized k-means algorithm, which, despite its inception over five\ndecades ago, remains a cornerstone in machine learning applications. The\nessence of Gk-means lies in its active utilization of geometric principles,\nspecifically scalar projection, to significantly accelerate the algorithm\nwithout sacrificing solution quality. This geometric strategy enables a more\ndiscerning focus on data points that are most likely to influence cluster\nupdates, which we call as high expressive data (HE). In contrast, low\nexpressive data (LE), does not impact clustering outcome, is effectively\nbypassed, leading to considerable reductions in computational overhead.\nExperiments spanning synthetic, real-world and high-dimensional datasets,\ndemonstrate Gk-means is significantly better than traditional and state of the\nart (SOTA) k-means variants in runtime and distance computations (DC).\nMoreover, Gk-means exhibits better resource efficiency, as evidenced by its\nreduced energy footprint, placing it as more sustainable alternative."}
{"id": "2508.06016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06016", "abs": "https://arxiv.org/abs/2508.06016", "authors": ["Sagar Gandhi", "Vishal Gandhi"], "title": "Crisp Attention: Regularizing Transformers via Structured Sparsity", "comment": null, "summary": "The quadratic computational cost of the self-attention mechanism is a primary\nchallenge in scaling Transformer models. While attention sparsity is widely\nstudied as a technique to improve computational efficiency, it is almost\nuniversally assumed to come at the cost of model accuracy. In this paper, we\nreport a surprising counter-example to this common wisdom. By introducing\nstructured, post-hoc sparsity to the attention mechanism of a DistilBERT model\nduring fine-tuning on the SST-2 sentiment analysis task, we find that model\naccuracy improves significantly. Our model with 80\\% attention sparsity\nachieves a validation accuracy of 91.59\\%, a 0.97\\% absolute improvement over\nthe dense baseline. We hypothesize that this phenomenon is due to sparsity\nacting as a powerful implicit regularizer, preventing the model from\noverfitting by forcing it to make predictions with a more constrained and\nrobust set of features. Our work recasts attention sparsity not just as a tool\nfor computational efficiency, but as a potential method for improving the\ngeneralization and performance of Transformer models."}
{"id": "2508.05731", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05731", "abs": "https://arxiv.org/abs/2508.05731", "authors": ["Yuhang Liu", "Zeyu Liu", "Shuanghe Zhu", "Pengxiang Li", "Congkai Xie", "Jiasheng Wang", "Xueyu Hu", "Xiaotian Han", "Jianbo Yuan", "Xinyao Wang", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization", "comment": "11 pages, 3 figures", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1."}
{"id": "2508.06361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06361", "abs": "https://arxiv.org/abs/2508.06361", "authors": ["Zhaomin Wu", "Mingzhe Du", "See-Kiong Ng", "Bingsheng He"], "title": "Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts", "comment": null, "summary": "Large Language Models (LLMs) have been widely deployed in reasoning,\nplanning, and decision-making tasks, making their trustworthiness a critical\nconcern. The potential for intentional deception, where an LLM deliberately\nfabricates or conceals information to serve a hidden objective, remains a\nsignificant and underexplored threat. Existing studies typically induce such\ndeception by explicitly setting a \"hidden\" objective through prompting or\nfine-tuning, which may not fully reflect real-world human-LLM interactions.\nMoving beyond this human-induced deception, we investigate LLMs' self-initiated\ndeception on benign prompts. To address the absence of ground truth in this\nevaluation, we propose a novel framework using \"contact searching questions.\"\nThis framework introduces two statistical metrics derived from psychological\nprinciples to quantify the likelihood of deception. The first, the Deceptive\nIntention Score, measures the model's bias towards a hidden objective. The\nsecond, Deceptive Behavior Score, measures the inconsistency between the LLM's\ninternal belief and its expressed output. Upon evaluating 14 leading LLMs, we\nfind that both metrics escalate as task difficulty increases, rising in\nparallel for most models. Building on these findings, we formulate a\nmathematical model to explain this behavior. These results reveal that even the\nmost advanced LLMs exhibit an increasing tendency toward deception when\nhandling complex problems, raising critical concerns for the deployment of LLM\nagents in complex and crucial domains."}
{"id": "2508.06026", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06026", "abs": "https://arxiv.org/abs/2508.06026", "authors": ["Yidong Wang", "Xin Wang", "Cunxiang Wang", "Junfeng Fang", "Qiufeng Wang", "Jianing Chu", "Xuran Meng", "Shuxun Yang", "Libo Qin", "Yue Zhang", "Wei Ye", "Shikun Zhang"], "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future", "comment": "12 pages, 5 figures", "summary": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose \\textbf{Temporal Self-Rewarding\nLanguage Models} that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) \\textit{Anchored Rejection} - fixing rejected responses using the past\ninitial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data."}
{"id": "2508.05913", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05913", "abs": "https://arxiv.org/abs/2508.05913", "authors": ["Stefan Pasch", "Min Chul Cha"], "title": "Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction", "comment": null, "summary": "As AI systems become increasingly embedded in organizational workflows and\nconsumer applications, ethical principles such as fairness, transparency, and\nrobustness have been widely endorsed in policy and industry guidelines.\nHowever, there is still scarce empirical evidence on whether these principles\nare recognized, valued, or impactful from the perspective of users. This study\ninvestigates the link between ethical AI and user satisfaction by analyzing\nover 100,000 user reviews of AI products from G2. Using transformer-based\nlanguage models, we measure sentiment across seven ethical dimensions defined\nby the EU Ethics Guidelines for Trustworthy AI. Our findings show that all\nseven dimensions are positively associated with user satisfaction. Yet, this\nrelationship varies systematically across user and product types. Technical\nusers and reviewers of AI development platforms more frequently discuss\nsystem-level concerns (e.g., transparency, data governance), while\nnon-technical users and reviewers of end-user applications emphasize\nhuman-centric dimensions (e.g., human agency, societal well-being). Moreover,\nthe association between ethical AI and user satisfaction is significantly\nstronger for non-technical users and end-user applications across all\ndimensions. Our results highlight the importance of ethical AI design from\nusers' perspectives and underscore the need to account for contextual\ndifferences across user roles and product types."}
{"id": "2508.06364", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2508.06364", "abs": "https://arxiv.org/abs/2508.06364", "authors": ["Renyi Zhou", "Huimin Zhu", "Jing Tang", "Min Li"], "title": "ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design", "comment": null, "summary": "Achieving precise control over a molecule's biological activity-encompassing\ntargeted activation/inhibition, cooperative multi-target modulation, and\noff-target toxicity mitigation-remains a critical challenge in de novo drug\ndesign. However, existing generative methods primarily focus on producing\nmolecules with a single desired activity, lacking integrated mechanisms for the\nsimultaneous management of multiple intended and unintended molecular\ninteractions. Here, we propose ActivityDiff, a generative approach based on the\nclassifier-guidance technique of diffusion models. It leverages separately\ntrained drug-target classifiers for both positive and negative guidance,\nenabling the model to enhance desired activities while minimizing harmful\noff-target effects. Experimental results show that ActivityDiff effectively\nhandles essential drug design tasks, including single-/dual-target generation,\nfragment-constrained dual-target design, selective generation to enhance target\nspecificity, and reduction of off-target effects. These results demonstrate the\neffectiveness of classifier-guided diffusion in balancing efficacy and safety\nin molecular design. Overall, our work introduces a novel paradigm for\nachieving integrated control over molecular activity, and provides ActivityDiff\nas a versatile and extensible framework."}
{"id": "2508.06034", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06034", "abs": "https://arxiv.org/abs/2508.06034", "authors": ["Qin Chen", "Guojie Song"], "title": "Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity", "comment": "Accepted tp CIKM 2025", "summary": "Heterogeneous graphs (HGs) are common in real-world scenarios and often\nexhibit heterophily. However, most existing studies focus on either\nheterogeneity or heterophily in isolation, overlooking the prevalence of\nheterophilic HGs in practical applications. Such ignorance leads to their\nperformance degradation. In this work, we first identify two main challenges in\nmodeling heterophily HGs: (1) varying heterophily distributions across hops and\nmeta-paths; (2) the intricate and often heterophily-driven diversity of\nsemantic information across different meta-paths. Then, we propose the Adaptive\nHeterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN\nemploys a heterophily-aware convolution that accounts for heterophily\ndistributions specific to both hops and meta-paths. It then integrates messages\nfrom diverse semantic spaces using a coarse-to-fine attention mechanism, which\nfilters out noise and emphasizes informative signals. Experiments on seven\nreal-world graphs and twenty baselines demonstrate the superior performance of\nAHGNN, particularly in high-heterophily situations."}
{"id": "2508.06065", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV", "H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06065", "abs": "https://arxiv.org/abs/2508.06065", "authors": ["Daniel Lee", "Nikhil Sharma", "Donghoon Shin", "DaEun Choi", "Harsh Sharma", "Jeonghwan Kim", "Heng Ji"], "title": "ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation", "comment": null, "summary": "Generative AI has made image creation more accessible, yet aligning outputs\nwith nuanced creative intent remains challenging, particularly for non-experts.\nExisting tools often require users to externalize ideas through prompts or\nreferences, limiting fluid exploration. We introduce ThematicPlane, a system\nthat enables users to navigate and manipulate high-level semantic concepts\n(e.g., mood, style, or narrative tone) within an interactive thematic design\nplane. This interface bridges the gap between tacit creative intent and system\ncontrol. In our exploratory study (N=6), participants engaged in divergent and\nconvergent creative modes, often embracing unexpected results as inspiration or\niteration cues. While they grounded their exploration in familiar themes,\ndiffering expectations of how themes mapped to outputs revealed a need for more\nexplainable controls. Overall, ThematicPlane fosters expressive, iterative\nworkflows and highlights new directions for intuitive, semantics-driven\ninteraction in generative design tools."}
{"id": "2508.06387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06387", "abs": "https://arxiv.org/abs/2508.06387", "authors": ["Anurag Tripathi", "Vaibhav Patle", "Abhinav Jain", "Ayush Pundir", "Sairam Menon", "Ajeet Kumar Singh"], "title": "End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation", "comment": "Accepted in IJCNN25", "summary": "Text-to-SQL bridges the gap between natural language and structured database\nlanguage, thus allowing non-technical users to easily query databases.\nTraditional approaches model text-to-SQL as a direct translation task, where a\ngiven Natural Language Query (NLQ) is mapped to an SQL command. Recent advances\nin large language models (LLMs) have significantly improved translation\naccuracy, however, these methods all require that the target database is\npre-specified. This becomes problematic in scenarios with multiple extensive\ndatabases, where identifying the correct database becomes a crucial yet\noverlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL\nframework to identify the user's intended database before generating SQL\nqueries. Our approach leverages LLMs and prompt engineering to extract implicit\ninformation from natural language queries (NLQs) in the form of a ruleset. We\nthen train a large db\\_id prediction model, which includes a RoBERTa-based\nfinetuned encoder, to predict the correct Database identifier (db\\_id) based on\nboth the NLQ and the LLM-generated rules. Finally, we refine the generated SQL\nby using critic agents to correct errors. Experimental results demonstrate that\nour framework outperforms the current state-of-the-art models in both database\nintent prediction and SQL generation accuracy."}
{"id": "2508.06041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06041", "abs": "https://arxiv.org/abs/2508.06041", "authors": ["Sangwoo Kwon", "Seong Hoon Seo", "Jae W. Lee", "Yeonhong Park"], "title": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment", "comment": null, "summary": "How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding iterations. Building on this\ninsight, we introduce DP-LLM, a novel mechanism that dynamically assigns\nprecision to each layer based on input values. DP-LLM augments each linear\nlayer in an LLM with a precision selector that determines the bitwidth at\nruntime using a lightweight error estimator and threshold values learned\nthrough fine-tuning. Experimental results across multiple models and benchmarks\ndemonstrate that DP-LLM achieves a superior performance-latency trade-off,\noutperforming prior approaches."}
{"id": "2508.06412", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06412", "abs": "https://arxiv.org/abs/2508.06412", "authors": ["Zichuan Liu", "Jinyu Wang", "Lei Song", "Jiang Bian"], "title": "Sample-efficient LLM Optimization with Reset Replay", "comment": null, "summary": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data."}
{"id": "2508.06409", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06409", "abs": "https://arxiv.org/abs/2508.06409", "authors": ["Wooyong Jung", "Sola Kim", "Dongwook Kim", "Maryam Tabar", "Dongwon Lee"], "title": "A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images", "comment": "10 pages, Accepted to SBP-BRiMS 2025", "summary": "Homelessness in the United States has surged to levels unseen since the Great\nDepression. However, existing methods for monitoring it, such as point-in-time\n(PIT) counts, have limitations in terms of frequency, consistency, and spatial\ndetail. This study proposes a new approach using publicly available,\ncrowdsourced data, specifically 311 Service Calls and street-level imagery, to\ntrack and forecast homeless tent trends in San Francisco. Our predictive model\ncaptures fine-grained daily and neighborhood-level variations, uncovering\npatterns that traditional counts often overlook, such as rapid fluctuations\nduring the COVID-19 pandemic and spatial shifts in tent locations over time. By\nproviding more timely, localized, and cost-effective information, this approach\nserves as a valuable tool for guiding policy responses and evaluating\ninterventions aimed at reducing unsheltered homelessness."}
{"id": "2508.06046", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06046", "abs": "https://arxiv.org/abs/2508.06046", "authors": ["Xinda Wang", "Zhengxu Hou", "Yangshijie Zhang", "Bingren Yan", "Zhibo Yang", "Xingsheng Zhang", "Luxi Xing", "Qiang Zhou", "Chen Zhang"], "title": "EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation", "comment": null, "summary": "Although the effectiveness of Large Language Models (LLMs) as judges\n(LLM-as-a-judge) has been validated, their performance remains limited in\nopen-ended tasks, particularly in story evaluation. Accurate story evaluation\nis crucial not only for assisting human quality judgment but also for providing\nkey signals to guide story generation. However, existing methods face a\ndilemma: prompt engineering for closed-source models suffers from poor\nadaptability, while fine-tuning approaches for open-source models lack the\nrigorous reasoning capabilities essential for story evaluation. To address\nthis, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.\nGrounded in pairwise comparison, the framework first self-synthesizes\nscore-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To\nensure data quality, these raw CoTs undergo a self-filtering process, utilizing\nmulti-agents to guarantee their logical rigor and robustness. Finally, the\nevaluator trained on the refined data is deployed as a reward model to guide\nthe story generation task. Experimental results demonstrate that our framework\nachieves state-of-the-art (SOTA) performance on three evaluation benchmarks\nincluding StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward\nmodel, it significantly enhances the quality of generated stories, thereby\nfully validating the superiority of our self-evolving approach."}
{"id": "2508.06412", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06412", "abs": "https://arxiv.org/abs/2508.06412", "authors": ["Zichuan Liu", "Jinyu Wang", "Lei Song", "Jiang Bian"], "title": "Sample-efficient LLM Optimization with Reset Replay", "comment": null, "summary": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data."}
{"id": "2508.06065", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV", "H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06065", "abs": "https://arxiv.org/abs/2508.06065", "authors": ["Daniel Lee", "Nikhil Sharma", "Donghoon Shin", "DaEun Choi", "Harsh Sharma", "Jeonghwan Kim", "Heng Ji"], "title": "ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation", "comment": null, "summary": "Generative AI has made image creation more accessible, yet aligning outputs\nwith nuanced creative intent remains challenging, particularly for non-experts.\nExisting tools often require users to externalize ideas through prompts or\nreferences, limiting fluid exploration. We introduce ThematicPlane, a system\nthat enables users to navigate and manipulate high-level semantic concepts\n(e.g., mood, style, or narrative tone) within an interactive thematic design\nplane. This interface bridges the gap between tacit creative intent and system\ncontrol. In our exploratory study (N=6), participants engaged in divergent and\nconvergent creative modes, often embracing unexpected results as inspiration or\niteration cues. While they grounded their exploration in familiar themes,\ndiffering expectations of how themes mapped to outputs revealed a need for more\nexplainable controls. Overall, ThematicPlane fosters expressive, iterative\nworkflows and highlights new directions for intuitive, semantics-driven\ninteraction in generative design tools."}
{"id": "2508.06467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06467", "abs": "https://arxiv.org/abs/2508.06467", "authors": ["Ameya Anjarlekar", "Sandeep Pombra"], "title": "LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection", "comment": "14 Pages, 3 Figures, 11 Tables", "summary": "The growing legal and ethical scrutiny of large language models (LLMs)\nnecessitates effective machine unlearning, particularly for sensitive or\nunauthorized data. Existing empirical methods often yield incomplete forgetting\nor unintended degradation of unrelated knowledge due to poor localization. In\nthis work, we propose GRIN: a modular and targeted framework for LLM\nunlearning. GRIN introduces a novel gradient-ratio-based metric to identify\nparameters most responsible for memorizing forget data. We then perform\nselective noise injection into these parameters prior to fine-tuning, which\nimproves unlearning performance while maintaining model utility. Finally, we\npropose new evaluation metrics tailored to the LLM setting and validate our\napproach on standard benchmarks such as TOFU, WMDP, and SafePKU."}
{"id": "2508.06066", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06066", "abs": "https://arxiv.org/abs/2508.06066", "authors": ["Barak Gahtan", "Alex M. Bronstein"], "title": "Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology", "comment": null, "summary": "Deep temporal architectures such as Temporal Convolutional Networks (TCNs)\nachieve strong predictive performance on sequential data, yet theoretical\nunderstanding of their generalization remains limited. We address this gap by\nproviding both the first non-vacuous, architecture-aware generalization bounds\nfor deep temporal models and a principled evaluation methodology.\n  For exponentially $\\beta$-mixing sequences, we derive bounds scaling as $\nO\\!\\Bigl(R\\,\\sqrt{\\tfrac{D\\,p\\,n\\,\\log N}{N}}\\Bigr), $ where $D$ is network\ndepth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our\ndelayed-feedback blocking mechanism transforms dependent samples into\neffectively independent ones while discarding only $O(1/\\log N)$ of the data,\nyielding $\\sqrt{D}$ scaling instead of exponential, implying that doubling\ndepth requires approximately quadrupling the training data.\n  We also introduce a fair-comparison methodology that fixes the effective\nsample size to isolate the effect of temporal structure from information\ncontent. Under $N_{\\text{eff}}=2{,}000$, strongly dependent sequences\n($\\rho=0.8$) exhibit $\\approx76\\%$ smaller generalization gaps than weakly\ndependent ones ($\\rho=0.2$), challenging the intuition that dependence is\npurely detrimental. Yet convergence rates diverge from theory: weak\ndependencies follow $N_{\\text{eff}}^{-1.21}$ scaling and strong dependencies\nfollow $N_{\\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.\nThese findings reveal that temporal dependence can enhance learning under fixed\ninformation budgets, while highlighting gaps between theory and practice that\nmotivate future research."}
{"id": "2107.06056", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2107.06056", "abs": "https://arxiv.org/abs/2107.06056", "authors": ["Prathamesh Kalamkar", "Janani Venugopalan Ph. D.", "Vivek Raghavan Ph. D"], "title": "Indian Legal NLP Benchmarks : A Survey", "comment": null, "summary": "Availability of challenging benchmarks is the key to advancement of AI in a\nspecific field.Since Legal Text is significantly different than normal English\ntext, there is a need to create separate Natural Language Processing benchmarks\nfor Indian Legal Text which are challenging and focus on tasks specific to\nLegal Systems. This will spur innovation in applications of Natural language\nProcessing for Indian Legal Text and will benefit AI community and Legal\nfraternity. We review the existing work in this area and propose ideas to\ncreate new benchmarks for Indian Legal Natural Language Processing."}
{"id": "2508.06108", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06108", "abs": "https://arxiv.org/abs/2508.06108", "authors": ["Xing Lei", "Wenyan Yang", "Kaiqiang Ke", "Shentao Yang", "Xuetao Zhang", "Joni Pajarinen", "Donglin Wang"], "title": "GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning", "comment": null, "summary": "Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a\nfundamental challenge in reinforcement learning. While hindsight experience\nreplay (HER) has shown promise by relabeling collected trajectories with\nachieved goals, we argue that trajectory relabeling alone does not fully\nexploit the available experiences in off-policy GCRL methods, resulting in\nlimited sample efficiency. In this paper, we propose Hindsight Goal-conditioned\nRegularization (HGR), a technique that generates action regularization priors\nbased on hindsight goals. When combined with hindsight self-imitation\nregularization (HSR), our approach enables off-policy RL algorithms to maximize\nexperience utilization. Compared to existing GCRL methods that employ HER and\nself-imitation techniques, our hindsight regularizations achieve substantially\nmore efficient sample reuse and the best performances, which we empirically\ndemonstrate on a suite of navigation and manipulation tasks."}
{"id": "2508.05766", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.05766", "abs": "https://arxiv.org/abs/2508.05766", "authors": ["Bo Wen"], "title": "A Framework for Inherently Safer AGI through Language-Mediated Active Inference", "comment": null, "summary": "This paper proposes a novel framework for developing safe Artificial General\nIntelligence (AGI) by combining Active Inference principles with Large Language\nModels (LLMs). We argue that traditional approaches to AI safety, focused on\npost-hoc interpretability and reward engineering, have fundamental limitations.\nWe present an architecture where safety guarantees are integrated into the\nsystem's core design through transparent belief representations and\nhierarchical value alignment. Our framework leverages natural language as a\nmedium for representing and manipulating beliefs, enabling direct human\noversight while maintaining computational tractability. The architecture\nimplements a multi-agent system where agents self-organize according to Active\nInference principles, with preferences and safety constraints flowing through\nhierarchical Markov blankets. We outline specific mechanisms for ensuring\nsafety, including: (1) explicit separation of beliefs and preferences in\nnatural language, (2) bounded rationality through resource-aware free energy\nminimization, and (3) compositional safety through modular agent structures.\nThe paper concludes with a research agenda centered on the Abstraction and\nReasoning Corpus (ARC) benchmark, proposing experiments to validate our\nframework's safety properties. Our approach offers a path toward AGI\ndevelopment that is inherently safer, rather than retrofitted with safety\nmeasures."}
{"id": "2508.06135", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06135", "abs": "https://arxiv.org/abs/2508.06135", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "title": "Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models", "comment": null, "summary": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs."}
{"id": "2508.05934", "categories": ["cs.HC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05934", "abs": "https://arxiv.org/abs/2508.05934", "authors": ["Xueyuan Xu", "Tianze Yu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "title": "ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection", "comment": null, "summary": "Recently, multi-modal physiological signals based emotion recognition has\ngarnered increasing attention in the field of brain-computer interfaces.\nNevertheness, the associated multi-modal physiological features are often\nhigh-dimensional and inevitably include irrelevant, redundant, and noisy\nrepresentation, which can easily lead to overfitting, poor performance, and\nhigh computational complexity in emotion classifiers. Feature selection has\nbeen widely applied to address these challenges. However, previous studies\ngenerally assumed that multi-modal physiological data are complete, whereas in\nreality, the data are often incomplete due to the openness of the acquisition\nand operational environment. For example, a part of samples are available in\nseveral modalities but not in others. To address this issue, we propose a novel\nmethod for incomplete multi-modal physiological signal feature selection called\nadaptive shared latent structure learning (ASLSL). Based on the property that\nsimilar features share similar emotional labels, ASLSL employs adaptive shared\nlatent structure learning to explore a common latent space shared for\nincomplete multi-modal physiological signals and multi-dimensional emotional\nlabels, thereby mitigating the impact of missing information and mining\nconsensus information. Two most popular multi-modal physiological emotion\ndatasets (DEAP and DREAMER) with multi-dimensional emotional labels were\nutilized to compare the performance between compare ASLSL and seventeen feature\nselection methods. Comprehensive experimental results on these datasets\ndemonstrate the effectiveness of ASLSL."}
{"id": "2508.06163", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06163", "abs": "https://arxiv.org/abs/2508.06163", "authors": ["Yingfeng Luo", "Dingyang Lin", "Junxin Wang", "Ziqiang Xu", "Kaiyan Chang", "Tong Zheng", "Bei Li", "Anxiang Ma", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging", "comment": "Under review", "summary": "Model merging has emerged as a compelling data-free paradigm for multi-task\nlearning, enabling the fusion of multiple fine-tuned models into a single,\npowerful entity. A key technique in merging methods is sparsification, which\nprunes redundant parameters from task vectors to mitigate interference.\nHowever, prevailing approaches employ a ``one-size-fits-all'' strategy,\napplying a uniform sparsity ratio that overlooks the inherent structural and\nstatistical heterogeneity of model parameters. This often leads to a suboptimal\ntrade-off, where critical parameters are inadvertently pruned while less useful\nones are retained. To address this limitation, we introduce \\textbf{TADrop}\n(\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive\nsparsification strategy that respects this heterogeneity. Instead of a global\nratio, TADrop assigns a tailored sparsity level to each parameter tensor based\non its distributional properties. The core intuition is that tensors with\ndenser, more redundant distributions can be pruned aggressively, while sparser,\nmore critical ones are preserved. As a simple and plug-and-play module, we\nvalidate TADrop by integrating it with foundational, classic, and SOTA merging\nmethods. Extensive experiments across diverse tasks (vision, language, and\nmultimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and\nsignificantly boosts their performance. For instance, when enhancing a leading\nmerging method, it achieves an average performance gain of 2.0\\% across 8\nViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter\ninterference by tailoring sparsification to the model's structure, offering a\nnew baseline for high-performance model merging."}
{"id": "2508.06030", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06030", "abs": "https://arxiv.org/abs/2508.06030", "authors": ["Kartik Sharma", "Yiqiao Jin", "Rakshit Trivedi", "Srijan Kumar"], "title": "Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings", "comment": null, "summary": "Large language models (LLMs) acquire knowledge across diverse domains such as\nscience, history, and geography encountered during generative pre-training.\nHowever, due to their stochasticity, it is difficult to predict what LLMs have\nacquired. Prior work has developed different ways to probe this knowledge by\ninvestigating the hidden representations, crafting specific task prompts,\ncurating representative samples, and estimating their uncertainty. However,\nthese methods require making forward passes through the underlying model to\nprobe the LLM's knowledge about a specific fact, making them computationally\nexpensive and time-consuming. To bridge this gap, we propose $\\textbf{PEEK}$ or\n$\\textbf{P}$roxy $\\textbf{E}$mbeddings to $\\textbf{E}$stimate\n$\\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models\nthat effectively encode factual knowledge as text or graphs as proxies for\nLLMs. First, we identify a training set of facts known by LLMs through various\nprobing strategies and then adapt embedding models to predict the LLM outputs\nwith a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived\ndatasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict\nLLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find\nthat sentence embedding models are more suitable than graph embeddings to\npredict LLM knowledge, shedding light on the underlying representation of the\nfactual landscape. Thus, we believe that knowledge-adapted embeddings can be\nused to identify knowledge gaps in LLMs at scale and can provide deeper\ninsights into LLMs' internal inductive bias. The code and data are made\navailable at https://github.com/claws-lab/peek."}
{"id": "2508.06165", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06165", "abs": "https://arxiv.org/abs/2508.06165", "authors": ["Weitao Li", "Boran Xiang", "Xiaolong Wang", "Zhinan Gou", "Weizhi Ma", "Yang Liu"], "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2."}
{"id": "2508.06062", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 68T30"], "pdf": "https://arxiv.org/pdf/2508.06062", "abs": "https://arxiv.org/abs/2508.06062", "authors": ["Evgenii E. Vityaev", "Andrei Mantsivoda"], "title": "Don't Forget Imagination!", "comment": "14 pages, 2 figures", "summary": "Cognitive imagination is a type of imagination that plays a key role in human\nthinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to\nmentally visualize coherent and holistic systems of concepts and causal links\nthat serve as semantic contexts for reasoning, decision making and prediction.\nOur position is that the role of cognitive imagination is still greatly\nunderestimated, and this creates numerous problems and diminishes the current\ncapabilities of AI. For instance, when reasoning, humans rely on imaginary\ncontexts to retrieve background info. They also constantly return to the\ncontext for semantic verification that their reasoning is still reasonable.\nThus, reasoning without imagination is blind. This paper is a call for greater\nattention to cognitive imagination as the next promising breakthrough in\nartificial intelligence. As an instrument for simulating cognitive imagination,\nwe propose semantic models -- a new approach to mathematical models that can\nlearn, like neural networks, and are based on probabilistic causal\nrelationships. Semantic models can simulate cognitive imagination because they\nensure the consistency of imaginary contexts and implement a glass-box approach\nthat allows the context to be manipulated as a holistic and coherent system of\ninterrelated facts glued together with causal relations."}
{"id": "2508.06183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06183", "abs": "https://arxiv.org/abs/2508.06183", "authors": ["Xiyuan Yang", "Shengyuan Hu", "Soyeon Kim", "Tian Li"], "title": "Differentially Private Federated Clustering with Random Rebalancing", "comment": "21 pages", "summary": "Federated clustering aims to group similar clients into clusters and produce\none model for each cluster. Such a personalization approach typically improves\nmodel performance compared with training a single model to serve all clients,\nbut can be more vulnerable to privacy leakage. Directly applying client-level\ndifferentially private (DP) mechanisms to federated clustering could degrade\nthe utilities significantly. We identify that such deficiencies are mainly due\nto the difficulties of averaging privacy noise within each cluster (following\nstandard privacy mechanisms), as the number of clients assigned to the same\nclusters is uncontrolled. To this end, we propose a simple and effective\ntechnique, named RR-Cluster, that can be viewed as a light-weight add-on to\nmany federated clustering algorithms. RR-Cluster achieves reduced privacy noise\nvia randomly rebalancing cluster assignments, guaranteeing a minimum number of\nclients assigned to each cluster. We analyze the tradeoffs between decreased\nprivacy noise variance and potentially increased bias from incorrect\nassignments and provide convergence bounds for RR-Clsuter. Empirically, we\ndemonstrate the RR-Cluster plugged into strong federated clustering algorithms\nresults in significantly improved privacy/utility tradeoffs across both\nsynthetic and real-world datasets."}
{"id": "2508.06163", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06163", "abs": "https://arxiv.org/abs/2508.06163", "authors": ["Yingfeng Luo", "Dingyang Lin", "Junxin Wang", "Ziqiang Xu", "Kaiyan Chang", "Tong Zheng", "Bei Li", "Anxiang Ma", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging", "comment": "Under review", "summary": "Model merging has emerged as a compelling data-free paradigm for multi-task\nlearning, enabling the fusion of multiple fine-tuned models into a single,\npowerful entity. A key technique in merging methods is sparsification, which\nprunes redundant parameters from task vectors to mitigate interference.\nHowever, prevailing approaches employ a ``one-size-fits-all'' strategy,\napplying a uniform sparsity ratio that overlooks the inherent structural and\nstatistical heterogeneity of model parameters. This often leads to a suboptimal\ntrade-off, where critical parameters are inadvertently pruned while less useful\nones are retained. To address this limitation, we introduce \\textbf{TADrop}\n(\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive\nsparsification strategy that respects this heterogeneity. Instead of a global\nratio, TADrop assigns a tailored sparsity level to each parameter tensor based\non its distributional properties. The core intuition is that tensors with\ndenser, more redundant distributions can be pruned aggressively, while sparser,\nmore critical ones are preserved. As a simple and plug-and-play module, we\nvalidate TADrop by integrating it with foundational, classic, and SOTA merging\nmethods. Extensive experiments across diverse tasks (vision, language, and\nmultimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and\nsignificantly boosts their performance. For instance, when enhancing a leading\nmerging method, it achieves an average performance gain of 2.0\\% across 8\nViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter\ninterference by tailoring sparsification to the model's structure, offering a\nnew baseline for high-performance model merging."}
{"id": "2508.06199", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06199", "abs": "https://arxiv.org/abs/2508.06199", "authors": ["Mateusz Praski", "Jakub Adamczyk", "Wojciech Czech"], "title": "Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning", "comment": null, "summary": "Pretrained neural networks have attracted significant interest in chemistry\nand small molecule drug design. Embeddings from these models are widely used\nfor molecular property prediction, virtual screening, and small data learning\nin molecular chemistry. This study presents the most extensive comparison of\nsuch models to date, evaluating 25 models across 25 datasets. Under a fair\ncomparison framework, we assess models spanning various modalities,\narchitectures, and pretraining strategies. Using a dedicated hierarchical\nBayesian statistical testing model, we arrive at a surprising result: nearly\nall neural models show negligible or no improvement over the baseline ECFP\nmolecular fingerprint. Only the CLAMP model, which is also based on molecular\nfingerprints, performs statistically significantly better than the\nalternatives. These findings raise concerns about the evaluation rigor in\nexisting studies. We discuss potential causes, propose solutions, and offer\npractical recommendations."}
{"id": "2508.06204", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06204", "abs": "https://arxiv.org/abs/2508.06204", "authors": ["Richard Willats", "Josh Pennington", "Aravind Mohan", "Bertie Vidgen"], "title": "Classification is a RAG problem: A case study on hate speech detection", "comment": null, "summary": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems."}
{"id": "2508.06204", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06204", "abs": "https://arxiv.org/abs/2508.06204", "authors": ["Richard Willats", "Josh Pennington", "Aravind Mohan", "Bertie Vidgen"], "title": "Classification is a RAG problem: A case study on hate speech detection", "comment": null, "summary": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems."}
{"id": "2508.06263", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06263", "abs": "https://arxiv.org/abs/2508.06263", "authors": ["Andrew Cropper", "David M. Cerna", "Matti Järvisalo"], "title": "Symmetry breaking for inductive logic programming", "comment": null, "summary": "The goal of inductive logic programming is to search for a hypothesis that\ngeneralises training data and background knowledge. The challenge is searching\nvast hypothesis spaces, which is exacerbated because many logically equivalent\nhypotheses exist. To address this challenge, we introduce a method to break\nsymmetries in the hypothesis space. We implement our idea in answer set\nprogramming. Our experiments on multiple domains, including visual reasoning\nand game playing, show that our approach can reduce solving times from over an\nhour to just 17 seconds."}
{"id": "2508.06208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06208", "abs": "https://arxiv.org/abs/2508.06208", "authors": ["Ce Na", "Kai Yang", "Dengzhao Fang", "Yu Li", "Jingtong Gao", "Chengcheng Zhu", "Jiale Zhang", "Xiaobing Sun", "Yi Chang"], "title": "Graph Federated Learning for Personalized Privacy Recommendation", "comment": null, "summary": "Federated recommendation systems (FedRecs) have gained significant attention\nfor providing privacy-preserving recommendation services. However, existing\nFedRecs assume that all users have the same requirements for privacy\nprotection, i.e., they do not upload any data to the server. The approaches\noverlook the potential to enhance the recommendation service by utilizing\npublicly available user data. In real-world applications, users can choose to\nbe private or public. Private users' interaction data is not shared, while\npublic users' interaction data can be shared. Inspired by the issue, this paper\nproposes a novel Graph Federated Learning for Personalized Privacy\nRecommendation (GFed-PP) that adapts to different privacy requirements while\nimproving recommendation performance. GFed-PP incorporates the interaction data\nof public users to build a user-item interaction graph, which is then used to\nform a user relationship graph. A lightweight graph convolutional network (GCN)\nis employed to learn each user's user-specific personalized item embedding. To\nprotect user privacy, each client learns the user embedding and the scoring\nfunction locally. Additionally, GFed-PP achieves optimization of the federated\nrecommendation framework through the initialization of item embedding on\nclients and the aggregation of the user relationship graph on the server.\nExperimental results demonstrate that GFed-PP significantly outperforms\nexisting methods for five datasets, offering superior recommendation accuracy\nwithout compromising privacy. This framework provides a practical solution for\naccommodating varying privacy preferences in federated recommendation systems."}
{"id": "2508.06277", "categories": ["cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.06277", "abs": "https://arxiv.org/abs/2508.06277", "authors": ["Theresa Pekarek Rosin", "Burak Can Kaplan", "Stefan Wermter"], "title": "Large Language Model Data Generation for Enhanced Intent Recognition in German Speech", "comment": "11 pages, 3 figures, accepted at KONVENS 2025", "summary": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility."}
{"id": "2508.06214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06214", "abs": "https://arxiv.org/abs/2508.06214", "authors": ["Hai Zhong", "Xun Wang", "Zhuoran Li", "Longbo Huang"], "title": "Reparameterization Proximal Policy Optimization", "comment": null, "summary": "Reparameterization policy gradient (RPG) is promising for improving sample\nefficiency by leveraging differentiable dynamics. However, a critical barrier\nis its training instability, where high-variance gradients can destabilize the\nlearning process. To address this, we draw inspiration from Proximal Policy\nOptimization (PPO), which uses a surrogate objective to enable stable sample\nreuse in the model-free setting. We first establish a connection between this\nsurrogate objective and RPG, which has been largely unexplored and is\nnon-trivial. Then, we bridge this gap by demonstrating that the\nreparameterization gradient of a PPO-like surrogate objective can be computed\nefficiently using backpropagation through time. Based on this key insight, we\npropose Reparameterization Proximal Policy Optimization (RPO), a stable and\nsample-efficient RPG-based method. RPO enables multiple epochs of stable sample\nreuse by optimizing a clipped surrogate objective tailored for RPG, while being\nfurther stabilized by Kullback-Leibler (KL) divergence regularization and\nremaining fully compatible with existing variance reduction methods. We\nevaluate RPO on a suite of challenging locomotion and manipulation tasks, where\nexperiments demonstrate that our method achieves superior sample efficiency and\nstrong performance."}
{"id": "2508.06296", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06296", "abs": "https://arxiv.org/abs/2508.06296", "authors": ["Pierre Peigné - Lefebvre", "Quentin Feuillade-Montixi", "Tom David", "Nicolas Miailhe"], "title": "LLM Robustness Leaderboard v1 --Technical report", "comment": null, "summary": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community."}
{"id": "2508.06220", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06220", "abs": "https://arxiv.org/abs/2508.06220", "authors": ["Keummin Ka", "Junhyeong Park", "Jahyun Jeon", "Youngjae Yu"], "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?", "comment": "14 pages, 9 figures", "summary": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems."}
{"id": "2508.06345", "categories": ["cs.CL", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06345", "abs": "https://arxiv.org/abs/2508.06345", "authors": ["Yanbin Wei", "Jiangyue Yan", "Chun Kang", "Yang Chen", "Hua Liu", "James T. Kwok", "Yu Zhang"], "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering", "comment": null, "summary": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy"}
{"id": "2508.06244", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06244", "abs": "https://arxiv.org/abs/2508.06244", "authors": ["Xurun Wang", "Guangrui Liu", "Xinjie Li", "Haoyu He", "Lin Yao", "Weizhe Zhang"], "title": "Membership Inference Attack with Partial Features", "comment": null, "summary": "Machine learning models have been shown to be susceptible to membership\ninference attack, which can be used to determine whether a given sample appears\nin the training data. Existing membership inference methods commonly assume\nthat the adversary has full access to the features of the target sample. This\nassumption, however, does not hold in many real-world scenarios where only\npartial features information is available, thereby limiting the applicability\nof these methods. In this work, we study an inference scenario where the\nadversary observes only partial features of each sample and aims to infer\nwhether this observed subset was present in the training set of the target\nmodel. We define this problem as Partial Feature Membership Inference (PFMI).\nTo address this problem, we propose MRAD (Memory-guided Reconstruction and\nAnomaly Detection), a two-stage attack framework. In the first stage, MRAD\noptimizes the unknown feature values to minimize the loss of the sample. In the\nsecond stage, it measures the deviation between the reconstructed sample and\nthe training distribution using anomaly detection. Empirical results\ndemonstrate that MRAD is effective across a range of datasets, and maintains\ncompatibility with various off-the-shelf anomaly detection techniques. For\nexample, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of\nthe missing features."}
{"id": "2508.06433", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06433", "abs": "https://arxiv.org/abs/2508.06433", "authors": ["Runnan Fang", "Yuan Liang", "Xiaobin Wang", "Jialong Wu", "Shuofei Qiao", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "title": "Memp: Exploring Agent Procedural Memory", "comment": "Work in progress", "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains."}
{"id": "2508.06249", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06249", "abs": "https://arxiv.org/abs/2508.06249", "authors": ["David Kaczér", "Magnus Jørgenvåg", "Clemens Vetter", "Lucie Flek", "Florian Mai"], "title": "In-Training Defenses against Emergent Misalignment in Language Models", "comment": "Under review", "summary": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research."}
{"id": "2508.06482", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06482", "abs": "https://arxiv.org/abs/2508.06482", "authors": ["Yilun Hua", "Evan Wang", "Yoav Artzi"], "title": "Post-training for Efficient Communication via Convention Formation", "comment": "Accepted to COLM 2025", "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods."}
{"id": "2508.06251", "categories": ["cs.LG", "cs.AI", "cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06251", "abs": "https://arxiv.org/abs/2508.06251", "authors": ["Alejandro Moreno R.", "Desale Fentaw", "Samuel Palmer", "Raúl Salles de Padua", "Ninad Dixit", "Samuel Mugel", "Roman Orús", "Manuel Radons", "Josef Menter", "Ali Abedi"], "title": "Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)", "comment": "10 pages", "summary": "Synthetic data generation is a key technique in modern artificial\nintelligence, addressing data scarcity, privacy constraints, and the need for\ndiverse datasets in training robust models. In this work, we propose a method\nfor generating privacy-preserving high-quality synthetic tabular data using\nTensor Networks, specifically Matrix Product States (MPS). We benchmark the\nMPS-based generative model against state-of-the-art models such as CTGAN, VAE,\nand PrivBayes, focusing on both fidelity and privacy-preserving capabilities.\nTo ensure differential privacy (DP), we integrate noise injection and gradient\nclipping during training, enabling privacy guarantees via R\\'enyi Differential\nPrivacy accounting. Across multiple metrics analyzing data fidelity and\ndownstream machine learning task performance, our results show that MPS\noutperforms classical models, particularly under strict privacy constraints.\nThis work highlights MPS as a promising tool for privacy-aware synthetic data\ngeneration. By combining the expressive power of tensor network representations\nwith formal privacy mechanisms, the proposed approach offers an interpretable\nand scalable alternative for secure data sharing. Its structured design\nfacilitates integration into sensitive domains where both data quality and\nconfidentiality are critical."}
{"id": "2508.06269", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06269", "abs": "https://arxiv.org/abs/2508.06269", "authors": ["Zhuoran Li", "Xun Wang", "Hai Zhong", "Longbo Huang"], "title": "OM2P: Offline Multi-Agent Mean-Flow Policy", "comment": null, "summary": "Generative models, especially diffusion and flow-based models, have been\npromising in offline multi-agent reinforcement learning. However, integrating\npowerful generative models into this framework poses unique challenges. In\nparticular, diffusion and flow-based policies suffer from low sampling\nefficiency due to their iterative generation processes, making them impractical\nin time-sensitive or resource-constrained settings. To tackle these\ndifficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel\noffline MARL algorithm to achieve efficient one-step action sampling. To\naddress the misalignment between generative objectives and reward maximization,\nwe introduce a reward-aware optimization scheme that integrates a\ncarefully-designed mean-flow matching loss with Q-function supervision.\nAdditionally, we design a generalized timestep distribution and a\nderivative-free estimation strategy to reduce memory overhead and improve\ntraining stability. Empirical evaluations on Multi-Agent Particle and MuJoCo\nbenchmarks demonstrate that OM2P achieves superior performance, with up to a\n3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.\nOur approach represents the first to successfully integrate mean-flow model\ninto offline MARL, paving the way for practical and scalable generative\npolicies in cooperative multi-agent settings."}
{"id": "2508.06301", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.06301", "abs": "https://arxiv.org/abs/2508.06301", "authors": ["Junhyeog Yun", "Minui Hong", "Gunhee Kim"], "title": "FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields", "comment": "ICCV 2025", "summary": "Neural fields provide a memory-efficient representation of data, which can\neffectively handle diverse modalities and large-scale data. However, learning\nto map neural fields often requires large amounts of training data and\ncomputations, which can be limited to resource-constrained edge devices. One\napproach to tackle this limitation is to leverage Federated Meta-Learning\n(FML), but traditional FML approaches suffer from privacy leakage. To address\nthese issues, we introduce a novel FML approach called FedMeNF. FedMeNF\nutilizes a new privacy-preserving loss function that regulates privacy leakage\nin the local meta-optimization. This enables the local meta-learner to optimize\nquickly and efficiently without retaining the client's private data. Our\nexperiments demonstrate that FedMeNF achieves fast optimization speed and\nrobust reconstruction performance, even with few-shot or non-IID data across\ndiverse data modalities, while preserving client data privacy."}
{"id": "2508.06336", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06336", "abs": "https://arxiv.org/abs/2508.06336", "authors": ["Constantin Ruhdorfer", "Matteo Bortoletto", "Victor Oei", "Anna Penzkofer", "Andreas Bulling"], "title": "Unsupervised Partner Design Enables Robust Ad-hoc Teamwork", "comment": "16 pages", "summary": "We introduce Unsupervised Partner Design (UPD) - a population-free,\nmulti-agent reinforcement learning framework for robust ad-hoc teamwork that\nadaptively generates training partners without requiring pretrained partners or\nmanual parameter tuning. UPD constructs diverse partners by stochastically\nmixing an ego agent's policy with biased random behaviours and scores them\nusing a variance-based learnability metric that prioritises partners near the\nego agent's current learning frontier. We show that UPD can be integrated with\nunsupervised environment design, resulting in the first method enabling fully\nunsupervised curricula over both level and partner distributions in a\ncooperative setting. Through extensive evaluations on Overcooked-AI and the\nOvercooked Generalisation Challenge, we demonstrate that this dynamic partner\ncurriculum is highly effective: UPD consistently outperforms both\npopulation-based and population-free baselines as well as ablations. In a user\nstudy, we further show that UPD achieves higher returns than all baselines and\nwas perceived as significantly more adaptive, more human-like, a better\ncollaborator, and less frustrating."}
{"id": "2508.06345", "categories": ["cs.CL", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06345", "abs": "https://arxiv.org/abs/2508.06345", "authors": ["Yanbin Wei", "Jiangyue Yan", "Chun Kang", "Yang Chen", "Hua Liu", "James T. Kwok", "Yu Zhang"], "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering", "comment": null, "summary": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy"}
{"id": "2508.06347", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.06347", "abs": "https://arxiv.org/abs/2508.06347", "authors": ["Ruiyu Zhang", "Ce Zhao", "Xin Zhao", "Lin Nie", "Wai-Fung Lam"], "title": "Structural Equation-VAE: Disentangled Latent Representations for Tabular Data", "comment": "10 pages, 2 figures", "summary": "Learning interpretable latent representations from tabular data remains a\nchallenge in deep generative modeling. We introduce SE-VAE (Structural\nEquation-Variational Autoencoder), a novel architecture that embeds measurement\nstructure directly into the design of a variational autoencoder. Inspired by\nstructural equation modeling, SE-VAE aligns latent subspaces with known\nindicator groupings and introduces a global nuisance latent to isolate\nconstruct-specific confounding variation. This modular architecture enables\ndisentanglement through design rather than through statistical regularizers\nalone. We evaluate SE-VAE on a suite of simulated tabular datasets and\nbenchmark its performance against a series of leading baselines using standard\ndisentanglement metrics. SE-VAE consistently outperforms alternatives in factor\nrecovery, interpretability, and robustness to nuisance variation. Ablation\nresults reveal that architectural structure, rather than regularization\nstrength, is the key driver of performance. SE-VAE offers a principled\nframework for white-box generative modeling in scientific and social domains\nwhere latent constructs are theory-driven and measurement validity is\nessential."}
{"id": "2508.06361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06361", "abs": "https://arxiv.org/abs/2508.06361", "authors": ["Zhaomin Wu", "Mingzhe Du", "See-Kiong Ng", "Bingsheng He"], "title": "Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts", "comment": null, "summary": "Large Language Models (LLMs) have been widely deployed in reasoning,\nplanning, and decision-making tasks, making their trustworthiness a critical\nconcern. The potential for intentional deception, where an LLM deliberately\nfabricates or conceals information to serve a hidden objective, remains a\nsignificant and underexplored threat. Existing studies typically induce such\ndeception by explicitly setting a \"hidden\" objective through prompting or\nfine-tuning, which may not fully reflect real-world human-LLM interactions.\nMoving beyond this human-induced deception, we investigate LLMs' self-initiated\ndeception on benign prompts. To address the absence of ground truth in this\nevaluation, we propose a novel framework using \"contact searching questions.\"\nThis framework introduces two statistical metrics derived from psychological\nprinciples to quantify the likelihood of deception. The first, the Deceptive\nIntention Score, measures the model's bias towards a hidden objective. The\nsecond, Deceptive Behavior Score, measures the inconsistency between the LLM's\ninternal belief and its expressed output. Upon evaluating 14 leading LLMs, we\nfind that both metrics escalate as task difficulty increases, rising in\nparallel for most models. Building on these findings, we formulate a\nmathematical model to explain this behavior. These results reveal that even the\nmost advanced LLMs exhibit an increasing tendency toward deception when\nhandling complex problems, raising critical concerns for the deployment of LLM\nagents in complex and crucial domains."}
{"id": "2508.06364", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2508.06364", "abs": "https://arxiv.org/abs/2508.06364", "authors": ["Renyi Zhou", "Huimin Zhu", "Jing Tang", "Min Li"], "title": "ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design", "comment": null, "summary": "Achieving precise control over a molecule's biological activity-encompassing\ntargeted activation/inhibition, cooperative multi-target modulation, and\noff-target toxicity mitigation-remains a critical challenge in de novo drug\ndesign. However, existing generative methods primarily focus on producing\nmolecules with a single desired activity, lacking integrated mechanisms for the\nsimultaneous management of multiple intended and unintended molecular\ninteractions. Here, we propose ActivityDiff, a generative approach based on the\nclassifier-guidance technique of diffusion models. It leverages separately\ntrained drug-target classifiers for both positive and negative guidance,\nenabling the model to enhance desired activities while minimizing harmful\noff-target effects. Experimental results show that ActivityDiff effectively\nhandles essential drug design tasks, including single-/dual-target generation,\nfragment-constrained dual-target design, selective generation to enhance target\nspecificity, and reduction of off-target effects. These results demonstrate the\neffectiveness of classifier-guided diffusion in balancing efficacy and safety\nin molecular design. Overall, our work introduces a novel paradigm for\nachieving integrated control over molecular activity, and provides ActivityDiff\nas a versatile and extensible framework."}
{"id": "2508.06387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06387", "abs": "https://arxiv.org/abs/2508.06387", "authors": ["Anurag Tripathi", "Vaibhav Patle", "Abhinav Jain", "Ayush Pundir", "Sairam Menon", "Ajeet Kumar Singh"], "title": "End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation", "comment": "Accepted in IJCNN25", "summary": "Text-to-SQL bridges the gap between natural language and structured database\nlanguage, thus allowing non-technical users to easily query databases.\nTraditional approaches model text-to-SQL as a direct translation task, where a\ngiven Natural Language Query (NLQ) is mapped to an SQL command. Recent advances\nin large language models (LLMs) have significantly improved translation\naccuracy, however, these methods all require that the target database is\npre-specified. This becomes problematic in scenarios with multiple extensive\ndatabases, where identifying the correct database becomes a crucial yet\noverlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL\nframework to identify the user's intended database before generating SQL\nqueries. Our approach leverages LLMs and prompt engineering to extract implicit\ninformation from natural language queries (NLQs) in the form of a ruleset. We\nthen train a large db\\_id prediction model, which includes a RoBERTa-based\nfinetuned encoder, to predict the correct Database identifier (db\\_id) based on\nboth the NLQ and the LLM-generated rules. Finally, we refine the generated SQL\nby using critic agents to correct errors. Experimental results demonstrate that\nour framework outperforms the current state-of-the-art models in both database\nintent prediction and SQL generation accuracy."}
{"id": "2508.06433", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06433", "abs": "https://arxiv.org/abs/2508.06433", "authors": ["Runnan Fang", "Yuan Liang", "Xiaobin Wang", "Jialong Wu", "Shuofei Qiao", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "title": "Memp: Exploring Agent Procedural Memory", "comment": "Work in progress", "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains."}
{"id": "2508.06435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06435", "abs": "https://arxiv.org/abs/2508.06435", "authors": ["Andrea Nasuto", "Stefano Maria Iacus", "Francisco Rowe", "Devika Jain"], "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages", "comment": null, "summary": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research."}
{"id": "2508.06445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06445", "abs": "https://arxiv.org/abs/2508.06445", "authors": ["Abolfazl Ansari", "Delvin Ce Zhang", "Nafis Irtiza Tripto", "Dongwon Lee"], "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking", "comment": "To appear in 18th International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, and to be published in the Springer LNCS series", "summary": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media."}
{"id": "2508.06482", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06482", "abs": "https://arxiv.org/abs/2508.06482", "authors": ["Yilun Hua", "Evan Wang", "Yoav Artzi"], "title": "Post-training for Efficient Communication via Convention Formation", "comment": "Accepted to COLM 2025", "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods."}
