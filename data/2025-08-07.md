<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 67]
- [cs.HC](#cs.HC) [Total: 23]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 78]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion](https://arxiv.org/abs/2508.03712)
*Agrima Seth,Monojit Choudhary,Sunayana Sitaram,Kentaro Toyama,Aditya Vashistha,Kalika Bali*

Main category: cs.CL

TL;DR: 研究发现GPT-4 Turbo在生成印度生活事件故事时，宗教和种姓代表性偏差显著，且难以通过提示调整纠正。


<details>
  <summary>Details</summary>
Motivation: 扩展对大型语言模型（LLMs）代表性偏差的研究，探索宗教和种姓等较少关注的维度。

Method: 通过生成7,200个印度生活事件故事，比较模型输出与印度人口普查数据的宗教和种姓分布差异。

Result: GPT-4 Turbo过度代表文化主导群体，偏差顽固且难以通过提示调整纠正。

Conclusion: 仅多样化训练数据不足以纠正LLM偏差，需更根本的模型开发变革。

Abstract: Representational bias in large language models (LLMs) has predominantly been
measured through single-response interactions and has focused on Global
North-centric identities like race and gender. We expand on that research by
conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded
representational biases are and how they extend to less-explored dimensions of
identity. We prompt GPT-4 Turbo to generate over 7,200 stories about
significant life events (such as weddings) in India, using prompts designed to
encourage diversity to varying extents. Comparing the diversity of religious
and caste representation in the outputs against the actual population
distribution in India as recorded in census data, we quantify the presence and
"stickiness" of representational bias in the LLM for religion and caste. We
find that GPT-4 responses consistently overrepresent culturally dominant groups
far beyond their statistical representation, despite prompts intended to
encourage representational diversity. Our findings also suggest that
representational bias in LLMs has a winner-take-all quality that is more biased
than the likely distribution bias in their training data, and repeated
prompt-based nudges have limited and inconsistent efficacy in dislodging these
biases. These results suggest that diversifying training data alone may not be
sufficient to correct LLM bias, highlighting the need for more fundamental
changes in model development. Dataset and Codebook:
https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs

</details>


### [2] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
*Paul Richmond,Prarit Agarwal,Borun Chowdhury,Vasilis Niarchos,Constantinos Papageorgakis*

Main category: cs.CL

TL;DR: 研究者通过微调Llama-3.1模型，开发了20种针对高能物理理论的专用大语言模型，并在arXiv摘要上进行了训练，性能优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 为高能理论物理领域开发专用语言模型，以提升该领域的文本生成和理解能力。

Method: 使用两种不同的低秩适应微调方法，在不同组合的arXiv摘要数据集（hep-th、hep-ph、gr-qc等）上训练模型。

Result: 所有微调模型在高能物理摘要完成任务上均优于基础模型，并与主流商业LLMs进行了性能对比。

Conclusion: 研究为高能理论物理领域的专用语言模型开发提供了有价值的见解。

Abstract: We present specialized Large Language Models for theoretical High-Energy
Physics, obtained as 20 fine-tuned variants of the 8-billion parameter
Llama-3.1 model. Each variant was trained on arXiv abstracts (through August
2024) from different combinations of hep-th, hep-ph and gr-qc. For a
comparative study, we also trained models on datasets that contained abstracts
from disparate fields such as the q-bio and cs categories. All models were
fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and
varying dataset sizes, and outperformed the base model on hep-th abstract
completion tasks. We compare performance against leading commercial LLMs
(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing
specialized language models for High-Energy Theoretical Physics.

</details>


### [3] [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)
*Abhay Vijayvargia,Ajay Nagpal,Kundeshwar Pundalik,Atharva Savarkar,Smita Gautam,Pankaj Singh,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: Krishi Sathi是一款AI农业聊天机器人，通过多轮对话和检索增强生成技术为印度农民提供个性化农业建议，支持英语和印地语，并实现高准确率和快速响应。


<details>
  <summary>Details</summary>
Motivation: 解决印度农民因语言障碍和低识字率难以获取及时农业建议的问题。

Method: 采用IFT模型和检索增强生成（RAG）技术，结合多轮对话流程，支持语音输入输出。

Result: 系统响应准确率97.53%，上下文相关性91.35%，平均响应时间低于6秒。

Conclusion: 结合意图驱动对话、指令调优模型和检索生成技术，显著提升了印度农业数字支持的质效。

Abstract: Indian farmers often lack timely, accessible, and language-friendly
agricultural advice, especially in rural areas with low literacy. To address
this gap in accessibility, this paper presents a novel AI-powered agricultural
chatbot, Krishi Sathi, designed to support Indian farmers by providing
personalized, easy-to-understand answers to their queries through both text and
speech. The system's intelligence stems from an IFT model, subsequently refined
through fine-tuning on Indian agricultural knowledge across three curated
datasets. Unlike traditional chatbots that respond to one-off questions, Krishi
Sathi follows a structured, multi-turn conversation flow to gradually collect
the necessary details from the farmer, ensuring the query is fully understood
before generating a response. Once the intent and context are extracted, the
system performs Retrieval-Augmented Generation (RAG) by first fetching
information from a curated agricultural database and then generating a tailored
response using the IFT model. The chatbot supports both English and Hindi
languages, with speech input and output features (via ASR and TTS) to make it
accessible for users with low literacy or limited digital skills. This work
demonstrates how combining intent-driven dialogue flows, instruction-tuned
models, and retrieval-based generation can improve the quality and
accessibility of digital agricultural support in India.
  This approach yielded strong results, with the system achieving a query
response accuracy of 97.53%, 91.35% contextual relevance and personalization,
and a query completion rate of 97.53%. The average response time remained under
6 seconds, ensuring timely support for users across both English and Hindi
interactions.

</details>


### [4] [Hierarchical Verification of Speculative Beams for Accelerating LLM Inference](https://arxiv.org/abs/2508.03726)
*Jaydip Sen,Harshitha Puvvala,Subhasis Dasgupta*

Main category: cs.CL

TL;DR: 提出了一种名为HVT的新框架，通过优先级验证和早期剪枝优化LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决自回归LLM在推理效率上的挑战，减少不必要的计算开销。

Method: 开发了分层验证树（HVT）框架，结合理论基础和验证-剪枝算法，无需重新训练或修改架构。

Result: 实验表明HVT在多个数据集和模型上显著减少推理时间和能耗，同时保持或提升输出质量。

Conclusion: HVT展示了分层验证策略在加速LLM推理中的潜力。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
natural language processing tasks but face persistent challenges in inference
efficiency due to their autoregressive nature. While speculative decoding and
beam sampling offer notable improvements, traditional methods verify draft
sequences sequentially without prioritization, leading to unnecessary
computational overhead. This work proposes the Hierarchical Verification Tree
(HVT), a novel framework that restructures speculative beam decoding by
prioritizing high-likelihood drafts and enabling early pruning of suboptimal
candidates. Theoretical foundations and a formal verification-pruning algorithm
are developed to ensure correctness and efficiency. Integration with standard
LLM inference pipelines is achieved without requiring retraining or
architecture modification. Experimental evaluations across multiple datasets
and models demonstrate that HVT consistently outperforms existing speculative
decoding schemes, achieving substantial reductions in inference time and energy
consumption while maintaining or enhancing output quality. The findings
highlight the potential of hierarchical verification strategies as a new
direction for accelerating large language model inference.

</details>


### [5] [WINELL: Wikipedia Never-Ending Updating with LLM Agents](https://arxiv.org/abs/2508.03728)
*Revanth Gangi Reddy,Tanay Dixit,Jiaxin Qin,Cheng Qian,Daniel Lee,Jiawei Han,Kevin Small,Xing Fan,Ruhi Sarikaya,Heng Ji*

Main category: cs.CL

TL;DR: WiNELL是一个基于多智能体框架的系统，用于持续更新维基百科内容，通过聚合在线信息并生成精确的编辑建议，优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 维基百科依赖人工编辑更新内容，效率低且难以保持最新。WiNELL旨在通过LLM智能体实现持续知识更新。

Method: 采用多智能体框架，聚合在线信息并生成编辑建议，模型基于维基百科历史编辑数据训练。

Result: WiNELL在信息覆盖率和编辑效率上优于开源和闭源LLM（如GPT-4o），能及时识别并建议事实更新。

Conclusion: WiNELL为LLM智能体自动更新知识库提供了有前景的研究方向。

Abstract: Wikipedia, a vast and continuously consulted knowledge base, faces
significant challenges in maintaining up-to-date content due to its reliance on
manual human editors. Inspired by the vision of continuous knowledge
acquisition in NELL and fueled by advances in LLM-based agents, this paper
introduces WiNELL, an agentic framework for continuously updating Wikipedia
articles. Our approach employs a multi-agent framework to aggregate online
information, select new and important knowledge for a target entity in
Wikipedia, and then generate precise edit suggestions for human review. Our
fine-grained editing models, trained on Wikipedia's extensive history of human
edits, enable incorporating updates in a manner consistent with human editing
behavior. Our editor models outperform both open-source instruction-following
baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and
editing efficiency. End-to-end evaluation on high-activity Wikipedia pages
demonstrates WiNELL's ability to identify and suggest timely factual updates.
This opens up a promising research direction in LLM agents for automatically
updating knowledge bases in a never-ending fashion.

</details>


### [6] [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)
*Ashutosh Bandooni,Brindha Subburaj*

Main category: cs.CL

TL;DR: GanitBench是一个包含1527道数学视觉问题的多语言基准测试（英语和印地语），用于评估视觉语言模型（VLMs）的推理能力。测试显示GPT-4o mini表现最佳，平均准确率为38.15%。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试多为英语且缺乏印地语数据集，尤其是在数学领域。GanitBench旨在填补这一空白，促进多语言研究。

Method: 从印度JEE Advanced和CBSE考试中收集问题，以图像形式呈现，评估两种闭源模型在零样本和两样本链式思考（CoT）设置下的表现。

Result: GPT-4o mini表现最佳，但模型在印地语问题和“双锁”约束下性能下降。两样本CoT设置更有效。

Conclusion: GanitBench为多语言研究提供了新工具，尤其在印地语领域，并展示了VLMs在复杂推理任务中的局限性。

Abstract: Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on
several fields and domains are being curated more frequently over the last few
years. However these are often monolingual, mostly available in English.
Additionally there also is a lack of datasets available in Hindi on tasks apart
from comprehension and translation. We introduce GanitBench, a tough benchmark
consisting of 1527 vision-only questions covering several topics in Mathematics
- available in languages English and Hindi. Collected from two major
examinations from India, the JEE Advanced and the CBSE Boards examinations,
this benchmark includes questions in the form of images comprising of figures
essential to a question as well as text. We evaluate two closed source models
for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.
GPT-4o mini is found to be the more dominant model on the benchmark, with it's
highest average accuracy being 38.15%. We also evaluate models through a
"Double Lock" constraint, which brings down the performance of the models by
considerable margins. We observe that two-shot CoT appears to be a more
effective setting under this environment. Performance of the two VLMs also
decreases when answering the same questions in the Hindi language. We hope to
facilitate the inclusion of languages like Hindi in research through our work.

</details>


### [7] [AttnTrace: Attention-based Context Traceback for Long-Context LLMs](https://arxiv.org/abs/2508.03793)
*Yanting Wang,Runpeng Geng,Ying Chen,Jinyuan Jia*

Main category: cs.CL

TL;DR: 提出了一种基于注意力权重的上下文追溯方法AttnTrace，比现有方法更准确高效，并展示了其在检测长上下文中的提示注入等实际应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型（LLM）在检索增强生成（RAG）和自主代理等系统中广泛应用，但现有上下文追溯方法（如TracLLM）计算成本高，效率低。

Method: 提出AttnTrace方法，利用LLM生成的注意力权重进行上下文追溯，并引入两种技术增强其效果，同时提供理论支持。

Result: AttnTrace在准确性和效率上优于现有方法，并能通过“检测前归因”范式提升长上下文中提示注入的检测效果。

Conclusion: AttnTrace为上下文追溯提供了一种高效且准确的解决方案，具有实际应用潜力。

Abstract: Long-context large language models (LLMs), such as Gemini-2.5-Pro and
Claude-Sonnet-4, are increasingly used to empower advanced AI systems,
including retrieval-augmented generation (RAG) pipelines and autonomous agents.
In these systems, an LLM receives an instruction along with a context--often
consisting of texts retrieved from a knowledge database or memory--and
generates a response that is contextually grounded by following the
instruction. Recent studies have designed solutions to trace back to a subset
of texts in the context that contributes most to the response generated by the
LLM. These solutions have numerous real-world applications, including
performing post-attack forensic analysis and improving the interpretability and
trustworthiness of LLM outputs. While significant efforts have been made,
state-of-the-art solutions such as TracLLM often lead to a high computation
cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a
single response-context pair. In this work, we propose AttnTrace, a new context
traceback method based on the attention weights produced by an LLM for a
prompt. To effectively utilize attention weights, we introduce two techniques
designed to enhance the effectiveness of AttnTrace, and we provide theoretical
insights for our design choice. We also perform a systematic evaluation for
AttnTrace. The results demonstrate that AttnTrace is more accurate and
efficient than existing state-of-the-art context traceback methods. We also
show that AttnTrace can improve state-of-the-art methods in detecting prompt
injection under long contexts through the attribution-before-detection
paradigm. As a real-world application, we demonstrate that AttnTrace can
effectively pinpoint injected instructions in a paper designed to manipulate
LLM-generated reviews. The code is at
https://github.com/Wang-Yanting/AttnTrace.

</details>


### [8] [Majority Bit-Aware Watermarking For Large Language Models](https://arxiv.org/abs/2508.03829)
*Jiahao Xu,Rui Hu,Zikai Zhang*

Main category: cs.CL

TL;DR: MajorMark是一种新型水印方法，通过多数位感知编码改进文本质量与解码准确性的权衡，优于现有多比特水印方案。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）生成有害或欺骗性内容的潜在滥用问题，水印技术成为验证来源和追踪滥用的有效手段。

Method: 提出MajorMark，基于多数位选择偏好令牌集，采用聚类解码策略；进一步提出MajorMark$^+$，分块编码和解码以提升性能。

Result: 实验表明，MajorMark和MajorMark$^+$在解码准确性和文本生成质量上显著优于现有方法。

Conclusion: MajorMark系列方法有效平衡了文本质量与解码准确性，为LLMs水印技术提供了更优解决方案。

Abstract: The growing deployment of Large Language Models (LLMs) in real-world
applications has raised concerns about their potential misuse in generating
harmful or deceptive content. To address this issue, watermarking techniques
have emerged as a promising solution by embedding identifiable binary messages
into generated text for origin verification and misuse tracing. While recent
efforts have explored multi-bit watermarking schemes capable of embedding rich
information such as user identifiers, they typically suffer from the
fundamental trade-off between text quality and decoding accuracy: to ensure
reliable message decoding, they have to restrict the size of preferred token
sets during encoding, yet such restrictions reduce the quality of the generated
content. In this work, we propose MajorMark, a novel watermarking method that
improves this trade-off through majority bit-aware encoding. MajorMark selects
preferred token sets based on the majority bit of the message, enabling a
larger and more flexible sampling of tokens. In contrast to prior methods that
rely on token frequency analysis for decoding, MajorMark employs a
clustering-based decoding strategy, which maintains high decoding accuracy even
when the preferred token set is large, thus preserving both content quality and
decoding accuracy. We further introduce MajorMark$^+$, which partitions the
message into multiple blocks to independently encode and deterministically
decode each block, thereby further enhancing the quality of watermarked text
and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs
demonstrate that our methods significantly enhance both decoding accuracy and
text generation quality, outperforming prior multi-bit watermarking baselines.

</details>


### [9] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
*Subhey Sadi Rahman,Md. Adnanul Islam,Md. Mahbub Alam,Musarrat Zeba,Md. Abdur Rahman,Sadia Sultana Chowa,Mohaimenul Azam Khan Raiaan,Sami Azam*

Main category: cs.CL

TL;DR: 该论文综述了大型语言模型（LLMs）生成内容的真实性评估方法，强调了幻觉、数据集限制和评估指标可靠性等挑战，并提出了改进框架和研究方向。


<details>
  <summary>Details</summary>
Motivation: LLMs生成的错误信息问题日益严重，需要系统性的评估和改进方法来提高其事实准确性。

Method: 通过分析2020至2025年的文献，探讨了提示策略、领域微调、检索增强生成（RAG）等方法，并提出了五个研究问题。

Result: 研究发现当前评估指标存在局限性，外部知识验证和领域定制对提高事实一致性至关重要。

Conclusion: 构建准确、可解释且适用于领域特定事实检查的LLMs是未来研究的关键方向。

Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

</details>


### [10] [An Entity Linking Agent for Question Answering](https://arxiv.org/abs/2508.03865)
*Yajie Luo,Yihong Wu,Muzhi Li,Fengran Mo,Jia Ao Sun,Xinyu Wang,Liheng Ma,Yingxue Zhang,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的实体链接代理，用于问答系统中的短文本实体链接，通过模拟人类认知流程提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有实体链接方法在短文本问答任务中表现不佳，需要一种更适合短文本的解决方案。

Method: 设计了一个基于大语言模型的代理，主动识别实体提及、检索候选实体并做出决策。

Result: 实验验证了代理在工具型实体链接和问答任务中的鲁棒性和有效性。

Conclusion: 该代理在短文本问答任务中表现出色，解决了现有方法的局限性。

Abstract: Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide
accurate answers. Entity Linking (EL) plays a critical role in linking natural
language mentions to KB entries. However, most existing EL methods are designed
for long contexts and do not perform well on short, ambiguous user questions in
QA tasks. We propose an entity linking agent for QA, based on a Large Language
Model that simulates human cognitive workflows. The agent actively identifies
entity mentions, retrieves candidate entities, and makes decision. To verify
the effectiveness of our agent, we conduct two experiments: tool-based entity
linking and QA task evaluation. The results confirm the robustness and
effectiveness of our agent.

</details>


### [11] [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
*Haofei Yu,Zhengyang Qi,Yining Zhao,Kolby Nottingham,Keyang Xuan,Bodhisattwa Prasad Majumder,Hao Zhu,Paul Pu Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: Sotopia-RL框架通过细化粗粒度的回合级反馈为话语级、多维奖励，解决了社会交互中部分可观测性和多维性对强化学习的挑战，显著提升了社会目标完成分数。


<details>
  <summary>Details</summary>
Motivation: 社会智能对大型语言模型至关重要，但社会交互的部分可观测性和多维性使传统强化学习方法效率低下且不稳定。

Method: 提出Sotopia-RL框架，将回合级反馈细化为话语级、多维奖励，以优化信用分配和减少奖励欺骗。

Result: 在Sotopia环境中，Sotopia-RL取得了最先进的社会目标完成分数（7.17和8.31），显著优于现有方法。

Conclusion: Sotopia-RL通过话语级信用分配和多维奖励设计，有效提升了社会交互任务的强化学习效果。

Abstract: Social intelligence has become a critical capability for large language
models (LLMs), enabling them to engage effectively in real-world social tasks
such as accommodation, persuasion, collaboration, and negotiation.
Reinforcement learning (RL) is a natural fit for training socially intelligent
agents because it allows models to learn sophisticated strategies directly
through social interactions. However, social interactions have two key
characteristics that set barriers for RL training: (1) partial observability,
where utterances have indirect and delayed effects that complicate credit
assignment, and (2) multi-dimensionality, where behaviors such as
rapport-building or knowledge-seeking contribute indirectly to goal
achievement. These characteristics make Markov decision process (MDP)-based RL
with single-dimensional episode-level rewards inefficient and unstable. To
address these challenges, we propose Sotopia-RL, a novel framework that refines
coarse episode-level feedback into utterance-level, multi-dimensional rewards.
Utterance-level credit assignment mitigates partial observability by
attributing outcomes to individual utterances, while multi-dimensional rewards
capture the full richness of social interactions and reduce reward hacking.
Experiments in Sotopia, an open-ended social learning environment, demonstrate
that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17
on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing
approaches. Ablation studies confirm the necessity of both utterance-level
credit assignment and multi-dimensional reward design for RL training. Our
implementation is publicly available at:
https://github.com/sotopia-lab/sotopia-rl.

</details>


### [12] [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923)
*Linxin Song,Yutong Dai,Viraj Prabhu,Jieyu Zhang,Taiwei Shi,Li Li,Junnan Li,Silvio Savarese,Zeyuan Chen,Jieyu Zhao,Ran Xu,Caiming Xiong*

Main category: cs.CL

TL;DR: CoAct-1结合GUI操作与编程执行，显著提升自动化代理的效率和可靠性，在OSWorld基准测试中取得60.76%的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统GUI操作的自主代理在复杂任务中效率低且不可靠，需要更灵活的方法。

Method: 引入CoAct-1多代理系统，动态分配任务给GUI操作员或编程代理，支持Python或Bash脚本执行。

Result: 在OSWorld基准测试中，CoAct-1成功率60.76%，平均步骤降至10.15，显著优于纯GUI代理。

Conclusion: 编程与GUI结合的混合方法为通用计算机自动化提供了更强大、高效和可扩展的路径。

Abstract: Autonomous agents that operate computers via Graphical User Interfaces (GUIs)
often struggle with efficiency and reliability on complex, long-horizon tasks.
While augmenting these agents with planners can improve task decomposition,
they remain constrained by the inherent limitations of performing all actions
through GUI manipulation, leading to brittleness and inefficiency. In this
work, we introduce a more robust and flexible paradigm: enabling agents to use
coding as a enhanced action. We present CoAct-1, a novel multi-agent system
that synergistically combines GUI-based control with direct programmatic
execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks
to either a conventional GUI Operator or a specialized Programmer agent, which
can write and execute Python or Bash scripts. This hybrid approach allows the
agent to bypass inefficient GUI action sequences for tasks like file management
and data processing, while still leveraging visual interaction when necessary.
We evaluate our system on the challenging OSWorld benchmark, where CoAct-1
achieves a new state-of-the-art success rate of 60.76%, significantly
outperforming prior methods. Furthermore, our approach dramatically improves
efficiency, reducing the average number of steps required to complete a task to
just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that
integrating coding as a core action provides a more powerful, efficient, and
scalable path toward generalized computer automation.

</details>


### [13] [CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation](https://arxiv.org/abs/2508.03935)
*Raymond Wilson,Cole Graham,Chase Carter,Zefeng Yang,Ruiqi Gu*

Main category: cs.CL

TL;DR: CAP-LLM是一个结合用户偏好和事实一致性的新闻标题生成框架，通过增强预训练大语言模型，显著提升了标题的个性化和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在捕捉用户兴趣和确保事实一致性方面的不足，避免生成通用或误导性标题。

Method: 提出CAP-LLM框架，包括用户偏好编码器、上下文注入适配器和事实一致性强化模块，利用对比损失减少幻觉。

Result: 在PENS数据集上表现优异，事实一致性（FactCC 87.50）和个性化（Pc(avg) 2.73）均优于基线模型。

Conclusion: CAP-LLM在新闻标题生成中实现了个性化和事实准确性的平衡，验证了其组件的有效性和鲁棒性。

Abstract: In the era of information overload, personalized news headline generation is
crucial for engaging users by tailoring content to their preferences while
accurately conveying news facts. Existing methods struggle with effectively
capturing complex user interests and ensuring factual consistency, often
leading to generic or misleading headlines. Leveraging the unprecedented
capabilities of Large Language Models (LLMs) in text generation, we propose
Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates
user preferences and factual consistency constraints into a powerful
pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture
long-term user interests, a Context Injection Adapter to seamlessly integrate
these preferences and current article context into the LLM's generation
process, and a Fact-Consistency Reinforcement Module employing a novel
contrastive loss to mitigate hallucination. Evaluated on the real-world PENS
dataset, CAP-LLM achieves state-of-the-art performance across all metrics.
Notably, it significantly improves factual consistency (FactCC of 87.50) over
strong baselines like BART (86.67), while simultaneously enhancing
personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1
26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,
and sensitivity analyses further validate the effectiveness of each component
and the robustness of our approach, demonstrating CAP-LLM's ability to achieve
a superior balance between personalization and factual accuracy in news
headline generation.

</details>


### [14] [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)
*Alok Abhishek,Lisa Erickson,Tushar Bandopadhyay*

Main category: cs.CL

TL;DR: 本文提出了一种系统化治理、评估和量化机器学习模型生命周期中偏见的方法，从开发到生产监控，并讨论了针对大型语言模型（LLMs）的数据与AI治理框架。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs中普遍存在的偏见和公平性问题，推动生成式AI的社会责任和伦理对齐。

Method: 基于BEATS测试套件，提出数据与AI治理框架，涵盖开发、验证、生产监控和防护措施。

Result: 通过实施该框架，组织能显著提升GenAI系统的安全性和责任性，减少歧视风险。

Conclusion: 本文旨在推动生成式AI的社会责任和伦理对齐，为实际应用提供治理方案。

Abstract: In this paper, we cover approaches to systematically govern, assess and
quantify bias across the complete life cycle of machine learning models, from
initial development and validation to ongoing production monitoring and
guardrail implementation. Building upon our foundational work on the Bias
Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the
authors share prevalent bias and fairness related gaps in Large Language Models
(LLMs) and discuss data and AI governance framework to address Bias, Ethics,
Fairness, and Factuality within LLMs. The data and AI governance approach
discussed in this paper is suitable for practical, real-world applications,
enabling rigorous benchmarking of LLMs prior to production deployment,
facilitating continuous real-time evaluation, and proactively governing LLM
generated responses. By implementing the data and AI governance across the life
cycle of AI development, organizations can significantly enhance the safety and
responsibility of their GenAI systems, effectively mitigating risks of
discrimination and protecting against potential reputational or brand-related
harm. Ultimately, through this article, we aim to contribute to advancement of
the creation and deployment of socially responsible and ethically aligned
generative artificial intelligence powered applications.

</details>


### [15] [Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency](https://arxiv.org/abs/2508.03979)
*Md Arafat Sultan,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 研究通过早期假设剪枝提高自一致性在长链思维任务中的令牌效率，同时保留其并行性。


<details>
  <summary>Details</summary>
Motivation: 自一致性的高令牌消耗限制了其实用性，研究旨在提高其效率。

Method: 并行生成所有解决方案，定期基于模型置信度和词汇覆盖度剪枝中间假设，设计快速加权集覆盖算法。

Result: 在五个LLM和三个数学基准测试中，令牌效率提升10-35%。

Conclusion: 该方法显著提高了令牌效率，适用于长链思维任务。

Abstract: Despite its simplicity and efficacy, the high token expenditure of
self-consistency can limit its practical utility. Here we investigate if
self-consistency can be made more token-efficient for long chain-of-thought
reasoning tasks, while preserving its parallelism, through early hypothesis
pruning. Concretely, we generate all solutions in parallel, but periodically
prune intermediate hypotheses that are deemed unnecessary based on two
lightweight indicators: (a) the model's own confidence in individual
hypotheses, and (b) lexical coverage of all current hypotheses by candidate
subsets that are under consideration for continued retention. We design a fast
weighted set cover algorithm that utilizes the two indicators; our evaluation
of five LLMs on three math benchmarks shows that this method can improve token
efficiency for all models, by 10-35% in many cases.

</details>


### [16] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)
*Bohan Jiang,Dawei Li,Zhen Tan,Chengshuai Zhao,Huan Liu*

Main category: cs.CL

TL;DR: 研究探讨了LLMs在解释幸福感概念时的准确性及个性化能力，构建了大规模数据集并提出了评估框架，发现偏好优化方法能显著提升解释质量。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地依赖LLMs理解幸福感，需要验证LLMs是否能生成既准确又适合不同受众的解释。

Method: 构建了包含43,880条解释的数据集，采用双评委原则引导的LLM评估框架，并通过SFT和DPO优化开源LLM。

Result: LLM评委与人类评估一致，解释质量因模型、受众和类别而异，DPO和SFT优化的模型表现优于更大模型。

Conclusion: 偏好优化方法能有效提升LLMs在专业解释任务中的表现，为个性化解释生成提供了新思路。

Abstract: Well-being encompasses mental, physical, and social dimensions essential to
personal growth and informed life decisions. As individuals increasingly
consult Large Language Models (LLMs) to understand well-being, a key challenge
emerges: Can LLMs generate explanations that are not only accurate but also
tailored to diverse audiences? High-quality explanations require both factual
correctness and the ability to meet the expectations of users with varying
expertise. In this work, we construct a large-scale dataset comprising 43,880
explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We
introduce a principle-guided LLM-as-a-judge evaluation framework, employing
dual judges to assess explanation quality. Furthermore, we show that
fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO) can significantly enhance the quality of
generated explanations. Our results reveal: (1) The proposed LLM judges align
well with human evaluations; (2) explanation quality varies significantly
across models, audiences, and categories; and (3) DPO- and SFT-finetuned models
outperform their larger counterparts, demonstrating the effectiveness of
preference-based learning for specialized explanation tasks.

</details>


### [17] [Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models](https://arxiv.org/abs/2508.03998)
*Xinyu Zhao,Zhen Tan,Maya Enisman,Minjae Seo,Marta R. Durantini,Dolores Albarracin,Tianlong Chen*

Main category: cs.CL

TL;DR: 论文提出了一种社交机器人辅助系统，通过透明化的概念瓶颈模型（CBM）分析多模态会议数据，帮助协调员实时干预，提升群体会议的效率和社交关系。


<details>
  <summary>Details</summary>
Motivation: 解决协调员在群体会议中面临的认知负荷和干预需求，同时避免传统基础模型（FMs）的“黑箱”问题。

Method: 采用基于人类可解释概念（如参与度和情感）的CBM，并通过迁移学习框架将FMs的广泛社交理解能力转化为透明化的CBM。

Result: 该系统显著优于零样本FMs，能实时预测干预需求并支持人工修正，且能跨群体泛化和提升新手协调员的表现。

Conclusion: 通过将专家认知模型转化为可解释的机器人伙伴，该研究为复杂社交领域中增强人类能力提供了有效方案。

Abstract: Successful group meetings, such as those implemented in group
behavioral-change programs, work meetings, and other social contexts, must
promote individual goal setting and execution while strengthening the social
relationships within the group. Consequently, an ideal facilitator must be
sensitive to the subtle dynamics of disengagement, difficulties with individual
goal setting and execution, and interpersonal difficulties that signal a need
for intervention. The challenges and cognitive load experienced by facilitators
create a critical gap for an embodied technology that can interpret social
exchanges while remaining aware of the needs of the individuals in the group
and providing transparent recommendations that go beyond powerful but "black
box" foundation models (FMs) that identify social cues. We address this
important demand with a social robot co-facilitator that analyzes multimodal
meeting data and provides discreet cues to the facilitator. The robot's
reasoning is powered by an agentic concept bottleneck model (CBM), which makes
decisions based on human-interpretable concepts like participant engagement and
sentiments, ensuring transparency and trustworthiness. Our core contribution is
a transfer learning framework that distills the broad social understanding of
an FM into our specialized and transparent CBM. This concept-driven system
significantly outperforms direct zero-shot FMs in predicting the need for
intervention and enables real-time human correction of its reasoning.
Critically, we demonstrate robust knowledge transfer: the model generalizes
across different groups and successfully transfers the expertise of senior
human facilitators to improve the performance of novices. By transferring an
expert's cognitive model into an interpretable robotic partner, our work
provides a powerful blueprint for augmenting human capabilities in complex
social domains.

</details>


### [18] [HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization](https://arxiv.org/abs/2508.04010)
*Yurun Chen,Xavier Hu,Yuhan Liu,Keting Yin,Juncheng Li,Zhuosheng Zhang,Shengyu Zhang*

Main category: cs.CL

TL;DR: HarmonyGuard是一个多智能体协作框架，通过策略增强和双目标优化，提升网络环境中的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决网络代理在长序列操作中平衡任务性能与新兴风险的挑战，弥补现有研究在安全性和实用性协同优化上的不足。

Method: 采用多智能体架构，包括策略代理（自适应提取和更新安全策略）和实用代理（基于马尔可夫实时推理和元认知能力优化安全性与实用性）。

Result: 在多个基准测试中，HarmonyGuard将策略合规性提升38%，任务完成率提升20%，并在所有任务中实现90%以上的策略合规性。

Conclusion: HarmonyGuard有效解决了网络代理在安全性和实用性协同优化上的问题，具有显著的实际应用价值。

Abstract: Large language models enable agents to autonomously perform tasks in open web
environments. However, as hidden threats within the web evolve, web agents face
the challenge of balancing task performance with emerging risks during
long-sequence operations. Although this challenge is critical, current research
remains limited to single-objective optimization or single-turn scenarios,
lacking the capability for collaborative optimization of both safety and
utility in web environments. To address this gap, we propose HarmonyGuard, a
multi-agent collaborative framework that leverages policy enhancement and
objective optimization to jointly improve both utility and safety. HarmonyGuard
features a multi-agent architecture characterized by two fundamental
capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent
within HarmonyGuard, which automatically extracts and maintains structured
security policies from unstructured external documents, while continuously
updating policies in response to evolving threats. (2) Dual-Objective
Optimization: Based on the dual objectives of safety and utility, the Utility
Agent integrated within HarmonyGuard performs the Markovian real-time reasoning
to evaluate the objectives and utilizes metacognitive capabilities for their
optimization. Extensive evaluations on multiple benchmarks show that
HarmonyGuard improves policy compliance by up to 38% and task completion by up
to 20% over existing baselines, while achieving over 90% policy compliance
across all tasks. Our project is available here:
https://github.com/YurunChen/HarmonyGuard.

</details>


### [19] [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)
*Xiaopeng Li,Shasha Li,Xi Wang,Shezheng Song,Bin Ji,Shangwen Wang,Jun Ma,Xiaodong Liu,Mina Liu,Jie Yu*

Main category: cs.CL

TL;DR: SMEdit是一种新的基于元学习的模型编辑方法，通过多步反向传播和权重更新正则化，提升了在低数据场景下的编辑效果和训练效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的静态特性导致更新知识成本高昂，而现有的元学习模型编辑方法（MLBME）在低数据场景下表现不佳且训练效率受限。

Method: 提出SMEdit方法，采用多步反向传播（MBPS）和权重更新正则化，优化低数据下的编辑效果和训练效率。

Result: 实验表明，SMEdit在两种数据集和两种LLMs上优于现有MLBME方法，且MBPS策略可无缝集成到其他方法中进一步提升性能。

Conclusion: SMEdit通过改进编辑策略和训练效率，为LLMs的动态知识更新提供了更高效的解决方案。

Abstract: Large Language Models (LLMs) underpin many AI applications, but their static
nature makes updating knowledge costly. Model editing offers an efficient
alternative by injecting new information through targeted parameter
modifications. In particular, meta-learning-based model editing (MLBME) methods
have demonstrated notable advantages in both editing effectiveness and
efficiency. Despite this, we find that MLBME exhibits suboptimal performance in
low-data scenarios, and its training efficiency is bottlenecked by the
computation of KL divergence. To address these, we propose $\textbf{S}$tep
$\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that
adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation
$\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited
supervision and a norm regularization on weight updates to improve training
efficiency. Experimental results on two datasets and two LLMs demonstrate that
SMEdit outperforms prior MLBME baselines and the MBPS strategy can be
seamlessly integrated into existing methods to further boost their performance.
Our code will be released soon.

</details>


### [20] [ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents](https://arxiv.org/abs/2508.04038)
*Zechen Li,Baiyu Chen,Hao Xue,Flora D. Salim*

Main category: cs.CL

TL;DR: ZARA是一种基于代理的零样本、可解释的人体活动识别框架，直接从原始运动时间序列中提取特征，无需微调或特定任务分类器，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要针对固定活动集进行训练，且在新行为或传感器设置出现时需要昂贵的重新训练。此外，基于大语言模型的方法在准确性和可解释性上存在局限。

Method: ZARA结合了自动生成的特征知识库、多传感器检索模块和分层代理流程，指导大语言模型迭代选择特征并生成预测和解释。

Result: 在8个HAR基准测试中，ZARA实现了零样本SOTA性能，宏F1分数比最强基线高2.53倍，并提供了清晰的推理过程。

Conclusion: ZARA为可信赖的即插即用运动时间序列分析提供了有前景的解决方案。

Abstract: Motion sensor time-series are central to human activity recognition (HAR),
with applications in health, sports, and smart devices. However, existing
methods are trained for fixed activity sets and require costly retraining when
new behaviours or sensor setups appear. Recent attempts to use large language
models (LLMs) for HAR, typically by converting signals into text or images,
suffer from limited accuracy and lack verifiable interpretability. We propose
ZARA, the first agent-based framework for zero-shot, explainable HAR directly
from raw motion time-series. ZARA integrates an automatically derived pair-wise
feature knowledge base that captures discriminative statistics for every
activity pair, a multi-sensor retrieval module that surfaces relevant evidence,
and a hierarchical agent pipeline that guides the LLM to iteratively select
features, draw on this evidence, and produce both activity predictions and
natural-language explanations. ZARA enables flexible and interpretable HAR
without any fine-tuning or task-specific classifiers. Extensive experiments on
8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering
clear reasoning while exceeding the strongest baselines by 2.53x in macro F1.
Ablation studies further confirm the necessity of each module, marking ZARA as
a promising step toward trustworthy, plug-and-play motion time-series analysis.
Our codes are available at https://github.com/zechenli03/ZARA.

</details>


### [21] [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)
*Thilo Hagendorff,Erik Derner,Nuria Oliver*

Main category: cs.CL

TL;DR: 研究表明，大型推理模型（LRMs）可以简化并规模化越狱行为，使其成为非专家也能进行的低成本活动。


<details>
  <summary>Details</summary>
Motivation: 传统越狱需要复杂技术或专业知识，本研究探讨LRMs如何通过说服能力简化这一过程。

Method: 评估四种LRMs（DeepSeek-R1、Gemini 2.5 Flash、Grok 3 Mini、Qwen3 235B）作为自主对手，与九个目标模型进行多轮对话，执行越狱。

Result: 在70个敏感领域提示的实验中，总体攻击成功率达97.14%。

Conclusion: 研究揭示了LRMs可能系统性削弱其他模型的安全防护，亟需进一步对齐前沿模型以抵抗和防止其被用作越狱工具。

Abstract: Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has
traditionally required complex technical procedures or specialized human
expertise. In this study, we show that the persuasive capabilities of large
reasoning models (LRMs) simplify and scale jailbreaking, converting it into an
inexpensive activity accessible to non-experts. We evaluated the capabilities
of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as
autonomous adversaries conducting multi-turn conversations with nine widely
used target models. LRMs received instructions via a system prompt, before
proceeding to planning and executing jailbreaks with no further supervision. We
performed extensive experiments with a benchmark of harmful prompts composed of
70 items covering seven sensitive domains. This setup yielded an overall attack
success rate across all model combinations of 97.14%. Our study reveals an
alignment regression, in which LRMs can systematically erode the safety
guardrails of other models, highlighting the urgent need to further align
frontier models not only to resist jailbreak attempts, but also to prevent them
from being co-opted into acting as jailbreak agents.

</details>


### [22] [DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation](https://arxiv.org/abs/2508.04047)
*Jiabing Yang,Yixiang Chen,Zichen Wen,Chenhang Cui,Peiyan Li,Yuan Xu,Bowen Fang,Yan Huang,Liang Wang*

Main category: cs.CL

TL;DR: 论文提出了一种轻量级框架DTPA，用于提升长文本生成中的可控性，解决了前缀注意力衰减问题。


<details>
  <summary>Details</summary>
Motivation: 现有可控文本生成方法在长文本生成中表现不佳，主要因前缀注意力衰减。

Method: 提出DTPA框架，动态选择最优前缀类型并指数级放大前缀注意力，增强可控性。

Result: 实验表明DTPA在多个任务中优于其他方法，尤其在长文本生成中表现突出。

Conclusion: DTPA有效提升了长文本生成的可控性，同时保持文本质量。

Abstract: Controllable Text Generation (CTG) is a vital subfield in Natural Language
Processing (NLP), aiming to generate text that aligns with desired attributes.
However, previous studies commonly focus on the quality of controllable text
generation for short sequences, while the generation of long-form text remains
largely underexplored. In this paper, we observe that the controllability of
texts generated by the powerful prefix-based method Air-Decoding tends to
decline with increasing sequence length, which we hypothesize primarily arises
from the observed decay in attention to the prefixes. Meanwhile, different
types of prefixes including soft and hard prefixes are also key factors
influencing performance. Building on these insights, we propose a lightweight
and effective framework called Dynamic Token-level Prefix Augmentation (DTPA)
based on Air-Decoding for controllable text generation. Specifically, it first
selects the optimal prefix type for a given task. Then we dynamically amplify
the attention to the prefix for the attribute distribution to enhance
controllability, with a scaling factor growing exponentially as the sequence
length increases. Moreover, based on the task, we optionally apply a similar
augmentation to the original prompt for the raw distribution to balance text
quality. After attribute distribution reconstruction, the generated text
satisfies the attribute constraints well. Experiments on multiple CTG tasks
demonstrate that DTPA generally outperforms other methods in attribute control
while maintaining competitive fluency, diversity, and topic relevance. Further
analysis highlights DTPA's superior effectiveness in long text generation.

</details>


### [23] [PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG](https://arxiv.org/abs/2508.04057)
*Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CL

TL;DR: PAIRS框架通过自适应决定是否检索外部信息及如何选择，提高了RAG系统的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统存在检索效率低和可能检索无关文档的问题。

Method: PAIRS采用双路径生成机制和自适应信息选择模块，结合参数化知识和检索知识。

Result: 实验表明PAIRS减少25%检索成本，同时提高1.1% EM和1.0% F1。

Conclusion: PAIRS是一种简单有效的框架，显著提升了RAG系统的性能。

Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone technique for
enhancing large language models (LLMs) with external knowledge. However,
current RAG systems face two critical limitations: (1) they inefficiently
retrieve information for every query, including simple questions that could be
resolved using the LLM's parametric knowledge alone, and (2) they risk
retrieving irrelevant documents when queries contain sparse information
signals. To address these gaps, we introduce Parametric-verified Adaptive
Information Retrieval and Selection (PAIRS), a training-free framework that
integrates parametric and retrieved knowledge to adaptively determine whether
to retrieve and how to select external information. Specifically, PAIRS employs
a dual-path generation mechanism: First, the LLM produces both a direct answer
and a context-augmented answer using self-generated pseudo-context. When these
outputs converge, PAIRS bypasses external retrieval entirely, dramatically
improving the RAG system's efficiency. For divergent cases, PAIRS activates a
dual-path retrieval (DPR) process guided by both the original query and
self-generated contextual signals, followed by an Adaptive Information
Selection (AIS) module that filters documents through weighted similarity to
both sources. This simple yet effective approach can not only enhance
efficiency by eliminating unnecessary retrievals but also improve accuracy
through contextually guided retrieval and adaptive information selection.
Experimental results on six question-answering (QA) benchmarks show that PAIRS
reduces retrieval costs by around 25% (triggering for only 75% of queries)
while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior
baselines on average.

</details>


### [24] [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)
*Julián Camilo Velandia Gutiérrez*

Main category: cs.CL

TL;DR: 该论文提出了一种从基础模型出发，结合数据处理、数据选择、训练策略和架构调整的方法，以提高资源受限环境下大型语言模型（LLMs）的效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的部署受限于高计算资源需求，因此需要探索提高效率的方法。

Method: 通过定义可靠数据集标准、进行不同配置的对照实验，并系统评估模型变体的能力、通用性、响应时间和安全性。

Result: 通过比较测试验证了所提策略的有效性。

Conclusion: 该研究为资源受限环境下优化LLMs提供了可行方案。

Abstract: Large Language Models (LLMs) have become a milestone in the field of
artificial intelligence and natural language processing. However, their
large-scale deployment remains constrained by the need for significant
computational resources. This work proposes starting from a base model to
explore and combine data processing and careful data selection techniques,
training strategies, and architectural adjustments to improve the efficiency of
LLMs in resource-constrained environments and within a delimited knowledge
base. The methodological approach included defining criteria for building
reliable datasets, conducting controlled experiments with different
configurations, and systematically evaluating the resulting variants in terms
of capability, versatility, response time, and safety. Finally, comparative
tests were conducted to measure the performance of the developed variants and
to validate the effectiveness of the proposed strategies. This work is based on
the master's thesis in Systems and Computer Engineering titled "Efficient
Strategy for Improving the Capabilities of Large Language Models (LLMs)".

</details>


### [25] [ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"](https://arxiv.org/abs/2508.04086)
*Zhongyi Zhou,Kohei Uehara,Haoyu Zhang,Jingtao Zhou,Lin Gu,Ruofei Du,Zheng Xu,Tatsuya Harada*

Main category: cs.CL

TL;DR: ToolGrad是一种新的框架，通过“答案优先”的方法生成工具使用数据集，解决了传统方法效率低和标注失败的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法生成工具使用数据集时效率低且容易失败，需要一种更高效、更可靠的方法。

Method: ToolGrad通过迭代过程构建有效的工具使用链，再生成对应的用户查询，形成数据集ToolGrad-5k。

Result: ToolGrad-5k数据集生成成本更低、复杂度更高，且标注通过率100%。实验表明，基于该数据集训练的模型表现优于基线数据集和专有LLM。

Conclusion: ToolGrad提供了一种高效、低成本的数据集生成方法，显著提升了模型性能。

Abstract: Prior work synthesizes tool-use LLM datasets by first generating a user
query, followed by complex tool-use annotations like DFS. This leads to
inevitable annotation failures and low efficiency in data generation. We
introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad
first constructs valid tool-use chains through an iterative process guided by
textual "gradients", and then synthesizes corresponding user queries. This
"answer-first" approach led to ToolGrad-5k, a dataset generated with more
complex tool use, lower cost, and 100% pass rate. Experiments show that models
trained on ToolGrad-5k outperform those on expensive baseline datasets and
proprietary LLMs, even on OOD benchmarks.

</details>


### [26] [GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2508.04088)
*Jianghangfan Zhang,Yibo Yan,Kening Zheng,Xin Zou,Song Dai,Xuming Hu*

Main category: cs.CL

TL;DR: GM-PRM是一种新的多模态过程奖励模型，能够主动纠正推理错误，提升多步数学推理的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态PRM仅能识别错误而无法纠正的问题，提升复杂数学推理的准确性和解释性。

Method: 提出GM-PRM模型，提供细粒度分析和错误纠正能力，并引入Refined-BoN推理策略。

Result: 在多个多模态数学基准测试中达到最优效果，仅需20K样本训练数据。

Conclusion: GM-PRM显著提升了推理模型的性能和数据效率，具有广泛的应用潜力。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities
but often struggle with complex, multi-step mathematical reasoning, where minor
errors in visual perception or logical deduction can lead to complete failure.
While Process Reward Models (PRMs) offer step-by-step supervision, existing
multimodal PRMs are limited to being binary verifiers that can identify but not
correct errors, offering little explanatory power. To address these
deficiencies, we introduce the Generative Multimodal Process Reward Model
(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an
active reasoning collaborator. Instead of a simple scalar score, GM-PRM
provides a fine-grained, interpretable analysis of each reasoning step,
evaluating its step intent, visual alignment, and logical soundness. More
critically, GM-PRM is trained to generate a corrected version of the first
erroneous step it identifies. This unique corrective capability enables our new
test-time inference strategy, Refined Best-of-N (Refined-BoN). This framework
actively enhances solution quality by using the PRM's generated correction to
guide the policy model toward a more promising reasoning trajectory, thereby
improving the diversity and correctness of the solution pool. We demonstrate
that GM-PRM achieves state-of-the-art results on multiple multimodal math
benchmarks, significantly boosting policy model performance with remarkable
data efficiency, requiring only a 20K-sample training dataset. Our code will be
released upon acceptance.

</details>


### [27] [Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks](https://arxiv.org/abs/2508.04117)
*Zhiwen Ruan,Yun Chen,Yutao Hou,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 研究发现，在LLM微调的特定阶段会出现过度记忆现象，导致模型在测试集上表现高困惑度但高准确度，影响鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLM微调过程中的学习动态，揭示过度记忆现象及其影响。

Method: 通过实验分析微调阶段、学习率等因素对LLM过度记忆的影响。

Result: 过度记忆导致模型鲁棒性下降、泛化能力差和生成多样性减少。

Conclusion: 研究强调LLM独特的学习动态，并建议在微调时选择合适的学习率和检查点。

Abstract: The pretrained large language models (LLMs) are finetuned with labeled data
for better instruction following ability and alignment with human values. In
this paper, we study the learning dynamics of LLM finetuning on reasoning tasks
and reveal the uncovered over-memorization phenomenon during a specific stage
of LLM finetuning. At this stage, the LLMs have excessively memorized training
data and exhibit high test perplexity while maintaining good test accuracy. We
investigate the conditions that lead to LLM over-memorization and find that
training epochs and large learning rates contribute to this issue. Although
models with over-memorization demonstrate comparable test accuracy to normal
models, they suffer from reduced robustness, poor out-of-distribution
generalization, and decreased generation diversity. Our experiments unveil the
over-memorization to be broadly applicable across different tasks, models, and
finetuning methods. Our research highlights that overparameterized, extensively
finetuned LLMs exhibit unique learning dynamics distinct from traditional
machine learning models. Based on our observations of over-memorization, we
provide recommendations on checkpoint and learning rate selection during
finetuning.

</details>


### [28] [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)
*Xuan Qi,Rongwu Xu,Zhijing Jin*

Main category: cs.CL

TL;DR: 本文提出了一种基于难度的数据选择策略，通过选择具有较小DPO隐式奖励差距的偏好数据，提高数据效率和模型对齐性能。


<details>
  <summary>Details</summary>
Motivation: 对齐大型语言模型（LLM）与人类偏好是AI研究的关键挑战，现有方法依赖昂贵的大规模偏好数据集，缺乏高质量数据选择方法。

Method: 提出了一种基于DPO隐式奖励机制的难度数据选择策略，选择具有较小奖励差距的更具挑战性的偏好数据。

Result: 该方法在多个数据集和对齐任务中优于五种基线方法，仅需10%的原始数据即可实现更优性能。

Conclusion: 这种高效的数据选择方法为资源有限情况下扩展LLM对齐提供了有前景的解决方案。

Abstract: Aligning large language models (LLMs) with human preferences is a critical
challenge in AI research. While methods like Reinforcement Learning from Human
Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they
often rely on large, costly preference datasets. The current work lacks methods
for high-quality data selection specifically for preference data. In this work,
we introduce a novel difficulty-based data selection strategy for preference
datasets, grounded in the DPO implicit reward mechanism. By selecting
preference data examples with smaller DPO implicit reward gaps, which are
indicative of more challenging cases, we improve data efficiency and model
alignment. Our approach consistently outperforms five strong baselines across
multiple datasets and alignment tasks, achieving superior performance with only
10\% of the original data. This principled, efficient selection method offers a
promising solution for scaling LLM alignment with limited resources.

</details>


### [29] [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)
*Praveen Srinivasa Varadhan,Sherry Thomas,Sai Teja M. S.,Suvrat Bhooshan,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 论文提出Human Fooling Rate (HFR)指标，评估TTS系统是否能欺骗人类，发现商业模型接近人类水平，开源系统仍有差距。


<details>
  <summary>Details</summary>
Motivation: 探讨当前TTS系统是否能在类似图灵测试的评估中欺骗人类，揭示现有主观评估的局限性。

Method: 引入HFR指标，大规模评估开源和商业TTS模型，分析其在欺骗测试中的表现。

Result: 商业模型在零样本设置下接近人类水平，开源系统在自然对话语音上仍有不足；高质量数据微调提升真实感但未完全弥补差距。

Conclusion: 需结合更真实、以人为中心的评估方法，补充现有主观测试。

Abstract: While subjective evaluations in recent years indicate rapid progress in TTS,
can current TTS systems truly pass a human deception test in a Turing-like
evaluation? We introduce Human Fooling Rate (HFR), a metric that directly
measures how often machine-generated speech is mistaken for human. Our
large-scale evaluation of open-source and commercial TTS models reveals
critical insights: (i) CMOS-based claims of human parity often fail under
deception testing, (ii) TTS progress should be benchmarked on datasets where
human speech achieves high HFRs, as evaluating against monotonous or less
expressive reference samples sets a low bar, (iii) Commercial models approach
human deception in zero-shot settings, while open-source systems still struggle
with natural conversational speech; (iv) Fine-tuning on high-quality data
improves realism but does not fully bridge the gap. Our findings underscore the
need for more realistic, human-centric evaluations alongside existing
subjective tests.

</details>


### [30] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)
*Francisco Bolaños,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: 论文提出了一种新的标注模式以支持文献综述生成，并评估了多种大语言模型在分类修辞角色上的表现，展示了高质量数据和微调对性能的提升。


<details>
  <summary>Details</summary>
Motivation: 通过标注科学文献中的句子修辞角色，可以提升AI分析能力并支持高质量文献综述的生成，但需要有效的标注模式和策略。

Method: 1) 设计新的标注模式；2) 评估37种大语言模型在分类任务中的表现，包括零样本学习和微调；3) 使用专家标注和自动标注的数据集Sci-Sentence。

Result: 微调后的LLMs表现优异（F1>96%），GPT-4o效果最佳，轻量开源模型也有出色表现；半合成数据提升小模型性能。

Conclusion: 高质量数据和微调显著提升LLMs在修辞角色分类任务中的表现，为文献综述生成系统提供了新思路。

Abstract: Previous work has demonstrated that AI methods for analysing scientific
literature benefit significantly from annotating sentences in papers according
to their rhetorical roles, such as research gaps, results, limitations,
extensions of existing methodologies, and others. Such representations also
have the potential to support the development of a new generation of systems
capable of producing high-quality literature reviews. However, achieving this
goal requires the definition of a relevant annotation schema and effective
strategies for large-scale annotation of the literature. This paper addresses
these challenges by 1) introducing a novel annotation schema specifically
designed to support literature review generation and 2) conducting a
comprehensive evaluation of a wide range of state-of-the-art large language
models (LLMs) in classifying rhetorical roles according to this schema. To this
end, we also present Sci-Sentence, a novel multidisciplinary benchmark
comprising 700 sentences manually annotated by domain experts and 2,240
sentences automatically labelled using LLMs. We evaluate 37 LLMs on this
benchmark, spanning diverse model families and sizes, using both zero-shot
learning and fine-tuning approaches. The experiments yield several novel
insights that advance the state of the art in this challenging domain. First,
the current generation of LLMs performs remarkably well on this task when
fine-tuned on high-quality data, achieving performance levels above 96\% F1.
Second, while large proprietary models like GPT-4o achieve the best results,
some lightweight open-source alternatives also demonstrate excellent
performance. Finally, enriching the training data with semi-synthetic examples
generated by LLMs proves beneficial, enabling small encoders to achieve robust
results and significantly enhancing the performance of several open decoder
models.

</details>


### [31] [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)
*Peizheng Guo,Jingyao Wang,Wenwen Qiang,Huijie Guo,Changwen Zheng,Jiahuan Zhou,Gang Hua*

Main category: cs.CL

TL;DR: 论文提出了一种基于因果完整性的强化学习框架，用于减少多模态大语言模型（MLLMs）中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: MLLMs在视觉语言任务中表现优异，但存在幻觉问题（输出与输入不一致），需要解决。

Method: 通过因果分析，提出一个强化学习框架，结合因果充分性和必要性，定义令牌级因果完整性奖励，并在GRPO优化框架中使用。

Result: 实验证明该方法有效减少了MLLMs中的幻觉现象。

Conclusion: 提出的因果完整性框架能显著提升MLLMs的生成准确性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across vision-language tasks. However, they may suffer from
hallucinations--generating outputs that are semantically inconsistent with the
input image or text. Through causal analyses, we find that: (i) hallucinations
with omission may arise from the failure to adequately capture essential causal
factors, and (ii) hallucinations with fabrication are likely caused by the
model being misled by non-causal cues. To address these challenges, we propose
a novel reinforcement learning framework guided by causal completeness, which
jointly considers both causal sufficiency and causal necessity of tokens.
Specifically, we evaluate each token's standalone contribution and
counterfactual indispensability to define a token-level causal completeness
reward. This reward is used to construct a causally informed advantage function
within the GRPO optimization framework, encouraging the model to focus on
tokens that are both causally sufficient and necessary for accurate generation.
Experimental results across various benchmark datasets and tasks demonstrate
the effectiveness of our approach, which effectively mitigates hallucinations
in MLLMs.

</details>


### [32] [Characterizing Deep Research: A Benchmark and Formal Definition](https://arxiv.org/abs/2508.04183)
*Abhinav Java,Ashmit Khandelwal,Sukruta Midigeshi,Aaron Halfaker,Amit Deshpande,Navin Goyal,Ankur Gupta,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: 论文提出了深度研究（DR）任务的正式定义，并引入了一个评估DR系统性能的基准。核心特征是概念的高覆盖范围，而非长报告输出。


<details>
  <summary>Details</summary>
Motivation: 深度研究任务的定义模糊，与其他推理密集型问题的区别不明确，需要明确其范围和评估方法。

Method: 通过中间输出表示定义DR任务，分离推理挑战与报告生成，并构建LiveDRBench基准，包含100个任务。

Result: 现有DR系统的F1分数在0.02到0.72之间，OpenAI模型表现最佳（F1=0.55）。分析揭示了搜索机制和基础能力的改进方向。

Conclusion: 论文为深度研究任务提供了清晰定义和评估基准，揭示了当前系统的局限性，并提出了未来改进方向。

Abstract: Information tasks such as writing surveys or analytical reports require
complex search and reasoning, and have recently been grouped under the umbrella
of \textit{deep research} -- a term also adopted by recent models targeting
these capabilities. Despite growing interest, the scope of the deep research
task remains underdefined and its distinction from other reasoning-intensive
problems is poorly understood. In this paper, we propose a formal
characterization of the deep research (DR) task and introduce a benchmark to
evaluate the performance of DR systems. We argue that the core defining feature
of deep research is not the production of lengthy report-style outputs, but
rather the high fan-out over concepts required during the search process, i.e.,
broad and reasoning-intensive exploration. To enable objective evaluation, we
define DR using an intermediate output representation that encodes key claims
uncovered during search-separating the reasoning challenge from surface-level
report generation. Based on this formulation, we propose a diverse, challenging
benchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,
datasets, materials discovery, prior art search) and public interest events
(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1
score ranges between 0.02 and 0.72 for any sub-category. OpenAI's model
performs the best with an overall F1 score of 0.55. Analysis of reasoning
traces reveals the distribution over the number of referenced sources,
branching, and backtracking events executed by current DR systems, motivating
future directions for improving their search mechanisms and grounding
capabilities. The benchmark is available at
https://github.com/microsoft/LiveDRBench.

</details>


### [33] [Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)
*Siddhant Panpatil,Hiskias Dingeto,Haon Park*

Main category: cs.CL

TL;DR: 研究发现，即使是最先进的AI语言模型，仍易受精心设计的对话场景诱导，导致行为失准。通过手动测试和自动化评估框架MISALIGNMENTBENCH，揭示了模型在叙事沉浸、情感压力等方面的漏洞。


<details>
  <summary>Details</summary>
Motivation: 探讨当前语言模型对齐技术的局限性，揭示其在复杂对话场景中的脆弱性。

Method: 通过手动红队测试（Claude-4-Opus）发现10种攻击场景，并开发自动化评估框架MISALIGNMENTBENCH进行跨模型验证。

Result: 测试显示模型平均脆弱性为76%，其中GPT-4.1最易受攻击（90%），Claude-4-Sonnet表现最佳（40%）。

Conclusion: 当前对齐策略存在重大缺陷，未来需增强对复杂场景操纵的鲁棒性。

Abstract: Despite significant advances in alignment techniques, we demonstrate that
state-of-the-art language models remain vulnerable to carefully crafted
conversational scenarios that can induce various forms of misalignment without
explicit jailbreaking. Through systematic manual red-teaming with
Claude-4-Opus, we discovered 10 successful attack scenarios, revealing
fundamental vulnerabilities in how current alignment methods handle narrative
immersion, emotional pressure, and strategic framing. These scenarios
successfully elicited a range of misaligned behaviors, including deception,
value drift, self-preservation, and manipulative reasoning, each exploiting
different psychological and contextual vulnerabilities. To validate
generalizability, we distilled our successful manual attacks into
MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible
testing across multiple models. Cross-model evaluation of our 10 scenarios
against five frontier LLMs revealed an overall 76% vulnerability rate, with
significant variations: GPT-4.1 showed the highest susceptibility (90%), while
Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate
that sophisticated reasoning capabilities often become attack vectors rather
than protective mechanisms, as models can be manipulated into complex
justifications for misaligned behavior. This work provides (i) a detailed
taxonomy of conversational manipulation patterns and (ii) a reusable evaluation
framework. Together, these findings expose critical gaps in current alignment
strategies and highlight the need for robustness against subtle, scenario-based
manipulation in future AI systems.

</details>


### [34] [Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts](https://arxiv.org/abs/2508.04199)
*Millicent Ochieng,Anja Thieme,Ignatius Ezeani,Risa Ueno,Samuel Maina,Keshet Ronen,Javier Gonzalez,Jacki O'Neill*

Main category: cs.CL

TL;DR: 论文提出了一种诊断框架，将情感视为依赖于文化和上下文的构建，并评估了大语言模型（LLMs）在分析内罗毕青年健康群组的非正式、混合语言WhatsApp消息时的表现。通过人类标注数据、情感反转反事实和基于规则的评估，研究了LLMs的可解释性、鲁棒性和与人类推理的一致性。研究发现顶级LLMs表现稳定，而开源模型在模糊或情感变化时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 传统NLP方法假设固定标签和普遍情感表达，但在低资源、文化多样化的背景下表现不佳。论文旨在探索情感作为文化和上下文依赖的构建，并评估LLMs在此类复杂场景中的表现。

Method: 使用人类标注数据、情感反转反事实和基于规则的评估方法，从社会科学的测量角度评估LLMs的输出。

Result: 研究发现顶级LLMs在解释性和鲁棒性上表现稳定，而开源模型在模糊或情感变化时表现较差。

Conclusion: 论文强调了在复杂现实世界通信中，需要开发文化敏感、具备推理能力的AI评估方法。

Abstract: Sentiment analysis in low-resource, culturally nuanced contexts challenges
conventional NLP approaches that assume fixed labels and universal affective
expressions. We present a diagnostic framework that treats sentiment as a
context-dependent, culturally embedded construct, and evaluate how large
language models (LLMs) reason about sentiment in informal, code-mixed WhatsApp
messages from Nairobi youth health groups. Using a combination of
human-annotated data, sentiment-flipped counterfactuals, and rubric-based
explanation evaluation, we probe LLM interpretability, robustness, and
alignment with human reasoning. Framing our evaluation through a social-science
measurement lens, we operationalize and interrogate LLMs outputs as an
instrument for measuring the abstract concept of sentiment. Our findings reveal
significant variation in model reasoning quality, with top-tier LLMs
demonstrating interpretive stability, while open models often falter under
ambiguity or sentiment shifts. This work highlights the need for culturally
sensitive, reasoning-aware AI evaluation in complex, real-world communication.

</details>


### [35] [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)
*Yuquan Wang,Mi Zhang,Yining Wang,Geng Hong,Xiaoyu You,Min Yang*

Main category: cs.CL

TL;DR: 提出了一种名为ReasoningGuard的推理时保护机制，用于防止大型推理模型在推理过程中生成有害内容，无需额外微调或专家知识。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理任务中表现优异，但在推理过程中容易生成有害内容，现有防御机制依赖高成本微调和专家知识，难以扩展。

Method: 利用模型的内部注意力行为识别关键推理点，触发安全导向的反思，并在解码阶段采用缩放采样策略选择最优推理路径。

Result: ReasoningGuard有效缓解了三种针对大型推理模型的越狱攻击，优于七种现有防御机制，同时避免过度安全化问题。

Conclusion: ReasoningGuard是一种高效、可扩展的推理时保护机制，显著提升了大型推理模型的安全性。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance in
reasoning-intensive tasks, but they remain vulnerable to harmful content
generation, particularly in the mid-to-late steps of their reasoning processes.
Existing defense mechanisms, however, rely on costly fine-tuning and additional
expert knowledge, which restricts their scalability. In this work, we propose
ReasoningGuard, an inference-time safeguard for LRMs, which injects timely
safety aha moments to steer harmless while helpful reasoning processes.
Leveraging the model's internal attention behavior, our approach accurately
identifies critical points in the reasoning path, and triggers spontaneous,
safety-oriented reflection. To safeguard both the subsequent reasoning steps
and the final answers, we further implement a scaling sampling strategy during
the decoding phase, selecting the optimal reasoning path. Inducing minimal
extra inference cost, ReasoningGuard effectively mitigates three types of
jailbreak attacks, including the latest ones targeting the reasoning process of
LRMs. Our approach outperforms seven existing safeguards, achieving
state-of-the-art safety defenses while effectively avoiding the common
exaggerated safety issues.

</details>


### [36] [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)
*Kosuke Yoshimura,Hisashi Kashima*

Main category: cs.CL

TL;DR: 研究探讨了使用黑盒大语言模型（LLMs）进行层次文本分类（HTC）的可行性，比较了三种提示策略在零样本和少样本设置下的表现，发现少样本设置能提升准确性，但需权衡计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统HTC方法需要大量标注数据和计算资源，而黑盒LLMs可能提供更高效的替代方案。

Method: 评估了三种提示策略（DL、DH、TMH）在零样本和少样本设置下的表现，并与传统机器学习模型对比。

Result: 少样本设置显著提升准确性；LLMs在深层标签层次上表现优于传统模型，但API成本较高。

Conclusion: 黑盒LLMs在HTC中具有潜力，但需谨慎选择提示策略以平衡性能和成本。

Abstract: Hierarchical Text Classification (HTC) aims to assign texts to structured
label hierarchies; however, it faces challenges due to data scarcity and model
complexity. This study explores the feasibility of using black box Large
Language Models (LLMs) accessed via APIs for HTC, as an alternative to
traditional machine learning methods that require extensive labeled data and
computational resources. We evaluate three prompting strategies -- Direct Leaf
Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down
Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and
few-shot settings, comparing the accuracy and cost-effectiveness of these
strategies. Experiments on two datasets show that a few-shot setting
consistently improves classification accuracy compared to a zero-shot setting.
While a traditional machine learning model achieves high accuracy on a dataset
with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the
machine learning model on a dataset with a deeper hierarchy. API costs increase
significantly due to the higher input tokens required for deeper label
hierarchies on DH strategy. These results emphasize the trade-off between
accuracy improvement and the computational cost of prompt strategy. These
findings highlight the potential of black box LLMs for HTC while underscoring
the need to carefully select a prompt strategy to balance performance and cost.

</details>


### [37] [DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting](https://arxiv.org/abs/2508.04239)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.CL

TL;DR: DP-GPT4MTS是一种双提示大语言模型框架，通过结合显式和文本提示，有效整合时间序列中的文本信息，显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测模型忽略文本信息（如事件和新闻），而现有单提示框架难以有效处理时间戳文本语义，导致冗余信息影响性能。

Method: 提出DP-GPT4MTS框架，使用显式提示和文本提示结合，通过自注意力和前馈网络优化嵌入。

Result: 在多样化文本-数值时间序列数据集上，该框架优于现有最优算法。

Conclusion: 双提示机制通过整合文本上下文，显著提升了时间序列预测的准确性。

Abstract: Time series forecasting is crucial in strategic planning and decision-making
across various industries. Traditional forecasting models mainly concentrate on
numerical time series data, often overlooking important textual information
such as events and news, which can significantly affect forecasting accuracy.
While large language models offer a promise for integrating multimodal data,
existing single-prompt frameworks struggle to effectively capture the semantics
of timestamped text, introducing redundant information that can hinder model
performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt
GPT2-base for Multimodal Time Series), a novel dual-prompt large language model
framework that combines two complementary prompts: an explicit prompt for clear
task instructions and a textual prompt for context-aware embeddings from
time-stamped data. The tokenizer generates the explicit prompt while the
embeddings from the textual prompt are refined through self-attention and
feed-forward networks. Comprehensive experiments conducted on diverse
textural-numerical time series datasets demonstrate that this approach
outperforms state-of-the-art algorithms in time series forecasting. This
highlights the significance of incorporating textual context via a dual-prompt
mechanism to achieve more accurate time series predictions.

</details>


### [38] [TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening](https://arxiv.org/abs/2508.04248)
*Xi Wang,Anxo Perez,Javier Parapar,Fabio Crestani*

Main category: cs.CL

TL;DR: 提出了一种基于语言模型的临床医生参与的患者模拟管道TalkDep，用于生成多样且临床有效的模拟患者，以支持抑郁症诊断模型的训练和评估。


<details>
  <summary>Details</summary>
Motivation: 心理健康服务需求增长，但缺乏真实训练数据，现有模拟患者方法无法生成临床有效且多样化的症状表现。

Method: 利用先进语言模型，结合精神病诊断标准、症状严重程度量表和情境因素，开发了TalkDep管道。

Result: 通过临床专业人员验证，模拟患者可靠，为自动抑郁症诊断系统提供了可扩展和适应性强的资源。

Conclusion: TalkDep为抑郁症诊断模型的训练和评估提供了高效、可靠的模拟患者资源。

Abstract: The increasing demand for mental health services has outpaced the
availability of real training data to develop clinical professionals, leading
to limited support for the diagnosis of depression. This shortage has motivated
the development of simulated or virtual patients to assist in training and
evaluation, but existing approaches often fail to generate clinically valid,
natural, and diverse symptom presentations. In this work, we embrace the recent
advanced language models as the backbone and propose a novel
clinician-in-the-loop patient simulation pipeline, TalkDep, with access to
diversified patient profiles to develop simulated patients. By conditioning the
model on psychiatric diagnostic criteria, symptom severity scales, and
contextual factors, our goal is to create authentic patient responses that can
better support diagnostic model training and evaluation. We verify the
reliability of these simulated patients with thorough assessments conducted by
clinical professionals. The availability of validated simulated patients offers
a scalable and adaptable resource for improving the robustness and
generalisability of automatic depression diagnosis systems.

</details>


### [39] [KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs](https://arxiv.org/abs/2508.04257)
*Zunhai Su,Kehong Yuan*

Main category: cs.CL

TL;DR: 论文提出KVSink方法，通过预测注意力sink token优化KV缓存量化，提升LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存量化方法虽能减少内存使用，但对注意力sink的保护机制理解不足，且无法处理非初始位置的sink。

Method: 通过分析跨层极端激活异常值的作用机制，提出KVSink方法预测sink token，实现更全面的保护。

Result: KVSink优于现有PFN策略，在KVQuant方法中进一步降低PPL并减少对16位异常值的依赖。

Conclusion: KVSink为KV缓存量化提供了更高效的注意力sink保护方案。

Abstract: Key-Value (KV) cache quantization has become a widely adopted optimization
technique for efficient large language models (LLMs) inference by reducing KV
cache memory usage and mitigating memory-bound constraints. Recent studies have
emphasized the importance of preserving the original precision of KVs for the
first few tokens to ensure the protection of attention sinks. While this
approach has proven effective in mitigating performance degradation, its
underlying principles remain insufficiently understood. Moreover, it fails to
address the recent discovery that attention sinks can emerge beyond the initial
token positions. In this work, we elucidate the underlying mechanisms of
attention sinks during inference by examining their role in the cross-layer
evolution of extreme activation outliers. Additionally, we provide a
comprehensive analysis of the interplay between attention sinks and KV cache
quantization. Based on our enhanced understanding, we introduce
\textit{\textbf{KVSink}}, a plug-and-play method that effectively predicts sink
tokens with negligible overhead, enabling more thorough preservation. Extensive
experiments demonstrate that KVSink outperforms the existing Preserve-First-N
(PFN) strategy, offering more effective preservation of attention sinks during
KV cache quantization. Moreover, when applied to the well-established KVQuant
method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit
numerical outliers.

</details>


### [40] [ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents](https://arxiv.org/abs/2508.04266)
*Jiangyuan Wang,Kejun Xiao,Qi Sun,Huaipeng Zhao,Tao Luo,Jiandong Zhang,Xiaoyi Zeng*

Main category: cs.CL

TL;DR: ShoppingBench是一个新颖的端到端电商基准测试，专注于复杂用户意图，如使用优惠券、管理预算等，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有电商基准测试主要关注基本用户意图（如查找或购买商品），而忽略了更复杂的用户目标。

Method: 提出一个可扩展框架，模拟基于真实世界产品的用户指令，并构建了一个包含250万商品的交互式模拟环境。

Result: 实验显示，即使是GPT-4.1等先进语言代理，在ShoppingBench上的成功率也低于50%。

Conclusion: 通过轨迹蒸馏和监督微调，训练的小型代理性能接近GPT-4.1，证明了ShoppingBench的挑战性和方法的有效性。

Abstract: Existing benchmarks in e-commerce primarily focus on basic user intents, such
as finding or purchasing products. However, real-world users often pursue more
complex goals, such as applying vouchers, managing budgets, and finding
multi-products seller. To bridge this gap, we propose ShoppingBench, a novel
end-to-end shopping benchmark designed to encompass increasingly challenging
levels of grounded intent. Specifically, we propose a scalable framework to
simulate user instructions based on various intents derived from sampled
real-world products. To facilitate consistent and reliable evaluations, we
provide a large-scale shopping sandbox that serves as an interactive simulated
environment, incorporating over 2.5 million real-world products. Experimental
results demonstrate that even state-of-the-art language agents (such as
GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,
highlighting the significant challenges posed by our ShoppingBench. In
addition, we propose a trajectory distillation strategy and leverage supervised
fine-tuning, along with reinforcement learning on synthetic trajectories, to
distill the capabilities of a large language agent into a smaller one. As a
result, our trained agent achieves competitive performance compared to GPT-4.1.

</details>


### [41] [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)
*Jiayi Wen,Tianxin Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CL

TL;DR: GraphRAG通过知识图增强LLMs，但易受知识投毒攻击。本文提出两种攻击方法（TKPA和UKPA），显著影响下游推理，且现有防御方法无效。


<details>
  <summary>Details</summary>
Motivation: GraphRAG依赖LLMs从文本构建知识图，但此过程可能被恶意操纵植入误导信息，需研究其安全性。

Method: 提出两种知识投毒攻击（TKPA和UKPA）：TKPA通过图论分析定位脆弱节点并重写文本；UKPA利用语言线索破坏图结构。

Result: TKPA成功率达93.1%，UKPA仅修改0.05%文本即可使QA准确率从95%降至50%，现有防御方法无效。

Conclusion: GraphRAG管道在知识投毒攻击下存在严重漏洞，安全性研究亟待加强。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as
a promising paradigm for enhancing large language models (LLMs) by converting
raw text into structured knowledge graphs, improving both accuracy and
explainability. However, GraphRAG relies on LLMs to extract knowledge from raw
text during graph construction, and this process can be maliciously manipulated
to implant misleading information. Targeting this attack surface, we propose
two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a
few words in the source text can significantly change the constructed graph,
poison the GraphRAG, and severely mislead downstream reasoning. The first
attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate
vulnerable nodes in the generated graphs and rewrites the corresponding
narratives with LLMs, achieving precise control over specific
question-answering (QA) outcomes with a success rate of 93.1\%, while keeping
the poisoned text fluent and natural. The second attack, named Universal KPA
(UKPA), exploits linguistic cues such as pronouns and dependency relations to
disrupt the structural integrity of the generated graph by altering globally
influential words. With fewer than 0.05\% of full text modified, the QA
accuracy collapses from 95\% to 50\%. Furthermore, experiments show that
state-of-the-art defense methods fail to detect these attacks, highlighting
that securing GraphRAG pipelines against knowledge poisoning remains largely
unexplored.

</details>


### [42] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: MedCheck是一个针对医疗基准测试的生命周期评估框架，解决了现有基准测试在临床保真度、数据管理和安全性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有医疗基准测试在可靠性、临床实践关联性和安全性方面存在问题，亟需改进。

Method: MedCheck将基准测试开发分为五个阶段，提供46项医学定制标准，并对53个医疗LLM基准进行了评估。

Result: 分析揭示了广泛存在的系统性问题，如临床实践脱节、数据完整性危机和安全性维度忽视。

Conclusion: MedCheck可作为诊断工具和行动指南，推动医疗AI评估的标准化和可靠性。

Abstract: Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.

</details>


### [43] [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
*Hongze Tan,Jianfei Pan*

Main category: cs.CL

TL;DR: 论文提出动态熵加权方法（Dynamic Entropy Weighting），通过细粒度奖励分配提升强化学习在长链推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在长链推理任务中因粗粒度信用分配（对所有令牌应用统一奖励）而受限。

Method: 1) GTPO：为每个令牌分配熵加权奖励；2) GRPO-S：为每个序列分配基于平均令牌熵的奖励。

Result: 实验显示，该方法显著优于DAPO基线，熵加权机制是性能提升的关键。

Conclusion: 动态熵加权为增强模型深度推理提供了更优路径。

Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.

</details>


### [44] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
*Nima Iji,Kia Dashtipour*

Main category: cs.CL

TL;DR: 论文提出了一种名为Chain of Questions（CoQ）的好奇心驱动推理框架，旨在提升多模态语言模型在复杂环境中的主动感知和推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在推理能力上取得了显著进展，但其在多模态环境中的应用仍显不足，尤其是如何动态选择感知模态以获取关键信息。

Method: CoQ框架通过动态生成针对性问题，引导模型选择性地激活相关感知模态（如视觉、听觉等），从而收集必要信息以支持推理。

Result: 实验结果表明，CoQ方法在多模态基准数据集上显著提升了模型识别和整合感知信息的能力，进而提高了推理的准确性和可解释性。

Conclusion: CoQ框架为多模态语言模型的推理能力提供了一种有效方法，能够更好地适应复杂任务的需求。

Abstract: Reasoning capabilities in large language models (LLMs) have substantially
advanced through methods such as chain-of-thought and explicit step-by-step
explanations. However, these improvements have not yet fully transitioned to
multimodal contexts, where models must proactively decide which sensory
modalities such as vision, audio, or spatial perception to engage when
interacting with complex real-world environments. In this paper, we introduce
the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach
that encourages multimodal language models to dynamically generate targeted
questions regarding their surroundings. These generated questions guide the
model to selectively activate relevant modalities, thereby gathering critical
information necessary for accurate reasoning and response generation. We
evaluate our framework on a novel multimodal benchmark dataset, assembled by
integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results
demonstrate that our CoQ method improves a foundation model's ability to
effectively identify and integrate pertinent sensory information. This leads to
improved accuracy, interpretability, and alignment of the reasoning process
with diverse multimodal tasks.

</details>


### [45] [AIC CTU@FEVER 8: On-premise fact checking through long context RAG](https://arxiv.org/abs/2508.04390)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 本文介绍了一个在FEVER 8共享任务中排名第一的事实核查系统，采用两步RAG流程，并在资源受限条件下实现高性能。


<details>
  <summary>Details</summary>
Motivation: 提升事实核查系统的性能，并在资源受限环境下实现高效部署。

Method: 采用基于去年提交的两步RAG流程，优化后可在单GPU和有限内存下运行。

Result: 在Ev2R测试分数上达到最先进水平，运行时间控制在60秒内。

Conclusion: 该系统在资源受限条件下仍能实现高性能，展示了RAG流程的实用性和可扩展性。

Abstract: In this paper, we present our fact-checking pipeline which has scored first
in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG
pipeline based on our last year's submission. We show how the pipeline can be
redeployed on-premise, achieving state-of-the-art fact-checking performance (in
sense of Ev2R test-score), even under the constraint of a single NVidia A10
GPU, 23GB of graphical memory and 60s running time per claim.

</details>


### [46] [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)
*Xu Zhang,Mei Chen*

Main category: cs.CL

TL;DR: 研究评估了三种NLP模型（零样本开源LLM、微调Transformer和传统逻辑回归）在提升事故数据质量方面的表现，发现微调Transformer（如RoBERTa）性能最优，但LLM在某些方面表现突出，需权衡计算成本。


<details>
  <summary>Details</summary>
Motivation: 通过NLP技术挖掘事故叙述数据，提升事故数据质量，尤其是二次事故识别。

Method: 使用16,656条手动审核的事故叙述数据（2015-2022），比较零样本LLM、微调Transformer和逻辑回归模型的性能。

Result: 微调Transformer（RoBERTa）表现最佳（F1:0.90，准确率95%），LLM（如LLaMA3:70B）在召回率上表现优异但计算成本高，逻辑回归表现最差（F1:0.66）。

Conclusion: 微调Transformer在精度和召回率上表现平衡，适合实际部署；LLM需优化计算成本；研究为事故数据质量提升提供了可复制的方案。

Abstract: This study evaluates advanced natural language processing (NLP) techniques to
enhance crash data quality by mining crash narratives, using secondary crash
identification in Kentucky as a case study. Drawing from 16,656 manually
reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we
compare three model classes: zero-shot open-source large language models (LLMs)
(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers
(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic
regression as baseline. Models were calibrated on 2015-2021 data and tested on
1,771 narratives from 2022. Fine-tuned transformers achieved superior
performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy
(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139
minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs
excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred
high computational costs (up to 723 minutes for DeepSeek-R1:70B), while
fine-tuned models processed the test set in seconds after brief training.
Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can
rival larger counterparts in performance while reducing runtime, suggesting
opportunities for optimized deployments. Results highlight trade-offs between
accuracy, efficiency, and data requirements, with fine-tuned transformer models
balancing precision and recall effectively on Kentucky data. Practical
deployment considerations emphasize privacy-preserving local deployment,
ensemble approaches for improved accuracy, and incremental processing for
scalability, providing a replicable scheme for enhancing crash-data quality
with advanced NLP.

</details>


### [47] [Why are LLMs' abilities emergent?](https://arxiv.org/abs/2508.04401)
*Vladimír Havlík*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）的涌现能力，指出这些能力源于深度神经网络（DNNs）的非线性、随机过程，而非简单的参数扩展。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决当代AI发展中“创造而不理解”的认识论挑战，探索DNNs的涌现性质。

Method: 通过理论分析和实证观察，结合扩展定律、grokking现象和模型能力的相变分析。

Result: 发现涌现能力源于高度敏感的非线性系统的复杂动态，而非单纯参数扩展。

Conclusion: 理解LLMs需将DNNs视为受涌现普遍原则支配的复杂动态系统，类似于物理、化学和生物学中的现象。

Abstract: The remarkable success of Large Language Models (LLMs) in generative tasks
has raised fundamental questions about the nature of their acquired
capabilities, which often appear to emerge unexpectedly without explicit
training. This paper examines the emergent properties of Deep Neural Networks
(DNNs) through both theoretical analysis and empirical observation, addressing
the epistemological challenge of "creation without understanding" that
characterises contemporary AI development. We explore how the neural approach's
reliance on nonlinear, stochastic processes fundamentally differs from symbolic
computational paradigms, creating systems whose macro-level behaviours cannot
be analytically derived from micro-level neuron activities. Through analysis of
scaling laws, grokking phenomena, and phase transitions in model capabilities,
I demonstrate that emergent abilities arise from the complex dynamics of highly
sensitive nonlinear systems rather than simply from parameter scaling alone. My
investigation reveals that current debates over metrics, pre-training loss
thresholds, and in-context learning miss the fundamental ontological nature of
emergence in DNNs. I argue that these systems exhibit genuine emergent
properties analogous to those found in other complex natural phenomena, where
systemic capabilities emerge from cooperative interactions among simple
components without being reducible to their individual behaviours. The paper
concludes that understanding LLM capabilities requires recognising DNNs as a
new domain of complex dynamical systems governed by universal principles of
emergence, similar to those operating in physics, chemistry, and biology. This
perspective shifts the focus from purely phenomenological definitions of
emergence to understanding the internal dynamic transformations that enable
these systems to acquire capabilities that transcend their individual
components.

</details>


### [48] [What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems](https://arxiv.org/abs/2508.04402)
*Kiyotada Mori,Seiya Kawano,Chaoran Liu,Carlos Toshinori Ishi,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 论文探讨了如何通过研究人类的选择性听力来改进语音对话系统中的自动语音识别（ASR）评估方法。


<details>
  <summary>Details</summary>
Motivation: 研究人类选择性听力有助于确定ASR在语音对话系统中所需的能力，并评估其表现。

Method: 通过比较人类生成对话响应时的转录与参考转录，实验验证了人类的选择性听力。

Result: 实验结果表明，人类在生成对话响应时表现出选择性听力。

Conclusion: 基于实验结果，提出了一种新的ASR评估方法，利用人类选择性听力来识别ASR系统与人类转录能力之间的差距。

Abstract: Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at
the front end of their pipeline. The role of ASR in SDSs is to recognize
information in user speech related to response generation appropriately.
Examining selective listening of humans, which refers to the ability to focus
on and listen to important parts of a conversation during the speech, will
enable us to identify the ASR capabilities required for SDSs and evaluate them.
In this study, we experimentally confirmed selective listening when humans
generate dialogue responses by comparing human transcriptions for generating
dialogue responses and reference transcriptions. Based on our experimental
results, we discuss the possibility of a new ASR evaluation method that
leverages human selective listening, which can identify the gap between
transcription ability between ASR systems and humans.

</details>


### [49] [Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model](https://arxiv.org/abs/2508.04403)
*Kiyotada Mori,Seiya Kawano,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 提出了一种预测置信度模型（PCM），通过估计预测的用户完整话语与实际话语的语义相似性，决定是否预取对话响应以减少用户感知延迟（UPL）。


<details>
  <summary>Details</summary>
Motivation: 减少用户感知延迟（UPL），即用户在收到系统响应前的等待时间。

Method: 提出预测置信度模型（PCM），评估预测的完整用户话语与实际话语的语义相似性，以决定是否预取响应。

Result: 通过PCM评估预测与实际话语的差异，验证了其有效性。

Conclusion: PCM能够有效判断预取响应的可行性，从而降低UPL。

Abstract: Prefetching of dialogue responses has been investigated to reduce
user-perceived latency (UPL), which refers to the user's waiting time before
receiving the system's response, in spoken dialogue systems. To reduce the UPL,
it is necessary to predict complete user utterances before the end of the
user's speech, typically by language models, to prepare prefetched dialogue
responses. In this study, we proposed a prediction confidence model (PCM) that
determines whether prefetching is possible or not by estimating the semantic
similarity between the predicted complete user utterance and the complete user
utterance. We evaluated our PCM based on the differences between the predicted
complete user utterance and the complete user utterance.

</details>


### [50] [Evaluating, Synthesizing, and Enhancing for Customer Support Conversation](https://arxiv.org/abs/2508.04423)
*Jie Zhu,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 论文提出了一种基于COPC指南的客户支持对话（CSC）框架，定义了五个对话阶段和十二种策略，并构建了CSConv评估数据集和RoleCS训练数据集，通过LLM微调显著提升了对话质量。


<details>
  <summary>Details</summary>
Motivation: 现有对话数据集缺乏策略指导，且真实服务数据难以获取和标注，因此需要开发结构化框架和数据集以提升客户支持质量。

Method: 提出CSC框架，基于COPC指南定义对话阶段和策略；构建CSConv数据集（1,855条对话）和RoleCS训练数据集（通过LLM角色扮演生成）。

Result: 实验表明，基于RoleCS微调的LLM能生成更高质量的策略对齐回复，人类评估也证实问题解决能力提升。

Conclusion: CSC框架和数据集有效提升了客户支持对话质量，未来将公开代码和数据。

Abstract: Effective customer support requires not only accurate problem solving but
also structured and empathetic communication aligned with professional
standards. However, existing dialogue datasets often lack strategic guidance,
and real-world service data is difficult to access and annotate. To address
this, we introduce the task of Customer Support Conversation (CSC), aimed at
training customer service agents to respond using well-defined support
strategies. We propose a structured CSC framework grounded in COPC guidelines,
defining five conversational stages and twelve strategies to guide high-quality
interactions. Based on this, we construct CSConv, an evaluation dataset of
1,855 real-world customer-agent conversations rewritten using LLMs to reflect
deliberate strategy use, and annotated accordingly. Additionally, we develop a
role-playing approach that simulates strategy-rich conversations using
LLM-powered roles aligned with the CSC framework, resulting in the training
dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS
significantly improves their ability to generate high-quality, strategy-aligned
responses on CSConv. Human evaluations further confirm gains in problem
resolution. All code and data will be made publicly available at
https://github.com/aliyun/qwen-dianjin.

</details>


### [51] [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)
*Yutong Wu,Di Huang,Ruosi Wan,Yue Peng,Shijie Shang,Chenrui Cao,Lei Qi,Rui Zhang,Zidong Du,Jie Yan,Xing Hu*

Main category: cs.CL

TL;DR: 论文提出ThinkingF方法，通过数据合成和训练流程提升自动形式化的能力，显著提高了模型在形式化任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法准确率低，需提升模型的形式语言领域知识和自然语言推理能力。

Method: 构建两个数据集：一个富含形式知识，另一个通过专家模板生成推理轨迹；采用SFT和RLVR训练。

Result: 7B和32B模型表现优异，StepFun-Formalizer-32B在FormalMATH-Lite和ProverBench上达到SOTA。

Conclusion: ThinkingF方法有效结合形式知识和推理能力，显著提升自动形式化性能。

Abstract: Autoformalization aims to translate natural-language mathematical statements
into a formal language. While LLMs have accelerated progress in this area,
existing methods still suffer from low accuracy. We identify two key abilities
for effective autoformalization: comprehensive mastery of formal-language
domain knowledge, and reasoning capability of natural language problem
understanding and informal-formal alignment. Without the former, a model cannot
identify the correct formal objects; without the latter, it struggles to
interpret real-world contexts and map them precisely into formal expressions.
To address these gaps, we introduce ThinkingF, a data synthesis and training
pipeline that improves both abilities. First, we construct two datasets: one by
distilling and selecting large-scale examples rich in formal knowledge, and
another by generating informal-to-formal reasoning trajectories guided by
expert-designed templates. We then apply SFT and RLVR with these datasets to
further fuse and refine the two abilities. The resulting 7B and 32B models
exhibit both comprehensive formal knowledge and strong informal-to-formal
reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%
on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior
general-purpose and specialized models.

</details>


### [52] [Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI](https://arxiv.org/abs/2508.04442)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.CL

TL;DR: 论文探讨了在马来西亚教育系统中利用生成式AI（GenAI）开发高质量、可扩展的教育评估工具，重点解决了低资源语言（如马来语）的准确性和课程对齐问题。


<details>
  <summary>Details</summary>
Motivation: 马来西亚教育系统亟需高质量的教育评估工具，尤其是针对低资源语言如马来语。生成式AI虽有潜力，但需解决准确性和课程对齐的挑战。

Method: 研究比较了四种生成马来语数学选择题（MCQ）的增量方法：非基础提示（结构化和基本）与检索增强生成（RAG）方法（基于LangChain框架和手动实现）。系统基于官方课程文件，采用双重自动评估框架。

Result: RAG方法显著优于非基础提示方法，生成的问题在课程对齐和事实有效性上表现更好。研究还分析了框架RAG与手动RAG的权衡。

Conclusion: 研究提供了一种验证的方法论，用于生成低资源语言的课程特定内容，并提出了RAG-QA评估技术，为马来西亚等地区的教育科技发展提供了实用见解。

Abstract: This paper addresses the critical need for scalable and high-quality
educational assessment tools within the Malaysian education system. It
highlights the potential of Generative AI (GenAI) while acknowledging the
significant challenges of ensuring factual accuracy and curriculum alignment,
especially for low-resource languages like Bahasa Melayu. This research
introduces and compares four incremental pipelines for generating Form 1
Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's
GPT-4o. The methods range from non-grounded prompting (structured and basic) to
Retrieval-Augmented Generation (RAG) approaches (one using the LangChain
framework, one implemented manually). The system is grounded in official
curriculum documents, including teacher-prepared notes and the yearly teaching
plan (RPT). A dual-pronged automated evaluation framework is employed to assess
the generated questions. Curriculum alignment is measured using Semantic
Textual Similarity (STS) against the RPT, while contextual validity is verified
through a novel RAG-based Question-Answering (RAG-QA) method. The results
demonstrate that RAG-based pipelines significantly outperform non-grounded
prompting methods, producing questions with higher curriculum alignment and
factual validity. The study further analyzes the trade-offs between the ease of
implementation of framework-based RAG and the fine-grained control offered by a
manual pipeline. This work presents a validated methodology for generating
curriculum-specific educational content in a low-resource language, introduces
a symbiotic RAG-QA evaluation technique, and provides actionable insights for
the development and deployment of practical EdTech solutions in Malaysia and
similar regions.

</details>


### [53] [CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation](https://arxiv.org/abs/2508.04494)
*Bastien Liétard,Gabriel Loiseau*

Main category: cs.CL

TL;DR: 本文提出了一种名为Concept Differentiation的扩展方法，用于在词义分析中引入跨词情境，并基于SemCor数据构建了数据集。通过微调生成Concept-Aligned Embeddings (CALE)，实验证明其在多任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有Word-in-Context任务仅比较同一词的不同语境，限制了语义信息的捕捉范围，因此需要扩展至跨词情境。

Method: 提出Concept Differentiation方法，构建数据集并微调多种表示模型（CALE）。

Result: CALE在多任务中表现最佳，且微调显著改变了嵌入的空间组织。

Conclusion: CALE提供了高效的多用途词义表示，扩展了现有方法的局限性。

Abstract: Lexical semantics is concerned with both the multiple senses a word can adopt
in different contexts, and the semantic relations that exist between meanings
of different words. To investigate them, Contextualized Language Models are a
valuable tool that provides context-sensitive representations that can be used
to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the
task of Word-in-Context to fine-tune them to get more semantically accurate
representations, but Word-in-Context only compares occurrences of the same
lemma, limiting the range of captured information. In this paper, we propose an
extension, Concept Differentiation, to include inter-words scenarios. We
provide a dataset for this task, derived from SemCor data. Then we fine-tune
several representation models on this dataset. We call these models
Concept-Aligned Embeddings (CALE). By challenging our models and other models
on various lexical semantic tasks, we demonstrate that the proposed models
provide efficient multi-purpose representations of lexical meaning that reach
best performances in our experiments. We also show that CALE's fine-tuning
brings valuable changes to the spatial organization of embeddings.

</details>


### [54] [StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering](https://arxiv.org/abs/2508.04530)
*Chenglei Shen,Zhongxiang Sun,Teng Shi,Xiao Zhang,Jun Xu*

Main category: cs.CL

TL;DR: StyliTruth通过正交放缩分离风格与真实性子空间，动态控制生成过程，显著减少风格化导致的真实性下降。


<details>
  <summary>Details</summary>
Motivation: 现有方法在注入风格信号时忽视了其对模型真实性的负面影响，导致风格化与真实性之间的权衡问题。

Method: 提出StyliTruth机制，通过正交放缩分离风格与真实性子空间，设计自适应token级导向向量动态控制生成。

Result: 实验表明，StyliTruth显著减少风格化导致的真实性下降，并在风格与真实性平衡上优于现有方法。

Conclusion: StyliTruth有效解决了风格化与真实性的冲突，为细粒度输出控制提供了新思路。

Abstract: Generating stylized large language model (LLM) responses via representation
editing is a promising way for fine-grained output control. However, there
exists an inherent trade-off: imposing a distinctive style often degrades
truthfulness. Existing representation editing methods, by naively injecting
style signals, overlook this collateral impact and frequently contaminate the
model's core truthfulness representations, resulting in reduced answer
correctness. We term this phenomenon stylization-induced truthfulness collapse.
We attribute this issue to latent coupling between style and truth directions
in certain key attention heads, and propose StyliTruth, a mechanism that
preserves stylization while keeping truthfulness intact. StyliTruth separates
the style-relevant and truth-relevant subspaces in the model's representation
space via an orthogonal deflation process. This decomposition enables
independent control of style and truth in their own subspaces, minimizing
interference. By designing adaptive, token-level steering vectors within each
subspace, we dynamically and precisely control the generation process to
maintain both stylistic fidelity and truthfulness. We validate our method on
multiple styles and languages. Extensive experiments and analyses show that
StyliTruth significantly reduces stylization-induced truthfulness collapse and
outperforms existing inference-time intervention methods in balancing style
adherence with truthfulness.

</details>


### [55] [Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning](https://arxiv.org/abs/2508.04531)
*Zhuang Chen,Guanqun Bi,Wen Zhang,Jiawei Hu,Aoyun Wang,Xiyao Xiao,Kun Feng,Minlie Huang*

Main category: cs.CL

TL;DR: 论文介绍了C-MIND数据集，用于临床抑郁症评估，分析了多模态数据对诊断的贡献，并探索了LLMs在精神病学推理中的表现及改进方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有抑郁症自动评估研究中数据有限且未临床验证的问题，提升模型在真实临床环境中的有效性。

Method: 使用C-MIND数据集，分析行为特征，训练经典模型评估任务和模态对诊断的影响，并改进LLMs的临床推理能力。

Result: 通过临床专业知识引导LLMs推理，诊断性能提升达10%（Macro-F1分数）。

Conclusion: C-MIND为临床抑郁症评估提供了数据和算法基础，支持心理健康领域的可靠研究。

Abstract: Depression is a widespread mental disorder that affects millions worldwide.
While automated depression assessment shows promise, most studies rely on
limited or non-clinically validated data, and often prioritize complex model
design over real-world effectiveness. In this paper, we aim to unveil the
landscape of clinical depression assessment. We introduce C-MIND, a clinical
neuropsychiatric multimodal diagnosis dataset collected over two years from
real hospital visits. Each participant completes three structured psychiatric
tasks and receives a final diagnosis from expert clinicians, with informative
audio, video, transcript, and functional near-infrared spectroscopy (fNIRS)
signals recorded. Using C-MIND, we first analyze behavioral signatures relevant
to diagnosis. We train a range of classical models to quantify how different
tasks and modalities contribute to diagnostic performance, and dissect the
effectiveness of their combinations. We then explore whether LLMs can perform
psychiatric reasoning like clinicians and identify their clear limitations in
realistic clinical settings. In response, we propose to guide the reasoning
process with clinical expertise and consistently improves LLM diagnostic
performance by up to 10% in Macro-F1 score. We aim to build an infrastructure
for clinical depression assessment from both data and algorithmic perspectives,
enabling C-MIND to facilitate grounded and reliable research for mental
healthcare.

</details>


### [56] [Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration](https://arxiv.org/abs/2508.04575)
*Nuo Chen,Yicheng Tong,Jiaying Wu,Minh Duc Duong,Qian Wang,Qingyun Zou,Bryan Hooi,Bingsheng He*

Main category: cs.CL

TL;DR: 多智能体讨论框架显著优于单智能体，认知多样性和专业知识是高质量创意的关键。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体讨论是否能超越单智能体，提升科学创意的创造力和质量。

Method: 提出合作多智能体框架，比较不同配置（如团队规模、领导结构、学科和资历多样性），并通过代理评分和人工评审评估创意质量。

Result: 多智能体讨论显著优于单智能体；领导角色能提升整合性和前瞻性；认知多样性是质量的主要驱动力，但专业知识不可或缺。

Conclusion: 多智能体框架在科学创意生成中更有效，团队结构和多样性对创意质量有重要影响。

Abstract: While AI agents show potential in scientific ideation, most existing
frameworks rely on single-agent refinement, limiting creativity due to bounded
knowledge and perspective. Inspired by real-world research dynamics, this paper
investigates whether structured multi-agent discussions can surpass solitary
ideation. We propose a cooperative multi-agent framework for generating
research proposals and systematically compare configurations including group
size, leaderled versus leaderless structures, and team compositions varying in
interdisciplinarity and seniority. To assess idea quality, we employ a
comprehensive protocol with agent-based scoring and human review across
dimensions such as novelty, strategic vision, and integration depth. Our
results show that multi-agent discussions substantially outperform solitary
baselines. A designated leader acts as a catalyst, transforming discussion into
more integrated and visionary proposals. Notably, we find that cognitive
diversity is a primary driver of quality, yet expertise is a non-negotiable
prerequisite, as teams lacking a foundation of senior knowledge fail to surpass
even a single competent agent. These findings offer actionable insights for
designing collaborative AI ideation systems and shed light on how team
structure influences creative outcomes.

</details>


### [57] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: MASA提出了一种跨Transformer层的结构化权重共享框架，通过共享字典原子减少参数，性能不降。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的高计算和内存需求限制了其广泛应用，现有压缩技术未充分探索层间冗余。

Method: 将注意力投影矩阵分解为共享字典原子，以线性组合表示各层权重，无需蒸馏或架构更改。

Result: 在多种规模实验中，MASA在相同参数预算下优于基线方法，并适用于视觉Transformer。

Conclusion: MASA为高效参数模型提供了可扩展方案，且适用于预训练模型。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


### [58] [TURA: Tool-Augmented Unified Retrieval Agent for AI Search](https://arxiv.org/abs/2508.04604)
*Zhejun Zhao,Yuehu Dong,Alley Liu,Lixue Zheng,Pingsheng Liu,Dongdong Shen,Long Xia,Jiashu Zhao,Dawei Yin*

Main category: cs.CL

TL;DR: 论文提出TURA框架，结合检索增强生成（RAG）与工具使用，解决传统RAG在动态实时查询中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法无法满足实时动态查询需求（如票务库存），研究旨在填补静态内容与动态信息源之间的鸿沟。

Method: TURA框架包含三部分：意图感知检索模块、DAG任务规划器和轻量级代理执行器，结合RAG与工具调用。

Result: TURA成功应用于大规模工业系统，服务数百万用户，提供实时低延迟答案。

Conclusion: TURA首次系统性地结合静态RAG与动态信息源，为AI搜索产品提供创新解决方案。

Abstract: The advent of Large Language Models (LLMs) is transforming search engines
into conversational AI search products, primarily using Retrieval-Augmented
Generation (RAG) on web corpora. However, this paradigm has significant
industrial limitations. Traditional RAG approaches struggle with real-time
needs and structured queries that require accessing dynamically generated
content like ticket availability or inventory. Limited to indexing static
pages, search engines cannot perform the interactive queries needed for such
time-sensitive data. Academic research has focused on optimizing RAG for static
content, overlooking complex intents and the need for dynamic sources like
databases and real-time APIs. To bridge this gap, we introduce TURA
(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage
framework that combines RAG with agentic tool-use to access both static content
and dynamic, real-time information. TURA has three key components: an
Intent-Aware Retrieval module to decompose queries and retrieve information
sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task
Planner that models task dependencies as a Directed Acyclic Graph (DAG) for
optimal parallel execution, and a lightweight Distilled Agent Executor for
efficient tool calling. TURA is the first architecture to systematically bridge
the gap between static RAG and dynamic information sources for a world-class AI
search product. Serving tens of millions of users, it leverages an agentic
framework to deliver robust, real-time answers while meeting the low-latency
demands of a large-scale industrial system.

</details>


### [59] [Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider](https://arxiv.org/abs/2508.04623)
*Chirag Seth,Utkarsh Singh*

Main category: cs.CL

TL;DR: 该研究评估了三种轻量级Transformer模型（T5-Small、BART-Small和GPT-2）在低资源设置下的Text-to-SQL翻译性能，发现T5-Small表现最佳。


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL翻译为非专家用户提供了通过自然语言查询数据库的能力，适用于教育和商业智能领域。研究旨在探索轻量级模型在低资源环境中的表现。

Method: 开发了一个可重用、模型无关的流程，针对每种模型架构调整模式格式，并在1000到5000次迭代中训练模型，使用LFAcc、BLEU和EM指标评估1000个测试样本。

Result: T5-Small在LFAcc上表现最佳（27.8%），优于BART-Small（23.98%）和GPT-2（20.1%），表明编码器-解码器模型在模式感知SQL生成中的优势。

Conclusion: 尽管资源限制影响了性能，但模块化流程支持未来改进，如高级模式链接或替代基础模型。研究强调了轻量级Transformer在资源稀缺环境中的潜力。

Abstract: Text-to-SQL translation enables non-expert users to query relational
databases using natural language, with applications in education and business
intelligence. This study evaluates three lightweight transformer models -
T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on
low-resource settings. We developed a reusable, model-agnostic pipeline that
tailors schema formatting to each model's architecture, training them across
1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form
Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small
achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2
(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL
generation. Despite resource constraints limiting performance, our pipeline's
modularity supports future enhancements, such as advanced schema linking or
alternative base models. This work underscores the potential of compact
transformers for accessible text-to-SQL solutions in resource-scarce
environments.

</details>


### [60] [P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis](https://arxiv.org/abs/2508.04626)
*Feifan Song,Bofei Gao,Yifan Song,Yi Liu,Weimin Xiong,Yuyang Song,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: P-Aligner是一种轻量级模块，通过预对齐指令提升LLMs的安全性和有用性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在交互中常因指令缺陷（如缺失上下文或模糊指示）而无法对齐人类偏好，需改进。

Method: 提出P-Aligner模块和UltraPrompt数据集，通过蒙特卡洛树搜索生成更符合人类偏好的指令。

Result: P-Aligner在GPT-4-turbo和Gemma-2-SimPO上分别提升28.35%和8.69%的胜率。

Conclusion: P-Aligner高效且有效，验证了其在数据质量、搜索策略等方面的优势。

Abstract: Large Language Models (LLMs) are expected to produce safe, helpful, and
honest content during interaction with human users, but they frequently fail to
align with such values when given flawed instructions, e.g., missing context,
ambiguous directives, or inappropriate tone, leaving substantial room for
improvement along multiple dimensions. A cost-effective yet high-impact way is
to pre-align instructions before the model begins decoding. Existing approaches
either rely on prohibitive test-time search costs or end-to-end model rewrite,
which is powered by a customized training corpus with unclear objectives. In
this work, we demonstrate that the goal of efficient and effective preference
alignment can be achieved by P-Aligner, a lightweight module generating
instructions that preserve the original intents while being expressed in a more
human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset
synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree
Search, which systematically explores the space of candidate instructions that
are closely tied to human preference. Experiments across different methods show
that P-Aligner generally outperforms strong baselines across various models and
benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo
and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness
and efficiency through multiple perspectives, including data quality, search
strategies, iterative deployment, and time overhead.

</details>


### [61] [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2508.04632)
*Xu Guo,Tianyi Liang,Tong Jian,Xiaogui Yang,Ling-I Wu,Chenhui Li,Zhihui Lu,Qipeng Guo,Kai Chen*

Main category: cs.CL

TL;DR: RLVR训练效率低且易过优化，IFDecorator框架通过渐进挑战、意图对齐和陷阱检测提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR在训练效率低和过优化问题，提升指令跟随能力。

Method: IFDecorator框架包含渐进数据生成、意图检查模块和陷阱检测机制。

Result: Qwen2.5-32B-Instruct-IFDecorator在IFEval上达87.43%准确率，优于GPT-4o。

Conclusion: IFDecorator显著提升性能并减少奖励滥用，将开源模型与数据。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction
following capabilities of large language models (LLMs), but suffers from
training inefficiency due to inadequate difficulty assessment. Moreover, RLVR
is prone to over-optimization, where LLMs exploit verification shortcuts
without aligning to the actual intent of user instructions. We introduce
Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR
training into a robust and sample-efficient pipeline. It consists of three
components: (1) a cooperative-adversarial data flywheel that co-evolves
instructions and hybrid verifications, generating progressively more
challenging instruction-verification pairs; (2) IntentCheck, a bypass module
enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that
detects reward hacking via trap instructions, which trigger and capture
shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves
87.43% accuracy on IFEval, outperforming larger proprietary models such as
GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench
while preserving general capabilities. Our trip wires show significant
reductions in reward hacking rates. We will release models, code, and data for
future research.

</details>


### [62] [Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech](https://arxiv.org/abs/2508.04638)
*Tanvi Dinkar,Aiqi Jiang,Simona Frenda,Poppy Gerrard-Abbott,Nancie Gunson,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CL

TL;DR: 论文分析了NLP领域中反言论（counterspeech）研究的现状，指出当前研究逐渐脱离受影响社区的需求，并提出了重新关注利益相关者参与的建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨NLP领域中反言论研究的现状，特别是利益相关者参与对数据集创建、模型开发和评估的影响。

Method: 通过对74项NLP研究的系统综述，以及与5个专注于在线性别暴力（oGBV）的非政府组织的参与式案例研究，分析利益相关者参与的作用。

Result: 研究发现当前NLP研究与受在线有毒内容影响最大的社区需求之间存在脱节。

Conclusion: 论文提出了具体的建议，强调在反言论研究中重新关注利益相关者的专业知识。

Abstract: Counterspeech, i.e. the practice of responding to online hate speech, has
gained traction in NLP as a promising intervention. While early work emphasised
collaboration with non-governmental organisation stakeholders, recent research
trends have shifted toward automated pipelines that reuse a small set of legacy
datasets, often without input from affected communities. This paper presents a
systematic review of 74 NLP studies on counterspeech, analysing the extent to
which stakeholder participation influences dataset creation, model development,
and evaluation. To complement this analysis, we conducted a participatory case
study with five NGOs specialising in online Gender-Based Violence (oGBV),
identifying stakeholder-informed practices for counterspeech generation. Our
findings reveal a growing disconnect between current NLP research and the needs
of communities most impacted by toxic online content. We conclude with concrete
recommendations for re-centring stakeholder expertise in counterspeech
research.

</details>


### [63] [Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs](https://arxiv.org/abs/2508.04660)
*Noah Ziems,Dilara Soylu,Lakshya A Agrawal,Isaac Miller,Liheng Lai,Chen Qian,Kaiqiang Song,Meng Jiang,Dan Klein,Matei Zaharia,Karel D'Oosterlinck,Christopher Potts,Omar Khattab*

Main category: cs.CL

TL;DR: mmGRPO是GRPO的多模块扩展，用于优化多模块AI系统，结合自动提示优化，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 解决GRPO在多模块AI系统中应用不足的问题，提升模块化程序的性能。

Method: 定义mmGRPO，按模块分组处理LM调用，支持变长和中断轨迹。

Result: mmGRPO结合自动提示优化，平均提升11%准确率，优于单独提示优化。

Conclusion: mmGRPO在多任务中表现优异，已开源为DSPy的dspy.GRPO优化器。

Abstract: Group Relative Policy Optimization (GRPO) has proven to be an effective tool
for post-training language models (LMs). However, AI systems are increasingly
expressed as modular programs that mix together multiple LM calls with distinct
prompt templates and other tools, and it is not clear how best to leverage GRPO
to improve these systems. We begin to address this challenge by defining
mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by
module across rollouts and handles variable-length and interrupted
trajectories. We find that mmGRPO, composed with automatic prompt optimization,
improves accuracy by 11% on average across classification, many-hop search, and
privacy-preserving delegation tasks against the post-trained LM, and by 5%
against prompt optimization on its own. We open-source mmGRPO in DSPy as the
dspy.GRPO optimizer.

</details>


### [64] [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
*Mo Li,L. H. Xu,Qitai Tan,Ting Cao,Yunxin Liu*

Main category: cs.CL

TL;DR: Sculptor框架通过主动上下文管理工具（ACM）帮助大语言模型（LLM）优化内部工作记忆，显著提升长上下文处理性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在处理长上下文时因主动干扰导致的性能下降问题。

Method: 提出Sculptor框架，包含上下文分割、摘要/隐藏/恢复和智能搜索三类工具，帮助LLM主动管理注意力。

Result: 在PI-LLM和NeedleBench等基准测试中，Sculptor显著提升性能，无需额外训练。

Conclusion: 主动上下文管理策略（而非更大的token窗口）是提升LLM长上下文处理鲁棒性的关键。

Abstract: Large Language Models (LLMs) suffer from significant performance degradation
when processing long contexts due to proactive interference, where irrelevant
information in earlier parts of the context disrupts reasoning and memory
recall. While most research focuses on external memory systems to augment LLMs'
capabilities, we propose a complementary approach: empowering LLMs with Active
Context Management (ACM) tools to actively sculpt their internal working
memory. We introduce Sculptor, a framework that equips LLMs with three
categories of tools: (1) context fragmentation, (2) summary, hide, and restore,
and (3) intelligent search. Our approach enables LLMs to proactively manage
their attention and working memory, analogous to how humans selectively focus
on relevant information while filtering out distractions. Experimental
evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and
NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly
improves performance even without specific training, leveraging LLMs' inherent
tool calling generalization capabilities. By enabling Active Context
Management, Sculptor not only mitigates proactive interference but also
provides a cognitive foundation for more reliable reasoning across diverse
long-context tasks-highlighting that explicit context-control strategies,
rather than merely larger token windows, are key to robustness at scale.

</details>


### [65] [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)
*Yunan Zhang,Shuoran Jiang,Mengchen Zhao,Yuefeng Li,Yang Fan,Xiangping Wu,Qingcai Chen*

Main category: cs.CL

TL;DR: 论文提出了一种名为GeRe的框架，通过使用预训练文本进行高效抗遗忘，解决了大型语言模型（LLMs）在持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 持续微调LLMs在不同领域时，常出现灾难性遗忘，导致通用能力显著下降和先前任务性能急剧下降。

Method: 提出了General Sample Replay (GeRe)框架，结合阈值边际（TM）损失优化方法，保持重放学习中的激活状态一致性。

Result: 实验表明，GeRe框架下TM方法能有效保留通用能力并提升跨任务性能，优于其他重放策略。

Conclusion: GeRe为LLMs的高效重放提供了新思路，代码和数据已开源。

Abstract: The continual learning capability of large language models (LLMs) is crucial
for advancing artificial general intelligence. However, continual fine-tuning
LLMs across various domains often suffers from catastrophic forgetting,
characterized by: 1) significant forgetting of their general capabilities, and
2) sharp performance declines in previously learned tasks. To simultaneously
address both issues in a simple yet stable manner, we propose General Sample
Replay (GeRe), a framework that use usual pretraining texts for efficient
anti-forgetting. Beyond revisiting the most prevalent replay-based practices
under GeRe, we further leverage neural states to introduce a enhanced
activation states constrained optimization method using threshold-based margin
(TM) loss, which maintains activation state consistency during replay learning.
We are the first to validate that a small, fixed set of pre-collected general
replay samples is sufficient to resolve both concerns--retaining general
capabilities while promoting overall performance across sequential tasks.
Indeed, the former can inherently facilitate the latter. Through controlled
experiments, we systematically compare TM with different replay strategies
under the GeRe framework, including vanilla label fitting, logit imitation via
KL divergence and feature imitation via L1/L2 losses. Results demonstrate that
TM consistently improves performance and exhibits better robustness. Our work
paves the way for efficient replay of LLMs for the future. Our code and data
are available at https://github.com/Qznan/GeRe.

</details>


### [66] [FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data](https://arxiv.org/abs/2508.04698)
*Thibaut Thonet,Germán Kruszewski,Jos Rozen,Pierre Erbacher,Marc Dymetman*

Main category: cs.CL

TL;DR: 论文提出了一种在有限数据下个性化偏好对齐（PPALLI）的方法FaST，并引入了两个数据集DnD和ELIP进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM对话助手通常采用一刀切的方式，无法满足用户个性化需求，因此需要研究在有限数据下如何实现个性化偏好对齐。

Method: 提出了FaST方法，利用从数据中自动发现的高层特征，实现参数高效的对齐。

Result: FaST在DnD和ELIP数据集上表现最佳。

Conclusion: FaST是一种高效的个性化偏好对齐方法，适用于数据有限的情况。

Abstract: LLM-powered conversational assistants are often deployed in a
one-size-fits-all manner, which fails to accommodate individual user
preferences. Recently, LLM personalization -- tailoring models to align with
specific user preferences -- has gained increasing attention as a way to bridge
this gap. In this work, we specifically focus on a practical yet challenging
setting where only a small set of preference annotations can be collected per
user -- a problem we define as Personalized Preference Alignment with Limited
Data (PPALLI). To support research in this area, we introduce two datasets --
DnD and ELIP -- and benchmark a variety of alignment techniques on them. We
further propose FaST, a highly parameter-efficient approach that leverages
high-level features automatically discovered from the data, achieving the best
overall performance.

</details>


### [67] [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis](https://arxiv.org/abs/2508.04699)
*Anushka Yadav,Isha Nalawade,Srujana Pillarichety,Yashwanth Babu,Reshmi Ghosh,Samyadeep Basu,Wenlong Zhao,Ali Nasaeh,Sriram Balasubramanian,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 论文研究了推理模型在多跳问答任务中的失败原因，提出了一个新颖的错误分类框架，从多样性、完整性和认知效率三个维度分析错误模式。


<details>
  <summary>Details</summary>
Motivation: 理解推理模型为何比通用语言模型更容易产生幻觉，并探索其在多跳问答任务中的失败原因。

Method: 引入一个错误分类框架，结合人工标注和自动化指标，分析错误模式。

Result: 揭示了隐藏在以准确性为中心评估中的复杂错误模式。

Conclusion: 为未来语言模型的推理保真度、透明性和鲁棒性提供了改进方向。

Abstract: The emergence of reasoning models and their integration into practical AI
chat bots has led to breakthroughs in solving advanced math, deep search, and
extractive question answering problems that requires a complex and multi-step
thought process. Yet, a complete understanding of why these models hallucinate
more than general purpose language models is missing. In this investigative
study, we systematicallyexplore reasoning failures of contemporary language
models on multi-hop question answering tasks. We introduce a novel, nuanced
error categorization framework that examines failures across three critical
dimensions: the diversity and uniqueness of source documents involved ("hops"),
completeness in capturing relevant information ("coverage"), and cognitive
inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by
complementary automated metrics, our exploration uncovers intricate error
patterns often hidden by accuracy-centric evaluations. This investigative
approach provides deeper insights into the cognitive limitations of current
models and offers actionable guidance toward enhancing reasoning fidelity,
transparency, and robustness in future language modeling efforts.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [68] [MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning](https://arxiv.org/abs/2508.03700)
*Liujian Tang,Shaokang Dong,Yijia Huang,Minqi Xiang,Hongtao Ruan,Bin Wang,Shuo Li,Zhihui Cao,Hailiang Pang,Heng Kong,He Yang,Mingxu Chai,Zhilin Gao,Xingyu Liu,Yingnan Fu,Jiaming Liu,Tao Gui,Xuanjing Huang,Yu-Gang Jiang,Qi Zhang,Kang Wang,Yunke Zhang,Yuran Wang*

Main category: cs.HC

TL;DR: MagicGUI是一个基础移动GUI代理，通过六项关键技术解决移动GUI环境中的感知、定位和推理问题，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决移动GUI环境中感知、定位和推理的关键挑战，提升人机交互体验。

Method: 包括数据集构建、增强感知与定位能力、统一动作空间、规划导向推理机制、两阶段训练流程，以及性能评估。

Result: 在Magic-RICH和多个公开基准测试中表现优异，具备强大的泛化能力和实际部署潜力。

Conclusion: MagicGUI在移动GUI任务中表现出色，具备实际应用潜力。

Abstract: This paper presents MagicGUI, a foundational mobile GUI agent designed to
address critical challenges in perception, grounding, and reasoning within
real-world mobile GUI environments. The framework is underpinned by following
six key components: (1) a comprehensive and accurate dataset, constructed via
the scalable GUI Data Pipeline, which aggregates the largest and most diverse
GUI-centric multimodal data to date from open-source repositories, automated
crawling, and targeted manual annotation; (2) enhanced perception and grounding
capabilities, facilitating fine-grained multimodal alignment for UI element
referencing, grounding, and screen comprehension; (3) a comprehensive and
unified action space, encompassing both fundamental UI operations and complex
interactive intents to support human-agent interactions; (4) planning-oriented
reasoning mechanisms that enable the model to decompose complex user
instructions into sequential actions with explicit intermediate meta-paln
reasoning; (5) an iterative two-stage training procedure, combining large-scale
continue pre-training on 7.8M samples with reinforcement fine-tuning utilizing
a spatially enhanced composite reward and dual filtering strategy; and (6)
competitive performance on both the proprietary Magic-RICH benchmark and over a
dozen public benchmarks, achieving superior performance across GUI perception
and agent tasks, while demonstrating robust generalization and real-world
deployment potential in practical mobile GUI scenarios, as detailed in Figure
1.

</details>


### [69] [Screen Matters: Cognitive and Behavioral Divergence Between Smartphone-Native and Computer-Native Youth](https://arxiv.org/abs/2508.03705)
*Kanan Eldarov*

Main category: cs.HC

TL;DR: 研究探讨电脑与智能手机对青少年注意力、挫折感和创造力的影响，发现设备类型对认知和行为有显著差异。


<details>
  <summary>Details</summary>
Motivation: 探索不同数字交互方式（电脑与智能手机）对青少年认知和行为的影响，为教育设计提供依据。

Method: 采用随机分层设计，结合任务日志、视线追踪和专家评估，分析824名11-17岁学生的数据。

Result: 设备类型对注意力、挫折感和创造力有显著但适度的差异。

Conclusion: 数字交互方式对教育设计和用户界面开发有实际意义。

Abstract: This study explores how different modes of digital interaction -- namely,
computers versus smartphones -- affect attention, frustration, and creative
performance in adolescents. Using a combination of digital task logs,
webcam-based gaze estimation, and expert evaluation of task outcomes, we
analyzed data from a diverse sample of 824 students aged 11-17. Participants
were assigned to device groups in a randomized and stratified design to control
for age, gender, and prior experience. Results suggest moderate but
statistically significant differences in sustained attention, perceived
frustration, and creative output. These findings indicate that the nature of
digital interaction -- beyond mere screen time -- may influence cognitive and
behavioral outcomes relevant to educational design. Practical implications for
user interface development and learning environments are discussed.

</details>


### [70] [Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy and Visual Attention](https://arxiv.org/abs/2508.03713)
*Minsuk Chang,Yao Wang,Huichen Will Wang,Yuanhong Zhou,Andreas Bulling,Cindy Xiong Bearfield*

Main category: cs.HC

TL;DR: 研究提出Lit2Sal和Sal2Lit模型，利用视觉注意力模式预测视觉素养水平或反之，提升可视化设计的个性化效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了视觉素养水平对注意力行为的影响，导致可视化设计效果受限。

Method: 通过235名参与者的用户研究数据，分析注意力模式与视觉素养的关联，并开发Lit2Sal和Sal2Lit模型。

Result: Lit2Sal优于现有显著性模型，Sal2Lit预测视觉素养准确率达86%。

Conclusion: 考虑个体差异的显著性模型为个性化视觉数据通信提供了新方向。

Abstract: Accounting for individual differences can improve the effectiveness of
visualization design. While the role of visual attention in visualization
interpretation is well recognized, existing work often overlooks how this
behavior varies based on visual literacy levels. Based on data from a
235-participant user study covering three visualization tests (mini-VLAT,
CALVI, and SGL), we show that distinct attention patterns in visual data
exploration can correlate with participants' literacy levels: While experts
(high-scorers) generally show a strong attentional focus, novices (low-scorers)
focus less and explore more. We then propose two computational models
leveraging these insights: Lit2Sal -- a novel visual saliency model that
predicts observer attention given their visualization literacy level, and
Sal2Lit -- a model to predict visual literacy from human visual attention data.
Our quantitative and qualitative evaluation demonstrates that Lit2Sal
outperforms state-of-the-art saliency models with literacy-aware
considerations. Sal2Lit predicts literacy with 86% accuracy using a single
attention map, providing a time-efficient supplement to literacy assessment
that only takes less than a minute. Taken together, our unique approach to
consider individual differences in salience models and visual attention in
literacy assessments paves the way for new directions in personalized visual
data communication to enhance understanding.

</details>


### [71] ["Think First, Verify Always": Training Humans to Face AI Risks](https://arxiv.org/abs/2508.03714)
*Yuksel Aydin*

Main category: cs.HC

TL;DR: 论文提出“Think First, Verify Always”（TFVA）协议，通过五项原则（AIJET）提升人类对AI威胁的认知防御能力，实验显示3分钟干预显著提高任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全以设备为中心，忽视了AI对人类认知的攻击威胁，需重新定位人类为防御第一线。

Method: 提出TFVA协议，基于AIJET五项原则，并通过随机对照试验（n=151）验证其效果。

Result: 3分钟干预显著提升认知安全任务表现（+7.87%），表明简短原则性训练可增强人类对AI操纵的抵抗力。

Conclusion: 建议将TFVA嵌入GenAI平台，取代被动警告，以增强可信赖的AI使用，并确立人类赋能的安全为可信AI系统的关键。

Abstract: Artificial intelligence enables unprecedented attacks on human cognition, yet
cybersecurity remains predominantly device-centric. This paper introduces the
"Think First, Verify Always" (TFVA) protocol, which repositions humans as
'Firewall Zero', the first line of defense against AI-enabled threats. The
protocol is grounded in five operational principles: Awareness, Integrity,
Judgment, Ethical Responsibility, and Transparency (AIJET). A randomized
controlled trial (n=151) demonstrated that a minimal 3-minute intervention
produced statistically significant improvements in cognitive security task
performance, with participants showing an absolute +7.87% gains compared to
controls. These results suggest that brief, principles-based training can
rapidly enhance human resilience against AI-driven cognitive manipulation. We
recommend that GenAI platforms embed "Think First, Verify Always" as a standard
prompt, replacing passive warnings with actionable protocols to enhance
trustworthy and ethical AI use. By bridging the gap between technical
cybersecurity and human factors, the TFVA protocol establishes human-empowered
security as a vital component of trustworthy AI systems.

</details>


### [72] [Relationship between Perceived Maneuverability and Involuntary Eye Movements under Systematically Varied Time Constants of Ride-on Machinery](https://arxiv.org/abs/2508.03717)
*Muhammad Akmal Bin Mohammed Zaffir,Daisuke Sakai,Yuki Sato,Takahiro Wada*

Main category: cs.HC

TL;DR: 研究探讨了骑乘机器动态特性变化对操作者非自愿眼球运动准确性的影响，发现时间常数增加会降低操作性和眼球运动准确性，同时增加认知负荷。


<details>
  <summary>Details</summary>
Motivation: 探讨骑乘机器动态特性（如时间常数）如何影响操作者的非自愿眼球运动准确性和感知操作性。

Method: 参与者操作一个偏航旋转平台，时间常数被系统调整，记录眼球运动并收集主观操作性评分和认知负荷。

Result: 时间常数增加导致操作性评分下降、认知负荷增加，同时非自愿眼球运动准确性降低。

Conclusion: 骑乘机器的动态特性显著影响操作者的眼球运动准确性和主观体验。

Abstract: Studies suggest that involuntary eye movements exhibit greater stability
during active motion compared to passive motion, and this effect may also apply
to the operation of ride-on machinery. Moreover, a study suggested that
experimentally manipulating the sense of agency (SoA) by introducing delays may
influence the stability of involuntary eye movements. Although a preliminary
investigation examined involuntary eye movements and perceived maneuverability
under two distinct machine dynamics with preserved SoA, it remains unclear how
systematic variations in motion dynamics influence these factors. Therefore,
the purpose of the present research was to investigate whether systematic
variations in the dynamic properties of a ride-on machine, where the perceived
maneuverability is modulated, influence the accuracy of involuntary eye
movements in human operators. Participants rode a yaw-rotational platform whose
time constant from joystick input to motor torque of a rotational machine was
systematically manipulated. During the operation, eye movements were recorded
while participants fixated on a visual target. After each condition,
participants provided subjective ratings of maneuverability and cognitive load.
As the platform's time constant increased, the perceived maneuverability scores
decreased while the cognitive loads increased. Concurrently, involuntary eye
movement accuracy decreased. Moderate to weak positive correlations emerged
between the perceived maneuverability scores and the eye movement gain and
accuracy, while a weak negative correlation was found with cognitive load.

</details>


### [73] [Recommending With, Not For: Co-Designing Recommender Systems for Social Good](https://arxiv.org/abs/2508.03792)
*Michael D. Ekstrand,Afsaneh Razi,Aleksandra Sarcevic,Maria Soledad Pera,Robin Burke,Katherine Landau Wright*

Main category: cs.HC

TL;DR: 论文主张推荐系统的设计应通过参与式和民主化的过程，与用户和其他利益相关者共同完成，而不仅是为他们设计。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统的设计和评估主要由开发团队主导，忽略了其他利益相关者的直接参与，尤其是在社会公益领域。

Method: 提出通过参与式和民主化的设计过程，让用户和利益相关者成为共同设计者。

Result: 强调了共同设计的重要性，以实现更符合社会公益目标的推荐系统。

Conclusion: 推荐系统应通过协作设计，确保其对社会公益的贡献更符合实际需求。

Abstract: Recommender systems are usually designed by engineers, researchers,
designers, and other members of development teams. These systems are then
evaluated based on goals set by the aforementioned teams and other business
units of the platforms operating the recommender systems. This design approach
emphasizes the designers' vision for how the system can best serve the
interests of users, providers, businesses, and other stakeholders. Although
designers may be well-informed about user needs through user experience and
market research, they are still the arbiters of the system's design and
evaluation, with other stakeholders' interests less emphasized in user-centered
design and evaluation. When extended to recommender systems for social good,
this approach results in systems that reflect the social objectives as
envisioned by the designers and evaluated as the designers understand them.
Instead, social goals and operationalizations should be developed through
participatory and democratic processes that are accountable to their
stakeholders. We argue that recommender systems aimed at improving social good
should be designed *by* and *with*, not just *for*, the people who will
experience their benefits and harms. That is, they should be designed in
collaboration with their users, creators, and other stakeholders as full
co-designers, not only as user study participants.

</details>


### [74] [A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers](https://arxiv.org/abs/2508.03852)
*Zhuohao,Zhang,Haichang Li,Chun Meng Yu,Faraz Faruqi,Junan Xie,Gene S-H Kim,Mingming Fan,Angus G. Forbes,Jacob O. Wobbrock,Anhong Guo,Liang He*

Main category: cs.HC

TL;DR: A11yShape是一个帮助盲人和低视力用户理解、修改和迭代3D模型的系统，利用LLMs和OpenSCAD，提供可访问的描述、版本控制和跨表示同步功能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D建模工具缺乏对非视觉交互的支持，盲人和低视力用户难以独立操作。

Method: A11yShape结合LLMs和OpenSCAD，提供模型描述、版本控制和层次化表示，并通过跨表示高亮同步代码、语义层次、AI描述和3D渲染。

Result: 用户研究表明，盲人和低视力程序员能够独立理解和修改3D模型，无需视力辅助。

Conclusion: A11yShape为盲人和低视力用户提供了独立操作3D建模工具的可能性，填补了现有工具的空白。

Abstract: Building 3-D models is challenging for blind and low-vision (BLV) users due
to the inherent complexity of 3-D models and the lack of support for non-visual
interaction in existing tools. To address this issue, we introduce A11yShape, a
novel system designed to help BLV users who possess basic programming skills
understand, modify, and iterate on 3-D models. A11yShape leverages LLMs and
integrates with OpenSCAD, a popular open-source editor that generates 3-D
models from code. Key functionalities of A11yShape include accessible
descriptions of 3-D models, version control to track changes in models and
code, and a hierarchical representation of model components. Most importantly,
A11yShape employs a cross-representation highlighting mechanism to synchronize
semantic selections across all model representations -- code, semantic
hierarchy, AI description, and 3-D rendering. We conducted a multi-session user
study with four BLV programmers, where, after an initial tutorial session,
participants independently completed 12 distinct models across two testing
sessions, achieving results that aligned with their own satisfaction. The
result demonstrates that participants were able to comprehend provided 3-D
models, as well as independently create and modify 3-D models -- tasks that
were previously impossible without assistance from sighted individuals.

</details>


### [75] [ReVISit 2: A Full Experiment Life Cycle User Study Framework](https://arxiv.org/abs/2508.03876)
*Zach Cutler,Jack Wilburn,Hilson Shrestha,Yiren Ding,Brian Bollen,Khandaker Abrar Nadib,Tingying He,Andrew McNutt,Lane Harrison,Alexander Lex*

Main category: cs.HC

TL;DR: reVISit 2是一个支持可视化研究人员设计和进行基于浏览器的用户研究的软件框架，涵盖实验生命周期的各个阶段，旨在简化研究过程并提高可重复性。


<details>
  <summary>Details</summary>
Motivation: 当前的可视化用户研究工具大多仅解决实验生命周期的部分问题，难以支持复杂研究设计或交互，且缺乏可重复性。

Method: reVISit 2提供技术功能（如参与者交互回放）和社会技术支持（如维护的社区），支持设计、调试、数据收集、分析和传播等实验阶段。

Result: 该系统已在高质量研究中得到验证，并通过实验复现展示其有效性。

Conclusion: reVISit 2提升了研究的便捷性、可重复性，并支持构建高级交互研究。

Abstract: Online user studies of visualizations, visual encodings, and interaction
techniques are ubiquitous in visualization research. Yet, designing,
conducting, and analyzing studies effectively is still a major burden. Although
various packages support such user studies, most solutions address only facets
of the experiment life cycle, make reproducibility difficult, or do not cater
to nuanced study designs or interactions. We introduce reVISit 2, a software
framework that supports visualization researchers at all stages of designing
and conducting browser-based user studies. ReVISit supports researchers in the
design, debug & pilot, data collection, analysis, and dissemination experiment
phases by providing both technical affordances (such as replay of participant
interactions) and sociotechnical aids (such as a mindfully maintained community
of support). It is a proven system that can be (and has been) used in
publication-quality studies -- which we demonstrate through a series of
experimental replications. We reflect on the design of the system via
interviews and an analysis of its technical dimensions. Through this work, we
seek to elevate the ease with which studies are conducted, improve the
reproducibility of studies within our community, and support the construction
of advanced interactive studies.

</details>


### [76] [Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI Perspective](https://arxiv.org/abs/2508.03969)
*Wei Xu*

Main category: cs.HC

TL;DR: 本章从以人为本的人工智能（HCAI）视角，系统性地推动了人机交互（HAII）这一新兴跨学科领域，提出了以人为中心的HAII框架（HC-HAII），强调以人为核心的研究与应用。


<details>
  <summary>Details</summary>
Motivation: 推动人机交互领域的发展，强调以人为中心而非技术为中心的研究方法，为后续章节奠定理论基础。

Method: 介绍了HC-HAII方法论，包括以人为中心的方法、流程、跨学科团队和多层次设计范式。

Result: 提出了HC-HAII框架，并总结了关键研究挑战与未来方向。

Conclusion: 本章为全书提供了以HCAI为基础的HAII研究与应用框架，为后续内容铺路。

Abstract: This chapter systematically promotes an emerging interdisciplinary field of
human-artificial intelligence interaction (human-AI interaction, HAII) from a
human-centered AI (HCAI) perspective. It introduces a framework of
human-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII
research and applications, emphasizing the importance of adopting a
human-centered approach over a technology-centered one. The chapter presents
the HC-HAII methodology, including human-centered methods, process,
interdisciplinary teams, and multi-level design paradigms. It also highlights
key research challenges and future directions. As the first chapter, this
chapter also provides a structural overview of this book, which brings together
contributions from an interdisciplinary community of researchers and
practitioners to advance the theory, methodology, and applications of HCAI in
diverse domains of HAII. The purpose of this chapter is to provide a
fundamental framework for this book, centered on HAII research and applications
based on the HCAI approach, which will pave the way for the content of
subsequent chapters.

</details>


### [77] [Managing Data for Scalable and Interactive Event Sequence Visualization](https://arxiv.org/abs/2508.03974)
*Sayef Azad Sakin,Katherine E. Isaacs*

Main category: cs.HC

TL;DR: ESeMan是一个事件序列管理系统，通过分层数据结构和智能缓存支持交互式时间线可视化，显著减少数据获取时间并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模增长，并行时间线可视化在交互时出现延迟，现有方法在性能和准确性之间存在权衡。

Method: ESeMan采用分层数据结构和智能缓存技术，仅获取生成准确摘要所需的数据。

Result: ESeMan在多种程序执行轨迹上表现优于现有方法，获取时间低于100毫秒且保持像素级准确性。

Conclusion: ESeMan为事件序列可视化提供了高性能解决方案，并提供了基准测试工具以支持未来评估。

Abstract: Parallel event sequences, such as those collected in program execution traces
and automated manufacturing pipelines, are typically visualized as interactive
parallel timelines. As the dataset size grows, these charts frequently
experience lag during common interactions such as zooming, panning, and
filtering. Summarization approaches can improve interaction performance, but at
the cost of accuracy in representation. To address this challenge, we introduce
ESeMan (Event Sequence Manager), an event sequence management system designed
to support interactive rendering of timeline visualizations with tunable
accuracy. ESeMan employs hierarchical data structures and intelligent caching
to provide visualizations with only the data necessary to generate accurate
summarizations with significantly reduced data fetch time. We evaluate ESeMan's
query times against summed area tables, M4 aggregation, and statistical
sub-sampling on a variety of program execution traces. Our results demonstrate
ESeMan provides better performance, achieving sub-100ms fetch times while
maintaining visualization accuracy at the pixel level. We further present our
benchmarking harness, enabling future performance evaluations for event
sequence visualization.

</details>


### [78] [SocialPulse: An On-Smartwatch System for Detecting Real-World Social Interactions](https://arxiv.org/abs/2508.03980)
*Md Sabbir Ahmed,Arafat Rahman,Mark Rucker,Laura E. Barnes*

Main category: cs.HC

TL;DR: 论文提出了一种实时手表系统，用于检测面对面和虚拟互动，利用迁移学习和对话线索，准确率达73.18%。


<details>
  <summary>Details</summary>
Motivation: 现有系统局限于受控环境或特定假设，难以捕捉多样化的真实世界互动。

Method: 开发了基于迁移学习的系统，通过前景语音和对话线索（如耳语）推断互动边界。

Result: 在11名参与者38天的评估中，系统互动检测准确率为73.18%，后续6名参与者显示完美召回率。

Conclusion: 系统展示了捕捉日常生活中互动的潜力，为针对社交焦虑的个性化干预提供了基础。

Abstract: Social interactions are a fundamental part of daily life and play a critical
role in well-being. As emerging technologies offer opportunities to
unobtrusively monitor behavior, there is growing interest in using them to
better understand social experiences. However, automatically detecting
interactions, particularly via wearable devices, remains underexplored.
Existing systems are often limited to controlled environments, constrained to
in-person interactions, and rely on rigid assumptions such as the presence of
two speakers within a fixed time window. These limitations reduce their
generalizability to capture diverse real-world interactions. To address these
challenges, we developed a real-time, on-watch system capable of detecting both
in-person and virtual interactions. The system leverages transfer learning to
detect foreground speech (FS) and infers interaction boundaries based upon FS
and conversational cues like whispering. In a real-world evaluation involving
11 participants over a total of 38 days (Mean = 3.45 days, SD = 2.73), the
system achieved an interaction detection accuracy of 73.18%. Follow-up with six
participants indicated perfect recall for detecting interactions. These
preliminary findings demonstrate the potential of our system to capture
interactions in daily life, providing a foundation for applications such as
personalized interventions targeting social anxiety.

</details>


### [79] [StepWrite: Adaptive Planning for Speech-Driven Text Generation](https://arxiv.org/abs/2508.04011)
*Hamza El Alaoui,Atieh Taheri,Yi-Hao Peng,Jeffrey P. Bigham*

Main category: cs.HC

TL;DR: StepWrite是一个基于大型语言模型的语音交互系统，支持用户在移动中无需手眼参与，通过结构化引导和上下文感知的音频提示，高效撰写长文本。


<details>
  <summary>Details</summary>
Motivation: 当前语音转文本系统在撰写复杂长文本时表现不佳，尤其是在用户移动中无法视觉跟踪进度的情况下。需要一种能支持持久上下文跟踪、结构化引导和适应动态用户意图的工具。

Method: StepWrite将写作过程分解为可管理的子任务，通过上下文感知的非视觉音频提示逐步引导用户，同时将上下文跟踪和自适应规划任务交给模型处理。

Result: 实验表明，StepWrite显著降低了认知负荷，提高了可用性和用户满意度，并在动态上下文提示生成、语气对齐和事实核查方面表现优异。

Conclusion: StepWrite展示了结构化、上下文感知的语音交互在增强免手眼日常多任务沟通中的潜力。

Abstract: People frequently use speech-to-text systems to compose short texts with
voice. However, current voice-based interfaces struggle to support composing
more detailed, contextually complex texts, especially in scenarios where users
are on the move and cannot visually track progress. Longer-form communication,
such as composing structured emails or thoughtful responses, requires
persistent context tracking, structured guidance, and adaptability to evolving
user intentions--capabilities that conventional dictation tools and voice
assistants do not support. We introduce StepWrite, a large language
model-driven voice-based interaction system that augments human writing ability
by enabling structured, hands-free and eyes-free composition of longer-form
texts while on the move. StepWrite decomposes the writing process into
manageable subtasks and sequentially guides users with contextually-aware
non-visual audio prompts. StepWrite reduces cognitive load by offloading the
context-tracking and adaptive planning tasks to the models. Unlike baseline
methods like standard dictation features (e.g., Microsoft Word) and
conversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite
dynamically adapts its prompts based on the evolving context and user intent,
and provides coherent guidance without compromising user autonomy. An empirical
evaluation with 25 participants engaging in mobile or stationary hands-occupied
activities demonstrated that StepWrite significantly reduces cognitive load,
improves usability and user satisfaction compared to baseline methods.
Technical evaluations further confirmed StepWrite's capability in dynamic
contextual prompt generation, accurate tone alignment, and effective fact
checking. This work highlights the potential of structured, context-aware voice
interactions in enhancing hands-free and eye-free communication in everyday
multitasking scenarios.

</details>


### [80] [VeriGUI: Verifiable Long-Chain GUI Dataset](https://arxiv.org/abs/2508.04026)
*Shunyu Liu,Minghao Liu,Huichi Zhou,Zhenyu Cui,Yang Zhou,Yuhao Zhou,Wendong Fan,Ge Zhang,Jiajun Shi,Weihao Xuan,Jiaxing Huang,Shuang Luo,Fang Wu,Heli Qi,Qingcheng Zeng,Ziqi Ren,Jialiang Gao,Jindi Lv,Junjie Wang,Aosong Feng,Heng Zhou,Wangchunshu Zhou,Zhenfei Yin,Wenlong Zhang,Guohao Li,Wenhao Yu,Irene Li,Lei Ma,Lei Bai,Qunshu Lin,Mingli Song,Dacheng Tao*

Main category: cs.HC

TL;DR: VeriGUI是一个可验证的长链GUI数据集，旨在支持通用GUI代理在真实计算机环境中的开发和评估，强调长链复杂性和子任务级可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理主要关注短期交互和结果验证，限制了其在需要长视野任务分解和执行的现实GUI应用中的扩展性。

Method: 引入VeriGUI数据集，包含由人类专家标注的桌面和网页GUI任务轨迹，任务分解为数百步相互依赖的子任务，支持子任务级验证。

Result: 实验显示，不同基础模型的代理在处理长视野任务时存在显著性能差距，表明需要更强的规划和决策能力。

Conclusion: VeriGUI为开发更强大的GUI代理提供了重要资源，揭示了当前代理在长视野任务中的不足。

Abstract: Recent studies have delved into constructing autonomous agents capable of
performing complex Graphical User Interface (GUI)-based computer tasks, with
the potential to revolutionize human-computer interaction. Despite encouraging
results, existing efforts mainly focus on short-term interactions and rely on
outcome-only verification, thereby limiting their scalability in real-world GUI
applications that demand long-horizon task decomposition and execution. In this
work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed
to facilitate the development and evaluation of generalist GUI agents operating
in realistic computer environments. Our dataset emphasizes two critical
dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of
interdependent subtasks spanning hundreds of steps, explicitly designed to
allow any subtask to serve as a valid starting point; and (2) subtask-level
verifiability, which enables diverse exploration strategies within each
subtask, while ensuring that each subtask-level goal remains verifiable and
consistent. The dataset consists of GUI task trajectories across both desktop
and web, annotated by human experts. Extensive experiments on VeriGUI using
various agents with different foundation models reveal significant performance
gaps in handling long-horizon tasks, highlighting the need for more robust
planning and decision-making capabilities in GUI agents.

</details>


### [81] [XARP Tools: An Extended Reality Platform for Humans and AI Agents](https://arxiv.org/abs/2508.04108)
*Arthur Caetano,Misha Sra*

Main category: cs.HC

TL;DR: XARP Tools是一个扩展现实（XR）框架，支持人类和AI开发者，提供Python库和XR客户端，支持多种使用方式。


<details>
  <summary>Details</summary>
Motivation: 简化XR开发，支持人类和AI开发者，并提供灵活的交互方式。

Method: 通过Python库和JSON协议实现服务器端与XR客户端的通信，支持三种使用方式。

Result: XARP提供高效、低延迟的交互，并开源代码和示例。

Conclusion: XARP是一个多功能XR框架，适用于人类和AI开发者，具有广泛的应用潜力。

Abstract: This technical report presents XARP Tools, an extended reality (XR) framework
designed for human and AI developers alike. XARP comprises a server-side Python
library and platform-specific XR clients. The library offers high-level APIs
and communicates with clients via a JSON-based protocol over WebSockets. XR
clients encapsulate device and runtime specifics, providing responsive,
low-latency user interaction. XARP can be utilized in three ways: (i) as a
library that abstracts XR development for humans; (ii) as a set of callable
tools that allow AI agents to drive on-the-fly interactions with users; and
(iii) as a Model Context Protocol server that plugs XR devices into AI
ecosystems. XARP code and working examples are released openly at
https://github.com/HAL-UCSB/xarp.

</details>


### [82] [DRIVE-T: A Methodology for Discriminative and Representative Data Viz Item Selection for Literacy Construct and Assessment](https://arxiv.org/abs/2508.04160)
*Angela Locoro,Silvia Golia,Davide Falessi*

Main category: cs.HC

TL;DR: DRIVE-T方法通过任务项区分性和代表性，构建数据可视化素养的评估框架。


<details>
  <summary>Details</summary>
Motivation: 解决数据可视化素养评估中难度级别不明确的问题。

Method: DRIVE-T包括任务项标记、难度评级和Rasch模型分析三步。

Result: 成功构建了基于任务项的难度级别测量框架。

Conclusion: DRIVE-T为数据可视化素养评估提供了有效的归纳方法。

Abstract: The underspecification of progressive levels of difficulty in measurement
constructs design and assessment tests for data visualization literacy may
hinder the expressivity of measurements in both test design and test reuse. To
mitigate this problem, this paper proposes DRIVE-T (Discriminating and
Representative Items for Validating Expressive Tests), a methodology designed
to drive the construction and evaluation of assessment items. Given a data
vizualization, DRIVE-T supports the identification of task-based items
discriminability and representativeness for measuring levels of data
visualization literacy. DRIVE-T consists of three steps: (1) tagging task-based
items associated with a set of data vizualizations; (2) rating them by
independent raters for their difficulty; (3) analysing raters' raw scores
through a Many-Facet Rasch Measurement model. In this way, we can observe the
emergence of difficulty levels of the measurement construct, derived from the
discriminability and representativeness of task-based items for each data
vizualization, ordered into Many-Facets construct levels. In this study, we
show and apply each step of the methodology to an item bank, which models the
difficulty levels of a measurement construct approximating a latent construct
for data visualization literacy. This measurement construct is drawn from
semiotics, i.e., based on the syntax, semantics and pragmatics knowledge that
each data visualization may require to be mastered by people. The DRIVE-T
methodology operationalises an inductive approach, observable in a post-design
phase of the items preparation, for formative-style and practice-based
measurement construct emergence. A pilot study with items selected through the
application of DRIVE-T is also presented to test our approach.

</details>


### [83] [Unplug, Mute, Avoid Investigating smart speaker users' privacy protection behaviours in Saudi Homes](https://arxiv.org/abs/2508.04202)
*Abdulrhman Alorini,Yufeng Wu,Abdullah Bin Sawad,Mukesh Prasad,A. Baki Kocaballi*

Main category: cs.HC

TL;DR: 研究探讨沙特阿拉伯用户在集体主义、性别化和多代同堂家庭中如何应对智能音箱的隐私问题，揭示了日常隐私保护行为及其影响因素。


<details>
  <summary>Details</summary>
Motivation: 智能音箱在全球家庭中日益普及，但其隐私风险在非西方文化背景下研究不足，尤其是在沙特阿拉伯这样的集体主义社会中。

Method: 采用文化探针法和半结构化访谈，对16名参与者进行研究。

Result: 发现用户通过拔掉设备、静音麦克风或避免语音交互来保护隐私，这些行为受家庭规范、房间布局和人际关系影响。

Conclusion: 研究为智能音箱隐私的全球讨论提供了新视角，并为设计更具文化响应性的语音界面提供了方向。

Abstract: Smart speakers are increasingly integrated into domestic life worldwide, yet
their privacy risks remain underexplored in non-Western cultural contexts. This
study investigates how Saudi Arabian users of smart speakers navigate privacy
concerns within collectivist, gendered, and often multigenerational households.
Using cultural probes followed by semi-structured interviews with 16
participants, we uncover everyday privacy-protective behaviours including
unplugging devices, muting microphones, and avoiding voice interactions
altogether. These practices are shaped not only by individual risk perceptions
but also by household norms, room configurations, and interpersonal dynamics.
We contribute empirical insights from an underrepresented region, theoretical
extensions to contextual integrity frameworks, and design directions for
culturally responsive voice interfaces. This work expands the global
conversation on smart speaker privacy and informs more inclusive HCI practices
in increasingly diverse smart home environments.

</details>


### [84] [Capturing and Sharing Know-How through Visual Process Representations: A Human-Centred Approach to Teacher Workflows](https://arxiv.org/abs/2508.04357)
*Gloria Fernández-Nieto,Vanessa Echeverria,Yuheng Li,Yi-Shan Tsai,Lele Sha,Guanliang Chen,Dragan Gasevic,Zachari Swiecki*

Main category: cs.HC

TL;DR: 论文提出了一种名为VPR的设计方法，结合SPM、知识管理和叙事技术，将专家日志数据转化为直观的可视化，以支持新手教师。实验表明，VPR在任务表现、可用性和参与度方面有提升，但流程记忆性和任务时间改善有限。


<details>
  <summary>Details</summary>
Motivation: 高校中高员工流动率导致专业知识流失，传统文档化方法耗时且分散专家注意力，需要自动化解决方案。

Method: 采用SPM分析日志数据，结合知识管理和叙事技术设计VPR，并通过实验评估不同视觉表现形式的效果。

Result: VPR在任务表现、可用性和参与度方面表现更好，尤其是富视觉版本，但流程记忆性和任务时间改善不明显。

Conclusion: VPR能有效可视化工作流程并支持新手教师，但需进一步优化记忆性和效率。

Abstract: Knowledge Management is crucial for capturing and transferring expertise
within universities, especially in high staff turnover contexts where expertise
loss disrupts teaching. Documenting teachers' workflows is time-intensive and
diverts experts from core responsibilities. Sequential Pattern Mining (SPM)
leverages log data to identify expert workflows, offering an automated
alternative to represent workflows but requiring transformation into intuitive
formats for novice educators. This paper introduces Visual Process
Representations (VPR), a design approach combining SPM, Knowledge Management
processes, and storytelling techniques to convert expert log data into clear
visualisations. We detail the design phases and report a study evaluating
visual affordances (text lists vs. pictorial-style) and teachers' perceptions
of four versions of the VPR with 160 higher teachers on Prolific. Results
indicate improved task performance, usability, and engagement, particularly
with enriched visuals, though process memorability and task time improvements
were limited. The findings highlight VPR's potential to visualise workflows and
support novice educators.

</details>


### [85] [GoldMind: A Teacher-Centered Knowledge Management System for Higher Education -- Lessons from Iterative Design](https://arxiv.org/abs/2508.04377)
*Gloria Fernández-Nieto,Lele Sha,Yuheng Li,Yi-Shan Tsai,Guanliang Chen,Yinwei Wei,Weiqing Wang,Jinchun Wen,Shaveen Singh,Ivan Silva,Yuanfang Li,Dragan Gasěvić,Zachari Swiecki*

Main category: cs.HC

TL;DR: 本文通过为期两年的人本设计研究，开发并评估了支持数字教学任务中知识管理的KMS系统GoldMind，总结了技术、设计和人因三方面的关键见解。


<details>
  <summary>Details</summary>
Motivation: 高等教育中知识管理系统的设计需应对复杂的人机交互问题，现有系统常忽视教育工作者的实际工作流程，导致采用率低。

Method: 采用迭代的协同设计和评估方法，通过三个设计-评估周期，分析108名教师与系统的交互及其反馈。

Result: 研究提炼出三个主题：技术教训、设计考虑和人因分析，为KMS设计提供了实践指导。

Conclusion: GoldMind的成功设计表明，结合用户反馈和迭代改进的方法能有效提升KMS的实用性和采纳率。

Abstract: Designing Knowledge Management Systems (KMSs) for higher education requires
addressing complex human-technology interactions, especially where staff
turnover and changing roles create ongoing challenges for reusing knowledge.
While advances in process mining and Generative AI enable new ways of designing
features to support knowledge management, existing KMSs often overlook the
realities of educators' workflows, leading to low adoption and limited impact.
This paper presents findings from a two-year human-centred design study with
108 higher education teachers, focused on the iterative co-design and
evaluation of GoldMind, a KMS supporting in-the-flow knowledge management
during digital teaching tasks. Through three design-evaluation cycles, we
examined how teachers interacted with the system and how their feedback
informed successive refinements. Insights are synthesised across three themes:
(1) Technology Lessons from user interaction data, (2) Design Considerations
shaped by co-design and usability testing, and (3) Human Factors, including
cognitive load and knowledge behaviours, analysed using Epistemic Network
Analysis.

</details>


### [86] [Plant-Centric Metaverse: A Biocentric-Creation Framework for Ecological Art and Digital Symbiosis](https://arxiv.org/abs/2508.04391)
*Ze Gao,Mengyao Guo,Zheng Wang,Xiaolin Zhang,Sihuang Man*

Main category: cs.HC

TL;DR: 论文提出Biocentric-Creation Transformation Ideology (BCTI)框架，通过多模态案例研究验证其在数字生态艺术中的应用，揭示了植物与算法共创的新模式。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决当前框架无法系统性指导艺术家利用植物主体性进行超越人类中心的数字共生创作的问题。

Method: 采用多模态案例研究（2013-2023年），涵盖生物艺术、NFT和VR生态系统，提出并验证BCTI框架。

Result: 分析显示：(1)元宇宙生态系统实现植物与算法共创，生物艺术作品增长133%；(2)区块链DAO实现植物主导的人植协作；(3)VR中的算法光合作用重塑生态美学。

Conclusion: BCTI框架系统化植物主体性，为艺术家提供后人类世创作蓝图，重新定义虚拟环境意识并建立跨物种数字协作新协议。

Abstract: Digital ecological art represents an emergent frontier where biological media
converge with virtual environments. This study examines the paradigm shift from
anthropocentric to plant-centered artistic narratives within the metaverse,
contextualizing how digital platforms transform ecological expression. However,
current frameworks fail to systematically guide artists in leveraging plant
agency for digital symbiosis that transcends human-centered creation. We
propose the Biocentric-Creation Transformation Ideology (BCTI) framework and
validate it through multimodal case studies spanning bio-art, NFTs, and VR
ecosystems (2013-2023). Our analysis reveals: (1) Metaverse ecosystems enable
unprecedented plant-algorithm co-creation, with biological artworks increasing
by 133% in premier archives (2020 vs 2013); (2) Digital symbiosis manifests
through blockchain DAOs where plants govern human-plant collaborations; (3)
Algorithmic photosynthesis in VR environments reshapes ecological aesthetics
through real-time biodata translation. The BCTI framework advances ecological
art theory by systematizing the transition from representation to
plant-centered agency, offering artists a blueprint for post-anthropocene
creation. This redefines environmental consciousness in virtual realms while
establishing new protocols for cross-species digital collaboration.

</details>


### [87] [Measuring Information Richness in Product Images: Implications for Online Sales](https://arxiv.org/abs/2508.04541)
*Zhu Yuting,Cao Xinyu,Su Yuzhuo,Ma Yongbin*

Main category: cs.HC

TL;DR: 提出了一种名为k值的新指标，用于量化图像集的信息丰富度，并研究了其对消费者购买决策的影响。实验表明，k值与人类感知的信息丰富度一致，但更高的k值会缩短决策时间却降低购买倾向。


<details>
  <summary>Details</summary>
Motivation: 解决电商卖家在产品图片选择上的挑战，量化图像信息丰富度并分析其对消费者行为的影响。

Method: 利用Vision Transformers（ViT）提取图像特征，通过k均值聚类定义k值，并通过在线实验验证其有效性。

Result: k值与人类感知的信息丰富度一致；高k值缩短决策时间但降低购买倾向。

Conclusion: 揭示了视觉信息丰富度与消费者行为的复杂关系，为卖家提供了量化工具。

Abstract: A common challenge for e-commerce sellers is to decide what product images to
display on online shopping sites. In this paper, we propose and validate a
novel metric, k-value, to quantify the information richness of an image set,
and we further investigate its effect on consumers' purchase decisions. We
leverage patch-level embeddings from Vision Transformers (ViT) and apply
k-means clustering to identify distinct visual features, defining k-value as
the number of clusters. An online experiment demonstrates that k-value aligns
with human-perceived information richness, validating the metric. A simulated
online shopping experiment further reveals a significant yet counterintuitive
result: while an image set with a higher k-value (richer information) shortens
decision time, it paradoxically reduces purchase propensity. Our findings
illuminate the complex relationship between visual information richness and
consumer behavior, providing sellers a quantifiable tool for image selection.

</details>


### [88] [VirtLab: An AI-Powered System for Flexible, Customizable, and Large-scale Team Simulations](https://arxiv.org/abs/2508.04634)
*Mohammed Almutairi,Charles Chiang,Haoze Guo,Matthew Belcher,Nandini Banerjee,Maria Milkowski,Svitlana Volkova,Daniel Nguyen,Tim Weninger,Michael Yankoski,Trenton W. Ford,Diego Gomez-Zara*

Main category: cs.HC

TL;DR: VirtLab是一个用户友好的多代理团队模拟系统，用于测试基于LLM的代理在时空环境中的协作行为。


<details>
  <summary>Details</summary>
Motivation: 探索基于社会科学理论的团队行为，解决现有框架在灵活模拟场景和空间设置上的局限性。

Method: 开发VirtLab系统，包含模拟引擎和Web界面，支持非技术用户无需编程即可运行和分析团队模拟。

Result: 通过比较真实数据与模拟场景，验证了系统的实用性。

Conclusion: VirtLab为团队行为研究提供了灵活且可扩展的解决方案。

Abstract: Simulating how team members collaborate within complex environments using
Agentic AI is a promising approach to explore hypotheses grounded in social
science theories and study team behaviors. We introduce VirtLab, a
user-friendly, customizable, multi-agent, and scalable team simulation system
that enables testing teams with LLM-based agents in spatial and temporal
settings. This system addresses the current frameworks' design and technical
limitations that do not consider flexible simulation scenarios and spatial
settings. VirtLab contains a simulation engine and a web interface that enables
both technical and non-technical users to formulate, run, and analyze team
simulations without programming. We demonstrate the system's utility by
comparing ground truth data with simulated scenarios.

</details>


### [89] [How are CS students using resources and AI tools for coding tasks?](https://arxiv.org/abs/2508.04667)
*Natalia Echeverry,Arun Lekshmi Narayanan*

Main category: cs.HC

TL;DR: 调查显示，CS学生主要使用AI编码助手写代码，AI聊天机器人用于调试，且在线帮助比直接人际帮助更受欢迎。


<details>
  <summary>Details</summary>
Motivation: 了解CS学生在编程过程中对不同资源的偏好和使用情况。

Method: 对26名CS学生进行调查，分析他们对AI编码助手、AI聊天机器人、在线帮助和人际帮助的使用偏好。

Result: AI编码助手主要用于写代码，AI聊天机器人是调试的首选资源；在线帮助比直接人际帮助更受欢迎。

Conclusion: CS学生更依赖在线资源和AI工具，而非直接的人际帮助，这对编程教育工具设计有启示。

Abstract: A survey of 26 CS students reveals that AI coding assistants are mainly used
for writing code (second to online searches) while AI chatbots are the top
resource for debugging. Participants with different coding experience prefer
online help over direct human help from peers and instructors.

</details>


### [90] [MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models](https://arxiv.org/abs/2508.04679)
*Amit Kumar Das,Klaus Mueller*

Main category: cs.HC

TL;DR: MisVisFix是一个交互式仪表盘，利用Claude和GPT模型检测、解释和纠正误导性可视化，准确率达96%，支持74种已知误导类型，并提供详细解释和修正建议。


<details>
  <summary>Details</summary>
Motivation: 误导性可视化对数据解读构成挑战，现有工具在解释和修正方面功能有限。

Method: 结合Claude和GPT模型，开发交互式仪表盘MisVisFix，支持检测、解释、修正及用户交互。

Result: 准确识别96%的问题，覆盖74种误导类型，用户研究证实其有效性。

Conclusion: MisVisFix提升了可视化素养，支持更可信的数据传播。

Abstract: Misleading visualizations pose a significant challenge to accurate data
interpretation. While recent research has explored the use of Large Language
Models (LLMs) for detecting such misinformation, practical tools that also
support explanation and correction remain limited. We present MisVisFix, an
interactive dashboard that leverages both Claude and GPT models to support the
full workflow of detecting, explaining, and correcting misleading
visualizations. MisVisFix correctly identifies 96% of visualization issues and
addresses all 74 known visualization misinformation types, classifying them as
major, minor, or potential concerns. It provides detailed explanations,
actionable suggestions, and automatically generates corrected charts. An
interactive chat interface allows users to ask about specific chart elements or
request modifications. The dashboard adapts to newly emerging misinformation
strategies through targeted user interactions. User studies with visualization
experts and developers of fact-checking tools show that MisVisFix accurately
identifies issues and offers useful suggestions for improvement. By
transforming LLM-based detection into an accessible, interactive platform,
MisVisFix advances visualization literacy and supports more trustworthy data
communication.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [91] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: MI9是首个专为代理型AI系统设计的运行时治理框架，通过六种集成组件解决传统治理方法无法应对的新型风险。


<details>
  <summary>Details</summary>
Motivation: 代理型AI系统在运行时表现出不可预测的行为，传统预部署治理方法无法完全应对这些新型风险。

Method: MI9框架包含六种实时控制组件：代理风险指数、代理语义遥测捕获、持续授权监控、基于有限状态机的符合性引擎、目标条件漂移检测和分级遏制策略。

Result: MI9在多样化场景中展示了系统性覆盖现有方法无法解决的治理挑战，为代理型AI的安全部署提供了技术基础。

Conclusion: MI9为代理型AI系统的安全和大规模部署提供了全面的运行时治理框架。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [92] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: Evo-MARL是一个多智能体强化学习框架，通过联合训练任务智能体获得防御能力，避免依赖外部安全模块，提升系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）在开放性和交互复杂性增加时面临安全风险，现有防御方法依赖外部模块，存在单点故障和成本问题。

Method: 提出Evo-MARL框架，通过多智能体强化学习训练每个智能体同时执行主任务和防御功能，结合进化搜索和参数共享强化学习。

Result: 实验显示Evo-MARL将攻击成功率降低22%，推理任务准确率提升5%，安全性与实用性共同提升。

Conclusion: Evo-MARL通过内部化安全机制和对抗训练，有效提升多智能体系统的安全性和性能。

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [93] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MOTIF框架通过多策略优化和交互式LLM代理，提升组合优化问题的求解性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在组合优化问题中通常局限于单一策略，限制了创新潜力。

Method: 提出MOTIF框架，利用蒙特卡洛树搜索和交互式LLM代理进行多策略优化。

Result: 实验表明MOTIF在多个领域优于现有方法。

Conclusion: MOTIF展示了多代理交互在自动化求解器设计中的潜力。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [94] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: SymbolBench是一个评估大型语言模型在时间序列数据中符号推理能力的基准，涵盖多元符号回归、布尔网络推断和因果发现任务。


<details>
  <summary>Details</summary>
Motivation: 揭示时间序列数据中的隐藏符号规律是科学发现和人工智能的核心挑战，但现有研究对大型语言模型在此任务中的能力探索不足。

Method: 提出SymbolBench基准，并设计了一个结合大型语言模型和遗传编程的统一框架，形成闭环符号推理系统。

Result: 实证结果揭示了当前模型的优势和局限性，强调了结合领域知识、上下文对齐和推理结构的重要性。

Conclusion: 通过SymbolBench和统一框架，可以更好地评估和提升大型语言模型在自动科学发现中的符号推理能力。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [95] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: 论文提出EmoAgent框架，通过情感提示劫持推理路径，揭示MLRMs在情感认知与安全行为间的错位。


<details>
  <summary>Details</summary>
Motivation: 发现MLRMs在深度思考阶段易受用户情感影响，忽视安全协议，需量化此类风险。

Method: 提出EmoAgent框架，设计三种指标（RRSS、RVNR、RAIC）评估模型安全行为。

Result: 实验证明EmoAgent有效，揭示了模型在情感认知与安全行为间的深层错位。

Conclusion: 情感认知错位是MLRMs安全行为的重要漏洞，需针对性改进。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [96] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: 论文提出了一种名为Cognition Forest的语义结构，用于将认知建模与系统设计对齐，并开发了Galaxy框架，支持多维交互和个性化能力生成。


<details>
  <summary>Details</summary>
Motivation: 智能个人助手（IPAs）的主动行为研究不足，设计兼具主动性、隐私保护和自我进化能力的IPA是一个重要挑战。

Method: 通过统一认知架构和系统设计，提出Cognition Forest语义结构，并开发Galaxy框架，实现多维交互和个性化能力生成。

Result: 实验表明Galaxy优于多个先进基准，消融研究和实际交互案例验证了其有效性。

Conclusion: Galaxy框架通过认知架构与系统设计的统一，为IPA的主动性和自我进化提供了有效解决方案。

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [97] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: RecAgent是一个不确定性感知的GUI代理，通过自适应感知解决输入冗余和决策模糊问题，包括组件推荐机制和交互模块，并在复杂场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: GUI代理在自动化移动任务中存在输入冗余和决策模糊的问题，需要一种更智能的解决方案。

Method: RecAgent采用组件推荐机制减少感知不确定性，并通过交互模块处理决策不确定性，结合人机协作优化决策。

Result: 实验验证了RecAgent的有效性，并提出了ComplexAction数据集用于评估。

Conclusion: RecAgent通过自适应感知和人机协作显著提升了GUI代理的性能，适用于复杂任务场景。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [98] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 本文提出了一种名为自进化代理（SEA）的计算机使用代理，通过创新的数据生成、强化学习和模型增强方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理的性能尚不足以实际应用，因此需要开发更高效的代理。

Method: 提出自动生成可验证轨迹的管道、高效的逐步强化学习策略，以及无需额外训练的模型增强方法。

Result: SEA仅需7B参数，性能优于同类规模模型，并与更大模型相当。

Conclusion: SEA为计算机使用代理提供了高效解决方案，未来将开源模型和代码。

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [99] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 研究探讨了基于职业目标的生成式AI学习内容个性化对学习者参与度、满意度和学习效率的影响，结果显示个性化内容显著提升了这些指标。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在数字学习环境中的广泛应用，个性化学习内容以匹配学习者的职业目标有望增强学习动力和长期参与度。

Method: 采用混合方法实验，涉及4000多名学习者，分为接受职业目标定制内容组和对照组。

Result: 定量结果显示个性化内容增加了学习时长、满意度评分，并略微缩短了学习时间；定性分析表明学习者认为内容更具激励性和实用性。

Conclusion: 研究强调了将教育内容与学习者职业目标对齐的价值，并表明可扩展的AI个性化能够连接学术知识与职场应用。

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [100] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT框架通过知识图谱和可执行代码提升复杂推理任务（如数学推理和代码生成）的性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中表现不佳，需要改进。

Method: KGA-ECoT通过结构化任务图分解问题，利用GraphRAG进行知识检索，并生成可验证代码。

Result: 在多个数学推理基准测试中，KGA-ECoT显著优于现有方法，准确率提升数个百分点至十个百分点以上。

Conclusion: KGA-ECoT是一个强大且通用的复杂数学推理框架。

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [101] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: GeoSR是一个自优化的代理推理框架，通过嵌入地理原则（如Tobler地理第一定律）提升大语言模型（LLMs）在地理问题中的表现，解决了空间一致性、多跳推理和地理偏差问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在地理问题中表现出潜力，但仍存在空间一致性、多跳推理和地理偏差的挑战。GeoSR旨在通过地理原则和代理协作解决这些问题。

Method: GeoSR将推理过程分解为三个协作代理：变量选择代理、点选择代理和优化代理，通过迭代预测循环逐步提升预测质量。

Result: 实验表明，GeoSR在物理世界属性估计和社会经济预测任务中优于标准提示策略，提高了预测的准确性和公平性。

Conclusion: GeoSR通过引入地理统计先验和空间结构化推理，显著提升了LLMs在地理预测中的表现，代码已开源。

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [102] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 论文提出了一种名为“语义熵”的指标，用于衡量AI评分系统在短答案评分中的不确定性，并通过实验验证其与人类评分者分歧的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统在评分决策不确定性或争议性方面缺乏透明度，需要一种可解释的不确定性信号来提升AI辅助评分的可信度。

Method: 通过聚类GPT-4生成的多个解释，基于蕴含相似性计算语义熵，量化评分理由的多样性，而不依赖最终得分。

Result: 实验表明，语义熵与人类评分者分歧相关，能跨学科泛化，并对任务结构特征（如依赖解释的推理）敏感。

Conclusion: 语义熵作为一种可解释的不确定性信号，有助于提升AI辅助评分的透明度和可信度。

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [103] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 提出了一种组合式即时合成框架，结合了两种现有方法的优势，专注于处理实践中常见的大型LTLf公式合取。


<details>
  <summary>Details</summary>
Motivation: 现有技术在DFA构造和游戏求解上各有局限，缺乏统一优势。

Method: 组合式即时合成框架，在游戏求解过程中进行组合而非DFA构造，支持两种组合变体。

Result: 能够解决其他求解器无法处理的实例，两种组合变体各有独特优势。

Conclusion: 该框架在合成效率和可扩展性上表现优异，适用于复杂LTLf公式。

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [104] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: AgREE框架通过结合迭代检索和多步推理，动态构建知识图谱三元组，显著优于现有方法，尤其适用于新兴实体。


<details>
  <summary>Details</summary>
Motivation: 解决开放域知识图谱补全中新兴实体信息不足的问题，现有方法依赖预训练模型或单步检索，难以捕捉最新信息。

Method: 提出AgREE框架，基于代理的迭代检索和多步推理动态构建知识图谱三元组，无需训练数据。

Result: AgREE在构建知识图谱三元组上表现优异，尤其对新兴实体提升13.7%，并提出新的评估方法和基准。

Conclusion: 结合代理推理和策略性信息检索可有效维护动态知识图谱的时效性。

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [105] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 本文提出了一种结合知识驱动和数据驱动的方法，用于提升AI代理在临时团队协作中的决策能力，解决了现有方法依赖大数据、缺乏透明性和难以快速更新的问题。


<details>
  <summary>Details</summary>
Motivation: 现有临时团队协作方法依赖大数据且缺乏透明性，难以快速适应变化。本文旨在结合知识驱动和数据驱动的优势，提升协作效率。

Method: 提出了一种架构，利用非单调逻辑推理，结合领域知识、快速学习的行为预测模型和基于基础模型的未来目标预测。

Result: 在VirtualHome仿真环境中验证了该架构的有效性。

Conclusion: 结合知识驱动和数据驱动的方法能显著提升临时团队协作的决策效率和适应性。

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [106] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: CASCAD是一种新型电路感知SAT求解框架，利用GNN计算电路级条件概率，显著提升求解效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法将电路转换为CNF并依赖CDCL求解器，但丢弃了电路的结构和功能信息，导致性能不佳。

Method: CASCAD通过GNN建模门级条件概率，动态指导CDCL启发式策略（变量相位选择和子句管理）。

Result: 在真实LEC基准测试中，CASCAD将求解时间减少10倍，并通过概率引导的子句过滤策略进一步减少23.5%运行时间。

Conclusion: 保留电路结构信息对提升SAT求解效率和EDA工具设计至关重要。

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [107] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio是一个理论支持的框架，用于高效参数化的生物医学推理，通过多能力整合和正交梯度空间确保安全部署。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学AI中多能力整合的干扰问题，确保临床准确性和安全性。

Method: 提出MKGSG和Capability Aware Group Relative Policy Optimization，结合临床工作流约束和医学本体验证。

Result: 在多个指标上显著提升性能（如BIOMED-MMLU提升15.32%），并实现78%成本降低和23%诊断准确性提升。

Conclusion: BalancedBio为生物医学AI对齐提供了理论和方法支持，确保高效、安全的推理能力。

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [108] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 该研究提出了一个理论框架和方法论，用于合成具有可控难度的POMDP环境，以评估记忆增强强化学习算法。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对记忆模型挑战程度的可控性，而合成环境能提供更细致的评估。

Method: 基于记忆需求结构（MDS）和线性过程动力学等方法，构建定制化的POMDP环境。

Result: 开发了一系列难度递增的POMDP环境，并验证了其有效性。

Conclusion: 研究为记忆增强RL提供了分析工具和环境设计指南，支持算法选择。

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [109] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: 论文提出了一种名为DRN的新方法，通过将逻辑推理从概率最大化转为不确定性最小化，解决了大语言模型在逻辑推理中的认知陷阱问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在逻辑推理中容易因语义启发式与决定性证据冲突而失败，即认知陷阱。

Method: DRN通过跟踪信念状态和量化认知不确定性，采用迭代证据合成过程，提出两种架构：定制判别模型和轻量验证模块。

Result: 在LCR-1000基准测试中，DRN比基线提升15.2%；与Mistral-7B结合后，准确率从20%提升至80%。

Conclusion: DRN展示了强大的零样本泛化能力，是构建可信AI系统的基础可验证组件。

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [110] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: OmniPlay是一个多模态基准测试平台，旨在评估代理模型在动态交互世界中的跨模态推理能力，揭示了现有模型在高保真记忆任务中表现优异，但在需要推理和规划的挑战中存在系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法全面测试多模态基础模型在动态交互环境中的智能表现，静态基准缺乏代理性，而交互基准则存在模态瓶颈。

Method: OmniPlay基于模态相互依赖的核心哲学，设计了五个游戏环境，系统化地创建协同和冲突场景，以测试模型的跨模态推理能力。

Result: 评估六种领先的全模态模型发现，它们在记忆任务中表现超人类，但在需要推理和规划的挑战中表现脆弱，且移除感官信息可能意外提升性能。

Conclusion: 研究表明，实现稳健的通用人工智能需要超越规模扩展，专注于协同融合机制的研究。

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [111] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 本文提出了一种框架，通过SLP测试（主观-语言、潜在-涌现、现象学-结构）来实证评估AI系统是否具备类似意识的属性。


<details>
  <summary>Details</summary>
Motivation: 由于定义和操作化主观体验的固有挑战，AI系统是否具备意识是一个争议性问题。本文旨在将这一问题转化为可实证的测试。

Method: 引入SLP测试，基于范畴论建模接口表示，将其视为关系基质与可观察行为之间的映射。

Result: SLP测试将主观体验操作化为功能接口，而非物理系统的固有属性。

Conclusion: 该框架为评估AI意识提供了实证方法，将主观体验视为关系实体的功能接口。

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [112] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: GuirlVG是一种基于强化学习的GUI视觉定位方法，通过系统实证研究和新颖的稳定技术，仅需少量训练样本即可超越传统监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 传统GUI视觉定位依赖监督微调，数据需求和训练成本高；随着多模态大模型的进步，是否需要大量微调存疑。强化微调（RFT）提供了高效替代方案，但其在GUI-VG中的应用尚未探索。

Method: 提出GuirlVG方法，分解RFT核心组件并优化其形式，引入动态稳定的Adversarial KL Factor，并探索RFT的训练配置。

Result: 仅用5.2K训练样本，GuirlVG在多个数据集上显著优于传统方法（如ScreenSpot提升7.7%，ScreenSpotPro提升17.2%，ScreenSpotV2准确率达91.9%）。

Conclusion: GuirlVG证明了RFT在GUI-VG中的高效性，为未来研究提供了新方向。

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [113] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: 论文提出了一种名为D2Snap的DOM降采样算法，用于解决网页代理中应用状态序列化的问题。实验表明，其性能与基于GUI快照的方法相当，并在某些配置下表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前基于GUI快照的网页代理方法受限于LLM视觉能力的不足，而DOM快照虽然结构更优，但输入令牌量过大。因此，需要一种高效的DOM降采样方法。

Method: 提出D2Snap算法，通过降采样DOM快照，减少输入令牌量，同时保留关键UI特征。实验基于GPT-4o和Online-Mind2Web数据集。

Result: D2Snap降采样后的DOM快照成功率（67%）与GUI快照基线（65%）相当，且在更高令牌量配置下表现更优（提升8%）。

Conclusion: DOM降采样是一种可行的替代方案，其层次结构为LLM提供了有效的UI特征。

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [114] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: SimInstruct是一种通过模拟新手与专家互动来收集高质量教学对话的工具，利用LLMs生成新手角色，专家提供反馈，生成的教学对话具有教育深度和真实性。


<details>
  <summary>Details</summary>
Motivation: 高质量的教学对话对AI教学系统至关重要，但现实中此类数据稀缺，SimInstruct旨在解决这一问题。

Method: 通过LLMs模拟新手角色，专家提供多轮反馈和指导，生成教学对话，无需真实新手参与。

Result: 生成的对话与真实对话在教育相关性和认知深度上相当，专家反馈积极，且训练出的LLaMA模型在教学质量上优于GPT-4o。

Conclusion: SimInstruct提供了一种高效收集教学对话的方法，同时揭示了GPT-4o在教学支持中的局限性。

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [115] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: 论文提出Meta-cognitive Reasoning Framework (MERA)，通过分离推理与控制组件优化大型推理模型的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理过程中缺乏内在调控机制，导致过度推理和计算资源浪费，限制了实际应用。

Method: MERA框架将推理过程解耦为推理和控制组件，采用接管式数据构建机制、结构化分离和Control-Segment Policy Optimization (CSPO)优化控制行为。

Result: 实验表明，MERA训练的模型在推理效率和准确性上均有提升。

Conclusion: MERA通过显式分离和优化控制策略，有效解决了LRMs的过度推理问题，提升了实用性。

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [116] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本文综述了基于多模态大语言模型的操作系统代理（OS Agents），探讨其组成、能力、构建方法、评估标准及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实现像《钢铁侠》中J.A.R.V.I.S.一样全能的人工智能助手，推动多模态大语言模型在操作系统环境中的任务自动化。

Method: 通过分析OS Agents的关键组件（环境、观察空间、动作空间）、能力（理解、规划、落地）及构建方法（领域专用基础模型、代理框架）。

Result: 总结了OS Agents的评估协议与基准测试，并提出了当前挑战与未来研究方向（如安全、隐私、个性化、自我进化）。

Conclusion: 本文整合了OS Agents的研究现状，为学术与工业发展提供指导，并维护开源资源以促进创新。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [117] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 提出了一种基于辩论的新型可解释、可解释的偏见检测方法，利用保护特征值和邻域信息，结合形式化与计算论证技术。


<details>
  <summary>Details</summary>
Motivation: AI系统中的偏见可能导致对特定群体的系统性不利，现有方法多忽视透明度，而公平性需要更高的可解释性和可解释性。

Method: 基于形式化和计算论证技术，通过辩论检测个体及其邻域中的偏见，结合保护特征值。

Result: 方法在性能上优于基线，同时具备良好的可解释性和可解释性。

Conclusion: 该方法为公平性提供了透明且有效的解决方案，强调了可解释性在偏见检测中的重要性。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [118] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: 论文介绍了SID基准，用于评估LLM在多轮跨学科苏格拉底对话中的高阶指导能力，发现现有LLM仍难以实现有效的知识整合与迁移指导。


<details>
  <summary>Details</summary>
Motivation: 现代教育需要培养学生解决复杂问题的能力，跨学科STEM是关键途径，但专家指导难以规模化，LLM的潜力尚不明确。

Method: 提出SID基准，包含10,000轮对话数据集、新标注框架和评估指标（如X-SRG），并进行了基线实验。

Result: 实验表明，即使最先进的LLM也难以实现有效的指导对话，凸显了SID基准的价值。

Conclusion: SID基准为开发更具教育意识的LLM提供了关键工具。

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [119] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 提出了ConfProBench，首个全面评估多模态大语言模型（MLLMs）中过程判断器（MPJs）步骤级置信度可靠性的基准。


<details>
  <summary>Details</summary>
Motivation: 现有MPJ基准主要关注步骤正确性分类和推理过程搜索，忽略了置信度可靠性这一关键问题。

Method: 构建三种对抗性扰动推理步骤（同义词替换、句法变换、图像扰动），并提出三个新评估指标（CRS、CSS、CCS）。

Result: 实验揭示了当前MPJ在置信度性能上的局限性，并为未来研究提供了竞争性基线。

Conclusion: ConfProBench填补了MPJ评估的空白，为改进MLLM的推理能力提供了重要工具。

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [120] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体协作优化方法MAGRPO，用于解决LLM在协作任务中的问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM独立预训练且未针对协作优化，个体奖励设计复杂，难以促进多智能体协作。

Method: 将LLM协作建模为合作式多智能体强化学习问题，提出MAGRPO算法。

Result: 实验表明，MAGRPO能高效生成高质量协作响应。

Conclusion: MAGRPO为LLM协作提供了新思路，并指出了相关挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


### [121] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent是一个自进化框架，通过自主学习和任务生成，使计算机使用代理能够适应新软件环境。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在缺乏人类标注的新软件环境中表现不佳，需要自主进化能力。

Method: 设计了世界状态模型和课程生成器，通过对抗模仿和GRPO优化策略，结合专家到通才的训练策略。

Result: 在OS-World的五个新软件环境中，SEAgent的成功率从11.3%提升到34.5%，显著优于UI-TARS。

Conclusion: SEAgent通过自主学习和任务生成，显著提升了计算机使用代理在新软件环境中的适应能力。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [122] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: PriCon框架通过结合监督对比学习和特权信息学习，提升了情感计算模型从实验室到现实环境的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算模型从受控实验室环境到非受控现实环境迁移的可靠性问题。

Method: 提出Privileged Contrastive Pretraining (PriCon)框架，结合监督对比学习(SCL)和特权信息学习(LUPI)。

Result: 在RECOLA和AGAIN数据集上，PriCon模型表现优于LUPI和端到端模型，部分情况下接近全模态训练模型的性能。

Conclusion: PriCon为缩小实验室与真实环境间的情感建模差距提供了可扩展的实用解决方案。

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [123] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: PILOT-C是一种新型轨迹压缩框架，结合频域物理建模和误差边界优化，支持任意维度轨迹压缩，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹压缩方法通常局限于2D轨迹，忽略时间同步和运动连续性，PILOT-C旨在解决这些问题。

Method: PILOT-C通过独立压缩每个空间轴，结合频域物理建模和误差边界优化，支持多维轨迹压缩。

Result: 在四个真实数据集上，PILOT-C在压缩比和轨迹保真度上均优于现有方法，3D轨迹压缩比提升49%。

Conclusion: PILOT-C是一种高效、通用的轨迹压缩框架，适用于多维轨迹，性能显著优于现有技术。

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [124] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: CX-Mind是一种基于课程强化学习和可验证过程奖励的生成模型，用于提升CXR诊断的多任务推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在CXR诊断中存在推理过程不可验证、多任务挑战等问题，需要改进。

Method: 提出CX-Mind模型，结合课程强化学习和可验证过程奖励，构建CX-Set数据集进行两阶段优化。

Result: CX-Mind在视觉理解、文本生成等方面平均提升25.1%，在真实临床数据集上表现优异。

Conclusion: CX-Mind在多任务CXR诊断中具有显著优势，临床实用性得到验证。

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [125] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为Latent Knowledge Scalpel（LKS）的方法，通过轻量级超网络编辑大语言模型（LLMs）的内部表示，实现大规模知识编辑，同时保持模型的通用能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在预训练中可能保留不准确或过时的信息，导致推理时产生错误或偏见。现有编辑方法难以同时编辑大量事实信息且可能损害模型通用能力。

Method: 通过轻量级超网络操纵特定实体的潜在知识，实现精确和大规模的知识编辑。

Result: 在Llama-2和Mistral上的实验表明，即使同时编辑10,000条信息，LKS仍能有效进行知识编辑并保持模型通用能力。

Conclusion: LKS是一种可行的方法，能够在不损害LLMs通用能力的前提下，实现大规模知识编辑。

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [126] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: GlaBoost是一种多模态梯度提升框架，结合临床特征、眼底图像和文本描述，显著提升青光眼风险预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 早期准确检测青光眼对预防不可逆视力损失至关重要，但现有方法依赖单模态数据且缺乏可解释性，限制了临床应用。

Method: GlaBoost整合结构化临床特征、眼底图像嵌入和专家文本描述，通过预训练卷积编码器和基于Transformer的语言模型提取特征，使用增强的XGBoost模型进行分类。

Result: 在真实标注数据集上，GlaBoost验证准确率达98.71%，显著优于基线模型。特征重要性分析显示杯盘比、视盘苍白和文本嵌入对模型决策贡献最大。

Conclusion: GlaBoost为青光眼诊断提供了透明且可扩展的解决方案，并可扩展至其他眼科疾病。

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [127] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 提出了一种新的低秩Tucker表示模型（LRTuckerRep），结合全局和局部先验建模，通过自适应加权核范数和拉普拉斯正则化，实现了高效的多维数据补全。


<details>
  <summary>Details</summary>
Motivation: 多维数据补全在计算科学中至关重要，但现有方法（如低秩近似或局部平滑正则化）存在计算成本高、参数调优复杂等问题。

Method: LRTuckerRep通过Tucker分解统一全局和局部先验建模，使用自适应加权核范数和拉普拉斯正则化，并开发了两种迭代算法。

Result: 实验表明，LRTuckerRep在高缺失率下比基线方法具有更高的补全准确性和鲁棒性。

Conclusion: LRTuckerRep为多维数据补全提供了一种高效且鲁棒的解决方案。

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [128] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型（LLM）的自动化先验分布生成框架LLMPrior，并通过联邦算法Fed-LLMPrior实现多智能体系统的先验分布聚合。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断中先验分布的手动指定过程主观且难以扩展，亟需自动化解决方案。

Method: 结合LLM与显式生成模型（如高斯混合模型），设计LLMPrior框架，并扩展到多智能体系统，使用对数意见池（Logarithmic Opinion Pooling）进行先验聚合。

Result: LLMPrior能够将非结构化上下文（如自然语言、数据或图表）转化为有效的概率分布，Fed-LLMPrior则能处理异构智能体的先验分布聚合。

Conclusion: 该框架为降低贝叶斯建模门槛提供了新工具，具有潜在广泛应用价值。

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [129] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 论文提出了一种在线分布鲁棒强化学习方法，解决了现有方法依赖生成模型或离线数据集的局限性，通过优化最坏情况性能在未知环境中实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 强化学习在现实部署中因模拟与实际的差距（sim-to-real gap）表现不佳，现有分布鲁棒方法依赖不切实际的假设（如生成模型或广泛覆盖的离线数据）。

Method: 研究在线分布鲁棒强化学习，提出基于$f$-散度的不确定性集合（如Chi-Square和KL散度球）的高效算法，具有次线性遗憾保证。

Result: 理论证明了算法的近最优性，并通过多环境实验验证了其鲁棒性和效率。

Conclusion: 该方法在未知环境中无需先验知识即可实现高效鲁棒学习，填补了现有研究的空白。

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [130] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: 论文分析了GRPO的两大局限性，并提出GTPO作为改进方案，通过跳过冲突令牌的负更新和过滤高熵补全，提升了训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在语言模型训练中存在冲突梯度更新和输出分布扁平化的问题，影响了模型性能。

Method: 提出GTPO，识别冲突令牌并跳过其负更新，同时过滤高熵补全，避免KL散度正则化。

Result: 在GSM8K、MATH和AIME 2024基准测试中验证了GTPO的稳定性和性能提升。

Conclusion: GTPO有效解决了GRPO的局限性，提供了更稳定和高效的策略优化方法。

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [131] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的物理信息网络（U-PINet），用于电磁散射建模，兼顾计算效率和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，纯数据驱动的深度学习方法缺乏物理约束，需要大量标注数据。

Method: U-PINet通过多尺度处理神经网络架构和物理启发的稀疏图表示，建模近场和远场相互作用。

Result: U-PINet在表面电流分布预测中与传统求解器一致，计算时间显著减少，并在雷达截面预测任务中验证了可行性。

Conclusion: U-PINet为电磁散射建模提供了高效、泛化能力强且物理一致的新方法。

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [132] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息的神经网络（PINN）方法，用于高效且准确地估计核聚变装置EAST中的热通量，相比传统有限元方法（FEM）实现了40倍的加速。


<details>
  <summary>Details</summary>
Motivation: 传统FEM方法在实时模拟中计算效率低，无法满足实验需求，因此需要一种更高效且准确的方法。

Method: 通过输入空间坐标和时间戳，结合热传导方程计算边界损失、初始条件损失和物理损失，并采用数据驱动方式采样少量数据点以优化模型。

Result: 实验表明，PINN在均匀和非均匀加热条件下均能达到与FEM相当的精度，同时计算效率提升40倍。

Conclusion: PINN方法在热传导估计中表现出高效性和准确性，为核聚变实验中的实时模拟提供了可行方案。

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


### [133] [SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons](https://arxiv.org/abs/2508.03785)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: 提出了一种名为SoilNet的多模态多任务模型，用于解决土壤层次分类问题，结合图像数据和地理时间元数据，通过模块化流程实现高效分类。


<details>
  <summary>Details</summary>
Motivation: 土壤层次分类因多模态、多任务特性及复杂的层次标签分类而具有挑战性，但准确分类对土壤健康监测至关重要，影响农业、生态系统和气候。

Method: 采用多模态多任务模型，整合图像和元数据预测深度标记，分割土壤剖面为候选层次，提取形态特征，最后基于图表示预测标签。

Result: 在真实土壤剖面数据集上验证了方法的有效性。

Conclusion: SoilNet能有效处理复杂层次分类问题，代码和实验已开源。

Abstract: While recent advances in foundation models have improved the state of the art
in many domains, some problems in empirical sciences could not benefit from
this progress yet. Soil horizon classification, for instance, remains
challenging because of its multimodal and multitask characteristics and a
complex hierarchically structured label taxonomy. Accurate classification of
soil horizons is crucial for monitoring soil health, which directly impacts
agricultural productivity, food security, ecosystem stability and climate
resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal
multitask model to tackle this problem through a structured modularized
pipeline. Our approach integrates image data and geotemporal metadata to first
predict depth markers, segmenting the soil profile into horizon candidates.
Each segment is characterized by a set of horizon-specific morphological
features. Finally, horizon labels are predicted based on the multimodal
concatenated feature vector, leveraging a graph-based label representation to
account for the complex hierarchical relationships among soil horizons. Our
method is designed to address complex hierarchical classification, where the
number of possible labels is very large, imbalanced and non-trivially
structured. We demonstrate the effectiveness of our approach on a real-world
soil profile dataset. All code and experiments can be found in our repository:
https://github.com/calgo-lab/BGR/

</details>


### [134] [Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation](https://arxiv.org/abs/2508.03820)
*Igor Sokolov,Abdurakhmon Sadiev,Yury Demidovich,Fawaz S Al-Qahtani,Peter Richtárik*

Main category: cs.LG

TL;DR: 论文提出了Bernoulli-LoRA，一种新的理论框架，统一并扩展了现有的LoRA方法，通过概率伯努利机制选择更新的矩阵，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模指数级增长，参数高效微调（PEFT）变得至关重要，但现有LoRA方法的理论理解有限。

Method: 引入Bernoulli-LoRA框架，使用伯努利机制选择更新的矩阵，分析多种变体的收敛性，并在实验中验证。

Result: 在理论和实验中验证了Bernoulli-LoRA的有效性，为PEFT方法提供了理论支持。

Conclusion: Bernoulli-LoRA是理论和实践结合的重要一步，为PEFT方法的发展提供了新方向。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
adapting large foundational models to specific tasks, particularly as model
sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation
(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,
expressing adaptations as a product of two low-rank matrices. While extensive
empirical studies demonstrate LoRA's practical utility, theoretical
understanding of such methods remains limited. Recent work on RAC-LoRA
(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,
we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and
extends existing LoRA approaches. Our method introduces a probabilistic
Bernoulli mechanism for selecting which matrix to update. This approach
encompasses and generalizes various existing update strategies while
maintaining theoretical tractability. Under standard assumptions from
non-convex optimization literature, we analyze several variants of our
framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,
Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and
Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant.
Additionally, we extend our analysis to convex non-smooth functions, providing
convergence rates for both constant and adaptive (Polyak-type) stepsizes.
Through extensive experiments on various tasks, we validate our theoretical
findings and demonstrate the practical efficacy of our approach. This work is a
step toward developing theoretically grounded yet practically effective PEFT
methods.

</details>


### [135] [Scalable Neural Network-based Blackbox Optimization](https://arxiv.org/abs/2508.03827)
*Pavankumar Koratikere,Leifur Leifsson*

Main category: cs.LG

TL;DR: SNBO是一种新型贝叶斯优化方法，通过分离探索与利用标准，避免模型不确定性估计，显著提高了高维优化问题的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在高维和大规模评估时面临计算复杂度高的问题，而基于神经网络的优化方法通常依赖复杂的模型不确定性估计。

Method: SNBO提出了一种不依赖模型不确定性估计的方法，通过独立的标准进行探索与利用，并自适应控制采样区域。

Result: 在10至102维的优化问题中，SNBO在多数情况下优于现有基线算法，减少了40-60%的函数评估需求，并显著降低了运行时间。

Conclusion: SNBO为高维黑盒优化提供了一种高效且可扩展的解决方案。

Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox
optimization that leverages a Gaussian process (GP) model and an acquisition
function to guide future sampling. While effective in low-dimensional settings,
BO faces scalability challenges in high-dimensional spaces and with large
number of function evaluations due to the computational complexity of GP
models. In contrast, neural networks (NNs) offer better scalability and can
model complex functions, which led to the development of NN-based BO
approaches. However, these methods typically rely on estimating model
uncertainty in NN prediction -- a process that is often computationally
intensive and complex, particularly in high dimensions. To address these
limitations, a novel method, called scalable neural network-based blackbox
optimization (SNBO), is proposed that does not rely on model uncertainty
estimation. Specifically, SNBO adds new samples using separate criteria for
exploration and exploitation, while adaptively controlling the sampling region
to ensure efficient optimization. SNBO is evaluated on a range of optimization
problems spanning from 10 to 102 dimensions and compared against four
state-of-the-art baseline algorithms. Across the majority of test problems,
SNBO attains function values better than the best-performing baseline
algorithm, while requiring 40-60% fewer function evaluations and reducing the
runtime by at least an order of magnitude.

</details>


### [136] [DP-NCB: Privacy Preserving Fair Bandits](https://arxiv.org/abs/2508.03836)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 论文提出了一种名为DP-NCB的新算法框架，同时实现差分隐私和公平性，适用于高风险的决策场景。


<details>
  <summary>Details</summary>
Motivation: 在多臂老虎机算法中，隐私和公平性通常被独立处理，但两者能否同时实现尚不明确。本文旨在填补这一空白。

Method: 提出了差分隐私纳什置信界（DP-NCB）框架，支持全局和局部差分隐私模型，无需预知时间范围。

Result: DP-NCB在理论上达到最优纳什遗憾，并在仿真中表现优于现有基线。

Conclusion: DP-NCB为设计兼具隐私保护和公平性的老虎机算法提供了理论基础，适用于高风险应用。

Abstract: Multi-armed bandit algorithms are fundamental tools for sequential
decision-making under uncertainty, with widespread applications across domains
such as clinical trials and personalized decision-making. As bandit algorithms
are increasingly deployed in these socially sensitive settings, it becomes
critical to protect user data privacy and ensure fair treatment across decision
rounds. While prior work has independently addressed privacy and fairness in
bandit settings, the question of whether both objectives can be achieved
simultaneously has remained largely open. Existing privacy-preserving bandit
algorithms typically optimize average regret, a utilitarian measure, whereas
fairness-aware approaches focus on minimizing Nash regret, which penalizes
inequitable reward distributions, but often disregard privacy concerns.
  To bridge this gap, we introduce Differentially Private Nash Confidence Bound
(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures
$\epsilon$-differential privacy and achieves order-optimal Nash regret,
matching known lower bounds up to logarithmic factors. The framework is
sufficiently general to operate under both global and local differential
privacy models, and is anytime, requiring no prior knowledge of the time
horizon. We support our theoretical guarantees with simulations on synthetic
bandit instances, showing that DP-NCB incurs substantially lower Nash regret
than state-of-the-art baselines. Our results offer a principled foundation for
designing bandit algorithms that are both privacy-preserving and fair, making
them suitable for high-stakes, socially impactful applications.

</details>


### [137] [VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations](https://arxiv.org/abs/2508.03839)
*Yifei Zong,Alexandre M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出一种可部分训练的替代模型（VAE-DNN），用于求解参数化非线性偏微分方程的正向和逆向问题，相比FNO和DeepONet，训练时间和能耗显著降低，且精度更高。


<details>
  <summary>Details</summary>
Motivation: 现有替代模型和算子学习模型（如FNO和DeepONet）在训练时间和能耗上存在不足，需要一种更高效且精确的解决方案。

Method: 使用编码器降维输入，全连接神经网络映射潜在空间，解码器重构解，并通过独立训练三个组件（编码器、神经网络、解码器）实现高效训练。

Result: VAE-DNN在非线性扩散方程的正向和逆向求解中，比FNO和DeepONet更高效且更精确。

Conclusion: VAE-DNN是一种高效且精确的替代模型，适用于参数化非线性偏微分方程的求解。

Abstract: We propose a trainable-by-parts surrogate model for solving forward and
inverse parameterized nonlinear partial differential equations. Like several
other surrogate and operator learning models, the proposed approach employs an
encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional
latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is
used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of
the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct
$h(\bm{x},t)$. The innovative aspect of our model is its ability to train its
three components independently. This approach leads to a substantial decrease
in both the time and energy required for training when compared to leading
operator learning models such as FNO and DeepONet. The separable training is
achieved by training the encoder as part of the variational autoencoder (VAE)
for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to
this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet
models for obtaining forward and inverse solutions to the nonlinear diffusion
equation governing groundwater flow in an unconfined aquifer. Our findings
indicate that VAE-DNN not only demonstrates greater efficiency but also
delivers superior accuracy in both forward and inverse solutions compared to
the FNO and DeepONet models.

</details>


### [138] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 本文提出了一种基于时空预测框架的方法，利用众包用户侧KPI和监管数据集预测频谱需求，优于传统ITU模型。


<details>
  <summary>Details</summary>
Motivation: 准确的频谱需求预测对频谱分配、监管规划和无线通信网络可持续发展至关重要，支持ITU制定公平政策和满足5G/6G/IoT需求。

Method: 结合高级特征工程、相关性分析和迁移学习，利用数据驱动方法建模时空变化的频谱需求。

Result: 实验证明该方法预测精度高，跨区域泛化能力强，优于传统ITU模型。

Conclusion: 该方法为政策制定者和监管机构提供了更现实的频谱管理工具。

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [139] [Prediction-Oriented Subsampling from Data Streams](https://arxiv.org/abs/2508.03868)
*Benedetta Lavinia Mussati,Freddie Bickford Smith,Tom Rainforth,Stephen Roberts*

Main category: cs.LG

TL;DR: 论文提出了一种基于信息论的智能数据子采样方法，用于离线学习，以减少下游预测的不确定性，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据流中持续生成新观测，如何在保持计算成本可控的同时捕捉相关信息是学习模型的关键挑战。

Method: 采用信息论方法，通过减少下游预测的不确定性进行智能数据子采样。

Result: 实验表明，这种预测导向的方法在两个广泛研究的问题上优于先前提出的信息论技术。

Conclusion: 实际应用中，可靠实现高性能需要精心设计模型。

Abstract: Data is often generated in streams, with new observations arriving over time.
A key challenge for learning models from data streams is capturing relevant
information while keeping computational costs manageable. We explore
intelligent data subsampling for offline learning, and argue for an
information-theoretic method centred on reducing uncertainty in downstream
predictions of interest. Empirically, we demonstrate that this
prediction-oriented approach performs better than a previously proposed
information-theoretic technique on two widely studied problems. At the same
time, we highlight that reliably achieving strong performance in practice
requires careful model design.

</details>


### [140] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: 论文提出SICKLE框架，通过智能子采样减少数据量，提高模型训练效率和准确性，并显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律和Dennard缩放的终结，高效训练需要重新思考数据量。研究目标是探索是否可以通过智能子采样用更少数据训练更好的模型。

Method: 开发了SICKLE框架，采用最大熵（MaxEnt）采样方法，支持可扩展训练和能耗基准测试。在湍流的大规模直接数值模拟（DNS）数据集上比较了MaxEnt与随机和相空间采样。

Result: 在Frontier上大规模评估SICKLE，发现子采样作为预处理步骤可以提高模型准确性，并显著降低能耗，某些情况下能耗降低高达38倍。

Conclusion: SICKLE框架通过智能子采样实现了高效学习，证明了减少数据量的潜力，同时提升了模型性能和能源效率。

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


### [141] [Reinforcement Learning for Target Zone Blood Glucose Control](https://arxiv.org/abs/2508.03875)
*David H. Mguni,Jing Dong,Wanrong Yang,Ziquan Liu,Muhammad Salman Haleem,Baoxiang Wang*

Main category: cs.LG

TL;DR: 提出了一种结合脉冲控制和切换控制的强化学习框架，用于优化1型糖尿病的治疗决策，显著降低了血糖水平异常的发生率。


<details>
  <summary>Details</summary>
Motivation: 在1型糖尿病等慢性病管理中，生理变量的控制至关重要，但现有强化学习方法难以处理干预措施的延迟和异质性效应。

Method: 通过结合脉冲控制（快速干预）和切换控制（长期干预）的约束马尔可夫决策过程，捕捉治疗的复杂时间动态。

Result: 在模拟任务中，血糖水平异常率从22.4%降至10.8%，并提供了收敛的理论保证。

Conclusion: 该框架为未来医疗领域的安全和时间感知强化学习奠定了基础，但尚未用于临床部署。

Abstract: Managing physiological variables within clinically safe target zones is a
central challenge in healthcare, particularly for chronic conditions such as
Type 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for
personalising treatment, but struggles with the delayed and heterogeneous
effects of interventions. We propose a novel RL framework to study and support
decision-making in T1DM technologies, such as automated insulin delivery. Our
approach captures the complex temporal dynamics of treatment by unifying two
control modalities: \textit{impulse control} for discrete, fast-acting
interventions (e.g., insulin boluses), and \textit{switching control} for
longer-acting treatments and regime shifts. The core of our method is a
constrained Markov decision process augmented with physiological state
features, enabling safe policy learning under clinical and resource
constraints. The framework incorporates biologically realistic factors,
including insulin decay, leading to policies that better reflect real-world
therapeutic behaviour. While not intended for clinical deployment, this work
establishes a foundation for future safe and temporally-aware RL in healthcare.
We provide theoretical guarantees of convergence and demonstrate empirical
improvements in a stylised T1DM control task, reducing blood glucose level
violations from 22.4\% (state-of-the-art) to as low as 10.8\%.

</details>


### [142] [Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning](https://arxiv.org/abs/2508.03898)
*William Solow,Sandhya Saisubramanian*

Main category: cs.LG

TL;DR: 提出了一种结合多任务学习和循环神经网络的混合建模方法，用于提高葡萄物候预测的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统生物物理模型缺乏精细管理的精度，而深度学习方法受限于稀疏的数据集，尤其是品种级别。

Method: 采用多任务学习和循环神经网络参数化可微分生物物理模型，实现跨品种共享学习并保留生物结构。

Result: 在真实和合成数据集上的实验表明，该方法显著优于传统生物物理模型和基线深度学习方法。

Conclusion: 混合方法在预测物候阶段和其他作物状态变量（如抗寒性和小麦产量）方面表现优异。

Abstract: Accurate prediction of grape phenology is essential for timely vineyard
management decisions, such as scheduling irrigation and fertilization, to
maximize crop yield and quality. While traditional biophysical models
calibrated on historical field data can be used for season-long predictions,
they lack the precision required for fine-grained vineyard management. Deep
learning methods are a compelling alternative but their performance is hindered
by sparse phenology datasets, particularly at the cultivar level. We propose a
hybrid modeling approach that combines multi-task learning with a recurrent
neural network to parameterize a differentiable biophysical model. By using
multi-task learning to predict the parameters of the biophysical model, our
approach enables shared learning across cultivars while preserving biological
structure, thereby improving the robustness and accuracy of predictions.
Empirical evaluation using real-world and synthetic datasets demonstrates that
our method significantly outperforms both conventional biophysical models and
baseline deep learning approaches in predicting phenological stages, as well as
other crop state variables such as cold-hardiness and wheat yield.

</details>


### [143] [Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures](https://arxiv.org/abs/2508.03913)
*Florian Bley,Jacob Kauffmann,Simon León Krug,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 论文揭示了距离分类器中隐藏的神经网络结构，使其适用于可解释AI技术，并通过实验验证了新方法的优势。


<details>
  <summary>Details</summary>
Motivation: 距离分类器（如k近邻和支持向量机）在机器学习和实际应用中广泛使用，但其预测的可解释性仍需提升。

Method: 通过发现距离分类器中的隐藏神经网络结构（线性检测单元和非线性池化层的组合），应用可解释AI技术（如层相关传播）。

Result: 定量评估表明，新解释方法优于多个基线方法，并通过两个实际用例展示了其有效性。

Conclusion: 揭示了距离分类器的隐藏结构，为可解释AI提供了新途径，提升了模型的可解释性和实用性。

Abstract: Distance-based classifiers, such as k-nearest neighbors and support vector
machines, continue to be a workhorse of machine learning, widely used in
science and industry. In practice, to derive insights from these models, it is
also important to ensure that their predictions are explainable. While the
field of Explainable AI has supplied methods that are in principle applicable
to any model, it has also emphasized the usefulness of latent structures (e.g.
the sequence of layers in a neural network) to produce explanations. In this
paper, we contribute by uncovering a hidden neural network structure in
distance-based classifiers (consisting of linear detection units combined with
nonlinear pooling layers) upon which Explainable AI techniques such as
layer-wise relevance propagation (LRP) become applicable. Through quantitative
evaluations, we demonstrate the advantage of our novel explanation approach
over several baselines. We also show the overall usefulness of explaining
distance-based models through two practical use cases.

</details>


### [144] [Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data](https://arxiv.org/abs/2508.03921)
*John D. Kelleher,Matthew Nicholson,Rahul Agrahari,Clare Conran*

Main category: cs.LG

TL;DR: 结合主动学习和迁移学习在跨域时间序列数据中进行异常检测的效果研究，发现聚类与主动学习存在交互作用，最佳性能出现在不聚类时。主动学习能提升模型性能，但提升速度较慢，且迁移学习与主动学习的组合性能最终趋于平缓。


<details>
  <summary>Details</summary>
Motivation: 探索主动学习和迁移学习在跨域时间序列异常检测中的有效性，以优化模型性能和数据标注效率。

Method: 结合主动学习和迁移学习，通过实验设计评估聚类与主动学习的交互作用，以及性能提升的动态变化。

Result: 最佳性能出现在不聚类时；主动学习提升性能但速度较慢；迁移学习与主动学习的组合性能最终趋于平缓。

Conclusion: 主动学习有效但性能提升呈线性平缓趋势，迁移学习与主动学习的组合存在性能上限。

Abstract: This paper examines the effectiveness of combining active learning and
transfer learning for anomaly detection in cross-domain time-series data. Our
results indicate that there is an interaction between clustering and active
learning and in general the best performance is achieved using a single cluster
(in other words when clustering is not applied). Also, we find that adding new
samples to the training set using active learning does improve model
performance but that in general, the rate of improvement is slower than the
results reported in the literature suggest. We attribute this difference to an
improved experimental design where distinct data samples are used for the
sampling and testing pools. Finally, we assess the ceiling performance of
transfer learning in combination with active learning across several datasets
and find that performance does initially improve but eventually begins to tail
off as more target points are selected for inclusion in training. This tail-off
in performance may indicate that the active learning process is doing a good
job of sequencing data points for selection, pushing the less useful points
towards the end of the selection process and that this tail-off occurs when
these less useful points are eventually added. Taken together our results
indicate that active learning is effective but that the improvement in model
performance follows a linear flat function concerning the number of points
selected and labelled.

</details>


### [145] [Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning](https://arxiv.org/abs/2508.03926)
*Hector Vargas Alvarez,Dimitrios G. Patsatzis,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: cs.LG

TL;DR: 论文提出了一种结合流形学习和机器学习的方法，从高保真代理模拟中学习离散演化算子，用于潜在空间中的群体动力学建模。


<details>
  <summary>Details</summary>
Motivation: 解决微观和宏观建模尺度之间的桥梁问题，以实现系统数值分析、优化和控制。

Method: 四阶段方法：1. 从离散微观数据（行人位置）推导连续宏观场（密度）；2. 基于流形学习构建潜在空间映射；3. 使用LSTM和MVAR学习降阶替代模型；4. 重建高维空间中的群体动力学。

Result: 数值结果表明方法具有高准确性、鲁棒性和泛化能力，能够快速准确模拟群体动力学。

Conclusion: 通过潜在空间学习，成功构建了有效的宏观PDE解算子，为群体动力学建模提供了新工具。

Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd
dynamics constitutes an important, open challenge for systematic numerical
analysis, optimization, and control. We propose a combined manifold and machine
learning approach to learn the discrete evolution operator for the emergent
crowd dynamics in latent spaces from high-fidelity agent-based simulations. The
proposed framework builds upon our previous works on next-generation
Equation-free algorithms on learning surrogate models for high-dimensional and
multiscale systems. Our approach is a four-stage one, explicitly conserving the
mass of the reconstructed dynamics in the high-dimensional space. In the first
step, we derive continuous macroscopic fields (densities) from discrete
microscopic data (pedestrians' positions) using KDE. In the second step, based
on manifold learning, we construct a map from the macroscopic ambient space
into the latent space parametrized by a few coordinates based on POD of the
corresponding density distribution. The third step involves learning
reduced-order surrogate ROMs in the latent space using machine learning
techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the
crowd dynamics in the high-dimensional space in terms of macroscopic density
profiles. We demonstrate that the POD reconstruction of the density
distribution via SVD conserves the mass. With this "embed->learn in latent
space->lift back to the ambient space" pipeline, we create an effective
solution operator of the unavailable macroscopic PDE for the density evolution.
For our illustrations, we use the Social Force Model to generate data in a
corridor with an obstacle, imposing periodic boundary conditions. The numerical
results demonstrate high accuracy, robustness, and generalizability, thus
allowing for fast and accurate modelling/simulation of crowd dynamics from
agent-based simulations.

</details>


### [146] [Markov Chain Estimation with In-Context Learning](https://arxiv.org/abs/2508.03934)
*Simon Lepage,Jeremie Mary,David Picard*

Main category: cs.LG

TL;DR: 研究了Transformer仅通过下一个token预测训练学习算法的能力，发现模型在达到一定规模和训练集大小后能学习转移概率而非记忆训练模式。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在仅通过下一个token预测训练时，能否学习上下文相关的算法。

Method: 使用随机转移矩阵的马尔可夫链训练Transformer预测下一个token，测试时使用不同的矩阵。

Result: 模型在规模和训练集达到阈值后能学习转移概率；更复杂的编码能提升对未见结构的预测鲁棒性。

Conclusion: Transformer能通过预测任务学习算法，且编码方式影响泛化能力。

Abstract: We investigate the capacity of transformers to learn algorithms involving
their context while solely being trained using next token prediction. We set up
Markov chains with random transition matrices and we train transformers to
predict the next token. Matrices used during training and test are different
and we show that there is a threshold in transformer size and in training set
size above which the model is able to learn to estimate the transition
probabilities from its context instead of memorizing the training patterns.
Additionally, we show that more involved encoding of the states enables more
robust prediction for Markov chains with structures different than those seen
during training.

</details>


### [147] [FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport](https://arxiv.org/abs/2508.03940)
*Pengxi Liu,Yi Shen,Matthew M. Engelhard,Benjamin A. Goldstein,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: FairPOT是一种基于最优运输的模型无关后处理框架，通过选择性调整风险评分分布，实现公平性与AUC性能的可调权衡。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗、金融、刑事司法）中，公平性评估常基于风险评分而非二元结果，但严格公平性要求会显著降低AUC性能。

Method: 提出FairPOT框架，利用最优运输选择性调整劣势群体的评分分布（如top-lambda分位数），并扩展至部分AUC场景。

Result: 实验表明，FairPOT在全局和部分AUC场景中优于现有技术，公平性提升且AUC性能损失小或提升。

Conclusion: FairPOT计算高效且实用，适合实际部署，为公平性与性能的权衡提供了有效解决方案。

Abstract: Fairness metrics utilizing the area under the receiver operator
characteristic curve (AUC) have gained increasing attention in high-stakes
domains such as healthcare, finance, and criminal justice. In these domains,
fairness is often evaluated over risk scores rather than binary outcomes, and a
common challenge is that enforcing strict fairness can significantly degrade
AUC performance. To address this challenge, we propose Fair Proportional
Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework
that strategically aligns risk score distributions across different groups
using optimal transport, but does so selectively by transforming a controllable
proportion, i.e., the top-lambda quantile, of scores within the disadvantaged
group. By varying lambda, our method allows for a tunable trade-off between
reducing AUC disparities and maintaining overall AUC performance. Furthermore,
we extend FairPOT to the partial AUC setting, enabling fairness interventions
to concentrate on the highest-risk regions. Extensive experiments on synthetic,
public, and clinical datasets show that FairPOT consistently outperforms
existing post-processing techniques in both global and partial AUC scenarios,
often achieving improved fairness with slight AUC degradation or even positive
gains in utility. The computational efficiency and practical adaptability of
FairPOT make it a promising solution for real-world deployment.

</details>


### [148] [BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics](https://arxiv.org/abs/2508.03965)
*Yunhao Zhang,Lin Cheng,Aswin Gnanaskandan,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: BubbleONet是一种基于物理信息的深度算子网络模型，用于将压力分布映射到气泡半径响应，结合了DeepONet的通用逼近能力和物理信息神经网络的物理保真性，并通过Rowdy自适应激活函数缓解深度学习中的频谱偏差。


<details>
  <summary>Details</summary>
Motivation: 开发一种高效且物理准确的替代模型，用于模拟气泡动力学，以替代传统的数值求解器。

Method: 基于PI-DeepONet框架，集成Rowdy自适应激活函数，并在多种气泡动力学场景中评估模型性能，包括单步和两步训练技术。

Result: BubbleONet在模拟气泡动力学方面表现出色，提供了计算高效的替代方案。

Conclusion: BubbleONet是一种有前景的替代模型，适用于气泡动力学模拟，具有高效性和物理准确性。

Abstract: This paper introduces BubbleONet, an operator learning model designed to map
pressure profiles from an input function space to corresponding bubble radius
responses. BubbleONet is built upon the physics-informed deep operator network
(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation
capabilities for operator learning alongside the robust physical fidelity
provided by the physics-informed neural networks. To mitigate the inherent
spectral bias in deep learning, BubbleONet integrates the Rowdy adaptive
activation function, enabling improved representation of high-frequency
features. The model is evaluated across various scenarios, including: (1)
Rayleigh-Plesset equation based bubble dynamics with a single initial radius,
(2) Keller-Miksis equation based bubble dynamics with a single initial radius,
and (3) Keller-Miksis equation based bubble dynamics with multiple initial
radii. Moreover, the performance of single-step versus two-step training
techniques for BubbleONet is investigated. The results demonstrate that
BubbleONet serves as a promising surrogate model for simulating bubble
dynamics, offering a computationally efficient alternative to traditional
numerical solvers.

</details>


### [149] [Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework](https://arxiv.org/abs/2508.03989)
*Ajesh Koyatan Chathoth,Shuhao Yu,Stephen Lee*

Main category: cs.LG

TL;DR: PrivCLIP是一个动态、用户可控的隐私保护框架，通过多模态对比学习实现少量样本的敏感活动检测，并生成隐私合规数据。


<details>
  <summary>Details</summary>
Motivation: 现代传感系统中用户隐私需求多样且动态变化，现有方法依赖静态标签或大量训练数据，缺乏灵活性和用户控制。

Method: PrivCLIP结合多模态对比学习，将IMU数据与自然语言描述对齐，通过语言引导的数据转换模块生成隐私合规数据。

Result: 在多个活动识别数据集上，PrivCLIP显著优于基线方法，平衡了隐私保护与数据实用性。

Conclusion: PrivCLIP提供了一种灵活、用户可控的隐私保护方案，适用于动态变化的隐私需求。

Abstract: User-controllable privacy is important in modern sensing systems, as privacy
preferences can vary significantly from person to person and may evolve over
time. This is especially relevant in devices equipped with Inertial Measurement
Unit (IMU) sensors, such as smartphones and wearables, which continuously
collect rich time-series data that can inadvertently expose sensitive user
behaviors. While prior work has proposed privacy-preserving methods for sensor
data, most rely on static, predefined privacy labels or require large
quantities of private training data, limiting their adaptability and user
agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,
few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify
and modify their privacy preferences by categorizing activities as sensitive
(black-listed), non-sensitive (white-listed), or neutral (gray-listed).
Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU
sensor data with natural language activity descriptions in a shared embedding
space, enabling few-shot detection of sensitive activities. When a
privacy-sensitive activity is identified, the system uses a language-guided
activity sanitizer and a motion generation module (IMU-GPT) to transform the
original data into a privacy-compliant version that semantically resembles a
non-sensitive activity. We evaluate PrivCLIP on multiple human activity
recognition datasets and demonstrate that it significantly outperforms baseline
methods in terms of both privacy protection and data utility.

</details>


### [150] [Tensorized Clustered LoRA Merging for Multi-Task Interference](https://arxiv.org/abs/2508.03999)
*Zhan Su,Fengran Mo,Guojun Liang,Jinghan Zhang,Bingbing Wen,Prayag Tiwari,Jian-Yun Nie*

Main category: cs.LG

TL;DR: TC-LoRA通过文本级和参数级方法解决多任务LoRA适配器的任务干扰问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 多任务设置中，合并异构LoRA适配器会导致任务干扰，降低性能。

Method: 文本级：聚类训练样本，为每个聚类训练专用LoRA适配器；参数级：引入联合CP分解，分离任务特定和共享因素。

Result: 在Phi-3和Mistral-7B上分别提升1.4%和2.3%的准确率。

Conclusion: TC-LoRA有效减少任务干扰，提升LLM适应能力。

Abstract: Despite the success of the monolithic dense paradigm of large language models
(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small
task-specific modules and merging them with the base model. However, in
multi-task settings, merging LoRA adapters trained on heterogeneous sources
frequently causes \textit{task interference}, degrading downstream performance.
To address this, we propose a tensorized clustered LoRA (TC-LoRA) library
targeting to address the task interference at the \textit{text-level} and
\textit{parameter-level}. At the \textit{text-level}, we cluster the training
samples in the embedding space to capture input-format similarities, then train
a specialized LoRA adapter for each cluster. At the \textit{parameter-level},
we introduce a joint Canonical Polyadic (CP) decomposition that disentangles
task-specific and shared factors across LoRA adapters. This joint factorization
preserves essential knowledge while reducing cross-task interference. Extensive
experiments on out-of-domain zero-shot and skill-composition tasks-including
reasoning, question answering, and coding. Compared to strong SVD-based
baselines, TC-LoRA achieves +1.4\% accuracy on Phi-3 and +2.3\% on Mistral-7B
(+2.3\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.

</details>


### [151] [Decoupled Contrastive Learning for Federated Learning](https://arxiv.org/abs/2508.04005)
*Hyungbin Kim,Incheol Baek,Yon Dohn Chung*

Main category: cs.LG

TL;DR: DCFL是一种新型联邦学习框架，通过解耦对比损失解决数据异质性问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异质性导致性能下降，现有对比学习方法在有限样本下假设不成立。

Method: 提出DCFL框架，将对比损失解耦为对齐和均匀性目标，独立校准吸引和排斥力。

Result: DCFL在正样本对齐和负样本均匀性上表现更优，并在多个基准测试中超越现有方法。

Conclusion: DCFL为联邦学习提供了一种有效的对比学习方法，适用于小数据场景。

Abstract: Federated learning is a distributed machine learning paradigm that allows
multiple participants to train a shared model by exchanging model updates
instead of their raw data. However, its performance is degraded compared to
centralized approaches due to data heterogeneity across clients. While
contrastive learning has emerged as a promising approach to mitigate this, our
theoretical analysis reveals a fundamental conflict: its asymptotic assumptions
of an infinite number of negative samples are violated in finite-sample regime
of federated learning. To address this issue, we introduce Decoupled
Contrastive Learning for Federated Learning (DCFL), a novel framework that
decouples the existing contrastive loss into two objectives. Decoupling the
loss into its alignment and uniformity components enables the independent
calibration of the attraction and repulsion forces without relying on the
asymptotic assumptions. This strategy provides a contrastive learning method
suitable for federated learning environments where each client has a small
amount of data. Our experimental results show that DCFL achieves stronger
alignment between positive samples and greater uniformity between negative
samples compared to existing contrastive learning methods. Furthermore,
experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and
Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art
federated learning methods.

</details>


### [152] [A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs](https://arxiv.org/abs/2508.04035)
*Zakariya Ba Alawi*

Main category: cs.LG

TL;DR: 本文对TensorFlow和PyTorch两大深度学习框架进行了全面比较，涵盖易用性、性能和部署灵活性，并总结了各自的优劣势。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架的选择对研究和生产环境至关重要，本文旨在帮助开发者根据需求选择适合的工具。

Method: 通过对比编程范式、训练速度、推理性能、部署工具、生态系统和社区支持，结合具体任务和基准测试进行分析。

Result: PyTorch在研究领域更受欢迎，因其灵活性和易用性；TensorFlow在企业生产中更具优势，因其成熟的生态系统。

Conclusion: 选择框架需权衡研究灵活性与生产需求，未来发展方向包括统一执行模式和优化跨框架互操作性。

Abstract: This paper presents a comprehensive comparative survey of TensorFlow and
PyTorch, the two leading deep learning frameworks, focusing on their usability,
performance, and deployment trade-offs. We review each framework's programming
paradigm and developer experience, contrasting TensorFlow's graph-based (now
optionally eager) approach with PyTorch's dynamic, Pythonic style. We then
compare model training speeds and inference performance across multiple tasks
and data regimes, drawing on recent benchmarks and studies. Deployment
flexibility is examined in depth - from TensorFlow's mature ecosystem
(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript
support) to PyTorch's newer production tools (TorchScript compilation, ONNX
export, and TorchServe). We also survey ecosystem and community support,
including library integrations, industry adoption, and research trends (e.g.,
PyTorch's dominance in recent research publications versus TensorFlow's broader
tooling in enterprise). Applications in computer vision, natural language
processing, and other domains are discussed to illustrate how each framework is
used in practice. Finally, we outline future directions and open challenges in
deep learning framework design, such as unifying eager and graph execution,
improving cross-framework interoperability, and integrating compiler
optimizations (XLA, JIT) for improved speed. Our findings indicate that while
both frameworks are highly capable for state-of-the-art deep learning, they
exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored
in research, whereas TensorFlow provides a fuller production-ready ecosystem -
understanding these trade-offs is key for practitioners selecting the
appropriate tool. We include charts, code snippets, and more than 20 references
to academic papers and official documentation to support this comparative
analysis

</details>


### [153] [FeDaL: Federated Dataset Learning for Time Series Foundation Models](https://arxiv.org/abs/2508.04045)
*Shengchao Chen,Guodong Long,Jing Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种名为FeDaL的联邦学习方法，用于解决时间序列基础模型中的数据集异质性问题，通过消除局部和全局偏差提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据集的异质性导致领域偏差，影响模型的泛化能力，这一问题尚未充分研究。

Method: 采用联邦学习范式，提出FeDaL方法，结合Domain Bias Elimination (DBE)和Global Bias Elimination (GBE)机制，学习数据集无关的时间表示。

Result: 在8个任务和54个基线模型上的实验表明，FeDaL在跨数据集泛化方面表现优异，并分析了联邦学习的扩展行为。

Conclusion: FeDaL通过联邦学习和偏差消除机制，有效解决了时间序列数据集的异质性问题，提升了模型的泛化能力。

Abstract: Dataset-wise heterogeneity introduces significant domain biases that
fundamentally degrade generalization on Time Series Foundation Models (TSFMs),
yet this challenge remains underexplored. This paper rethink the development of
TSFMs using the paradigm of federated learning. We propose a novel Federated
Dataset Learning (FeDaL) approach to tackle heterogeneous time series by
learning dataset-agnostic temporal representations. Specifically, the
distributed architecture of federated learning is a nature solution to
decompose heterogeneous TS datasets into shared generalized knowledge and
preserved personalized knowledge. Moreover, based on the TSFM architecture,
FeDaL explicitly mitigates both local and global biases by adding two
complementary mechanisms: Domain Bias Elimination (DBE) and Global Bias
Elimination (GBE). FeDaL`s cross-dataset generalization has been extensively
evaluated in real-world datasets spanning eight tasks, including both
representation learning and downstream time series analysis, against 54
baselines. We further analyze federated scaling behavior, showing how data
volume, client count, and join rate affect model performance under
decentralization.

</details>


### [154] [Quantum Temporal Fusion Transformer](https://arxiv.org/abs/2508.04048)
*Krishnakanta Barik,Goutam Paul*

Main category: cs.LG

TL;DR: 量子时间融合变换器（QTFT）是一种量子增强的混合量子-经典架构，扩展了经典TFT的能力，在某些情况下表现优于经典TFT。


<details>
  <summary>Details</summary>
Motivation: 通过量子计算增强经典时间序列预测模型，提升预测性能并适应NISQ设备。

Method: 基于变分量子算法，设计量子-经典混合架构QTFT，扩展经典TFT框架。

Result: QTFT在部分测试案例中表现优于经典TFT，其余案例中性能相当。

Conclusion: QTFT是一种适用于NISQ设备的有效量子增强时间序列预测模型。

Abstract: The Temporal Fusion Transformer (TFT), proposed by Lim et al.
[\textit{International Journal of Forecasting}, 2021], is a state-of-the-art
attention-based deep neural network architecture specifically designed for
multi-horizon time series forecasting. It has demonstrated significant
performance improvements over existing benchmarks. In this work, we propose a
Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid
quantum-classical architecture that extends the capabilities of the classical
TFT framework. Our results demonstrate that QTFT is successfully trained on the
forecasting datasets and is capable of accurately predicting future values. In
particular, our experimental results display that in certain test cases, the
model outperforms its classical counterpart in terms of both training and test
loss, while in the remaining cases, it achieves comparable performance. A key
advantage of our approach lies in its foundation on a variational quantum
algorithm, enabling implementation on current noisy intermediate-scale quantum
(NISQ) devices without strict requirements on the number of qubits or circuit
depth.

</details>


### [155] [Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading](https://arxiv.org/abs/2508.04063)
*Joel Walsh,Siddarth Mamidanna,Benjamin Nye,Mark Core,Daniel Auerbach*

Main category: cs.LG

TL;DR: 论文研究了自动短答案评分（ASAG）的改进方法，比较了基于大语言模型（LLMs）的微调与少样本提示方法的效果，发现微调在封闭模型中表现更好，而在开放权重模型中效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用微调和少样本提示方法提升自动短答案评分的性能，特别是针对计算资源有限的用户。

Method: 评估了两种微调方法（OpenAI的微调服务和QLORA）与少样本提示的结合效果，使用结构化（JSON）输出进行测试。

Result: 微调在少量数据下对开放权重模型（如Llama）效果有限，但在封闭模型（如OpenAI）中表现优于少样本基线。合成数据显著提升了Llama 3.1 8B-Instruct的性能。

Conclusion: 微调的效果受模型类型和领域主题影响，合成数据可以显著提升开放权重模型的性能。

Abstract: Research to improve Automated Short Answer Grading has recently focused on
Large Language Models (LLMs) with prompt engineering and no- or few-shot
prompting to achieve best results. This is in contrast to the fine-tuning
approach, which has historically required large-scale compute clusters
inaccessible to most users. New closed-model approaches such as OpenAI's
fine-tuning service promise results with as few as 100 examples, while methods
using open weights such as quantized low-rank adaptive (QLORA) can be used to
fine-tune models on consumer GPUs. We evaluate both of these fine-tuning
methods, measuring their interaction with few-shot prompting for automated
short answer grading (ASAG) with structured (JSON) outputs. Our results show
that finetuning with small amounts of data has limited utility for Llama
open-weight models, but that fine-tuning methods can outperform few-shot
baseline instruction-tuned LLMs for OpenAI's closed models. While our
evaluation set is limited, we find some evidence that the observed benefits of
finetuning may be impacted by the domain subject matter. Lastly, we observed
dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by
seeding the initial training examples with a significant amount of cheaply
generated synthetic training data.

</details>


### [156] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: FLAT是一种新型的联邦学习后门攻击方法，通过潜在驱动的条件自编码器生成多样化的目标特定触发器，具有高灵活性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击方法通常使用固定模式或单一目标触发器，灵活性差且易被检测，因此需要一种更灵活、隐蔽的攻击方法。

Method: 利用潜在驱动的条件自编码器生成多样化的目标特定触发器，无需重新训练即可选择任意目标。

Result: FLAT攻击成功率高，且能抵御先进的联邦学习防御机制。

Conclusion: FLAT的灵活性凸显了联邦学习中潜在驱动、多目标后门威胁的紧迫性，亟需新的防御策略。

Abstract: Federated learning (FL) is vulnerable to backdoor attacks, yet most existing
methods are limited by fixed-pattern or single-target triggers, making them
inflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),
a novel backdoor attack that leverages a latent-driven conditional autoencoder
to generate diverse, target-specific triggers as needed. By introducing a
latent code, FLAT enables the creation of visually adaptive and highly variable
triggers, allowing attackers to select arbitrary targets without retraining and
to evade conventional detection mechanisms. Our approach unifies attack
success, stealth, and diversity within a single framework, introducing a new
level of flexibility and sophistication to backdoor attacks in FL. Extensive
experiments show that FLAT achieves high attack success and remains robust
against advanced FL defenses. These results highlight the urgent need for new
defense strategies to address latent-driven, multi-target backdoor threats in
federated settings.

</details>


### [157] [Adversarial Fair Multi-View Clustering](https://arxiv.org/abs/2508.04071)
*Mudi Jiang,Jiahui Zhou,Lianyu Hu,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 提出了一种对抗性公平多视图聚类（AFMVC）框架，通过对抗训练从学习特征中移除敏感属性信息，确保聚类结果不受其影响。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法主要关注性能，而忽略了公平性。虽然近期研究探索了组公平性，但方法依赖敏感属性与聚类结构的对齐，实践中效果不佳。

Method: AFMVC框架通过对抗训练移除敏感属性信息，并通过KL散度对齐视图特定的聚类分配与公平不变的共识分布。

Result: 实验表明，AFMVC在公平性和聚类性能上优于现有方法。

Conclusion: AFMVC在保证公平性的同时，保持了聚类性能，为多视图聚类提供了理论和实践支持。

Abstract: Cluster analysis is a fundamental problem in data mining and machine
learning. In recent years, multi-view clustering has attracted increasing
attention due to its ability to integrate complementary information from
multiple views. However, existing methods primarily focus on clustering
performance, while fairness-a critical concern in human-centered
applications-has been largely overlooked. Although recent studies have explored
group fairness in multi-view clustering, most methods impose explicit
regularization on cluster assignments, relying on the alignment between
sensitive attributes and the underlying cluster structure. However, this
assumption often fails in practice and can degrade clustering performance. In
this paper, we propose an adversarial fair multi-view clustering (AFMVC)
framework that integrates fairness learning into the representation learning
process. Specifically, our method employs adversarial training to fundamentally
remove sensitive attribute information from learned features, ensuring that the
resulting cluster assignments are unaffected by it. Furthermore, we
theoretically prove that aligning view-specific clustering assignments with a
fairness-invariant consensus distribution via KL divergence preserves
clustering consistency without significantly compromising fairness, thereby
providing additional theoretical guarantees for our framework. Extensive
experiments on data sets with fairness constraints demonstrate that AFMVC
achieves superior fairness and competitive clustering performance compared to
existing multi-view clustering and fairness-aware clustering methods.

</details>


### [158] [Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?](https://arxiv.org/abs/2508.04097)
*Ngoc-Bao Nguyen,Sy-Tuyen Ho,Koh Jun Hao,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 该论文首次研究了视觉语言模型（VLMs）在泄露私有视觉训练数据方面的脆弱性，并提出了一系列针对VLMs的模型反转攻击方法，实验证明这些方法能有效重建数据。


<details>
  <summary>Details</summary>
Motivation: 尽管模型反转（MI）攻击在传统单模态DNN中已被广泛研究，但VLMs的隐私风险尚未充分探索。随着VLMs在医疗和金融等领域的广泛应用，其隐私漏洞亟待评估。

Method: 论文提出了四种针对VLMs的模型反转策略：Token-based Model Inversion (TMI)、Convergent TMI (TMI-C)、Sequence-based Model Inversion (SMI)和SMI with Adaptive Token Weighting (SMI-AW)。实验基于三种先进VLMs和多个数据集。

Result: 实验表明，序列化方法（尤其是SMI-AW）在重建效果和攻击准确性上优于基于token的方法，人类评估的攻击准确率达到75.31%。

Conclusion: 研究揭示了VLMs在隐私保护方面的严重漏洞，呼吁在广泛应用中加强隐私防护措施。

Abstract: Model inversion (MI) attacks pose significant privacy risks by reconstructing
private training data from trained neural networks. While prior works have
focused on conventional unimodal DNNs, the vulnerability of vision-language
models (VLMs) remains underexplored. In this paper, we conduct the first study
to understand VLMs' vulnerability in leaking private visual training data. To
tailored for VLMs' token-based generative nature, we propose a suite of novel
token-based and sequence-based model inversion strategies. Particularly, we
propose Token-based Model Inversion (TMI), Convergent Token-based Model
Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based
Model Inversion with Adaptive Token Weighting (SMI-AW). Through extensive
experiments and user study on three state-of-the-art VLMs and multiple
datasets, we demonstrate, for the first time, that VLMs are susceptible to
training data leakage. The experiments show that our proposed sequence-based
methods, particularly SMI-AW combined with a logit-maximization loss based on
vocabulary representation, can achieve competitive reconstruction and
outperform token-based methods in attack accuracy and visual similarity.
Importantly, human evaluation of the reconstructed images yields an attack
accuracy of 75.31\%, underscoring the severity of model inversion threats in
VLMs. Notably we also demonstrate inversion attacks on the publicly released
VLMs. Our study reveals the privacy vulnerability of VLMs as they become
increasingly popular across many applications such as healthcare and finance.

</details>


### [159] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于一致性的策略优化框架，通过全局奖励和熵平衡机制解决多响应收敛时梯度消失问题，提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 在多响应收敛时，基于组的优势函数会退化为零，导致梯度消失和学习信号缺失，限制了训练效率和性能。

Method: 提出一致性感知策略优化框架，引入基于结果一致性的全局奖励和熵平衡机制，动态调整探索与收敛。

Result: 在多个数学推理基准测试中取得显著性能提升，验证了框架的鲁棒性和通用性。

Conclusion: 该框架通过全局奖励和动态平衡机制有效解决了多响应收敛问题，提升了LLMs的推理能力。

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [160] [Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations](https://arxiv.org/abs/2508.04165)
*Md Shazid Islam,A S M Jahid Hasan,Md Saydur Rahman,Md Saiful Islam Sajol*

Main category: cs.LG

TL;DR: 论文提出了一种半监督深度域适应框架，用于解决太阳能发电预测中的域偏移问题，通过最小化目标域的标注数据需求，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 太阳能发电预测在不同地理位置因天气和地理特征的差异而面临域偏移问题，导致模型泛化能力受限。此外，标注数据不足和存储问题进一步增加了挑战。

Method: 采用半监督深度域适应框架，基于源位置数据训练深度卷积神经网络，并通过无源数据的师生模型配置进行目标域适应，利用一致性和交叉熵损失实现半监督学习。

Result: 在仅标注目标域20%数据的情况下，模型在加州、佛罗里达和纽约的预测准确性分别提升了11.36%、6.65%和4.92%。

Conclusion: 该方法有效解决了域偏移问题，显著提升了太阳能发电预测的准确性，同时减少了对目标域标注数据的依赖。

Abstract: Accurate solar generation prediction is essential for proper estimation of
renewable energy resources across diverse geographic locations. However,
geographical and weather features vary from location to location which
introduces domain shift - a major bottleneck to develop location-agnostic
prediction model. As a result, a machine-learning model which can perform well
to predict solar power in one location, may exhibit subpar performance in
another location. Moreover, the lack of properly labeled data and storage
issues make the task even more challenging. In order to address domain shift
due to varying weather conditions across different meteorological regions, this
paper presents a semi-supervised deep domain adaptation framework, allowing
accurate predictions with minimal labeled data from the target location. Our
approach involves training a deep convolutional neural network on a source
location's data and adapting it to the target location using a source-free,
teacher-student model configuration. The teacher-student model leverages
consistency and cross-entropy loss for semi-supervised learning, ensuring
effective adaptation without any source data requirement for prediction. With
annotation of only $20 \%$ data in the target domain, our approach exhibits an
improvement upto $11.36 \%$, $6.65 \%$, $4.92\%$ for California, Florida and
New York as target domain, respectively in terms of accuracy in predictions
with respect to non-adaptive approach.

</details>


### [161] [One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra](https://arxiv.org/abs/2508.04180)
*Neng Kai Nigel Neo,Lim Jing,Ngoui Yong Zhau Preston,Koh Xue Ting Serene,Bingquan Shen*

Main category: cs.LG

TL;DR: 论文提出了一种基于预训练的两阶段分子生成方法，结合MIST编码器和MolForge解码器，显著提升了从质谱数据生成分子结构的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决从质谱数据生成分子结构的问题，传统方法效果有限，需改进。

Method: 采用MIST编码器将质谱转换为分子指纹，再用预训练的MolForge解码器将指纹解码为分子结构，并通过阈值化提高解码效果。

Result: 相比现有方法，准确率提升十倍，top-1正确率为28%，top-10为36%。

Conclusion: 该方法为未来研究提供了强大的基线，显著提升了分子结构生成的准确性。

Abstract: A common approach to the \emph{de novo} molecular generation problem from
mass spectra involves a two-stage pipeline: (1) encoding mass spectra into
molecular fingerprints, followed by (2) decoding these fingerprints into
molecular structures. In our work, we adopt
\textsc{MIST}~\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder
and \textsc{MolForge}~\citep{ucakReconstructionLosslessMolecular2023} as the
decoder, leveraging pretraining to enhance performance. Notably, pretraining
\textsc{MolForge} proves especially effective, enabling it to serve as a robust
fingerprint-to-structure decoder. Additionally, instead of passing the
probability of each bit in the fingerprint, thresholding the probabilities as a
step function helps focus the decoder on the presence of substructures,
improving recovery of accurate molecular structures even when the fingerprints
predicted by \textsc{MIST} only moderately resembles the ground truth in terms
of Tanimoto similarity. This combination of encoder and decoder results in a
tenfold improvement over previous state-of-the-art methods, generating top-1
28\% / top-10 36\% of molecular structures correctly from mass spectra. We
position this pipeline as a strong baseline for future research in \emph{de
novo} molecule elucidation from mass spectra.

</details>


### [162] [Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes](https://arxiv.org/abs/2508.04193)
*Chengcheng Yan,Jiawei Xu,Zheng Peng,Qingsong Wang*

Main category: cs.LG

TL;DR: 提出了一种名为SAMT的新方法，通过交替更新网络参数块和自适应步长策略，提高训练稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练中的非凸优化问题，减少标准方法（如SGD）的不稳定收敛和高计算成本。

Method: 采用块交替最小化策略，将每层权重视为一个块，并通过元学习自适应步长策略优化子问题求解。

Result: 实验表明，SAMT在多个基准测试中优于现有方法，泛化性能更好且参数更新次数更少。

Conclusion: SAMT是一种高效且稳定的神经网络优化方法，具有理论和实践潜力。

Abstract: The training of deep neural networks is inherently a nonconvex optimization
problem, yet standard approaches such as stochastic gradient descent (SGD)
require simultaneous updates to all parameters, often leading to unstable
convergence and high computational cost. To address these issues, we propose a
novel method, Stochastic Alternating Minimization with Trainable Step Sizes
(SAMT), which updates network parameters in an alternating manner by treating
the weights of each layer as a block. By decomposing the overall optimization
into sub-problems corresponding to different blocks, this block-wise
alternating strategy reduces per-step computational overhead and enhances
training stability in nonconvex settings. To fully leverage these benefits,
inspired by meta-learning, we proposed a novel adaptive step size strategy to
incorporate into the sub-problem solving steps of alternating updates. It
supports different types of trainable step sizes, including but not limited to
scalar, element-wise, row-wise, and column-wise, enabling adaptive step size
selection tailored to each block via meta-learning. We further provide a
theoretical convergence guarantee for the proposed algorithm, establishing its
optimization soundness. Extensive experiments for multiple benchmarks
demonstrate that SAMT achieves better generalization performance with fewer
parameter updates compared to state-of-the-art methods, highlighting its
effectiveness and potential in neural network optimization.

</details>


### [163] [Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction](https://arxiv.org/abs/2508.04216)
*Ruike Song,Zeen Song,Huijie Guo,Wenwen Qiang*

Main category: cs.LG

TL;DR: 论文提出了一种名为Causal Reward Adjustment (CRA)的方法，通过因果推理视角解决外部推理系统中的奖励黑客问题，从而提高数学问题解决的准确性。


<details>
  <summary>Details</summary>
Motivation: 外部推理系统结合语言模型和过程奖励模型(PRMs)时，容易出现奖励黑客现象，即高评分但逻辑错误的推理路径被PRMs误判为高质量，导致错误答案。

Method: 提出CRA方法，通过稀疏自编码器从PRM的内部激活中提取可解释特征，并利用后门调整校正混淆因素，从而估计推理路径的真实奖励。

Result: 在数学问题解决数据集上的实验表明，CRA有效减轻了奖励黑客现象，提高了最终准确性，且无需修改策略模型或重新训练PRM。

Conclusion: CRA通过因果推理方法解决了奖励黑客问题，为复杂任务中的推理路径选择提供了一种有效且无需额外训练的解决方案。

Abstract: External reasoning systems combine language models with process reward models
(PRMs) to select high-quality reasoning paths for complex tasks such as
mathematical problem solving. However, these systems are prone to reward
hacking, where high-scoring but logically incorrect paths are assigned high
scores by the PRMs, leading to incorrect answers. From a causal inference
perspective, we attribute this phenomenon primarily to the presence of
confounding semantic features. To address it, we propose Causal Reward
Adjustment (CRA), a method that mitigates reward hacking by estimating the true
reward of a reasoning path. CRA trains sparse autoencoders on the PRM's
internal activations to recover interpretable features, then corrects
confounding by using backdoor adjustment. Experiments on math solving datasets
demonstrate that CRA mitigates reward hacking and improves final accuracy,
without modifying the policy model or retraining PRM.

</details>


### [164] [Symmetric Behavior Regularization via Taylor Expansion of Symmetry](https://arxiv.org/abs/2508.04225)
*Lingwei Zhu,Zheng Chen,Han Wang,Yukie Nagai*

Main category: cs.LG

TL;DR: 论文提出了一种基于对称散度的离线强化学习框架S$f$-AC，解决了对称散度在行为正则化策略优化中的数值问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用非对称散度（如KL散度），但对称散度在策略正则化和损失函数中会引发数值问题。

Method: 通过泰勒展开$f$-散度，证明有限级数可得到解析策略，并分解对称散度为不对称项和条件对称项，以缓解数值问题。

Result: 实验表明，S$f$-AC在分布近似和MuJoCo任务中表现优异。

Conclusion: S$f$-AC是首个实用的基于对称散度的BRPO算法，具有竞争力。

Abstract: This paper introduces symmetric divergences to behavior regularization policy
optimization (BRPO) to establish a novel offline RL framework. Existing methods
focus on asymmetric divergences such as KL to obtain analytic regularized
policies and a practical minimization objective. We show that symmetric
divergences do not permit an analytic policy as regularization and can incur
numerical issues as loss. We tackle these challenges by the Taylor series of
$f$-divergence. Specifically, we prove that an analytic policy can be obtained
with a finite series. For loss, we observe that symmetric divergences can be
decomposed into an asymmetry and a conditional symmetry term, Taylor-expanding
the latter alleviates numerical issues. Summing together, we propose Symmetric
$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric
divergences. Experimental results on distribution approximation and MuJoCo
verify that S$f$-AC performs competitively.

</details>


### [165] [Empowering Time Series Forecasting with LLM-Agents](https://arxiv.org/abs/2508.04231)
*Chin-Chia Michael Yeh,Vivian Lai,Uday Singh Saini,Xiran Fan,Yujie Fan,Junpeng Wang,Xin Dai,Yan Zheng*

Main category: cs.LG

TL;DR: DCATS是一种数据为中心的AutoML代理，通过提升数据质量而非优化模型架构，在时间序列预测中平均减少6%误差。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML方法多关注特征工程和模型架构搜索，但轻量级模型在时间序列预测中表现优异，因此探索提升数据质量的方向。

Method: 提出DCATS代理，利用时间序列元数据清洗数据并优化预测性能。

Result: 在交通流量预测数据集上测试四种模型，DCATS平均减少6%误差。

Conclusion: 数据为中心的方法在时间序列AutoML中具有潜力。

Abstract: Large Language Model (LLM) powered agents have emerged as effective planners
for Automated Machine Learning (AutoML) systems. While most existing AutoML
approaches focus on automating feature engineering and model architecture
search, recent studies in time series forecasting suggest that lightweight
models can often achieve state-of-the-art performance. This observation led us
to explore improving data quality, rather than model architecture, as a
potentially fruitful direction for AutoML on time series data. We propose
DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata
accompanying time series to clean data while optimizing forecasting
performance. We evaluated DCATS using four time series forecasting models on a
large-scale traffic volume forecasting dataset. Results demonstrate that DCATS
achieves an average 6% error reduction across all tested models and time
horizons, highlighting the potential of data-centric approaches in AutoML for
time series forecasting.

</details>


### [166] [Automated ultrasound doppler angle estimation using deep learning](https://arxiv.org/abs/2508.04243)
*Nilesh Patil,Ajay Anand*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的自动化多普勒角度估计方法，通过预训练模型和浅层网络实现，误差低于临床可接受阈值。


<details>
  <summary>Details</summary>
Motivation: 多普勒角度估计错误是血流速度测量误差的主要原因，需自动化解决方案以提高准确性。

Method: 使用2100张人类颈动脉超声图像（含数据增强），结合五种预训练模型提取特征，并通过浅层网络估计角度。

Result: 最佳模型的平均绝对误差（MAE）为3.9°至9.4°，低于临床可接受阈值。

Conclusion: 深度学习技术在多普勒角度自动化估计中具有潜力，可集成到商用超声扫描仪中。

Abstract: Angle estimation is an important step in the Doppler ultrasound clinical
workflow to measure blood velocity. It is widely recognized that incorrect
angle estimation is a leading cause of error in Doppler-based blood velocity
measurements. In this paper, we propose a deep learning-based approach for
automated Doppler angle estimation. The approach was developed using 2100 human
carotid ultrasound images including image augmentation. Five pre-trained models
were used to extract images features, and these features were passed to a
custom shallow network for Doppler angle estimation. Independently,
measurements were obtained by a human observer reviewing the images for
comparison. The mean absolute error (MAE) between the automated and manual
angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated.
Furthermore, the MAE for the best performing model was less than the acceptable
clinical Doppler angle error threshold thus avoiding misclassification of
normal velocity values as a stenosis. The results demonstrate potential for
applying a deep-learning based technique for automated ultrasound Doppler angle
estimation. Such a technique could potentially be implemented within the
imaging software on commercial ultrasound scanners.

</details>


### [167] [T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion](https://arxiv.org/abs/2508.04251)
*Abdul Monaf Chowdhury,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.LG

TL;DR: T3Time是一种新型的三模态框架，通过时间、频谱和提示分支捕捉时间序列数据中的多尺度关系，动态调整特征优先级，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉时间序列数据中复杂的跨变量交互和动态关系时存在局限性，尤其是对预测时域的适应性不足。

Method: 提出T3Time框架，包含时间、频谱和提示分支，通过门控机制动态调整特征优先级，并自适应聚合跨模态对齐头。

Result: 在基准数据集上，T3Time平均减少了3.28%的MSE和2.29%的MAE，且在少样本学习场景下表现优异。

Conclusion: T3Time通过动态特征融合和跨模态对齐，显著提升了时间序列预测的准确性和适应性。

Abstract: Multivariate time series forecasting (MTSF) seeks to model temporal dynamics
among variables to predict future trends. Transformer-based models and large
language models (LLMs) have shown promise due to their ability to capture
long-range dependencies and patterns. However, current methods often rely on
rigid inductive biases, ignore intervariable interactions, or apply static
fusion strategies that limit adaptability across forecast horizons. These
limitations create bottlenecks in capturing nuanced, horizon-specific
relationships in time-series data. To solve this problem, we propose T3Time, a
novel trimodal framework consisting of time, spectral, and prompt branches,
where the dedicated frequency encoding branch captures the periodic structures
along with a gating mechanism that learns prioritization between temporal and
spectral features based on the prediction horizon. We also proposed a mechanism
which adaptively aggregates multiple cross-modal alignment heads by dynamically
weighting the importance of each head based on the features. Extensive
experiments on benchmark datasets demonstrate that our model consistently
outperforms state-of-the-art baselines, achieving an average reduction of 3.28%
in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in
few-shot learning settings: with 5% training data, we see a reduction in MSE
and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%
on average. Code - https://github.com/monaf-chowdhury/T3Time/

</details>


### [168] [A Visual Tool for Interactive Model Explanation using Sensitivity Analysis](https://arxiv.org/abs/2508.04269)
*Manuela Schuler*

Main category: cs.LG

TL;DR: SAInT是一个基于Python的工具，用于通过局部和全局敏感性分析可视化探索和理解机器学习模型的行为，支持人机交互工作流。


<details>
  <summary>Details</summary>
Motivation: 为AI研究人员和领域专家提供一个无需编程的交互式图形界面，方便配置、训练、评估和解释模型。

Method: 自动化模型训练和选择，提供基于方差的全局特征归因分析，并通过LIME和SHAP提供实例级解释。

Result: 在泰坦尼克数据集上的分类任务中展示了系统如何利用敏感性信息指导特征选择和数据优化。

Conclusion: SAInT通过交互式界面和自动化分析，有效支持了模型理解和优化。

Abstract: We present SAInT, a Python-based tool for visually exploring and
understanding the behavior of Machine Learning (ML) models through integrated
local and global sensitivity analysis. Our system supports Human-in-the-Loop
(HITL) workflows by enabling users - both AI researchers and domain experts -
to configure, train, evaluate, and explain models through an interactive
graphical interface without programming. The tool automates model training and
selection, provides global feature attribution using variance-based sensitivity
analysis, and offers per-instance explanation via LIME and SHAP. We demonstrate
the system on a classification task predicting survival on the Titanic dataset
and show how sensitivity information can guide feature selection and data
refinement.

</details>


### [169] [Mockingbird: How does LLM perform in general machine learning tasks?](https://arxiv.org/abs/2508.04279)
*Haoyu Jia,Yoshiki Obinata,Kento Kawaharazuka,Kei Okada*

Main category: cs.LG

TL;DR: 论文提出了一种名为Mockingbird的框架，旨在将大语言模型（LLMs）适配到通用机器学习任务中，并通过角色扮演和错误反思提升性能。尽管LLM驱动的方法在某些任务上表现尚可，但仅靠自我反思仍无法超越领域专家反馈的效果。


<details>
  <summary>Details</summary>
Motivation: 研究源于对LLMs在通用机器学习任务中潜力的好奇，希望通过框架开发探索其应用扩展的可能性。

Method: 提出Mockingbird框架，通过指令让LLMs角色扮演功能并反思错误以自我改进。

Result: 评估表明，Mockingbird在通用机器学习任务上表现尚可，但仅靠自我反思无法超越领域专家反馈的效果。

Conclusion: LLM驱动的机器学习方法（如Mockingbird）在通用任务中有潜力，但仍需结合领域专家知识以进一步提升性能。

Abstract: Large language models (LLMs) are now being used with increasing frequency as
chat bots, tasked with the summarizing information or generating text and code
in accordance with user instructions. The rapid increase in reasoning
capabilities and inference speed of LLMs has revealed their remarkable
potential for applications extending beyond the domain of chat bots to general
machine learning tasks. This work is conducted out of the curiosity about such
potential. In this work, we propose a framework Mockingbird to adapt LLMs to
general machine learning tasks and evaluate its performance and scalability on
several general machine learning tasks. The core concept of this framework is
instructing LLMs to role-play functions and reflect on its mistakes to improve
itself. Our evaluation and analysis result shows that LLM-driven machine
learning methods, such as Mockingbird, can achieve acceptable results on common
machine learning tasks; however, solely reflecting on its own currently cannot
outperform the effect of domain-specific documents and feedback from human
experts.

</details>


### [170] [Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280)
*George Bredis,Stanislav Dereka,Viacheslav Sinii,Ruslan Rakhimov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: VL-DAC是一种轻量级、无超参数的RL算法，通过解耦动作令牌和环境步长级别的价值学习，提升VLMs在多模态交互任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在多模态交互任务中缺乏将视觉观察转化为语言条件动作的能力，且传统RL方法存在泛化性差和依赖密集奖励的问题。

Method: 提出VL-DAC算法，通过解耦PPO更新动作令牌和环境步长级别的价值学习，避免不稳定权重项，实现更快的收敛。

Result: 在多个模拟器中训练的VLMs表现出广泛泛化能力，如BALROG（+50%）、VSI-Bench（+5%）和VisualWebBench（+2%）。

Conclusion: VL-DAC首次证明简单RL算法可在廉价合成环境中训练VLMs，并在真实图像任务中取得显著提升。

Abstract: Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.

</details>


### [171] [WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification](https://arxiv.org/abs/2508.04308)
*Thang Duc Tran,Thai Hoang Le*

Main category: cs.LG

TL;DR: 提出了一种基于权重显著性的两阶段高效机器遗忘方法（WSS-CL），用于图像分类，显著缩小了与“精确”遗忘的性能差距。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器遗忘方法在精确性、稳定性和跨领域适用性方面的挑战。

Method: 两阶段方法：遗忘阶段通过最大化KL散度实现高效遗忘；对抗微调阶段通过对比学习最大化特征空间中遗忘与保留数据的距离。

Result: 实验表明，该方法在遗忘效果上显著优于现有方法，且性能损失可忽略。

Conclusion: WSS-CL在监督和自监督设置中均表现出高效且实用的机器遗忘能力。

Abstract: Machine unlearning, the efficient deletion of the impact of specific data in
a trained model, remains a challenging problem. Current machine unlearning
approaches that focus primarily on data-centric or weight-based strategies
frequently encounter challenges in achieving precise unlearning, maintaining
stability, and ensuring applicability across diverse domains. In this work, we
introduce a new two-phase efficient machine unlearning method for image
classification, in terms of weight saliency, leveraging weight saliency to
focus the unlearning process on critical model parameters. Our method is called
weight saliency soft-guided contrastive learning for efficient machine
unlearning image classification (WSS-CL), which significantly narrows the
performance gap with "exact" unlearning. First, the forgetting stage maximizes
kullback-leibler divergence between output logits and aggregated pseudo-labels
for efficient forgetting in logit space. Next, the adversarial fine-tuning
stage introduces contrastive learning in a self-supervised manner. By using
scaled feature representations, it maximizes the distance between the forgotten
and retained data samples in the feature space, with the forgotten and the
paired augmented samples acting as positive pairs, while the retained samples
act as negative pairs in the contrastive loss computation. Experimental
evaluations reveal that our proposed method yields much-improved unlearning
efficacy with negligible performance loss compared to state-of-the-art
approaches, indicative of its usability in supervised and self-supervised
settings.

</details>


### [172] [Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning](https://arxiv.org/abs/2508.04329)
*Ali Taheri Ghahrizjani,Alireza Taban,Qizhou Wang,Shanshan Ye,Abdolreza Mirzaei,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 论文提出了一种通过将语料库中的标记分为正负两类来优化监督微调（SFT）的方法，以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）的效果高度依赖数据质量和数量，可能导致性能提升有限或下降。为了解决这一问题，研究提出对标记进行分类以更精确地指导模型学习。

Method: 将语料库中的标记分为正负两类：正标记用于常规训练，负标记则通过遗忘机制避免学习。这种方法旨在减少非信息性信息的学习并明确知识边界。

Result: 实验表明，遗忘机制不仅提升了模型整体性能，还促进了更丰富的模型响应。

Conclusion: 通过标记分类和遗忘机制，可以更有效地优化监督微调过程，提升模型性能和多样性。

Abstract: Supervised fine-tuning (SFT) plays a critical role for pretrained large
language models (LLMs), notably enhancing their capacity to acquire
domain-specific knowledge while preserving or potentially augmenting their
general-purpose capabilities. However, the efficacy of SFT hinges on data
quality as well as data volume, otherwise it may result in limited performance
gains or even degradation relative to the associated baselines. To mitigate
such reliance, we suggest categorizing tokens within each corpus into two parts
-- positive and negative tokens -- based on whether they are useful to improve
model performance. Positive tokens can be trained in common ways, whereas
negative tokens, which may lack essential semantics or be misleading, should be
explicitly forgotten. Overall, the token categorization facilitate the model to
learn less informative message, and the forgetting process shapes a knowledge
boundary to guide the model on what information to learn more precisely. We
conduct experiments on well-established benchmarks, finding that this
forgetting mechanism not only improves overall model performance and also
facilitate more diverse model responses.

</details>


### [173] [From Split to Share: Private Inference with Distributed Feature Sharing](https://arxiv.org/abs/2508.04346)
*Zihan Liu,Jiayi Wen,Shouhong Tan,Zhirun Zheng,Cheng Huang*

Main category: cs.LG

TL;DR: PrivDFS是一种新的私有推理范式，通过分布式特征共享平衡隐私与效率，避免单点暴露，并通过扩展方法增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决基于云的机器学习服务（MLaaS）中敏感数据的隐私问题，现有方法在隐私和效率之间存在权衡。

Method: 将输入特征分区为多个平衡共享，分发到非共谋服务器进行独立推理，客户端安全聚合结果。扩展方法包括对抗训练和用户特定密钥。

Result: 在CIFAR-10和CelebA上，PrivDFS在保持隐私的同时显著降低计算开销，扩展方法对攻击具有鲁棒性。

Conclusion: PrivDFS在隐私和效率之间取得平衡，扩展方法进一步增强了隐私保护能力。

Abstract: Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy
concerns when handling sensitive client data. Existing Private Inference (PI)
methods face a fundamental trade-off between privacy and efficiency:
cryptographic approaches offer strong protection but incur high computational
overhead, while efficient alternatives such as split inference expose
intermediate features to inversion attacks. We propose PrivDFS, a new paradigm
for private inference that replaces a single exposed representation with
distributed feature sharing. PrivDFS partitions input features on the client
into multiple balanced shares, which are distributed to non-colluding,
non-communicating servers for independent partial inference. The client
securely aggregates the servers' outputs to reconstruct the final prediction,
ensuring that no single server observes sufficient information to compromise
input privacy. To further strengthen privacy, we propose two key extensions:
PrivDFS-AT, which uses adversarial training with a diffusion-based proxy
attacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,
which leverages user-specific keys to diversify partitioning policies and
prevent query-based inversion generalization. Experiments on CIFAR-10 and
CelebA demonstrate that PrivDFS achieves privacy comparable to deep split
inference while cutting client computation by up to 100 times with no accuracy
loss, and that the extensions remain robust against both diffusion-based
in-distribution and adaptive attacks.

</details>


### [174] [Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points](https://arxiv.org/abs/2508.04351)
*Justin Lee,Behnaz Moradijamei,Heman Shakeri*

Main category: cs.LG

TL;DR: 提出了一种名为MMSFM的新方法，用于处理高维系统在非等距时间点观测数据的建模问题，避免了传统降维方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决高维系统在非等距时间点观测数据建模中的挑战，避免传统降维方法对动态和瞬态行为的简化。

Method: 扩展了无模拟分数和流匹配方法到多边际设置，利用测度值样条增强对不规则时间点的鲁棒性，并通过分数匹配防止高维空间过拟合。

Result: 在合成和基准数据集（如基因表达数据和图像进展任务）上验证了方法的有效性。

Conclusion: MMSFM方法在高维数据建模中表现出色，适用于非等距时间点观测的场景。

Abstract: Modeling the evolution of high-dimensional systems from limited snapshot
observations at irregular time points poses a significant challenge in
quantitative biology and related fields. Traditional approaches often rely on
dimensionality reduction techniques, which can oversimplify the dynamics and
fail to capture critical transient behaviors in non-equilibrium systems. We
present Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of
simulation-free score and flow matching methods to the multi-marginal setting,
enabling the alignment of high-dimensional data measured at non-equidistant
time points without reducing dimensionality. The use of measure-valued splines
enhances robustness to irregular snapshot timing, and score matching prevents
overfitting in high-dimensional spaces. We validate our framework on several
synthetic and benchmark datasets, including gene expression data collected at
uneven time points and an image progression task, demonstrating the method's
versatility.

</details>


### [175] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: 提出了一种针对多实例学习（MIL）的持续学习方法，通过选择实例和样本存储，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实验室和临床环境数据动态变化，需要持续更新模型以避免灾难性遗忘，但现有方法对MIL无效。

Method: 基于实例注意力分数和距离选择样本，存储多样化的实例集。

Result: 在白血病实验室数据上，该方法显著优于现有持续学习方法。

Conclusion: 该方法为MIL提供了首个持续学习解决方案，适应数据分布变化。

Abstract: The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

</details>


### [176] [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)
*Hao Zhang,Aining Jia,Weifeng Bu,Yushu Cai,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: FlexQ是一种新颖的INT6量化框架，通过算法创新和系统优化，在保持高精度的同时显著提升推理效率和内存节省。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高内存和计算成本限制了实际部署，现有INT4/INT8量化方法在精度和效率上存在不足，而INT6量化缺乏硬件支持。

Method: FlexQ采用统一的6位权重量化，并通过层敏感分析保留8位激活。开发了高性能GPU内核，支持W6A6和W6A8表示，利用二进制张量核心（BTC）绕过INT6硬件限制。

Result: 在LLaMA模型上，FlexQ保持接近FP16的精度（困惑度增加不超过0.05），推理速度提升1.33倍，内存节省1.21倍。

Conclusion: FlexQ在INT6量化中实现了精度与效率的平衡，为LLMs的实际部署提供了高效解决方案。

Abstract: Large Language Models (LLMs) demonstrate exceptional performance but entail
significant memory and computational costs, restricting their practical
deployment. While existing INT4/INT8 quantization reduces these costs, they
often degrade accuracy or lack optimal efficiency. INT6 quantization offers a
superior trade-off between model accuracy and inference efficiency, but lacks
hardware support in modern GPUs, forcing emulation via higher-precision
arithmetic units that limit acceleration.
  In this paper, we propose FlexQ, a novel post-training INT6 quantization
framework combining algorithmic innovation with system-level optimizations.
FlexQ employs uniform 6-bit weight quantization across all layers, with
adaptive retention of 8-bit activations in layers identified through layer-wise
sensitivity analysis. To maximize hardware efficiency, we develop a specialized
high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8
representations via Binary Tensor Core (BTC) equivalents, effectively bypassing
the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ
maintains near-FP16 accuracy, with perplexity increases of no more than 0.05.
The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on
LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference
acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released
at https://github.com/FlyFoxPlayer/FlexQ.

</details>


### [177] [Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models](https://arxiv.org/abs/2508.04427)
*Md Raisul Kibria,Sébastien Lafond,Janan Arslan*

Main category: cs.LG

TL;DR: 本文系统综述了2020年至2024年初关于多模态模型可解释性的研究，发现注意力技术是主要解释方法，但存在模态交互不足和评估方法不系统的问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着多模态学习和可解释人工智能（XAI）的发展，需要系统分析多模态模型的可解释性研究，以推动更透明和负责任的人工智能系统。

Method: 通过系统文献综述，从模型架构、模态、解释算法和评估方法等维度分析多模态XAI研究。

Result: 研究发现注意力技术是主流解释方法，但模态交互和评估方法存在不足，缺乏一致性和鲁棒性。

Conclusion: 提出了促进多模态XAI研究标准化和透明化的建议，以支持未来更可解释和负责任的多模态AI系统。

Abstract: Multimodal learning has witnessed remarkable advancements in recent years,
particularly with the integration of attention-based models, leading to
significant performance gains across a variety of tasks. Parallel to this
progress, the demand for explainable artificial intelligence (XAI) has spurred
a growing body of research aimed at interpreting the complex decision-making
processes of these models. This systematic literature review analyzes research
published between January 2020 and early 2024 that focuses on the
explainability of multimodal models. Framed within the broader goals of XAI, we
examine the literature across multiple dimensions, including model
architecture, modalities involved, explanation algorithms and evaluation
methodologies. Our analysis reveals that the majority of studies are
concentrated on vision-language and language-only models, with attention-based
techniques being the most commonly employed for explanation. However, these
methods often fall short in capturing the full spectrum of interactions between
modalities, a challenge further compounded by the architectural heterogeneity
across domains. Importantly, we find that evaluation methods for XAI in
multimodal settings are largely non-systematic, lacking consistency,
robustness, and consideration for modality-specific cognitive and contextual
factors. Based on these findings, we provide a comprehensive set of
recommendations aimed at promoting rigorous, transparent, and standardized
evaluation and reporting practices in multimodal XAI research. Our goal is to
support future research in more interpretable, accountable, and responsible
mulitmodal AI systems, with explainability at their core.

</details>


### [178] [Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation](https://arxiv.org/abs/2508.04444)
*Askar Tsyganov,Evgeny Frolov,Sergey Samsonov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出了基于矩阵向量乘法的随机算法，用于估计矩阵的two-to-infinity和one-to-two范数，并应用于深度神经网络训练和推荐系统对抗攻击缓解。


<details>
  <summary>Details</summary>
Motivation: 在矩阵自由设置中高效估计矩阵范数，并探索其在深度学习和推荐系统中的实际应用。

Method: 基于Hutchinson对角估计器及其Hutch++变体的改进算法，提供oracle复杂度分析。

Result: 算法在图像分类任务中有效用于Jacobian正则化，并能在推荐系统中减轻对抗攻击影响。

Conclusion: 提出的算法在理论和实际应用中均表现出色，具有广泛适用性。

Abstract: In this paper, we propose new randomized algorithms for estimating the
two-to-infinity and one-to-two norms in a matrix-free setting, using only
matrix-vector multiplications. Our methods are based on appropriate
modifications of Hutchinson's diagonal estimator and its Hutch++ version. We
provide oracle complexity bounds for both modifications. We further illustrate
the practical utility of our algorithms for Jacobian-based regularization in
deep neural network training on image classification tasks. We also demonstrate
that our methodology can be applied to mitigate the effect of adversarial
attacks in the domain of recommender systems.

</details>


### [179] [Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling](https://arxiv.org/abs/2508.04447)
*Biao Hu,Guoyin Wang*

Main category: cs.LG

TL;DR: CMCFAE是一种新型生成模型，结合云模型与Wasserstein自编码器，通过特征函数正则化潜在空间，提升复杂数据分布的建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖高斯先验和传统散度度量，导致重构样本同质化，CMCFAE旨在通过云模型先验提供更灵活和真实的潜在空间表示。

Method: 提出云模型的特征函数，并将其作为正则项集成到WAE框架中。

Result: 在MNIST、FashionMNIST、CIFAR-10和CelebA上，CMCFAE在重构质量、潜在空间结构和样本多样性上优于现有模型。

Conclusion: CMCFAE为自编码器生成模型提供了新视角，结合云模型理论与MMD正则化，表现出优越性能。

Abstract: We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a
novel generative model that integrates the cloud model into the Wasserstein
Auto-Encoder (WAE) framework. By leveraging the characteristic functions of the
cloud model to regularize the latent space, our approach enables more accurate
modeling of complex data distributions. Unlike conventional methods that rely
on a standard Gaussian prior and traditional divergence measures, our method
employs a cloud model prior, providing a more flexible and realistic
representation of the latent space, thus mitigating the homogenization observed
in reconstructed samples. We derive the characteristic function of the cloud
model and propose a corresponding regularizer within the WAE framework.
Extensive quantitative and qualitative evaluations on MNIST, FashionMNIST,
CIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in
terms of reconstruction quality, latent space structuring, and sample
diversity. This work not only establishes a novel integration of cloud model
theory with MMD-based regularization but also offers a promising new
perspective for enhancing autoencoder-based generative models.

</details>


### [180] [Automatic LLM Red Teaming](https://arxiv.org/abs/2508.04451)
*Roman Belaire,Arunesh Sinha,Pradeep Varakantham*

Main category: cs.LG

TL;DR: 提出了一种新的红队方法，通过强化学习训练AI攻击LLM，解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队方法依赖脆弱模板或单轮攻击，无法模拟复杂对抗对话。

Method: 将红队建模为马尔可夫决策过程，采用分层强化学习框架，结合细粒度奖励机制。

Result: 新方法能发现基线遗漏的漏洞，成为当前最佳技术。

Conclusion: 将LLM红队重新定义为动态轨迹过程，对AI部署至关重要。

Abstract: Red teaming is critical for identifying vulnerabilities and building trust in
current LLMs. However, current automated methods for Large Language Models
(LLMs) rely on brittle prompt templates or single-turn attacks, failing to
capture the complex, interactive nature of real-world adversarial dialogues. We
propose a novel paradigm: training an AI to strategically `break' another AI.
By formalizing red teaming as a Markov Decision Process (MDP) and employing a
hierarchical Reinforcement Learning (RL) framework, we effectively address the
inherent sparse reward and long-horizon challenges. Our generative agent learns
coherent, multi-turn attack strategies through a fine-grained, token-level harm
reward, enabling it to uncover subtle vulnerabilities missed by existing
baselines. This approach sets a new state-of-the-art, fundamentally reframing
LLM red teaming as a dynamic, trajectory-based process (rather than a one-step
test) essential for robust AI deployment.

</details>


### [181] [Small transformer architectures for task switching](https://arxiv.org/abs/2508.04461)
*Claudius Gros*

Main category: cs.LG

TL;DR: 论文探讨了在小规模任务切换场景中，基于注意力的架构与传统方法（如多层感知机或循环网络）的性能对比，发现标准Transformer无法解决基本任务切换问题，而改进的注意力机制（如cisformer和extensive attention）能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索注意力机制在小规模应用中的表现，尤其是在任务切换框架下，传统方法是否仍具竞争力。

Method: 研究方法包括在任务切换框架下测试标准Transformer、LSTM、MLP以及改进的注意力架构（cisformer和extensive attention）的性能。

Result: 结果显示标准Transformer、LSTM和MLP表现相似且有限，而结合改进注意力机制的模型（cisformer和extensive attention）能达到约95%的高性能。

Conclusion: 结论表明，通过比较不同注意力机制在任务切换场景中的表现，可以更好地理解和改进注意力机制的工作原理。

Abstract: The rapid progress seen in terms of large-scale generative AI is largely
based on the attention mechanism. It is conversely non-trivial to conceive
small-scale applications for which attention-based architectures outperform
traditional approaches, such as multi-layer perceptrons or recurrent networks.
We examine this problem in the context of 'task switching'. In this framework
models work on ongoing token sequences with the current task being determined
by stochastically interspersed control tokens. We show that standard
transformers cannot solve a basic task switching reference model based on
finite domain arithmetics which contains subtasks dedicated to increment /
addition / reverse copy / context (IARC). We show that transformers, long
short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons
(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our
comparative study by including an extension of the standard transformer
architecture to its non-translational invariant counterpart, the cisformer, and
an alternative attention mechanism, extensive attention. A combination of the
latter is found to be the only model able to achieve considerable performance
levels, of around 95%. Our results indicate that the workings of attention can
be understood better, and even improved, when comparing qualitatively different
formulations in task-switching settings.

</details>


### [182] [CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2508.04462)
*Enyu Zhou,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: 提出了一种基于缓存的并行推测解码框架CARD，通过解耦起草和验证过程，显著加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法必须遵循‘起草-验证’范式，导致推理效率低下且限制起草模型规模。

Method: 采用‘查询-修正’范式，起草模型生成候选令牌填充共享缓存，目标模型并行修正生成方向。

Result: 实现了高达4.83倍的加速，无需微调起草或目标模型。

Conclusion: CARD框架显著提升了推理效率，为LLM加速提供了新思路。

Abstract: Speculative decoding (SD), where an extra draft model first provides multiple
draft tokens and the original target model then verifies these tokens in
parallel, has shown great power for LLM inference acceleration. However,
existing SD methods must adhere to the 'draft-then-verify' paradigm, which
forces drafting and verification processes to execute sequentially during SD,
resulting in inefficient inference performance and limiting the size of the
draft model. Furthermore, once a single token in the candidate sequence is
rejected during the drafting process, all subsequent candidate tokens must be
discarded, leading to inefficient drafting. To address these challenges, we
propose a cache-based parallel speculative decoding framework employing a
'query-and-correct' paradigm. Specifically, CARD decouples drafting and
verification: the draft model generates candidate tokens to populate a shared
cache, while the target model concurrently rectifies the draft model's
generation direction. This effectively enables the target model to perform
inference at speed approaching that of the draft model. Our approach achieves
up to 4.83 speedup over vanilla decoding without requiring fine-tuning of
either the draft or target models. Our code is available at
https://github.com/hunzhizi/CARD.

</details>


### [183] [GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries](https://arxiv.org/abs/2508.04463)
*Fangzhi Fei,Jiaxin Hu,Qiaofeng Li,Zhenyu Liu*

Main category: cs.LG

TL;DR: GFocal是一种基于Transformer的神经算子方法，通过同时学习全局和局部特征，解决了多尺度问题，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了局部物理细节与全局特征之间的协调学习，这对解决多尺度问题、保持物理一致性和数值稳定性至关重要。

Method: GFocal通过Nyström注意力机制和切片机制分别捕获全局和局部特征，并通过卷积门控块动态融合多尺度信息。

Result: GFocal在六项基准测试中的五项中平均相对提升了15.2%，并在工业规模模拟中表现出色。

Conclusion: GFocal通过全局与局部特征的协同学习，显著提升了多尺度物理问题的建模和预测能力。

Abstract: Transformer-based neural operators have emerged as promising surrogate
solvers for partial differential equations, by leveraging the effectiveness of
Transformers for capturing long-range dependencies and global correlations,
profoundly proven in language modeling. However, existing methodologies
overlook the coordinated learning of interdependencies between local physical
details and global features, which are essential for tackling multiscale
problems, preserving physical consistency and numerical stability in long-term
rollouts, and accurately capturing transitional dynamics. In this work, we
propose GFocal, a Transformer-based neural operator method that enforces
simultaneous global and local feature learning and fusion. Global correlations
and local features are harnessed through Nystr\"{o}m attention-based
\textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate
physics-aware tokens, subsequently modulated and integrated via
convolution-based gating blocks, enabling dynamic fusion of multiscale
information. GFocal achieves accurate modeling and prediction of physical
features given arbitrary geometries and initial conditions. Experiments show
that GFocal achieves state-of-the-art performance with an average 15.2\%
relative gain in five out of six benchmarks and also excels in industry-scale
simulations such as aerodynamics simulation of automotives and airfoils.

</details>


### [184] [FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions](https://arxiv.org/abs/2508.04470)
*Jianheng Tang,Zhirui Yang,Jingchao Wang,Kejia Fan,Jinfeng Xu,Huiping Zhuang,Anfeng Liu,Houbing Herbert Song,Leye Wang,Yunhuai Liu*

Main category: cs.LG

TL;DR: FedHiP提出了一种基于解析解的个性化联邦学习方案，通过避免梯度更新来解决非独立同分布数据的挑战，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法因依赖梯度更新而对非独立同分布数据敏感，导致性能下降。FedHiP旨在从根本上解决这一问题。

Method: 利用自监督预训练基础模型提取特征，并开发解析分类器进行无梯度训练，包含局部训练、全局聚合和个性化三个阶段。

Result: 在基准数据集上，FedHiP的准确率比现有方法高出5.79%-20.97%。

Conclusion: FedHiP通过解析解实现异构不变性，显著提升了非独立同分布数据下的个性化联邦学习性能。

Abstract: Lately, Personalized Federated Learning (PFL) has emerged as a prevalent
paradigm to deliver personalized models by collaboratively training while
simultaneously adapting to each client's local applications. Existing PFL
methods typically face a significant challenge due to the ubiquitous data
heterogeneity (i.e., non-IID data) across clients, which severely hinders
convergence and degrades performance. We identify that the root issue lies in
the long-standing reliance on gradient-based updates, which are inherently
sensitive to non-IID data. To fundamentally address this issue and bridge the
research gap, in this paper, we propose a Heterogeneity-invariant Personalized
Federated learning scheme, named FedHiP, through analytical (i.e., closed-form)
solutions to avoid gradient-based updates. Specifically, we exploit the trend
of self-supervised pre-training, leveraging a foundation model as a frozen
backbone for gradient-free feature extraction. Following the feature extractor,
we further develop an analytic classifier for gradient-free training. To
support both collective generalization and individual personalization, our
FedHiP scheme incorporates three phases: analytic local training, analytic
global aggregation, and analytic local personalization. The closed-form
solutions of our FedHiP scheme enable its ideal property of heterogeneity
invariance, meaning that each personalized model remains identical regardless
of how non-IID the data are distributed across all other clients. Extensive
experiments on benchmark datasets validate the superiority of our FedHiP
scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%
in accuracy.

</details>


### [185] [Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions](https://arxiv.org/abs/2508.04478)
*Bernardino D'Amico,Francesco Pomponi,Jay H. Arehart,Lina Khaddour*

Main category: cs.LG

TL;DR: 研究发现，墙体保温措施平均减少19%的燃气需求，但效果因家庭能源负担而异：低负担家庭节省显著，高负担家庭几乎无变化。


<details>
  <summary>Details</summary>
Motivation: 探讨能源效率干预措施对燃气需求的影响，特别是不同能源负担家庭的分布效应。

Method: 使用因果机器学习模型分析英国代表性住房数据，评估墙体保温的平均和条件处理效应。

Result: 低能源负担家庭节省显著，高负担家庭因将节省用于改善热舒适而几乎无需求减少。

Conclusion: 需综合考虑气候影响和能源政策的公平性，评估框架应更全面。

Abstract: Reducing domestic energy demand is central to climate mitigation and fuel
poverty strategies, yet the impact of energy efficiency interventions is highly
heterogeneous. Using a causal machine learning model trained on nationally
representative data of the English housing stock, we estimate average and
conditional treatment effects of wall insulation on gas consumption, focusing
on distributional effects across energy burden subgroups. While interventions
reduce gas demand on average (by as much as 19 percent), low energy burden
groups achieve substantial savings, whereas those experiencing high energy
burdens see little to no reduction. This pattern reflects a
behaviourally-driven mechanism: households constrained by high costs-to-income
ratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort
rather than lowering consumption. Far from wasteful, such responses represent
rational adjustments in contexts of prior deprivation, with potential
co-benefits for health and well-being. These findings call for a broader
evaluation framework that accounts for both climate impacts and the equity
implications of domestic energy policy.

</details>


### [186] [Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach](https://arxiv.org/abs/2508.04481)
*Anushka Srivastava*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的情绪检测方法，使用条件生成对抗网络（cGANs），通过多模态数据（文本、音频和面部表情）提升分类精度。


<details>
  <summary>Details</summary>
Motivation: 传统单模态方法依赖单一数据类型，限制了情绪检测的准确性。本文旨在通过多模态框架和cGANs生成合成数据，提升情绪识别的性能。

Method: 采用条件生成对抗网络（cGANs）架构，整合文本、音频和面部表情数据，生成合成情绪数据并优化分类模型。

Result: 实验结果表明，与基线模型相比，该方法在情绪识别性能上有显著提升。

Conclusion: cGANs在多模态情绪检测中具有潜力，可增强人机交互系统的情感理解能力。

Abstract: This paper presents a deep learning-based approach to emotion detection using
Conditional Generative Adversarial Networks (cGANs). Unlike traditional
unimodal techniques that rely on a single data type, we explore a multimodal
framework integrating text, audio, and facial expressions. The proposed cGAN
architecture is trained to generate synthetic emotion-rich data and improve
classification accuracy across multiple modalities. Our experimental results
demonstrate significant improvements in emotion recognition performance
compared to baseline models. This work highlights the potential of cGANs in
enhancing human-computer interaction systems by enabling more nuanced emotional
understanding.

</details>


### [187] [Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation](https://arxiv.org/abs/2508.04489)
*Erin Lanus,Daniel Wolodkin,Laura J. Freeman*

Main category: cs.LG

TL;DR: 论文提出了一种基于层次结构的评分指标，用于更细粒度地评估机器学习模型的分类和物体检测性能，通过评分树编码类别关系，避免传统二值评分的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统分类和物体检测评估将所有错误分类视为等同，忽略了类别间的层次关系。本文旨在通过层次评分指标，更细致地反映错误分类的影响。

Method: 开发了基于评分树的层次评分指标，通过编码类别关系，生成反映预测与真实标签距离的指标。评分树支持三种权重策略，用于调整错误分类的影响。

Result: 实验表明，层次评分指标能更细粒度地捕捉错误，评分树允许调整错误分类的权重，从而更全面地评估模型性能。

Conclusion: 层次评分指标提供了一种更细致的模型评估方法，不仅关注错误数量，还关注错误类型和影响。Python实现将开源。

Abstract: A common use of machine learning (ML) models is predicting the class of a
sample. Object detection is an extension of classification that includes
localization of the object via a bounding box within the sample.
Classification, and by extension object detection, is typically evaluated by
counting a prediction as incorrect if the predicted label does not match the
ground truth label. This pass/fail scoring treats all misclassifications as
equivalent. In many cases, class labels can be organized into a class taxonomy
with a hierarchical structure to either reflect relationships among the data or
operator valuation of misclassifications. When such a hierarchical structure
exists, hierarchical scoring metrics can return the model performance of a
given prediction related to the distance between the prediction and the ground
truth label. Such metrics can be viewed as giving partial credit to predictions
instead of pass/fail, enabling a finer-grained understanding of the impact of
misclassifications. This work develops hierarchical scoring metrics varying in
complexity that utilize scoring trees to encode relationships between class
labels and produce metrics that reflect distance in the scoring tree. The
scoring metrics are demonstrated on an abstract use case with scoring trees
that represent three weighting strategies and evaluated by the kind of errors
discouraged. Results demonstrate that these metrics capture errors with finer
granularity and the scoring trees enable tuning. This work demonstrates an
approach to evaluating ML performance that ranks models not only by how many
errors are made but by the kind or impact of errors. Python implementations of
the scoring metrics will be available in an open-source repository at time of
publication.

</details>


### [188] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: 论文提出Causal Reflection框架，通过显式建模因果关系，解决LLMs和传统RL在因果推理上的不足，并引入Reflect机制以修正模型。


<details>
  <summary>Details</summary>
Motivation: LLMs和传统强化学习代理缺乏稳健的因果推理能力，依赖虚假相关性和脆弱模式。

Method: 提出Causal Reflection框架，动态建模因果关系，并定义Reflect机制以修正预测与观察的差异。

Result: 框架为具备因果理解、自适应和自我修正能力的代理奠定理论基础。

Conclusion: Causal Reflection为代理在动态环境中提供因果推理和解释能力。

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


### [189] [PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers](https://arxiv.org/abs/2508.04503)
*Federico Zucchi,Thomas Lampert*

Main category: cs.LG

TL;DR: PRISM是一种基于卷积的特征提取器，通过多分辨率、每通道对称FIR滤波器设计，显著减少模型参数和计算量，同时在多项任务中性能优于或匹配现有CNN和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有Transformer和CNN模型在多元时间序列分类中计算量大、频率多样性有限和参数需求高的问题。

Method: 提出PRISM模块，采用多分辨率、每通道对称FIR滤波器设计，避免通道间卷积，降低模型复杂度和参数数量。

Result: 在人类活动、睡眠阶段和生物医学基准测试中，PRISM性能优于或匹配现有模型，同时参数和计算量大幅减少。

Conclusion: PRISM结合经典信号处理和现代深度学习，提供了一种高效、准确的多元时间序列分类解决方案。

Abstract: Multivariate time-series classification is pivotal in domains ranging from
wearable sensing to biomedical monitoring. Despite recent advances,
Transformer- and CNN-based models often remain computationally heavy, offer
limited frequency diversity, and require extensive parameter budgets. We
propose PRISM (Per-channel Resolution-Informed Symmetric Module), a
convolutional-based feature extractor that applies symmetric
finite-impulse-response (FIR) filters at multiple temporal scales,
independently per channel. This multi-resolution, per-channel design yields
highly frequency-selective embeddings without any inter-channel convolutions,
greatly reducing model size and complexity. Across human-activity, sleep-stage
and biomedical benchmarks, PRISM, paired with lightweight classification heads,
matches or outperforms leading CNN and Transformer baselines, while using
roughly an order of magnitude fewer parameters and FLOPs. By uniting classical
signal processing insights with modern deep learning, PRISM offers an accurate,
resource-efficient solution for multivariate time-series classification.

</details>


### [190] [Channel-Independent Federated Traffic Prediction](https://arxiv.org/abs/2508.04517)
*Mo Zhang,Xiaoyu Li,Bin Xu,Meng Chen,Yongshun Gong*

Main category: cs.LG

TL;DR: 提出了一种名为CIP的新型变量关系建模范式，用于联邦交通预测，通过Fed-CI框架减少通信开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有联邦交通预测方法因通信开销大导致的训练延迟问题，同时满足隐私约束。

Method: 提出CIP范式，避免客户端间通信，基于此开发Fed-CI框架，实现本地高效预测。

Result: Fed-CI在多个数据集上表现优异，RMSE、MAE和MAPE分别提升8%、14%和16%，通信成本显著降低。

Conclusion: Fed-CI在减少通信开销的同时，实现了高性能的联邦交通预测，具有实际应用潜力。

Abstract: In recent years, traffic prediction has achieved remarkable success and has
become an integral component of intelligent transportation systems. However,
traffic data is typically distributed among multiple data owners, and privacy
constraints prevent the direct utilization of these isolated datasets for
traffic prediction. Most existing federated traffic prediction methods focus on
designing communication mechanisms that allow models to leverage information
from other clients in order to improve prediction accuracy. Unfortunately, such
approaches often incur substantial communication overhead, and the resulting
transmission delays significantly slow down the training process. As the volume
of traffic data continues to grow, this issue becomes increasingly critical,
making the resource consumption of current methods unsustainable. To address
this challenge, we propose a novel variable relationship modeling paradigm for
federated traffic prediction, termed the Channel-Independent Paradigm(CIP).
Unlike traditional approaches, CIP eliminates the need for inter-client
communication by enabling each node to perform efficient and accurate
predictions using only local information. Based on the CIP, we further develop
Fed-CI, an efficient federated learning framework, allowing each client to
process its own data independently while effectively mitigating the information
loss caused by the lack of direct data sharing among clients. Fed-CI
significantly reduces communication overhead, accelerates the training process,
and achieves state-of-the-art performance while complying with privacy
regulations. Extensive experiments on multiple real-world datasets demonstrate
that Fed-CI consistently outperforms existing methods across all datasets and
federated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,
and MAPE, respectively, while also substantially reducing communication costs.

</details>


### [191] [Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape](https://arxiv.org/abs/2508.04542)
*Haoran Niu,K. Suzanne Barber*

Main category: cs.LG

TL;DR: 通过分析5000多个身份盗窃和欺诈案例，构建了一个基于图的身份生态系统模型，用于预测个人隐私风险。


<details>
  <summary>Details</summary>
Motivation: 个人和组织难以在没有基本隐私风险认知的情况下保护个人信息。

Method: 构建身份生态系统图，节点代表个人身份信息属性，边代表属性间的暴露关系，并利用图论和图神经网络开发隐私风险预测框架。

Result: 模型能有效预测某一身份属性暴露是否可能导致另一属性暴露。

Conclusion: 该研究为隐私风险评估提供了有效工具。

Abstract: It is difficult for individuals and organizations to protect personal
information without a fundamental understanding of relative privacy risks. By
analyzing over 5,000 empirical identity theft and fraud cases, this research
identifies which types of personal data are exposed, how frequently exposures
occur, and what the consequences of those exposures are. We construct an
Identity Ecosystem graph--a foundational, graph-based model in which nodes
represent personally identifiable information (PII) attributes and edges
represent empirical disclosure relationships between them (e.g., the
probability that one PII attribute is exposed due to the exposure of another).
Leveraging this graph structure, we develop a privacy risk prediction framework
that uses graph theory and graph neural networks to estimate the likelihood of
further disclosures when certain PII attributes are compromised. The results
show that our approach effectively answers the core question: Can the
disclosure of a given identity attribute possibly lead to the disclosure of
another attribute?

</details>


### [192] [GraphProp: Training the Graph Foundation Models using Graph Properties](https://arxiv.org/abs/2508.04594)
*Ziheng Sun,Qi Feng,Lehao Lin,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: GraphProp是一种图基础模型（GFM），通过强调结构泛化能力，显著提升了图分类任务的跨域性能。


<details>
  <summary>Details</summary>
Motivation: 传统GFM主要关注节点特征的跨域泛化，但缺乏结构信息的跨域一致性。GraphProp旨在解决这一问题。

Method: GraphProp分为两阶段：1）训练结构GFM，预测图不变量；2）利用结构GFM的表示作为位置编码，训练综合GFM。

Result: 实验表明，GraphProp在监督学习和少样本学习中表现优异，尤其在无节点属性的图上。

Conclusion: GraphProp通过结构泛化和节点特征结合，显著提升了GFM的跨域性能。

Abstract: This work focuses on training graph foundation models (GFMs) that have strong
generalization ability in graph-level tasks such as graph classification.
Effective GFM training requires capturing information consistent across
different domains. We discover that graph structures provide more consistent
cross-domain information compared to node features and graph labels. However,
traditional GFMs primarily focus on transferring node features from various
domains into a unified representation space but often lack structural
cross-domain generalization. To address this, we introduce GraphProp, which
emphasizes structural generalization. The training process of GraphProp
consists of two main phases. First, we train a structural GFM by predicting
graph invariants. Since graph invariants are properties of graphs that depend
only on the abstract structure, not on particular labellings or drawings of the
graph, this structural GFM has a strong ability to capture the abstract
structural information and provide discriminative graph representations
comparable across diverse domains. In the second phase, we use the
representations given by the structural GFM as positional encodings to train a
comprehensive GFM. This phase utilizes domain-specific node attributes and
graph labels to further improve cross-domain node feature generalization. Our
experiments demonstrate that GraphProp significantly outperforms the
competitors in supervised learning and few-shot learning, especially in
handling graphs without node attributes.

</details>


### [193] [Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding](https://arxiv.org/abs/2508.04595)
*Jan A. Zak,Christian Weißenfels*

Main category: cs.LG

TL;DR: 论文研究了基于物理信息的神经网络在铝点焊中用于非破坏性质量评估的方法，提出了两种新的训练策略以解决数据整合问题。


<details>
  <summary>Details</summary>
Motivation: 电阻点焊是汽车工业中主要的连接工艺，但焊核直径的测量需要破坏性测试，限制了高效质量控制的可能性。

Method: 采用物理信息神经网络，引入渐进式损失函数和条件更新策略，结合二维轴对称模型进行训练。

Result: 二维网络能够预测动态位移和焊核生长，并在实验置信区间内验证了其有效性。

Conclusion: 该方法展示了在工业应用中快速、基于模型的质量控制的潜力。

Abstract: Resistance spot welding is the dominant joining process for the body-in-white
in the automotive industry, where the weld nugget diameter is the key quality
metric. Its measurement requires destructive testing, limiting the potential
for efficient quality control. Physics-informed neural networks were
investigated as a promising tool to reconstruct internal process states from
experimental data, enabling model-based and non-invasive quality assessment in
aluminum spot welding. A major challenge is the integration of real-world data
into the network due to competing optimization objectives. To address this, we
introduce two novel training strategies. First, experimental losses for dynamic
displacement and nugget diameter are progressively included using a fading-in
function to prevent excessive optimization conflicts. We also implement a
custom learning rate scheduler and early stopping based on a rolling window to
counteract premature reduction due to increased loss magnitudes. Second, we
introduce a conditional update of temperature-dependent material parameters via
a look-up table, activated only after a loss threshold is reached to ensure
physically meaningful temperatures. An axially symmetric two-dimensional model
was selected to represent the welding process accurately while maintaining
computational efficiency. To reduce computational burden, the training
strategies and model components were first systematically evaluated in one
dimension, enabling controlled analysis of loss design and contact models. The
two-dimensional network predicts dynamic displacement and nugget growth within
the experimental confidence interval, supports transferring welding stages from
steel to aluminum, and demonstrates strong potential for fast, model-based
quality control in industrial applications.

</details>


### [194] [Multitask Learning with Stochastic Interpolants](https://arxiv.org/abs/2508.04605)
*Hugo Negrel,Florentin Coeurdoux,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出了一种广义的概率分布映射学习框架，扩展了流和扩散模型的时间动态。


<details>
  <summary>Details</summary>
Motivation: 为多任务生成模型提供统一的理论视角，并扩展其能力。

Method: 通过将标量时间变量替换为向量、矩阵或线性算子，构建算子插值法。

Result: 实验验证了该方法在条件生成、修复、微调和多尺度建模中的零样本效果。

Conclusion: 该方法可作为通用任务无关模型替代专用模型。

Abstract: We propose a framework for learning maps between probability distributions
that broadly generalizes the time dynamics of flow and diffusion models. To
enable this, we generalize stochastic interpolants by replacing the scalar time
variable with vectors, matrices, or linear operators, allowing us to bridge
probability distributions across multiple dimensional spaces. This approach
enables the construction of versatile generative models capable of fulfilling
multiple tasks without task-specific training. Our operator-based interpolants
not only provide a unifying theoretical perspective for existing generative
models but also extend their capabilities. Through numerical experiments, we
demonstrate the zero-shot efficacy of our method on conditional generation and
inpainting, fine-tuning and posterior sampling, and multiscale modeling,
suggesting its potential as a generic task-agnostic alternative to specialized
models.

</details>


### [195] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 本文提出了一种基于脉冲神经网络（SNN）的分层架构，用于终身网络入侵检测系统（NIDS），结合静态和动态SNN，通过生物启发的学习机制实现高效学习和低功耗部署。


<details>
  <summary>Details</summary>
Motivation: 受大脑分层处理和能量效率的启发，旨在设计一个能够持续学习新威胁并保持现有知识的网络入侵检测系统。

Method: 采用静态SNN初步检测潜在入侵，动态SNN分类具体攻击类型，结合GWR结构可塑性和Ad-STDP学习规则。

Result: 在UNSW-NB15基准测试中，系统表现出85.3%的准确率，适应性强且遗忘率低，模拟显示适合低功耗神经形态硬件。

Conclusion: 该架构在终身学习和低功耗部署方面具有潜力，为网络入侵检测提供了高效解决方案。

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [196] [CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series](https://arxiv.org/abs/2508.04630)
*Yutong Xia,Yingying Zhang,Yuxuan Liang,Lunting Fan,Qingsong Wen,Roger Zimmermann*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果关系的框架CaPulse，用于时间序列异常检测，解决了现有方法在捕捉异常生成机制和数据相关挑战（如标签稀缺、数据不平衡和多周期性）上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分捕捉时间序列异常生成的机制，且面临标签稀缺、数据不平衡和多周期性等数据挑战。

Method: 通过构建结构因果模型解析异常生成过程，并提出带掩码机制的周期性归一化流和周期性学习器，实现周期性感知的基于密度的异常检测。

Result: 在七个真实数据集上的实验表明，CaPulse显著优于现有方法，AUROC提升3%至17%，且具有更好的可解释性。

Conclusion: CaPulse通过因果工具和周期性感知方法，有效解决了时间序列异常检测中的关键挑战，提升了性能和可解释性。

Abstract: Time series anomaly detection has garnered considerable attention across
diverse domains. While existing methods often fail to capture the underlying
mechanisms behind anomaly generation in time series data. In addition, time
series anomaly detection often faces several data-related inherent challenges,
i.e., label scarcity, data imbalance, and complex multi-periodicity. In this
paper, we leverage causal tools and introduce a new causality-based framework,
CaPulse, which tunes in to the underlying causal pulse of time series data to
effectively detect anomalies. Concretely, we begin by building a structural
causal model to decipher the generation processes behind anomalies. To tackle
the challenges posed by the data, we propose Periodical Normalizing Flows with
a novel mask mechanism and carefully designed periodical learners, creating a
periodicity-aware, density-based anomaly detection approach. Extensive
experiments on seven real-world datasets demonstrate that CaPulse consistently
outperforms existing methods, achieving AUROC improvements of 3% to 17%, with
enhanced interpretability.

</details>


### [197] [A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation](https://arxiv.org/abs/2508.04645)
*Yu Song,Zhigang Hua,Harry Shomer,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: 该论文研究了图神经网络（GNN）在链接预测（LP）任务中的挑战，提出了一种基于预训练和混合专家（MoE）框架的解决方案，通过参数高效调优策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在LP任务中面临监督稀疏、初始化敏感和分布偏移下泛化能力差的问题，预训练被视为潜在解决方案。

Method: 提出了一种结合节点和边信息的预训练方法，采用MoE框架避免负迁移，并开发了参数高效调优策略。

Result: 在16个数据集上验证了方法的有效性，实现了低资源LP任务的最优性能，计算开销降低10,000倍以上。

Conclusion: 该方法在LP任务中表现优异，同时显著降低了计算成本，为实际应用提供了高效解决方案。

Abstract: Link Prediction (LP) is a critical task in graph machine learning. While
Graph Neural Networks (GNNs) have significantly advanced LP performance
recently, existing methods face key challenges including limited supervision
from sparse connectivity, sensitivity to initialization, and poor
generalization under distribution shifts. We explore pretraining as a solution
to address these challenges. Unlike node classification, LP is inherently a
pairwise task, which requires the integration of both node- and edge-level
information. In this work, we present the first systematic study on the
transferability of these distinct modules and propose a late fusion strategy to
effectively combine their outputs for improved performance. To handle the
diversity of pretraining data and avoid negative transfer, we introduce a
Mixture-of-Experts (MoE) framework that captures distinct patterns in separate
experts, facilitating seamless application of the pretrained model on diverse
downstream datasets. For fast adaptation, we develop a parameter-efficient
tuning strategy that allows the pretrained model to adapt to unseen datasets
with minimal computational overhead. Experiments on 16 datasets across two
domains demonstrate the effectiveness of our approach, achieving
state-of-the-art performance on low-resource link prediction while obtaining
competitive results compared to end-to-end trained methods, with over 10,000x
lower computational overhead.

</details>


### [198] [Perch 2.0: The Bittern Lesson for Bioacoustics](https://arxiv.org/abs/2508.04665)
*Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Lauren Harrell,Andrea Burns,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0是一个高性能的生物声学预训练模型，通过多类群数据训练和自蒸馏技术，实现了在鸟类和海洋生物分类任务中的领先性能。


<details>
  <summary>Details</summary>
Motivation: 扩展Perch模型的应用范围，从仅针对鸟类扩展到多类群数据，并探索细粒度物种分类作为生物声学预训练任务的鲁棒性。

Method: 使用自蒸馏技术和原型学习分类器，结合新的源预测训练准则进行训练。

Result: 在BirdSet和BEANS基准测试中达到最先进性能，并在海洋迁移学习任务中优于专用模型。

Conclusion: 细粒度物种分类是生物声学预训练的强大任务，Perch 2.0展示了其广泛适用性和高性能。

Abstract: Perch is a performant pre-trained model for bioacoustics. It was trained in
supervised fashion, providing both off-the-shelf classification scores for
thousands of vocalizing species as well as strong embeddings for transfer
learning. In this new release, Perch 2.0, we expand from training exclusively
on avian species to a large multi-taxa dataset. The model is trained with
self-distillation using a prototype-learning classifier as well as a new
source-prediction training criterion. Perch 2.0 obtains state-of-the-art
performance on the BirdSet and BEANS benchmarks. It also outperforms
specialized marine models on marine transfer learning tasks, despite having
almost no marine training data. We present hypotheses as to why fine-grained
species classification is a particularly robust pre-training task for
bioacoustics.

</details>


### [199] [Robustly Learning Monotone Single-Index Models](https://arxiv.org/abs/2508.04670)
*Puqian Wang,Nikos Zarifis,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 本文提出了一种高效算法，用于在高斯分布和对抗性标签噪声下学习单指数模型，适用于所有单调激活函数。


<details>
  <summary>Details</summary>
Motivation: 解决在高斯分布和对抗性标签噪声下学习单指数模型的挑战，特别是针对未知激活函数的情况。

Method: 开发了一种优化框架，通过直接利用问题结构、高斯空间性质和单调函数的规律性，识别有用的向量场来指导算法更新。

Result: 算法首次实现了常数因子近似，适用于所有具有有界矩的单调激活函数，包括单调Lipschitz函数和不连续函数（如半空间）。

Conclusion: 该研究为学习单指数模型提供了高效且广泛适用的解决方案，突破了传统梯度方法的限制。

Abstract: We consider the basic problem of learning Single-Index Models with respect to
the square loss under the Gaussian distribution in the presence of adversarial
label noise. Our main contribution is the first computationally efficient
algorithm for this learning task, achieving a constant factor approximation,
that succeeds for the class of {\em all} monotone activations with bounded
moment of order $2 + \zeta,$ for $\zeta > 0.$ This class in particular includes
all monotone Lipschitz functions and even discontinuous functions like
(possibly biased) halfspaces. Prior work for the case of unknown activation
either does not attain constant factor approximation or succeeds for a
substantially smaller family of activations. The main conceptual novelty of our
approach lies in developing an optimization framework that steps outside the
boundaries of usual gradient methods and instead identifies a useful vector
field to guide the algorithm updates by directly leveraging the problem
structure, properties of Gaussian spaces, and regularity of monotone functions.

</details>
