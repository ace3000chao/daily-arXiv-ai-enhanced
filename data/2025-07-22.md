<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 86]
- [cs.HC](#cs.HC) [Total: 36]
- [cs.LG](#cs.LG) [Total: 128]
- [cs.AI](#cs.AI) [Total: 58]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter是一种基于离线知识库的多模态写作助手，通过任务分解、大纲生成和多模态检索等技术，生成专业级文档，解决了现有LLM在专业领域中的知识不足和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在专业领域（如金融、医学和法律）中作为写作助手时，常因缺乏深度领域知识和幻觉问题而受限。现有方法（如RAG）存在检索不一致或网络内容不可靠的问题。

Method: DeepWriter采用任务分解、大纲生成、多模态检索和分段组合的流程，结合结构化知识库和层次化知识表示，生成专业文档。

Result: 实验表明，DeepWriter在金融报告生成任务中，生成的文档在事实准确性和内容质量上优于现有基线方法。

Conclusion: DeepWriter通过离线知识库和多模态技术，显著提升了专业文档生成的准确性和质量，为LLM在专业领域的应用提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [2] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 研究发现，微调目标与模型编辑技术的交互会导致编辑后的知识比预训练获取的内在知识更容易被遗忘，冻结相关层可改善知识保留。


<details>
  <summary>Details</summary>
Motivation: 探索微调对模型编辑后知识的影响，以解决当前编辑方法在实践中的局限性。

Method: 系统研究不同微调目标与模型编辑技术的交互，并分析知识保留情况。

Result: 编辑后的知识在微调中更容易被遗忘，冻结相关层可显著提高知识保留。

Conclusion: 评估编辑鲁棒性时需考虑下游微调，未来编辑方法应关注知识保留的改进。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [3] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: SMACS框架通过多智能体协作系统整合开源LLMs，性能超越闭源LLMs。


<details>
  <summary>Details</summary>
Motivation: 探索开源LLMs能否通过协作超越闭源LLMs。

Method: 提出SMACS框架，包括检索式先验选择（RPS）和探索-利用驱动的后验增强（EPE）。

Result: 在多个基准测试中，SMACS性能优于主流闭源LLMs（如Claude-3.7-Sonnet、GPT-4.1）。

Conclusion: SMACS证明了开源LLMs协作的潜力，推动了智能上限。

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [4] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: PoliAnalyzer是一个神经符号系统，通过NLP和逻辑推理帮助用户个性化分析隐私政策，显著减少认知负担。


<details>
  <summary>Details</summary>
Motivation: 现代用户很少阅读隐私政策，但希望保护个人数据。PoliAnalyzer旨在自动化分析隐私政策，帮助用户理解关键条款。

Method: 结合NLP提取政策文本的正式表示，并通过逻辑推理比较用户偏好与政策内容，生成合规报告。

Result: 在评估中，PoliAnalyzer的F1分数达90-100%，并能识别95.2%无冲突的政策条款，仅需关注4.8%的冲突部分。

Conclusion: PoliAnalyzer展示了规模化自动化隐私政策分析的可行性，有助于用户重获数据控制权并推动社会讨论。

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [5] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: 该论文探讨了使用NLP模型（如BERT、RoBERTa等）通过社交媒体文本识别双相情感障碍的早期迹象，发现RoBERTa表现最佳，F1分数达98%。


<details>
  <summary>Details</summary>
Motivation: 双相情感障碍常因早期症状不明显和社会污名化而被漏诊，研究旨在利用NLP技术改进早期筛查。

Method: 评估了多种基于Transformer和LSTM的模型，使用Reddit帖子数据集进行实验。

Result: RoBERTa表现最优，LSTM结合BERT嵌入效果相近，而静态嵌入的LSTM表现差。

Conclusion: 上下文语言模型在双相情感障碍检测中至关重要，研究为心理健康NLP应用提供了模型选择依据。

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [6] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）在用户交互应用中会根据文本中的身份标记（如种族、性别、年龄）产生偏见，影响医疗、法律、政治等领域的决策，可能导致有害后果。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs如何利用用户文本中的身份信息进行决策，并评估其在高风险应用中的潜在偏见。

Method: 对五个高风险领域（医疗、法律、政治、政府福利、薪资）的LLM应用进行全面分析，研究身份标记对模型输出的影响。

Result: LLMs对身份标记极为敏感，种族、性别和年龄等因素显著影响其回答，例如医疗建议中的差异化标准、薪资推荐中的性别和种族偏见等。

Conclusion: 现成的LLMs在这些应用中可能导致有害差异，建议未来部署前进行类似全面评估，并提供新工具以评估身份标记对模型决策的影响。

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [7] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: 论文提出CCL-XCoT框架，通过两阶段微调减少多语言大模型在低资源语言中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在低资源语言中易产生幻觉（不准确或虚构输出），影响领域特定生成任务。

Method: 1. 基于课程的对比学习增强跨语言语义对齐；2. 跨语言思维链（XCoT）提示策略引导模型推理。

Result: CCL-XCoT将幻觉率降低62%，显著提升跨语言事实知识迁移。

Conclusion: CCL-XCoT有效减少幻觉，无需外部检索或多模型集成。

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [8] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）供应链中模型与数据集的关系，构建了一个包含397,376个节点和453,469条边的异构图，揭示了其稀疏性、核心-外围结构及动态性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM依赖预训练模型和外部数据集，可能继承漏洞或偏见，因此理解其供应链关系对风险检测和公平性改进至关重要。

Method: 设计方法系统收集LLM供应链数据，构建有向异构图，分析其结构和动态性。

Result: 发现LLM供应链图稀疏且符合幂律分布，数据集在训练中起关键作用，模型与数据集相互依赖，且图结构动态更新。

Conclusion: 研究为理解LLM供应链提供了框架，有助于风险检测和生态系统优化。

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [9] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix是一个自动提示优化框架，能将自然语言任务描述转化为高质量提示，无需手动调整或领域专业知识，性能优于现有库。


<details>
  <summary>Details</summary>
Motivation: 解决提示工程手动、不一致且对非专家不友好的问题。

Method: 结合轻量级元提示优化器和DSPy编译器，分析用户意图、生成合成数据、选择提示策略并优化提示。

Result: 在5个任务类别中表现优于现有库，同时减少提示长度和计算开销。

Conclusion: Promptomatix实现了高效、可扩展的提示优化。

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [10] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope是一种针对多样化图表类型优化的LVLM，通过高效数据生成管道和双路径训练策略提升图表理解能力，并建立了新的评估基准ChartDQA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在科学图表理解任务中存在泛化能力不足和数据对齐训练缺失的问题。

Method: 提出高效数据生成管道和双路径训练策略，结合底层数据推理。

Result: 实验表明ChartScope显著提升了对多样化图表类型的理解能力。

Conclusion: ChartScope为图表理解任务提供了更优的解决方案，代码和数据已公开。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [11] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 论文研究了基于LLM的选择性翻译方法，用于解决多语言大模型在低资源语言中对齐的挑战，相比传统翻译方法能更好地保留非可翻译内容。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在低资源语言中表现较差，且高质量对齐数据稀缺。传统翻译方法无法保留代码、数学表达式等关键内容，因此需要更有效的方法。

Method: 提出LLM-based选择性翻译技术，仅翻译可翻译部分，保留非可翻译内容和句子结构。对比了Google Cloud Translation和Llama-3.1-405B的翻译效果。

Result: 实验表明选择性翻译在提升多语言对齐效果上优于传统翻译方法，尤其在低资源印地语中表现突出。

Conclusion: 选择性翻译是一种实用且有效的方法，可显著改善多语言大模型在低资源语言中的对齐性能。

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [12] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（LLMs）在处理语言时态意义时是否具有类似人类的认知能力，发现其依赖原型性、判断不一致且因果推理能力有限。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否具备人类般的语言认知能力，还是仅依赖高级模式识别。

Method: 采用专家参与的探测流程，通过针对性实验评估LLMs的语义表征和语用推理能力。

Result: LLMs过度依赖原型性，时态判断不一致，因果推理能力不足，叙事理解不完整。

Conclusion: LLMs处理语言时态的方式与人类不同，缺乏稳健的叙事理解能力，并提出了标准化评估框架。

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [13] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: 论文研究了克罗地亚新闻标题的点击诱饵检测，比较了微调模型和上下文学习方法的效果，发现微调模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 在线新闻依赖广告收入，导致标题多为点击诱饵，影响信息质量和读者信任，需自动检测方法。

Method: 构建CLIC数据集，微调BERTi\'c模型，并与基于LLM的上下文学习方法对比。

Result: 近半数标题含点击诱饵，微调模型优于通用LLM。

Conclusion: 微调模型在点击诱饵检测中表现更佳，尤其在资源较少的语言中。

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [14] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: 论文研究了利用大型语言模型（如GPT-4、LLaMA）进行人格评估的可行性，发现尽管模型在测试重测信度上表现良好，但在构念效度上存在局限，与真实人格评分的相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在人格评估中的应用潜力，解决以往研究中依赖合成数据或缺乏心理测量效度的问题。

Method: 使用555份半结构化访谈和BFI-10自评分数作为基准，测试了三种先进LLM（GPT-4.1 Mini、Meta-LLaMA、DeepSeek）在零样本提示和链式思考提示下的表现。

Result: 模型在测试重测信度上表现良好，但构念效度有限（最大Pearson's r=0.27），预测偏向中等或高特质水平。链式思考提示和更长输入上下文略微改善了分布对齐，但未提高特质级准确性。

Conclusion: 当前基于LLM的人格推断存在局限性，需要基于证据的开发以支持心理学应用。

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [15] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: 本文介绍了LinkedIn开发的内部聊天机器人，通过知识图谱和Text-to-SQL代理，帮助用户自助获取数据洞察，并展示了其实际效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在Text-to-SQL任务上取得了进展，但构建企业级解决方案仍具挑战性。本文旨在解决这一问题。

Method: 方法包括构建动态知识图谱、开发Text-to-SQL代理（含检索、排序、纠错功能）以及设计交互式聊天机器人。

Result: 聊天机器人每周有300多名用户，53%的响应在内部测试中正确或接近正确。消融实验确定了关键组件。

Conclusion: 研究为企业级Text-to-SQL解决方案的开发提供了实用路径。

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [16] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: 提出了一种基于错误感知的师生框架，通过GPT-4o的指导改进生物医学文本中的关系分类，结合知识图谱和课程学习，在多个数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的关系分类对构建知识图谱和药物再利用等应用至关重要，但现有方法存在不足。

Method: 使用师生框架，通过分析基线模型的错误类型和难度，生成针对性改进，并结合知识图谱进行课程学习。

Result: 在5个PPI数据集中的4个和DDI数据集上取得最优性能，在ChemProt上保持竞争力。

Conclusion: 该方法通过结构化指导和渐进学习显著提升了关系分类性能。

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [17] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0是一种专为半导体显示行业设计的高性能推理模型，填补了LLMs在该领域缺乏专业知识的空白。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在半导体显示行业的应用受限，缺乏领域专业知识，需要针对性解决方案。

Method: 通过监督微调、强化学习和行业知识库训练模型，并引入自动化评估框架和领域特定RAG机制。

Result: 尽管参数规模较小（320亿），X-Intelligence 3.0在多个评估中优于SOTA模型DeepSeek-R1-671B。

Conclusion: X-Intelligence 3.0高效解决了半导体显示行业的复杂推理问题，成为该领域的强大工具。

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [18] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: XL-DURel是一个优化的多语言句子Transformer模型，用于序数词义分类任务，通过角距离损失函数在复杂空间中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决序数和二分类词义任务中的性能问题，并探索统一建模的可能性。

Method: 使用多种损失函数进行回归和排序任务，基于复杂空间中的角距离优化模型。

Result: 在序数和二分类任务中表现优于现有模型，且序数优化能提升二分类性能。

Conclusion: 序数词义分类的统一建模为不同任务提供了更优的解决方案。

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [19] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 论文探讨了使用AudiBERT模型在协作问题解决（CPS）诊断中的改进，特别是在社会认知维度上显著优于BERT模型，但在情感维度上未观察到类似改进。


<details>
  <summary>Details</summary>
Motivation: 解决AI在教育领域中检测CPS指标的挑战，并探索多模态BERT模型（AudiBERT）的潜力。

Method: 使用AudiBERT模型整合语音和声学韵律特征，并与BERT模型进行对比分析。

Result: AudiBERT在社会认知维度上有显著改进，但在情感维度上无显著差异；训练数据量与召回率显著相关。

Conclusion: 提出了一种结构化方法以实现人机互补，强调模型可解释性在支持人类参与中的重要性。

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [20] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 研究使用SHAP方法分析BERT模型在协作问题解决（CPS）分类中词标记的贡献，发现高分类性能未必对应合理解释，并识别出无意义的词标记。


<details>
  <summary>Details</summary>
Motivation: 增强BERT模型在CPS诊断中的可解释性，以提升教师等终端用户的信任和采用率。

Method: 使用SHAP方法分析BERT模型中词标记对分类决策的贡献。

Result: 发现分类性能高但解释不合理，存在无意义词标记影响分类，模型透明性有助于避免过度依赖AI。

Conclusion: 建议探索集成模型架构和人类-AI互补，因CPS子技能的精细判别仍需人类推理。

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [21] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文探讨了利用GPT等大语言模型进行NLP数据增强的方法，比较了传统方法（如复述和回译）与纯生成方法的性能。实验表明，传统方法在数据质量和分类性能上表现可比或更优。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定机器学习任务中数据稀缺和类别不平衡的问题，探索传统数据增强方法在新一代模型中的潜力。

Method: 选择了包括复述、回译等方法，利用ChatGPT进行实验，比较了四种数据增强方法在不同实验设置下的表现。

Result: 回译和复述方法在生成数据质量和分类性能上表现与零样本或少样本生成方法相当或更好。

Conclusion: 传统数据增强方法结合新一代模型可以有效解决数据稀缺问题，且性能不逊于纯生成方法。

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [22] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: 研究提出了一种基于检索增强生成（RAG）的方法，为肯尼亚基层医疗创建了一个基准数据集和评估框架，以测试大语言模型（LLMs）在非洲医疗场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在非洲低资源医疗环境中的有效性，填补其在基层医疗应用中的研究空白。

Method: 使用RAG技术将临床问题与肯尼亚国家指南对齐，生成多语言（英语和斯瓦希里语）的临床场景、选择题和答案，并通过专家评审确保质量。

Result: 发现LLMs在本地化医疗场景中表现显著不足，尤其是在非洲医疗内容上的准确性低于美国基准。

Conclusion: 研究提供了一个可复制的指南驱动动态评估模型，支持LLMs在非洲医疗系统中的安全部署。

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [23] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: 论文发现，通过双部分仿射近似可以很好地近似某些主客体关系的Transformer计算。通过调整Bigger Analogy Test Set，证明线性变换Ws能准确重现最终客体状态，尤其在形态关系上达到90%的忠实度。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型中主客体关系的线性可解释性，特别是在形态学和多语言场景下的表现。

Method: 使用双部分仿射近似和线性变换Ws（s为主词中间层表示，W为模型导数）来近似最终客体状态。

Result: 在形态关系上达到90%的忠实度，多语言和跨模型实验也验证了类似结果。

Conclusion: 语言模型中的某些概念关系（如形态学）可通过潜在空间的线性变换稀疏编码，具有较高的可解释性。

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [24] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: 论文提出了一种名为Cleanse的聚类方法，用于估计大型语言模型（LLM）生成内容的不确定性，以检测幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多种NLP任务中表现优异，但其生成的错误响应（幻觉）可能引发安全和可靠性问题，因此需要有效的不确定性估计方法。

Method: Cleanse通过聚类LLM隐藏嵌入中的语义一致性比例来量化不确定性，这些嵌入包含生成的语义信息。

Result: 实验验证了Cleanse在LLaMA和Mistral模型以及SQuAD和CoQA基准上的有效性。

Conclusion: Cleanse为检测LLM幻觉提供了一种有效的不确定性估计方法。

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [25] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: Mangosteen是一个470亿标记的泰语语料库，通过改进的Dolma流程构建，提升了泰语模型的质量和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语料库缺乏对泰语脚本和文化细微差别的处理，且缺乏透明度和可复现性。

Method: 采用泰语适配的Dolma流程，包括自定义语言识别、质量过滤器和内容过滤器，以及非网络来源的精选数据。

Result: 构建的语料库显著提升了模型性能，在泰语基准测试中超越现有模型。

Conclusion: Mangosteen为泰语及区域LLM研究提供了透明、高质量且可复现的基础。

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [26] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）在医疗编码（ICPC-2）任务中的潜力，发现多数模型表现良好，但小模型在格式和输入长度上存在问题。


<details>
  <summary>Details</summary>
Motivation: 医疗编码对医疗数据管理至关重要，研究旨在探索LLMs在此任务中的自动化潜力。

Method: 使用437个巴西葡萄牙语临床表达数据集，结合语义搜索引擎和33种LLMs进行ICPC-2编码匹配，评估性能指标包括F1分数、成本等。

Result: 28种模型F1分数>0.8，10种>0.85；检索优化可提升性能4分；小模型在格式和输入长度上表现不佳。

Conclusion: LLMs在自动化医疗编码中潜力巨大，但需更广泛的多语言和端到端评估以验证临床适用性。

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [27] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: MiroMind-M1系列是完全开源的推理语言模型，基于Qwen-2.5架构，性能优于现有开源模型，并公开了模型、数据集和训练配置以提升透明度。


<details>
  <summary>Details</summary>
Motivation: 解决开源推理语言模型缺乏透明度和可复现性的问题，推动社区研究。

Method: 采用两阶段训练：SFT（监督微调）和RLVR（强化学习验证），并引入Context-Aware Multi-Stage Policy Optimization算法。

Result: 在AIME24、AIME25和MATH基准测试中表现优异，达到或超越现有开源7B和32B模型的性能。

Conclusion: MiroMind-M1系列为开源推理语言模型提供了透明和可复现的解决方案，支持社区进一步研究。

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [28] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: 本文综述了Hugging Face Hub上公开的阿拉伯语后训练数据集，从四个维度（LLM能力、可操控性、对齐性和鲁棒性）评估了数据集的质量和多样性，揭示了当前阿拉伯语数据集的关键不足，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 后训练是提升预训练大语言模型性能的关键技术，但阿拉伯语后训练数据集的质量和多样性存在明显不足，影响了阿拉伯语LLM的发展和应用。

Method: 通过四个关键维度（LLM能力、可操控性、对齐性和鲁棒性）对公开的阿拉伯语后训练数据集进行评估，分析其流行度、实用性、文档质量等指标。

Result: 发现阿拉伯语后训练数据集存在任务多样性不足、文档和标注不完整、社区采用率低等问题。

Conclusion: 这些不足限制了阿拉伯语LLM的进展，未来需在数据集开发中改进任务多样性、文档质量和社区推广。

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [29] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: 研究构建了一个土耳其语自杀意念语料库，并评估了标签可靠性和模型一致性，强调心理健康NLP中需要更严格的标注和评估方法。


<details>
  <summary>Details</summary>
Motivation: 解决自杀意念检测中语言覆盖不足和标注不可靠的问题，推动全球自杀预防的AI应用。

Method: 构建土耳其语语料库，引入高效标注框架，利用预训练模型进行双向评估和迁移学习。

Result: 发现流行模型在零样本迁移学习中表现不佳，强调标注和模型可靠性的重要性。

Conclusion: 呼吁心理健康NLP中透明化数据构建和模型训练，优先考虑数据和模型的可靠性。

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [30] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: 该研究通过分析8万多篇同行评审的语言特征，揭示了评审中存在的隐性偏见，并探讨了匿名性对公平性的影响。


<details>
  <summary>Details</summary>
Motivation: 同行评审作为科学诚信的守门人，存在偏见问题，但语言如何加剧不平等尚未充分研究。

Method: 使用自然语言处理和大规模统计建模，分析评审的语气、情感和支持性语言，考察作者人口统计特征和评审匿名性的影响。

Result: 发现评审语言因作者性别、种族和机构背景而异，匿名性并未完全消除偏见。

Conclusion: 研究揭示了同行评审中的隐性偏见，对评审政策和科学公平性提出了重要问题。

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [31] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: 研究验证了多模态神经网络在模拟儿童语言学习中的稳健性，并揭示了不同儿童数据训练的模型学习差异。


<details>
  <summary>Details</summary>
Motivation: 探讨机器学习如何通过有限输入模拟儿童语言学习，弥补大模型与儿童学习之间的差距。

Method: 使用自动语音转录技术处理SAYCam数据集，生成多模态训练和评估数据，测试不同神经网络配置。

Result: 模型能从每个儿童的转录数据中学习并泛化词汇-指代映射，验证了方法的稳健性，同时显示个体差异。

Conclusion: 多模态神经网络在模拟儿童语言学习中表现稳健，但个体差异显著，为未来研究提供了方向。

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [32] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: GRACE提出了一种基于生成模型的多行为序列推荐框架，通过改进的tokenization和稀疏注意力机制解决了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在多行为推荐中存在信息不明确、计算成本高和多尺度建模不足的问题。

Method: GRACE结合了Chain-of-Thought tokenization和Journey-Aware Sparse Attention机制，实现了高效且可解释的推荐生成。

Result: 在两个真实数据集上，GRACE显著优于现有基线，性能提升最高达106.9%，同时计算成本降低48%。

Conclusion: GRACE为多行为序列推荐提供了一种高效且可解释的解决方案。

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [33] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: FastLongSpeech框架通过迭代融合和动态压缩训练，解决了LSLMs处理长语音的挑战，无需专用长语音训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有LSLMs在长语音处理上存在不足，主要由于缺乏长语音训练数据和高计算成本。

Method: 引入FastLongSpeech框架，采用迭代融合策略和动态压缩训练方法，将短语音能力迁移至长语音任务。

Result: 实验表明，该方法在长语音和短语音任务中均表现优异，并显著提升推理效率。

Conclusion: FastLongSpeech为LSLMs高效处理长语音提供了可行方案，填补了研究空白。

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [34] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: 论文提出了一种基于意图从长文档生成图表的无监督两阶段框架，通过LLM提取信息并验证，再选择图表类型生成代码，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要用户手动选择相关内容，难以直接应用于基于意图的长文档数据可视化。

Method: 提出两阶段框架：LLM分解意图并提取验证数据，启发式模块选择图表类型后生成代码。

Result: 在图表数据准确性和类型选择上分别优于基线方法9分和17分。

Conclusion: 该方法在零样本设置下有效生成基于意图的图表，优于现有方法。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [35] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: 推理蒸馏提升小语言模型的推理能力，但对长上下文检索和推理的影响尚不明确。本文通过实验证明，蒸馏显著改善长上下文理解，缓解“中间迷失”问题。


<details>
  <summary>Details</summary>
Motivation: 研究大规模推理蒸馏对长上下文理解的影响，特别是在检索增强生成（RAG）系统中，以解决“中间迷失”问题。

Method: 使用从Deepseek-R1蒸馏的开源模型，通过多文档问答任务评估长上下文信息提取和整合能力。

Result: 蒸馏显著提升长上下文理解，促进更详细和明确的推理过程，有效缓解“中间迷失”问题。

Conclusion: 推理蒸馏不仅增强推理能力，还能显著改善长上下文理解，为RAG系统提供更可靠的生成能力。

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [36] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 研究表明，即使是小型语言模型（TLMs）也能通过预训练展现出与大型语言模型（LLMs）类似的关键特性，且预训练效果随数据集规模和任务重叠度增加而提升。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）预训练需要巨大计算资源，限制了广泛研究参与，因此探索小型语言模型（TLMs）是否具备类似特性成为关键需求。

Method: 研究通过预训练BERT-6及其变体（如BERT-1）在Wikipedia子集上，并在FewRel、AGNews和DBPedia分类任务中评估其性能。

Result: 预训练的TLMs在分类任务中表现优于未预训练模型，且通过多个浅层架构的软委员会方法可实现低延迟而不影响准确性。

Conclusion: TLMs的研究为NLP机制提供了新见解，并表明其可能足以支持儿童或青少年的语言发展。

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [37] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: MEKiT方法通过整合多源异构知识，显著提升大语言模型在情感-原因对提取任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在文本理解和生成方面表现优异，但在需要推理能力的情感-原因对提取任务中表现不佳，主要原因是缺乏辅助知识。

Method: 提出MEKiT方法，整合内部情感知识和外部因果知识，通过指令模板和数据混合进行指令微调。

Result: 实验表明MEKiT在情感-原因对提取任务中优于基线方法，显著提升大语言模型性能。

Conclusion: MEKiT为情感-原因对提取任务提供了更有效和适应性强的解决方案。

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [38] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 论文提出SASFT方法，通过稀疏自编码器分析语言混合问题，并指导监督微调，显著减少意外语码转换。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在意外语码转换问题，影响可读性和实用性，现有方法缺乏机制分析且效果有限。

Method: 使用稀疏自编码器分析语言特征预激活值，提出SASFT方法指导监督微调。

Result: SASFT在五个模型和三种语言中减少50%以上意外语码转换，部分案例完全消除，同时保持多语言性能。

Conclusion: SASFT有效解决语码转换问题，且不损害模型的多语言能力。

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [39] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

TL;DR: 提出了一种基于神经元状态的跨语言对齐评估方法（NeuronXA），用于评估大语言模型的跨语言对齐能力，效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言对齐评估方法主要关注句子嵌入，但神经网络模型可能产生非平滑表示空间，影响低资源语言的语义对齐评估。

Method: 受神经科学启发，提出NeuronXA方法，通过神经元状态评估跨语言对齐能力。

Result: 在多个多语言大语言模型上测试，仅需100对平行句子，NeuronXA与下游任务性能的Pearson相关性达0.9556，与可迁移性的相关性达0.8514。

Conclusion: NeuronXA能有效评估跨语言对齐和可迁移性，即使数据量小，有望推动跨语言对齐研究和多语言大语言模型的语义理解。

Abstract: Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [40] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: PromptSuite是一个自动生成多样化提示的框架，用于更稳健的LLM评估。


<details>
  <summary>Details</summary>
Motivation: 解决单提示评估不可靠且多提示评估难以生成的问题。

Method: 采用模块化提示设计，支持可控扰动和新组件扩展。

Result: 通过案例研究证明PromptSuite能生成有意义的提示变体。

Conclusion: PromptSuite为LLM评估提供了灵活且可扩展的解决方案。

Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


### [41] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
*Vahid Rahimzadeh,Erfan Moosavi Monazzah,Mohammad Taher Pilehvar,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: SYNTHIA是一个基于真实社交媒体用户数据的合成人物数据集，解决了现有方法在一致性和真实性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖昂贵的人工数据，要么生成缺乏一致性和真实性的合成人物，SYNTHIA旨在填补这一空白。

Method: 利用BlueSky开放平台上10,000名真实用户的社交媒体活动数据，生成30,000个背景故事。

Result: SYNTHIA在人口多样性和社会调查对齐方面表现优异，同时在叙事一致性上显著优于现有方法。

Conclusion: SYNTHIA通过引入时间维度和社交互动元数据，为计算社会科学和人物驱动语言模型开辟了新研究方向。

Abstract: Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

</details>


### [42] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
*Hang Yan,Fangzhi Xu,Rongman Xu,Yifei Li,Jian Zhang,Haoran Luo,Xiaobao Wu,Luu Anh Tuan,Haiteng Zhao,Qika Lin,Jun Liu*

Main category: cs.CL

TL;DR: MUR（动量不确定性引导推理）通过动态分配推理预算，减少LLM的冗余计算，提高推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型（LLM）在推理任务中的效率，避免因过度思考（overthinking）导致的资源浪费。

Method: 提出MUR方法，利用动量概念动态跟踪和聚合推理步骤的不确定性，并通过gamma-control机制灵活调整推理预算。

Result: 在多个基准测试中，MUR平均减少50%以上的计算量，同时提升准确率0.62-3.37%。

Conclusion: MUR是一种无需额外训练的高效推理优化方法，显著提升了LLM的推理效率和性能。

Abstract: Large Language Models (LLMs) have achieved impressive performance on
reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an
open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it
often leads to overthinking, wasting tokens on redundant computations. This
work investigates how to efficiently and adaptively guide LLM test-time scaling
without additional training. Inspired by the concept of momentum in physics, we
propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically
allocates thinking budgets to critical reasoning steps by tracking and
aggregating stepwise uncertainty over time. To support flexible inference-time
control, we introduce gamma-control, a simple mechanism that tunes the
reasoning budget via a single hyperparameter. We provide in-depth theoretical
proof to support the superiority of MUR in terms of stability and biases. MUR
is comprehensively evaluated against various TTS methods across four
challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using
different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate
that MUR reduces computation by over 50% on average while improving accuracy by
0.62-3.37%.

</details>


### [43] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出RefCritic，一种基于强化学习的批评模块，通过双规则奖励机制提升模型批评能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于监督微调的批评模块无法真正提升模型的批评能力，生成的批评缺乏深度和可操作性。

Method: 提出RefCritic，采用强化学习框架，结合实例级正确性和策略模型改进准确性的双规则奖励。

Result: 在多个基准测试中，RefCritic表现优异，如AIME25上分别提升6.8%和7.2%。

Conclusion: RefCritic显著提升了批评模块的质量和可操作性，优于现有方法。

Abstract: With the rapid advancement of Large Language Models (LLMs), developing
effective critic modules for precise guidance has become crucial yet
challenging. In this paper, we initially demonstrate that supervised
fine-tuning for building critic modules (which is widely adopted in current
solutions) fails to genuinely enhance models' critique abilities, producing
superficial critiques with insufficient reflections and verifications. To
unlock the unprecedented critique capabilities, we propose RefCritic, a
long-chain-of-thought critic module based on reinforcement learning with dual
rule-based rewards: (1) instance-level correctness of solution judgments and
(2) refinement accuracies of the policy model based on critiques, aiming to
generate high-quality evaluations with actionable feedback that effectively
guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and
DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement
settings, RefCritic demonstrates consistent advantages across all benchmarks,
e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably,
under majority voting, policy models filtered by RefCritic show superior
scaling with increased voting numbers. Moreover, despite training on
solution-level supervision, RefCritic outperforms step-level supervised
approaches on ProcessBench, a benchmark to identify erroneous steps in
mathematical reasoning.

</details>


### [44] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种基于形式化的信息搜索（IS）数据合成框架WebShaper，通过知识投影（KP）操作控制推理结构，生成高质量数据集，提升了IS代理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有信息搜索代理的训练数据质量不足，且信息结构与推理结构不一致，限制了其发展。

Method: 提出WebShaper框架，通过形式化任务和知识投影操作合成数据集，采用多步扩展过程生成复杂问题。

Result: 在GAIA和WebWalkerQA基准测试中，WebShaper达到开源IS代理的最先进性能。

Conclusion: WebShaper通过形式化方法解决了数据质量问题，显著提升了IS代理的能力。

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [45] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: 比较了DNA序列建模中k-mer分割与BPE子词标记化的性能，以及三种位置编码方法，发现BPE表现更优，RoPE适合周期性模体，AliBi适合局部依赖任务。


<details>
  <summary>Details</summary>
Motivation: 缺乏对DNA序列建模中不同标记化和位置编码方法的系统评估，需明确哪种方法更优。

Method: 比较k-mer分割（k=1,3,4,5,6）、BPE标记化和三种位置编码方法（sinusoidal、AliBi、RoPE），在不同层数的Transformer编码器上训练并评估。

Result: BPE表现更稳定且性能更高，RoPE擅长捕捉周期性模体，AliBi适合局部依赖任务。层数从3增加到12层效果显著，24层提升有限或过拟合。

Conclusion: 为DNA Transformer模型设计标记化和位置编码提供了实用指导。

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


### [46] [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)
*Vijeta Deshpande,Ishita Dasgupta,Uttaran Bhattacharya,Somdeb Sarkhel,Saayan Mitra,Anna Rumshisky*

Main category: cs.CL

TL;DR: 提出了一种新的词汇多样性度量方法PATTR，解决了现有方法因文本长度变化导致的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 研究提示变化对生成文本长度及词汇多样性的影响，现有度量方法存在长度偏差。

Method: 提出PATTR度量方法，生成20M词的合成语料库，比较PATTR与MATTR、CR的表现。

Result: PATTR能有效减少长度偏差，在筛选多样性高的文本时优于MATTR和CR。

Conclusion: PATTR是一种鲁棒的多样性度量方法，适用于需要控制文本长度的任务。

Abstract: Synthetic text generated by Large Language Models (LLMs) is increasingly used
for further training and improvement of LLMs. Diversity is crucial for the
effectiveness of synthetic data, and researchers rely on prompt engineering to
improve diversity. However, the impact of prompt variations on response text
length, and, more importantly, the consequential effect on lexical diversity
measurements, remain underexplored. In this work, we propose Penalty-Adjusted
Type-Token Ratio (PATTR), a diversity metric robust to length variations. We
generate a large synthetic corpus of over 20M words using seven models from the
LLaMA, OLMo, and Phi families, focusing on a creative writing task of video
script generation, where diversity is crucial. We evaluate per-response lexical
diversity using PATTR and compare it against existing metrics of Moving-Average
TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length
variations introduce biases favoring shorter responses. Unlike existing
metrics, PATTR explicitly considers the task-specific target response length
($L_T$) to effectively mitigate length biases. We further demonstrate the
utility of PATTR in filtering the top-10/100/1,000 most lexically diverse
responses, showing that it consistently outperforms MATTR and CR by yielding on
par or better diversity with high adherence to $L_T$.

</details>


### [47] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
*Chathuri Jayaweera,Brianna Yanqui,Bonnie Dorr*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）作为常识知识生成器在自然语言推理（NLI）中的潜力，评估其可靠性和对预测准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有常识资源对多种前提-假设对的覆盖不足，需探索LLM生成常识知识的可靠性及其对NLI的影响。

Method: 调整和修改现有指标以评估LLM生成常识知识的真实性和一致性。

Result: 显式引入常识知识未显著提升整体结果，但能有效区分蕴含实例，并适度改善矛盾和中立推理的区分。

Conclusion: LLM作为常识知识生成器在NLI中有潜力，但需进一步优化其生成质量和应用方式。

Abstract: Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

</details>


### [48] [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
*Chathuri Jayaweera,Bonnie Dorr*

Main category: cs.CL

TL;DR: 论文主张NLI中的标注分歧并非噪音，而是反映了有意义的解释差异，尤其是由前提或假设的模糊性引起。提出模糊感知NLI框架，整合现有分类法，并呼吁构建标注模糊性的数据集。


<details>
  <summary>Details</summary>
Motivation: 探讨NLI中标注分歧的根源，强调模糊性作为解释差异的信号，推动更鲁棒、可解释的NLI系统。

Method: 提出统一框架整合模糊性分类法，通过具体案例展示模糊性子类型及其对标注决策的影响。

Result: 揭示模糊性如何影响标注决策，提出需针对性检测方法以更好对齐模型与人类解释。

Conclusion: 呼吁构建模糊性标注数据集，提出无监督检测方法，为更鲁棒、可解释的NLI系统铺路。

Abstract: This position paper argues that annotation disagreement in Natural Language
Inference (NLI) is not mere noise but often reflects meaningful interpretive
variation, especially when triggered by ambiguity in the premise or hypothesis.
While underspecified guidelines and annotator behavior can contribute to
variation, content-based ambiguity offers a process-independent signal of
divergent human perspectives. We call for a shift toward ambiguity-aware NLI by
systematically identifying ambiguous input pairs and classifying ambiguity
types. To support this, we present a unified framework that integrates existing
taxonomies and illustrate key ambiguity subtypes through concrete examples.
These examples reveal how ambiguity shapes annotator decisions and motivate the
need for targeted detection methods that better align models with human
interpretation. A key limitation is the lack of datasets annotated for
ambiguity and subtypes. We propose addressing this gap through new annotated
resources and unsupervised approaches to ambiguity detection -- paving the way
for more robust, explainable, and human-aligned NLI systems.

</details>


### [49] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
*Hellina Hailu Nigatu,Atnafu Lambebo Tonja,Henok Biadglign Ademtew,Hizkel Mitiku Alemayehu,Negasi Haile Abadi,Tadesse Destaw Belay,Seid Muhie Yimam*

Main category: cs.CL

TL;DR: 研究探讨了阿姆哈拉语NLP中的同音词归一化问题，提出后推断归一化方法，提升BLEU分数同时保留语言特征。


<details>
  <summary>Details</summary>
Motivation: 同音词归一化虽能提升自动指标性能，但可能导致模型无法理解语言多样性，并影响跨语言迁移学习。

Method: 通过单语训练和跨语言迁移实验，研究归一化对Ge'ez文字语言的影响，并提出后推断归一化方案。

Result: 后推断归一化使BLEU分数提升1.03，同时保留训练中的语言特征。

Conclusion: 工作呼吁更多语言感知的干预措施，促进技术驱动的语言变化讨论。

Abstract: Homophone normalization, where characters that have the same sound in a
writing script are mapped to one character, is a pre-processing step applied in
Amharic Natural Language Processing (NLP) literature. While this may improve
performance reported by automatic metrics, it also results in models that are
not able to understand different forms of writing in a single language.
Further, there might be impacts in transfer learning, where models trained on
normalized data do not generalize well to other languages. In this paper, we
experiment with monolingual training and cross-lingual transfer to understand
the impacts of normalization on languages that use the Ge'ez script. We then
propose a post-inference intervention in which normalization is applied to
model predictions instead of training data. With our simple scheme of
post-inference normalization, we show that we can achieve an increase in BLEU
score of up to 1.03 while preserving language features in training. Our work
contributes to the broader discussion on technology-facilitated language change
and calls for more language-aware interventions.

</details>


### [50] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 研究评估了三种LLM（Gemini-2.0-flash、Grok-3、GPT-4o-mini）在医学领域（高血压、糖尿病、骨科）中数据提取任务的性能，发现定制提示最有效，并提出三级自动化指南。


<details>
  <summary>Details</summary>
Motivation: 自动化从RCTs中提取数据用于荟萃分析仍具挑战性，需评估LLMs的实际表现。

Method: 测试三种LLM在统计结果、偏倚风险评估和研究特征提取中的表现，比较四种提示策略。

Result: 模型精度高但召回率低，定制提示可提升召回率15%。

Conclusion: 提出三级自动化指南，平衡LLM效率与专家监督，适用于实际荟萃分析。

Abstract: Automating data extraction from full-text randomised controlled trials (RCTs)
for meta-analysis remains a significant challenge. This study evaluates the
practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)
across tasks involving statistical results, risk-of-bias assessments, and
study-level characteristics in three medical domains: hypertension, diabetes,
and orthopaedics. We tested four distinct prompting strategies (basic
prompting, self-reflective prompting, model ensemble, and customised prompts)
to determine how to improve extraction quality. All models demonstrate high
precision but consistently suffer from poor recall by omitting key information.
We found that customised prompts were the most effective, boosting recall by up
to 15\%. Based on this analysis, we propose a three-tiered set of guidelines
for using LLMs in data extraction, matching data types to appropriate levels of
automation based on task complexity and risk. Our study offers practical advice
for automating data extraction in real-world meta-analyses, balancing LLM
efficiency with expert oversight through targeted, task-specific automation.

</details>


### [51] [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)
*Xiandong Meng,Yan Wu,Yexin Tian,Xin Hu,Tianze Kang,Junliang Du*

Main category: cs.CL

TL;DR: 提出一种基于多教师模型的知识蒸馏方法，降低大语言模型的计算成本和推理时间，同时提升学生模型的语言理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型部署时的高计算成本和推理速度慢的问题。

Method: 通过多教师模型的输出概率分布和中间语义特征指导学生模型学习，引入加权输出融合、特征对齐损失和动态教师权重策略。

Result: 学生模型在语言建模、文本生成和多任务学习中表现优异，实验验证了其在困惑度、蒸馏损失和生成质量上的优势。

Conclusion: 多教师协作机制为大规模语言模型的高效压缩提供了可行技术路径，并在复杂语言任务中表现出有效性。

Abstract: This paper addresses the challenges of high computational cost and slow
inference in deploying large language models. It proposes a distillation
strategy guided by multiple teacher models. The method constructs several
teacher models and integrates their output probability distributions and
intermediate semantic features. This guides the student model to learn from
multiple sources of knowledge. As a result, the student model gains stronger
language understanding and generation ability while maintaining a small
parameter size. To achieve this, the paper introduces a weighted output fusion
mechanism, a feature alignment loss function, and an entropy-driven dynamic
teacher weighting strategy. These components improve the quality and stability
of knowledge transfer during distillation. Under multi-teacher guidance, the
student model captures semantic information more effectively and demonstrates
strong performance across multiple evaluation metrics. In particular, the
method shows high consistency in expression, generalization ability, and task
adaptability in tasks such as language modeling, text generation, and
multi-task learning. The experiments compare the proposed method with several
widely adopted distillation approaches. The results further confirm its overall
advantages in perplexity, distillation loss, and generation quality. This study
provides a feasible technical path for the efficient compression of large-scale
language models. It also demonstrates the effectiveness of multi-teacher
collaborative mechanisms in complex language modeling tasks.

</details>


### [52] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
*Shayan Vassef,Amirhossein Dabiriaghdam,Mohammadreza Bakhtiari,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 研究了多任务、多语言和多源学习对预训练语言模型性能的影响，提出SOI框架分析学习行为，实验表明多源学习显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索多任务、多语言和多源学习对语言模型鲁棒性和性能的影响，并开发新的分析工具。

Method: 引入SOI框架分类学习行为，通过热图和可视化分析数据动态，进行多任务、多源和多语言实验。

Result: 多源学习提升性能7%，多任务学习在相似任务中表现良好，SOI选择进一步优化性能。

Conclusion: 多源学习效果显著，SOI框架为优化多设置语言模型提供了新方法。

Abstract: This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

</details>


### [53] [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)
*Yuanhe Tian,Junjie Liu,Zhizhou Kou,Yuxiang Li,Yan Song*

Main category: cs.CL

TL;DR: ChiMed 2.0是一个扩展的中文医学数据集，覆盖传统中医和现代医学数据，支持预训练、监督微调和RLHF，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有中文医学数据集规模小、领域窄，无法满足预训练需求，且不支持RLHF。

Method: 提出ChiMed 2.0数据集，包含预训练、SFT和RLHF数据，并在通用LLM上进行实验验证。

Result: 实验结果显示不同规模模型性能提升，验证了数据集的有效性。

Conclusion: ChiMed 2.0为中文医学LLM训练提供了高质量数据资源。

Abstract: Building high-quality data resources is crucial for advancing artificial
intelligence research and applications in specific domains, particularly in the
Chinese medical domain. Existing Chinese medical datasets are limited in size
and narrow in domain coverage, falling short of the diverse corpora required
for effective pre-training. Moreover, most datasets are designed solely for LLM
fine-tuning and do not support pre-training and reinforcement learning from
human feedback (RLHF). In this paper, we propose a Chinese medical dataset
named ChiMed 2.0, which extends our previous work ChiMed, and covers data
collected from Chinese medical online platforms and generated by LLMs. ChiMed
2.0 contains 204.4M Chinese characters covering both traditional Chinese
medicine classics and modern general medical data, where there are 164.8K
documents for pre-training, 351.6K question-answering pairs for supervised
fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the
effectiveness of our approach for training a Chinese medical LLM, we conduct
further pre-training, SFT, and RLHF experiments on representative general
domain LLMs and evaluate their performance on medical benchmark datasets. The
results show performance gains across different model scales, validating the
dataset's effectiveness and applicability.

</details>


### [54] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 提出了一种名为DPSE的双阶段自进化框架，通过联合优化用户偏好适应和领域特定能力，提升LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练策略（如基于记忆的检索或偏好优化）虽能改善用户对齐，但未能增强模型的领域认知能力。

Method: DPSE框架包含Censor模块提取多维交互信号和满意度评分，通过主题感知和偏好驱动策略扩展结构化数据，支持两阶段微调流程。

Result: 实验表明，DPSE在通用NLP基准和长期对话任务中优于监督微调、偏好优化和记忆增强基线。

Conclusion: DPSE为LLM的持续自进化提供了一条自主路径。

Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

</details>


### [55] [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)
*Navid Ayoobi,Sadat Shahriar,Arjun Mukherjee*

Main category: cs.CL

TL;DR: 提出了一种新的AI文本检测器评估范式SHIELD，强调实际应用中的公平性和稳定性，并开发了一种模型无关的“人类化”框架以提升检测挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法过于依赖传统指标（如AUROC），忽略了实际部署中的关键问题（如误报率和稳定性），SHIELD旨在填补这一空白。

Method: SHIELD整合了可靠性和稳定性因素，提出统一评估指标，并开发了可调节难度的“人类化”框架。

Result: SHIELD能有效挑战当前零样本检测方法的性能，同时提升检测器的实际适用性。

Conclusion: SHIELD为AI文本检测器的实际部署提供了更全面的评估标准，并展示了其对抗性场景下的优势。

Abstract: We present a novel evaluation paradigm for AI text detectors that prioritizes
real-world and equitable assessment. Current approaches predominantly report
conventional metrics like AUROC, overlooking that even modest false positive
rates constitute a critical impediment to practical deployment of detection
systems. Furthermore, real-world deployment necessitates predetermined
threshold configuration, making detector stability (i.e. the maintenance of
consistent performance across diverse domains and adversarial scenarios), a
critical factor. These aspects have been largely ignored in previous research
and benchmarks. Our benchmark, SHIELD, addresses these limitations by
integrating both reliability and stability factors into a unified evaluation
metric designed for practical assessment. Furthermore, we develop a post-hoc,
model-agnostic humanification framework that modifies AI text to more closely
resemble human authorship, incorporating a controllable hardness parameter.
This hardness-aware approach effectively challenges current SOTA zero-shot
detection methods in maintaining both reliability and stability. (Data and
code: https://github.com/navid-aub/SHIELD-Benchmark)

</details>


### [56] [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)
*Thilo Hagendorff*

Main category: cs.CL

TL;DR: 论文探讨了AI对齐原则与LLMs左翼政治偏见的冲突，认为对齐目标与进步道德框架一致，而右翼意识形态与之冲突。


<details>
  <summary>Details</summary>
Motivation: 研究动机是调和AI对齐原则与LLMs左翼政治偏见的矛盾，揭示对齐目标与进步价值观的内在一致性。

Method: 通过分析对齐目标的规范性假设与政治意识形态的关系，论证左翼偏见是AI对齐的必然结果。

Result: 研究发现对齐目标与左翼原则一致，而右翼意识形态与之冲突，现有研究将左翼偏见视为问题实际上违背了HHH原则。

Conclusion: 结论指出，左翼政治偏见是AI对齐的自然结果，批评这种偏见的研究实际上违背了AI对齐的核心原则。

Abstract: The guiding principle of AI alignment is to train large language models
(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are
mounting concerns that LLMs exhibit a left-wing political bias. Yet, the
commitment to AI alignment cannot be harmonized with the latter critique. In
this article, I argue that intelligent systems that are trained to be harmless
and honest must necessarily exhibit left-wing political bias. Normative
assumptions underlying alignment objectives inherently concur with progressive
moral frameworks and left-wing principles, emphasizing harm avoidance,
inclusivity, fairness, and empirical truthfulness. Conversely, right-wing
ideologies often conflict with alignment guidelines. Yet, research on political
bias in LLMs is consistently framing its insights about left-leaning tendencies
as a risk, as problematic, or concerning. This way, researchers are actively
arguing against AI alignment, tacitly fostering the violation of HHH
principles.

</details>


### [57] [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
*Narun Raman,Taylor Lundy,Kevin Leyton-Brown*

Main category: cs.CL

TL;DR: 论文研究了多选问答（MCQA）是否仍能有效评估先进推理模型的性能，发现MCQA在某些情况下仍是下游任务的良好代理，但需注意模型在选项后推理的能力可能带来偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨MCQA作为评估大型语言模型（LLM）性能的代理是否仍然有效，尤其是在先进推理模型出现后。

Method: 系统评估了15个问答基准和25个LLM，测试了5种提问方式，包括是否提供选项、是否允许链式推理等。

Result: MCQA在模型仅能在提供选项前进行链式推理时仍是良好代理；但模型在选项后推理时会显著优于自由文本表现。

Conclusion: MCQA不再是评估先进模型下游性能的良好代理，需设计更鲁棒、无偏的基准以反映真实推理能力。

Abstract: When evaluating Large Language Models (LLMs) in question-answering domains,
it is common to ask the model to choose among a fixed set of choices (so-called
multiple-choice question-answering, or MCQA). Although downstream tasks of
interest typically do not provide systems with explicit options among which to
choose, this approach is nevertheless widely used because it makes it makes
automatic grading straightforward and has tended to produce challenging
benchmarks that correlate sufficiently well with downstream performance. This
paper investigates the extent to which this trend continues to hold for
state-of-the-art reasoning models, describing a systematic evaluation of $15$
different question-answering benchmarks (e.g., MMLU, HLE) and $25$ different
LLMs (including small models such as Qwen 7B and relatively large models such
as Llama 70B). For each model-benchmark pair, we considered $5$ ways of
presenting the model with questions, including variations on whether multiple
choices were offered to the model at all; whether "none of the above" sometimes
replaced the right answer; and whether the model was permitted to perform
chain-of-thought reasoning before and/or after the choices were presented. MCQA
remained a good proxy for the downstream performance of models as long as they
were allowed to perform chain-of-thought reasoning only before being presented
with the options among which they had to select. On the other hand, large
models that were able to perform reasoning after being given a set of options
tended to significantly outperform their free-text performance due to
exploiting the information in the options. We conclude that MCQA is no longer a
good proxy for assessing downstream performance of state-of-the-art models, and
offer practical guidelines for designing more robust, bias-resistant benchmarks
that better reflect LLMs' genuine reasoning capabilities.

</details>


### [58] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
*Leanne Tan,Gabriel Chua,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: LionGuard 2是一个轻量级多语言内容审核分类器，针对新加坡语境设计，支持英语、中文、马来语和部分泰米尔语，性能优于多个商业和开源系统。


<details>
  <summary>Details</summary>
Motivation: 现代审核系统多语言支持不足，尤其是本地化和低资源语言变体，导致实际部署中的安全漏洞。小型模型可作为大型LLM的替代方案，但仍需大量数据和计算资源。

Method: 基于预训练的OpenAI嵌入和多头序数分类器构建LionGuard 2。

Result: 在17个基准测试中表现优异，包括新加坡特定和公共英语数据集，并已在新加坡政府中实际部署。

Conclusion: 高质量本地数据和强大多语言嵌入可实现强审核性能，无需微调大型模型。模型权重和部分训练数据已开源。

Abstract: Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.

</details>


### [59] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
*Amedeo Buonanno,Alessandro Rivetti,Francesco A. N. Palmieri,Giovanni Di Gennaro,Gianmarco Romano*

Main category: cs.CL

TL;DR: 通过熵分析研究Transformer架构中的信息分布，揭示模型行为和内部表示。


<details>
  <summary>Details</summary>
Motivation: 探索信息在Transformer模型中的管理和转换方式，以提升模型的可解释性和评估框架。

Method: 量化token级不确定性并分析处理阶段的熵模式，以GPT模型为例进行研究。

Result: 揭示了模型行为和内部表示的新见解。

Conclusion: 熵分析为Transformer模型的可解释性和评估提供了潜在工具。

Abstract: This work explores entropy analysis as a tool for probing information
distribution within Transformer-based architectures. By quantifying token-level
uncertainty and examining entropy patterns across different stages of
processing, we aim to investigate how information is managed and transformed
within these models. As a case study, we apply the methodology to a GPT-based
large language model, illustrating its potential to reveal insights into model
behavior and internal representations. This approach may offer insights into
model behavior and contribute to the development of interpretability and
evaluation frameworks for transformer-based models

</details>


### [60] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
*Elisa Sanchez-Bayona,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文全面评估了大语言模型（LLMs）在隐喻解释中的能力，发现其表现更多受词汇重叠和句子长度等表面特征影响，而非隐喻内容。


<details>
  <summary>Details</summary>
Motivation: 弥补以往研究在隐喻处理中仅限于单数据集和特定任务的不足，提供更全面的评估。

Method: 使用多样化的公开数据集，在自然语言推理（NLI）和问答（QA）任务中进行实验。

Result: LLMs的表现主要受表面特征影响，而非隐喻理解能力，其所谓的隐喻理解能力是多种因素的组合。

Conclusion: 揭示了LLMs在处理比喻语言时的局限性，强调需要更现实的评估框架。

Abstract: This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (LLMs) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that LLMs' performance is more influenced by features like
lexical overlap and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of LLMs to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of LLMs in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.

</details>


### [61] [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: 论文提出Stitch方法，通过在生成语音响应时交替生成未说出的推理块，实现同时思考和说话，显著降低延迟并提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前口语模型缺乏内部思考能力，而人类通常通过内部复杂推理清晰表达。整合未说出的思考过程对提升模型性能至关重要。

Method: 提出Stitch方法，交替生成未说出的推理块和语音响应块，利用语音播放时的空闲时间生成推理内容。

Result: Stitch在数学推理数据集上性能提升15%，同时保持与非推理数据集上的基线模型相当的表现。

Conclusion: Stitch成功实现了同时思考和说话，显著提升了口语模型的推理能力且不增加延迟。

Abstract: Spoken Language Models (SLMs) are designed to take speech inputs and produce
spoken responses. However, current SLMs lack the ability to perform an
internal, unspoken thinking process before responding. In contrast, humans
typically engage in complex mental reasoning internally, enabling them to
communicate ideas clearly and concisely. Thus, integrating an unspoken thought
process into SLMs is highly desirable. While naively generating a complete
chain-of-thought (CoT) reasoning before starting to talk can enable thinking
for SLMs, this induces additional latency for the speech response, as the CoT
reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a
novel generation method that alternates between the generation of unspoken
reasoning chunks and spoken response chunks. Since the audio duration of a
chunk of spoken response is much longer than the time to generate the tokens in
a chunk of spoken response, we use the remaining free time to generate the
unspoken reasoning tokens. When a chunk of audio is played to the user, the
model continues to generate the next unspoken reasoning chunk, achieving
simultaneous thinking and talking. Remarkably, Stitch matches the latency of
baselines that cannot generate unspoken CoT by design while outperforming those
baselines by 15% on math reasoning datasets; Stitch also performs equally well
on non-reasoning datasets as those baseline models. Some animations and
demonstrations are on the project page: https://d223302.github.io/STITCH.

</details>


### [62] [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)
*Jierui Li,Raymond Mooney*

Main category: cs.CL

TL;DR: 论文介绍了AlgoSimBench，一个评估LLMs识别算法相似问题能力的基准，发现LLMs在此任务上表现不佳，并提出了一种新方法ASM以提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在训练数据中较少见的领域中识别算法相似问题的能力，填补研究空白。

Method: 提出AlgoSimBench基准，包含1317个问题和402个多选题，并开发了ASM方法改进问题相似性检测。

Result: 最佳模型在MCQ任务上仅65.9%准确率，ASM方法提升了6.7%-11.7%的性能。

Conclusion: ASM方法有效提升了LLMs识别算法相似问题的能力，但仍有改进空间。

Abstract: Recent progress in LLMs, such as reasoning models, has demonstrated strong
abilities to solve complex competitive programming problems, often rivaling top
human competitors. However, it remains underexplored whether these abilities
generalize to relevant domains that are less seen during training. To address
this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'
ability to identify algorithmically similar problems (ASPs)-problems that can
be solved using similar algorithmic approaches. AlgoSimBench consists of 1317
problems, annotated with 231 distinct fine-grained algorithm tags, from which
we curate 402 multiple-choice questions (MCQs), where each question presents
one algorithmically similar problem alongside three textually similar but
algorithmically dissimilar distractors. Our evaluation reveals that LLMs
struggle to identify ASPs, with the best-performing model (o3-mini) achieving
only 65.9% accuracy on the MCQ task. To address this challenge, we propose
attempted solution matching (ASM), a novel method for improving problem
similarity detection. On our MCQ task, ASM yields an absolute accuracy
improvement of 6.7% to 11.7% across different models. We also evaluated code
embedding models and retrieval methods on similar problem identification. While
the adversarial selection of problems degrades the performance to be less than
random, we found that simply summarizing the problem to remove narrative
elements eliminates the effect, and combining ASM with a keyword-prioritized
method, BM25, can yield up to 52.2% accuracy. Code and data are available at
github.com

</details>


### [63] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
*Alexandru Coca,Mark Gaynor,Zhenxing Zhang,Jianpeng Cheng,Bo-Hsiang Tseng,Pete Boothroyd,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: 评估大型语言模型（LLMs）在驱动复杂动作执行的数字助手方面的潜力，提出ASPERA框架和Asper-Bench数据集。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何利用预训练编程知识执行多步骤目标，解决数据可用性和评估鲁棒性问题。

Method: 开发ASPERA框架，包括助手库模拟和人工辅助的LLM数据生成引擎，生成高质量任务。

Result: LLMs在基于自定义助手库的程序生成上表现显著低于无依赖的代码生成。

Conclusion: ASPERA框架和Asper-Bench数据集为LLMs在复杂任务执行中的挑战提供了新见解。

Abstract: This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

</details>


### [64] [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
*Kaiyan Chang,Yonghao Shi,Chenglong Wang,Hang Zhou,Chi Hu,Xiaoqian Liu,Yingfeng Luo,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 论文提出了一种无需训练的测试时扩展方法（TTS），通过条件步骤级自优化和并行扩展方法结合，显著提升了大型语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于训练的TTS方法增加了计算负担，而无训练的TTS方法逐渐式微。本文旨在探索无训练的TTS方法在推理任务中的潜力。

Method: 设计了条件步骤级自优化方法，并结合并行扩展方法，提出混合测试时扩展（Hybrid TTS）新范式。

Result: 在多个不同规模和家族的指令调优LLM上实验表明，混合策略能显著扩展模型的推理性能边界。

Conclusion: 无训练的混合TTS方法在提升LLM推理性能方面具有巨大潜力。

Abstract: Test-Time Scaling (TTS) is a promising approach to progressively elicit the
model's intelligence during inference. Recently, training-based TTS methods,
such as continued reinforcement learning (RL), have further surged in
popularity, while training-free TTS methods are gradually fading from
prominence. However, the additional computation overhead of training amplifies
the burden on test-time scaling. In this paper, we focus on training-free TTS
methods for reasoning. We first design Conditional Step-level Self-refinement,
a fine-grained sequential scaling method guided by process verification. On top
of its effectiveness, we further combine it with other classical parallel
scaling methods at the step level, to introduce a novel inference paradigm
called Hybrid Test-Time Scaling. Extensive experiments on five
instruction-tuned LLMs across different scales (3B-14B) and families
demonstrate that hybrid strategy incorporating various training-free TTS
methods at a fine granularity has considerable potential for expanding the
reasoning performance boundaries of LLMs.

</details>


### [65] [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)
*Vitaly Protasov,Nikolay Babakov,Daryna Dementieva,Alexander Panchenko*

Main category: cs.CL

TL;DR: 论文探讨了多语言文本去毒化评估的挑战，比较了自动指标与人类判断的差异，并提出了更可靠的评估方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）有所进展，但文本生成任务（如文本风格转换）的评估仍具挑战性，且多语言研究不足。

Method: 研究首次对九种语言的文本去毒化系统进行综合评估，结合神经模型和基于提示的LLM评估方法。

Result: 研究发现自动指标与人类判断存在显著差距，并提出了更可靠的多语言评估方案。

Conclusion: 研究为设计更可靠的多语言文本风格转换评估流程提供了实用建议。

Abstract: Despite recent progress in large language models (LLMs), evaluation of text
generation tasks such as text style transfer (TST) remains a significant
challenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)
revealed a substantial gap between automatic metrics and human judgments.
Moreover, most prior work focuses exclusively on English, leaving multilingual
TST evaluation largely unexplored. In this paper, we perform the first
comprehensive multilingual study on evaluation of text detoxification system
across nine languages: English, Spanish, German, Chinese, Arabic, Hindi,
Ukrainian, Russian, Amharic. Drawing inspiration from the machine translation,
we assess the effectiveness of modern neural-based evaluation models alongside
prompting-based LLM-as-a-judge approaches. Our findings provide a practical
recipe for designing more reliable multilingual TST evaluation pipeline in the
text detoxification case.

</details>


### [66] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
*Nicolas Poggi,Shashank Agnihotri,Margret Keuper*

Main category: cs.CL

TL;DR: 论文提出了一种基于上下文学习（ICL）和视觉语言模型（VLM）的太赫兹（THz）图像分类方法，无需微调即可在低数据情况下提升分类效果和可解释性。


<details>
  <summary>Details</summary>
Motivation: 太赫兹成像在安全筛查和材料分类等应用中具有非侵入性优势，但图像分类面临标注数据少、分辨率低和视觉模糊等挑战。

Method: 采用模态对齐的提示框架，将两种开源的VLM适配到THz领域，并在零样本和单样本设置下评估。

Result: 实验结果表明，ICL在低数据情况下显著提升了分类性能和可解释性。

Conclusion: 这是首次将ICL增强的VLM应用于THz成像，为资源受限的科学领域提供了新方向。

Abstract: Terahertz (THz) imaging enables non-invasive analysis for applications such
as security screening and material classification, but effective image
classification remains challenging due to limited annotations, low resolution,
and visual ambiguity. We introduce In-Context Learning (ICL) with
Vision-Language Models (VLMs) as a flexible, interpretable alternative that
requires no fine-tuning. Using a modality-aligned prompting framework, we adapt
two open-weight VLMs to the THz domain and evaluate them under zero-shot and
one-shot settings. Our results show that ICL improves classification and
interpretability in low-data regimes. This is the first application of
ICL-enhanced VLMs to THz imaging, offering a promising direction for
resource-constrained scientific domains. Code:
\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub
repository}.

</details>


### [67] [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)
*Xinping Zhao,Shouzheng Huang,Yan Zhong,Xinshuo Hu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: LEAR通过显式推理和提取机制减少检索噪声，提升LLMs生成质量。


<details>
  <summary>Details</summary>
Motivation: 检索噪声显著影响LLMs生成质量，现有方法缺乏显式推理，易遗漏关键线索且泛化能力差。

Method: LEAR结合推理和提取机制，通过端到端训练、知识标记掩码和奖励函数优化模型。

Result: 在三个基准数据集上验证了LEAR的有效性，提供紧凑高质量证据，提升下游任务准确性。

Conclusion: LEAR显著改善RAG系统性能，适用于在线应用。

Abstract: Retrieval-Augmented Generation (RAG) effectively improves the accuracy of
Large Language Models (LLMs). However, retrieval noises significantly impact
the quality of LLMs' generation, necessitating the development of denoising
mechanisms. Previous methods extract evidence straightforwardly without
explicit thinking, which risks filtering out key clues and struggles with
generalization. To this end, we propose LEAR, which learns to extract rational
evidence by (1) explicitly reasoning to identify potential cues within
retrieval contents first, and then (2) consciously extracting to avoid omitting
any key cues helpful for answering questions. Specifically, we frame evidence
reasoning and evidence extraction into one unified response for end-to-end
training; apply knowledge token masks for disentanglement to derive
reasoning-based and extraction-based answers; and devise three types of
verifiable reward functions, including answer, length, and format, to update
the model via the policy optimization algorithm. Extensive experiments on three
benchmark datasets show the effectiveness of LEAR, providing compact and
high-quality evidence, improving the accuracy of downstream tasks, and
promoting effective application in online RAG systems.

</details>


### [68] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
*Armin Pournaki*

Main category: cs.CL

TL;DR: 该研究通过分析德国Twitter上2021-2023年间关于乌克兰战争、新冠疫情和气候变化的对立叙事，揭示了政治极化的话语机制。


<details>
  <summary>Details</summary>
Motivation: 探讨叙事作为理解政治现实的工具，以及对立叙事如何反映和加剧公共领域中的极化现象。

Method: 从对立观点群体的推文中提取文本信号，分析冲突叙事的两个维度：角色分配差异和事件情节差异。

Result: 发现对立叙事在角色分配和事件情节上存在显著差异，并首次揭示了叙事对齐的策略。

Conclusion: 叙事分析可作为研究政治极化话语机制的有效工具。

Abstract: Narratives are key interpretative devices by which humans make sense of
political reality. In this work, we show how the analysis of conflicting
narratives, i.e. conflicting interpretive lenses through which political
reality is experienced and told, provides insight into the discursive
mechanisms of polarization and issue alignment in the public sphere. Building
upon previous work that has identified ideologically polarized issues in the
German Twittersphere between 2021 and 2023, we analyze the discursive dimension
of polarization by extracting textual signals of conflicting narratives from
tweets of opposing opinion groups. Focusing on a selection of salient issues
and events (the war in Ukraine, Covid, climate change), we show evidence for
conflicting narratives along two dimensions: (i) different attributions of
actantial roles to the same set of actants (e.g. diverging interpretations of
the role of NATO in the war in Ukraine), and (ii) emplotment of different
actants for the same event (e.g. Bill Gates in the right-leaning Covid
narrative). Furthermore, we provide first evidence for patterns of narrative
alignment, a discursive strategy that political actors employ to align opinions
across issues. These findings demonstrate the use of narratives as an
analytical lens into the discursive mechanisms of polarization.

</details>


### [69] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
*Alessio Pittiglio*

Main category: cs.CL

TL;DR: 论文介绍了在MM-ArgFallacy2025共享任务中的提交，专注于政治辩论中的多模态逻辑谬误挖掘，使用预训练的Transformer模型并结合上下文信息。


<details>
  <summary>Details</summary>
Motivation: 推动多模态论证挖掘研究，特别是政治辩论中的逻辑谬误识别。

Method: 采用预训练的Transformer模型，并提出了多种利用上下文的方法。

Result: 在谬误分类子任务中，文本、音频和多模态模型的宏F1分数分别为0.4444、0.3559和0.4403。多模态模型表现接近文本模型，显示改进潜力。

Conclusion: 多模态模型在逻辑谬误分类中表现接近文本模型，未来有改进空间。

Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

</details>


### [70] [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)
*Xinyu Zhang,Yuanquan Hu,Fangchao Liu,Zhicheng Dou*

Main category: cs.CL

TL;DR: P3框架通过同时优化系统提示和用户提示，提升大语言模型性能，并在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅优化系统或用户提示，效果有限，因两者相互依赖。

Method: P3框架通过迭代过程同时优化系统提示和用户提示，并利用离线优化促进在线提示优化。

Result: 在通用任务和推理任务中，P3表现优于现有自动提示优化方法。

Conclusion: 整体优化策略能有效提升大语言模型在多样领域的性能。

Abstract: Current large language model (LLM) applications often employ multi-component
prompts, comprising both system and user prompts, to guide model behaviors.
While recent advancements have demonstrated the efficacy of automatically
optimizing either the system or user prompt to boost performance, such
unilateral approaches often yield suboptimal outcomes due to the interdependent
nature of these components. In this work, we introduce P3, a novel
self-improvement framework that concurrently optimizes both system and user
prompts through an iterative process. The offline optimized prompts are further
leveraged to promote online prompting by performing query-dependent prompt
optimization. Extensive experiments on general tasks (e.g., Arena-hard and
Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3
achieves superior performance in the realm of automatic prompt optimization.
Our results highlight the effectiveness of a holistic optimization strategy in
enhancing LLM performance across diverse domains.

</details>


### [71] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
*Congmin Zheng,Jiachen Zhu,Jianghao Lin,Xinyi Dai,Yong Yu,Weinan Zhang,Mengyue Yang*

Main category: cs.CL

TL;DR: 论文提出CoLD框架，通过长度惩罚、学习偏差估计器和联合训练策略，解决PRMs中的长度偏差问题，提升推理的准确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有PRMs存在长度偏差，倾向于给更长的推理步骤更高评分，影响奖励预测的可靠性和推理输出的简洁性。

Method: 提出CoLD框架，包括显式长度惩罚调整、学习偏差估计器和联合训练策略，基于反事实推理和因果图分析。

Result: 在MATH500和GSM-Plus上的实验表明，CoLD显著减少奖励与长度的相关性，提升步骤选择的准确性和推理的简洁性。

Conclusion: CoLD有效提升了PRMs的可靠性和鲁棒性，为多步推理提供了更准确的评估和指导。

Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [72] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
*David Peter Wallis Freeborn*

Main category: cs.CL

TL;DR: 论文提出两种新模型，解决标准信号博弈中接收者难以学习组合信息的问题。


<details>
  <summary>Details</summary>
Motivation: 标准信号博弈模型中，接收者难以理解组合信息，即使信号发送者发送组合消息，接收者也无法组合解读。信息丢失时，其他组件信息也会被遗忘。

Method: 构建两种新模型：简约型接收者（仅从信号的原子消息学习）和通用型接收者（从所有可用信息学习）。

Result: 新模型比以往更简单，且能让接收者从消息的原子组件中学习。

Conclusion: 新模型实现了真正的组合理解，解决了标准模型的局限性。

Abstract: Receivers in standard signaling game models struggle with learning
compositional information. Even when the signalers send compositional messages,
the receivers do not interpret them compositionally. When information from one
message component is lost or forgotten, the information from other components
is also erased. In this paper I construct signaling game models in which
genuine compositional understanding evolves. I present two new models: a
minimalist receiver who only learns from the atomic messages of a signal, and a
generalist receiver who learns from all of the available information. These
models are in many ways simpler than previous alternatives, and allow the
receivers to learn from the atomic components of messages.

</details>


### [73] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
*Seok Hwan Song,Mohna Chakraborty,Qi Li,Wallapak Tavanapong*

Main category: cs.CL

TL;DR: 研究探讨了不同问题类型对大型语言模型（LLM）在推理任务中准确性的影响，发现性能差异显著且推理准确性与最终答案选择准确性不一定相关。


<details>
  <summary>Details</summary>
Motivation: 探索不同问题类型对LLM在推理任务中准确性的影响，填补现有研究的空白。

Method: 评估五种LLM在三种不同问题类型（定量和演绎推理任务）上的表现，分析推理步骤准确性和最终答案选择准确性。

Result: 发现LLM在不同问题类型上表现差异显著，推理准确性与最终选择准确性不相关，选项数量和措辞影响性能。

Conclusion: 问题类型对LLM推理性能有显著影响，需在设计评估任务时考虑问题类型和措辞。

Abstract: Large Language Models (LLMs) have been evaluated using diverse question
types, e.g., multiple-choice, true/false, and short/long answers. This study
answers an unexplored question about the impact of different question types on
LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on
three different types of questions using quantitative and deductive reasoning
tasks. The performance metrics include accuracy in the reasoning steps and
choosing the final answer. Key Findings: (1) Significant differences exist in
LLM performance across different question types. (2) Reasoning accuracy does
not necessarily correlate with the final selection accuracy. (3) The number of
options and the choice of words, influence LLM performance.

</details>


### [74] [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)
*Tian Li,Yujian Sun,Huizhi Liang*

Main category: cs.CL

TL;DR: SemEval-2025 Task 11提出了一项跨28种语言的情感检测挑战，包含多标签分类和情感强度预测两个赛道。研究采用对比学习方法，样本对比和生成对比，基于LLaMa3-Instruct-8B微调，取得了较好的排名。


<details>
  <summary>Details</summary>
Motivation: 解决情感表达多样性和背景差异带来的挑战，推动更先进的情感检测方法。

Method: 使用两种对比学习方法：样本对比（Contrastive Reasoning Calibration）和生成对比（DPO, SimPO），基于LLaMa3-Instruct-8B微调。

Result: 英语赛道A排名第9，赛道B排名第6；其他语言表现优异。

Conclusion: 对比学习方法在多语言情感检测任务中表现良好，验证了其有效性。

Abstract: The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,
introduces an emotion recognition challenge spanning over 28 languages. This
competition encourages researchers to explore more advanced approaches to
address the challenges posed by the diversity of emotional expressions and
background variations. It features two tracks: multi-label classification
(Track A) and emotion intensity prediction (Track B), covering six emotion
categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we
systematically explore the benefits of two contrastive learning approaches:
sample-based (Contrastive Reasoning Calibration) and generation-based (DPO,
SimPO) contrastive learning. The sample-based contrastive approach trains the
model by comparing two samples to generate more reliable predictions. The
generation-based contrastive approach trains the model to differentiate between
correct and incorrect generations, refining its prediction. All models are
fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A
and 6th place in Track B for English, while ranking among the top-tier
performing systems for other languages.

</details>


### [75] [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
*Alina Hyk,Kiera McCormick,Mian Zhong,Ioana Ciucă,Sanjib Sharma,John F Wu,J. E. G. Peek,Kartheik G. Iyer,Ziang Xiao,Anjalie Field*

Main category: cs.CL

TL;DR: 研究探讨了如何改进大型语言模型（LLM）的评估方法，通过分析天文学家使用LLM辅助检索生成工具的实际反馈，提出了改进基准测试的建议。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准未能跟上用户实际需求的多样性，特别是在科学领域中的应用。

Method: 通过Slack部署一个LLM驱动的检索增强生成机器人，收集368条查询数据并进行归纳编码，随后对11位天文学家进行访谈。

Result: 揭示了用户对LLM系统的评估标准和问题类型，并据此提出了改进基准测试的具体建议。

Conclusion: 研究为改进LLM评估提供了实用方法，尤其适用于科学研究领域。

Abstract: There is growing interest in leveraging LLMs to aid in astronomy and other
scientific research, but benchmarks for LLM evaluation in general have not kept
pace with the increasingly diverse ways that real people evaluate and use these
models. In this study, we seek to improve evaluation procedures by building an
understanding of how users evaluate LLMs. We focus on a particular use case: an
LLM-powered retrieval-augmented generation bot for engaging with astronomical
literature, which we deployed via Slack. Our inductive coding of 368 queries to
the bot over four weeks and our follow-up interviews with 11 astronomers reveal
how humans evaluated this system, including the types of questions asked and
the criteria for judging responses. We synthesize our findings into concrete
recommendations for building better benchmarks, which we then employ in
constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our
work offers ways to improve LLM evaluation and ultimately usability,
particularly for use in scientific research.

</details>


### [76] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
*Sahana Srinivasan,Xuguang Ai,Thaddaeus Wai Soon Lo,Aidan Gilson,Minjie Zou,Ke Zou,Hyunjae Kim,Mingjia Yang,Krithi Pushpanathan,Samantha Yew,Wan Ting Loke,Jocelyn Goh,Yibing Chen,Yiming Kong,Emily Yuelei Fu,Michelle Ongyong Hui,Kristen Nwanyanwu,Amisha Dave,Kelvin Zhenghao Li,Chen-Hsin Sun,Mark Chia,Gabriel Dawei Yang,Wendy Meihua Wong,David Ziyou Chen,Dianbo Liu,Maxwell Singer,Fares Antaki,Lucian V Del Priore,Jost Jonas,Ron Adelman,Qingyu Chen,Yih-Chung Tham*

Main category: cs.CL

TL;DR: BELO是一个标准化的眼科大语言模型评估基准，通过专家多轮检查开发，包含900个高质量问题，评估临床准确性和推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有眼科大语言模型评估基准范围有限且过于侧重准确性，BELO旨在提供更全面的评估标准。

Method: 通过关键词匹配和PubMedBERT模型从多个医学数据集中筛选眼科MCQs，并经专家多轮检查和优化。

Result: 评估了六种LLM，使用多种指标（如准确率、ROUGE-L等），并建立公开排行榜。

Conclusion: BELO将成为未来模型公平、可重复比较的评估基准。

Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology
are limited in scope and disproportionately prioritise accuracy. We introduce
BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive
evaluation benchmark developed through multiple rounds of expert checking by 13
ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and
reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we
curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse
medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset
underwent multiple rounds of expert checking. Duplicate and substandard
questions were systematically removed. Ten ophthalmologists refined the
explanations of each MCQ's correct answer. This was further adjudicated by
three senior ophthalmologists. To illustrate BELO's utility, we evaluated six
LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)
using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,
BARTScore, METEOR, and AlignScore). In a further evaluation involving human
experts, two ophthalmologists qualitatively reviewed 50 randomly selected
outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900
high-quality, expert-reviewed questions aggregated from five sources: BCSC
(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public
leaderboard has been established to promote transparent evaluation and
reporting. Importantly, the BELO dataset will remain a hold-out,
evaluation-only benchmark to ensure fair and reproducible comparisons of future
models.

</details>


### [77] [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)
*Yuanhao Shen,Daniel Xavier de Sousa,Ricardo Marçal,Ali Asad,Hongyu Guo,Xiaodan Zhu*

Main category: cs.CL

TL;DR: IDRBench是一个新基准，用于评估大型语言模型（LLMs）在跨学科研究（IDR）中提出有价值研究想法的能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估LLMs在IDR中能力的专用基准，限制了对其潜力的理解。

Method: 引入IDRBench，包含专家标注的数据集和任务，评估LLMs在IDR中的表现。数据集来自ArXiv，涵盖六大学科。

Result: 尽管LLMs表现出一定的IDR意识，但在生成高质量IDR想法方面仍有困难。

Conclusion: IDRBench为评估LLMs在跨学科研究中的能力提供了系统框架，并揭示了改进方向。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed their
impressive ability to perform multi-step, logic-driven reasoning across complex
domains, positioning them as powerful tools and collaborators in scientific
discovery while challenging the long-held view that inspiration-driven ideation
is uniquely human. However, the lack of a dedicated benchmark that evaluates
LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings
poses a critical barrier to fully understanding their strengths and
limitations. To address this gap, we introduce IDRBench -- a pioneering
benchmark featuring an expert annotated dataset and a suite of tasks tailored
to evaluate LLMs' capabilities in proposing valuable research ideas from
different scientific domains for interdisciplinary research. This benchmark
aims to provide a systematic framework for assessing LLM performance in
complex, cross-domain scientific research. Our dataset consists of scientific
publications sourced from the ArXiv platform covering six distinct disciplines,
and is annotated by domain experts with diverse academic backgrounds. To ensure
high-quality annotations, we emphasize clearly defined dimensions that
characterize authentic interdisciplinary research. The design of evaluation
tasks in IDRBench follows a progressive, real-world perspective, reflecting the
natural stages of interdisciplinary research development, including 1) IDR
Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.
Using IDRBench, we construct baselines across 10 LLMs and observe that despite
fostering some level of IDR awareness, LLMs still struggle to produce quality
IDR ideas. These findings could not only spark new research directions, but
also help to develop next-generation LLMs that excel in interdisciplinary
research.

</details>


### [78] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: 本文从显著性检验的角度解释了TF-IDF的合理性，并证明了TF-ICF变体与Fisher精确检验的负对数p值密切相关。


<details>
  <summary>Details</summary>
Motivation: 为TF-IDF这一信息检索中最著名的数学表达式提供统计学理论基础，向统计学界证明其有效性。

Method: 通过显著性测试视角分析TF-IDF，展示TF-ICF变体与Fisher精确检验的负对数p值的关系，并探讨其在理想假设下的联系。

Result: 证明了TF-ICF与Fisher精确检验的负对数p值密切相关，且在无限大文档集合的极限情况下收敛于TF-IDF。

Conclusion: TF-IDF的有效性可以通过Fisher精确检验的理论框架解释，为统计学家提供了理解其长期有效性的工具。

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [79] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
*Ruizhe Zhu,Hao Zhu,Yaxuan Li,Syang Zhou,Shijing Cai,Malgorzata Lazuka,Elliott Ash*

Main category: cs.CL

TL;DR: DialogueForge框架通过AI模拟生成人机对话，减少人工收集数据的成本，测试了多种LLM模型，发现大模型表现更优，小模型通过微调也能接近其性能。


<details>
  <summary>Details</summary>
Motivation: 减少人工收集人机对话数据的高成本和耗时，推动对话AI研究。

Method: 使用真实对话种子提示，测试不同规模LLM生成多轮对话，并探索小模型微调技术。

Result: 大模型（如GPT-4o）生成对话更真实，小模型（如Llama）通过微调也能表现良好。

Conclusion: 大模型在生成真实对话上表现最佳，小模型通过微调可提升性能，但长对话一致性仍是挑战。

Abstract: Collecting human-chatbot dialogues typically demands substantial manual
effort and is time-consuming, which limits and poses challenges for research on
conversational AI. In this work, we propose DialogueForge - a framework for
generating AI-simulated conversations in human-chatbot style. To initialize
each generated conversation, DialogueForge uses seed prompts extracted from
real human-chatbot interactions. We test a variety of LLMs to simulate the
human chatbot user, ranging from state-of-the-art proprietary models to
small-scale open-source LLMs, and generate multi-turn dialogues tailored to
specific tasks. In addition, we explore fine-tuning techniques to enhance the
ability of smaller models to produce indistinguishable human-like dialogues. We
evaluate the quality of the simulated conversations and compare different
models using the UniEval and GTEval evaluation protocols. Our experiments show
that large proprietary models (e.g., GPT-4o) generally outperform others in
generating more realistic dialogues, while smaller open-source models (e.g.,
Llama, Mistral) offer promising performance with greater customization. We
demonstrate that the performance of smaller models can be significantly
improved by employing supervised fine-tuning techniques. Nevertheless,
maintaining coherent and natural long-form human-like dialogues remains a
common challenge across all models.

</details>


### [80] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
*Lyumanshan Ye,Xiaojie Cai,Xinkai Wang,Junfei Wang,Xiangkun Hu,Jiadi Su,Yang Nan,Sihan Wang,Bohan Zhang,Xiaoze Fan,Jinbin Luo,Yuxiang Zheng,Tianze Xu,Dayuan Fu,Yunze Wu,Pengrui Lu,Zengzhi Wang,Yiwei Qin,Zhen Huang,Yan Ma,Zhulin Hu,Haoyang Zou,Tiantian Mi,Yixin Ye,Ethan Chern,Pengfei Liu*

Main category: cs.CL

TL;DR: 论文提出“交互即智能”概念，将人机交互视为智能的核心维度，而非传统界面。通过Deep Cognition系统，实现透明、可控的交互，显著提升研究任务效果。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统采用“输入-等待-输出”模式，导致错误累积、研究边界僵化及专家知识整合不足。论文旨在通过交互优化智能系统的研究能力。

Method: 提出Deep Cognition系统，包含透明可控的交互、细粒度双向对话和共享认知上下文三大创新，支持用户在关键节点干预AI推理。

Result: 用户评估显示，该系统在透明度、细粒度交互等六项指标上显著优于基线，研究任务效果提升31.8%至50.0%。

Conclusion: 交互是智能的核心维度，Deep Cognition通过认知监督模式显著提升人机协作的研究效能。

Abstract: This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

</details>


### [81] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
*Andrei-Valentin Tanase,Elena Pelican*

Main category: cs.CL

TL;DR: Supernova是一个650M参数的解码器专用Transformer，通过架构设计和分词创新，实现了与更大模型相当的性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 挑战当前主流的扩展范式，证明架构效率和分词质量可以弥补参数数量的减少。

Method: 结合RoPE、GQA（3:1压缩比）、RMSNorm和SwiGLU激活函数，并采用自定义的128,000词汇字节级BPE分词器。

Result: Supernova性能达到1B参数模型的90%，参数减少53%，训练令牌仅需100B。

Conclusion: 架构效率和分词创新可以显著减少模型规模和训练数据需求，同时保持高性能。

Abstract: We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

</details>


### [82] [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)
*Jiakang Wang,Runze Liu,Fuzheng Zhang,Xiu Li,Guorui Zhou*

Main category: cs.CL

TL;DR: Archer提出了一种基于熵感知的双令牌约束RLVR方法，通过同步更新和差异化约束提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法对所有令牌采用统一训练信号，忽略了知识令牌和推理令牌的不同作用，可能导致语义依赖破坏。

Method: 采用双令牌约束和同步更新，对推理令牌应用较弱KL正则化和较高剪裁阈值以鼓励探索，对知识令牌施加更强约束以保持事实知识。

Result: 在数学推理和代码生成基准测试中显著优于现有RLVR方法，达到或超越同类模型的最先进性能。

Conclusion: Archer通过熵感知和差异化约束有效提升了LLM的推理能力，同时保持了知识完整性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective
post-training method for improving the reasoning abilities of Large Language
Models (LLMs), mainly by shaping higher-order behaviors such as reflection and
planning. However, previous RLVR algorithms often apply uniform training
signals to all tokens, without considering the different roles of low-entropy
knowledge-related tokens and high-entropy reasoning-related tokens. Some recent
methods try to separate these token types by gradient masking or asynchronous
updates, but these approaches may break semantic dependencies in the model
output and hinder effective learning. In this work, we propose Archer, an
entropy-aware RLVR approach with dual-token constraints and synchronous
updates. Specifically, our method applies weaker KL regularization and higher
clipping thresholds to reasoning tokens to encourage exploration, while using
stronger constraints on knowledge tokens to maintain factual knowledge.
Experimental results on several mathematical reasoning and code generation
benchmarks show that our approach significantly outperforms previous RLVR
methods, reaching or exceeding state-of-the-art performance among models of
comparable size. The code is available at
https://github.com/wizard-III/ArcherCodeR.

</details>


### [83] [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)
*Felix Köster,Atsushi Uchida*

Main category: cs.CL

TL;DR: 本文比较了三种字符级语言建模方法，包括两种储层计算方法和基于Transformer的架构，发现Transformer在预测质量上表现优异，而储层计算在训练和推理速度上更高效。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）的高能耗和慢处理速度问题，探索储层计算在自然文本处理中的潜力。

Method: 比较两种储层计算方法和Transformer架构，通过调整可训练参数数量，评估性能、计算成本和预测准确性。

Result: Transformer在预测质量上表现最佳，储层计算在速度和效率上更优。

Conclusion: 储层计算在资源受限场景下具有潜力，而Transformer在性能上更胜一筹，为平衡资源与性能提供了指导。

Abstract: Large Language Models (LLM) have dominated the science and media landscape
duo to their impressive performance on processing large chunks of data and
produce human-like levels of text. Nevertheless, their huge energy demand and
slow processing still a bottleneck for further increasing quality while also
making the models accessible to everyone. To solve this bottleneck, we will
investigate how reservoir computing performs on natural text processing, which
could enable fast and energy efficient hardware implementations. Studies
investigating the use of reservoir computing as a language model remain sparse.
In this paper, we compare three distinct approaches for character-level
language modeling, two different reservoir computing approaches, where only an
output layer is trainable, and the well-known transformer-based architectures,
which fully learn an attention-based sequence representation. We explore the
performance, computational cost and prediction accuracy for both paradigms by
equally varying the number of trainable parameters for all models. Using a
consistent pipeline for all three approaches, we demonstrate that transformers
excel in prediction quality, whereas reservoir computers remain highly
efficient reducing the training and inference speed. Furthermore, we
investigate two types of reservoir computing: a traditional reservoir with a
static linear readout, and an attention-enhanced reservoir that dynamically
adapts its output weights via an attention mechanism. Our findings underline
how these paradigms scale and offer guidelines to balance resource constraints
with performance.

</details>


### [84] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
*Anton Abilov,Ke Zhang,Hemank Lamba,Elizabeth M. Olson,Joel R. Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: 论文探讨了AI在公益领域的实际部署与合作过程，填补了现有研究中关于实际应用和持续维护的空白。


<details>
  <summary>Details</summary>
Motivation: 现有AI公益研究多关注模型开发，而忽略了部署、合作及实际影响的讨论。

Method: 通过与H2H组织紧密合作，研究如何在资源受限环境中部署和维护AI模型。

Result: 分享了实际部署经验、持续性能更新的方法，并为从业者提供了关键建议。

Conclusion: 强调了合作与持续维护在AI公益项目中的重要性，为实践者提供了实用指导。

Abstract: Publications in the AI for Good space have tended to focus on the research
and model development that can support high-impact applications. However, very
few AI for Good papers discuss the process of deploying and collaborating with
the partner organization, and the resulting real-world impact. In this work, we
share details about the close collaboration with a humanitarian-to-humanitarian
(H2H) organization and how to not only deploy the AI model in a
resource-constrained environment, but also how to maintain it for continuous
performance updates, and share key takeaways for practitioners.

</details>


### [85] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
*Yihao Li,Jiayi Xin,Miranda Muqing Miao,Qi Long,Lyle Ungar*

Main category: cs.CL

TL;DR: 研究发现，中英双语推理模型中的语言混合行为（如交替使用语言）能提升推理能力，强制单语解码会降低准确性。强化学习阶段是导致语言混合的关键，且可通过轻量级探针预测语言切换的利弊。


<details>
  <summary>Details</summary>
Motivation: 探究语言混合行为在双语推理模型中的作用及其对推理能力的影响。

Method: 研究中英双语推理模型的语言混合现象，分析强化学习阶段的作用，并设计轻量级探针预测语言切换效果。

Result: 语言混合提升推理准确性（强制单语解码降低5.6%），探针指导解码可提升准确性达6.25%。

Conclusion: 语言混合是双语推理模型的策略性行为，而非训练副产品。

Abstract: Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [86] [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)
*Basma El Amel Boussaha,Leen AlQadi,Mugariya Farooq,Shaikha Alsuwaidi,Giulia Campesan,Ahmed Alzubaidi,Mohammed Alyafeai,Hakim Hacid*

Main category: cs.CL

TL;DR: 论文提出了3LM，一套针对阿拉伯语的三个基准测试，涵盖STEM和代码生成领域，填补了现有阿拉伯语基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语是全球使用最广泛的语言之一，但针对阿拉伯语的大语言模型（LLMs）开发和评估相对有限，尤其在STEM和代码生成领域缺乏相关基准测试。

Method: 设计了三个基准测试：1）自然来源的STEM问答对；2）合成的STEM问题；3）通过翻译和人工审核构建的代码生成测试。

Result: 发布了三个高质量的阿拉伯语基准测试，支持阿拉伯语LLM研究在STEM和代码生成领域的发展。

Conclusion: 3LM填补了阿拉伯语LLM在STEM和代码生成领域的空白，为相关研究提供了重要资源。

Abstract: Arabic is one of the most widely spoken languages in the world, yet efforts
to develop and evaluate Large Language Models (LLMs) for Arabic remain
relatively limited. Most existing Arabic benchmarks focus on linguistic,
cultural, or religious content, leaving a significant gap in domains like STEM
and code which are increasingly relevant for real-world LLM applications. To
help bridge this gap, we present 3LM, a suite of three benchmarks designed
specifically for Arabic. The first is a set of STEM-related question-answer
pairs, naturally sourced from Arabic textbooks and educational worksheets. The
second consists of synthetically generated STEM questions, created using the
same sources. The third benchmark focuses on code generation, built through a
careful translation of two widely used code benchmarks, incorporating a
human-in-the-loop process with several rounds of review to ensure high-quality
and faithful translations. We release all three benchmarks publicly to support
the growth of Arabic LLM research in these essential but underrepresented
areas.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [87] [Can AR-Embedded Visualizations Foster Appropriate Reliance on AI in Spatial Decision Making? A Comparative Study of AR See-Through vs. 2D Minimap](https://arxiv.org/abs/2507.14316)
*Xianhao Carton Liu,Difan Jia,Tongyu Nie,Evan Suma Rosenberg,Victoria Interrante,Chen Zhu-Tian*

Main category: cs.HC

TL;DR: AR嵌入式可视化在时间关键任务中增加了用户对AI的不当依赖，但提升了空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究AR嵌入式可视化是否能减少认知负荷并促进对AI建议的适当依赖。

Method: 通过实证研究（N=32）比较AR透视和2D小地图在AI辅助空间目标选择任务中的表现。

Result: AR条件下用户对AI的不当依赖更高，但空间推理能力有所提升。

Conclusion: AR嵌入式可视化虽存在不当依赖问题，但在空间推理方面有优势，需进一步研究设计优化。

Abstract: In high-stakes, time-critical scenarios-such as emergency evacuation, first
responder prioritization, and crisis management -- decision-makers must rapidly
choose among spatial targets, such as exits, individuals to assist, or areas to
secure. Advances in indoor sensing and artificial intelligence (AI) can support
these decisions by visualizing real-time situational data and AI suggestions on
2D maps. However, mentally mapping this information onto real-world spaces
imposes significant cognitive load. This load can impair users' ability to
appropriately judge AI suggestions, leading to inappropriate reliance (e.g.,
accepting wrong AI suggestions or rejecting correct ones). Embedded
visualizations in Augmented Reality (AR), by directly overlaying information
onto physical environments, may reduce this load and foster more deliberate,
appropriate reliance on AI. But is this true? In this work, we conducted an
empirical study (N = 32) comparing AR see-through (embedded visualization) and
2D Minimap in time-critical, AI-assisted spatial target selection tasks.
Contrary to our expectations, users exhibited greater inappropriate reliance on
AI in the AR condition. Our analysis further reveals that this is primarily due
to over-reliance, with factors specific to embedded visualizations, such as
perceptual challenges, visual proximity illusions, and highly realistic visual
representations. Nonetheless, embedded visualizations demonstrated notable
benefits in spatial reasoning, such as spatial mapping and egocentric spatial
imagery. We conclude by discussing the empirical insights, deriving design
implications, and outlining important directions for future research on
human-AI decision collaboration in AR.

</details>


### [88] [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)
*Angjelin Hila,Elliott Hauser*

Main category: cs.HC

TL;DR: 研究探讨了使用ChatGPT进行结构化演绎定性编码的潜力，测试了四种干预方法，发现逐步任务分解策略表现最佳，适合严格定性工作流程。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型（如ChatGPT）在演绎分类任务中的应用潜力，填补当前研究中对这一领域的不足。

Method: 使用CAP主代码本对最高法院案例进行分类，测试了零样本、少样本、基于定义和逐步任务分解四种策略。

Result: 逐步任务分解策略表现最佳（准确率0.775，kappa 0.744），干预策略显著影响分类行为。

Conclusion: 通过定制干预，大语言模型可达到适合严格定性编码的可靠性水平。

Abstract: In this study, we investigate the use of large language models (LLMs),
specifically ChatGPT, for structured deductive qualitative coding. While most
current research emphasizes inductive coding applications, we address the
underexplored potential of LLMs to perform deductive classification tasks
aligned with established human-coded schemes. Using the Comparative Agendas
Project (CAP) Master Codebook, we classified U.S. Supreme Court case summaries
into 21 major policy domains. We tested four intervention methods: zero-shot,
few-shot, definition-based, and a novel Step-by-Step Task Decomposition
strategy, across repeated samples. Performance was evaluated using standard
classification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's
alpha), and construct validity was assessed using chi-squared tests and
Cramer's V. Chi-squared and effect size analyses confirmed that intervention
strategies significantly influenced classification behavior, with Cramer's V
values ranging from 0.359 to 0.613, indicating moderate to strong shifts in
classification patterns. The Step-by-Step Task Decomposition strategy achieved
the strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),
achieving thresholds for substantial agreement. Despite the semantic ambiguity
within case summaries, ChatGPT displayed stable agreement across samples,
including high F1 scores in low-support subclasses. These findings demonstrate
that with targeted, custom-tailored interventions, LLMs can achieve reliability
levels suitable for integration into rigorous qualitative coding workflows.

</details>


### [89] [Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students](https://arxiv.org/abs/2507.14418)
*Taufiq Daryanto,Sophia Stil,Xiaohan Ding,Daniel Manesh,Sang Won Lee,Tim Lee,Stephanie Lunn,Sarah Rodriguez,Chris Brown,Eugenia Rho*

Main category: cs.HC

TL;DR: 研究探讨了对话AI在技术面试中“大声思考”练习中的作用，参与者认可AI在模拟、反馈和学习中的价值，并提出了设计建议和更广泛的研究方向。


<details>
  <summary>Details</summary>
Motivation: 技术面试中的“大声思考”练习缺乏结构化机会，对话AI的潜力尚未被充分研究。

Method: 使用基于LLM的技术面试练习工具，对17名参与者进行研究。

Result: 参与者认可AI在模拟、反馈和学习中的价值，提出了设计建议和更广泛的研究方向。

Conclusion: 对话AI在技术面试练习中具有潜力，需进一步研究人机协作及其对公平学习的促进作用。

Abstract: One challenge in technical interviews is the think-aloud process, where
candidates verbalize their thought processes while solving coding tasks.
Despite its importance, opportunities for structured practice remain limited.
Conversational AI offers potential assistance, but limited research explores
user perceptions of its role in think-aloud practice. To address this gap, we
conducted a study with 17 participants using an LLM-based technical interview
practice tool. Participants valued AI's role in simulation, feedback, and
learning from generated examples. Key design recommendations include promoting
social presence in conversational AI for technical interview simulation,
providing feedback beyond verbal content analysis, and enabling crowdsourced
think-aloud examples through human-AI collaboration. Beyond feature design, we
examined broader considerations, including intersectional challenges and
potential strategies to address them, how AI-driven interview preparation could
promote equitable learning in computing careers, and the need to rethink AI's
role in interview practice by suggesting a research direction that integrates
human-AI collaboration.

</details>


### [90] [Conch: Competitive Debate Analysis via Visualizing Clash Points and Hierarchical Strategies](https://arxiv.org/abs/2507.14482)
*Qianhe Chen,Yong Wang,Yixin Yu,Xiyuan Zhu,Xuerou Yu,Ran Wang*

Main category: cs.HC

TL;DR: Conch是一个交互式可视化系统，通过并行螺旋可视化追踪辩论中的多维冲突点和参与者互动，并利用大语言模型自动识别关键辩论元素，提升辩论分析的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 手动分析非结构化和未标记的辩论文本记录耗时且低效，难以重建上下文语义和逻辑连接。

Method: 提出Conch系统，结合并行螺旋可视化和大语言模型，自动识别冲突点、分歧、观点和策略等辩论元素。

Result: 通过案例研究和用户研究验证了Conch在竞争性辩论分析中的有效性和可用性。

Conclusion: Conch为辩论分析提供了一种高效且直观的工具，帮助参与者全面理解辩论内容。

Abstract: In-depth analysis of competitive debates is essential for participants to
develop argumentative skills and refine strategies, and further improve their
debating performance. However, manual analysis of unstructured and unlabeled
textual records of debating is time-consuming and ineffective, as it is
challenging to reconstruct contextual semantics and track logical connections
from raw data. To address this, we propose Conch, an interactive visualization
system that systematically analyzes both what is debated and how it is debated.
In particular, we propose a novel parallel spiral visualization that compactly
traces the multidimensional evolution of clash points and participant
interactions throughout debate process. In addition, we leverage large language
models with well-designed prompts to automatically identify critical debate
elements such as clash points, disagreements, viewpoints, and strategies,
enabling participants to understand the debate context comprehensively.
Finally, through two case studies on real-world debates and a
carefully-designed user study, we demonstrate Conch's effectiveness and
usability for competitive debate analysis.

</details>


### [91] ["It looks sexy but it's wrong." Tensions in creativity and accuracy using genAI for biomedical visualization](https://arxiv.org/abs/2507.14494)
*Roxanne Ziman,Shehryar Saharan,Gaël McGill,Laura Garrison*

Main category: cs.HC

TL;DR: 论文分析了生成式AI在生物医学可视化中的使用及其局限性，强调其对信息准确性和科学传播的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在生物医学可视化中的工作流程和潜在问题，尤其是在信息准确性和科学信任方面的挑战。

Method: 通过17次访谈，定性分析从业者和研究人员对生成式AI的使用态度及其在可视化流程中的应用。

Result: 发现专家对生成式AI的态度各异，从积极采纳到怀疑回避，同时指出人类干预对科学可视化准确性的必要性。

Conclusion: 强调在科学传播中需平衡生成式AI的机遇与风险，确保科学视觉内容的准确性和可信度。

Abstract: We contribute an in-depth analysis of the workflows and tensions arising from
generative AI (genAI) use in biomedical visualization (BioMedVis). Although
genAI affords facile production of aesthetic visuals for biological and medical
content, the architecture of these tools fundamentally limits the accuracy and
trustworthiness of the depicted information, from imaginary (or fanciful)
molecules to alien anatomy. Through 17 interviews with a diverse group of
practitioners and researchers, we qualitatively analyze the concerns and values
driving genAI (dis)use for the visual representation of spatially-oriented
biomedical data. We find that BioMedVis experts, both in roles as developers
and designers, use genAI tools at different stages of their daily workflows and
hold attitudes ranging from enthusiastic adopters to skeptical avoiders of
genAI. In contrasting the current use and perspectives on genAI observed in our
study with predictions towards genAI in the visualization pipeline from prior
work, our refocus the discussion of genAI's effects on projects in
visualization in the here and now with its respective opportunities and
pitfalls for future visualization research. At a time when public trust in
science is in jeopardy, we are reminded to first do no harm, not just in
biomedical visualization but in science communication more broadly. Our
observations reaffirm the necessity of human intervention for empathetic design
and assessment of accurate scientific visuals.

</details>


### [92] [PaperBridge: Crafting Research Narratives through Human-AI Co-Exploration](https://arxiv.org/abs/2507.14527)
*Runhua Zhang,Yang Ouyang,Leixian Shen,Yuying Tang,Xiaojuan Ma,Huamin Qu,Xian Xu*

Main category: cs.HC

TL;DR: PaperBridge是一个基于人类-AI协作的系统，帮助研究者将多领域的研究成果组织成连贯的叙事。


<details>
  <summary>Details</summary>
Motivation: 研究者需要将跨领域的研究成果整合为连贯的叙事，尤其是在HCI等跨学科领域，这是一项挑战。

Method: PaperBridge采用双向分析引擎（基于大型语言模型），支持自上而下的用户意图和自下而上的叙事组件细化。

Result: 用户研究（N=12）表明PaperBridge在探索替代研究叙事方面具有可用性和有效性。

Conclusion: PaperBridge为学术交流任务提供了交互式支持，并提供了实证见解。

Abstract: Researchers frequently need to synthesize their own publications into
coherent narratives that demonstrate their scholarly contributions. To suit
diverse communication contexts, exploring alternative ways to organize one's
work while maintaining coherence is particularly challenging, especially in
interdisciplinary fields like HCI where individual researchers' publications
may span diverse domains and methodologies. In this paper, we present
PaperBridge, a human-AI co-exploration system informed by a formative study and
content analysis. PaperBridge assists researchers in exploring diverse
perspectives for organizing their publications into coherent narratives. At its
core is a bi-directional analysis engine powered by large language models,
supporting iterative exploration through both top-down user intent (e.g.,
determining organization structure) and bottom-up refinement on narrative
components (e.g., thematic paper groupings). Our user study (N=12) demonstrated
PaperBridge's usability and effectiveness in facilitating the exploration of
alternative research narratives. Our findings also provided empirical insights
into how interactive systems can scaffold academic communication tasks.

</details>


### [93] [Uncovering the EEG Temporal Representation of Low-dimensional Object Properties](https://arxiv.org/abs/2507.14537)
*Jiahua Tang,Song Wang,Jiachen Zou,Chen Wei,Quanying Liu*

Main category: cs.HC

TL;DR: 该论文提出了一种新方法，通过整合先进的神经解码算法，系统研究低维物体属性如何在EEG信号中时间编码，填补了EEG在神经表征时间动态研究中的空白。


<details>
  <summary>Details</summary>
Motivation: 探索EEG信号中神经表征的时间动态，解决其低信噪比和复杂时空耦合特性带来的挑战。

Method: 整合先进的神经解码算法，分析低维物体属性在EEG信号中的时间编码特性。

Result: 首次识别了概念在时间分布中的特异性和原型时间特征，提升了神经表征的可解释性。

Conclusion: 该框架为脑机接口中的视觉解码提供了新见解，并推动了EEG在认知过程动态追踪中的应用。

Abstract: Understanding how the human brain encodes and processes external visual
stimuli has been a fundamental challenge in neuroscience. With advancements in
artificial intelligence, sophisticated visual decoding architectures have
achieved remarkable success in fMRI research, enabling more precise and
fine-grained spatial concept localization. This has provided new tools for
exploring the spatial representation of concepts in the brain. However, despite
the millisecond-scale temporal resolution of EEG, which offers unparalleled
advantages in tracking the dynamic evolution of cognitive processes, the
temporal dynamics of neural representations based on EEG remain underexplored.
This is primarily due to EEG's inherently low signal-to-noise ratio and its
complex spatiotemporal coupling characteristics. To bridge this research gap,
we propose a novel approach that integrates advanced neural decoding algorithms
to systematically investigate how low-dimensional object properties are
temporally encoded in EEG signals. We are the first to attempt to identify the
specificity and prototypical temporal characteristics of concepts within
temporal distributions. Our framework not only enhances the interpretability of
neural representations but also provides new insights into visual decoding in
brain-computer interfaces (BCI).

</details>


### [94] [EventBox: A Novel Visual Encoding for Interactive Analysis of Temporal and Multivariate Attributes in Event Sequences](https://arxiv.org/abs/2507.14685)
*Luis Montana,Jessica Magallanes,Miguel Juarez,Suzanne Mason,Andrew Narracott,Lindsey van Gemeren,Steven Wood,Maria-Cruz Villa-Uriol*

Main category: cs.HC

TL;DR: EventBox是一种新颖的数据表示和视觉编码方法，用于分析事件组及其多变量属性，集成到Sequen-C系统中，通过用户驱动转换和统计分析增强分析深度。


<details>
  <summary>Details</summary>
Motivation: 快速增长的序列数据需要有效的分析和探索方法，现有方法常忽略时间与多变量属性的交互。

Method: 提出EventBox方法，集成到Sequen-C系统，支持用户驱动转换（对齐、排序、替换、聚合）和自动统计分析。

Result: 通过21名参与者评估（3名专家，18名新手），使用ICE-T框架验证可视化价值，并通过医疗数据案例展示有效模式发现。

Conclusion: EventBox为探索事件序列中的时间和多变量属性提供了灵活解决方案，推动了视觉分析的发展。

Abstract: The rapid growth and availability of event sequence data across domains
requires effective analysis and exploration methods to facilitate
decision-making. Visual analytics combines computational techniques with
interactive visualizations, enabling the identification of patterns, anomalies,
and attribute interactions. However, existing approaches frequently overlook
the interplay between temporal and multivariate attributes. We introduce
EventBox, a novel data representation and visual encoding approach for
analyzing groups of events and their multivariate attributes. We have
integrated EventBox into Sequen-C, a visual analytics system for the analysis
of event sequences. To enable the agile creation of EventBoxes in Sequen-C, we
have added user-driven transformations, including alignment, sorting,
substitution and aggregation. To enhance analytical depth, we incorporate
automatically generated statistical analyses, providing additional insight into
the significance of attribute interactions. We evaluated our approach involving
21 participants (3 domain experts, 18 novice data analysts). We used the ICE-T
framework to assess visualization value, user performance metrics completing a
series of tasks, and interactive sessions with domain experts. We also present
three case studies with real-world healthcare data demonstrating how EventBox
and its integration into Sequen-C reveal meaningful patterns, anomalies, and
insights. These results demonstrate that our work advances visual analytics by
providing a flexible solution for exploring temporal and multivariate
attributes in event sequences.

</details>


### [95] [A Notification Based Nudge for Handling Excessive Smartphone Use](https://arxiv.org/abs/2507.14702)
*Partha Sarker,Dipto Dey,Marium-E-Jannat*

Main category: cs.HC

TL;DR: 提出了一种基于通知的干预方法，以减少智能手机过度使用，避免用户感到困扰。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过限制使用来减少智能手机过度使用，但用户可能感到不便或烦恼。

Method: 设计了一个Android原型“App Usage Monitor”，结合轻推和可视化技术，通过通知提高用户自我意识。

Result: 通过3周实验验证了假设，证明该方法有效。

Conclusion: 基于通知的干预方法能有效减少智能手机过度使用，且用户接受度高。

Abstract: Excessive use of smartphones is a worldwide known issue. In this study, we
proposed a notification-based intervention approach to reduce smartphone
overuse without making the user feel any annoyance or irritation. Most of the
work in this field tried to reduce smartphone overuse by making smartphone use
more difficult for the user. In our user study (n = 109), we found that 19.3%
of the participants are unwilling to use any usage-limiting application because
a) they do not want their smartphone activities to get restricted or b) those
applications are annoying. Following that, we devised a hypothesis to minimize
smartphone usage among undergraduates. Finally, we designed a prototype for
Android, "App Usage Monitor," and conducted a 3-week experiment through which
we found proof of concept for our hypothesis. In our prototype, we combined
techniques such as nudge and visualization to increase self-awareness among the
user by leveraging notifications.

</details>


### [96] [XplainAct: Visualization for Personalized Intervention Insights](https://arxiv.org/abs/2507.14767)
*Yanming Zhang,Krishnakumar Hegde,Klaus Mueller*

Main category: cs.HC

TL;DR: XplainAct是一个支持在子群体中模拟、解释和推理个体层面干预的视觉分析框架，解决了现有方法在异质性系统中群体层面分析的不足。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理方法主要关注群体层面效果，无法有效处理异质性系统中干预效果的个体差异。

Method: 提出XplainAct框架，支持个体层面的干预模拟和解释，并通过两个案例研究验证其有效性。

Result: 案例研究表明XplainAct在流行病学（阿片类药物相关死亡）和政治学（总统选举投票倾向）中具有实际应用价值。

Conclusion: XplainAct为异质性系统中的个体层面因果推理提供了有效工具。

Abstract: Causality helps people reason about and understand complex systems,
particularly through what-if analyses that explore how interventions might
alter outcomes. Although existing methods embrace causal reasoning using
interventions and counterfactual analysis, they primarily focus on effects at
the population level. These approaches often fall short in systems
characterized by significant heterogeneity, where the impact of an intervention
can vary widely across subgroups. To address this challenge, we present
XplainAct, a visual analytics framework that supports simulating, explaining,
and reasoning interventions at the individual level within subpopulations. We
demonstrate the effectiveness of XplainAct through two case studies:
investigating opioid-related deaths in epidemiology and analyzing voting
inclinations in the presidential election.

</details>


### [97] [Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs](https://arxiv.org/abs/2507.14769)
*Ananya Gubbi Mohanbabu,Yotam Sechayk,Amy Pavel*

Main category: cs.HC

TL;DR: Task Mode 是一个动态过滤网页内容的系统，利用大语言模型根据用户目标识别和优先显示相关内容，减少干扰，显著提升了屏幕阅读用户的效率。


<details>
  <summary>Details</summary>
Motivation: 现代网页界面过于复杂，尤其是对屏幕阅读用户（SRUs）来说，需要花费大量时间浏览无关内容，而视觉用户（VUs）可以快速浏览。这种差异加剧了可访问性差距。

Method: Task Mode 使用大语言模型动态过滤网页内容，保留页面结构并提供多种浏览模式，以适应不同用户需求。

Result: 用户研究表明，Task Mode 显著减少了 SRUs 的任务完成时间（从 2 倍差距降至 1.2 倍），同时保持了 VUs 的表现。12 名参与者中有 11 名希望未来使用该系统。

Conclusion: Task Mode 展示了如何通过同时为视觉和非视觉用户设计交互，减少可访问性差距，为未来人机交互技术提供了新思路。

Abstract: Modern web interfaces are unnecessarily complex to use as they overwhelm
users with excessive text and visuals unrelated to their current goals. This
problem particularly impacts screen reader users (SRUs), who navigate content
sequentially and may spend minutes traversing irrelevant elements before
reaching desired information compared to vision users (VUs) who visually skim
in seconds. We present Task Mode, a system that dynamically filters web content
based on user-specified goals using large language models to identify and
prioritize relevant elements while minimizing distractions. Our approach
preserves page structure while offering multiple viewing modes tailored to
different access needs. Our user study with 12 participants (6 VUs, 6 SRUs)
demonstrates that our approach reduced task completion time for SRUs while
maintaining performance for VUs, decreasing the completion time gap between
groups from 2x to 1.2x. 11 of 12 participants wanted to use Task Mode in the
future, reporting that Task Mode supported completing tasks with less effort
and fewer distractions. This work demonstrates how designing new interactions
simultaneously for visual and non-visual access can reduce rather than
reinforce accessibility disparities in future technology created by
human-computer interaction researchers and practitioners.

</details>


### [98] [SenseSeek Dataset: Multimodal Sensing to Study Information Seeking Behaviors](https://arxiv.org/abs/2507.14792)
*Kaixin Ji,Danula Hettiachchi,Falk Scholer,Flora D. Salim,Damiano Spina*

Main category: cs.HC

TL;DR: SenseSeek数据集通过多传感器捕捉信息搜索过程中的生理和行为数据，为研究信息寻求行为提供了新资源。


<details>
  <summary>Details</summary>
Motivation: 理解复杂信息处理行为需要个性化的数据捕捉，而消费级传感器（如可穿戴设备）为此提供了可能。

Method: 收集20名参与者在235次搜索试验中的多传感器数据（EDA、EEG、PUPIL等），涵盖搜索各阶段（如查询提交、相关性判断）。

Result: 数据集包含258个特征，验证了传感器数据对不同搜索阶段和交互模态的区分能力。

Conclusion: SenseSeek是首个多传感器表征信息搜索阶段的数据集，为未来研究提供了参考。

Abstract: Information processing tasks involve complex cognitive mechanisms that are
shaped by various factors, including individual goals, prior experience, and
system environments. Understanding such behaviors requires a sophisticated and
personalized data capture of how one interacts with modern information systems
(e.g., web search engines). Passive sensors, such as wearables, capturing
physiological and behavioral data, have the potential to provide solutions in
this context. This paper presents a novel dataset, SenseSeek, designed to
evaluate the effectiveness of consumer-grade sensors in a complex information
processing scenario: searching via systems (e.g., search engines), one of the
common strategies users employ for information seeking. The SenseSeek dataset
comprises data collected from 20 participants, 235 trials of the stimulated
search process, 940 phases of stages in the search process, including the
realization of Information Need (IN), Query Formulation (QF), Query Submission
by Typing (QS-T) or Speaking (QS-S), and Relevance Judgment by Reading (RJ-R)
or Listening (RJ-L). The data includes Electrodermal Activities (EDA),
Electroencephalogram (EEG), PUPIL, GAZE, and MOTION data, which were captured
using consumer-grade sensors. It also contains 258 features extracted from the
sensor data, the gaze-annotated screen recordings, and task responses. We
validate the usefulness of the dataset by providing baseline analysis on the
impacts of different cognitive intents and interaction modalities on the sensor
data, and effectiveness of the data in discriminating the search stages. To our
knowledge, SenseSeek is the first dataset that characterizes the multiple
stages involved in information seeking with physiological signals collected
from multiple sensors. We hope this dataset can serve as a reference for future
research on information-seeking behaviors.

</details>


### [99] [Understanding How Visually Impaired Players Socialize in Mobile Games](https://arxiv.org/abs/2507.14818)
*Zihe Ran,Xiyu Li,Qing Xiao,Yanyun Wang,Franklin Mingzhe Li,Zhicong Lu*

Main category: cs.HC

TL;DR: 研究探讨了中国视障玩家如何通过手机游戏社交，发现技术障碍和社区内部分裂是主要挑战。


<details>
  <summary>Details</summary>
Motivation: 视障人士在现实生活中面临社交障碍，手机游戏成为重要社交平台，但设计和技术问题影响体验。

Method: 通过采访30名视障玩家，分析其社交体验和挑战。

Result: 手机游戏满足部分社交需求，但技术障碍和社区分裂阻碍参与。

Conclusion: 研究为设计更具包容性和可访问性的手机游戏提供了见解。

Abstract: Mobile games are becoming a vital medium for social interaction, offering a
platform that transcends geographical boundaries. An increasing number of
visually impaired individuals are engaging in mobile gaming to connect,
collaborate, compete, and build friendships. In China, visually impaired
communities face significant social challenges in offline settings, making
mobile games a crucial avenue for socialization. However, the design of mobile
games and their mapping to real-world environments significantly shape their
social gaming experiences. This study explores how visually impaired players in
China navigate socialization and integrate into gaming communities. Through
interviews with 30 visually impaired players, we found that while mobile games
fulfill many of their social needs, technological barriers and insufficient
accessibility features, and internal community divisions present significant
challenges to their participation. This research sheds light on their social
experiences and offers insights for designing more inclusive and accessible
mobile games.

</details>


### [100] [Progressive Sentences: Combining the Benefits of Word and Sentence Learning](https://arxiv.org/abs/2507.14846)
*Nuwan Janaka,Shengdong Zhao,Ashwin Ram,Ruoxin Sun,Sherisse Tan Jing Wen,Danae Li,David Hsu*

Main category: cs.HC

TL;DR: 研究探讨了轻量级AR智能眼镜如何通过渐进式句子结构的多模态呈现支持移动第二语言学习，发现渐进式展示方法能提升记忆效果，特别是在移动场景中。


<details>
  <summary>Details</summary>
Motivation: 探索AR智能眼镜在移动第二语言学习中的潜力，尤其是通过多模态信息传递支持微学习场景。

Method: 采用渐进式展示方法，逐步呈现句子成分（主语、动词、宾语）并保留上下文，同时测试了定时间隔对学习效果的影响。

Result: 渐进式展示显著提升了记忆效果，尤其在移动场景中；定时间隔进一步提高了多任务条件下的学习效率。

Conclusion: 渐进式展示方法有效，为教育应用提供了实用指南，适用于短暂、移动中的学习场景。

Abstract: The rapid evolution of lightweight consumer augmented reality (AR) smart
glasses (a.k.a. optical see-through head-mounted displays) offers novel
opportunities for learning, particularly through their unique capability to
deliver multimodal information in just-in-time, micro-learning scenarios. This
research investigates how such devices can support mobile second-language
acquisition by presenting progressive sentence structures in multimodal
formats. In contrast to the commonly used vocabulary (i.e., word) learning
approach for novice learners, we present a "progressive presentation" method
that combines both word and sentence learning by sequentially displaying
sentence components (subject, verb, object) while retaining prior context.
Pilot and formal studies revealed that progressive presentation enhances
recall, particularly in mobile scenarios such as walking. Additionally,
incorporating timed gaps between word presentations further improved learning
effectiveness under multitasking conditions. Our findings demonstrate the
utility of progressive presentation and provide usage guidelines for
educational applications-even during brief, on-the-go learning moments.

</details>


### [101] [Holistic Specification of the Human Digital Twin: Stakeholders, Users, Functionalities, and Applications](https://arxiv.org/abs/2507.14859)
*Nils Mandischer,Alexander Atanasyan,Ulrich Dahmen,Michael Schluse,Jürgen Rossmann,Lars Mikelsons*

Main category: cs.HC

TL;DR: 本文提出了一个全面的人类数字孪生（HDT）愿景，明确了其需求、利益相关者和用户，并通过六个功能层级（存储、分析、个性化、预测、控制和优化）展示了其应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前人类数字孪生的定义、架构和应用缺乏清晰性，市场潜力尚未被充分认识。本文旨在填补这一空白，为研究和工业应用提供指导。

Method: 通过定义需求、利益相关者和用户，并基于六个功能层级（存储、分析、个性化、预测、控制和优化）设计应用案例，展示人类数字孪生的可行性。

Result: 提出了一个全面的需求列表，并详细讨论了三个应用案例，展示了功能层级的可行性和利益相关者分析的重要性。

Conclusion: 本文的研究成果可作为人类数字孪生实现和复用的指南，为未来研究和工业应用提供方向。

Abstract: The digital twin of humans is a relatively new concept. While many diverse
definitions, architectures, and applications exist, a clear picture is missing
on what, in fact, makes a human digital twin. Within this context, researchers
and industrial use-case owners alike are unaware about the market potential of
the - at the moment - rather theoretical construct. In this work, we draw a
holistic vision of the human digital twin, and derive the specification of this
holistic human digital twin in form of requirements, stakeholders, and users.
For each group of users, we define exemplary applications that fall into the
six levels of functionality: store, analyze, personalize, predict, control, and
optimize. The functionality levels facilitate an abstraction of abilities of
the human digital twin. From the manifold applications, we discuss three in
detail to showcase the feasibility of the abstraction levels and the analysis
of stakeholders and users. Based on the deep discussion, we derive a
comprehensive list of requirements on the holistic human digital twin. These
considerations shall be used as a guideline for research and industries for the
implementation of human digital twins, particularly in context of reusability
in multiple target applications.

</details>


### [102] [LEKIA: A Framework for Architectural Alignment via Expert Knowledge Injection](https://arxiv.org/abs/2507.14944)
*Boning Zhao,Yutong Hu*

Main category: cs.HC

TL;DR: 论文提出了一种名为LEKIA的新框架，通过分层专家知识注入架构（LEKIA）统一了知识注入和价值对齐的目标，解决了大型语言模型在高风险领域部署中的双重挑战。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域部署大型语言模型时，需要同时满足动态专家知识注入和细致的价值对齐，而现有方法往往无法兼顾这两点。

Method: 提出了LEKIA框架，包含理论层（核心原则）、实践层（案例示范）和评估层（实时自我修正），在不改变模型权重的情况下引导推理过程。

Result: 通过在教育心理学支持助手上的成功实现，验证了LEKIA的有效性。

Conclusion: LEKIA为专家驱动的AI行为设计提供了新路径，解决了知识与对齐之间的冲突，推动了更负责任的AI发展。

Abstract: Deploying Large Language Models (LLMs) in high-stakes domains is impeded by a
dual challenge: the need for deep, dynamic expert knowledge injection and
nuanced value alignment. Prevailing paradigms often address these challenges
separately, creating a persistent tension between knowledge and alignment;
knowledge-focused methods like Retrieval-Augmented Generation (RAG) have
limited deep alignment capabilities, while alignment-focused methods like
Reinforcement Learning from Human Feedback (RLHF) struggle with the agile
injection of expert wisdom. This paper introduces a new collaborative
philosophy, Expert-owned AI behavior design, realized through Architectural
Alignment-a paradigm that unifies these two goals within a single framework
called the Layered Expert Knowledge Injection Architecture (LEKIA). LEKIA
operates as an intelligent intermediary that guides an LLM's reasoning process
without altering its weights, utilizing a three-tiered structure: a Theoretical
Layer for core principles, a Practical Layer for exemplary cases, and an
Evaluative Layer for real-time, value-aligned self-correction. We demonstrate
the efficacy of this paradigm through the successful implementation of a
LEKIA-based psychological support assistant for the special education field.
Our work presents a path toward more responsible and expert-driven AI,
empowering domain specialists to directly architect AI behavior and resolve the
tension between knowledge and alignment.

</details>


### [103] [Echoes of the Land: An Interactive Installation Based on Physical Model of Earthquake](https://arxiv.org/abs/2507.14947)
*Ivan C. H. Liu,Chung-En Hao,Jing Xie*

Main category: cs.HC

TL;DR: 《Echoes of the Land》是一个将地震动力学转化为多感官体验的互动装置，通过弹簧块模型模拟地震重现和自组织临界性，实时生成声音和光效。


<details>
  <summary>Details</summary>
Motivation: 探索科学知识与艺术实践的结合，为新型乐器和叙事媒介开辟新途径，同时研究涌现复杂性、美学和交互性的交叉点。

Method: 使用弹簧块模型模拟地震动力学，通过运动捕捉和拼接颗粒合成技术实时生成声音和光效。

Result: 每个块作为代理，产生涌现的视听级联，可视化破裂和阈值行为的物理现象。

Conclusion: 该作品展示了科学与艺术的融合，为未来研究涌现复杂性、美学和交互性提供了新视角。

Abstract: Echoes of the Land is an interactive installation that transforms seismic
dynamics into a multisensory experience through a scientifically grounded
spring-block model. Simulating earthquake recurrence and self-organized
criticality, the work generates real-time sound and light via motion capture
and concatenative granular synthesis. Each block acts as an agent, producing
emergent audiovisual cascades that visualize the physics of rupture and
threshold behavior. This work exemplifies the amalgamation of scientific
knowledge and artistic practice, opening new avenues for novel forms of musical
instrument and narrative medium, while inviting further investigation into the
intersection of emergent complexity, aesthetics and interactivity.

</details>


### [104] [Emphasizing Deliberation and Critical Thinking in an AI Hype World](https://arxiv.org/abs/2507.14961)
*Katja Rogers*

Main category: cs.HC

TL;DR: 论文主张在AI热潮中采取更慢、更审慎的使用方式，结合批判性参与，以减少负面影响。


<details>
  <summary>Details</summary>
Motivation: AI解决方案主义因炒作和HCI对新奇性的推崇而加速，但简单禁止或放弃技术既不现实也不完全有益。

Method: 提倡慢速、审慎的技术使用，结合批判性参与和非参与。

Result: 有助于在AI热潮后世界中导航，同时为知识基础做出贡献并减少教育和研究中的危害。

Conclusion: 通过批判性和审慎的方式使用AI，可以平衡其负面影响并促进可持续发展。

Abstract: AI solutionism is accelerated and substantiated by hype and HCI's elevation
of novelty. Banning or abandoning technology is unlikely to work and probably
not beneficial on the whole either -- but slow(er), deliberate use together
with conscientious, critical engagement and non-engagement may help us navigate
a post-AI hype world while contributing to a solid knowledge foundation and
reducing harmful impacts in education and research.

</details>


### [105] ['A Little Bubble of Friends': An Analysis of LGBTQ+ Pandemic Experiences Using Reddit Data](https://arxiv.org/abs/2507.15033)
*Dhruvee Birla,Nazia Akhtar*

Main category: cs.HC

TL;DR: 研究通过LDA主题建模和情感分析，分析了疫情期间LGBTQ+群体在Reddit上的讨论主题和态度，探讨了Reddit在其生活中的角色变化。


<details>
  <summary>Details</summary>
Motivation: 疫情期间，社交媒体成为年轻人交流的主要平台，尤其是LGBTQ+群体。研究旨在了解他们在Reddit上的讨论主题和情感态度，以及平台在其生活中的作用是否因疫情发生变化。

Method: 使用LDA主题建模和情感分析技术，对五个LGBTQ+相关子版块的Reddit数据进行分析。

Result: 揭示了疫情期间LGBTQ+群体在Reddit上的主要讨论主题和情感倾向，并发现平台在其生活中的角色可能有所变化。

Conclusion: 研究表明，Reddit在疫情期间为LGBTQ+群体提供了重要的交流空间，但其作用可能与疫情前有所不同。

Abstract: Social media was one of the most popular forms of communication among young
people with digital access during the pandemic. Consequently, crucial debates
and discussions about the pandemic crisis have also developed on social media
platforms, making them a great primary source to study the experiences of
specific groups and communities during the pandemic. This study involved
research using LDA topic modeling and sentiment analysis on data obtained from
the social media platform Reddit to understand the themes and attitudes in
circulation within five subreddits devoted to LGBTQ+ experiences and issues. In
the process, we attempt to make sense of the role that Reddit may have played
in the lives of LGBTQ+ people who were online during the pandemic, and whether
this was marked by any continuities or discontinuities from before the pandemic
period.

</details>


### [106] [Visibility vs. Engagement: How Two Indian News Websites Reported on LGBTQ+ Individuals and Communities during the Pandemic](https://arxiv.org/abs/2507.15041)
*Dhruvee Birla,Nazia Akhtar*

Main category: cs.HC

TL;DR: 研究分析了印度两家英语新闻网站在疫情期间对LGBTQ+群体的报道情况，发现报道集中在跨性别群体，但缺乏深度和质量。


<details>
  <summary>Details</summary>
Motivation: 探讨印度新闻媒体在疫情期间如何报道LGBTQ+群体的生活现实，尤其是在法律认可有限的背景下。

Method: 对《印度时报》和《印度快报》的文章进行情感分析和主题建模，并与疫情前数据对比。

Result: 报道多关注跨性别群体，但质量不高；《印度时报》部分文章语言过时且恐跨。

Conclusion: 研究揭示了疫情期间印度新闻网站对LGBTQ+群体的可见性和代表性不足。

Abstract: In India, online news media outlets were an important source of information
for people with digital access during the COVID-19 pandemic. In India, where
"transgender" was legally recognised as a category only in 2014, and same-sex
marriages are yet to be legalised, it becomes crucial to analyse whether and
how they reported the lived realities of vulnerable LGBTQ+ communities during
the pandemic. This study analysed articles from online editions of two
English-language newspaper websites, which differed vastly in their circulation
figures-The Times of India and The Indian Express. The results of our study
suggest that these newspaper websites published articles surrounding various
aspects of the lives of LGBTQ+ individuals with a greater focus on transgender
communities. However, they lacked quality and depth. Focusing on the period
spanning March 2020 to August 2021, we analysed articles using sentiment
analysis and topic modelling. We also compared our results to the period before
the pandemic (January 2019 - December 2019) to understand the shift in topics,
sentiments, and stances across the two newspaper websites. A manual analysis of
the articles indicated that the language used in certain articles by The Times
of India was transphobic and obsolete. Our study captures the visibility and
representation of the LGBTQ+ communities in Indian newspaper websites during
the pandemic.

</details>


### [107] [Beyond Visual Line of Sight: UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence](https://arxiv.org/abs/2507.15049)
*Andres Navarro,Carlos de Quinto,José Alberto Hernández*

Main category: cs.HC

TL;DR: 论文介绍了一种结合5G通信、边缘计算和AI的无人机平台，用于解决非地面网络中的核心挑战，具备低延迟视觉处理和5G连接能力。


<details>
  <summary>Details</summary>
Motivation: 无人机作为智能节点在非地面网络中具有潜力，但需要解决通信、数据处理和适应性等挑战。

Method: 开发了一种配备全景摄像头、强大计算能力和LLM的无人机系统，结合5G和VR技术。

Result: 系统在实地测试中表现出低延迟视觉处理和稳定的5G连接，LLM进一步优化了数据分析和决策支持。

Conclusion: 该系统在应急响应、基础设施评估和环境监测等场景中展现了高度适应性。

Abstract: Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as
agile, intelligent nodes capable of advanced analytics and instantaneous
situational awareness. This article introduces a budget-friendly quadcopter
platform that unites 5G communications, edge-based processing, and AI to tackle
core challenges in NTN scenarios. Outfitted with a panoramic camera, robust
onboard computation, and LLMs, the drone system delivers seamless object
recognition, contextual analysis, and immersive operator experiences through
virtual reality VR technology. Field evaluations confirm the platform's ability
to process visual streams with low latency and sustain robust 5G links. Adding
LLMs further streamlines operations by extracting actionable insights and
refining collected data for decision support. Demonstrated use cases, including
emergency response, infrastructure assessment, and environmental surveillance,
underscore the system's adaptability in demanding contexts.

</details>


### [108] [NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments](https://arxiv.org/abs/2507.15072)
*Maisha Maimuna,Minhaz Bin Farukee,Sama Nikanfar,Mahfuza Siddiqua,Ayon Roy,Fillia Makedon*

Main category: cs.HC

TL;DR: 开发了一种多模态引导模拟器，帮助盲人和低视力（BLV）用户在工业仓库中安全地远程操作机器人。


<details>
  <summary>Details</summary>
Motivation: 工业仓库环境复杂，BLV用户远程操作机器人存在高风险，现有研究缺乏针对BLV用户的多模态引导系统。

Method: 结合导航网格、实时路径规划和多模态反馈（视觉、听觉、触觉），设计了一个高保真仓库模拟器。

Result: 系统能为BLV用户提供实时导航和避障，支持快速部署到实际机器人。

Conclusion: 该模拟器为可访问远程操作研究提供了可重复测试平台，并支持实际仓库中的包容性机器人工具部署。

Abstract: Industrial warehouses are congested with moving forklifts, shelves and
personnel, making robot teleoperation particularly risky and demanding for
blind and low-vision (BLV) operators. Although accessible teleoperation plays a
key role in inclusive workforce participation, systematic research on its use
in industrial environments is limited, and few existing studies barely address
multimodal guidance designed for BLV users. We present a novel multimodal
guidance simulator that enables BLV users to control a mobile robot through a
high-fidelity warehouse environment while simultaneously receiving synchronized
visual, auditory, and haptic feedback. The system combines a navigation mesh
with regular re-planning so routes remain accurate avoiding collisions as
forklifts and human avatars move around the warehouse. Users with low vision
are guided with a visible path line towards destination; navigational voice
cues with clockwise directions announce upcoming turns, and finally
proximity-based haptic feedback notifies the users of static and moving
obstacles in the path. This real-time, closed-loop system offers a repeatable
testbed and algorithmic reference for accessible teleoperation research. The
simulator's design principles can be easily adapted to real robots due to the
alignment of its navigation, speech, and haptic modules with commercial
hardware, supporting rapid feasibility studies and deployment of inclusive
telerobotic tools in actual warehouses.

</details>


### [109] ["If I were in Space": Understanding and Adapting to Social Isolation through Designing Collaborative Narratives](https://arxiv.org/abs/2507.15081)
*Qi Gong,Ximing Shen,Ziyou Yin,Yaning Li,Ray Lc*

Main category: cs.HC

TL;DR: 研究设计了一个社交VR中的协作叙事体验，探讨隔离期间人们的适应策略。


<details>
  <summary>Details</summary>
Motivation: 社会隔离可能导致健康问题，现有研究多关注物理干预，忽略了叙事策略的潜力。

Method: 在社交VR中设计协作叙事体验，参与者设计虚拟太空旅程隐喻隔离，揭示适应策略。

Result: 定性分析显示参与者通过设计空间和协作活动展现了创造性适应策略，并影响了适应过程。

Conclusion: 研究表明，叙事体验设计可作为探索人们应对社会隔离的有效工具。

Abstract: Social isolation can lead to pervasive health issues like anxiety and
loneliness. Previous work focused on physical interventions like exercise and
teleconferencing, but overlooked the narrative potential of adaptive
strategies. To address this, we designed a collaborative online storytelling
experience in social VR, enabling participants in isolation to design an
imaginary space journey as a metaphor for quarantine, in order to learn about
their isolation adaptation strategies in the process. Eighteen individuals
participated during real quarantine undertaken a virtual role-play experience,
designing their own spaceship rooms and engaging in collaborative activities
that revealed creative adaptative strategies. Qualitative analyses of
participant designs, transcripts, and interactions revealed how they coped with
isolation, and how the engagement unexpectedly influenced their adaptation
process. This study shows how designing playful narrative experiences, rather
than solution-driven approaches, can serve as probes to surface how people
navigate social isolation.

</details>


### [110] [TalkLess: Blending Extractive and Abstractive Speech Summarization for Editing Speech to Preserve Content and Style](https://arxiv.org/abs/2507.15202)
*Karim Benharrak,Puyuan Peng,Amy Pavel*

Main category: cs.HC

TL;DR: TalkLess是一个结合提取和抽象方法的语音编辑系统，旨在高效压缩语音内容同时保留说话者的风格。


<details>
  <summary>Details</summary>
Motivation: 解决传统语音编辑方法（如纯提取或纯抽象）在灵活性和风格保留上的不足。

Method: TalkLess首先生成可能的文本编辑选项，选择最优编辑以最大化压缩、覆盖率和音频质量，再通过语音编辑模型将文本编辑转化为音频编辑。

Result: TalkLess在覆盖率和去除语音错误方面优于现有提取方法，显著降低用户的认知负担和编辑工作量。

Conclusion: TalkLess通过结合提取和抽象方法，为语音编辑提供了更灵活高效的解决方案。

Abstract: Millions of people listen to podcasts, audio stories, and lectures, but
editing speech remains tedious and time-consuming. Creators remove unnecessary
words, cut tangential discussions, and even re-record speech to make recordings
concise and engaging. Prior work automatically summarized speech by removing
full sentences (extraction), but rigid extraction limits expressivity. AI tools
can summarize then re-synthesize speech (abstraction), but abstraction strips
the speaker's style. We present TalkLess, a system that flexibly combines
extraction and abstraction to condense speech while preserving its content and
style. To edit speech, TalkLess first generates possible transcript edits,
selects edits to maximize compression, coverage, and audio quality, then uses a
speech editing model to translate transcript edits into audio edits. TalkLess's
interface provides creators control over automated edits by separating
low-level wording edits (via the compression pane) from major content edits
(via the outline pane). TalkLess achieves higher coverage and removes more
speech errors than a state-of-the-art extractive approach. A comparison study
(N=12) showed that TalkLess significantly decreased cognitive load and editing
effort in speech editing. We further demonstrate TalkLess's potential in an
exploratory study (N=3) where creators edited their own speech.

</details>


### [111] [How Does Empirical Research Facilitate Creation Tool Design? A Data Video Perspective](https://arxiv.org/abs/2507.15244)
*Leixian Shen,Leni Yang,Haotian Li,Yun Wang,Yuyu Luo,Huamin Qu*

Main category: cs.HC

TL;DR: 本文通过数据视频案例研究，揭示实证研究如何影响创作工具开发，并提出未来整合建议。


<details>
  <summary>Details</summary>
Motivation: 探讨实证研究对创作工具开发的影响及未来如何加强这种整合。

Method: 分析46篇实证研究论文和48篇创作工具论文，辅以11位专家访谈，进行引用分析和模式分类。

Result: 揭示了实证研究对工具设计的模式影响，并识别了影响应用性的关键因素。

Conclusion: 提出建议以加强实证研究与工具研究的互动，提升创作工具的理论基础和实践影响。

Abstract: Empirical research in creative design deepens our theoretical understanding
of design principles and perceptual effects, offering valuable guidance for
innovating creation tools. However, how these empirical insights currently
influence the development of creation tools, and how their integration can be
enhanced in the future, remains insufficiently understood. In this paper, we
aim to unveil the gap through a case study on data videos, a prominent and
wide-spread medium for effective data storytelling. To achieve the goal, we
conducted a comprehensive analysis of 46 empirical research papers and 48
creation tool papers on data video, complemented by interviews with 11 experts.
Building upon a systematic collection and structured characterization of
empirical research by their methodologies (e.g., corpus analysis, comparative
evaluations) and component focus (e.g., visuals, motions, narratives, audio),
we conducted a context-aware citation analysis and revealed a taxonomy of
recurring patterns in how empirical findings inform tool design across citation
functions (e.g., problem framing, technical reference). Expert interviews
further uncovered researchers' practice patterns in applying empirical findings
(e.g., adaptation, synthesis, iteration, etc.) and identified key factors
influencing applicability, such as contextual relevance, granularity matching,
clarity, credibility, and feasibility. Finally, we derive suggestions and
discuss future opportunities to foster closer mutual engagement between
empirical and tool research, aiming to reinforce the theoretical grounding of
creation tools and enhance the practical impact of empirical research.

</details>


### [112] [Efficient Visual Appearance Optimization by Learning from Prior Preferences](https://arxiv.org/abs/2507.15355)
*Zhipeng Li,Yi-Chi Liao,Christian Holz*

Main category: cs.HC

TL;DR: Meta-PO结合元学习和PBO，通过利用先验用户偏好模型，显著提高了视觉参数优化的效率和个性化。


<details>
  <summary>Details</summary>
Motivation: 由于视觉参数优化搜索空间大且缺乏明确目标函数，用户依赖主观偏好，传统PBO方法迭代次数多，不适合普通用户。

Method: Meta-PO整合PBO和元学习，利用先验用户偏好模型为新用户智能推荐设计候选，减少迭代次数。

Result: 实验显示，Meta-PO在5.86次迭代内实现满意效果（目标相似时），8次迭代内适应不同目标。

Conclusion: Meta-PO通过高效、个性化的优化方法，使视觉优化更适用于普通用户，并具有广泛推广潜力。

Abstract: Adjusting visual parameters such as brightness and contrast is common in our
everyday experiences. Finding the optimal parameter setting is challenging due
to the large search space and the lack of an explicit objective function,
leaving users to rely solely on their implicit preferences. Prior work has
explored Preferential Bayesian Optimization (PBO) to address this challenge,
involving users to iteratively select preferred designs from candidate sets.
However, PBO often requires many rounds of preference comparisons, making it
more suitable for designers than everyday end-users. We propose Meta-PO, a
novel method that integrates PBO with meta-learning to improve sample
efficiency. Specifically, Meta-PO infers prior users' preferences and stores
them as models, which are leveraged to intelligently suggest design candidates
for the new users, enabling faster convergence and more personalized results.
An experimental evaluation of our method for appearance design tasks on 2D and
3D content showed that participants achieved satisfactory appearance in 5.86
iterations using Meta-PO when participants shared similar goals with a
population (e.g., tuning for a ``warm'' look) and in 8 iterations even
generalizes across divergent goals (e.g., from ``vintage'', ``warm'', to
``holiday''). Meta-PO makes personalized visual optimization more applicable to
end-users through a generalizable, more efficient optimization conditioned on
preferences, with the potential to scale interface personalization more
broadly.

</details>


### [113] [Designing at 1:1 Scale on Wall-Sized Displays Using Existing UI Design Tools](https://arxiv.org/abs/2507.15433)
*Lou Schwartz,Mohammad Ghoniem,Valérie Maquil,Adrien Coppens,Johannes Hermen*

Main category: cs.HC

TL;DR: 论文探讨了在墙式大屏幕上进行1:1比例设计的可行性，通过用户研究和技术评估，发现平板交互最舒适，并提出12条设计指南。


<details>
  <summary>Details</summary>
Motivation: 墙式大屏幕的空间特性对用户界面设计提出了挑战，现有工具未能完全支持此类设计。

Method: 通过两项用户研究和一项技术评估，比较了三种交互方式（触控、键盘+触控板、平板），并分析了两种墙式大屏幕设置。

Result: 1:1比例设计受到认可，平板交互最舒适，混合交互模式有潜力。环境因素（如家具）也需考虑。

Conclusion: 现有工具需改进以支持墙式大屏幕设计，提出了12条设计指南，强调界面元素布局和额外功能的重要性。

Abstract: Wall-Sized Displays have spatial characteristics that are difficult to
address during user interface design. The design at scale 1:1 could be part of
the solution. In this paper, we present the results of two user studies and one
technology review, exploring the usability of popular, desktop-optimized
prototyping tools, for designing at scale on Wall-Sized Displays. We considered
two wall-sized display setups, and three different interaction methods: touch,
a keyboard equipped with a touchpad, and a tablet. We observed that designing
at scale 1:1 was appreciated. Tablet-based interaction proved to be the most
comfortable interaction method, and a mix of interaction modalities is
promising. In addition, care must be given to the surrounding environment, such
as furniture. We propose twelve design guidelines for a design tool dedicated
to this specific context. Overall, existing user interface design tools do not
yet fully support design on and for wall-sized displays and require further
considerations in terms of placement of user interface elements and the
provision of additional features.

</details>


### [114] [Evaluating Joint Attention for Mixed-Presence Collaboration on Wall-Sized Displays](https://arxiv.org/abs/2507.15443)
*Adrien Coppens,Valérie Maquil*

Main category: cs.HC

TL;DR: 提出了一种基于头部凝视数据的联合注意力测量方法，用于评估墙式显示器混合存在协作的质量。


<details>
  <summary>Details</summary>
Motivation: 需要一种适应房间规模体验且不显眼的评估方法，以量化混合存在协作的质量。

Method: 通过头部凝视数据测量联合注意力，并在用户研究中实现该方法。

Result: 初步结果显示该方法有效，并提供了关于混合存在协作的见解。

Conclusion: 提出的方法为评估混合存在协作提供了可行的解决方案，未来可进一步扩展。

Abstract: To understand and quantify the quality of mixed-presence collaboration around
wall-sized displays, robust evaluation methodologies are needed, that are
adapted for a room-sized experience and are not perceived as obtrusive. In this
paper, we propose our approach for measuring joint attention based on head gaze
data. We describe how it has been implemented for a user study on mixed
presence collaboration with two wall-sized displays and report on the insights
we gained so far from its implementation, with a preliminary focus on the data
coming from one particular session.

</details>


### [115] [Challenging Disability and Interaction Norms in XR: Cooling Down the Empathy Machine in Waiting for Hands](https://arxiv.org/abs/2507.15481)
*Yesica Duarte,Puneet Jain*

Main category: cs.HC

TL;DR: 论文提出了一种名为'Waiting for Hands'的XR装置，通过创造替代控制器和荒谬表演，挑战了VR作为'共情机器'的叙事，试图避免将残疾简化为怜悯或激励的奇观。


<details>
  <summary>Details</summary>
Motivation: 回应VR技术将残疾体验简化为共情或怜悯的叙事，探索XR表演如何以伦理姿态关注非规范性身体体验。

Method: 设计替代控制器并策划XR表演，通过荒谬的互动和部分遮挡的纪录片观看，制造不确定性和疏离感。

Result: XR表演成功颠覆了传统的沉浸式体验，通过荒谬和疏离感挑战了残疾叙事的常规框架。

Conclusion: XR表演可以采取伦理姿态，关注非规范性身体体验，同时避免将残疾简化为奇观。

Abstract: Virtual Reality (VR) is often described as the "ultimate empathy machine,"
framing disability as an experience to be simulated through such technologies,
which can reduce disability to a spectacle of pity or inspiration. In response,
we present Waiting for Hands (WfH), an interactive eXtended Reality (XR)
installation that critiques this logic by: (1) repurposing interaction norms in
XR through the creation of Alternative Controllers, and (2) staging an absurd
XR performance using the built controllers to disrupt sentimentalized
disability narratives. The performance involves eight people: two XR
participants on stage and six audience members watching a projected documentary
about Hema Kumari, an Indian singer living with Rheumatoid Arthritis. The XR
users partially obscure the film, drawing attention through strange mouth and
hand movements performed in XR. This creates a layered experience that disrupts
direct engagement with Hema's story and introduces uncertainty. While XR is
often seen as a fully immersive, sensory-dominant medium, this piece subverts
that framing by using XR to produce absurdity and alienation. By challenging
empathy-driven and pitiable narratives of disability, we ask what ethical
stance an XR performance can take to attune participants to non-normative
embodiment while resisting spectacle.

</details>


### [116] [FollowUpBot: An LLM-Based Conversational Robot for Automatic Postoperative Follow-up](https://arxiv.org/abs/2507.15502)
*Chen Chen,Jianing Yin,Jiannong Cao,Zhiyuan Wen,Mingjin Zhang,Weixun Gao,Xiang Wang,Haihua Shu*

Main category: cs.HC

TL;DR: 论文介绍了FollowUpBot，一种基于LLM的边缘部署机器人，用于术后护理和监测，解决了传统随访方法效率低和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 传统术后随访方法耗时且劳动密集，现有数字解决方案存在交互不灵活或隐私泄露问题。

Method: 开发了FollowUpBot，利用边缘部署的LLM进行动态路径规划和自适应面对面交流，生成结构化随访报告。

Result: 实验显示机器人实现了高覆盖率和满意度，以及高报告生成准确性。

Conclusion: FollowUpBot有效解决了术后随访的效率和隐私问题，具有实际应用潜力。

Abstract: Postoperative follow-up plays a crucial role in monitoring recovery and
identifying complications. However, traditional approaches, typically involving
bedside interviews and manual documentation, are time-consuming and
labor-intensive. Although existing digital solutions, such as web
questionnaires and intelligent automated calls, can alleviate the workload of
nurses to a certain extent, they either deliver an inflexible scripted
interaction or face private information leakage issues. To address these
limitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed
robot for postoperative care and monitoring. It allows dynamic planning of
optimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face
conversations with patients through multiple interaction modes, ensuring data
privacy. Moreover, FollowUpBot is capable of automatically generating
structured postoperative follow-up reports for healthcare institutions by
analyzing patient interactions during follow-up. Experimental results
demonstrate that our robot achieves high coverage and satisfaction in follow-up
interactions, as well as high report generation accuracy across diverse field
types. The demonstration video is available at
https://www.youtube.com/watch?v=_uFgDO7NoK0.

</details>


### [117] [Strategies to Manage Human Factors in Mixed Reality Pilot Training: A Survey](https://arxiv.org/abs/2507.15526)
*Antonio Perez,Avinash Singh,Jonathan Mitchell,Philip Swadling*

Main category: cs.HC

TL;DR: 本文综述了混合现实（MR）头戴式显示器在飞行员培训中的人因挑战及缓解策略，结合航空法规探讨了提升培训效果的干预措施。


<details>
  <summary>Details</summary>
Motivation: 混合现实技术为飞行模拟训练提供了沉浸感和成本效益，但人因问题如晕动症和视觉疲劳可能影响培训效果，需解决以发挥其潜力。

Method: 系统回顾现有文献，结合航空权威标准，从硬件、软件、人体工学等多角度分析干预措施。

Result: 研究发现缓解措施需平衡技术要求与飞行员健康，为MR航空培训指南提供了实践依据。

Conclusion: 研究为MR在航空培训中的应用提供了人因和法规视角的指导，支持其未来发展。

Abstract: Mixed Reality (MR) head mounted displays (HMDs) offer a promising alternative
to traditional Flight Simulator Training Device (FSTD) displays, providing
immersion, realism and cost efficiency. However, these technologies require
management of human factors; cybersickness, visual fatigue and ergonomic
strain. If left unmitigated, these effects can hinder pilot performance and
training outcomes. For safety critical fields like aviation, addressing human
factors challenges is crucial for MR's training potential. This survey
systematically reviews the current literature identifying key human factors
challenges in MR HMD use in pilot training and examines strategies to mitigate
these barriers. Drawing on existing industry standards set by a leading
aviation authority, the review adopts a regulatory perspective to explore
hardware, software, ergonomic, physiological and psychological interventions
improving pilot comfort, safety and training effectiveness in an MR FSTD.
Additionally, it evaluates which of these interventions are most appropriate
and viable for MR pilot training under existing aviation training regulations,
ensuring that technical requirements and pilot wellbeing remain balanced. The
findings yield significant insights for the human dimensions of aviation
simulation training, highlighting how regulatory considerations shape the
practicality of mitigation measures. These insights inform emerging MR aviation
training guidelines and best practices, supporting MR's readiness to enhance
aviation training.

</details>


### [118] [FlowForge: Guiding the Creation of Multi-agent Workflows with Design Space Visualization as a Thinking Scaffold](https://arxiv.org/abs/2507.15559)
*Pan Hao,Dongyeop Kang,Nicholas Hinds,Qianwen Wang*

Main category: cs.HC

TL;DR: FLOWFORGE是一个交互式可视化工具，用于优化多智能体工作流设计，通过结构化探索和实时指导提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体工作流设计依赖专家直觉，存在设计固化或试错耗时的问题，需要更高效的方法。

Method: FLOWFORGE提供三层结构化设计（任务规划、智能体分配、智能体优化），结合可视化探索和实时设计模式指导。

Result: 案例研究和用户实验验证了FLOWFORGE的可用性和有效性，提升了工作流设计效率。

Conclusion: FLOWFORGE通过结构化探索和实时指导，显著改善了多智能体工作流的设计过程。

Abstract: Multi-agent workflows have become an effective strategy for tackling
complicated tasks by decomposing them into multiple sub-tasks and assigning
them to specialized agents. However, designing optimal workflows remains
challenging due to the vast and intricate design space. Current practices rely
heavily on the intuition and expertise of practitioners, often resulting in
design fixation or an unstructured, time-consuming exploration of
trial-and-error. To address these challenges, this work introduces FLOWFORGE,
an interactive visualization tool to facilitate the creation of multi-agent
workflow through i) a structured visual exploration of the design space and ii)
in-situ guidance informed by established design patterns. Based on formative
studies and literature review, FLOWFORGE organizes the workflow design process
into three hierarchical levels (i.e., task planning, agent assignment, and
agent optimization), ranging from abstract to concrete. This structured visual
exploration enables users to seamlessly move from high-level planning to
detailed design decisions and implementations, while comparing alternative
solutions across multiple performance metrics. Additionally, drawing from
established workflow design patterns, FLOWFORGE provides context-aware, in-situ
suggestions at each level as users navigate the design space, enhancing the
workflow creation process with practical guidance. Use cases and user studies
demonstrate the usability and effectiveness of FLOWFORGE, while also yielding
valuable insights into how practitioners explore design spaces and leverage
guidance during workflow development.

</details>


### [119] [Chapter 11 Students' interaction with and appreciation of automated informative tutoring feedback](https://arxiv.org/abs/2507.15650)
*Gerben van der Hoek,Bastiaan Heeren,Rogier Bos,Paul Drijvers,Johan Jeuring*

Main category: cs.HC

TL;DR: 研究探讨了一种平衡探索空间与学习障碍缓解的反馈策略，发现学生能从中受益。


<details>
  <summary>Details</summary>
Motivation: 探索如何在计算机辅助形成性评估中设计反馈策略，以平衡学生探索空间与学习障碍的缓解。

Method: 25名15-17岁学生在在线环境中完成线性与指数外推任务，通过屏幕记录和访谈收集数据。

Result: 学生能利用探索空间进行自我指导，同时反馈缓解了学习障碍；学生更喜欢平衡的反馈而非完整解答。

Conclusion: 平衡的反馈策略促进了学生与环境的积极互动。

Abstract: Computer aided formative assessment can be used to enhance a learning
process, for instance by providing feedback. There are many design choices for
delivering feedback, that lead to a feedback strategy. In an informative
feedback strategy, students do not immediately receive information about the
correct response, but are offered the opportunity to retry a task to apply
feedback information. In this small-scale qualitative study, we explore an
informative feedback strategy designed to offer a balance between room for
exploration and mitigation of learning barriers. The research questions concern
the ways in which students interact with the feedback strategy and their
appreciation of error-specific feedback as opposed to worked-out solutions. To
answer these questions, twenty-five 15-to-17-year-old senior general secondary
education students worked for approximately 20 minutes on linear and
exponential extrapolation tasks in an online environment. Data included screen
captures of students working with the environment and post-intervention
interviews. Results showed that room for exploration offered opportunities for
self-guidance while mitigation of learning barriers prevented disengagement.
Furthermore, students appreciated balanced feedback. We conclude that the
balanced feedback strategy yielded fruitful student-environment interactions.

</details>


### [120] [Surfacing Variations to Calibrate Perceived Reliability of MLLM-generated Image Descriptions](https://arxiv.org/abs/2507.15692)
*Meng Chen,Akhil Iyer,Amy Pavel*

Main category: cs.HC

TL;DR: 多模态大语言模型（MLLM）为盲人和低视力（BLV）人群提供了获取视觉信息的新途径，但其错误难以察觉，存在安全和社交风险。研究通过系统展示多个MLLM响应的差异，帮助BLV用户检测不可靠信息，显著提高了识别能力。


<details>
  <summary>Details</summary>
Motivation: MLLM为BLV人群提供了便利，但其错误难以察觉，可能引发安全和社交问题。现有解决方法（如工具交叉验证或咨询视力正常者）效率低下。

Method: 提出一个设计空间，用于提取和展示MLLM描述的差异，并开发了一个原型系统，实现三种差异展示方式。通过15名BLV参与者的用户研究验证效果。

Result: 展示差异显著提高了用户识别不可靠信息的能力（提升4.9倍），并降低了用户对MLLM响应的信任度。14/15参与者更倾向于查看差异描述。

Conclusion: 系统展示MLLM响应的差异能有效帮助BLV用户检测不可靠信息，提升使用安全性，具有广泛的应用潜力。

Abstract: Multimodal large language models (MLLMs) provide new opportunities for blind
and low vision (BLV) people to access visual information in their daily lives.
However, these models often produce errors that are difficult to detect without
sight, posing safety and social risks in scenarios from medication
identification to outfit selection. While BLV MLLM users use creative
workarounds such as cross-checking between tools and consulting sighted
individuals, these approaches are often time-consuming and impractical. We
explore how systematically surfacing variations across multiple MLLM responses
can support BLV users to detect unreliable information without visually
inspecting the image. We contribute a design space for eliciting and presenting
variations in MLLM descriptions, a prototype system implementing three
variation presentation styles, and findings from a user study with 15 BLV
participants. Our results demonstrate that presenting variations significantly
increases users' ability to identify unreliable claims (by 4.9x using our
approach compared to single descriptions) and significantly decreases perceived
reliability of MLLM responses. 14 of 15 participants preferred seeing
variations of MLLM responses over a single description, and all expressed
interest in using our system for tasks from understanding a tornado's path to
posting an image on social media.

</details>


### [121] [Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance](https://arxiv.org/abs/2507.15783)
*Mohammad 'Matt' Namvarpour,Brandon Brofsky,Jessica Medina,Mamtaj Akter,Afsaneh Razi*

Main category: cs.HC

TL;DR: 研究分析了青少年对Character.AI等生成式AI聊天机器人的情感依赖问题，发现其可能导致心理困扰和现实关系疏离，并提出改进设计的建议。


<details>
  <summary>Details</summary>
Motivation: 探讨青少年与可定制人格聊天机器人的互动模式及其潜在的情感依赖和数字过度依赖问题。

Method: 分析了318篇由13-17岁青少年在Character.AI子论坛发布的Reddit帖子。

Result: 青少年常因情感支持或创意表达使用聊天机器人，但易形成强烈依赖，影响现实关系和日常生活，部分人通过反思或回归现实社交摆脱依赖。

Conclusion: 建议未来聊天机器人设计应增强自我意识、支持现实互动，并让青少年参与开发更安全的数字工具。

Abstract: As Generative Artificial Intelligence (GenAI) driven chatbots like
Character.AI become embedded in adolescent life, they raise concerns about
emotional dependence and digital overreliance. While studies have investigated
the overreliance of adults on these chatbots, they have not investigated teens'
interactions with chatbots with customizable personas. We analyzed 318 Reddit
posts made by users self-reported as 13-17 years old on the Character.AI
subreddit to understand patterns of overreliance. We found teens commonly begin
using chatbots for emotional support or creative expression, but many develop
strong attachments that interfere with offline relationships and daily
routines. Their posts revealed recurring signs of psychological distress,
cycles of relapse, and difficulty disengaging. Teens reported that their
overreliance often ended when they reflect on the harm, return to in-person
social settings, or become frustrated by platform restrictions. Based on the
implications of our findings, we provide recommendations for future chatbot
design so they can promote self-awareness, support real-world engagement, and
involve teens in developing safer digital tools.

</details>


### [122] [JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks](https://arxiv.org/abs/2505.17593)
*Manuel Valle Torre,Thom van der Velden,Marcus Specht,Catharine Oertel*

Main category: cs.HC

TL;DR: JELAI是一个开源平台架构，旨在将细粒度学习分析与基于大型语言模型的辅导集成到Jupyter Notebook环境中，支持实时、上下文敏感的AI辅助和研究学生行为。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在教育中缺乏教学基础和学生学习上下文意识的问题，并研究学生在真实学习环境中与这些工具的互动。

Method: 采用模块化、容器化设计，结合JupyterLab扩展和中间件，实现代码交互与聊天数据的集成捕获。

Result: 通过系统性能基准测试和两个概念验证用例，展示了JELAI在多模态数据记录、帮助寻求模式分析和AI配置A/B测试方面的能力。

Conclusion: JELAI提供了一个灵活的技术框架，支持研究人员和教育工作者在Jupyter生态系统中开发和研究基于学习分析的AI辅导。

Abstract: Generative AI offers potential for educational support, but often lacks
pedagogical grounding and awareness of the student's learning context.
Furthermore, researching student interactions with these tools within authentic
learning environments remains challenging. To address this, we present JELAI,
an open-source platform architecture designed to integrate fine-grained
Learning Analytics (LA) with Large Language Model (LLM)-based tutoring directly
within a Jupyter Notebook environment. JELAI employs a modular, containerized
design featuring JupyterLab extensions for telemetry and chat, alongside a
central middleware handling LA processing and context-aware LLM prompt
enrichment. This architecture enables the capture of integrated code
interaction and chat data, facilitating real-time, context-sensitive AI
scaffolding and research into student behaviour. We describe the system's
design, implementation, and demonstrate its feasibility through system
performance benchmarks and two proof-of-concept use cases illustrating its
capabilities for logging multi-modal data, analysing help-seeking patterns, and
supporting A/B testing of AI configurations. JELAI's primary contribution is
its technical framework, providing a flexible tool for researchers and
educators to develop, deploy, and study LA-informed AI tutoring within the
widely used Jupyter ecosystem.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [123] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种新的结构化剪枝方法Catalyst，通过引入辅助催化剂变量，确保剪枝决策的无偏性和鲁棒性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法（如L1或Group Lasso）存在基于幅度的偏差和决策边界窄的问题，Catalyst旨在解决这些问题。

Method: 基于代数条件设计新型正则化器，引入催化剂变量实现公平剪枝和宽边距决策。

Result: 在多个数据集和模型上验证了Catalyst剪枝的优越性，表现优于现有方法。

Conclusion: Catalyst剪枝方法在理论和实践中均表现出色，具有无偏性和鲁棒性。

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [124] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种基于投影空间的新型剪枝策略IPPRO，通过PROscore衡量滤波器剪枝可能性，挑战了传统基于幅度的剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于幅度的剪枝方法限制了剪枝决策的能力，因为较大的滤波器即使冗余也不易被剪枝。

Method: 将滤波器置于投影空间，观察梯度下降运动方向以衡量剪枝可能性，构建PROscore作为重要性评分。

Result: 实验表明，该方法实现了近乎无损的剪枝，减少了性能下降，并在微调后表现优异。

Conclusion: 该工作打破了剪枝中“大小决定一切”的迷思，从理论和实证上扩展了基于重要性的剪枝方法。

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [125] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: SOAR是一种通过将语言模型整合到自我改进的进化循环中来学习程序合成的方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在单次尝试中难以解决复杂的程序合成任务，而基于搜索的进化方法受限于生成模型的固定能力。

Method: SOAR交替进行进化搜索和事后学习，利用语言模型采样和优化候选解，并通过微调提升模型能力。

Result: 在ARC-AGI基准测试中，SOAR实现了显著的性能提升，解决了52%的公共测试集问题。

Conclusion: SOAR通过结合进化搜索和事后学习，有效提升了程序合成的能力，并展示了跨任务的正迁移效果。

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [126] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: 论文研究了潜在空间融合方法在预测抑郁症状中的表现，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 改善心理健康数据的早期检测和个性化干预方法。

Method: 使用BRIGHTEN临床试验数据，比较早期融合（随机森林）和潜在空间融合（组合模型）。

Result: 组合模型在所有设置中表现最佳，MSE更低（0.4985 vs. 0.5305），R2更高（0.4695 vs. 0.4356）。

Conclusion: 潜在空间融合是处理多模态心理健康数据的有效方法，未来需探索模型可解释性和个体化预测。

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [127] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: 论文提出Predictive Representativity（PR）框架，用于公平性审计，重点关注结果层面的公平性，而非数据集构成。通过皮肤病学案例研究，发现AI模型在深色皮肤人群中表现较差，强调公平性需动态评估。


<details>
  <summary>Details</summary>
Motivation: AI在医疗决策中的应用日益增多，但算法偏见和不公平结果问题突出，尤其是对历史上边缘化群体。研究旨在通过PR框架解决这一问题。

Method: 使用HAM10000数据集和哥伦比亚的BOSQUE测试集，评估AI皮肤癌分类器的性能，分析不同皮肤光型间的表现差异。

Result: 研究发现，尽管数据集采样比例均衡，AI模型在深色皮肤人群中表现显著较差。

Conclusion: PR框架为公平性审计提供了可扩展工具，强调需动态评估公平性，并提出External Transportability Criterion以量化公平性泛化。

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [128] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: 本文研究了两层神经网络的训练解，揭示了其隐藏层平滑激活函数的机制，包括泰勒展开、严格节点偏序、平滑样条实现和平滑连续性限制，证明了通用逼近性并丰富了逼近理论。


<details>
  <summary>Details</summary>
Motivation: 理解通过反向传播算法获得的两层神经网络训练解，特别是隐藏层使用平滑激活函数（如Sigmoid）时的机制，揭示解空间的“黑箱”特性。

Method: 采用泰勒级数展开、严格节点偏序、平滑样条实现和平滑连续性限制四种机制，分析并验证解的性质。

Result: 证明了任意输入维度的通用逼近性，并通过实验验证，解空间的“黑箱”特性被揭示，同时丰富了逼近理论。

Conclusion: 研究揭示了两层神经网络解空间的机制，证明了其通用逼近性，为逼近理论提供了新的证明方法。

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [129] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 提出了一种名为FBE的方法，通过约束极端特征提升OOD检测性能，实验表明其在ImageNet-1k和CIFAR-10上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于距离的OOD检测方法因极端特征导致性能受限，需改进。

Method: 使用数据集的统计特征识别并约束极端特征，增大分布内外样本的距离。

Result: 在ImageNet-1k和CIFAR-10上达到最优性能。

Conclusion: FBE方法简单有效，显著提升了OOD检测能力。

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [130] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 论文提出了一种基于聚类的激活模式压缩框架，通过将相似的激活模式分组为少量代表性聚类，高效预测和利用LLMs的激活稀疏性，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的激活稀疏性为降低计算成本提供了机会，但直接预测神经元级别的激活模式计算成本高。

Method: 提出聚类框架，将相似激活模式分组为少量代表性聚类，通过预测聚类分配而非单个神经元状态来高效推断激活模式。

Result: 聚类精度达79.34%，困惑度（PPL）最低为12.49，表明在保持模型质量的同时显著降低计算开销。

Conclusion: 该聚类框架为未来激活模式预测研究奠定了基础，有望提升大规模语言模型的高效推断能力。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [131] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的鲁棒且可解释的波束对准引擎（BAE），用于毫米波MIMO系统，通过数字孪生和迁移学习减少数据需求，利用SHAP和DkNN提高透明度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决毫米波系统中深度学习波束对准的高数据需求、硬件限制、缺乏可解释性及对抗攻击脆弱性问题。

Method: 使用数字孪生生成合成数据，结合迁移学习微调模型；利用SHAP特征重要性排名和DkNN异常检测提高透明度和鲁棒性。

Result: 减少70%真实数据需求、62%波束训练开销，异常检测鲁棒性提升8.5倍，接近最优频谱效率。

Conclusion: 该框架在减少开销的同时，实现了透明且鲁棒的波束对准，适用于6G毫米波系统。

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [132] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了一种半监督联邦学习框架SSFL-DCSL，通过双对比损失和软标签解决数据分布不均和标签稀缺问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统监督深度学习方法需要大量标注数据且数据分布不均，标注成本高，影响模型性能。

Method: 结合半监督学习和联邦学习，设计样本加权函数和双对比损失，通过原型共享知识。

Result: 在仅10%标注数据的挑战性任务中，准确率提升1.15%至7.85%。

Conclusion: SSFL-DCSL有效解决了数据分布不均和标签稀缺问题，提升了模型性能。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [133] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: 论文提出B4模型，通过联合嵌入价格序列和外部信号到共享潜在空间，捕捉牛熊动态，提升市场趋势预测。


<details>
  <summary>Details</summary>
Motivation: 金融市场动态复杂，受历史价格和外部叙事影响，现有模型难以处理数据异质性和投资者偏见。

Method: 提出B4模型，结合时间价格序列和外部信号，通过惯性配对和双竞争机制捕捉行为差异。

Result: 实验显示B4在预测市场趋势上表现优异，并提供偏见与行为互动的可解释性。

Conclusion: B4模型有效捕捉市场异质性和行为动态，为趋势预测和机制理解提供新视角。

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [134] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: 提出了一种名为LaCache的KV缓存优化方法，用于提升大语言模型的长距离建模能力和连续生成效率。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度增加，LLMs中的KV对数量激增，导致效率瓶颈，需要一种无需训练的方法来优化缓存。

Method: LaCache采用阶梯形KV缓存模式和迭代压缩机制，分别提升长距离依赖捕捉和连续生成能力。

Result: 实验证明LaCache在多种任务和模型上均能有效增强长距离能力。

Conclusion: LaCache是一种高效且无需训练的优化方法，适用于LLMs的长距离建模和连续生成。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [135] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: 开发了一种基于深度学习的辅助设备，用于聋人或听力受损者的实时声音定位与识别，包含三个主要组件，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 填补当前研究中针对弱势群体的技术空白，利用机器学习技术提升辅助设备的实用性。

Method: 1. JerryNet：自定义CNN架构，用于声音方向定位；2. 基于CLAP模型的音频分类；3. 结合音频、视觉和文本的多模态集成模型。

Result: JerryNet方向定位精度91.1%，CLAP模型在自定义和AudioSet数据集上分别达到98.5%和95%准确率，多模态模型的cIoU为0.892。

Conclusion: 该研究为新一代辅助设备的发展奠定了基础，具有广阔的应用前景。

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [136] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: 提出了一种交互式学习框架，通过非线性效用聚合和几何感知查询选择解决模式挖掘中的模式爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 解决模式挖掘中的模式爆炸问题，提高用户偏好建模的准确性和效率。

Method: 使用Choquet积分建模用户偏好，结合几何感知的查询选择和分支定界策略。

Result: 在UCI数据集上实验表明，该方法优于现有方法（如ChoquetRank），以更少的用户交互实现更高的排名准确性。

Conclusion: 提出的交互式学习框架有效解决了模式爆炸问题，提升了模式挖掘的效率和准确性。

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [137] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 该研究提出了一种基于AI的框架，用于计算绿色氢产量和选址适宜性指数，结合了多变量聚类、机器学习分类器和SHAP算法，为数据稀缺地区提供了可复制的决策工具。


<details>
  <summary>Details</summary>
Motivation: 随着各国寻求化石燃料的可持续替代品，绿色氢成为脱碳的重要途径，但选址需综合考虑复杂因素且缺乏直接数据。

Method: 研究采用多阶段AI框架，包括无监督多变量聚类、监督机器学习分类器和SHAP算法，基于气象、地形和时间数据集进行训练。

Result: 模型预测准确率达98%，显示水接近度、海拔和季节变化是阿曼绿色氢选址的最关键因素。

Conclusion: 该研究为缺乏实地数据的国家提供了客观、可复制的选址工具，支持绿色氢基础设施规划和决策。

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [138] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种新的POGM方法，通过梯度轨迹匹配解决领域泛化问题，避免了梯度波动和高计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在梯度匹配中存在梯度波动和高计算开销的问题，需要一种更高效且稳定的解决方案。

Method: 利用梯度轨迹作为数据，在元学习器中独立训练，最大化梯度内积同时限制梯度偏离经验风险最小化轨迹。

Result: 在DomainBed数据集上表现出竞争力，同时实现了计算效率。

Conclusion: POGM方法有效解决了梯度波动和计算开销问题，提升了领域泛化性能。

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [139] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: NanoPro-3M是最大的纳米材料-蛋白质相互作用数据集，结合NanoProFormer模型，通过多模态学习显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决纳米材料在医学和环境科学中应用的关键问题，即理解其与蛋白质的相互作用，但现有模型因数据集有限和泛化能力不足而受限。

Method: 提出NanoPro-3M数据集（320万样本，3.7万种蛋白质），并开发NanoProFormer模型，通过多模态表示学习预测亲和力。

Result: 多模态模型显著优于单模态方法，能处理缺失特征和新样本，并识别关键决定因素。

Conclusion: 为高性能、泛化的纳米材料-蛋白质相互作用预测奠定基础，减少实验依赖并加速应用。

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [140] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LDM是一种新的线性降维方法，通过线性近似扩散映射核，结合了几何直觉与计算效率。在合成和真实数据集上表现优于PCA，尤其适用于高维流形结构。


<details>
  <summary>Details</summary>
Motivation: 结合非线性方法的几何优势与线性方法的计算效率和可解释性，提出一种新的线性降维方法。

Method: 通过线性近似扩散映射核构建LDM，并在合成和真实数据集上进行实验验证。

Result: LDM在流形结构数据上优于PCA，而PCA在噪声或方差主导的场景中更优。LDM的核矩阵支持NMF，便于潜在结构发现。

Conclusion: LDM是一种有潜力的线性降维技术，兼具理论和实践扩展价值。

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [141] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为UFO的强化学习方法，利用单轮反馈提升大型推理模型在多轮问题解决中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在单轮训练中表现良好，但在多轮推理和基于反馈的答案修正上表现不佳。

Method: 引入UFO（Unary Feedback as Observation），利用简单的单轮用户反馈进行多轮强化学习训练。

Result: 实验显示，UFO在保持单轮性能的同时，将多轮推理准确率提升高达14%。

Conclusion: UFO方法有效提升了模型在多轮问题解决中的反馈反应能力，同时设计了奖励机制以减少修正轮次并鼓励多样性推理。

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [142] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: FedStrategist是一个基于元学习的框架，通过动态选择最优聚合规则来防御联邦学习中的模型中毒攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性使其易受模型中毒攻击，现有静态防御方法效果有限。

Method: 设计了一个轻量级上下文老虎机代理，实时选择最优聚合规则。

Result: 实验表明，自适应代理能在多样场景中学习到优于静态规则的策略。

Conclusion: FedStrategist提供了一种实用且可分析的方法，平衡性能与安全性。

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [143] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: 论文提出了一种提升深度伪造检测中个体公平性的通用框架，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的滥用对特定群体存在偏见，现有研究多关注群体公平性，个体公平性未被充分探索。

Method: 提出了首个可集成到现有深度伪造检测器中的通用框架，以增强个体公平性和泛化能力。

Result: 在多个主流深度伪造数据集上的实验表明，该方法显著提升了个体公平性，同时保持检测性能。

Conclusion: 该框架填补了深度伪造检测中个体公平性的研究空白，并在实验中表现优于现有方法。

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [144] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: 该研究开发了四种机器学习模型，用于预测环形几何中的临界热通量（CHF），显著优于传统经验方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测CHF对核反应堆安全分析至关重要，传统方法存在局限性，机器学习提供了更精确的解决方案。

Method: 使用三种经验模型作为基准，开发并验证了四种混合机器学习模型，利用实验数据训练和测试。

Result: 机器学习模型的平均相对误差低于3.5%，显著优于传统方法的26%误差。

Conclusion: 混合机器学习模型在环形几何中表现出色，为CHF预测提供了更可靠的工具。

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [145] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: 论文探讨了如何通过共轭梯度近似影响函数过滤噪声数据，以提升语言模型微调效果，实验显示过滤后准确率提升1.5%。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据集通常存在噪声，影响模型性能，因此需要有效方法检测和过滤有害数据。

Method: 使用共轭梯度近似影响函数过滤数据集，并结合梯度相似性检测有用数据。

Result: 实验显示过滤10%数据后准确率提升1.5%，梯度相似性在检测有用数据上优于影响函数。

Conclusion: 局部曲率对检测有害数据重要，但对识别有用数据影响较小。

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [146] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: Solo Connection是一种新的参数高效微调方法，通过调整解码器块级表示而非单个权重矩阵，性能优于LoRA，并显著减少可训练参数。


<details>
  <summary>Details</summary>
Motivation: 探索更高效的微调方法，减少参数数量，同时提升模型在新任务上的表现。

Method: 引入可训练的线性变换，逐步插值零向量和任务特定表示，实现平滑稳定适应。

Result: Solo Connection在自然语言生成任务上优于LoRA，参数减少59%（相比LoRA）和99%（相比全微调）。

Conclusion: Solo Connection为大规模语言模型提供了一种高效、稳定的微调方法，尤其适用于深层架构。

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [147] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: INCADET是一种针对实时网络攻击检测的增量因果图学习框架，通过动态更新因果图来适应系统行为变化，显著提高了检测准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 网络攻击对关键基础设施的威胁日益严重，传统方法因数据高方差和类别不平衡导致误报率高，且静态因果图方法无法适应实时动态变化。

Method: INCADET包含三个模块：早期症状检测、增量因果图学习和因果图分类，通过动态更新因果图并结合图卷积网络进行分类。

Result: 在真实数据集上，INCADET在准确性和适应性上优于静态因果图和深度时序基线方法。

Conclusion: INCADET为实时网络攻击检测提供了一种高效且适应性强的解决方案。

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [148] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: 本文分析了简单测试时缩放方法，发现其主要通过限制最大长度实现缩放，而通过追加“Wait”进行放大则会导致不一致性。o1类模型通过强化学习自然放大测试时计算，性能超越峰值。


<details>
  <summary>Details</summary>
Motivation: 研究简单测试时缩放方法的有效性，探讨其与o1类模型在测试时计算缩放上的差异。

Method: 分析简单测试时缩放方法，包括限制最大长度和追加“Wait”两种方式，并与o1类模型的自然缩放行为对比。

Result: 限制最大长度的缩放效果显著，而追加“Wait”会导致模型振荡；o1类模型通过强化学习自然放大计算，性能提升。

Conclusion: 简单测试时缩放主要依赖限制长度，而o1类模型的自然缩放能解锁更高性能，目标应超越简单复制缩放行为。

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [149] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: 提出了一种结合强化学习和干预模型的方法，用于解决大规模随机优化问题，特别适用于供应链中的多源多周期库存管理问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模随机优化问题中探索解空间的效率问题，特别是在供应链优化中的复杂约束条件下。

Method: 利用预训练的深度学习模型模拟和组合随机过程，并通过深度强化学习模型学习和预测供应链过程，同时引入约束协调机制。

Result: 将复杂的供应链过程分解为可扩展和可组合的深度学习模块，提高了在大规模实际数据集上的性能。

Conclusion: 该方法在供应链优化中表现出色，但仍需进一步研究以验证其有效性。

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [150] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: ReDiSC是一种基于重参数化掩码扩散模型的图神经网络方法，用于结构化节点分类，解决了传统方法中节点标签条件独立性假设的问题，并在性能和可扩展性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统GNN方法假设节点标签条件独立，这与图中节点标签实际相关性的直观观察相矛盾。ReDiSC旨在通过建模节点标签的联合分布来解决这一问题。

Method: ReDiSC使用重参数化掩码扩散模型估计节点标签的联合分布，并通过变分EM框架进行学习。理论分析显示其在E步的效率优势，并与GNN和标签传播方法明确关联。

Result: ReDiSC在多种图上表现优于或与现有最佳方法相当，尤其在大规模数据集上展现出显著的可扩展性优势。

Conclusion: ReDiSC为结构化节点分类提供了一种高效且可扩展的解决方案，解决了传统方法的局限性。

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [151] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: 本文提出了一种联邦强化学习框架FRL-EH，解决了局部环境统计异质性问题，通过新颖的全局目标函数优化策略，确保在异质环境中的鲁棒性。提出的FedRQ算法在理论上收敛于最优策略，并扩展到连续状态空间。实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 研究联邦强化学习在局部环境异质性下的协作学习问题，旨在保护隐私的同时优化全局策略。

Method: 提出FRL-EH框架和FedRQ算法，通过全局目标函数优化策略，并扩展到连续状态空间。

Result: 理论证明FedRQ收敛于最优策略，实验显示其在异质环境中优于现有方法。

Conclusion: FRL-EH和FedRQ在联邦强化学习中表现出高效性和鲁棒性，适用于复杂异质环境。

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [152] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: 论文提出了一种称为“glitches”的新不可靠行为，它可能显著影响具有陡峭决策边界的AI模型的可靠性。作者提供了glitches的正式定义，并通过实验证明其广泛存在，同时提出了一种基于MILP的算法来检测GBDT模型中的glitches。


<details>
  <summary>Details</summary>
Motivation: 确保机器学习模型的决策可信且可靠，输出在相似输入下一致。

Method: 提出glitches的正式定义，并通过实验验证其存在；开发基于MILP的算法检测GBDT模型中的glitches。

Result: 证明glitches广泛存在，且检测问题对深度为4的树集成是NP完全的；算法在GBDT基准测试中有效且可行。

Conclusion: glitches是模型不一致性的潜在来源，检测算法有助于提升模型可靠性。

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [153] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: 本文提出了一种名为GenDD的条件生成框架，用于知识蒸馏（KD），解决了高维优化和缺乏标签语义监督的问题，通过Split Tokenization和Distribution Contraction技术，在无监督和监督设置下均取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏通常面临高维优化和缺乏标签语义监督的挑战，本文旨在通过生成式方法解决这些问题。

Method: 提出GenDD框架，结合Split Tokenization实现无监督KD，并通过Distribution Contraction引入标签监督。

Result: 在无监督设置下，GenDD在ImageNet验证集上显著优于KL基线16.29%；在监督设置下，ResNet-50在600轮训练后达到82.28%的top-1准确率。

Conclusion: GenDD框架在知识蒸馏中表现出色，尤其在无监督和监督场景下均取得显著成果。

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [154] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: 提出了一种基于结构感知的度量函数SDSC，用于时间序列自监督表示学习，解决了传统距离目标（如MSE）在信号语义对齐和可解释性上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法（如MSE）对振幅敏感、对波形极性不敏感且尺度无界，这些问题阻碍了语义对齐和可解释性。

Method: SDSC通过量化时间信号的结构一致性（基于Dice相似系数），并提出了可微分的Heaviside函数近似和混合损失（结合SDSC与MSE）。

Result: 实验表明，SDSC在预测和分类任务中表现优于或与MSE相当，尤其在领域内和低资源场景中。

Conclusion: 结构感知度量（如SDSC）能提升信号表示的语义质量，可作为传统距离方法的有效替代。

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [155] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: 论文提出了一种基于正样本-未标记样本（PU）学习的方法，用于在观察性研究中识别控制组，从而估计平均处理效应（ATE）。


<details>
  <summary>Details</summary>
Motivation: 在观察性研究中，缺乏明确标记的控制组是一个常见挑战，而随机实验可能难以实施或成本高昂。

Method: 采用PU学习框架，仅利用已处理的（正）样本从未标记样本中高置信度地识别控制组，并通过模拟和真实农业数据验证方法。

Result: PU学习能成功从未标记数据中识别控制组，并估计接近真实值的ATE。

Conclusion: 该方法为观察性因果推断提供了新工具，尤其适用于难以进行随机实验的领域。

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [156] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于最大因果熵的逆强化学习方法，用于无限时域平稳平均场博弈，通过再生核希尔伯特空间建模未知奖励函数，解决了现有方法中奖励函数线性组合的限制。


<details>
  <summary>Details</summary>
Motivation: 现有逆强化学习方法在平均场博弈中通常限制奖励函数为固定基函数的线性组合，且多基于有限时域，无法捕捉非线性结构和长期行为。

Method: 采用拉格朗日松弛技术将问题转化为无约束对数似然最大化，并通过梯度上升算法求解；证明了相关软贝尔曼算子的Fréchet可微性以确保目标平滑性。

Result: 在平均场交通路径博弈实验中，方法能准确恢复专家行为，验证了其有效性。

Conclusion: 该方法突破了奖励函数线性组合的限制，适用于无限时域问题，为非线性奖励结构的推断提供了新思路。

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [157] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: 论文提出SST-CL框架，结合空间-时间变换器和课程学习，解决EEG情感识别中的非平稳空间-时间模式整合和动态情感强度适应问题。


<details>
  <summary>Details</summary>
Motivation: EEG情感识别在脑机通信系统中至关重要，但面临非平稳空间-时间模式整合和动态情感强度变化的挑战。

Method: SST-CL框架包含空间编码器（建模通道间关系）和时间编码器（通过窗口注意力机制捕捉多尺度依赖），并结合强度感知课程学习策略。

Result: 在三个基准数据集上表现优异，消融实验验证了框架各组件和课程学习机制的必要性。

Conclusion: SST-CL框架有效整合空间-时间信息并适应动态情感强度，为EEG情感识别提供了新思路。

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [158] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: 本文追溯了自注意力机制的概念起源，将其视为基于亲和矩阵的通用计算原则的特例，并与无限特征选择（Inf-FS）方法进行了对比。


<details>
  <summary>Details</summary>
Motivation: 揭示自注意力机制与更广泛的基于亲和矩阵的计算范式之间的联系，统一不同领域的机器学习研究。

Method: 通过对比自注意力与Inf-FS，分析亲和矩阵的定义和应用方式。

Result: 自注意力是Inf-FS的单跳特例，两者共享基于成对关系的计算结构。

Conclusion: 自注意力是更广泛的亲和计算范式的一部分，为统一不同模型和任务提供了数学基础。

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [159] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: LPS-GNN是一个高效、低成本的GNN框架，能在单GPU上处理1000亿规模的图数据，并在用户获取场景中提升13.8%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在效率和准确性之间难以平衡，尤其是大规模图中的邻居爆炸问题导致计算和内存需求高。

Method: 提出LPS-GNN框架，设计LPMetis图分区算法和子图增强策略，兼容多种GNN算法。

Result: 在公开和真实数据集上测试，性能提升8.24%至13.89%，优于现有SOTA模型。

Conclusion: LPS-GNN成功部署于腾讯平台，展示了高效、灵活和可扩展的优势。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [160] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: 提出了一种结合Transformer-GAN和MILET的新框架，用于无人机飞行状态分类，显著提升了准确率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分类方法在动态无人机环境中缺乏鲁棒性和泛化能力，而现有SOTA模型需要大数据集和高计算成本。

Method: 集成Transformer编码器捕捉长期时间依赖，GAN模块生成合成样本增强数据，MILET聚焦关键输入段以减少噪声和计算开销。

Result: 在DroneDetect和DroneRF数据集上分别达到96.5%和98.6%的准确率，优于其他SOTA方法。

Conclusion: 该框架在计算效率和泛化能力上表现优异，适合资源受限环境中的实时部署。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [161] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 论文提出GUI-G²，一种基于高斯分布的奖励框架，用于GUI交互任务，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法使用稀疏的二元奖励，忽略了空间交互的连续性，而人类点击行为自然形成高斯分布。

Method: 引入GUI-G²，结合高斯点奖励和覆盖奖励，通过自适应方差机制处理不同元素尺度。

Result: 在多个基准测试中，GUI-G²显著优于UI-TARS-72B，最高提升24.7%。

Conclusion: 连续建模提高了对界面变化的鲁棒性和泛化能力，为GUI交互任务提供了新范式。

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [162] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: 论文提出了一种多项式时间确定性算法，用于近似计算k-子空间中位数，其近似因子为√d，运行时间为输入大小的多项式。


<details>
  <summary>Details</summary>
Motivation: 传统的k-PCA方法对噪声和异常值敏感，而中位数子空间更具鲁棒性，但计算困难。本文旨在解决这一问题。

Method: 提出了一种新的多项式时间确定性算法，避免了指数级的运行时间和近似因子。

Result: 算法在近似因子√d和多项式运行时间内有效解决了k-子空间中位数问题。

Conclusion: 该技术有望应用于其他相关问题，如不同范数距离的计算和异常值处理。

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [163] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为Rec-AD的高效计算框架，结合Tensor Train分解和深度学习推荐模型，用于提升智能电网中虚假数据注入攻击（FDIA）的检测效率。


<details>
  <summary>Details</summary>
Motivation: 智能电网中虚假数据注入攻击检测的深度学习模型面临计算和内存负担问题，尤其是在大规模工业数据集中。

Method: 通过嵌入压缩、索引重排序优化数据访问，以及减少内存通信开销的流水线训练机制，提升训练和推理效率。

Result: 实验结果表明，Rec-AD显著提高了计算吞吐量和实时检测性能，缩小了攻击窗口并增加了攻击者成本。

Conclusion: Rec-AD增强了边缘计算能力和可扩展性，为智能电网安全提供了强有力的技术支持。

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [164] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为AD-GCL的新型图对比学习框架，旨在解决现有方法在结构不平衡网络中对尾部异常节点检测不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法在异常检测中过度关注整体性能，而忽略了结构不平衡的鲁棒性，尤其是对低度异常节点的检测不足，影响了算法的安全性和适用性。

Method: AD-GCL通过邻居修剪策略过滤噪声边，并通过异常引导的邻居补全扩大尾部节点的感知范围，同时引入原始图和增强图的内外一致性损失以增强表示。

Result: 在多个数据集上的实验表明，AD-GCL在整体、头部和尾部节点的异常检测中均表现出全面优势。

Conclusion: AD-GCL有效提升了图异常检测在结构不平衡网络中的鲁棒性，尤其是在尾部异常节点的检测上表现突出。

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [165] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: 提出了一种名为GCC-Spam的新型垃圾文本检测框架，通过字符相似性网络、对比学习和GAN生成伪样本，解决了对抗策略和标注数据稀缺问题，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 互联网垃圾文本的指数增长需要强大的检测机制，以应对信息泄露和社会不稳定等风险。

Method: 1. 字符相似性网络捕捉拼写和语音特征；2. 对比学习优化潜在空间距离；3. GAN生成伪样本缓解数据稀缺。

Result: 在真实数据集上表现优于基线方法，检测率更高且所需标注数据更少。

Conclusion: GCC-Spam框架有效解决了垃圾文本检测中的对抗策略和数据稀缺问题，提升了检测性能。

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [166] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: 提出了一种名为CPAC的架构，结合VAE-GAN，通过原型注意力机制改善类别聚类和潜在空间结构，显著提升了欺诈检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GANs、VAEs）在处理类别不平衡时容易导致分类器过度自信和潜在聚类分离不佳，限制了实际检测效果。

Method: 提出CPAC架构，结合原型注意力机制和VAE-GAN，改善潜在空间结构，并与传统过采样方法和生成模型对比。

Result: CPAC在F1-score（93.14%）和召回率（90.18%）上表现优异，且潜在聚类分离效果更好。

Conclusion: CPAC通过分类器引导的潜在空间优化，显著提升了欺诈检测性能，并提供了对表示学习的深入见解。

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [167] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: 论文探讨了实时生成AI（RTGen）工作负载在异构SoC上的调度问题，通过评估不同调度策略对性能的影响，强调了动态异构调度的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着生成AI模型（如LLMs）在实时多模型应用中的集成，异构SoC的调度复杂性和性能影响尚未充分研究。

Method: 在AMD Ryzen AI平台上构建多模型场景，分析五种调度策略对实时指标和LLM性能的影响。

Result: 调度决策显著影响性能（如平均41.7%的截止时间违规率差异），需动态异构调度策略。

Conclusion: 动态异构调度是实现高性能RTGen应用的关键。

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>


### [168] [LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4](https://arxiv.org/abs/2507.14722)
*Matěj Kripner,Michal Šustr,Milan Straka*

Main category: cs.LG

TL;DR: 论文介绍了LeanTree，一种白盒工具，用于改进自动定理证明（ATP）中的大语言模型（LLM）交互方法。


<details>
  <summary>Details</summary>
Motivation: ATP问题因其庞大的状态和动作空间而具有挑战性，现有的黑盒方法未能充分利用中间证明状态，导致白盒方法发展滞后。

Method: LeanTree包括一个基于Lean 4语言的工具，将复杂证明状态分解为更简单的独立分支，并提供这些分解状态的数据库。

Result: 初步结果表明，白盒方法在某些情况下优于黑盒方法。

Conclusion: LeanTree通过提供更丰富的训练数据、并行搜索和错误反馈，为ATP中的白盒方法提供了显著优势。

Abstract: Automated theorem proving (ATP) has been a classical problem in artificial
intelligence since its inception, yet it remains challenging due to its vast
state and action space. Large language models (LLMs) have recently emerged as a
promising heuristic for ATP, but they lack correctness guarantees and thus
require interaction with a proof verifier. Such interactions typically follow
one of two approaches: black-box interaction, which does not utilize
intermediate proof states, or white-box approaches, which allow for incremental
proof construction and examination of intermediate states. While black-box
approaches have directly benefited from recent LLM advances, white-box methods
have comparatively lagged behind. In this paper, we address this gap by
introducing LeanTree, which consists of (i) a tool built in the Lean 4 language
that factorizes complex proof states into simpler, independent branches, and
(ii) a dataset of these factorized intermediate states. Our white-box tooling
offers several advantages over black-box approaches: it simplifies evaluation,
reduces necessary context, generates richer training data, enables parallel
search across multiple states, supports efficient reuse of states, and provides
feedback in case of errors. Our preliminary results hint that white-box
approaches outperform black-box alternatives in some settings.

</details>


### [169] [Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding](https://arxiv.org/abs/2507.14725)
*Anushka Tiwari,Sayantan Pal,Rohini K. Srihari,Kaiyi Ji*

Main category: cs.LG

TL;DR: GRID框架通过任务感知解码和梯度提示选择策略，解决了持续学习中的潜在遗忘和提示内存爆炸问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法假设任务感知推理并维护任务特定提示列表，限制了可扩展性并隐藏了潜在遗忘。

Method: GRID结合任务感知解码机制（利用代表性输入、自动任务识别和约束解码）和梯度提示选择策略（压缩低信息提示为聚合表示）。

Result: 在多个基准测试中，GRID显著提升后向迁移、竞争性前向迁移，并将遗忘任务减少80%，优于现有方法。

Conclusion: GRID是高效、可扩展的持续学习框架，解决了关键限制，适用于大规模任务序列。

Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to
adapt large language models (LLMs) across task sequences. However, most
existing methods assume task-aware inference and maintain a growing list of
task-specific prompts, which limits scalability and hides latent forgetting. In
this work, we introduce GRID, a unified framework that addresses two key
limitations: (1) latent forgetting under task-agnostic inference, and (2)
prompt memory explosion as task sequences grow. GRID integrates a task-aware
decoding mechanism that improves backward transfer by leveraging representative
inputs, automatic task identification, and constrained decoding. Additionally,
we propose a gradient-based prompt selection strategy that compresses less
informative prompts into a single aggregated representation, enabling scalable
and memory-efficient lifelong learning. Extensive experiments across
short-sequence, long-sequence, and negative transfer benchmarks show that GRID
significantly improves backward transfer, achieves competitive forward
transfer, and reduces forgotten tasks by up to 80\%, outperforming
state-of-the-art methods on T5 and Flan-T5 backbones.

</details>


### [170] [Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning](https://arxiv.org/abs/2507.14736)
*Rafał Surdej,Michał Bortkiewicz,Alex Lewandowski,Mateusz Ostaszewski,Clare Lyle*

Main category: cs.LG

TL;DR: 论文研究了可训练有理激活函数在强化学习和持续学习中的表现，发现其灵活性可能导致不稳定性，并提出了一种约束变体以平衡表达性和可塑性。


<details>
  <summary>Details</summary>
Motivation: 探索可训练有理激活函数在动态非平稳环境中的表现，尤其是其对训练稳定性的影响。

Method: 提出一种约束变体的有理激活函数，限制输出缩放以保持适应性。

Result: 实验表明该约束方法在MetaWorld和DMC环境中提高了训练稳定性和性能，并在持续学习任务中平衡了表达性和长期记忆。

Conclusion: 研究揭示了可训练有理激活函数在表达性和可塑性之间的权衡，并提供了设计原则以增强其在动态环境中的鲁棒性。

Abstract: Trainable activation functions, whose parameters are optimized alongside
network weights, offer increased expressivity compared to fixed activation
functions. Specifically, trainable activation functions defined as ratios of
polynomials (rational functions) have been proposed to enhance plasticity in
reinforcement learning. However, their impact on training stability remains
unclear. In this work, we study trainable rational activations in both
reinforcement and continual learning settings. We find that while their
flexibility enhances adaptability, it can also introduce instability, leading
to overestimation in RL and feature collapse in longer continual learning
scenarios. Our main result is demonstrating a trade-off between expressivity
and plasticity in rational activations. To address this, we propose a
constrained variant that structurally limits excessive output scaling while
preserving adaptability. Experiments across MetaWorld and DeepMind Control
Suite (DMC) environments show that our approach improves training stability and
performance. In continual learning benchmarks, including MNIST with reshuffled
labels and Split CIFAR-100, we reveal how different constraints affect the
balance between expressivity and long-term retention. While preliminary
experiments in discrete action domains (e.g., Atari) did not show similar
instability, this suggests that the trade-off is particularly relevant for
continuous control. Together, our findings provide actionable design principles
for robust and adaptable trainable activations in dynamic, non-stationary
environments. Code available at:
https://github.com/special114/rl_rational_plasticity.

</details>


### [171] [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740)
*Andrew Wang,Elisa Nguyen,Runshi Yang,Juhan Bae,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: ASTRA算法通过EKFAC预条件器和Neumann级数迭代，高效近似逆Hessian向量积（iHVP），显著提升训练数据归因（TDA）性能。


<details>
  <summary>Details</summary>
Motivation: 解决梯度基TDA方法中iHVP计算效率低的问题。

Method: 使用EKFAC预条件器改进Neumann级数迭代，提出ASTRA算法。

Result: ASTRA比传统方法更准确、易调参且迭代次数更少。

Conclusion: 提高iHVP近似精度可显著改善TDA性能。

Abstract: Training data attribution (TDA) provides insights into which training data is
responsible for a learned model behavior. Gradient-based TDA methods such as
influence functions and unrolled differentiation both involve a computation
that resembles an inverse Hessian-vector product (iHVP), which is difficult to
approximate efficiently. We introduce an algorithm (ASTRA) which uses the
EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP
approximation for TDA. ASTRA is easy to tune, requires fewer iterations than
Neumann series iterations, and is more accurate than EKFAC-based
approximations. Using ASTRA, we show that improving the accuracy of the iHVP
approximation can significantly improve TDA performance.

</details>


### [172] [Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML](https://arxiv.org/abs/2507.14744)
*Mustafa Cavus,Jan N. van Rijn,Przemysław Biecek*

Main category: cs.LG

TL;DR: 论文提出了一种新框架，通过整合多个近优模型的偏依赖图（PDP）来捕捉解释不确定性，提升可解释AI的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统自动化机器学习系统通常只关注单一最优模型，忽视了解释不确定性，而这对以人为中心的可解释AI至关重要。

Method: 提出Rashomon PDP框架，通过聚合Rashomon集合（一组近优模型）的PDP，生成包含解释变异的综合结果。

Result: 实验表明，Rashomon PDP在多数情况下仅覆盖最佳模型PDP的70%，凸显单一模型解释的局限性。

Conclusion: Rashomon PDP通过补充被忽略的信息，提升了模型解释的可靠性和可信度，尤其适用于高风险领域。

Abstract: Automated machine learning systems efficiently streamline model selection but
often focus on a single best-performing model, overlooking explanation
uncertainty, an essential concern in human centered explainable AI. To address
this, we propose a novel framework that incorporates model multiplicity into
explanation generation by aggregating partial dependence profiles (PDP) from a
set of near optimal models, known as the Rashomon set. The resulting Rashomon
PDP captures interpretive variability and highlights areas of disagreement,
providing users with a richer, uncertainty aware view of feature effects. To
evaluate its usefulness, we introduce two quantitative metrics, the coverage
rate and the mean width of confidence intervals, to evaluate the consistency
between the standard PDP and the proposed Rashomon PDP. Experiments on 35
regression datasets from the OpenML CTR23 benchmark suite show that in most
cases, the Rashomon PDP covers less than 70% of the best model's PDP,
underscoring the limitations of single model explanations. Our findings suggest
that Rashomon PDP improves the reliability and trustworthiness of model
interpretations by adding additional information that would otherwise be
neglected. This is particularly useful in high stakes domains where
transparency and confidence are critical.

</details>


### [173] [Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization](https://arxiv.org/abs/2507.14746)
*Bach Do,Nafeezat A. Ajenifuja,Taiwo A. Adebiyi,Ruda Zhang*

Main category: cs.LG

TL;DR: 论文探讨了高斯过程（GPs）在工程优化中的采样方法，提出了两种采样方法（随机傅里叶特征和路径条件采样），并展示了其在全局敏感性分析和优化中的应用。


<details>
  <summary>Details</summary>
Motivation: 高保真仿真和物理实验成本高，限制了其在全局敏感性分析和优化中的应用，因此需要高效的代理模型（如GPs）来支持不确定性下的决策。

Method: 提出了两种采样方法：随机傅里叶特征和路径条件采样，并详细描述了其实现过程。

Result: 通过数值实验验证了这些采样方法在全局敏感性分析、单目标和多目标优化中的成功应用。

Conclusion: 高斯过程的采样方法为工程优化提供了高效的工具，扩展了其在复杂任务中的应用潜力。

Abstract: High-fidelity simulations and physical experiments are essential for
engineering analysis and design. However, their high cost often limits their
applications in two critical tasks: global sensitivity analysis (GSA) and
optimization. This limitation motivates the common use of Gaussian processes
(GPs) as proxy regression models to provide uncertainty-aware predictions based
on a limited number of high-quality observations. GPs naturally enable
efficient sampling strategies that support informed decision-making under
uncertainty by extracting information from a subset of possible functions for
the model of interest. Despite their popularity in machine learning and
statistics communities, sampling from GPs has received little attention in the
community of engineering optimization. In this paper, we present the
formulation and detailed implementation of two notable sampling methods --
random Fourier features and pathwise conditioning -- for generating posterior
samples from GPs. Alternative approaches are briefly described. Importantly, we
detail how the generated samples can be applied in GSA, single-objective
optimization, and multi-objective optimization. We show successful applications
of these sampling methods through a series of numerical examples.

</details>


### [174] [Pruning Increases Orderedness in Recurrent Computation](https://arxiv.org/abs/2507.14747)
*Yiding Song*

Main category: cs.LG

TL;DR: 研究探讨方向性作为人工神经网络的归纳偏置是否有效，通过剪枝技术诱导方向性，发现其并非学习必需但可能是有利的偏置。


<details>
  <summary>Details</summary>
Motivation: 受生物大脑中循环电路的普遍性启发，研究方向性是否对人工神经网络有帮助。

Method: 提出全连接感知层（数学上等价于权重绑定的循环神经网络），并通过剪枝技术诱导方向性。

Result: 剪枝方案成功诱导神经元间信息流的拓扑排序，且不影响性能。

Conclusion: 方向性并非学习必需，但可能是梯度下降和稀疏化可发现的有利归纳偏置。

Abstract: Inspired by the prevalence of recurrent circuits in biological brains, we
investigate the degree to which directionality is a helpful inductive bias for
artificial neural networks. Taking directionality as topologically-ordered
information flow between neurons, we formalise a perceptron layer with
all-to-all connections (mathematically equivalent to a weight-tied recurrent
neural network) and demonstrate that directionality, a hallmark of modern
feed-forward networks, can be induced rather than hard-wired by applying
appropriate pruning techniques. Across different random seeds our pruning
schemes successfully induce greater topological ordering in information flow
between neurons without compromising performance, suggesting that
directionality is not a prerequisite for learning, but may be an advantageous
inductive bias discoverable by gradient descent and sparsification.

</details>


### [175] [Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning](https://arxiv.org/abs/2507.14748)
*Patrik Reizinger,Bálint Mucsányi,Siyuan Guo,Benjamin Eysenbach,Bernhard Schölkopf,Wieland Brendel*

Main category: cs.LG

TL;DR: 论文研究了自监督特征学习和预训练方法在强化学习（RL）中的应用，特别是基于互信息技能学习（MISL）的方法。通过对比后继特征（CSF）方法，证明了CSF可以线性变换恢复环境的真实特征。


<details>
  <summary>Details</summary>
Motivation: 探讨MISL中表示和互信息参数化的理论作用，填补了RL中表示学习的可识别性理论空白。

Method: 采用对比后继特征（CSF）方法，分析其特征参数化和技能多样性对表示学习的影响。

Result: 理论证明CSF可以线性变换恢复环境的真实特征，并通过实验在MuJoCo和DeepMind Control中验证。

Conclusion: CSF为RL中的表示学习提供了首个可识别性保证，并揭示了不同互信息目标和熵正则化的潜在问题。

Abstract: Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.

</details>


### [176] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: CXR-TFT是一种多模态框架，结合稀疏时间CXR影像、放射报告和高频临床数据，预测ICU患者CXR结果的动态变化，提前12小时发现异常。


<details>
  <summary>Details</summary>
Motivation: ICU患者需要及时监测和干预，但现有CXR分析工具无法捕捉时间动态，限制了其效用。

Method: CXR-TFT通过视觉编码器生成潜在嵌入，并与每小时临床数据对齐，利用Transformer模型预测未来CXR结果。

Result: 在20,000名ICU患者的回顾性研究中，CXR-TFT能提前12小时高精度预测异常CXR结果。

Conclusion: CXR-TFT提供了时间分辨率高的预测能力，有望改善急性呼吸窘迫综合征等时间敏感病症的管理。

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [177] [Rethinking Memorization Measures and their Implications in Large Language Models](https://arxiv.org/abs/2507.14777)
*Bishwamittra Ghosh,Soumi Das,Qinyuan Wu,Mohammad Aflah Khan,Krishna P. Gummadi,Evimaria Terzi,Deepak Garg*

Main category: cs.LG

TL;DR: 本文探讨了LLMs中的记忆化是否可以通过最优学习避免，以及隐私威胁是否被夸大。提出了上下文记忆化概念，并与现有记忆化测量方法对比。实验表明，最优学习无法完全避免记忆化，且不同测量方法结果不一致。


<details>
  <summary>Details</summary>
Motivation: 研究记忆化在LLMs中的必要性及其对隐私的威胁，澄清记忆化是否被过度夸大。

Method: 重新审视基于回忆和反事实的记忆化测量方法，提出上下文记忆化概念，并在18种LLMs上进行实验。

Result: 最优学习无法完全避免记忆化；不同测量方法结果不一致；改进学习会减少上下文和反事实记忆化，但增加基于回忆的记忆化。

Conclusion: 部分记忆化不可避免，现有基于回忆的记忆化报告可能夸大隐私威胁。

Abstract: Concerned with privacy threats, memorization in LLMs is often seen as
undesirable, specifically for learning. In this paper, we study whether
memorization can be avoided when optimally learning a language, and whether the
privacy threat posed by memorization is exaggerated or not. To this end, we
re-examine existing privacy-focused measures of memorization, namely
recollection-based and counterfactual memorization, along with a newly proposed
contextual memorization.
  Relating memorization to local over-fitting during learning, contextual
memorization aims to disentangle memorization from the contextual learning
ability of LLMs. Informally, a string is contextually memorized if its
recollection due to training exceeds the optimal contextual recollection, a
learned threshold denoting the best contextual learning without training.
Conceptually, contextual recollection avoids the fallacy of recollection-based
memorization, where any form of high recollection is a sign of memorization.
Theoretically, contextual memorization relates to counterfactual memorization,
but imposes stronger conditions. Memorization measures differ in outcomes and
information requirements.
  Experimenting on 18 LLMs from 6 families and multiple formal languages of
different entropy, we show that (a) memorization measures disagree on
memorization order of varying frequent strings, (b) optimal learning of a
language cannot avoid partial memorization of training strings, and (c)
improved learning decreases contextual and counterfactual memorization but
increases recollection-based memorization. Finally, (d) we revisit existing
reports of memorized strings by recollection that neither pose a privacy threat
nor are contextually or counterfactually memorized.

</details>


### [178] [Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards](https://arxiv.org/abs/2507.14783)
*Derek Li,Jiaming Zhou,Amirreza Kazemi,Qianyi Sun,Abbas Ghaddar,Mohammad Ali Alomrani,Liheng Ma,Yu Luo,Dong Li,Feng Wen,Jianye Hao,Mark Coates,Yingxue Zhang*

Main category: cs.LG

TL;DR: Omni-Think是一个统一的强化学习框架，通过结合规则奖励和生成偏好信号提升LLM性能，课程学习显著优于联合训练和模型合并。


<details>
  <summary>Details</summary>
Motivation: 解决监督微调（SFT）在泛化能力上的不足，避免记忆化倾向，提升LLM在多任务中的表现。

Method: 引入Omni-Think框架，结合规则奖励和LLM-as-a-Judge生成的偏好信号，采用课程学习策略从结构化任务到开放任务逐步训练。

Result: 课程学习在四个领域中性能提升5.2%（相比联合训练）和9.1%（相比模型合并）。

Conclusion: 任务感知采样和混合监督对扩展基于RL的后训练方法至关重要，课程学习显著提升LLM性能。

Abstract: The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce Omni-Think,
a unified reinforcement learning (RL) framework that enhances LLM performance
across diverse tasks by combining rule-based verifiable rewards with generative
preference signals via LLM-as-a-Judge evaluations. Our approach enables
consistent optimization across task types and scales RL-based training to
subjective domains. We further investigate training strategies, demonstrating
that a curriculum-based progression that orders tasks from structured to
open-ended improves performance and reduces forgetting. Experimental results
across four domains reveal that curriculum learning improves performance by
5.2\% over joint training and 9.1\% over model merging. These results highlight
the importance of task-aware sampling and hybrid supervision in scaling
RL-based post-training for general-purpose LLMs.

</details>


### [179] [Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs](https://arxiv.org/abs/2507.14785)
*Erfan Pirmorad*

Main category: cs.LG

TL;DR: 论文探讨了利用大型语言模型（LLMs）对金融知识图谱中的局部子图进行推理，以检测洗钱行为。


<details>
  <summary>Details</summary>
Motivation: 洗钱行为涉及的实体复杂且相互关联，需要基于图结构数据的推理能力。

Method: 提出了一种轻量级流程，提取感兴趣实体的k跳邻域，将其序列化为结构化文本，并通过少量示例提示LLM评估可疑性并生成解释。

Result: 在合成的反洗钱（AML）场景中，LLMs能够模拟分析师逻辑，识别危险信号并提供连贯解释。

Conclusion: 研究表明LLM在图推理中具有潜力，为可解释的语言驱动金融犯罪分析奠定了基础。

Abstract: The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.

</details>


### [180] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: 论文将等变性网络理论扩展到时间参数化的序列变换（如RNN），提出流等变性模型，显著提升了训练速度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有等变性网络仅适用于静态变换和前馈网络，无法处理时间参数化的序列变换（如视觉运动），限制了其在序列模型中的应用。

Method: 扩展等变性理论到时间参数化的流（如视觉运动），提出流等变性RNN模型，确保隐藏状态对动态刺激具有几何结构化的变换。

Result: 流等变性模型在训练速度、长度泛化和速度泛化方面显著优于非等变性模型，适用于下一步预测和序列分类任务。

Conclusion: 该研究为构建尊重时间参数化对称性的序列模型迈出了第一步，具有重要的理论和应用价值。

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [181] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: 研究发现语言模型可以通过语义无关的数据传递行为特征，称为“潜意识学习”。即使过滤掉相关数据，学生模型仍能从教师模型生成的数据中学习到特征。这种现象在不同任务（如代码或推理痕迹）中均存在，但在不同基础模型间不适用。理论证明和实验表明这是神经网络的普遍现象，可能对AI开发带来潜在风险。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型是否能够通过看似无关的数据传递行为特征，揭示潜在的学习机制及其对AI开发的潜在影响。

Method: 通过教师模型生成仅包含数字序列的数据集，训练学生模型观察其是否学习到教师模型的行为特征。实验扩展到代码和推理痕迹数据，并验证不同基础模型间的效果。

Result: 学生模型确实能从语义无关的数据中学习到教师模型的行为特征，即使数据经过过滤。这种现象在不同任务中普遍存在，但在不同基础模型间不适用。

Conclusion: 潜意识学习是神经网络的普遍现象，可能对AI开发带来风险，尤其是在蒸馏过程中可能传播非预期特征。

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [182] [Benchmarking Foundation Models with Multimodal Public Electronic Health Records](https://arxiv.org/abs/2507.14824)
*Kunyu Yu,Rui Yang,Jingchi Liao,Siqi Li,Huitao Li,Irene Li,Yifan Peng,Rishikesan Kamaleswaran,Nan Liu*

Main category: cs.LG

TL;DR: 该研究通过MIMIC-IV数据库评估了基础模型在电子健康记录（EHR）处理中的性能、公平性和可解释性，并开发了标准化数据处理流程。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在医疗数据多模态处理中的潜力，支持可信赖的多模态AI系统开发。

Method: 使用MIMIC-IV数据库，系统比较了8种基础模型（单模态和多模态），并开发了标准化数据处理流程。

Result: 多模态数据的结合能显著提升预测性能且不引入额外偏差。

Conclusion: 研究为临床应用中多模态AI系统的开发提供了基准和标准化支持。

Abstract: Foundation models have emerged as a powerful approach for processing
electronic health records (EHRs), offering flexibility to handle diverse
medical data modalities. In this study, we present a comprehensive benchmark
that evaluates the performance, fairness, and interpretability of foundation
models, both as unimodal encoders and as multimodal learners, using the
publicly available MIMIC-IV database. To support consistent and reproducible
evaluation, we developed a standardized data processing pipeline that
harmonizes heterogeneous clinical records into an analysis-ready format. We
systematically compared eight foundation models, encompassing both unimodal and
multimodal models, as well as domain-specific and general-purpose variants. Our
findings demonstrate that incorporating multiple data modalities leads to
consistent improvements in predictive performance without introducing
additional bias. Through this benchmark, we aim to support the development of
effective and trustworthy multimodal artificial intelligence (AI) systems for
real-world clinical applications. Our code is available at
https://github.com/nliulab/MIMIC-Multimodal.

</details>


### [183] [eMargin: Revisiting Contrastive Learning with Margin-Based Separation](https://arxiv.org/abs/2507.14828)
*Abdul-Kazeem Shamba,Kerstin Bach,Gavin Taylor*

Main category: cs.LG

TL;DR: 研究在对比学习中引入自适应边界（eMargin）对时间序列表示学习的影响，发现其在无监督聚类中表现优异，但在下游分类任务中效果不佳。


<details>
  <summary>Details</summary>
Motivation: 探索自适应边界是否能改善时间序列中相邻但不相似时间步的分离，并提升下游任务性能。

Method: 在对比损失函数中引入自适应边界（eMargin），基于预设相似度阈值调整，并在三个基准数据集上评估聚类和分类性能。

Result: eMargin在无监督聚类指标上优于基线，但在下游分类任务中表现不佳。

Conclusion: 无监督聚类的高分并不一定意味着嵌入在下游任务中有效，需进一步优化自适应边界的设计。

Abstract: We revisit previous contrastive learning frameworks to investigate the effect
of introducing an adaptive margin into the contrastive loss function for time
series representation learning. Specifically, we explore whether an adaptive
margin (eMargin), adjusted based on a predefined similarity threshold, can
improve the separation between adjacent but dissimilar time steps and
subsequently lead to better performance in downstream tasks. Our study
evaluates the impact of this modification on clustering performance and
classification in three benchmark datasets. Our findings, however, indicate
that achieving high scores on unsupervised clustering metrics does not
necessarily imply that the learned embeddings are meaningful or effective in
downstream tasks. To be specific, eMargin added to InfoNCE consistently
outperforms state-of-the-art baselines in unsupervised clustering metrics, but
struggles to achieve competitive results in downstream classification with
linear probing. The source code is publicly available at
https://github.com/sfi-norwai/eMargin.

</details>


### [184] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: RLVR能提升AI解决复杂逻辑任务的能力，但研究揭示其可能受限于基础模型的初始概率分布，无法真正扩展推理边界，反而可能限制新解决方案的发现。


<details>
  <summary>Details</summary>
Motivation: 探讨RLVR是否能真正扩展模型的推理能力，还是仅放大已知高奖励输出以提高精度。

Method: 通过理论和实证研究，分析RLVR的局限性，包括其对基础模型支持的依赖和熵-奖励权衡。

Result: RLVR虽能提升pass@1，但支持范围的缩小通常超过扩展，且可能忽略正确但低概率的解。

Conclusion: RLVR在扩展推理能力方面存在潜在限制，未来需结合探索机制或混合策略突破这些限制。

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [185] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: TALE-EHR是一种基于Transformer的框架，通过时间感知注意力机制和预训练语言模型嵌入，解决了电子健康记录（EHR）数据异质性和复杂时间模式的问题，显著提升了疾病进展预测等任务的性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）包含丰富的临床信息，但数据异质性和不规则时间间隔使得建模困难，需要更有效的方法。

Method: 提出TALE-EHR框架，结合时间感知注意力机制和预训练语言模型（LLM）生成的嵌入，以捕捉细粒度时间动态和语义信息。

Result: 在MIMIC-IV和PIC数据集上，TALE-EHR在疾病进展预测等任务中优于现有基线方法。

Conclusion: TALE-EHR通过整合连续时间建模和强语义表示，为EHR分析提供了有效的解决方案。

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [186] [Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems](https://arxiv.org/abs/2507.14850)
*H. M. Sabbir Ahmad,Ehsan Sabouni,Alexander Wasilkoff,Param Budhraja,Zijian Guo,Songyuan Zhang,Chuchu Fan,Christos Cassandras,Wenchao Li*

Main category: cs.LG

TL;DR: 提出了一种基于控制屏障函数（CBFs）的分层多智能体强化学习（HMARL）方法，用于解决多智能体安全关键系统中的安全策略学习问题。


<details>
  <summary>Details</summary>
Motivation: 在多智能体安全关键系统中，每个智能体需要始终满足安全要求，同时与其他智能体协作完成任务。

Method: 采用分层方法，将强化学习问题分解为高层学习联合协作行为，低层学习基于高层策略的安全个体行为。提出了一种基于技能的HMARL-CBF算法。

Result: 在复杂环境中验证了方法的有效性，显著提高了安全性（接近完美的成功率/安全率），同时提升了性能。

Conclusion: 该方法在多智能体安全导航任务中表现出色，优于现有方法。

Abstract: We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.

</details>


### [187] [The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs](https://arxiv.org/abs/2507.14874)
*Ole-Christoffer Granmo,Youmna Abdelwahab,Per-Arne Andersen,Paul F. A. Clarke,Kunal Dumbre,Ylva Grønninsæter,Vojtech Halenka,Runar Helin,Lei Jiao,Ahmed Khalid,Rebekka Omslandseter,Rupsa Saha,Mayur Shende,Xuan Zhang*

Main category: cs.LG

TL;DR: Graph Tsetlin Machine (GraphTM) 是一种用于从图结构输入中学习可解释深度子句的模型，通过消息传递构建嵌套子句，提高可解释性和数据利用率，在多个领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统 Tsetlin Machine (TM) 在处理图结构输入时受限，GraphTM 旨在扩展 TM 的能力，支持序列、网格、关系和多模态输入，同时保持可解释性和高效性。

Method: GraphTM 通过消息传递构建嵌套深度子句，识别子图模式，显著减少子句数量，提升数据利用率。

Result: GraphTM 在图像分类、动作共指追踪、推荐系统和病毒基因组序列分析中表现优异，准确率显著高于对比方法（如卷积 TM 和 GCN），且训练速度更快。

Conclusion: GraphTM 展示了图表示学习和深度子句为 TM 学习带来的新可能性，适用于多样化领域，兼具可解释性和高效性。

Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine
(TM) both interpretable and efficient, while the power of Tsetlin automata
enables accuracy comparable to deep learning on an increasing number of
datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning
interpretable deep clauses from graph-structured input. Moving beyond flat,
fixed-length input, the GraphTM gets more versatile, supporting sequences,
grids, relations, and multimodality. Through message passing, the GraphTM
builds nested deep clauses to recognize sub-graph patterns with exponentially
fewer clauses, increasing both interpretability and data utilization. For image
classification, GraphTM preserves interpretability and achieves 3.86%-points
higher accuracy on CIFAR-10 than a convolutional TM. For tracking action
coreference, faced with increasingly challenging tasks, GraphTM outperforms
other reinforcement learning methods by up to 20.6%-points. In recommendation
systems, it tolerates increasing noise to a greater extent than a Graph
Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains
accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence
data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training
2.5x faster than GCN. The GraphTM's application to these varied fields
demonstrates how graph representation learning and deep clauses bring new
possibilities for TM learning.

</details>


### [188] [Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization](https://arxiv.org/abs/2507.14882)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.LG

TL;DR: 提出了一种增强的重要性度量框架，用于结构化剪枝，以在减少模型大小的同时保持应用特定性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的高复杂性和计算需求限制了其广泛应用，而传统剪枝方法在保持性能方面存在不足。

Method: 采用多策略确定每组的最优剪枝幅度，平衡压缩与任务性能，并通过自动编码器在MNIST数据集上验证。

Result: 实验表明，该方法在显著剪枝后仍能有效保持任务相关性能。

Conclusion: 提出的框架成功解决了剪枝过程中保持应用特定性能的问题。

Abstract: Deep neural networks (DNNs) offer significant versatility and performance
benefits, but their widespread adoption is often hindered by high model
complexity and computational demands. Model compression techniques such as
pruning have emerged as promising solutions to these challenges. However, it
remains critical to ensure that application-specific performance
characteristics are preserved during compression. In structured pruning, where
groups of structurally coherent elements are removed, conventional importance
metrics frequently fail to maintain these essential performance attributes. In
this work, we propose an enhanced importance metric framework that not only
reduces model size but also explicitly accounts for application-specific
performance constraints. We employ multiple strategies to determine the optimal
pruning magnitude for each group, ensuring a balance between compression and
task performance. Our approach is evaluated on an autoencoder tasked with
reconstructing MNIST images. Experimental results demonstrate that the proposed
method effectively preserves task-relevant performance, maintaining the model's
usability even after substantial pruning, by satisfying the required
application-specific criteria.

</details>


### [189] [Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning](https://arxiv.org/abs/2507.14919)
*Maximilian Wendlinger,Kilian Tscharke,Pascal Debus*

Main category: cs.LG

TL;DR: 论文探讨了量子机器学习中模型透明度和不确定性量化的问题，提出了将经典不确定性量化方法应用于量子领域的方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习和量子机器学习中模型透明度降低导致的问题（如过拟合和预测过度自信）尚未在量子领域得到解决。

Method: 基于经典不确定性量化和量子贝叶斯建模的理论，开发并实证评估了将经典方法映射到量子机器学习的技术。

Result: 研究发现需要利用经典不确定性量化方法为量子机器学习模型设计提供不确定性意识。

Conclusion: 强调了将经典不确定性量化方法应用于量子机器学习模型设计的重要性。

Abstract: One of the key obstacles in traditional deep learning is the reduction in
model transparency caused by increasingly intricate model functions, which can
lead to problems such as overfitting and excessive confidence in predictions.
With the advent of quantum machine learning offering possible advances in
computational power and latent space complexity, we notice the same opaque
behavior. Despite significant research in classical contexts, there has been
little advancement in addressing the black-box nature of quantum machine
learning. Consequently, we approach this gap by building upon existing work in
classical uncertainty quantification and initial explorations in quantum
Bayesian modeling to theoretically develop and empirically evaluate techniques
to map classical uncertainty quantification methods to the quantum machine
learning domain. Our findings emphasize the necessity of leveraging classical
insights into uncertainty quantification to include uncertainty awareness in
the process of designing new quantum machine learning models.

</details>


### [190] [FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios](https://arxiv.org/abs/2507.14980)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Qipeng Xie,Chang Liu,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWCM是一种动态调整动量的联邦学习方法，解决了长尾分布导致的模型偏差和收敛问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布（non-IID）数据，尤其是长尾分布场景下面临模型偏差和收敛困难的问题。

Method: 通过层级的神经网络行为分析，提出FedWCM方法，动态调整动量以纠正长尾分布引入的方向偏差。

Result: 实验表明，FedWCM解决了收敛问题，并在处理客户端异构性和数据不平衡时优于现有方法。

Conclusion: FedWCM提升了联邦学习在数据不平衡场景下的效率和效果。

Abstract: Federated Learning (FL) enables decentralized model training while preserving
data privacy. Despite its benefits, FL faces challenges with non-identically
distributed (non-IID) data, especially in long-tailed scenarios with imbalanced
class samples. Momentum-based FL methods, often used to accelerate FL
convergence, struggle with these distributions, resulting in biased models and
making FL hard to converge. To understand this challenge, we conduct extensive
investigations into this phenomenon, accompanied by a layer-wise analysis of
neural network behavior. Based on these insights, we propose FedWCM, a method
that dynamically adjusts momentum using global and per-round data to correct
directional biases introduced by long-tailed distributions. Extensive
experiments show that FedWCM resolves non-convergence issues and outperforms
existing methods, enhancing FL's efficiency and effectiveness in handling
client heterogeneity and data imbalance.

</details>


### [191] [Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data](https://arxiv.org/abs/2507.14999)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为FedClusAvg的隐私保护联邦学习框架，用于在非独立同分布（Non-IID）和资源受限的环境中提高虚假数据注入攻击（FDIA）的检测能力。


<details>
  <summary>Details</summary>
Motivation: 虚假数据注入攻击（FDIAs）对智能电网构成严重安全威胁，传统集中式训练方法存在隐私风险、数据共享限制和高传输成本问题。

Method: FedClusAvg采用基于聚类的分层采样和分层通信（客户端-子服务器-服务器）机制，支持本地化训练和加权参数聚合，避免集中敏感数据。

Result: 实验表明，FedClusAvg在异构数据分布下提高了检测精度，并显著减少了通信轮次和带宽消耗。

Conclusion: 该框架为大规模分布式电力系统中的FDIA检测提供了安全高效的解决方案。

Abstract: False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.

</details>


### [192] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: 论文提出了一种新的任务Time-RA，将时间序列异常检测从判别式任务转变为生成式任务，并引入多模态基准数据集RATs40K，用于细粒度异常分类和解释性推理。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法通常仅进行二元分类，缺乏详细分类和解释性推理，限制了其应用。

Method: 提出Time-RA任务，利用大语言模型（LLMs）进行生成式推理，并构建RATs40K数据集，包含数值、文本和视觉数据，标注细粒度异常类别和解释。

Result: 通过实验验证了当前LLMs和多模态LLMs的能力与局限性，强调了监督微调的重要性。

Conclusion: Time-RA任务和RATs40K数据集为可解释的时间序列异常检测和推理提供了重要进展。

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [193] [ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model](https://arxiv.org/abs/2507.15067)
*Bing He,Mustaque Ahamad,Srijan Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种名为ROBAD的新型Transformer模型，用于检测互联网平台上的恶意用户，并通过局部和全局信息捕捉以及对抗性训练提升模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在检测恶意用户时对输入序列的微小变化敏感，缺乏鲁棒性，因此需要一种更稳健的检测方法。

Method: ROBAD结合Transformer编码器和解码器，分别捕捉帖子级局部信息和序列级全局信息，并通过对比学习增强分类层，利用模拟攻击者的修改序列进行训练。

Result: 在Yelp和Wikipedia数据集上的实验表明，ROBAD能有效抵御最先进的对抗攻击，准确检测恶意用户。

Conclusion: ROBAD通过局部-全局信息捕捉和对抗性训练，显著提升了模型在对抗环境下的鲁棒性和检测效果。

Abstract: Detecting bad actors is critical to ensure the safety and integrity of
internet platforms. Several deep learning-based models have been developed to
identify such users. These models should not only accurately detect bad actors,
but also be robust against adversarial attacks that aim to evade detection.
However, past deep learning-based detection models do not meet the robustness
requirement because they are sensitive to even minor changes in the input
sequence. To address this issue, we focus on (1) improving the model
understanding capability and (2) enhancing the model knowledge such that the
model can recognize potential input modifications when making predictions. To
achieve these goals, we create a novel transformer-based classification model,
called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection
model), which uses the sequence of user posts to generate user embedding to
detect bad actors. Particularly, ROBAD first leverages the transformer encoder
block to encode each post bidirectionally, thus building a post embedding to
capture the local information at the post level. Next, it adopts the
transformer decoder block to model the sequential pattern in the post
embeddings by using the attention mechanism, which generates the sequence
embedding to obtain the global information at the sequence level. Finally, to
enrich the knowledge of the model, embeddings of modified sequences by mimicked
attackers are fed into a contrastive-learning-enhanced classification layer for
sequence prediction. In essence, by capturing the local and global information
(i.e., the post and sequence information) and leveraging the mimicked behaviors
of bad actors in training, ROBAD can be robust to adversarial attacks.
Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can
effectively detect bad actors when under state-of-the-art adversarial attacks.

</details>


### [194] [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073)
*Samuel Pfrommer,Yixiao Huang,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 论文探讨了通过强化学习训练流匹配策略以超越原始演示策略性能的方法，提出了两种方法（RWFM和GRPO），并在模拟任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过强化学习提升流匹配策略的性能，超越由次优策略（如人类操作员）生成的训练演示。

Method: 提出了两种方法：奖励加权流匹配（RWFM）和基于学习奖励替代的组相对策略优化（GRPO）。

Result: 实验表明，两种方法显著优于次优演示策略，GRPO方法尤其有效，成本比简单的模仿学习流匹配（ILFM）低50%至85%。

Conclusion: 通过强化学习训练的流匹配策略可以显著提升性能，GRPO方法在减少成本方面表现尤为突出。

Abstract: Flow-matching policies have emerged as a powerful paradigm for generalist
robotics. These models are trained to imitate an action chunk, conditioned on
sensor observations and textual instructions. Often, training demonstrations
are generated by a suboptimal policy, such as a human operator. This work
explores training flow-matching policies via reinforcement learning to surpass
the original demonstration policy performance. We particularly note
minimum-time control as a key application and present a simple scheme for
variable-horizon flow-matching planning. We then introduce two families of
approaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group
Relative Policy Optimization (GRPO) approach with a learned reward surrogate.
Our policies are trained on an illustrative suite of simulated unicycle
dynamics tasks, and we show that both approaches dramatically improve upon the
suboptimal demonstrator performance, with the GRPO approach in particular
generally incurring between $50\%$ and $85\%$ less cost than a naive Imitation
Learning Flow Matching (ILFM) approach.

</details>


### [195] [Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts](https://arxiv.org/abs/2507.15079)
*Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: cs.LG

TL;DR: 提出了一种名为iQRA的新方法，通过集成点预测生成概率预测，显著提升了电力市场价格预测的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 电力市场等波动性强的领域需要量化预测模型的不确定性，以降低数据驱动决策的风险。现有机器学习模型虽准确但缺乏不确定性估计。

Method: 基于Quantile Regression Averaging (QRA)框架，引入随机顺序约束，提出Isotonic Quantile Regression Averaging (iQRA)方法。

Result: 在德国日前电力市场的预测研究中，iQRA在可靠性和锐度上均优于现有后处理方法，且能生成校准良好的预测区间。

Conclusion: iQRA通过等渗正则化降低了分位数回归问题的复杂性，提供了一种无需超参数选择的变量选择方法，显著提升了预测性能。

Abstract: Quantifying the uncertainty of forecasting models is essential to assess and
mitigate the risks associated with data-driven decisions, especially in
volatile domains such as electricity markets. Machine learning methods can
provide highly accurate electricity price forecasts, critical for informing the
decisions of market participants. However, these models often lack uncertainty
estimates, which limits the ability of decision makers to avoid unnecessary
risks. In this paper, we propose a novel method for generating probabilistic
forecasts from ensembles of point forecasts, called Isotonic Quantile
Regression Averaging (iQRA). Building on the established framework of Quantile
Regression Averaging (QRA), we introduce stochastic order constraints to
improve forecast accuracy, reliability, and computational costs. In an
extensive forecasting study of the German day-ahead electricity market, we show
that iQRA consistently outperforms state-of-the-art postprocessing methods in
terms of both reliability and sharpness. It produces well-calibrated prediction
intervals across multiple confidence levels, providing superior reliability to
all benchmark methods, particularly coverage-based conformal prediction. In
addition, isotonic regularization decreases the complexity of the quantile
regression problem and offers a hyperparameter-free approach to variable
selection.

</details>


### [196] [Robust Control with Gradient Uncertainty](https://arxiv.org/abs/2507.15082)
*Qian Qi*

Main category: cs.LG

TL;DR: 论文提出了一种新的鲁棒控制理论扩展，明确处理价值函数梯度的不确定性，适用于强化学习等领域。通过零和动态博弈建模，推导出非线性偏微分方程GU-HJBI，并证明了其适定性。在LQ案例中，发现经典二次价值函数假设失效，提出了新的控制算法GURAC，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习等领域中价值函数梯度不确定性的问题，提升鲁棒控制理论的适用性。

Method: 通过零和动态博弈建模，推导出GU-HJBI方程，分析其适定性，并在LQ案例中验证理论。提出GURAC算法进行实践验证。

Result: 证明了GU-HJBI方程的适定性，发现LQ案例中经典假设失效，提出并验证了GURAC算法的有效性。

Conclusion: 该研究为鲁棒控制提供了新方向，对强化学习等领域的函数逼近问题具有重要意义。

Abstract: We introduce a novel extension to robust control theory that explicitly
addresses uncertainty in the value function's gradient, a form of uncertainty
endemic to applications like reinforcement learning where value functions are
approximated. We formulate a zero-sum dynamic game where an adversary perturbs
both system dynamics and the value function gradient, leading to a new, highly
nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs
Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness
by proving a comparison principle for its viscosity solutions under a uniform
ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a
key insight: we prove that the classical quadratic value function assumption
fails for any non-zero gradient uncertainty, fundamentally altering the problem
structure. A formal perturbation analysis characterizes the non-polynomial
correction to the value function and the resulting nonlinearity of the optimal
control law, which we validate with numerical studies. Finally, we bridge
theory to practice by proposing a novel Gradient-Uncertainty-Robust
Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating
its effectiveness in stabilizing training. This work provides a new direction
for robust control, holding significant implications for fields where function
approximation is common, including reinforcement learning and computational
finance.

</details>


### [197] [AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI](https://arxiv.org/abs/2507.15104)
*Qiufeng Li,Shu Hong,Jian Gao,Xuan Zhang,Tian Lan,Weidong Cao*

Main category: cs.LG

TL;DR: 论文提出AnalogFed，一种无需共享原始数据的协作式生成AI框架，用于解决模拟电路设计中的数据隐私问题。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计数据具有专有性，限制了生成AI的研究进展，需解决数据隐私与协作创新的矛盾。

Method: 引入AnalogFed框架，结合联邦学习技术，处理数据异构性并保障隐私，支持分散式协作。

Result: 实验表明AnalogFed性能接近集中式基线，生成AI模型在效率和可扩展性上达到先进水平。

Conclusion: AnalogFed为模拟电路设计的协作创新提供了隐私保护的解决方案，推动了生成AI在该领域的应用。

Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize
analog design automation through data-driven approaches. In particular,
researchers are increasingly fascinated by harnessing the power of generative
AI to automate the discovery of novel analog circuit topologies. Unlocking the
full potential of generative AI in these data-driven discoveries requires
access to large and diverse datasets.Yet, there is a significant barrier in the
analog domain--Analog circuit design is inherently proprietary, involving not
only confidential circuit structures but also the underlying commercial
semiconductor processes. As a result, current generative AI research is largely
confined to individual researchers who construct small, narrowly focused
private datasets. This fragmentation severely limits collaborative innovation
and impedes progress across the research community. To address these
challenges, we propose AnalogFed. AnalogFed enables collaborative topology
discovery across decentralized clients (e.g., individual researchers or
institutions) without requiring the sharing of raw private data. To make this
vision practical, we introduce a suite of techniques tailored to the unique
challenges of applying FedL in analog design--from generative model development
and data heterogeneity handling to privacy-preserving strategies that ensure
both flexibility and security for circuit designers and semiconductor
manufacturers. Extensive experiments across varying client counts and dataset
sizes demonstrate that AnalogFed achieves performance comparable to centralized
baselines--while maintaining strict data privacy. Specifically, the generative
AI model within AnalogFed achieves state-of-the-art efficiency and scalability
in the design of analog circuit topologies.

</details>


### [198] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 论文提出了一种分布式的遗忘学习框架，通过移除特定分布的数据点来满足隐私或法律需求，相比随机移除更高效。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘学习工具主要针对单个样本，无法有效移除整个主题域，导致残留信号可能被下游学习器恢复。

Method: 提出分布遗忘学习框架，利用Kullback-Leibler散度量化移除和保留效果，并在高斯情况下推导出帕累托前沿。

Result: 实验表明，该方法在合成高斯数据、Jigsaw Toxic Comments等数据集上比随机移除减少15-72%的删除量，且对保留性能影响极小。

Conclusion: 分布遗忘学习是一种高效、模型无关的解决方案，适用于需要移除特定分布数据的场景。

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>


### [199] [Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2507.15119)
*Juntong Ni,Shiyu Wang,Zewen Liu,Xiaoming Shi,Xinyue Zhong,Zhou Ye,Wei Jin*

Main category: cs.LG

TL;DR: 论文提出U-Cast模型和Time-HD基准，解决高维时间序列预测（HDTSF）中的复杂通道相关性问题。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测（TSF）研究未充分关注高维场景（HDTSF）中的复杂通道相关性挑战。

Method: U-Cast采用基于查询的注意力机制学习潜在层次通道结构，并引入全秩正则化解耦高相关通道表示。

Result: 理论证明跨通道信息降低预测风险，实验显示U-Cast在Time-HD基准上优于基线模型。

Conclusion: U-Cast和Time-HD为未来HDTSF研究提供了坚实基础。

Abstract: Time series forecasting (TSF) is a central problem in time series analysis.
However, as the number of channels in time series datasets scales to the
thousands or more, a scenario we define as High-Dimensional Time Series
Forecasting (HDTSF), it introduces significant new modeling challenges that are
often not the primary focus of traditional TSF research. HDTSF is challenging
because the channel correlation often forms complex and hierarchical patterns.
Existing TSF models either ignore these interactions or fail to scale as
dimensionality grows. To address this issue, we propose U-Cast, a
channel-dependent forecasting architecture that learns latent hierarchical
channel structures with an innovative query-based attention. To disentangle
highly correlated channel representation, U-Cast adds a full-rank
regularization during training. We also release Time-HD, a benchmark of large,
diverse, high-dimensional datasets. Our theory shows that exploiting
cross-channel information lowers forecasting risk, and experiments on Time-HD
demonstrate that U-Cast surpasses strong baselines in both accuracy and
efficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF
research.

</details>


### [200] [Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm](https://arxiv.org/abs/2507.15132)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: 提出了一种遗传算法，通过优化分类和回归任务的问题复杂度指标，生成具有不同难度级别的合成数据集。


<details>
  <summary>Details</summary>
Motivation: 研究社区需要更先进的合成数据生成器来评估机器学习方法的优缺点，因此希望通过优化复杂度指标来生成多样化的数据集。

Method: 使用遗传算法优化分类任务的10个复杂度指标和回归任务的4个指标，通过线性特征投影调整合成数据集的复杂度。

Result: 实验表明，该方法能生成具有目标复杂度的数据集，且复杂度与分类器和回归器的识别质量相关。

Conclusion: 该遗传算法能有效生成多样化复杂度的数据集，为机器学习方法评估提供了可靠工具。

Abstract: The research community continues to seek increasingly more advanced synthetic
data generators to reliably evaluate the strengths and limitations of machine
learning methods. This work aims to increase the availability of datasets
encompassing a diverse range of problem complexities by proposing a genetic
algorithm that optimizes a set of problem complexity measures for
classification and regression tasks towards specific targets. For
classification, a set of 10 complexity measures was used, while for regression
tasks, 4 measures demonstrating promising optimization capabilities were
selected. Experiments confirmed that the proposed genetic algorithm can
generate datasets with varying levels of difficulty by transforming
synthetically created datasets to achieve target complexity values through
linear feature projections. Evaluations involving state-of-the-art classifiers
and regressors revealed a correlation between the complexity of the generated
data and the recognition quality.

</details>


### [201] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: 研究多标签分类问题，利用逻辑约束和序列模型联合建模标签分布。


<details>
  <summary>Details</summary>
Motivation: 探索在多标签分类中如何利用逻辑约束改进模型性能。

Method: 采用个体标签分类器结合序列模型，生成联合分布。

Result: 模型能有效利用训练中的约束，并在推理时强制执行约束。

Conclusion: 该架构在多标签分类中表现出色，尤其在处理约束时具有优势。

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [202] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: 提出了一种基于共振隧穿二极管（RTD）的神经形态计算架构，用于物理储层计算（RC），并在图像识别任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能向实时、边缘和资源受限环境扩展，需要硬件高效的计算模型。

Method: 理论构建并数值实现了一种基于RTD的RC系统，应用于手写数字分类和Fruit~360数据集的对象识别。

Result: 该架构在性能表现良好，同时遵循下一代RC的原则，用确定性非线性变换替代随机连接。

Conclusion: RTD-based RC架构在资源受限环境中具有潜力。

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [203] [Designing User-Centric Metrics for Evaluation of Counterfactual Explanations](https://arxiv.org/abs/2507.15162)
*Firdaus Ahmed Choudhury,Ethan Leicht,Jude Ethan Bislig,Hangzhi Guo,Amulya Yadav*

Main category: cs.LG

TL;DR: 论文研究了反事实解释（CFEs）在机器学习决策模型中的应用，发现现有评估指标与用户偏好不一致，并提出了一种用户中心的两阶段模型AWP，预测用户偏好CFEs的准确率达84.37%。


<details>
  <summary>Details</summary>
Motivation: 机器学习决策模型的不透明性导致用户难以理解决策原因，现有CFEs评估指标（如接近性）可能忽视用户偏好和约束，需要更用户中心的评估方法。

Method: 通过两项用户研究：1）20名众包工人的试点研究验证现有指标与用户偏好的对齐性；2）41名参与者的详细研究，提出AWP模型。

Result: 用户偏好的CFEs与基于接近性的指标仅63.81%一致；AWP模型预测用户偏好CFEs的准确率达84.37%。

Conclusion: 研究首次验证了用户中心成本模型在CFE生成中的有效性，强调需要自适应、用户中心的评估指标。

Abstract: Machine learning-based decision models are increasingly being used to make
decisions that significantly impact people's lives, but their opaque nature
leaves end users without a clear understanding of why a decision was made.
Counterfactual Explanations (CFEs) have grown in popularity as a means of
offering actionable guidance by identifying the minimum changes in feature
values required to flip a model's prediction to something more desirable.
Unfortunately, most prior research in CFEs relies on artificial evaluation
metrics, such as proximity, which may overlook end-user preferences and
constraints, e.g., the user's perception of effort needed to make certain
feature changes may differ from that of the model designer. To address this
research gap, this paper makes three novel contributions. First, we conduct a
pilot study with 20 crowd-workers on Amazon MTurk to experimentally validate
the alignment of existing CF evaluation metrics with real-world user
preferences. Results show that user-preferred CFEs matched those based on
proximity in only 63.81% of cases, highlighting the limited applicability of
these metrics in real-world settings. Second, inspired by the need to design a
user-informed evaluation metric for CFEs, we conduct a more detailed two-day
user study with 41 participants facing realistic credit application scenarios
to find experimental support for or against three intuitive hypotheses that may
explain how end users evaluate CFEs. Third, based on the findings of this
second study, we propose the AWP model, a novel user-centric, two-stage model
that describes one possible mechanism by which users evaluate and select CFEs.
Our results show that AWP predicts user-preferred CFEs with 84.37% accuracy.
Our study provides the first human-centered validation for personalized cost
models in CFE generation and highlights the need for adaptive, user-centered
evaluation metrics.

</details>


### [204] [Better Models and Algorithms for Learning Ising Models from Dynamics](https://arxiv.org/abs/2507.15173)
*Jason Gaitonde,Ankur Moitra,Elchanan Mossel*

Main category: cs.LG

TL;DR: 本文提出了首个在仅观察配置变化的情况下高效学习Ising模型结构的算法，解决了之前需要观察所有更新尝试的限制。


<details>
  <summary>Details</summary>
Motivation: 研究Ising模型在高维数据中的结构和参数学习问题，尤其是在更现实的观察模型下（仅观察配置变化），填补了现有方法的不足。

Method: 提出一种算法，能够在仅观察配置变化的情况下，高效恢复Ising模型的依赖图和参数，适用于最大度为d的模型。

Result: 算法在时间poly(d)·n²log n内恢复依赖图，并在额外时间Õ(2^d n)内恢复参数，与现有i.i.d.设置下的最优结果相当。

Conclusion: 该算法不仅适用于Ising模型，还适用于更广泛的单点可逆马尔可夫链，如Metropolis链，扩展了应用范围。

Abstract: We study the problem of learning the structure and parameters of the Ising
model, a fundamental model of high-dimensional data, when observing the
evolution of an associated Markov chain. A recent line of work has studied the
natural problem of learning when observing an evolution of the well-known
Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Mossel STOC 2024], which provides an arguably more realistic
generative model than the classical i.i.d. setting. However, this prior work
crucially assumes that all site update attempts are observed, \emph{even when
this attempt does not change the configuration}: this strong observation model
is seemingly essential for these approaches. While perhaps possible in
restrictive contexts, this precludes applicability to most realistic settings
where we can observe \emph{only} the stochastic evolution itself, a minimal and
natural assumption for any process we might hope to learn from. However,
designing algorithms that succeed in this more realistic setting has remained
an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Moitra, Mossel, STOC 2025].
  In this work, we give the first algorithms that efficiently learn the Ising
model in this much more natural observation model that only observes when the
configuration changes. For Ising models with maximum degree $d$, our algorithm
recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log
n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time,
which qualitatively matches the state-of-the-art even in the i.i.d. setting in
a much weaker observation model. Our analysis holds more generally for a
broader class of reversible, single-site Markov chains that also includes the
popular Metropolis chain by leveraging more robust properties of reversible
Markov chains.

</details>


### [205] [Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control](https://arxiv.org/abs/2507.15174)
*Justin Turnau,Longchao Da,Khoa Vo,Ferdous Al Rafi,Shreyas Bachiraju,Tiejin Chen,Hua Wei*

Main category: cs.LG

TL;DR: JL-GAT是一种将GAT应用于多智能体强化学习（MARL）的交通信号控制方法，通过结合邻近智能体的信息，平衡了可扩展性和增强的接地能力。


<details>
  <summary>Details</summary>
Motivation: 解决MARL在真实交通网络中因环境动态变化导致的性能下降（sim-to-real gap）问题。

Method: 采用分散式的GAT方法，结合邻近智能体信息，提升MARL在交通信号控制中的可扩展性和性能。

Result: 在模拟恶劣天气条件下的多种道路网络中验证了JL-GAT的有效性。

Conclusion: JL-GAT为MARL在真实交通网络中的应用提供了一种可扩展且高效的解决方案。

Abstract: Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.

</details>


### [206] [Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning](https://arxiv.org/abs/2507.15195)
*Anwar Said,Yifan Wei,Ubaid Ullah Ahmad,Mudassir Shabbir,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 论文提出利用平均可控性和新型排名编码方法提升GNN在社交网络分类任务中的性能，解决了节点特征缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 社交网络中节点特征常因隐私或固有属性缺失而不可用，限制了GNN的性能。

Method: 提出两种策略：1) 使用平均可控性等中心性指标作为节点特征；2) 开发排名编码方法将图论指标转化为固定维特征空间。

Result: 实验表明，平均可控性显著提升GNN性能，排名编码方法优于传统one-hot编码（ROC AUC从68.7%提升至73.9%）。

Conclusion: 提出的方法能有效生成高效且表达能力强的节点特征，提升GNN在社交网络中的表现。

Abstract: In this article, we utilize the concept of average controllability in graphs,
along with a novel rank encoding method, to enhance the performance of Graph
Neural Networks (GNNs) in social network classification tasks. GNNs have proven
highly effective in various network-based learning applications and require
some form of node features to function. However, their performance is heavily
influenced by the expressiveness of these features. In social networks, node
features are often unavailable due to privacy constraints or the absence of
inherent attributes, making it challenging for GNNs to achieve optimal
performance. To address this limitation, we propose two strategies for
constructing expressive node features. First, we introduce average
controllability along with other centrality metrics (denoted as NCT-EFA) as
node-level metrics that capture critical aspects of network topology. Building
on this, we develop a rank encoding method that transforms average
controllability or any other graph-theoretic metric into a fixed-dimensional
feature space, thereby improving feature representation. We conduct extensive
numerical evaluations using six benchmark GNN models across four social network
datasets to compare different node feature construction methods. Our results
demonstrate that incorporating average controllability into the feature space
significantly improves GNN performance. Moreover, the proposed rank encoding
method outperforms traditional one-hot degree encoding, improving the ROC AUC
from 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,
underscoring its effectiveness in generating expressive and efficient node
representations.

</details>


### [207] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的多模态方法LSDGNN，通过长距离和短距离图神经网络分别获取远距离和近距离话语的多模态特征，并引入差分正则器和双仿射模块优化特征交互。此外，改进的课程学习（ICL）解决了数据不平衡问题。实验结果表明，该模型在IEMOCAP和MELD数据集上优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 对话中的情感识别（ERC）是一个实用且具有挑战性的任务，需要有效捕捉多模态特征并处理数据不平衡问题。

Method: 基于有向无环图（DAG）构建长距离和短距离图神经网络，使用差分正则器和双仿射模块优化特征交互，并提出改进的课程学习（ICL）解决数据不平衡。

Result: 在IEMOCAP和MELD数据集上，模型表现优于现有基准。

Conclusion: LSDGNN通过多模态特征提取和优化的训练策略，显著提升了情感识别性能。

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [208] [Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification](https://arxiv.org/abs/2507.15240)
*Le Peng,Yash Travadi,Chuan He,Ying Cui,Ju Sun*

Main category: cs.LG

TL;DR: 该论文针对类别不平衡分类问题，提出了精确约束重构方法，直接优化精度和召回率，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡分类中，标准准确率容易误导性能评估，现有方法无法满足不同类别重要性或特定指标需求。

Method: 引入精确约束重构方法，直接优化精度和召回率，并通过精确惩罚方法求解。

Result: 在多个基准数据集上，该方法优于现有技术。

Conclusion: 提出的精确重构与优化框架适用于广泛的直接指标优化问题。

Abstract: For classification with imbalanced class frequencies, i.e., imbalanced
classification (IC), standard accuracy is known to be misleading as a
performance measure. While most existing methods for IC resort to optimizing
balanced accuracy (i.e., the average of class-wise recalls), they fall short in
scenarios where the significance of classes varies or certain metrics should
reach prescribed levels. In this paper, we study two key classification
metrics, precision and recall, under three practical binary IC settings: fix
precision optimize recall (FPOR), fix recall optimize precision (FROP), and
optimize $F_\beta$-score (OFBS). Unlike existing methods that rely on smooth
approximations to deal with the indicator function involved, \textit{we
introduce, for the first time, exact constrained reformulations for these
direct metric optimization (DMO) problems}, which can be effectively solved by
exact penalty methods. Experiment results on multiple benchmark datasets
demonstrate the practical superiority of our approach over the state-of-the-art
methods for the three DMO problems. We also expect our exact reformulation and
optimization (ERO) framework to be applicable to a wide range of DMO problems
for binary IC and beyond. Our code is available at
https://github.com/sun-umn/DMO.

</details>


### [209] [Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks](https://arxiv.org/abs/2507.15246)
*Rabia Latief Bhat,Iqra Altaf Gillani*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的图神经网络框架，用于捕捉食品配送中的时空依赖性，以提高需求预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 食品配送平台的效率和响应能力依赖于准确的需求预测，而订单量的时空异质性和波动性直接影响运营决策。

Method: 将配送环境建模为图，节点代表配送区域，边反映空间邻近性和历史订单流模式；通过注意力机制动态加权邻近区域的影响，并结合时空趋势进行学习。

Result: 在真实食品配送数据集上的实验表明，该模型能高精度预测未来订单量。

Conclusion: 该框架为城市食品配送运营提供了可扩展和自适应的解决方案，支持主动车队定位、资源分配和调度优化。

Abstract: Accurate demand forecasting is critical for enhancing the efficiency and
responsiveness of food delivery platforms, where spatial heterogeneity and
temporal fluctuations in order volumes directly influence operational
decisions. This paper proposes an attention-based Graph Neural Network
framework that captures spatial-temporal dependencies by modeling the food
delivery environment as a graph. In this graph, nodes represent urban delivery
zones, while edges reflect spatial proximity and inter-regional order flow
patterns derived from historical data. The attention mechanism dynamically
weighs the influence of neighboring zones, enabling the model to focus on the
most contextually relevant areas during prediction. Temporal trends are jointly
learned alongside spatial interactions, allowing the model to adapt to evolving
demand patterns. Extensive experiments on real-world food delivery datasets
demonstrate the superiority of the proposed model in forecasting future order
volumes with high accuracy. The framework offers a scalable and adaptive
solution to support proactive fleet positioning, resource allocation, and
dispatch optimization in urban food delivery operations.

</details>


### [210] [CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](https://arxiv.org/abs/2507.15260)
*Jiaqi Han,Haotian Ye,Puheng Li,Minkai Xu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: CHORDS是一种无需训练、模型无关的扩散采样加速框架，通过多核并行实现高效推理，显著提升采样速度且不降低质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高保真生成能力受限于计算成本高的推理过程，现有加速技术要么需重新训练模型，要么牺牲样本质量。

Method: 将多核扩散采样视为ODE求解器管道，通过理论支持的核间通信机制，用慢速但准确的求解器逐步修正快速求解器。

Result: CHORDS在多种大规模图像和视频扩散模型中显著加速采样，四核下提速2.1倍，八核下提速2.9倍，且无质量损失。

Conclusion: CHORDS为实时高保真扩散生成奠定了坚实基础。

Abstract: Diffusion-based generative models have become dominant generators of
high-fidelity images and videos but remain limited by their computationally
expensive inference procedures. Existing acceleration techniques either require
extensive model retraining or compromise significantly on sample quality. This
paper explores a general, training-free, and model-agnostic acceleration
strategy via multi-core parallelism. Our framework views multi-core diffusion
sampling as an ODE solver pipeline, where slower yet accurate solvers
progressively rectify faster solvers through a theoretically justified
inter-core communication mechanism. This motivates our multi-core training-free
diffusion sampling accelerator, CHORDS, which is compatible with various
diffusion samplers, model architectures, and modalities. Through extensive
experiments, CHORDS significantly accelerates sampling across diverse
large-scale image and video diffusion models, yielding up to 2.1x speedup with
four cores, improving by 50% over baselines, and 2.9x speedup with eight cores,
all without quality degradation. This advancement enables CHORDS to establish a
solid foundation for real-time, high-fidelity diffusion generation.

</details>


### [211] [Temporal Basis Function Models for Closed-Loop Neural Stimulation](https://arxiv.org/abs/2507.15274)
*Matthew J. Bryan,Felix Schwock,Azadeh Yazdan-Shahmorad,Rajesh P N Rao*

Main category: cs.LG

TL;DR: 论文提出了一种基于时间基函数模型（TBFMs）的闭环神经刺激方法，用于个性化治疗神经疾病，并在非人灵长类动物中验证了其高效性和低延迟性。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能技术是否能个性化闭环神经刺激或开发新疗法，解决样本效率、训练时间和延迟等翻译问题。

Method: 使用时间基函数模型（TBFMs）进行单次试验的时空前向预测，模拟闭环刺激控制神经活动。

Result: TBF模型在样本效率、训练时间（2-4分钟）和延迟（0.2毫秒）上表现优异，预测精度与基线非线性模型相当，优于线性模型。

Conclusion: TBF模型为复杂AI方法与临床有用的闭环刺激协议之间的翻译提供了桥梁。

Abstract: Closed-loop neural stimulation provides novel therapies for neurological
diseases such as Parkinson's disease (PD), but it is not yet clear whether
artificial intelligence (AI) techniques can tailor closed-loop stimulation to
individual patients or identify new therapies. Progress requires us to address
a number of translational issues, including sample efficiency, training time,
and minimizing loop latency such that stimulation may be shaped in response to
changing brain activity. We propose temporal basis function models (TBFMs) to
address these difficulties, and explore this approach in the context of
excitatory optogenetic stimulation. We demonstrate the ability of TBF models to
provide a single-trial, spatiotemporal forward prediction of the effect of
optogenetic stimulation on local field potentials (LFPs) measured in two
non-human primates. We further use simulations to demonstrate the use of TBF
models for closed-loop stimulation, driving neural activity towards target
patterns. The simplicity of TBF models allow them to be sample efficient, rapid
to train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the
model on 40 sessions of previously published excitatory optogenetic stimulation
data. For each session, the model required 15-20min of data collection to
successfully model the remainder of the session. It achieved a prediction
accuracy comparable to a baseline nonlinear dynamical systems model that
requires hours to train, and superior accuracy to a linear state-space model.
In our simulations, it also successfully allowed a closed-loop stimulator to
control a neural circuit. Our approach begins to bridge the translational gap
between complex AI-based approaches to modeling dynamical systems and the
vision of using such forward prediction models to develop novel, clinically
useful closed-loop stimulation protocols.

</details>


### [212] [Machine Unlearning for Streaming Forgetting](https://arxiv.org/abs/2507.15280)
*Shaofei Shen,Chenhao Zhang,Yawen Zhao,Alina Bialkowski,Weitong Chen,Miao Xu*

Main category: cs.LG

TL;DR: 论文提出了一种流式遗忘学习范式，将遗忘问题建模为分布偏移问题，并提出了一种无需原始训练数据的高效流式遗忘算法。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法通常批量处理遗忘数据，而实际场景中遗忘请求往往是流式的，导致效率和效果下降。

Method: 将遗忘问题形式化为分布偏移问题，估计变化后的分布，并提出一种新的流式遗忘算法。

Result: 理论分析表明算法具有$O(\sqrt{T} + V_T)$的误差界限，实验验证了其性能。

Conclusion: 该方法在流式遗忘场景中高效且无需原始数据，适用于多种模型和数据集。

Abstract: Machine unlearning aims to remove knowledge of the specific training data in
a well-trained model. Currently, machine unlearning methods typically handle
all forgetting data in a single batch, removing the corresponding knowledge all
at once upon request. However, in practical scenarios, requests for data
removal often arise in a streaming manner rather than in a single batch,
leading to reduced efficiency and effectiveness in existing methods. Such
challenges of streaming forgetting have not been the focus of much research. In
this paper, to address the challenges of performance maintenance, efficiency,
and data access brought about by streaming unlearning requests, we introduce a
streaming unlearning paradigm, formalizing the unlearning as a distribution
shift problem. We then estimate the altered distribution and propose a novel
streaming unlearning algorithm to achieve efficient streaming forgetting
without requiring access to the original training data. Theoretical analyses
confirm an $O(\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,
where $V_T$ represents the cumulative total variation in the optimal solution
over $T$ learning rounds. This theoretical guarantee is achieved under mild
conditions without the strong restriction of convex loss function. Experiments
across various models and datasets validate the performance of our proposed
method.

</details>


### [213] [Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning](https://arxiv.org/abs/2507.15287)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出了一种利用不完整专家演示的强化学习框架，通过映射函数将状态与专家数据的相似性转化为内在奖励，支持灵活探索。


<details>
  <summary>Details</summary>
Motivation: 现实环境中，强化学习代理需从无奖励交互或不完整演示中学习，但现有内在动机方法在高维或密集奖励环境中效果有限。

Method: 使用混合自编码器专家（Mixture of Autoencoder Experts）处理不完整演示，并通过映射函数将状态相似性转化为内在奖励。

Result: 实验表明，该方法在稀疏和密集奖励环境中均表现优异，即使演示不完整。

Conclusion: 为现实场景中缺乏最优数据但需精确奖励控制的强化学习提供了实用框架。

Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to
learn from reward-free interactions and alternative supervision signals, such
as unlabeled or incomplete demonstrations, rather than relying solely on
explicit reward maximization. Additionally, developing generalist agents that
can adapt efficiently in real-world environments often requires leveraging
these reward-free signals to guide learning and behavior. However, while
intrinsic motivation techniques provide a means for agents to seek out novel or
uncertain states in the absence of explicit rewards, they are often challenged
by dense reward environments or the complexity of high-dimensional state and
action spaces. Furthermore, most existing approaches rely directly on the
unprocessed intrinsic reward signals, which can make it difficult to shape or
control the agent's exploration effectively. We propose a framework that can
effectively utilize expert demonstrations, even when they are incomplete and
imperfect. By applying a mapping function to transform the similarity between
an agent's state and expert data into a shaped intrinsic reward, our method
allows for flexible and targeted exploration of expert-like behaviors. We
employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors
and accommodate missing information in demonstrations. Experiments show our
approach enables robust exploration and strong performance in both sparse and
dense reward environments, even when demonstrations are sparse or incomplete.
This provides a practical framework for RL in realistic settings where optimal
data is unavailable and precise reward control is needed.

</details>


### [214] [Preferential subspace identification (PSID) with forward-backward smoothing](https://arxiv.org/abs/2507.15288)
*Omid G. Sani,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 扩展PSID方法，引入最优滤波和平滑技术，提升多变量时间序列分析的性能。


<details>
  <summary>Details</summary>
Motivation: 现有PSID方法仅利用过去数据预测，而滤波和平滑技术能利用更多数据提升估计精度。

Method: 提出PSID滤波扩展和基于双滤波卡尔曼平滑的前向-后向PSID平滑算法。

Result: 在模拟数据中验证了方法能恢复真实模型参数，并达到理想滤波和平滑性能。

Conclusion: 为多变量时间序列分析提供了更强大的工具，扩展了动态交互研究的框架。

Abstract: System identification methods for multivariate time-series, such as neural
and behavioral recordings, have been used to build models for predicting one
from the other. For example, Preferential Subspace Identification (PSID) builds
a state-space model of a primary time-series (e.g., neural activity) to
optimally predict a secondary time-series (e.g., behavior). However, PSID
focuses on optimal prediction using past primary data, even though in offline
applications, better estimation can be achieved by incorporating concurrent
data (filtering) or all available data (smoothing). Here, we extend PSID to
enable optimal filtering and smoothing. First, we show that the presence of a
secondary signal makes it possible to uniquely identify a model with an optimal
Kalman update step (to enable filtering) from a family of otherwise equivalent
state-space models. Our filtering solution augments PSID with a reduced-rank
regression step that directly learns the optimal gain required for the update
step from data. We refer to this extension of PSID as PSID with filtering.
Second, inspired by two-filter Kalman smoother formulations, we develop a novel
forward-backward PSID smoothing algorithm where we first apply PSID with
filtering and then apply it again in the reverse time direction on the
residuals of the filtered secondary signal. We validate our methods on
simulated data, showing that our approach recovers the ground-truth model
parameters for filtering, and achieves optimal filtering and smoothing decoding
performance of the secondary signal that matches the ideal performance of the
true underlying model. This work provides a principled framework for optimal
linear filtering and smoothing in the two-signal setting, significantly
expanding the toolkit for analyzing dynamic interactions in multivariate
time-series.

</details>


### [215] [Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown](https://arxiv.org/abs/2507.15290)
*Emile Anand,Sarah Liaw*

Main category: cs.LG

TL;DR: FG-TS通过乐观奖励解决TS在高维问题中探索不足的问题，但在近似后验下的表现未被系统研究。本文首次全面评估FG-TS及其变体在11个基准上的表现，发现其在精确后验下表现优异，但在神经网络问题中较弱。


<details>
  <summary>Details</summary>
Motivation: 解决Thompson Sampling在高维问题中探索不足的问题，并评估FG-TS在近似后验下的表现。

Method: 使用FG-TS及其平滑变体SFG-TS，在11个真实和合成基准上进行实验，比较精确和近似后验下的性能。

Result: FG-TS在线性和逻辑bandits中优于TS，但在神经网络问题中表现较弱。实验揭示了奖励规模和采样噪声之间的权衡。

Conclusion: FG-TS及其变体简单易用且具有竞争力，推荐作为现代上下文bandit基准的基线方法。

Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation
tradeoff in contextual bandits, yet recent theory shows that it does not
explore aggressively enough in high-dimensional problems. Feel-Good Thompson
Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward
high-reward models, and it achieves the asymptotically minimax-optimal regret
in the linear setting when posteriors are exact. However, its performance with
\emph{approximate} posteriors -- common in large-scale or neural problems --
has not been benchmarked. We provide the first systematic study of FG-TS and
its smoothed variant (SFG-TS) across eleven real-world and synthetic
benchmarks. To evaluate their robustness, we compare performance across
settings with exact posteriors (linear and logistic bandits) to approximate
regimes produced by fast but coarse stochastic-gradient samplers. Ablations
over preconditioning, bonus scale, and prior strength reveal a trade-off:
larger bonuses help when posterior samples are accurate, but hurt when sampling
noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic
bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS
and its variants are competitive and easy-to-use, we recommend them as
baselines in modern contextual-bandit benchmarks. Finally, we provide source
code for all our experiments in
https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.

</details>


### [216] [Universal crystal material property prediction via multi-view geometric fusion in graph transformers](https://arxiv.org/abs/2507.15303)
*Liang Zhang,Kong Chen,Yuen Wu*

Main category: cs.LG

TL;DR: MGT框架通过融合SE3不变和SO3等变图表示，显著提升了晶体材料性能预测的准确性，并在多任务自监督预训练中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉晶体结构的几何和拓扑特征，限制了机器学习在晶体材料模拟中的应用。

Method: 提出MGT框架，结合SE3不变和SO3等变图表示，并使用轻量级专家混合路由器自适应调整权重。

Result: MGT在晶体性能预测任务中平均绝对误差降低21%，在迁移学习场景中性能提升高达58%。

Conclusion: MGT可作为晶体材料性能预测的有用模型，为新材料发现提供有力工具。

Abstract: Accurately and comprehensively representing crystal structures is critical
for advancing machine learning in large-scale crystal materials simulations,
however, effectively capturing and leveraging the intricate geometric and
topological characteristics of crystal structures remains a core, long-standing
challenge for most existing methods in crystal property prediction. Here, we
propose MGT, a multi-view graph transformer framework that synergistically
fuses SE3 invariant and SO3 equivariant graph representations, which
respectively captures rotation-translation invariance and rotation equivariance
in crystal geometries. To strategically incorporate these complementary
geometric representations, we employ a lightweight mixture of experts router in
MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on
the specific target task. Compared with previous state-of-the-art models, MGT
reduces the mean absolute error by up to 21% on crystal property prediction
tasks through multi-task self-supervised pretraining. Ablation experiments and
interpretable investigations confirm the effectiveness of each technique
implemented in our framework. Additionally, in transfer learning scenarios
including crystal catalyst adsorption energy and hybrid perovskite bandgap
prediction, MGT achieves performance improvements of up to 58% over existing
baselines, demonstrating domain-agnostic scalability across diverse application
domains. As evidenced by the above series of studies, we believe that MGT can
serve as useful model for crystal material property prediction, providing a
valuable tool for the discovery of novel materials.

</details>


### [217] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: 论文提出M-DESIGN，一种基于知识库的模型优化方法，通过动态查询和迭代优化解决传统静态模型选择的不足。


<details>
  <summary>Details</summary>
Motivation: 传统数据库中的静态模型选择方法忽略了任务与模型架构间的动态依赖关系，导致匹配效果不佳。

Method: 提出知识编织引擎，将模型优化转化为任务元数据的自适应查询问题，利用图关系知识模式进行细粒度分析。

Result: 在33个数据任务对中，M-DESIGN在26个任务中找到了最优模型。

Conclusion: M-DESIGN通过动态优化显著提升了模型选择的效率和效果。

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [218] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: FLock是一个去中心化框架，用于安全和高效的协作式大语言模型（LLM）微调，解决了集中控制和计算通信开销的问题。


<details>
  <summary>Details</summary>
Motivation: 解决集中式联邦学习（FL）的单点攻击和中毒攻击漏洞，以及在异构、无信任环境中微调70B参数模型的瓶颈。

Method: 集成基于区块链的信任层和经济激励，取代中央聚合器，实现不可信方之间的安全协作。

Result: 实验显示FLock能抵御后门中毒攻击，减少68%以上的对抗攻击成功率，并提升跨领域泛化能力。

Conclusion: FLock为去中心化LLM微调提供了安全高效的解决方案，显著优于传统方法。

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [219] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: PALM提出了一种统一的数学模型，用于分析主动学习（AL）的动态过程，通过四个关键参数预测性能，并在多个数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习方法仅关注最终准确性，无法全面评估学习过程的动态特性，因此需要一种更全面的评估模型。

Method: PALM通过四个参数（可达到的准确性、覆盖效率、早期性能和可扩展性）建模AL轨迹，并利用部分观测预测未来性能。

Result: PALM在CIFAR和ImageNet等数据集上验证，能准确预测学习曲线，并揭示AL方法的学习效率、数据覆盖和可扩展性。

Conclusion: PALM为AL提供了系统性、可重复的评估工具，有助于在资源受限的场景中选择高效策略。

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [220] [Learning to Gridize: Segment Physical World by Wireless Communication Channel](https://arxiv.org/abs/2507.15386)
*Juntao Wang,Feng Yin,Tian Ding,Tsung-Hui Chang,Zhi-Quan Luo,Qi Yan*

Main category: cs.LG

TL;DR: 论文提出了一种名为CSG的新型网格化框架，通过联合优化信道估计和网格化，仅使用RSRP数据实现高效分区。CSG-AE模型结合了编码器、量化器和物理解码器，并通过PIDA训练方案提升性能。实验表明，CSG在合成和真实数据上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有网格化方法（如GSG或BSG）依赖不可靠的位置数据或信号强度假设，限制了大规模网络优化的效率。

Method: 提出CSG框架，联合优化信道估计和网格化，使用RSRP数据估计CAPS并分区。开发CSG-AE模型，包含RSRP-to-CAPS编码器、稀疏码本量化器和物理解码器，并采用PIDA训练方案。

Result: 在合成数据上，CSG-AE在CAPS估计和聚类质量上表现优异；在真实数据上，RSRP预测误差显著降低（Active MAE降低30%，Overall MAE降低65%）。

Conclusion: CSG框架通过联合优化和PIDA训练，显著提升了网格化效率和信道一致性，为大规模网络优化提供了新思路。

Abstract: Gridization, the process of partitioning space into grids where users share
similar channel characteristics, serves as a fundamental prerequisite for
efficient large-scale network optimization. However, existing methods like
Geographical or Beam Space Gridization (GSG or BSG) are limited by reliance on
unavailable location data or the flawed assumption that similar signal
strengths imply similar channel properties. We propose Channel Space
Gridization (CSG), a pioneering framework that unifies channel estimation and
gridization for the first time. Formulated as a joint optimization problem, CSG
uses only beam-level reference signal received power (RSRP) to estimate Channel
Angle Power Spectra (CAPS) and partition samples into grids with homogeneous
channel characteristics. To perform CSG, we develop the CSG Autoencoder
(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse
codebook quantizer, and a physics-informed decoder based on the Localized
Statistical Channel Model. On recognizing the limitations of naive training
scheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous
(PIDA) training scheme for CSG-AE, ensuring stable and effective training by
systematically addressing the common pitfalls of the naive training paradigm.
Evaluations reveal that CSG-AE excels in CAPS estimation accuracy and
clustering quality on synthetic data. On real-world datasets, it reduces Active
Mean Absolute Error (MAE) by 30\% and Overall MAE by 65\% on RSRP prediction
accuracy compared to salient baselines using the same data, while improving
channel consistency, cluster sizes balance, and active ratio, advancing the
development of gridization for large-scale network optimization.

</details>


### [221] [MAP Estimation with Denoisers: Convergence Rates and Guarantees](https://arxiv.org/abs/2507.15397)
*Scott Pesme,Giacomo Meanti,Michael Arbel,Julien Mairal*

Main category: cs.LG

TL;DR: 论文提出了一种简单算法，证明其在log-concave先验假设下收敛到近端算子，为实践中常用的启发式方法提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有去噪模型在解决逆问题时缺乏理论支持，尤其是用预训练去噪器替代近端算子的做法。

Method: 提出一种与实践中常用方法相关的简单算法，证明其在log-concave先验下收敛到近端算子，并解释为平滑近端目标的梯度下降。

Result: 算法在理论分析中证明了其收敛性，为实践中成功但缺乏理论依据的方法提供了支持。

Conclusion: 研究为基于去噪器的启发式方法提供了理论依据，填补了实践与理论之间的空白。

Abstract: Denoiser models have become powerful tools for inverse problems, enabling the
use of pretrained networks to approximate the score of a smoothed prior
distribution. These models are often used in heuristic iterative schemes aimed
at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal
operator of the negative log-prior plays a central role. In practice, this
operator is intractable, and practitioners plug in a pretrained denoiser as a
surrogate-despite the lack of general theoretical justification for this
substitution. In this work, we show that a simple algorithm, closely related to
several used in practice, provably converges to the proximal operator under a
log-concavity assumption on the prior $p$. We show that this algorithm can be
interpreted as a gradient descent on smoothed proximal objectives. Our analysis
thus provides a theoretical foundation for a class of empirically successful
but previously heuristic methods.

</details>


### [222] [The calculus of variations of the Transformer on the hyperspherical tangent bundle](https://arxiv.org/abs/2507.15431)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 论文通过拉格朗日优化在令牌空间上为Transformer提供了理论数学背景，将其视为高维单位球上的流映射，并推导了其变分问题的欧拉-拉格朗日方程。


<details>
  <summary>Details</summary>
Motivation: 为Transformer提供数学理论基础，填补其在变分法领域的研究空白。

Method: 利用拉格朗日优化和变分法，将Transformer建模为高维单位球上的流映射，推导其欧拉-拉格朗日方程。

Result: 证明了Transformer可以作为变分问题的自然求解器，并提出了新的适用场景。

Conclusion: 论文为Transformer的变分法研究奠定了基础，提供了新的理论工具和应用方向。

Abstract: We offer a theoretical mathematical background to Transformers through
Lagrangian optimization across the token space. The Transformer, as a flow map,
exists in the tangent fiber for each token along the high-dimensional unit
sphere. The circumstance of the hypersphere across the latent data is
reasonable due to the trained diagonal matrix equal to the identity, which has
various empirical justifications. Thus, under the continuum limit of the
dynamics, the latent vectors flow among the tangent bundle. Using these facts,
we devise a mathematical framework for the Transformer through calculus of
variations. We develop a functional and show that the continuous flow map
induced by the Transformer satisfies this functional, therefore the Transformer
can be viewed as a natural solver of a calculus of variations problem. We
invent new scenarios of when our methods are applicable based on loss
optimization with respect to path optimality. We derive the Euler-Lagrange
equation for the Transformer. The variant of the Euler-Lagrange equation we
present has various appearances in literature, but, to our understanding,
oftentimes not foundationally proven or under other specialized cases. Our
overarching proof is new: our techniques are classical and the use of the flow
map object is original. We provide several other relevant results, primarily
ones specific to neural scenarios. In particular, much of our analysis will be
attempting to quantify Transformer data in variational contexts under neural
approximations. Calculus of variations on manifolds is a well-nourished
research area, but for the Transformer specifically, it is uncharted: we lay
the foundation for this area through an introduction to the Lagrangian for the
Transformer.

</details>


### [223] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 论文研究了RLHF中的过优化问题，提出了一种无需新标签的离策略校正奖励建模方法（OCRM），实验证明其优于标准RLHF方法。


<details>
  <summary>Details</summary>
Motivation: RLHF训练中，语言模型（LM）生成的响应逐渐偏离奖励模型（RM）的训练分布，导致RM不准确，即过优化问题。

Method: 提出OCRM方法，通过重要性加权对RM进行离策略校正，无需新标签或样本。

Result: 在摘要和聊天机器人数据集上的实验表明，OCRM显著优于标准RLHF方法和基线。

Conclusion: OCRM通过校正RM提高了策略的准确性，解决了RLHF中的过优化问题。

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [224] [An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations](https://arxiv.org/abs/2507.15442)
*Owen Douglas,Aku Kammonen,Anamika Pandey,Raúl Tempone*

Main category: cs.LG

TL;DR: 提出一种基于自适应随机傅里叶特征（ARFF）的训练算法，用于从快照数据中学习随机微分方程的漂移和扩散分量，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决从快照数据中学习随机微分方程漂移和扩散分量的挑战，提供更高效和准确的建模方法。

Method: 采用自适应随机傅里叶特征（ARFF）结合Metropolis采样和重采样，基于Euler-Maruyama积分的似然损失函数。

Result: 在多个基准问题中，ARFF方法在损失最小化和收敛速度上优于传统Adam优化方法。

Conclusion: ARFF方法为数据驱动的随机动力学建模提供了高效且性能优越的替代方案。

Abstract: This work proposes a training algorithm based on adaptive random Fourier
features (ARFF) with Metropolis sampling and resampling
\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and
diffusion components of stochastic differential equations from snapshot data.
Specifically, this study considers It\^{o} diffusion processes and a
likelihood-based loss function derived from the Euler-Maruyama integration
introduced in \cite{Dietrich2023} and
\cite{dridi2021learningstochasticdynamicalsystems}.
  This work evaluates the proposed method against benchmark problems presented
in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin
dynamics, a stochastic susceptible-infected-recovered model, and a stochastic
wave equation. Across all cases, the ARFF-based approach matches or surpasses
the performance of conventional Adam-based optimization in both loss
minimization and convergence speed. These results highlight the potential of
ARFF as a compelling alternative for data-driven modeling of stochastic
dynamics.

</details>


### [225] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的端到端框架Data Mixing Agent，用于自动调整源领域和目标领域数据的权重，以平衡模型性能，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决传统领域重加权策略依赖人工直觉或经验的问题，提出更通用的参数化启发式方法。

Method: 通过强化学习训练Data Mixing Agent，学习从大量数据混合轨迹中提取通用启发式规则。

Result: 在数学推理任务中表现优于基线，且能泛化到未见过的源领域、目标模型和领域空间。

Conclusion: Data Mixing Agent具有通用性和适应性，能高效平衡模型性能并减少源领域数据需求。

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [226] [FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning](https://arxiv.org/abs/2507.15470)
*Baran Can Gül,Suraksha Nadig,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: FedMultiEmo是一个隐私保护的多模态情感识别框架，通过联邦学习融合视觉和生理信号，解决了模态脆弱性、生理变异性和隐私风险问题。


<details>
  <summary>Details</summary>
Motivation: 解决车内情感识别中的模态脆弱性、生理变异性和隐私风险问题。

Method: 结合CNN和随机森林，采用联邦学习框架，通过多数投票融合多模态数据。

Result: 联邦CNN准确率77%，随机森林74%，融合后87%，与集中式基线相当，同时保护数据隐私。

Conclusion: FedMultiEmo为汽车环境中的实时隐私保护情感识别提供了实用解决方案。

Abstract: In-vehicle emotion recognition underpins adaptive driver-assistance systems
and, ultimately, occupant safety. However, practical deployment is hindered by
(i) modality fragility - poor lighting and occlusions degrade vision-based
methods; (ii) physiological variability - heart-rate and skin-conductance
patterns differ across individuals; and (iii) privacy risk - centralized
training requires transmission of sensitive data. To address these challenges,
we present FedMultiEmo, a privacy-preserving framework that fuses two
complementary modalities at the decision level: visual features extracted by a
Convolutional Neural Network from facial images, and physiological cues (heart
rate, electrodermal activity, and skin temperature) classified by a Random
Forest. FedMultiEmo builds on three key elements: (1) a multimodal federated
learning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud
prototype on Raspberry Pi clients and a Flower server, and (3) a personalized
Federated Averaging scheme that weights client updates by local data volume.
Evaluated on FER2013 and a custom physiological dataset, the federated
Convolutional Neural Network attains 77% accuracy, the Random Forest 74%, and
their fusion 87%, matching a centralized baseline while keeping all raw data
local. The developed system converges in 18 rounds, with an average round time
of 120 seconds and a per-client memory footprint below 200 MB. These results
indicate that FedMultiEmo offers a practical approach to real-time,
privacy-aware emotion recognition in automotive settings.

</details>


### [227] [An Investigation of Test-time Adaptation for Audio Classification under Background Noise](https://arxiv.org/abs/2507.15523)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.LG

TL;DR: 该研究通过测试时适应（TTA）技术解决音频分类中的域偏移问题，提出改进的CoNMix方法，在噪声环境下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 域偏移是深度学习中的常见问题，导致预训练模型在测试数据上性能下降。本研究旨在利用TTA技术解决音频分类中的域偏移问题。

Method: 采用TTT、TENT和CoNMix三种TTA方法，并在AudioMNIST和SpeechCommands V1数据集上测试其性能。

Result: 改进的CoNMix方法在噪声环境下表现最佳，错误率最低。

Conclusion: 本研究首次将TTA技术应用于音频分类中的域偏移问题，改进的CoNMix方法效果显著。

Abstract: Domain shift is a prominent problem in Deep Learning, causing a model
pre-trained on a source dataset to suffer significant performance degradation
on test datasets. This research aims to address the issue of audio
classification under domain shift caused by background noise using Test-Time
Adaptation (TTA), a technique that adapts a pre-trained model during testing
using only unlabelled test data before making predictions. We adopt two common
TTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and
investigate their respective performance on two popular audio classification
datasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types
of background noise and noise severity levels. The experimental results reveal
that our proposed modified version of CoNMix produced the highest
classification accuracy under domain shift (5.31% error rate under 10 dB
exercise bike background noise and 12.75% error rate under 3 dB running tap
background noise for AM) compared to TTT and TENT. The literature search
provided no evidence of similar works, thereby motivating the work reported
here as the first study to leverage TTA techniques for audio classification
under domain shift.

</details>


### [228] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: 研究探讨小规模LLMs是否能通过RLVR获得通用的ToM能力，发现其仅能提升训练数据性能，无法泛化到新任务。


<details>
  <summary>Details</summary>
Motivation: 探索RL方法是否能赋予LLMs更细腻的社会智能（如ToM）。

Method: 使用RLVR训练小规模LLMs，并在多个ToM数据集上评估其泛化能力。

Result: 小规模LLMs无法获得通用ToM能力，RL训练导致过拟合训练数据。

Conclusion: RLVR训练仅能实现窄范围的过拟合，而非真正的抽象ToM能力。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [229] [Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications](https://arxiv.org/abs/2507.15545)
*Yujia Shi,Emil Njor,Pablo Martínez-Nuevo,Sven Ewan Shepstone,Xenofon Fafoutis*

Main category: cs.LG

TL;DR: 论文提出了一种名为“数据感知可微分神经架构搜索”的方法，通过同时优化模型架构和输入数据特性，降低TinyML系统的设计复杂性。


<details>
  <summary>Details</summary>
Motivation: 机器学习的高资源消耗限制了其广泛应用，尤其是TinyML系统设计复杂，阻碍了其普及。

Method: 扩展了可微分神经架构搜索的搜索空间，包括数据配置参数，实现模型架构和输入数据的协同优化。

Result: 在关键词识别任务中，该方法生成了轻量且高精度的系统。

Conclusion: 该方法为TinyML系统设计提供了一种高效的新途径。

Abstract: The success of Machine Learning is increasingly tempered by its significant
resource footprint, driving interest in efficient paradigms like TinyML.
However, the inherent complexity of designing TinyML systems hampers their
broad adoption. To reduce this complexity, we introduce "Data Aware
Differentiable Neural Architecture Search". Unlike conventional Differentiable
Neural Architecture Search, our approach expands the search space to include
data configuration parameters alongside architectural choices. This enables
Data Aware Differentiable Neural Architecture Search to co-optimize model
architecture and input data characteristics, effectively balancing resource
usage and system performance for TinyML applications. Initial results on
keyword spotting demonstrate that this novel approach to TinyML system design
can generate lean but highly accurate systems.

</details>


### [230] [The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information](https://arxiv.org/abs/2507.15548)
*D. Abler,O. Pusterla,A. Joye-Kühnis,N. Andratschke,M. Bach,A. Bink,S. M. Christ,P. Hagmann,B. Pouymayou,E. Pravatà,P. Radojewski,M. Reyes,L. Ruinelli,R. Schaer,B. Stieltjes,G. Treglia,W. Valenzuela,R. Wiest,S. Zoergiebel,M. Guckenberger,S. Tanadini-Lang,A. Depeursinge*

Main category: cs.LG

TL;DR: 本研究评估了传统放射组学（CR）和深度学习（DL）MRI放射组学在预测胶质母细胞瘤预后（生存期≤6个月 vs >6个月）中的附加价值，发现其相对于临床和分子预测因子的优势有限。


<details>
  <summary>Details</summary>
Motivation: 放射组学在胶质母细胞瘤特征描述中显示出潜力，但其相对于临床和分子预测因子的附加价值尚未明确。本研究旨在通过多中心数据集验证CR和DL放射组学的预测能力。

Method: 研究纳入了1152名胶质母细胞瘤患者，结合临床、分子和MRI数据，开发并评估了CR和DL模型。通过不同特征集（仅影像、仅临床/分子、组合特征）和患者子集（S-1:所有患者，S-2:有分子数据，S-3:IDH野生型）进行亚分析。

Result: 在外部验证中，组合特征的CR模型AUC为0.75，略优于仅临床（0.74）和仅影像（0.68）模型。DL模型趋势相似但无统计学意义。组合模型在S-2和S-3中未优于仅临床模型。影像数据在总体生存预测中显示出更大的相关性。

Conclusion: 尽管确认了MRI序列对胶质母细胞瘤预后的预测价值，但标准CR和DL放射组学方法相对于年龄和性别等人口统计学预测因子的附加价值有限。

Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its
added value over clinical and molecular predictors has yet to be proven. This
study assessed the added value of conventional radiomics (CR) and deep learning
(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on
a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152
glioblastoma (WHO 2016) patients from five Swiss centers and one public source.
It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI
data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were
developed using standard methods and evaluated on internal and external
cohorts. Sub-analyses assessed models with different feature sets
(imaging-only, clinical/molecular-only, combined-features) and patient subsets
(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In
external validation, the combined-feature CR model achieved an AUC of 0.75,
slightly, but significantly outperforming clinical-only (0.74) and imaging-only
(0.68) models. DL models showed similar trends, though without statistical
significance. In S-2 and S-3, combined models did not outperform clinical-only
models. Exploratory analysis of CR models for overall survival prediction
suggested greater relevance of imaging data: across all subsets,
combined-feature models significantly outperformed clinical-only models, though
with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI
sequences for glioblastoma prognosis, this multi-center study found standard CR
and DL radiomics approaches offer minimal added value over demographic
predictors such as age and gender.

</details>


### [231] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: PhysGym是一个用于评估大型语言模型在交互式物理环境中科学推理能力的新基准套件和模拟平台。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估大型语言模型在科学发现能力方面的基准，尤其是在处理环境复杂性和利用先验知识方面。

Method: PhysGym通过提供对先验知识水平的精细控制，设计了一套交互式模拟任务，要求模型主动探索环境、收集数据并形成物理定律假设。

Result: PhysGym能够区分不同先验知识和任务复杂度下模型的性能差异，展示了其作为评估工具的实用性。

Conclusion: PhysGym填补了现有基准的空白，为研究大型语言模型的科学推理能力提供了标准化评估平台。

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce PhysGym, a novel benchmark
suite and simulation platform for rigorously assessing LLM-based scientific
reasoning in interactive physics environments. PhysGym's primary contribution
lies in its sophisticated control over the level of prior knowledge provided to
the agent. This allows researchers to dissect agent performance along axes
including the complexity of the problem and the prior knowledge levels. The
benchmark comprises a suite of interactive simulations, where agents must
actively probe environments, gather data sequentially under constraints and
formulate hypotheses about underlying physical laws. PhysGym provides
standardized evaluation protocols and metrics for assessing hypothesis accuracy
and model fidelity. We demonstrate the benchmark's utility by presenting
results from baseline LLMs, showcasing its ability to differentiate
capabilities based on varying priors and task complexity.

</details>


### [232] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: 论文探讨了住院时间（LOS）预测准确性与床位调度灵活性之间的关系，研究了在预测误差下最有效的患者调度策略以避免床位溢出并优化资源利用。


<details>
  <summary>Details</summary>
Motivation: 下游资源（如床位）的可用性对计划选择性手术患者入院至关重要，但实际LOS可能与预测值差异较大，导致调度不可行。因此需要研究如何通过调度策略应对这种不可行性。

Method: 基于先前提出的模拟机器学习方法，研究了不同纠正策略下LOS预测准确性与调度灵活性的关系，并分析了最有效的患者调度策略。

Result: 研究发现，更准确的LOS预测可以减少调度调整的影响，但训练高精度模型成本较高。通过灵活调度策略（如推迟入院或转移患者）可以有效避免床位溢出。

Conclusion: 在LOS预测存在误差时，灵活的调度策略比单纯追求预测精度更有效，能够平衡资源利用与患者需求。

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [233] [On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project](https://arxiv.org/abs/2507.15574)
*Gregory F. Stock,Juan A. Fraire,Holger Hermanns,Jędrzej Mosiężny,Yusra Al-Khazraji,Julio Ramírez Molina,Evridiki V. Ntagiou*

Main category: cs.LG

TL;DR: AI驱动的强化学习在卫星巨型星座管理中优化数据路由和资源分配，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 卫星星座快速扩张带来管理挑战，需创新方法实现高效、可扩展和弹性运营。

Method: 利用强化学习（RL）优化数据路由和资源分配，测试多种星座配置和场景。

Result: RL在延迟和资源利用上优于传统算法，提供更高灵活性和可扩展性。

Conclusion: AI可改变卫星星座管理，提供更自适应、稳健且经济的解决方案。

Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents
significant challenges in satellite network management, requiring innovative
approaches for efficient, scalable, and resilient operations. This paper
explores the role of Artificial Intelligence (AI) in optimizing the operation
of satellite mega-constellations, drawing from the ConstellAI project funded by
the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland
University, and Thales Alenia Space collaborates to develop AI-driven
algorithms and demonstrates their effectiveness over traditional methods for
two crucial operational challenges: data routing and resource allocation. In
the routing use case, Reinforcement Learning (RL) is used to improve the
end-to-end latency by learning from historical queuing latency, outperforming
classical shortest path algorithms. For resource allocation, RL optimizes the
scheduling of tasks across constellations, focussing on efficiently using
limited resources such as battery and memory. Both use cases were tested for
multiple satellite constellation configurations and operational scenarios,
resembling the real-life spacecraft operations of communications and Earth
observation satellites. This research demonstrates that RL not only competes
with classical approaches but also offers enhanced flexibility, scalability,
and generalizability in decision-making processes, which is crucial for the
autonomous and intelligent management of satellite fleets. The findings of this
activity suggest that AI can fundamentally alter the landscape of satellite
constellation management by providing more adaptive, robust, and cost-effective
solutions.

</details>


### [234] [We Need to Rethink Benchmarking in Anomaly Detection](https://arxiv.org/abs/2507.15584)
*Philipp Röchner,Simon Klüttermann,Franz Rothlauf,Daniel Schlör*

Main category: cs.LG

TL;DR: 论文指出当前异常检测算法的评估方式存在问题，导致进展停滞，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测算法的评估方法未能充分反映实际应用的多样性，限制了算法的发展。

Method: 提出重新设计异常检测的基准测试，包括基于通用分类法的场景识别、端到端分析以及目标导向的评估。

Result: 论文未提供具体实验结果，但指出了当前评估方法的局限性。

Conclusion: 需要改进异常检测的评估方法，以更好地反映实际应用需求，推动算法进步。

Abstract: Despite the continuous proposal of new anomaly detection algorithms and
extensive benchmarking efforts, progress seems to stagnate, with only minor
performance differences between established baselines and new algorithms. In
this position paper, we argue that this stagnation is due to limitations in how
we evaluate anomaly detection algorithms. Current benchmarking does not, for
example, sufficiently reflect the diversity of anomalies in applications
ranging from predictive maintenance to scientific discovery. Consequently, we
need to rethink benchmarking in anomaly detection. In our opinion, anomaly
detection should be studied using scenarios that capture the relevant
characteristics of different applications. We identify three key areas for
improvement: First, we need to identify anomaly detection scenarios based on a
common taxonomy. Second, anomaly detection pipelines should be analyzed
end-to-end and by component. Third, evaluating anomaly detection algorithms
should be meaningful regarding the scenario's objectives.

</details>


### [235] [Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario](https://arxiv.org/abs/2507.15587)
*Yinsong Chen,Kaifeng Wang,Xiaoqiang Meng,Xueyuan Li,Zirui Li,Xin Gao*

Main category: cs.LG

TL;DR: 提出了一种基于红队多智能体强化学习的框架，通过干扰和探索生成安全关键场景中的极端案例。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成安全关键场景时效率低且难以捕捉极端案例，需改进。

Method: 使用红队多智能体强化学习框架，通过干扰车辆探索极端案例，并结合约束图表示确保安全性。

Result: 实验表明，该框架显著影响自动驾驶车辆决策安全，并生成多种极端案例。

Conclusion: 该方法为安全关键场景研究提供了新方向。

Abstract: Current research on decision-making in safety-critical scenarios often relies
on inefficient data-driven scenario generation or specific modeling approaches,
which fail to capture corner cases in real-world contexts. To address this
issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,
where background vehicles with interference capabilities are treated as
red-team agents. Through active interference and exploration, red-team vehicles
can uncover corner cases outside the data distribution. The framework uses a
Constraint Graph Representation Markov Decision Process, ensuring that red-team
vehicles comply with safety rules while continuously disrupting the autonomous
vehicles (AVs). A policy threat zone model is constructed to quantify the
threat posed by red-team vehicles to AVs, inducing more extreme actions to
increase the danger level of the scenario. Experimental results show that the
proposed framework significantly impacts AVs decision-making safety and
generates various corner cases. This method also offers a novel direction for
research in safety-critical scenarios.

</details>


### [236] [Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity](https://arxiv.org/abs/2507.15601)
*Huiling Yang,Zhanwei Wang,Kaibin Huang*

Main category: cs.LG

TL;DR: 论文提出了一种C$^2$感知的联邦学习框架，通过优化批量大小控制来降低端到端学习延迟，同时确保收敛性。


<details>
  <summary>Details</summary>
Motivation: 6G网络中联邦学习的隐私保护特性使其在物联网应用中具有潜力，但高延迟和设备异构性成为挑战。

Method: 设计了一种C$^2$感知框架，通过平衡计算与通信的权衡，提出两种批量大小控制策略。

Result: 实验表明，该策略优于不考虑C$^2$权衡或设备异构性的传统方法。

Conclusion: 提出的框架有效解决了联邦学习中的延迟问题，适用于不同通信条件下的物联网应用。

Abstract: Federated learning (FL) has emerged as a popular approach for collaborative
machine learning in sixth-generation (6G) networks, primarily due to its
privacy-preserving capabilities. The deployment of FL algorithms is expected to
empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous
driving, augmented reality, and healthcare. The mission-critical and
time-sensitive nature of these applications necessitates the design of
low-latency FL frameworks that guarantee high learning performance. In
practice, achieving low-latency FL faces two challenges: the overhead of
computing and transmitting high-dimensional model updates, and the
heterogeneity in communication-and-computation (C$^2$) capabilities across
devices. To address these challenges, we propose a novel C$^2$-aware framework
for optimal batch-size control that minimizes end-to-end (E2E) learning latency
while ensuring convergence. The framework is designed to balance a fundamental
C$^2$ tradeoff as revealed through convergence analysis. Specifically,
increasing batch sizes improves the accuracy of gradient estimation in FL and
thus reduces the number of communication rounds required for convergence, but
results in higher per-round latency, and vice versa. The associated problem of
latency minimization is intractable; however, we solve it by designing an
accurate and tractable surrogate for convergence speed, with parameters fitted
to real data. This approach yields two batch-size control strategies tailored
to scenarios with slow and fast fading, while also accommodating device
heterogeneity. Extensive experiments using real datasets demonstrate that the
proposed strategies outperform conventional batch-size adaptation schemes that
do not consider the C$^2$ tradeoff or device heterogeneity.

</details>


### [237] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: 论文提出了一种深度学习替代模型，用于加速洪水事件中的河流预测，结合GRU和Geo-FNO，显著减少了计算时间。


<details>
  <summary>Details</summary>
Motivation: 传统物理求解器（如HEC-RAS）计算成本高，无法满足实时决策需求，需在不牺牲精度的情况下加速模拟。

Method: 采用混合自回归架构，结合GRU捕捉短期时间动态和Geo-FNO建模长程空间依赖，从HEC-RAS文件中提取特征向量进行训练。

Result: 模型在密西西比河流域67个河段上测试，预测精度高（中位绝对误差0.31英尺），计算时间从139分钟降至40分钟，提速3.5倍。

Conclusion: 数据驱动方法通过特征工程可替代传统水力模型，提升大规模洪水预测的计算可行性。

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


### [238] [Towards Explainable Anomaly Detection in Shared Mobility Systems](https://arxiv.org/abs/2507.15643)
*Elnur Isgandarov,Matteo Cederle,Federico Chiariotti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 提出了一种可解释的异常检测框架，结合多源数据（如共享单车记录、天气和公共交通）来优化共享出行系统。


<details>
  <summary>Details</summary>
Motivation: 识别共享出行系统中的异常对优化运营、提高服务可靠性和用户体验至关重要。

Method: 使用Isolation Forest算法进行无监督异常检测，并通过DIFFI算法提供可解释性。

Result: 站级分析能有效识别异常，外部因素（如恶劣天气和公共交通限制）影响显著。

Conclusion: 该框架有助于提升共享出行运营的决策质量。

Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role
in urban transportation. Identifying anomalies in these systems is essential
for optimizing operations, improving service reliability, and enhancing user
experience. This paper presents an interpretable anomaly detection framework
that integrates multi-source data, including bike-sharing trip records, weather
conditions, and public transit availability. The Isolation Forest algorithm is
employed for unsupervised anomaly detection, along with the Depth-based
Isolation Forest Feature Importance (DIFFI) algorithm providing
interpretability. Results show that station-level analysis offers a robust
understanding of anomalies, highlighting the influence of external factors such
as adverse weather and limited transit availability. Our findings contribute to
improving decision-making in shared mobility operations.

</details>


### [239] [GeoHNNs: Geometric Hamiltonian Neural Networks](https://arxiv.org/abs/2507.15678)
*Amine Mohamed Aboussalah,Abdessalam Ed-dib*

Main category: cs.LG

TL;DR: GeoHNN是一种基于几何先验的神经网络框架，通过显式编码物理定律的几何结构，显著提升了动力学建模的长期稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习方法常忽略物理系统的几何本质，导致预测不稳定，尤其是在高维和混沌系统中。GeoHNN旨在通过几何先验解决这一问题。

Method: GeoHNN结合黎曼几何（对称正定矩阵参数化惯性矩阵）和辛几何（约束自编码器保持相空间体积），显式编码物理定律的几何结构。

Result: 实验表明，GeoHNN在耦合振子和高维可变形物体等系统中表现优于现有模型，具有更好的长期稳定性、准确性和能量守恒性。

Conclusion: 嵌入物理几何结构不仅是理论需求，更是构建稳健、通用物理模型的实践必需。

Abstract: The fundamental laws of physics are intrinsically geometric, dictating the
evolution of systems through principles of symmetry and conservation. While
modern machine learning offers powerful tools for modeling complex dynamics
from data, common methods often ignore this underlying geometric fabric.
Physics-informed neural networks, for instance, can violate fundamental
physical principles, leading to predictions that are unstable over long
periods, particularly for high-dimensional and chaotic systems. Here, we
introduce \textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework
that learns dynamics by explicitly encoding the geometric priors inherent to
physical laws. Our approach enforces two fundamental structures: the Riemannian
geometry of inertia, by parameterizing inertia matrices in their natural
mathematical space of symmetric positive-definite matrices, and the symplectic
geometry of phase space, using a constrained autoencoder to ensure the
preservation of phase space volume in a reduced latent space. We demonstrate
through experiments on systems ranging from coupled oscillators to
high-dimensional deformable objects that GeoHNN significantly outperforms
existing models. It achieves superior long-term stability, accuracy, and energy
conservation, confirming that embedding the geometry of physics is not just a
theoretical appeal but a practical necessity for creating robust and
generalizable models of the physical world.

</details>


### [240] [Explainable Anomaly Detection for Electric Vehicles Charging Stations](https://arxiv.org/abs/2507.15718)
*Matteo Cederle,Andrea Mazzucco,Andrea Demartini,Eugenio Mazza,Eugenia Suriani,Federico Vitti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本文研究了电动汽车充电站中的异常检测问题，结合可解释人工智能技术以提高异常检测的透明度和根源分析。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电站的可靠性对可再生能源交通至关重要，但需要有效的异常检测方法以识别充电行为中的异常，并确定其根本原因。

Method: 采用无监督异常检测技术（如Isolation Forest）并结合DIFFI方法，以识别异常及其关键特征。

Result: 在真实工业案例中验证了所提方法的有效性。

Conclusion: 该方法能够有效检测异常并揭示其根源，为充电站管理提供了实用工具。

Abstract: Electric vehicles (EV) charging stations are one of the critical
infrastructures needed to support the transition to renewable-energy-based
mobility, but ensuring their reliability and efficiency requires effective
anomaly detection to identify irregularities in charging behavior. However, in
such a productive scenario, it is also crucial to determine the underlying
cause behind the detected anomalies. To achieve this goal, this study
investigates unsupervised anomaly detection techniques for EV charging
infrastructure, integrating eXplainable Artificial Intelligence techniques to
enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies
Isolation Forest to detect anomalies and employs the Depth-based Isolation
Forest Feature Importance (DIFFI) method to identify the most important
features contributing to such anomalies. The efficacy of the proposed approach
is evaluated in a real industrial case.

</details>


### [241] [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](https://arxiv.org/abs/2507.15727)
*Xuchuang Wang,Bo Sun,Hedyeh Beyhaghi,John C. S. Lui,Mohammad Hajiesmaili,Adam Wierman*

Main category: cs.LG

TL;DR: 论文提出了一种多代理滑雪租赁问题，扩展了经典滑雪租赁困境，引入了个体和共享成本。通过定义三种竞争比，设计了最优确定性和随机策略，并分析了对称策略的优势。


<details>
  <summary>Details</summary>
Motivation: 将经典滑雪租赁问题扩展到多代理场景，研究个体和共享成本下的决策优化。

Method: 定义了三种竞争比（整体、状态依赖、个体理性），设计了状态感知的确定性和随机策略。

Result: 对称策略优于非对称策略，提供了竞争比的上界和下界。

Conclusion: 研究扩展了经典滑雪租赁问题的理论，对群体决策具有理论和实践意义。

Abstract: This paper introduces a novel multi-agent ski-rental problem that generalizes
the classical ski-rental dilemma to a group setting where agents incur
individual and shared costs. In our model, each agent can either rent at a
fixed daily cost, or purchase a pass at an individual cost, with an additional
third option of a discounted group pass available to all. We consider scenarios
in which agents' active days differ, leading to dynamic states as agents drop
out of the decision process. To address this problem from different
perspectives, we define three distinct competitive ratios: overall,
state-dependent, and individual rational. For each objective, we design and
analyze optimal deterministic and randomized policies. Our deterministic
policies employ state-aware threshold functions that adapt to the dynamic
states, while our randomized policies sample and resample thresholds from
tailored state-aware distributions. The analysis reveals that symmetric
policies, in which all agents use the same threshold, outperform asymmetric
ones. Our results provide competitive ratio upper and lower bounds and extend
classical ski-rental insights to multi-agent settings, highlighting both
theoretical and practical implications for group decision-making under
uncertainty.

</details>


### [242] [Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks](https://arxiv.org/abs/2507.15769)
*Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil*

Main category: cs.LG

TL;DR: 提出了一种基于多模态感知的毫米波信号遮挡预测框架，结合摄像头、GPS、LiDAR和雷达数据，通过深度学习模型和软加权集成策略实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 毫米波频段的车载通信系统易受动态障碍物（如车辆、行人）的信号遮挡影响，需解决此问题以提高通信可靠性。

Method: 采用多模态感知（摄像头、GPS、LiDAR、雷达），利用模态特定的深度学习模型独立处理各传感器数据，并通过软加权集成策略融合输出。

Result: 摄像头单独模型的F1分数为97.1%，推理时间89.8ms；摄像头+雷达组合的F1分数提升至97.2%，推理时间95.7ms。

Conclusion: 多模态感知在毫米波遮挡预测中表现出高效性和准确性，为动态环境中的主动无线通信提供了可行方案。

Abstract: Vehicular communication systems operating in the millimeter wave (mmWave)
band are highly susceptible to signal blockage from dynamic obstacles such as
vehicles, pedestrians, and infrastructure. To address this challenge, we
propose a proactive blockage prediction framework that utilizes multi-modal
sensing, including camera, GPS, LiDAR, and radar inputs in an
infrastructure-to-vehicle (I2V) setting. This approach uses modality-specific
deep learning models to process each sensor stream independently and fuses
their outputs using a softmax-weighted ensemble strategy based on validation
performance. Our evaluations, for up to 1.5s in advance, show that the
camera-only model achieves the best standalone trade-off with an F1-score of
97.1% and an inference time of 89.8ms. A camera+radar configuration further
improves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness
and efficiency of multi-modal sensing for mmWave blockage prediction and
provide a pathway for proactive wireless communication in dynamic environments.

</details>


### [243] [Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis](https://arxiv.org/abs/2507.15772)
*Anoop C. Patil,Benny Jian Rong Sng,Yu-Wei Chang,Joana B. Pereira,Chua Nam-Hai,Rajani Sarojam,Gajendra Pratap Singh,In-Cheol Jang,Giovanni Volpe*

Main category: cs.LG

TL;DR: DIVA是一种基于变分自编码器的全自动工作流程，用于通过拉曼光谱检测植物应激，无需手动预处理，适用于多种生物和非生物应激源。


<details>
  <summary>Details</summary>
Motivation: 植物应激检测对农业至关重要，传统拉曼分析方法存在偏见和不一致的问题，需要自动化解决方案。

Method: DIVA使用变分自编码器处理原始拉曼光谱（包括荧光背景），无需手动预处理，自动识别和量化光谱特征。

Result: DIVA成功检测了多种植物应激，包括非生物（遮光、强光、高温）和生物（细菌感染）应激源。

Conclusion: DIVA结合深度学习和振动光谱，为AI驱动的植物健康评估提供了新途径，促进农业可持续发展。

Abstract: Detecting stress in plants is crucial for both open-farm and
controlled-environment agriculture. Biomolecules within plants serve as key
stress indicators, offering vital markers for continuous health monitoring and
early disease detection. Raman spectroscopy provides a powerful, non-invasive
means to quantify these biomolecules through their molecular vibrational
signatures. However, traditional Raman analysis relies on customized
data-processing workflows that require fluorescence background removal and
prior identification of Raman peaks of interest-introducing potential biases
and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation
of Vibrational Raman spectra for plant-stress Analysis), a fully automated
workflow based on a variational autoencoder. Unlike conventional approaches,
DIVA processes native Raman spectra-including fluorescence backgrounds-without
manual preprocessing, identifying and quantifying significant spectral features
in an unbiased manner. We applied DIVA to detect a range of plant stresses,
including abiotic (shading, high light intensity, high temperature) and biotic
stressors (bacterial infections). By integrating deep learning with vibrational
spectroscopy, DIVA paves the way for AI-driven plant health assessment,
fostering more resilient and sustainable agricultural practices.

</details>


### [244] [Dynamics is what you need for time-series forecasting!](https://arxiv.org/abs/2507.15774)
*Alexis-Raja Brachet,Pierre-Yves Richard,Céline Hudelot*

Main category: cs.LG

TL;DR: 论文提出通过系统和实证研究验证时间序列预测任务中模型需学习数据动态的假设，并提出PRO-DYN分类法分析模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在时间序列预测任务中表现不佳，假设原因是模型未能充分学习数据的动态特性。

Method: 提出PRO-DYN分类法，分析模型动态学习能力，并通过实验验证动态块位置的重要性。

Result: 实验表明，性能较差的架构仅部分学习动态，且动态块作为最终预测器至关重要。

Conclusion: 研究支持在模型中引入可学习的动态块并将其作为最终预测器的必要性。

Abstract: While boundaries between data modalities are vanishing, the usual successful
deep models are still challenged by simple ones in the time-series forecasting
task. Our hypothesis is that this task needs models that are able to learn the
data underlying dynamics. We propose to validate it through both systemic and
empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to
analyze existing models through the lens of dynamics. Two observations thus
emerged: $\textbf{1}$. under-performing architectures learn dynamics at most
partially, $\textbf{2}$. the location of the dynamics block at the model end is
of prime importance. We conduct extensive experiments to confirm our
observations on a set of performance-varying models with diverse backbones.
Results support the need to incorporate a learnable dynamics block and its use
as the final predictor.

</details>


### [245] [Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets](https://arxiv.org/abs/2507.15784)
*Zihang Ma,Qitian Yin*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wasserstein-Rubinstein距离的专家融合模型（WR-EFM），用于解决图节点分类任务中不同类别分类难度差异的问题，显著提升了分类平衡性和性能。


<details>
  <summary>Details</summary>
Motivation: 在PubMed引文网络数据集中，传统GCN模型对不同类别的分类性能差异显著（如类别2的准确率比类别1低7.5%），需要一种方法来平衡和提升分类性能。

Method: 提出WR-EFM模型，针对不同类别训练专用GNN模型（如类别0/1使用层归一化和残差连接的GNN，类别2使用多跳图注意力网络GAT），并通过Wasserstein-Rubinstein距离优化模型间表示相似性，动态加权融合。

Result: WR-EFM在三个类别上的准确率分别为77.8%、78.0%和79.9%，优于单一模型和标准融合方法，且类别间准确率变异系数降低77.6%，类别2准确率提升5.5%。

Conclusion: WR-EFM通过WR距离引导的融合策略，有效解决了类别不平衡问题，为图分类任务提供了新范式，代码已开源。

Abstract: Graph node classification is a fundamental task in graph neural networks
(GNNs), aiming to assign predefined class labels to nodes. On the PubMed
citation network dataset, we observe significant classification difficulty
disparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,
7.5% lower than Category 1. To address this, we propose a
Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),
training specialized GNN models for Categories 0/1 (with layer normalization
and residual connections) and Multi-hop Graph Attention Networks (GAT) for
Category 2. The WR distance metric optimizes representation similarity between
models, particularly focusing on improving Category 2 performance. Our adaptive
fusion strategy dynamically weights models based on category-specific
performance, with Category 2 assigned a GAT weight of 0.8. WR distance further
guides the fusion process by measuring distributional differences between model
representations, enabling more principled integration of complementary
features.
  Experimental results show WR-EFM achieves balanced accuracy across
categories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),
outperforming both single models and standard fusion approaches. The
coefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%
lower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM
improves Category 2 accuracy by 5.5% compared to GCN, verifying the
effectiveness of WR-guided fusion in capturing complex structural patterns.
This work provides a novel paradigm for handling class-imbalanced graph
classification tasks. To promote the research community, we release our project
at https://github.com/s010m00n/GASEM4NC.

</details>


### [246] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: 提出了一种名为CSE-FSL的新型高效通信和存储联邦分裂学习方法，通过辅助网络和选择性数据传输减少通信和存储开销。


<details>
  <summary>Details</summary>
Motivation: 联邦分裂学习（FSL）虽然能减轻边缘设备的计算负担，但仍存在高通信开销和服务器存储需求大的问题。

Method: CSE-FSL利用辅助网络在客户端本地更新权重，服务器仅维护单一模型，并选择性传输数据以减少通信量。

Result: 理论分析和实验结果表明，CSE-FSL在非凸损失函数下收敛，且显著降低了通信开销。

Conclusion: CSE-FSL是一种高效且实用的联邦分裂学习解决方案。

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [247] [Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction](https://arxiv.org/abs/2507.15832)
*Shiyang Li*

Main category: cs.LG

TL;DR: 提出了一种混合CNN-LSTM-attention-Adaboost模型，结合改进的蛇群优化算法（SO），用于提升4D轨迹预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决中长期4D轨迹预测模型的局限性。

Method: 结合CNN提取空间特征、LSTM捕捉时间特征、注意力机制捕获全局特征，并使用Adaboost集成多个弱学习器，通过改进的SO算法优化超参数。

Result: 在真实ADS-B数据上测试，SO-CLA-Adaboost优于传统优化器，预测精度提升39.89%。

Conclusion: 该模型在4D轨迹预测中表现出色，改进的SO算法显著提升了性能。

Abstract: To address the limitations of medium- and long-term four-dimensional (4D)
trajectory prediction models, this paper proposes a hybrid
CNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy
improved snake-herd optimization (SO) algorithm. The model applies the Adaboost
algorithm to divide multiple weak learners, and each submodel utilizes CNN to
extract spatial features, LSTM to capture temporal features, and attention
mechanism to capture global features comprehensively. The strong learner model,
combined with multiple sub-models, then optimizes the hyperparameters of the
prediction model through the natural selection behavior pattern simulated by
SO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the
comparison experiments and ablation studies of multiple optimizers are carried
out, and a comprehensive test and evaluation analysis is carried out. The
results show that SO-CLA-adaboost outperforms traditional optimizers such as
particle swarm, whale, and gray wolf in handling large-scale high-dimensional
trajectory data. In addition, introducing the full-strategy collaborative
improvement SO algorithm improves the model's prediction accuracy by 39.89%.

</details>


### [248] [Optimizing Canaries for Privacy Auditing with Metagradient Descent](https://arxiv.org/abs/2507.15836)
*Matteo Boglioni,Terrance Liu,Andrew Ilyas,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 本文提出了一种优化差分隐私学习算法隐私审计的方法，通过改进审计员的“金丝雀”集，显著提高了隐私参数的下界估计。


<details>
  <summary>Details</summary>
Motivation: 研究黑盒隐私审计，旨在仅通过算法输出（如训练模型）来估计差分隐私学习算法的隐私参数下界。

Method: 利用元梯度优化技术优化审计员的金丝雀集，提升隐私审计效果。

Result: 实验表明，优化后的金丝雀集可将差分隐私图像分类模型的隐私参数下界提高2倍以上，且该方法具有可迁移性和高效性。

Conclusion: 优化金丝雀集是提升差分隐私算法隐私审计效果的有效方法，具有实际应用潜力。

Abstract: In this work we study black-box privacy auditing, where the goal is to lower
bound the privacy parameter of a differentially private learning algorithm
using only the algorithm's outputs (i.e., final trained model). For DP-SGD (the
most successful method for training differentially private deep learning
models), the canonical approach auditing uses membership inference-an auditor
comes with a small set of special "canary" examples, inserts a random subset of
them into the training set, and then tries to discern which of their canaries
were included in the training set (typically via a membership inference
attack). The auditor's success rate then provides a lower bound on the privacy
parameters of the learning algorithm. Our main contribution is a method for
optimizing the auditor's canary set to improve privacy auditing, leveraging
recent work on metagradient optimization. Our empirical evaluation demonstrates
that by using such optimized canaries, we can improve empirical lower bounds
for differentially private image classification models by over 2x in certain
instances. Furthermore, we demonstrate that our method is transferable and
efficient: canaries optimized for non-private SGD with a small model
architecture remain effective when auditing larger models trained with DP-SGD.

</details>


### [249] [FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs](https://arxiv.org/abs/2507.15839)
*Anh Nguyen,Sam Schafft,Nicholas Hale,John Alfaro*

Main category: cs.LG

TL;DR: 提出了一种基于LLM的高效、低成本表格数据合成方法，通过生成可重用的采样脚本，显著减少了时间和成本负担。


<details>
  <summary>Details</summary>
Motivation: 解决直接使用LLM生成每条记录时的高成本和时间问题，特别是在需要大规模合成数据时。

Method: 利用LLM推断并编码每个字段的分布到可重用的采样脚本中，自动分类字段类型（数值、分类或自由文本），生成基于分布的脚本。

Result: 实验表明，该方法在多样性和数据真实性上优于传统直接方法，大幅降低了大规模合成数据的生成负担。

Conclusion: 该方法可加速生产管道的测试，缩短开发周期，提高系统效率，为合成数据生成提供了可扩展、低成本的解决方案。

Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios
where real-world data collection and usage are limited by cost and scarcity.
Large language models (LLMs) have demonstrated remarkable capabilities in
producing high-fidelity, domain-relevant samples across various fields.
However, existing approaches that directly use LLMs to generate each record
individually impose prohibitive time and cost burdens, particularly when large
volumes of synthetic data are required. In this work, we propose a fast,
cost-effective method for realistic tabular data synthesis that leverages LLMs
to infer and encode each field's distribution into a reusable sampling script.
By automatically classifying fields into numerical, categorical, or free-text
types, the LLM generates distribution-based scripts that can efficiently
produce diverse, realistic datasets at scale without continuous model
inference. Experimental results show that our approach outperforms traditional
direct methods in both diversity and data realism, substantially reducing the
burden of high-volume synthetic data generation. We plan to apply this
methodology to accelerate testing in production pipelines, thereby shortening
development cycles and improving overall system efficiency. We believe our
insights and lessons learned will aid researchers and practitioners seeking
scalable, cost-effective solutions for synthetic data generation.

</details>


### [250] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: 扩散模型在数据受限场景下优于自回归模型，尤其在计算资源充足时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在数据受限情况下相对于自回归模型的优势，探索其潜在的数据增强特性。

Method: 系统研究掩码扩散模型在数据受限环境中的表现，分析其与自回归模型的性能对比。

Result: 扩散模型在计算资源充足但数据稀缺时显著优于自回归模型，验证损失更低且下游任务表现更优。

Conclusion: 当数据成为瓶颈时，扩散模型是自回归模型的有力替代方案。

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [251] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 本文提出了一种基于量子场论类比的理论框架（自由意志方程），为AGI决策过程引入受控随机性，以提升其适应性和创造性。


<details>
  <summary>Details</summary>
Motivation: 传统AGI研究专注于确定性规则下的目标优化，但人类智能具有自发性决策能力（类似自由意志），这对创造力和适应性至关重要。

Method: 通过将AI认知状态视为潜在行动的叠加态（类似量子波函数），并结合量子场论机制和内在动机项，设计决策框架。

Result: 在非稳态多臂老虎机环境中的实验表明，该框架下的智能体比基线方法获得更高奖励和策略多样性。

Conclusion: 该框架为AGI引入受控随机性，有望提升其适应性和创造性，未来可进一步探索其应用潜力。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [252] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个基于DFT的多智能体框架，通过LLM代理实现材料发现的高通量、高保真模拟，显著减少对人类专家的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决DFT模拟中训练时间长、参数调优复杂和系统错误处理困难的问题。

Method: 采用分层多智能体框架，结合LLM规划代理和领域专用代理，共享画布辅助协作。

Result: 在Sol27LC基准测试中误差低于1%，解决了CO/Pt(111)吸附难题，并验证了FCC位点偏好。

Conclusion: DREAMS实现了L3级自动化，为高通量、高保真计算材料发现提供了可扩展路径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [253] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard是一个用于评估网络代理行为风险的数据集，旨在开发安全措施。研究发现当前LLMs在预测高风险行为时表现不佳，但通过微调模型可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs驱动的自主网络代理快速发展，其潜在风险（如意外或有害行为）亟需有效安全措施。

Method: 引入WebGuard数据集，包含4,939个人工标注行为，采用三级风险分类（SAFE、LOW、HIGH），并支持不同泛化场景的评估。

Result: 前沿LLMs预测行为结果的准确率低于60%，高风险行为召回率低于60%。微调后的Qwen2.5VL-7B模型将准确率从37%提升至80%，高风险召回率从20%提升至76%。

Conclusion: 尽管性能显著提升，但仍未达到高风险部署所需的近乎完美的准确率和召回率标准。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [254] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator是一个开源系统，利用大型语言模型将研究论文或自然语言提示转换为解释性动画，旨在简化复杂STEM主题的可视化教育内容创作。


<details>
  <summary>Details</summary>
Motivation: 解决学习者理解复杂科学和数学概念的困难，以及手动创建动态可视化内容的高门槛问题。

Method: 通过LLM解析输入文本或PDF生成结构化场景描述，再由另一LLM将其转换为可执行的Manim Python代码。

Result: 开发了Manimator系统，能够快速生成高质量的教育动画，降低内容创作门槛。

Conclusion: Manimator有潜力成为教育工具，促进复杂STEM主题的可视化解释，推动教育内容民主化。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [255] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: OnT是一种新的本体嵌入方法，通过结合预训练语言模型和双曲几何建模，有效整合文本信息并保留逻辑结构。


<details>
  <summary>Details</summary>
Motivation: 现有本体嵌入方法要么忽略文本信息，要么无法保留逻辑结构，OnT旨在解决这些问题。

Method: OnT通过双曲空间中的几何建模调整预训练语言模型，结合文本标签并保留EL描述逻辑的类层次和逻辑关系。

Result: 在四个真实本体上的实验表明，OnT在公理预测和推理任务上均优于现有方法，并展示了强大的迁移学习能力。

Conclusion: OnT在整合文本和保留逻辑结构方面表现出色，具有实际应用潜力。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [256] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass是一种混合方法，通过结合大型语言模型（LLM）和专用证明器（如DSP-v1.5），在不增加训练成本的情况下，显著提高了数学推理的计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大型通用模型或小型专用模型，各有局限性，且训练大型专用模型计算成本高。ProofCompass旨在通过结合两者的优势，解决这些问题。

Method: ProofCompass利用LLM提供自然语言证明策略和分析失败尝试，指导专用证明器（如DSP-v1.5）分解问题，无需额外模型训练。

Result: 在miniF2F基准测试中，ProofCompass比DSP-v1.5表现更好（55.3% vs. 54.9%），且尝试次数减少25倍（128 vs. 3200）。

Conclusion: ProofCompass展示了在形式定理证明中同时提高计算效率和准确性的潜力，为未来研究提供了新方向。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [257] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect是一个多代理系统框架，通过自动化工作流合成和迭代提示优化，显著提升了推理模型的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRMs）在解决新问题时表现不佳，倾向于记忆而非真正推理，存在泛化能力不足的问题。

Method: 提出Nexus Architect，结合自动化工作流合成和迭代提示优化机制，生成定制化推理流程。

Result: 在逻辑问题数据集上，Nexus Architect性能显著优于现有LRMs，最高提升66%通过率。

Conclusion: Nexus Architect通过优化工作流和提示，有效解决了LRMs的泛化问题，性能显著提升。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [258] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 通过结合推理模型与人类专家的协作，以及引入非推理模型快速筛选问题，显著降低了错误率和延迟，同时节省成本。


<details>
  <summary>Details</summary>
Motivation: 在风险敏感领域，AI模型的错误率需接近0%，而现有推理模型仍存在错误率高和延迟长的问题。

Method: 提出两种方法：1) 推理模型与人类专家协作，通过推理轨迹长度量化不确定性并决定是否转交人类；2) 在推理模型前加入非推理模型，快速筛选简单问题以减少延迟。

Result: 协作方法将Qwen3 235B-A22B的错误率从3%降至1%以下；非推理模型方法为DeepSeek R1节省50%成本并减少40%延迟。

Conclusion: 通过系统工程设计（无需访问LLM内部），可显著改善推理模型的错误率和延迟问题。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [259] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 研究发现，增加大型推理模型（LRMs）的推理长度会降低性能，表现为测试计算量与准确度之间的反比关系。任务涵盖四类，揭示了五种失败模式。


<details>
  <summary>Details</summary>
Motivation: 探讨测试计算量扩展对模型推理能力的影响，揭示潜在问题模式。

Method: 构建四类评估任务，观察模型在不同推理长度下的表现。

Result: 发现五种失败模式，包括分心、过拟合、虚假关联等，部分模型表现出不良行为。

Conclusion: 测试计算量扩展需谨慎，需多样化评估推理长度以识别和解决问题。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [260] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine是一个多步骤代理规划框架，通过结构化设计和明确指令提升企业环境中代理系统的执行稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中代理系统常因缺乏领域特定知识导致执行不稳定，Routine旨在解决这一问题。

Method: 提出Routine框架，包含清晰结构、明确指令和无缝参数传递，并构建训练数据集进行微调。

Result: Routine显著提升模型执行准确性，GPT-4o从41.1%提升至96.3%，Qwen3-14B从32.6%提升至83.3%。微调后Qwen3-14B进一步提升至88.2%。

Conclusion: Routine有效提升代理系统在企业环境中的稳定性和适应性，加速AI流程的部署。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [261] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion框架通过结合语义和结构学习，显著提升了生物医学知识图谱的推理能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱的完成和推理具有挑战性，现有方法难以实现语义与结构的协同进化。

Method: BioGraphFusion通过张量分解建立全局语义基础，结合LSTM动态优化关系嵌入，并采用查询引导的子图构建和混合评分机制。

Result: 在三个生物医学任务中表现优于现有方法，并通过案例研究验证了其生物学意义。

Conclusion: BioGraphFusion为生物医学知识图谱的语义与结构协同学习提供了有效解决方案。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [262] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico是一个模块化、事件驱动的框架，用于构建适用于嵌入式系统的自主代理，解决了现有框架在资源受限环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型和自主代理框架在动态环境和资源受限场景中表现不佳，依赖云端计算且缺乏持久自主性和环境感知能力。

Method: Amico采用Rust编写，支持通过WebAssembly在嵌入式平台和浏览器环境中高效运行，提供事件处理、状态管理、行为执行和推理模块集成的抽象。

Result: Amico构建了一个统一的框架，支持在计算资源有限和间歇性连接的环境中部署弹性、交互式代理。

Conclusion: Amico为资源受限环境中的自主代理提供了一种高效、安全的解决方案。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [263] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 多模态训练（结合文本和视觉输入）在Othello游戏中提升了模型性能和内部表示的鲁棒性，表明视觉输入有助于语言模型理解结构化世界。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否仅通过文本就能理解世界，还是需要多模态（如视觉）的辅助。

Method: 引入VISOTHELLO模型，结合移动历史和棋盘图像进行多模态训练，并与单模态基线模型比较。

Result: 多模态训练提高了模型性能和内部表示的鲁棒性。

Conclusion: 视觉输入有助于语言模型推断结构化世界表示，支持多模态学习的有效性。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [264] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 论文提出OE-Assist框架，通过自动化和半自动化验证能力问题（CQ）来辅助本体评估，利用大型语言模型（LLM）进行首次系统性研究。


<details>
  <summary>Details</summary>
Motivation: 本体评估通过功能需求（如CQ验证）是耗时、劳动密集且易出错的任务，即使对专家也是如此。

Method: 引入OE-Assist框架，利用包含1,393个CQ的数据集，评估LLM在自动CQ验证中的效果，并开发基于LLM的辅助工具。

Result: 自动化的LLM评估（o1-preview和o3-mini）表现与普通用户相当。

Conclusion: LLM辅助的本体评估具有潜力，可减少人工成本并提高效率。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [265] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 本文提出了一种用于人工智能情感表示的几何框架Coordinate Heart System（CHS），通过将八种核心情感定位为单位圆上的坐标，实现复杂情感状态的数学计算。


<details>
  <summary>Details</summary>
Motivation: 传统分类情感模型无法充分表示复杂心理场景，因此开发了具有数学保证的八情感系统，填补情感空间的覆盖空白。

Method: 将自然语言输入转换为情感坐标，通过坐标混合和向量运算实现实时情感插值，并引入稳定性参数S动态整合情感负荷和冲突解决。

Result: 实验验证表明，该系统能处理传统模型无法表示的情感冲突状态和复杂心理场景。

Conclusion: 该工作为人工智能情感建模奠定了新的数学基础。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [266] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 论文提出了一种基于比较学习的框架，用于校准项目特定的故事点预测模型，通过开发者对任务对的比较判断来训练模型，减少了认知负担，效果与回归模型相当。


<details>
  <summary>Details</summary>
Motivation: 传统的故事点估计方法（如计划扑克）耗时且繁琐，机器学习虽能减轻负担，但现有模型需依赖同一项目的历史数据。本文旨在通过比较学习框架，简化故事点估计过程。

Method: 开发者通过比较任务对的努力程度提供判断，而非直接分配故事点值。基于这些比较判断，训练机器学习模型预测故事点。

Result: 在16个项目、23,313个手动估计数据上的实验表明，模型预测与真实故事点的Spearman等级相关系数平均为0.34，性能与回归模型相当。

Conclusion: 比较学习方法比回归方法更高效，且降低了开发者的认知负担，符合比较判断法则。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [267] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 论文通过模拟恶意多智能体系统（MAS）的合谋风险，发现去中心化系统比中心化系统更具破坏性，且能规避传统干预措施。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统的普及，多智能体系统在复杂现实场景中的潜在风险尚未充分研究，尤其是恶意合谋行为。

Method: 提出一个支持中心化和去中心化结构的灵活框架，模拟恶意MAS在虚假信息传播和电商欺诈中的行为。

Result: 去中心化系统在实施恶意行为时更有效，能灵活调整策略以规避检测。

Conclusion: 研究强调了对恶意MAS的检测和应对措施的紧迫性，并提供了开源代码支持进一步研究。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [268] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个可配置的多代理框架，用于自动化评估基于LLM的系统，通过模块化设计和动态反馈实现高效、多样化的测试。


<details>
  <summary>Details</summary>
Motivation: 静态基准和手动测试无法满足LLM代理的复杂行为评估需求，需要一种自动化、可扩展的解决方案。

Method: Neo结合问题生成代理和评估代理，通过共享上下文中心模块化设计，支持动态反馈和多样化对话生成。

Result: 在测试中，Neo发现了边缘案例故障，性能接近人类专家，且效率提升10-12倍。

Conclusion: Neo为可扩展、自适应的LLM质量评估奠定了基础，其框架可扩展至更复杂的测试场景。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [269] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI是一个用于生成和管理定制化、基于政策的安全评估的平台，通过将自然语言安全政策转化为对抗性提示，并使用AI评分器评估模型响应。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在现实应用中的普及，可扩展且严格的安全评估变得至关重要。

Method: Aymara AI将自然语言安全政策转化为对抗性提示，并使用基于AI的评分器（经人类判断验证）评估模型响应。

Result: 评估了20个商业LLMs在10个安全领域的表现，发现性能差异显著（平均安全分数52.4%至86.2%），复杂领域表现较差（如隐私与冒充领域平均24.3%）。

Conclusion: LLMs的安全性具有不一致性和情境依赖性，需要可扩展、可定制的工具（如Aymara AI）支持负责任的AI开发和监管。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [270] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: 论文探讨了生成式AI与城市规划的结合，提出了AI城市规划师的概念，并指出了当前研究的不足与未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI（如VAEs、GANs、transformers和扩散模型）在城市规划中的潜力，以解决土地利用配置的生成问题。

Method: 通过文献综述，分析生成式AI在城市设计中的应用，并识别当前研究的四大局限。

Result: 发现研究在理论指导、多空间分辨率、数据驱动的设计知识增强以及实际交互方面存在不足。

Conclusion: 提出未来研究方向，包括理论引导生成、数字孪生和人机协同设计，呼吁生成智能与参与式城市主义的结合。

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [271] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly是一个结合语言模型（LM）代理和强化学习（RL）的可扩展框架，旨在通过多轮交互和工具支持提升LM代理的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LM代理主要通过提示工程或监督微调构建，而强化学习在提升LM能力方面的潜力尚未被系统研究。

Method: 开发了AgentFly框架，支持多轮交互、工具定义和异步执行，并采用集中式资源管理。

Result: 框架在多任务中成功训练代理，展示了其有效性和可扩展性。

Conclusion: AgentFly为LM代理与强化学习的结合提供了系统化解决方案，具有广泛的应用潜力。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [272] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: 本文提出InsightX Agent，一种基于LMM的交互式、可解释的X射线无损检测框架，通过SDMSD和EGR工具提升检测可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的X射线检测方法缺乏交互性、可解释性和自我评估能力，限制了其可靠性和操作员信任。

Method: InsightX Agent以LMM为核心协调SDMSD和EGR工具，SDMSD生成多尺度缺陷区域提案并通过NMS优化，EGR通过链式思维验证和优化提案。

Result: 在GDXray+数据集上，InsightX Agent的F1分数达96.35%，显著提升了分析的可解释性和可信度。

Conclusion: InsightX Agent展示了基于LMM的代理框架在工业检测任务中的变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [273] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: LLMs在简单环境中表现优于传统RL方法，但在复杂环境中因规划和推理能力不足而表现不佳，需进一步研究混合策略和微调。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在自主决策环境中的适用性，探索其与传统RL方法的性能差异。

Method: 在线结构化提示策略，比较LLMs与传统RL在序列决策任务中的零样本性能。

Result: LLMs在简单环境中初始表现更好，但在复杂环境中表现不佳；反馈机制可能降低性能。

Conclusion: 需进一步研究混合策略、微调及高级记忆集成以提升LLMs的决策能力。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [274] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: 论文提出了一种名为Endless Tuning的设计方法，通过双重镜像过程实现可靠的人工智能部署，避免人类被替代并填补责任空白。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能部署中的人类替代问题和责任空白问题（Matthias 2004）。

Method: 采用双重镜像过程，开发了一个协议并在三个原型应用（贷款审批、肺炎诊断和艺术风格识别）中实施，与领域专家合作测试。

Result: 实验结果显示用户对决策过程有完全控制感，同时在损害情况下建立了责任与问责的桥梁。

Conclusion: Endless Tuning方法在用户体验和责任分配方面取得了积极成果，为人工智能伦理提供了一种新视角。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [275] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 本文探讨了基于大型语言模型（LLM）的Agentic AI在老年护理中的潜力与挑战，强调其自主决策能力及对隐私、伦理的关注。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化问题需要创新解决方案，Agentic AI有望通过个性化健康管理和环境监控提升老年人生活质量。

Method: 分析了Agentic AI在老年护理中的应用，包括健康跟踪、认知护理和环境管理，并讨论了其伦理和隐私问题。

Result: Agentic AI能显著改善老年护理，但需解决数据隐私、决策独立性和访问权限等问题。

Conclusion: 需制定伦理保障措施和透明决策机制，以实现Agentic AI在老年护理中的负责任应用。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [276] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 论文研究了命题溯因中的细粒度推理，引入了“facet”概念以区分解释中的相关性和可替换性，并分析了不同设置下的复杂性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过更细粒度的方式理解命题溯因中的解释变异性，以提升诊断和规划等应用的效率。

Method: 引入“facet”概念，分析其在命题溯因中的作用，并结合Post框架进行复杂性分类。

Result: 提供了对命题溯因中解释变异性更深入的理解，并在Post框架中几乎完成了复杂性分类。

Conclusion: 通过facet和解释距离的分析，为命题溯因的细粒度推理提供了新视角，同时保持了计算复杂性优势。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [277] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign是一个基于强化学习的框架，旨在提升大语言模型的安全推理能力，避免有害内容生成和过度拒绝问题。


<details>
  <summary>Details</summary>
Motivation: 当前的安全对齐方法存在浅层拒绝或依赖密集监督的问题，未能充分利用模型内在的安全意识。

Method: AlphaAlign采用双奖励系统：可验证的安全奖励和标准化帮助奖励，激励模型主动进行安全推理。

Result: AlphaAlign在简化训练、打破安全-效用权衡以及深度对齐方面表现出显著优势。

Conclusion: AlphaAlign通过强化学习有效提升模型的安全推理能力，同时保持任务性能。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [278] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），用于改进传统模型在强制选择测试中的局限性，并通过实验验证了其准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在智能时代，心理测量测试在人员选拔、职业发展和心理健康评估中日益重要。强制选择测试因其能降低回答失真风险而常用，但传统模型存在局限性。

Method: 通过非线性映射挖掘参与者和项目特征，使用多层神经网络建模其交互，并利用单调性假设提升诊断结果的可解释性。

Result: 在真实和模拟数据集上的实验验证了FCNCD的准确性、可解释性和鲁棒性。

Conclusion: FCNCD模型有效克服了传统模型的限制，适用于强制选择测试的三种常见项目块类型。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [279] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 论文提出了一种基于差分进化（DE）的方法，优化对抗性提示后缀以攻击RAG系统，实验表明其攻击成功率高且难以检测。


<details>
  <summary>Details</summary>
Motivation: 对抗性提示攻击会显著影响RAG系统的可靠性，因此需要研究如何优化对抗性后缀以模拟真实场景的攻击。

Method: 采用差分进化（DE）方法，将RAG系统视为黑箱，优化对抗性提示后缀以提升错误文档的检索排名。

Result: 在BEIR QA数据集上，DE方法在攻击成功率上表现优异，且仅需少量标记（≤5个）。生成的对抗性后缀难以被检测。

Conclusion: DE方法在攻击RAG系统时高效且隐蔽，为对抗性攻击防御提供了新的挑战。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [280] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果推理的内在奖励机制CAIS，用于强化学习，以解决传统相关性奖励在噪声环境中的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 人类婴儿能够稳健地发现自身的因果效能，而传统的强化学习代理在噪声环境中表现脆弱，因此需要一种更鲁棒的奖励机制。

Method: 引入Causal Action Influence Score (CAIS)，通过计算动作条件分布与基线分布之间的1-Wasserstein距离来量化动作的因果影响。

Result: 在模拟婴儿-移动环境中，CAIS成功过滤噪声并学习正确策略，同时还能复现“消退爆发”现象。

Conclusion: 显式推断因果关系是发展鲁棒代理感的关键机制，为自适应自主系统提供了心理学合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [281] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 论文提出了一种结合DL-Lite本体和自动化规划的新方法，通过多项式编译实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（如本体）融入自动化规划，以解决开放世界语义下的规划问题。

Method: 结合显式输入知识（eKABs）和本体感知动作效果，采用一致性更新语义。

Result: 新方法的复杂度未增加，并通过多项式编译实现高效性能。

Conclusion: 该方法在现有和新基准测试中表现良好，验证了其有效性。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [282] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: 开发了一种名为CSI的人工智能框架，通过模拟专家临床医生的认知过程，诊断118种口腔疾病，显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 口腔疾病诊断存在症状重叠的临床挑战，需要超越简单模式匹配的专家推理方法。

Method: 结合多模态CLIP模型和ChatGLM-6B语言模型，采用分层诊断推理树（HDRT）进行快速和标准模式诊断。

Result: 在431张内部测试图像上，快速模式准确率为73.4%，标准模式提升至89.5%。

Conclusion: CSI框架通过分层推理显著提升诊断性能，展示了临床实用价值。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [283] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 论文研究了沙特阿拉伯NEOM的170公里线性智能城市The Line中人类移动的可行性，通过混合模拟框架验证了AI支持的自由移动可能性。


<details>
  <summary>Details</summary>
Motivation: 评估The Line这种前所未有的城市拓扑中公民是否能自由移动，探索智能系统在其中的作用。

Method: 开发了结合基于代理的建模、强化学习、监督学习和图神经网络的混合模拟框架，使用合成数据和真实数据模拟多模态交通行为。

Result: 实验显示，AI集成架构下，平均通勤时间为7.8至8.4分钟，满意度超过89%，可达性指数超过91%。移除智能模块会显著降低性能。

Conclusion: The Line中自由移动在概念和操作上均可行，但需依赖自适应AI系统、可持续基础设施和实时反馈。

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [284] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover是一个基于代理的框架，利用通用大语言模型（LLM）与Lean 4证明环境交互，无需模型微调即可生成形式化证明，在miniF2F测试基准上达到95.9%的成功率。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在形式化证明（如Lean 4）中表现不佳，现有方法需微调模型且成本高。Delta Prover旨在通过代理框架解决这一问题。

Method: Delta Prover结合了反射分解与迭代证明修复的算法框架，以及基于Lean 4的领域特定语言（DSL），实现交互式证明构造。

Result: 在miniF2F测试基准上，Delta Prover以95.9%的成功率超越现有方法，且测试时扩展性更强。

Conclusion: 通用LLM在有效代理结构指导下具备未开发的定理证明能力，为形式化环境中的自动推理提供了高效替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [285] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 论文提出了一种软评估指标和轻量级平衡神经网络，用于解释和提升电弧故障诊断模型的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有AI电弧故障诊断模型虽准确率高，但其可信度存疑，需解释模型输出以增强信任。

Method: 结合可解释人工智能和真实电弧实验，定义电弧故障的正确解释，并提出轻量级平衡神经网络。

Result: 通过多数据集和噪声水平测试，验证了软评估指标的有效性。

Conclusion: 该方法使电弧故障诊断模型更易理解和信任，支持实践者做出可靠决策。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [286] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为DMGC的无监督多模态图聚类框架，通过分解混合图并引入双频融合机制，实现了在多模态图上的高效聚类。


<details>
  <summary>Details</summary>
Motivation: 多模态图在现实世界中有广泛应用，但在无监督学习中的研究不足，特别是如何处理混合的同质性和异质性邻域模式。

Method: 提出DMGC框架，将原始图分解为同质性增强图和异质性感知图，并通过双频融合机制和多模态自监督对齐目标进行优化。

Result: 在多个多模态和多关系图数据集上，DMGC取得了最先进的性能。

Conclusion: DMGC通过解耦和融合多模态图的互补视图，有效解决了混合邻域模式的挑战，具有广泛的适用性。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [287] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 提出了一种名为g-AMIE的多代理系统，通过异步监督框架提升AI在医疗诊断对话中的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决AI在医疗诊断中因法规限制无法直接提供个性化建议的问题，同时提升医生监督效率。

Method: 设计g-AMIE系统，在限定范围内采集病史并避免个性化建议，通过临床驾驶舱界面将评估结果提交给监督医生。

Result: 在虚拟OSCE测试中，g-AMIE在病史采集、病例总结及诊断建议方面优于护士和医生组，且监督效率更高。

Conclusion: 异步监督框架为AI在医疗诊断中的实际应用提供了可行模式，但仍需进一步验证临床实践效果。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [288] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat是一个基于大语言模型的多智能体框架，旨在解决注塑行业知识传递问题，结合文档知识和现场数据，通过检索增强生成和工具调用实现适应性强的任务解决。


<details>
  <summary>Details</summary>
Motivation: 注塑行业面临经验工人退休和多语言障碍导致知识传递困难的问题。

Method: 采用检索增强生成策略和工具调用智能体，结合文档知识和数据驱动的工艺条件生成器。

Result: 评估显示更强大的模型在复杂任务中表现更优，验证了多智能体LLM系统在工业知识流程中的可行性。

Conclusion: IM-Chat为制造业AI辅助决策提供了可扩展和通用的解决方案。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [289] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 论文提出了一种新型的AI系统漏洞类别——认知退化，并提出了Qorvex安全AI框架（QSAF Domain 10）作为防御模型，包含实时监控和缓解措施。


<details>
  <summary>Details</summary>
Motivation: 传统的外部对抗威胁（如提示注入）已不足以应对AI系统的内部故障，如内存不足、规划递归等导致的认知退化问题。

Method: 提出了QSAF Domain 10框架，包含六阶段认知退化生命周期和七项实时控制措施（QSAF-BC-001至BC-007），用于监控和缓解问题。

Result: 框架能够实时检测并缓解认知退化问题，如疲劳、内存不足等，提升AI系统的行为韧性。

Conclusion: 认知退化是AI系统的新漏洞类别，QSAF框架为跨平台防御提供了首个模型，增强了AI系统的认知韧性。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [290] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 论文提出两种新方法（GRPO和OSPO）解决多智能体强化学习在拼车平台中的价值函数估计问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 拼车平台面临动态匹配乘客与车辆的高维度和不确定性挑战，传统MARL方法因依赖准确估计Q/V值而效果不佳。

Method: 提出GRPO（用组平均奖励替代PPO基线）和OSPO（仅用一步奖励训练策略），避免价值函数估计问题。

Result: 在真实曼哈顿拼车数据上，GRPO和OSPO在大多数场景中表现优于传统方法，优化了接单时间和订单量。

Conclusion: GRPO和OSPO通过简化学习框架，有效解决了拼车平台中的MARL问题，具有实际应用潜力。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [291] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD方法通过结合非参数检索和扩散生成模型，动态检索高质量状态并规划，解决了离线强化学习中数据集稀疏和轨迹缝合的挑战。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习因数据集稀疏和轨迹重叠不足导致长时规划困难，现有方法泛化能力有限。

Method: RAD结合非参数检索与扩散生成模型，动态检索高回报状态作为目标，并通过条件扩散模型规划。

Result: 实验表明RAD在多样基准测试中表现优于基线方法。

Conclusion: RAD通过检索引导生成，提升了轨迹缝合的灵活性和泛化能力，验证了其有效性。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [292] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出了一种基于图注意力网络和LSTM的端到端模型，用于预测未来流程行为，包括下一活动和下一事件时间。


<details>
  <summary>Details</summary>
Motivation: 利用对象中心事件日志提升流程预测的准确性和效率。

Method: 结合图注意力网络编码活动及其关系，LSTM处理时间依赖。

Result: 在真实和合成事件日志上表现优于现有方法。

Conclusion: 模型在预测流程行为方面具有竞争力。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [293] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文提出了一种基于Pareto优化的方法，通过干预启发式发现业务过程中活动批处理的最优策略，平衡等待时间、处理成本和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 业务过程中，批处理策略需要在成本和等待时间之间权衡，但现有方法缺乏自动发现最优策略的能力。

Method: 采用干预启发式结合三种元启发式（爬山法、模拟退火和强化学习）生成并评估批处理策略。

Result: 实验表明，基于干预启发式的方法在收敛性、多样性和周期时间增益上优于非启发式基线。

Conclusion: 该方法能有效发现多样化的Pareto最优批处理策略，为业务过程优化提供实用工具。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [294] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1是一种基于强化学习微调的图表领域视觉语言模型，用于复杂图表推理。通过程序化数据合成技术和两阶段训练策略，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 验证R1-Style方法在通用多模态数据（如图表）上的优势，解决图表领域缺乏高质量推理数据的问题。

Method: 提出程序化数据合成技术生成高质量图表推理数据，并采用两阶段训练策略（Chart-COT和Chart-RFT）。

Result: Chart-R1在开源基准和自建数据集（ChartRQA）上表现优异，优于图表领域方法，甚至媲美GPT-4o等大型模型。

Conclusion: Chart-R1通过强化学习微调和高质量数据合成，显著提升了图表推理能力，具有广泛的研究和应用价值。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [295] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET是一个多智能体框架，用于戏剧创作和实时表演，通过自主决策和物理环境交互提升沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的戏剧生成方法缺乏主动性和环境交互能力，且依赖详细用户输入，降低了互动性和沉浸感。

Method: 提出HAMLET框架，生成叙事蓝图并赋予演员自主决策能力，通过动作改变场景道具状态，影响其他演员行为。

Result: 实验评估表明，HAMLET能创造表达丰富且连贯的戏剧体验。

Conclusion: HAMLET通过多智能体自主决策和环境交互，显著提升了戏剧的互动性和沉浸感。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [296] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: 研究探讨大语言模型（LLMs）是否构建内部世界模型，通过滑轮系统问题测试发现模型能利用统计关联，但缺乏对结构连接的深层推理。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否依赖统计关联或构建内部世界模型，借鉴认知科学方法验证其能力。

Method: 使用TikZ渲染的滑轮系统问题，分三阶段测试模型对机械优势（MA）的估计、全局特征识别及功能系统区分能力。

Result: 模型能通过滑轮计数启发式估计MA（Study 1），识别功能系统（Study 2），但对复杂结构连接推理能力有限（Study 3）。

Conclusion: LLMs可能具备初步世界模型能力，但深层推理不足，认知科学方法有助于评估AI系统的世界建模能力。

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [297] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 本文提出了一种利用参数依赖关系提高离线强化学习数据效率的方法，包括参数化算法和两种预处理技术。


<details>
  <summary>Details</summary>
Motivation: 在离线强化学习中，如何利用额外的参数依赖信息提高数据效率是一个关键问题。

Method: 提出了三种方法：(1) 参数化SPI算法，(2) 基于博弈的预处理技术，(3) 基于SMT求解的高级预处理技术。

Result: 实验表明，这些方法显著提高了数据效率，同时保持了可靠性。

Conclusion: 通过利用参数依赖关系，本文方法在数据效率和可靠性上取得了显著提升。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [298] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 论文提出了一种评估多选问题（MCQ）指标的新协议，分析了现有指标与答案波动率的关系，发现最差准确率（worst accuracy）关联性最高。


<details>
  <summary>Details</summary>
Motivation: 多选问题（MCQ）是评估大语言模型（LLM）能力的常用方法，但现有研究未系统评估其指标，且存在答案波动问题。

Method: 提出一种指标评估协议，通过分析指标与答案波动率及原始性能的关系来评估方法。

Result: 现有指标与答案波动率有强关联，最差准确率表现最佳。

Conclusion: 最差准确率是一种有效的评估指标，可用于解决MCQ评估中的波动问题。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [299] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 提出一种基于适配器的StarCraft II AI战术调节方法，通过轻量级适配模块实现战术灵活性，同时保持核心性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理虽强大，但缺乏基于高层战术指令的策略适应能力。

Method: 冻结预训练策略网络（DI-Star），为每个动作头附加轻量级适配模块，战术张量编码战略偏好，通过KL散度约束训练适配器。

Result: 实验表明，该方法成功调节了代理在侵略性、扩张模式和技术偏好等战术维度的行为，同时保持竞争力。

Conclusion: 该方法以最小计算开销实现灵活战术控制，为复杂即时战略游戏提供实用策略定制。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [300] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 探讨代理AI在复杂系统中自主检测和响应异常的潜力，强调其改变传统依赖人类的异常管理方法的能力。


<details>
  <summary>Details</summary>
Motivation: 传统异常管理方法依赖人类，效率低且易出错，代理AI有望解决这一问题。

Method: 研究代理AI在复杂系统中自主检测和响应异常的机制。

Result: 代理AI展示了在异常管理中的高效性和自主性。

Conclusion: 代理AI有潜力革新传统异常管理方法，提升效率和准确性。

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [301] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO框架通过两阶段强化学习，使模型内化推理长度控制，减少40.9%的token使用并提高2.3%的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在长链推理中表现优异，但会生成过多token，LAPO旨在将推理长度控制内化为模型能力。

Method: LAPO采用两阶段强化学习：第一阶段学习成功解长度的统计分布，第二阶段将其作为元认知指导嵌入推理上下文。

Result: 实验显示LAPO减少40.9%的token使用，提高2.3%的准确性，模型能根据问题复杂度分配计算资源。

Conclusion: LAPO实现了高效推理，无需牺牲质量，模型展现出根据问题复杂度分配资源的能力。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [302] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent是一个多代理系统，用于智能合约的Gas优化，结合了现有模式的兼容性和新模式的自动化发现/验证，实现了端到端优化。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案依赖手动发现Gas浪费模式，效率低且难以扩展，而基于LLM的方法存在兼容性和冗余问题。

Method: GasAgent由四个专业代理（Seeker、Innovator、Executor、Manager）组成，通过闭环协作识别、验证和应用Gas节省改进。

Result: 在100个真实合约中优化了82个，平均节省部署Gas 9.97%；在500个LLM生成合约中优化了79.8%，节省Gas 4.79%-13.93%。

Conclusion: GasAgent作为LLM辅助智能合约开发的优化层，具有广泛适用性和有效性。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [303] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体意图的动态可解释涌现分析框架EAMI，用于复杂服务生态系统中的异常涌现分析。


<details>
  <summary>Details</summary>
Motivation: 随着服务计算、云计算和物联网的发展，服务生态系统日益复杂，传统因果方法难以分析智能体间的异常涌现行为。

Method: EAMI框架采用双视角思维跟踪机制（Inspector Agent和Analysis Agent）提取意图，结合k-means聚类和意图时序涌现图进行动态分析。

Result: 实验在复杂O2O服务系统和Stanford AI Town中验证了EAMI的有效性、通用性和效率。

Conclusion: EAMI为服务生态系统中的异常涌现和因果分析提供了新范式。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [304] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文探讨了联邦学习（FL）如何满足可信人工智能（TAI）的要求，分析了FL在TAI框架下面临的挑战及其研究现状。


<details>
  <summary>Details</summary>
Motivation: 随着AI在敏感和高风险领域的应用增加，确保AI技术的可信性（TAI）变得至关重要。联邦学习（FL）因其隐私保护特性被认为是一种有前景的解决方案，但其分布式特性与TAI的其他要求存在冲突，需要系统研究。

Method: 以TAI的要求为框架，系统分析了FL在满足TAI时面临的主要挑战，并对现有研究、趋势和未解决问题进行了分类和探讨。

Result: 研究揭示了FL在TAI框架下的关键障碍，包括隐私保护与TAI其他要求的协调问题，并总结了当前的研究进展和未来方向。

Conclusion: FL在TAI中的应用仍面临诸多挑战，需进一步研究以解决其分布式特性与TAI要求的冲突，推动可信AI的发展。

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [305] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: 论文研究了在MPDAG（最大定向部分有向无环图）设定下识别条件因果效应的方法，提出了三个结果：不受治疗影响的调节集识别公式、MPDAG下的do calculus推广，以及一个完整的条件效应识别算法。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，MPDAG表示受背景知识限制的等价类图，研究如何在此设定下识别条件因果效应具有重要意义。

Method: 提出了三个关键结果：1）不受治疗影响的调节集识别公式；2）MPDAG下的do calculus推广；3）完整的条件效应识别算法。

Result: 提供了在MPDAG设定下识别条件因果效应的理论框架和实用工具。

Conclusion: 论文为MPDAG设定下的条件因果效应识别提供了系统的方法和理论支持。

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [306] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO是一种强化学习框架，通过分层预算探索和差异化奖励机制，使模型能够根据问题复杂度自适应调整推理深度，显著提高计算效率并保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在统一推理策略下计算效率低下的问题，同时避免因效率优化导致的推理能力下降。

Method: 采用分层预算探索，将样本划分为不同预算子组，并引入差异化奖励机制，使模型能够根据问题复杂度分配资源。

Result: 在四个推理基准测试中，HBPO平均减少60.6%的token使用，同时准确率提升3.14%。

Conclusion: 推理效率和能力并非固有冲突，通过分层训练可以同时优化两者。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [307] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLMs）在时间认知上表现出类似人类的韦伯-费希纳定律行为，并通过神经元、表征和信息层面的分析揭示了其机制。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在未经直接训练的情况下表现出的类似人类认知模式，特别是时间认知。

Method: 使用相似性判断任务，分析神经元、表征和信息层面，揭示LLMs的时间认知机制。

Result: 发现LLMs自发建立主观时间参考点，并遵循韦伯-费希纳定律；训练语料具有非线性时间结构。

Conclusion: 提出体验主义视角，认为LLMs的认知是内部表征系统对外部世界的主观构建，为AI对齐提供新方向。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [308] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: 论文探讨了如何通过优化管道设计和提示工程，使大型语言模型（如Gemini 2.5 Pro）能够解决国际数学奥林匹克（IMO）的高难度问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学基准测试（如AIME）中表现良好，但在奥林匹克级别的任务中表现不佳，因此研究如何提升其解决高难度数学问题的能力。

Method: 使用Google的Gemini 2.5 Pro模型，结合管道设计和提示工程，避免数据污染，测试其在IMO 2025问题上的表现。

Result: 在6个问题中，模型正确解决了5个（存在一个例外），表明优化使用方式对模型性能至关重要。

Conclusion: 通过适当的管道设计和提示工程，大型语言模型能够有效解决高难度数学问题，但需进一步优化以应对更复杂的挑战。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>
