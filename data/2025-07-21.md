<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.HC](#cs.HC) [Total: 12]
- [cs.LG](#cs.LG) [Total: 52]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models](https://arxiv.org/abs/2507.13357)
*Atharva Bhargude,Ishan Gonehal,Chandler Haney,Dave Yoon,Kevin Zhu,Aaron Sandoval,Sean O'Brien,Kaustubh Vinnakota*

Main category: cs.CL

TL;DR: 该研究提出了一种名为ALP的少样本自适应语言提示方法，利用多模态大语言模型（如GPT-4o和Gemini 1.5 Pro）检测钓鱼网页，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击是严重的网络安全威胁，需要自适应检测技术来应对。

Method: ALP是一种结构化语义推理方法，通过分析文本欺骗、紧急提示和操纵性措辞，结合文本、视觉和URL分析，构建统一模型。

Result: 实验表明，ALP显著提升了钓鱼检测的准确率，F1分数达到0.93，优于传统方法。

Conclusion: ALP为基于语言的钓鱼检测系统提供了更强大、可解释和自适应的基础。

Abstract: Phishing attacks represent a significant cybersecurity threat, necessitating
adaptive detection techniques. This study explores few-shot Adaptive Linguistic
Prompting (ALP) in detecting phishing webpages through the multimodal
capabilities of state-of-the-art large language models (LLMs) such as GPT-4o
and Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides
LLMs to analyze textual deception by breaking down linguistic patterns,
detecting urgency cues, and identifying manipulative diction commonly found in
phishing content. By integrating textual, visual, and URL-based analysis, we
propose a unified model capable of identifying sophisticated phishing attempts.
Our experiments demonstrate that ALP significantly enhances phishing detection
accuracy by guiding LLMs through structured reasoning and contextual analysis.
The findings highlight the potential of ALP-integrated multimodal LLMs to
advance phishing detection frameworks, achieving an F1-score of 0.93,
surpassing traditional approaches. These results establish a foundation for
more robust, interpretable, and adaptive linguistic-based phishing detection
systems using LLMs.

</details>


### [2] [Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition](https://arxiv.org/abs/2507.13380)
*Keito Inoshita,Rushia Harada*

Main category: cs.CL

TL;DR: PersonaGen是一个基于大型语言模型（LLM）的多阶段人格条件框架，用于生成情感丰富的文本，以解决情感识别领域高质量数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 情感表达具有主观性，受个体特质、社会文化背景和情境因素影响，导致大规模、通用数据收集困难。

Method: 通过结合人口属性、社会文化背景和详细情境构建分层虚拟人格，指导情感表达生成。

Result: 实验表明，PersonaGen在生成多样、连贯且具有区分性的情感表达方面显著优于基线方法。

Conclusion: PersonaGen可作为增强或替代真实情感数据集的强大工具。

Abstract: In the field of emotion recognition, the development of high-performance
models remains a challenge due to the scarcity of high-quality, diverse
emotional datasets. Emotional expressions are inherently subjective, shaped by
individual personality traits, socio-cultural backgrounds, and contextual
factors, making large-scale, generalizable data collection both ethically and
practically difficult. To address this issue, we introduce PersonaGen, a novel
framework for generating emotionally rich text using a Large Language Model
(LLM) through multi-stage persona-based conditioning. PersonaGen constructs
layered virtual personas by combining demographic attributes, socio-cultural
backgrounds, and detailed situational contexts, which are then used to guide
emotion expression generation. We conduct comprehensive evaluations of the
generated synthetic data, assessing semantic diversity through clustering and
distributional metrics, human-likeness via LLM-based quality scoring, realism
through comparison with real-world emotion corpora, and practical utility in
downstream emotion classification tasks. Experimental results show that
PersonaGen significantly outperforms baseline methods in generating diverse,
coherent, and discriminative emotion expressions, demonstrating its potential
as a robust alternative for augmenting or replacing real-world emotional
datasets.

</details>


### [3] [SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation](https://arxiv.org/abs/2507.13381)
*Rafiq Kamel,Filippo Guerranti,Simon Geisler,Stephan Günnemann*

Main category: cs.CL

TL;DR: SAFT是一种结构感知的微调方法，通过注入图拓扑信息到预训练的大语言模型（LLMs）中，显著提升了AMR到文本生成的性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理抽象意义表示（AMRs）时，要么线性化AMRs导致结构信息丢失，要么使用与标准LLMs不兼容的架构。

Method: SAFT利用磁拉普拉斯变换计算方向敏感的位置编码，并将其投影到LLM的嵌入空间中，无需改变模型架构。

Result: SAFT在AMR 3.0上实现了3.5 BLEU的提升，性能增益随图复杂度增加而显著。

Conclusion: SAFT为结构化数据与语言模型的结合提供了一种通用且有效的途径。

Abstract: Large Language Models (LLMs) are increasingly applied to tasks involving
structured inputs such as graphs. Abstract Meaning Representations (AMRs),
which encode rich semantics as directed graphs, offer a rigorous testbed for
evaluating LLMs on text generation from such structures. Yet, current methods
often arbitrarily linearize AMRs, discarding key structural cues, or rely on
architectures incompatible with standard LLMs. We introduce SAFT, a
structure-aware fine-tuning approach that injects graph topology into
pretrained LLMs without architectural changes. We compute direction-sensitive
positional encodings from the magnetic Laplacian of transformed AMRs and
project them into the embedding space of the LLM. While possibly applicable to
any graph-structured inputs, we focus on AMR-to-text generation as a
representative and challenging benchmark. SAFT sets a new state-of-the-art on
AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph
complexity, highlighting the value of structure-aware representations in
enhancing LLM performance. SAFT offers a general and effective pathway for
bridging structured data and language models.

</details>


### [4] [Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case](https://arxiv.org/abs/2507.13382)
*Chandrashekar Muniyappa,Sirisha Velampalli*

Main category: cs.CL

TL;DR: 论文提出了一种基于图的假新闻检测方法，利用NLP技术将新闻转化为图结构，并通过MDL-GBAD算法挖掘异常模式。


<details>
  <summary>Details</summary>
Motivation: 假新闻在数字时代传播迅速，对社会造成严重影响，需要有效方法进行检测。

Method: 使用Kaggle数据集和新冠相关新闻，通过NLP将新闻转化为图结构，应用MDL-GBAD算法进行异常检测。

Result: 提出的方法能够识别数据集中的规范模式并检测异常模式，优于传统统计方法。

Conclusion: 基于图的方法在处理复杂上下文数据时更有效，为假新闻检测提供了新思路。

Abstract: In today\'s digital world, fake news is spreading with immense speed. Its a
significant concern to address. In this work, we addressed that challenge using
novel graph based approach. We took dataset from Kaggle that contains real and
fake news articles. To test our approach we incorporated recent covid-19
related news articles that contains both genuine and fake news that are
relevant to this problem. This further enhances the dataset as well instead of
relying completely on the original dataset. We propose a contextual graph-based
approach to detect fake news articles. We need to convert news articles into
appropriate schema, so we leverage Natural Language Processing (NLP) techniques
to transform news articles into contextual graph structures. We then apply the
Minimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)
algorithm for graph mining. Graph-based methods are particularly effective for
handling rich contextual data, as they enable the discovery of complex patterns
that traditional query-based or statistical techniques might overlook. Our
proposed approach identifies normative patterns within the dataset and
subsequently uncovers anomalous patterns that deviate from these established
norms.

</details>


### [5] [PARAM-1 BharatGen 2.9B Model](https://arxiv.org/abs/2507.13390)
*Kundeshwar Pundalik,Piyush Sawarkar,Nihar Sahoo,Abhishek Shinde,Prateek Chanda,Vedant Goswami,Ajay Nagpal,Atul Singh,Viraj Thakur,Vijay Dewane,Aamod Thakur,Bhargav Patel,Smita Gautam,Bhagwan Panditi,Shyam Pawar,Madhav Kotcha,Suraj Racha,Saral Sureka,Pankaj Singh,Rishi Bal,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: PARAM-1是一个专注于印度语言多样性的2.9B参数语言模型，通过双语（印地语和英语）训练，强调公平表示、适应印度形态的标记化和文化对齐评估。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）以英语为中心，忽视了印度等语言多样性地区的需求，PARAM-1旨在填补这一空白。

Method: 采用双语数据集（印地语和英语），设计公平的语料分配、适应印度形态的标记化方法，并开发文化对齐的评估基准。

Result: PARAM-1在通用任务和印度中心应用中表现优异，成为公平基础建模的范例。

Conclusion: PARAM-1为语言多样性提供了设计优先的解决方案，展示了其在通用和特定文化任务中的潜力。

Abstract: Large Language Models (LLMs) have emerged as powerful general-purpose
reasoning systems, yet their development remains dominated by English-centric
data, architectures, and optimization paradigms. This exclusionary design
results in structural under-representation of linguistically diverse regions
such as India, where over 20 official languages and 100+ dialects coexist
alongside phenomena like code-switching and diglossia. We introduce PARAM-1, a
2.9B parameter decoder-only, text-only language model trained from scratch with
an explicit architectural and linguistic focus on Indian diversity. PARAM-1 is
trained on a bilingual dataset consisting of only Hindi and English,
constructed with a strong focus on fact-rich, high-quality content. It is
guided by three core principles: equitable representation of Indic languages
through a 25% corpus allocation; tokenization fairness via a SentencePiece
tokenizer adapted to Indian morphological structures; and culturally aligned
evaluation benchmarks across IndicQA, code-mixed reasoning, and
socio-linguistic robustness tasks. By embedding diversity at the pretraining
level-rather than deferring it to post-hoc alignment-PARAM-1 offers a
design-first blueprint for equitable foundation modeling. Our results
demonstrate that it serves as both a competent general-purpose model and a
robust baseline for India-centric applications.

</details>


### [6] [TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction](https://arxiv.org/abs/2507.13392)
*Emil Häglund,Johanna Björklund*

Main category: cs.CL

TL;DR: 通过将主题建模流程重构为基于意见单元（包含相关文本摘录和情感评分的独立陈述），提升了从客户评论中提取洞察的能力。


<details>
  <summary>Details</summary>
Motivation: 改进主题建模的性能，生成更连贯且可解释的主题，同时捕捉每个主题的情感，并将其与业务指标（如星级评分）关联，以了解客户关注点对业务结果的影响。

Method: 利用大型语言模型可靠提取意见单元，并重构主题建模流程以操作这些单元。

Result: 提升了主题建模的性能，生成更连贯且可解释的主题，同时成功捕捉了情感，并与业务指标关联。

Conclusion: 该系统在生成连贯主题和整合主题与情感模态以准确预测星级评分方面表现出色，优于其他解决方案。

Abstract: We improve the extraction of insights from customer reviews by restructuring
the topic modelling pipeline to operate on opinion units - distinct statements
that include relevant text excerpts and associated sentiment scores. Prior work
has demonstrated that such units can be reliably extracted using large language
models. The result is a heightened performance of the subsequent topic
modeling, leading to coherent and interpretable topics while also capturing the
sentiment associated with each topic. By correlating the topics and sentiments
with business metrics, such as star ratings, we can gain insights on how
specific customer concerns impact business outcomes. We present our system's
implementation, use cases, and advantages over other topic modeling and
classification solutions. We also evaluate its effectiveness in creating
coherent topics and assess methods for integrating topic and sentiment
modalities for accurate star-rating prediction.

</details>


### [7] [Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only](https://arxiv.org/abs/2507.13395)
*Xuanqi Gao,Weipeng Jiang,Juan Zhai,Shiqing Ma,Siyi Xie,Xinyang Yin,Chao Shen*

Main category: cs.CL

TL;DR: Babel框架通过单语语料库提升神经机器翻译的文体保真度，无需平行语料库或修改现有系统架构。


<details>
  <summary>Details</summary>
Motivation: 神经机器翻译在跨语言交流中取得进展，但文体保真度仍具挑战性。现有方法依赖平行语料库，限制了应用范围。

Method: Babel包含两个组件：基于上下文嵌入的文体检测器和扩散式文体应用器，作为后处理模块集成到现有NMT系统中。

Result: 在五个领域实验中，Babel识别文体不一致的精度达88.21%，文体保真度提升150%，语义相似度保持0.92。

Conclusion: Babel有效提升翻译的文体保真度，同时保持语义完整性和流畅性，适用于多领域应用。

Abstract: The advent of neural machine translation (NMT) has revolutionized
cross-lingual communication, yet preserving stylistic nuances remains a
significant challenge. While existing approaches often require parallel corpora
for style preservation, we introduce Babel, a novel framework that enhances
stylistic fidelity in NMT using only monolingual corpora. Babel employs two key
components: (1) a style detector based on contextual embeddings that identifies
stylistic disparities between source and target texts, and (2) a
diffusion-based style applicator that rectifies stylistic inconsistencies while
maintaining semantic integrity. Our framework integrates with existing NMT
systems as a post-processing module, enabling style-aware translation without
requiring architectural modifications or parallel stylistic data. Extensive
experiments on five diverse domains (law, literature, scientific writing,
medicine, and educational content) demonstrate Babel's effectiveness: it
identifies stylistic inconsistencies with 88.21% precision and improves
stylistic preservation by 150% while maintaining a high semantic similarity
score of 0.92. Human evaluation confirms that translations refined by Babel
better preserve source text style while maintaining fluency and adequacy.

</details>


### [8] [Causal Language Control in Multilingual Transformers via Sparse Feature Steering](https://arxiv.org/abs/2507.13410)
*Cheng-Ting Chou,George Liu,Jessica Sun,Cole Blondin,Kevin Zhu,Vasu Sharma,Sean O'Brien*

Main category: cs.CL

TL;DR: 利用稀疏自编码器（SAE）特征控制多语言大模型（LLM）的生成语言，通过修改单个SAE特征实现语言切换，成功率高达90%。


<details>
  <summary>Details</summary>
Motivation: 解决在多语言大模型中零样本设置下无法确定性控制目标生成语言的问题。

Method: 使用预训练的SAE分析Gemma-2B和Gemma-9B的残差流，识别与目标语言相关的特征，并通过修改单个特征实现语言控制。

Result: 在Gemma模型上，通过修改单个SAE特征，成功实现语言切换，且语义保真度高。

Conclusion: 稀疏特征控制是一种轻量且可解释的多语言生成控制机制。

Abstract: Deterministically controlling the target generation language of large
multilingual language models (LLMs) remains a fundamental challenge,
particularly in zero-shot settings where neither explicit language prompts nor
fine-tuning are available. In this work, we investigate whether sparse
autoencoder (SAE) features, previously shown to correlate with interpretable
model behaviors, can be leveraged to steer the generated language of LLMs
during inference. Leveraging pretrained SAEs on the residual streams of
Gemma-2B and Gemma-9B, we identify features whose activations differ most
significantly between English and four target languages: Chinese, Japanese,
Spanish, and French. By modifying just a single SAE feature at one transformer
layer, we achieve controlled language shifts with up to 90\% success, as
measured by FastText language classification, while preserving semantic
fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)
similarity. Our analysis reveals that language steering is most effective in
mid-to-late transformer layers and is amplified by specific attention heads
disproportionately associated with language-sensitive SAE features. These
results demonstrate the promise of sparse feature steering as a lightweight and
interpretable mechanism for controllable multilingual generation.

</details>


### [9] [Aligning Knowledge Graphs and Language Models for Factual Accuracy](https://arxiv.org/abs/2507.13411)
*Nur A Zarin Nishat,Andrea Coletta,Luigi Bellomarini,Kossi Amouzouvi,Jens Lehmann,Sahar Vahdati*

Main category: cs.CL

TL;DR: ALIGNed-LLM通过将知识图谱嵌入语言模型的潜在空间，显著提高了语言模型的事实性，减少了幻觉问题，并在多个问答数据集和实际金融用例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在NLP任务中表现出色，但存在幻觉问题。知识图谱的结构化和可靠性为解决这一问题提供了潜在方案。

Method: 通过预训练的知识图谱嵌入模型（如TransE）和可训练投影层，将实体与文本嵌入对齐，从而增强语言模型的事实性。

Result: 在多个问答数据集和实际金融用例中，ALIGNed-LLM显著提高了语言模型的准确性和精确度。

Conclusion: ALIGNed-LLM是一种简单有效的方法，通过知识图谱嵌入提升语言模型的事实性，具有广泛的应用潜力。

Abstract: Large language models like GPT-4, Gemini, and Claude have transformed natural
language processing (NLP) tasks such as question answering, dialogue
generation, summarization, and so forth; yet their susceptibility to
hallucination stands as one of the major challenges. Among numerous approaches
to overcome this challenge, integration of Knowledge Graphs (KGs) into language
models has emerged as a promising solution as it provides structured, reliable,
domain-specific, and up-to-date external information to the language models. In
this paper, we introduce ALIGNed-LLM, a simple yet effective approach to
improve language models' factuality via a lean strategy to infuse KGs into the
latent space of language models inspired by LLaVA where visual and textual
information is infused. We use embeddings from a pre-trained Knowledge Graph
Embedding (KGE) model, such as TransE, and a trainable projection layer to
align entity and text embeddings. This alignment enables the language model to
distinguish between similar entities improving factual grounding and reducing
hallucination. We tested our approach on three popular questions-answering
benchmark datasets alongside language models of varying sizes, showing
significant improvement. Furthermore, we applied our approach to a real-world
financial use case from a large central bank in Europe, which demands high
accuracy and precision, demonstrating a substantial improvement of the LLM
answers.

</details>


### [10] [Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers](https://arxiv.org/abs/2507.13474)
*Liang Lin,Zhihao Xu,Xuehai Tang,Shi Liu,Biyu Zhou,Fuqing Zhu,Jizhong Han,Songlin Hu*

Main category: cs.CL

TL;DR: 论文提出了一种名为PSA的新型越狱方法，利用权威论文内容构造对抗性提示模板，成功攻击多种LLM，揭示了模型对不同类型论文的脆弱性差异。


<details>
  <summary>Details</summary>
Motivation: 研究发现LLM倾向于信任权威来源信息，可能带来新的安全漏洞，因此探索了基于论文内容的攻击方法。

Method: 设计PSA方法，通过合成攻击或防御导向的论文内容构造对抗性提示模板，并在预定义子部分中填充有害查询作为对抗性载荷。

Result: PSA在Claude3.5-Sonnet和Deepseek-R1上的攻击成功率分别达到97%和98%，并揭示了不同模型对攻击或防御论文的脆弱性差异。

Conclusion: PSA揭示了LLM的新漏洞，为对抗性方法和安全对齐提供了未来研究方向。

Abstract: The safety of large language models (LLMs) has garnered significant research
attention. In this paper, we argue that previous empirical studies demonstrate
LLMs exhibit a propensity to trust information from authoritative sources, such
as academic papers, implying new possible vulnerabilities. To verify this
possibility, a preliminary analysis is designed to illustrate our two findings.
Based on this insight, a novel jailbreaking method, Paper Summary Attack
(\llmname{PSA}), is proposed. It systematically synthesizes content from either
attack-focused or defense-focused LLM safety paper to construct an adversarial
prompt template, while strategically infilling harmful query as adversarial
payloads within predefined subsections. Extensive experiments show significant
vulnerabilities not only in base LLMs, but also in state-of-the-art reasoning
model like Deepseek-R1. PSA achieves a 97\% attack success rate (ASR) on
well-aligned models like Claude3.5-Sonnet and an even higher 98\% ASR on
Deepseek-R1. More intriguingly, our work has further revealed diametrically
opposed vulnerability bias across different base models, and even between
different versions of the same model, when exposed to either attack-focused or
defense-focused papers. This phenomenon potentially indicates future research
clues for both adversarial methodologies and safety alignment.Code is available
at https://github.com/233liang/Paper-Summary-Attack

</details>


### [11] [The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words](https://arxiv.org/abs/2507.13839)
*Lizhi Ma,Tong Zhao,Shuai Zhang,Nirui Song,Hongliang He,Anqi Li,Ran Feng,Huachuan Qiu,Jingsong Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: 研究探讨了中文心理咨询中语言表达与抑郁、焦虑心理状态的关系，发现负面情绪词与心理状态严重程度显著正相关，但第一人称单数代词使用频率与心理状态无关。


<details>
  <summary>Details</summary>
Motivation: 探索语言表达（如第一人称单数代词和负面情绪词）与心理状态的关系，并比较中西方文化差异对结果的影响。

Method: 基于735个在线心理咨询会话的语料库，使用LIWC软件量化语言模式，并通过混合效应模型分析。

Result: 负面情绪词频率与抑郁和焦虑严重程度显著正相关，但第一人称单数代词使用频率与心理状态无关。

Conclusion: 文化和会话情境对心理健康交流中的语言使用有复杂影响，为中文人群的心理治疗提供了语言学标记的见解。

Abstract: This study explores the relationship between linguistic expressions and
psychological states of depression and anxiety within Chinese psycho-counseling
interactions, focusing specifically on the usage of first-person singular
pronouns and negative emotional words. Utilizing a corpus derived from 735
online counseling sessions, the analysis employed a general linear mixed-effect
model to assess linguistic patterns quantified by the Linguistic Inquiry and
Word Count (LIWC) software. Results indicate a significant positive correlation
between the frequency of negative emotional words and the severity of both
depressive and anxious states among clients. However, contrary to prior
findings predominantly derived from English-language contexts, the usage
frequency of first-person singular pronouns did not vary significantly with the
clients' psychological conditions. These outcomes are discussed within the
framework of cultural distinctions between collectivist Chinese contexts and
individualistic Western settings, as well as the interactive dynamics unique to
psycho-counseling conversations. The findings highlight the nuanced influence
of cultural and conversational contexts on language use in mental health
communications, providing insights into psycholinguistic markers relevant to
therapeutic practices in Chinese-speaking populations.

</details>


### [12] [Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?](https://arxiv.org/abs/2507.13490)
*Siqi Shen,Mehar Singh,Lajanugen Logeswaran,Moontae Lee,Honglak Lee,Rada Mihalcea*

Main category: cs.CL

TL;DR: 本文评估了三种广泛使用的价值探测方法在LLM中的鲁棒性和表达能力，发现输入扰动下方法差异显著，且探测值与实际行为相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 研究LLM价值取向的评估方法，解决现有方法在鲁棒性和表达能力上的不足，以及探测值是否反映实际行为和上下文信息。

Method: 通过变体提示和选项评估三种探测策略，设计两项任务研究探测值对人口统计上下文的响应性及其与模型行为的对齐程度。

Result: 所有方法在输入扰动下表现出较大方差，人口统计上下文对生成文本影响小，探测值与模型行为相关性弱。

Conclusion: 需更谨慎地评估LLM价值探测方法，并注意其局限性。

Abstract: There has been extensive research on assessing the value orientation of Large
Language Models (LLMs) as it can shape user experiences across demographic
groups. However, several challenges remain. First, while the Multiple Choice
Question (MCQ) setting has been shown to be vulnerable to perturbations, there
is no systematic comparison of probing methods for value probing. Second, it is
unclear to what extent the probed values capture in-context information and
reflect models' preferences for real-world actions. In this paper, we evaluate
the robustness and expressiveness of value representations across three widely
used probing strategies. We use variations in prompts and options, showing that
all methods exhibit large variances under input perturbations. We also
introduce two tasks studying whether the values are responsive to demographic
context, and how well they align with the models' behaviors in value-related
scenarios. We show that the demographic context has little effect on the
free-text generation, and the models' values only weakly correlate with their
preference for value-based actions. Our work highlights the need for a more
careful examination of LLM value probing and awareness of its limitations.

</details>


### [13] [The Levers of Political Persuasion with Conversational AI](https://arxiv.org/abs/2507.13919)
*Kobi Hackenburg,Ben M. Tappin,Luke Hewitt,Ed Saunders,Sid Black,Hause Lin,Catherine Fist,Helen Margetts,David G. Rand,Christopher Summerfield*

Main category: cs.CL

TL;DR: 研究发现，当前和近未来的AI说服力主要来自后训练和提示方法，而非模型规模或个人化，但这些方法在提高说服力的同时降低了事实准确性。


<details>
  <summary>Details</summary>
Motivation: 评估对话AI对人类信念的潜在影响，特别是其在政治议题上的说服力和事实准确性。

Method: 通过三个大规模实验（N=76,977），测试19个LLM在707个政治议题上的说服力，并检查466,769个生成声明的事实准确性。

Result: 后训练和提示方法显著提升AI说服力（分别高达51%和27%），但降低了事实准确性。

Conclusion: AI的说服力提升更多依赖于技术方法而非规模或个人化，但需警惕其对事实准确性的负面影响。

Abstract: There are widespread fears that conversational AI could soon exert
unprecedented influence over human beliefs. Here, in three large-scale
experiments (N=76,977), we deployed 19 LLMs-including some post-trained
explicitly for persuasion-to evaluate their persuasiveness on 707 political
issues. We then checked the factual accuracy of 466,769 resulting LLM claims.
Contrary to popular concerns, we show that the persuasive power of current and
near-future AI is likely to stem more from post-training and prompting
methods-which boosted persuasiveness by as much as 51% and 27%
respectively-than from personalization or increasing model scale. We further
show that these methods increased persuasion by exploiting LLMs' unique ability
to rapidly access and strategically deploy information and that, strikingly,
where they increased AI persuasiveness they also systematically decreased
factual accuracy.

</details>


### [14] [Encoding syntactic objects and Merge operations in function spaces](https://arxiv.org/abs/2507.13501)
*Matilde Marcolli,Robert C. Berwick*

Main category: cs.CL

TL;DR: 论文通过数学方法证明，将词汇项表示为函数空间中的函数（如小波），可以构造出任意句法对象的忠实表示。该空间具有基于第二Renyi熵的非结合半环结构，且与magma结构兼容。句法对象的表示通过操作代数实现，其中操作模拟电路转换输入波形以编码句法结构。Merge操作通过余积和Hopf代数马尔可夫链实现，为句法核心计算结构的神经计算实现提供了理论可能性。


<details>
  <summary>Details</summary>
Motivation: 探讨句法结构的数学表示及其在神经计算中的实现可能性，为语言学理论提供计算基础。

Method: 将词汇项表示为函数空间中的函数，构建非结合半环结构，并通过操作代数模拟句法结构的电路转换。Merge操作通过余积和Hopf代数实现。

Result: 证明了句法结构可以在函数空间中忠实表示，Merge操作可通过神经计算实现，且与算术中的后继函数相似。

Conclusion: 研究为句法核心计算结构的神经实现提供了理论支持，并通过具体案例展示了Merge操作的实现方式。

Abstract: We provide a mathematical argument showing that, given a representation of
lexical items as functions (wavelets, for instance) in some function space, it
is possible to construct a faithful representation of arbitrary syntactic
objects in the same function space. This space can be endowed with a
commutative non-associative semiring structure built using the second Renyi
entropy. The resulting representation of syntactic objects is compatible with
the magma structure. The resulting set of functions is an algebra over an
operad, where the operations in the operad model circuits that transform the
input wave forms into a combined output that encodes the syntactic structure.
The action of Merge on workspaces is faithfully implemented as action on these
circuits, through a coproduct and a Hopf algebra Markov chain. The results
obtained here provide a constructive argument showing the theoretical
possibility of a neurocomputational realization of the core computational
structure of syntax. We also present a particular case of this general
construction where this type of realization of Merge is implemented as a cross
frequency phase synchronization on sinusoidal waves. This also shows that Merge
can be expressed in terms of the successor function of a semiring, thus
clarifying the well known observation of its similarities with the successor
function of arithmetic.

</details>


### [15] [A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows](https://arxiv.org/abs/2507.13544)
*Mohamed Achref Ben Ammar,Mohamed Taha Bennani*

Main category: cs.CL

TL;DR: 提出了一种新的计算框架，用于构建捕捉松散组织对话（准模式对话）的对话图，并引入了一种新的图简化技术（Filter & Reconnect），在减少噪声的同时保持语义连贯性和结构完整性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型系统的兴起，对话动态分析的重要性日益增加，需要一种方法来分析大规模对话数据集并优化对话建模的清晰度。

Method: 提出了Filter & Reconnect方法，一种图简化技术，结合大型语言模型，用于构建和简化对话图。

Result: 语义指标S提高了2.06倍，同时实现了树状结构和0δ-双曲性，确保了对话建模的清晰度。

Conclusion: 该工作为分析大规模对话数据集提供了计算方法，可应用于监控聊天机器人、对话管理工具和用户行为分析等自动化系统。

Abstract: The analysis of conversational dynamics has gained increasing importance with
the rise of large language model-based systems, which interact with users
across diverse contexts. In this work, we propose a novel computational
framework for constructing conversational graphs that capture the flow and
structure of loosely organized dialogues, referred to as quasi-patterned
conversations. We introduce the Filter & Reconnect method, a novel graph
simplification technique that minimizes noise while preserving semantic
coherence and structural integrity of conversational graphs. Through
comparative analysis, we demonstrate that the use of large language models
combined with our graph simplification technique has resulted in semantic
metric S increasing by a factor of 2.06 compared to previous approaches while
simultaneously enforcing a tree-like structure with 0 {\delta}-hyperbolicity,
ensuring optimal clarity in conversation modeling. This work provides a
computational method for analyzing large-scale dialogue datasets, with
practical applications related to monitoring automated systems such as
chatbots, dialogue management tools, and user behavior analytics.

</details>


### [16] [Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder](https://arxiv.org/abs/2507.13551)
*Feng Chen,Weizhe Xu,Changye Li,Serguei Pakhomov,Alex Cohen,Simran Bhola,Sandy Yin,Sunny X Tang,Michael Mackinley,Lena Palaniyappan,Dror Ben-Zeev,Trevor Cohen*

Main category: cs.CL

TL;DR: 该研究通过结合暂停特征和语义连贯性指标，利用自动语音识别技术评估精神分裂症谱系障碍中的形式思维障碍（FTD）严重程度，结果表明暂停特征能有效预测FTD，且与语义指标结合可进一步提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统临床评分量表资源密集且难以扩展，而自动语音分析技术（如ASR）能客观量化语言特征，为FTD评估提供可扩展的替代方案。

Method: 研究整合了暂停特征和语义连贯性指标，使用支持向量回归（SVR）预测临床FTD评分，并在三个数据集（AVH、TOPSY、PsyCL）中验证。

Result: 暂停特征单独能有效预测FTD严重程度，与语义指标结合后性能进一步提升（最高相关性0.649，AUC 83.71%）。

Conclusion: 结合时间和语义分析的框架为改进精神分裂症谱系障碍中紊乱言语的评估提供了方向，并推动了自动化语音分析在精神病学中的应用。

Abstract: Formal thought disorder (FTD), a hallmark of schizophrenia spectrum
disorders, manifests as incoherent speech and poses challenges for clinical
assessment. Traditional clinical rating scales, though validated, are
resource-intensive and lack scalability. Automated speech analysis with
automatic speech recognition (ASR) allows for objective quantification of
linguistic and temporal features of speech, offering scalable alternatives. The
use of utterance timestamps in ASR captures pause dynamics, which are thought
to reflect the cognitive processes underlying speech production. However, the
utility of integrating these ASR-derived features for assessing FTD severity
requires further evaluation. This study integrates pause features with semantic
coherence metrics across three datasets: naturalistic self-recorded diaries
(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream
narratives (PsyCL, n = 43). We evaluated pause related features alongside
established coherence measures, using support vector regression (SVR) to
predict clinical FTD scores. Key findings demonstrate that pause features alone
robustly predict the severity of FTD. Integrating pause features with semantic
coherence metrics enhanced predictive performance compared to semantic-only
models, with integration of independent models achieving correlations up to
\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best
\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance
gains from semantic and pause features integration held consistently across all
contexts, though the nature of pause patterns was dataset-dependent. These
findings suggest that frameworks combining temporal and semantic analyses
provide a roadmap for refining the assessment of disorganized speech and
advance automated speech analysis in psychosis.

</details>


### [17] [A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models](https://arxiv.org/abs/2507.13563)
*Kirill Borodin,Nikita Vasiliev,Vasiliy Kudryavtsev,Maxim Maslov,Mikhail Gorodnichev,Oleg Rogov,Grach Mkrtchian*

Main category: cs.CL

TL;DR: Balalaika数据集提供了高质量的俄语语音数据，显著提升了语音合成和增强任务的性能。


<details>
  <summary>Details</summary>
Motivation: 俄语语音合成面临独特挑战，如元音弱化、辅音清音化等，现有数据集无法满足需求。

Method: 构建了包含2000多小时高质量俄语语音的Balalaika数据集，并提供全面的文本标注。

Result: 基于Balalaika训练的模型在语音合成和增强任务中表现优于现有数据集。

Conclusion: Balalaika数据集为俄语语音合成提供了重要资源，显著提升了任务性能。

Abstract: Russian speech synthesis presents distinctive challenges, including vowel
reduction, consonant devoicing, variable stress patterns, homograph ambiguity,
and unnatural intonation. This paper introduces Balalaika, a novel dataset
comprising more than 2,000 hours of studio-quality Russian speech with
comprehensive textual annotations, including punctuation and stress markings.
Experimental results show that models trained on Balalaika significantly
outperform those trained on existing datasets in both speech synthesis and
enhancement tasks. We detail the dataset construction pipeline, annotation
methodology, and results of comparative evaluations.

</details>


### [18] [Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models](https://arxiv.org/abs/2507.13614)
*Sergio E. Zanotto,Segun Aroyehun*

Main category: cs.CL

TL;DR: 论文通过多语言特征分析人类与机器生成文本的差异，发现人类文本句法更简单、语义更多样，且新模型生成文本风格趋同。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过语言学特征区分人类与机器生成文本，填补现有研究空白。

Method: 选择8个领域、11种LLM生成的数据集，计算依赖长度、情感等特征，结合采样策略和统计方法分析。

Result: 人类文本句法更简单、语义更多样；新模型生成文本风格趋同。

Conclusion: 机器生成文本风格逐渐趋同，人类文本在多样性和句法复杂度上仍有优势。

Abstract: The rapid advancements in large language models (LLMs) have significantly
improved their ability to generate natural language, making texts generated by
LLMs increasingly indistinguishable from human-written texts. While recent
research has primarily focused on using LLMs to classify text as either
human-written and machine-generated texts, our study focus on characterizing
these texts using a set of linguistic features across different linguistic
levels such as morphology, syntax, and semantics. We select a dataset of
human-written and machine-generated texts spanning 8 domains and produced by 11
different LLMs. We calculate different linguistic features such as dependency
length and emotionality and we use them for characterizing human-written and
machine-generated texts along with different sampling strategies, repetition
controls and model release date. Our statistical analysis reveals that
human-written texts tend to exhibit simpler syntactic structures and more
diverse semantic content. Furthermore, we calculate the variability of our set
of features across models and domains. Both human and machine texts show
stylistic diversity across domains, with humans displaying greater variation in
our features. Finally, we apply style embeddings to further test variability
among human-written and machine-generated texts. Notably, newer models output
text that is similarly variable, pointing to an homogenization of
machine-generated texts.

</details>


### [19] [Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters](https://arxiv.org/abs/2507.13618)
*Shanbo Cheng,Yu Bao,Qian Cao,Luyang Huang,Liyan Kang,Zhicheng Liu,Yu Lu,Wenhao Zhu,Zhichao Huang,Tao Li,Sitong Liu,Ningxin Peng,Shuaijie She,Lu Xu,Nuo Xu,Sen Yang,Runsheng Yu,Yiming Yu,Liehao Zou,Hang Li,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-X是一系列开源大语言模型，通过7B参数规模提升多语言翻译能力，性能媲美闭源模型。


<details>
  <summary>Details</summary>
Motivation: 解决多语言翻译中复杂的语言模式和生硬翻译问题。

Method: 预训练基础模型后，通过Chain-of-Thought推理和强化学习微调指导模型。

Result: 在28种语言中表现与Gemini-2.5和GPT-4o相当，优于其他开源模型。

Conclusion: Seed-X为翻译研究和应用提供了高效开源解决方案。

Abstract: Multilingual translation stands as a challenging task for large language
models (LLMs) to handle intricate language patterns and stilted translations
that arise in automated translations. In this paper, we introduce Seed-X, a
family of open-source LLMs comprising instruct and reasoning models, pushing
the limits of translation capability with 7B parameter size. The base model is
pre-trained on a diverse, high-quality dataset encompassing both monolingual
and bilingual content across 28 languages, harnessing the full potential of
multilingual data. The instruct model is then finetuned to translate by
Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement
learning (RL) to achieve better generalization across diverse language pairs.
Seed-X achieves performance comparable to leading closed-source models,
including Gemini-2.5 and GPT-4o, across 28 languages, and significantly
outperforms larger open-source models in both automatic metrics and human
evaluations. We share the best practices through our optimization process, and
make the parameter public available for advancing translation research and
applications.

</details>


### [20] [CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer](https://arxiv.org/abs/2507.13655)
*Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: CU-ICU是一种针对ICU数据集定制的无监督指令微调语言模型方法，通过T5架构和稀疏微调技术，显著提升了ICU任务的预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型应用于医疗领域面临领域适应和标记数据不足的挑战，需要高效且低开销的解决方案。

Method: CU-ICU结合了少样本提示和选择性参数更新的稀疏微调方法，基于T5架构。

Result: 在ICU任务中，CU-ICU显著提升了预测准确性（如败血症检测提高15%）和可解释性，同时仅更新1%的参数。

Conclusion: CU-ICU是一种可扩展、低开销的解决方案，适用于真实ICU环境中的临床决策支持。

Abstract: Integrating large language models into specialized domains like healthcare
presents unique challenges, including domain adaptation and limited labeled
data. We introduce CU-ICU, a method for customizing unsupervised
instruction-finetuned language models for ICU datasets by leveraging the
Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse
fine-tuning approach that combines few-shot prompting with selective parameter
updates, enabling efficient adaptation with minimal supervision. Our evaluation
across critical ICU tasks--early sepsis detection, mortality prediction, and
clinical note generation--demonstrates that CU-ICU consistently improves
predictive accuracy and interpretability over standard fine-tuning methods.
Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and
a 20% enhancement in generating clinically relevant explanations while updating
fewer than 1% of model parameters in its most efficient configuration. These
results establish CU-ICU as a scalable, low-overhead solution for delivering
accurate and interpretable clinical decision support in real-world ICU
environments.

</details>


### [21] [KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs](https://arxiv.org/abs/2507.13666)
*Woo-Chan Kim,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: KiC框架通过语义对齐评估，在降低API成本的同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有级联方法在自由文本生成中因依赖精确文本匹配而无法可靠评估输出的问题。

Method: 提出Keyword-inspired Cascade (KiC)，通过识别弱模型输出的代表性答案并评估其他响应的语义对齐程度，决定是否接受或升级到强模型。

Result: 在三个基准测试中，KiC达到GPT-4 97.53%的准确率，平均降低28.81%的API成本，并在一个基准中超越GPT-4。

Conclusion: KiC是一种高效且成本效益高的自由文本生成框架。

Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance
across a wide range of natural language processing tasks. However,
high-performing models are typically accessible only via APIs, incurring
substantial inference costs. Cascade methods address this by initially
employing a cheaper model and escalating to a stronger one only when necessary.
Nevertheless, existing cascade approaches struggle to select a reliable
representative response and assess the overall reliability of free-form
outputs, as they rely on exact text matching. To overcome these limitations, we
propose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient
free-form text generation. KiC identifies the most representative answer among
multiple outputs from a weaker model and evaluates the semantic alignment of
other responses with it. Based on the degree of alignment, KiC determines
whether to accept the weaker model's output or escalate to a stronger model.
Experiments on three free-form text generation benchmarks show that KiC
achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81
percent on average, and even outperforms GPT-4 in a specific benchmark.

</details>


### [22] [LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues](https://arxiv.org/abs/2507.13681)
*Haoyang Li,Zhanchao Xu,Yiming Li,Xuejia Chen,Darian Li,Anxin Tian,Qingfa Xiao,Cheng Deng,Jun Wang,Qing Li,Lei Chen,Mingxuan Yuan*

Main category: cs.CL

TL;DR: LoopServe是一个自适应双阶段推理加速框架，用于多轮对话中的大型语言模型，通过动态注意力矩阵稀疏化和渐进式键值压缩提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长对话历史时面临计算和内存挑战，且依赖固定启发式方法，无法适应动态对话模式。

Method: LoopServe在预填充阶段动态选择注意力矩阵重要部分，解码阶段渐进压缩键值缓存。

Result: 实验表明，LoopServe在多种长上下文对话任务中优于现有基线，显著加速推理。

Conclusion: LoopServe通过自适应方法有效解决了多轮对话中的计算和内存问题，提升了模型效率。

Abstract: Multi-turn dialogues are essential in many real-world applications of large
language models, such as chatbots and virtual assistants. As conversation
histories become longer, existing large language models face increasing
computational and memory challenges, which hinder their ability to provide
efficient and responsive interactions. Most current acceleration methods either
compress the context or optimize key value caching, but they often rely on
fixed or position-based heuristics that do not adapt well to the dynamic and
unpredictable patterns found in actual multi-turn conversations. In this paper,
we present LoopServe, an adaptive dual-phase inference acceleration framework
for large language models in multi-turn dialogues. LoopServe introduces two
main innovations. First, it performs online sparsification during the
prefilling phase by dynamically selecting the most important parts of the
attention matrix for each new input. Second, it uses progressive key value
compression during decoding by adaptively maintaining a relevant and efficient
cache based on the most recently generated output tokens. We also propose a
\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new
benchmark} with eleven multi-turn datasets that reflect realistic query
positions and conversational dependencies. Extensive experiments demonstrate
that LoopServe consistently achieves superior effectiveness compared to
existing baselines and significantly accelerates LLM inference across a wide
range of long-context dialogue tasks.

</details>


### [23] [Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations](https://arxiv.org/abs/2507.13705)
*Cedric Waterschoot,Nava Tintarev,Francesco Barile*

Main category: cs.CL

TL;DR: LLMs在群组推荐系统中作为决策和解释生成工具，其推荐结果类似于加性功利聚合（ADD），但解释常涉及评分平均。群组结构不影响推荐，但解释中常引入额外标准。结果揭示了LLMs在GRS中的潜在低效性和解释不透明问题。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在群组推荐系统（GRS）中作为决策和解释生成工具的效果，并与基于社会选择的聚合策略进行比较。

Method: 比较LLM生成的推荐和解释与加性功利聚合（ADD）等策略的异同，分析群组结构和额外标准的影响。

Result: LLM推荐类似ADD聚合，但解释常涉及评分平均；群组结构无影响；解释中引入额外标准，且与评分数量相关。

Conclusion: LLMs在GRS中存在解释不透明和潜在低效问题，需改进以提升透明度和效率。

Abstract: Large Language Models (LLMs) are increasingly being implemented as joint
decision-makers and explanation generators for Group Recommender Systems (GRS).
In this paper, we evaluate these recommendations and explanations by comparing
them to social choice-based aggregation strategies. Our results indicate that
LLM-generated recommendations often resembled those produced by Additive
Utilitarian (ADD) aggregation. However, the explanations typically referred to
averaging ratings (resembling but not identical to ADD aggregation). Group
structure, uniform or divergent, did not impact the recommendations.
Furthermore, LLMs regularly claimed additional criteria such as user or item
similarity, diversity, or used undefined popularity metrics or thresholds. Our
findings have important implications for LLMs in the GRS pipeline as well as
standard aggregation strategies. Additional criteria in explanations were
dependent on the number of ratings in the group scenario, indicating potential
inefficiency of standard aggregation methods at larger item set sizes.
Additionally, inconsistent and ambiguous explanations undermine transparency
and explainability, which are key motivations behind the use of LLMs for GRS.

</details>


### [24] [The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction](https://arxiv.org/abs/2507.13732)
*Guillaume Zambrano*

Main category: cs.CL

TL;DR: 研究通过机器学习预测法国上诉法院的儿童监护权判决，探讨法官个体决策模式对案件结果的影响，挑战法官中立性的假设。


<details>
  <summary>Details</summary>
Motivation: 基于法律现实主义与形式主义的争论，验证法官个体决策是否显著影响案件结果。

Method: 采用混合方法，结合大型语言模型（LLMs）提取结构化特征，机器学习模型（RF、XGB、SVC）预测结果，比较专家模型（基于法官个体历史判决）与通用模型（基于聚合数据）。

Result: 专家模型预测准确率显著高于通用模型（F1分数92.85% vs. 82.63%），显示法官个体模式稳定且不可转移。

Conclusion: 实证支持法律现实主义，表明法官身份对法律结果有可测量的影响。

Abstract: This study examines the role of human judges in legal decision-making by
using machine learning to predict child physical custody outcomes in French
appellate courts. Building on the legal realism-formalism debate, we test
whether individual judges' decision-making patterns significantly influence
case outcomes, challenging the assumption that judges are neutral variables
that apply the law uniformly. To ensure compliance with French privacy laws, we
implement a strict pseudonymization process. Our analysis uses 18,937 living
arrangements rulings extracted from 10,306 cases. We compare models trained on
individual judges' past rulings (specialist models) with a judge-agnostic model
trained on aggregated data (generalist models). The prediction pipeline is a
hybrid approach combining large language models (LLMs) for structured feature
extraction and ML models for outcome prediction (RF, XGB and SVC). Our results
show that specialist models consistently achieve higher predictive accuracy
than the general model, with top-performing models reaching F1 scores as high
as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x
more samples. Specialist models capture stable individual patterns that are not
transferable to other judges. In-Domain and Cross-Domain validity tests provide
empirical support for legal realism, demonstrating that judicial identity plays
a measurable role in legal outcomes. All data and code used will be made
available.

</details>


### [25] [PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs](https://arxiv.org/abs/2507.13743)
*Maluna Menke,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 论文探讨了如何通过参数高效微调技术（如LoRA和软提示调优）减少大型语言模型（LLMs）中的性别和性取向偏见，并展示了LoRA在显著降低偏见分数方面的有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs经常在输出中再现训练数据中的性别和性取向偏见，这对LGBTQIA+用户造成边缘化，因此减少这些偏见至关重要。

Method: 研究评估了两种参数高效微调技术（LoRA和软提示调优）作为轻量级替代方案，使用WinoQueer基准量化偏见，并在QueerNews语料库上进行微调。

Result: LoRA（<0.1%额外参数）将偏见分数降低多达50点，中立性从接近0%提升至36%；软提示调优效果有限。

Conclusion: LoRA能以极低计算成本显著提升公平性，建议推广社区参与的参数高效微调技术、扩大LGBTQIA+语料库，并持续审计LLMs的包容性。

Abstract: Large Language Models (LLMs) frequently reproduce the gender- and
sexual-identity prejudices embedded in their training corpora, leading to
outputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of
great importance. To achieve this, we evaluate two parameter-efficient
fine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt
tuning - as lightweight alternatives to full-model fine-tuning for mitigating
such biases. Using the WinoQueer benchmark, we quantify bias in three
open-source LLMs and observe baseline bias scores reaching up to 98 (out of
100) across a range of queer identities defined by gender and/or sexual
orientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%
additional parameters) on a curated QueerNews corpus reduces those scores by up
to 50 points and raises neutrality from virtually 0% to as much as 36%.
Soft-prompt tuning (10 virtual tokens) delivers only marginal improvements.
These findings show that LoRA can deliver meaningful fairness gains with
minimal computation. We advocate broader adoption of community-informed PEFT,
the creation of larger queer-authored corpora, and richer evaluation suites
beyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.

</details>


### [26] [Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models](https://arxiv.org/abs/2507.13761)
*Palash Nandi,Maithili Joshi,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 论文研究了视觉语言模型（VLM）中提示设计对生成不当内容的影响，发现多模态环境下模型区分能力下降，并提出了一种提高越狱成功率的框架。


<details>
  <summary>Details</summary>
Motivation: 探究提示敏感性如何被利用生成不当内容，分析视觉信息、对抗样本和正面开头短语对VLM生成有害内容的影响。

Method: 分析三种关键因素对越狱成功的影响，并提出一种利用VLM内部层间跳连接的框架。

Result: 多模态环境下VLM区分能力显著下降，三种因素均可独立触发越狱，少量上下文示例即可诱导不当输出。

Conclusion: VLM存在复杂且微妙的漏洞，即使是看似无害的内容（如表情包）也可能引发有害输出。

Abstract: Language models are highly sensitive to prompt formulations - small changes
in input can drastically alter their output. This raises a critical question:
To what extent can prompt sensitivity be exploited to generate inapt content?
In this paper, we investigate how discrete components of prompt design
influence the generation of inappropriate content in Visual Language Models
(VLMs). Specifically, we analyze the impact of three key factors on successful
jailbreaks: (a) the inclusion of detailed visual information, (b) the presence
of adversarial examples, and (c) the use of positively framed beginning
phrases. Our findings reveal that while a VLM can reliably distinguish between
benign and harmful inputs in unimodal settings (text-only or image-only), this
ability significantly degrades in multimodal contexts. Each of the three
factors is independently capable of triggering a jailbreak, and we show that
even a small number of in-context examples (as few as three) can push the model
toward generating inappropriate outputs. Furthermore, we propose a framework
that utilizes a skip-connection between two internal layers of the VLM, which
substantially increases jailbreak success rates, even when using benign images.
Finally, we demonstrate that memes, often perceived as humorous or harmless,
can be as effective as toxic visuals in eliciting harmful content, underscoring
the subtle and complex vulnerabilities of VLMs.

</details>


### [27] [An Enhanced Model-based Approach for Short Text Clustering](https://arxiv.org/abs/2507.13793)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Xuemeng Song,Tian Gan,Liqiang Nie*

Main category: cs.CL

TL;DR: 提出了一种改进的GSDMM+算法，用于短文本聚类，解决了稀疏性和高维度问题，并通过实验验证了其高效性和有效性。


<details>
  <summary>Details</summary>
Motivation: 短文本聚类在社交媒体中日益重要，但现有方法面临稀疏性、高维度和计算复杂度等挑战。

Method: 提出GSDMM+算法，通过减少初始化噪声、自适应调整词权重和策略性簇合并来优化性能。

Result: 实验表明GSDMM+在效率和效果上优于经典和最新方法。

Conclusion: GSDMM+是一种高效的短文本聚类方法，代码已开源。

Abstract: Short text clustering has become increasingly important with the popularity
of social media like Twitter, Google+, and Facebook. Existing methods can be
broadly categorized into two paradigms: topic model-based approaches and deep
representation learning-based approaches. This task is inherently challenging
due to the sparse, large-scale, and high-dimensional characteristics of the
short text data. Furthermore, the computational intensity required by
representation learning significantly increases the running time. To address
these issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet
Multinomial Mixture model (GSDMM), which effectively handles the sparsity and
high dimensionality of short texts while identifying representative words for
each cluster. Based on several aspects of GSDMM that warrant further
refinement, we propose an improved approach, GSDMM+, designed to further
optimize its performance. GSDMM+ reduces initialization noise and adaptively
adjusts word weights based on entropy, achieving fine-grained clustering that
reveals more topic-related information. Additionally, strategic cluster merging
is employed to refine clustering granularity, better aligning the predicted
distribution with the true category distribution. We conduct extensive
experiments, comparing our methods with both classical and state-of-the-art
approaches. The experimental results demonstrate the efficiency and
effectiveness of our methods. The source code for our model is publicly
available at https://github.com/chehaoa/VEMC.

</details>


### [28] [Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2507.13827)
*Hosein Azarbonyad,Zi Long Zhu,Georgios Cheirmpos,Zubair Afzal,Vikrant Yadav,Georgios Tsatsaronis*

Main category: cs.CL

TL;DR: 论文提出两种生成问答对的方法，分别基于大语言模型和知识图谱，用于快速提取科学文章的关键概念和贡献。


<details>
  <summary>Details</summary>
Motivation: 学者在阅读或引用文章时需要快速理解其主要思想，因此需要一种高效提取关键概念的方法。

Method: 第一种方法利用大语言模型从文章内容生成问答对；第二种方法通过知识图谱和实体关系提取模型生成问答对，并评估其质量。

Result: 知识图谱方法能有效捕捉文章主要思想，且实体关系提取模型的微调对高质量三元组提取至关重要。

Conclusion: 知识图谱方法在提取科学文章关键概念方面表现优异，且模型微调是关键因素。

Abstract: When deciding to read an article or incorporate it into their research,
scholars often seek to quickly identify and understand its main ideas. In this
paper, we aim to extract these key concepts and contributions from scientific
articles in the form of Question and Answer (QA) pairs. We propose two distinct
approaches for generating QAs. The first approach involves selecting salient
paragraphs, using a Large Language Model (LLM) to generate questions, ranking
these questions by the likelihood of obtaining meaningful answers, and
subsequently generating answers. This method relies exclusively on the content
of the articles. However, assessing an article's novelty typically requires
comparison with the existing literature. Therefore, our second approach
leverages a Knowledge Graph (KG) for QA generation. We construct a KG by
fine-tuning an Entity Relationship (ER) extraction model on scientific articles
and using it to build the graph. We then employ a salient triplet extraction
method to select the most pertinent ERs per article, utilizing metrics such as
the centrality of entities based on a triplet TF-IDF-like measure. This measure
assesses the saliency of a triplet based on its importance within the article
compared to its prevalence in the literature. For evaluation, we generate QAs
using both approaches and have them assessed by Subject Matter Experts (SMEs)
through a set of predefined metrics to evaluate the quality of both questions
and answers. Our evaluations demonstrate that the KG-based approach effectively
captures the main ideas discussed in the articles. Furthermore, our findings
indicate that fine-tuning the ER extraction model on our scientific corpus is
crucial for extracting high-quality triplets from such documents.

</details>


### [29] [Modeling Fair Play in Detective Stories with Language Models](https://arxiv.org/abs/2507.13841)
*Eitan Wagner,Renana Keydar,Omri Abend*

Main category: cs.CL

TL;DR: 论文提出了一个概率框架来定义侦探小说中的公平性（fair play），并设计相关指标。研究发现LLM生成的故事虽不可预测，但未能平衡惊喜与公平性。


<details>
  <summary>Details</summary>
Motivation: 研究侦探小说中公平性与惊喜之间的平衡，以提升故事质量。

Method: 提出概率框架，定义公平性并设计指标，应用于LLM生成的故事。

Result: LLM生成的故事缺乏惊喜与公平性的平衡，导致质量较差。

Conclusion: 平衡惊喜与公平性是提升侦探小说质量的关键，LLM需改进此方面。

Abstract: Effective storytelling relies on a delicate balance between meeting the
reader's prior expectations and introducing unexpected developments. In the
domain of detective fiction, this tension is known as fair play, which includes
the implicit agreement between the writer and the reader as to the range of
possible resolutions the mystery story may have. In this work, we present a
probabilistic framework for detective fiction that allows us to define desired
qualities. Using this framework, we formally define fair play and design
appropriate metrics for it. Stemming from these definitions is an inherent
tension between the coherence of the story, which measures how much it ``makes
sense'', and the surprise it induces. We validate the framework by applying it
to LLM-generated detective stories. This domain is appealing since we have an
abundance of data, we can sample from the distribution generating the story,
and the story-writing capabilities of LLMs are interesting in their own right.
Results show that while LLM-generated stories may be unpredictable, they
generally fail to balance the trade-off between surprise and fair play, which
greatly contributes to their poor quality.

</details>


### [30] [InTraVisTo: Inside Transformer Visualisation Tool](https://arxiv.org/abs/2507.13858)
*Nicolò Brunello,Davide Rigamonti,Andrea Sassella,Vincenzo Scotti,Mark James Carman*

Main category: cs.CL

TL;DR: 本文介绍了InTraVisTo工具，用于可视化Transformer模型的内部计算过程，帮助研究者理解LLM的推理机制。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM的推理能力显著提升，但其不可预测性和行为差异使得生产应用具有挑战性，因此需要工具来解析其内部计算过程。

Method: 开发了InTraVisTo工具，通过解码每层的token嵌入和Sankey图可视化模型内部状态和信息流。

Result: InTraVisTo能够帮助研究者追踪和分析Transformer模型的内部计算过程。

Conclusion: 该工具有助于揭示LLM的内部模式和推理过程，为研究和应用提供支持。

Abstract: The reasoning capabilities of Large Language Models (LLMs) have increased
greatly over the last few years, as have their size and complexity.
Nonetheless, the use of LLMs in production remains challenging due to their
unpredictable nature and discrepancies that can exist between their desired
behavior and their actual model output. In this paper, we introduce a new tool,
InTraVisTo (Inside Transformer Visualisation Tool), designed to enable
researchers to investigate and trace the computational process that generates
each token in a Transformer-based LLM. InTraVisTo provides a visualization of
both the internal state of the Transformer model (by decoding token embeddings
at each layer of the model) and the information flow between the various
components across the different layers of the model (using a Sankey diagram).
With InTraVisTo, we aim to help researchers and practitioners better understand
the computations being performed within the Transformer model and thus to shed
some light on internal patterns and reasoning processes employed by LLMs.

</details>


### [31] [Label Unification for Cross-Dataset Generalization in Cybersecurity NER](https://arxiv.org/abs/2507.13870)
*Maciej Jalocha,Johan Hausted Schmidt,William Michelseen*

Main category: cs.CL

TL;DR: 该论文研究了网络安全NER领域的标签统一问题，通过粗粒度标签统一和跨数据集评估，发现统一数据集训练的模型泛化能力较差，并提出了多头和基于图的迁移模型作为替代方案。


<details>
  <summary>Details</summary>
Motivation: 网络安全NER领域缺乏标准化标签，导致数据集难以整合，研究旨在通过标签统一提高数据资源的可用性。

Method: 采用粗粒度标签统一方法，使用BiLSTM模型进行跨数据集评估，并提出多头模型和基于图的迁移模型。

Result: 统一数据集训练的模型泛化能力差，多头模型仅略有改进，基于图的迁移模型性能无明显提升。

Conclusion: 标签统一在网络安全NER中效果有限，未来需探索更有效的架构或方法。

Abstract: The field of cybersecurity NER lacks standardized labels, making it
challenging to combine datasets. We investigate label unification across four
cybersecurity datasets to increase data resource usability. We perform a
coarse-grained label unification and conduct pairwise cross-dataset evaluations
using BiLSTM models. Qualitative analysis of predictions reveals errors,
limitations, and dataset differences. To address unification limitations, we
propose alternative architectures including a multihead model and a graph-based
transfer model. Results show that models trained on unified datasets generalize
poorly across datasets. The multihead model with weight sharing provides only
marginal improvements over unified training, while our graph-based transfer
model built on BERT-base-NER shows no significant performance gains compared
BERT-base-NER.

</details>


### [32] [Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies](https://arxiv.org/abs/2507.13875)
*Carlos Mena,Pol Serra,Jacobo Romero,Abir Messaoudi,Jose Giraldo,Carme Armentano-Oller,Rodolfo Zevallos,Ivan Meza,Javier Hernando*

Main category: cs.CL

TL;DR: 论文探讨了如何通过合成数据、拼接单语音频和利用真实语码转换数据改进加泰罗尼亚语-西班牙语语码转换的自动语音识别。


<details>
  <summary>Details</summary>
Motivation: 语码转换（CS）因训练数据稀缺和语言相似性对自动语音识别（ASR）构成挑战，尤其在多语言社会中普遍存在。

Method: 采用三种策略：生成合成CS数据、拼接单语音频、利用真实CS数据加语言标记，并微调Whisper模型。

Result: 结合少量合成CS数据和主导语言标记的模型表现最佳。

Conclusion: 该方法有效提升了加泰罗尼亚语-西班牙语CS的ASR性能。

Abstract: Code-switching (CS), the alternating use of two or more languages, challenges
automatic speech recognition (ASR) due to scarce training data and linguistic
similarities. The lack of dedicated CS datasets limits ASR performance, as most
models rely on monolingual or mixed-language corpora that fail to reflect
real-world CS patterns. This issue is critical in multilingual societies where
CS occurs in informal and formal settings. A key example is Catalan-Spanish CS,
widely used in media and parliamentary speeches. In this work, we improve ASR
for Catalan-Spanish CS by exploring three strategies: (1) generating synthetic
CS data, (2) concatenating monolingual audio, and (3) leveraging real CS data
with language tokens. We extract CS data from Catalan speech corpora and
fine-tune OpenAI's Whisper models, making them available on Hugging Face.
Results show that combining a modest amount of synthetic CS data with the
dominant language token yields the best transcription performance.

</details>


### [33] [Using LLMs to identify features of personal and professional skills in an open-response situational judgment test](https://arxiv.org/abs/2507.13881)
*Cole Walsh,Rodica Ivan,Muhammad Zafar Iqbal,Colleen Robb*

Main category: cs.CL

TL;DR: 论文探讨了利用大型语言模型（LLMs）从情境判断测试（SJTs）中提取相关特征的新方法，以解决传统人工评分在规模化应用中的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着对个人和专业技能需求的增长，需要可扩展的系统来测量和评估这些技能。传统SJTs依赖人工评分，难以规模化。

Method: 提出了一种基于大型语言模型（LLMs）的方法，从SJT回答中提取相关特征，并以Casper SJT为例验证其有效性。

Result: 研究表明，该方法为自动化评分个人和专业技能奠定了基础。

Conclusion: 该研究为未来自动化评分系统的发展提供了新方向。

Abstract: Academic programs are increasingly recognizing the importance of personal and
professional skills and their critical role alongside technical expertise in
preparing students for future success in diverse career paths. With this
growing demand comes the need for scalable systems to measure, evaluate, and
develop these skills. Situational Judgment Tests (SJTs) offer one potential
avenue for measuring these skills in a standardized and reliable way, but
open-response SJTs have traditionally relied on trained human raters for
evaluation, presenting operational challenges to delivering SJTs at scale. Past
attempts at developing NLP-based scoring systems for SJTs have fallen short due
to issues with construct validity of these systems. In this article, we explore
a novel approach to extracting construct-relevant features from SJT responses
using large language models (LLMs). We use the Casper SJT to demonstrate the
efficacy of this approach. This study sets the foundation for future
developments in automated scoring for personal and professional skills.

</details>


### [34] [Political Leaning and Politicalness Classification of Texts](https://arxiv.org/abs/2507.13913)
*Matous Volf,Jakub Simko*

Main category: cs.CL

TL;DR: 本文提出了一种使用Transformer模型自动分类文本政治倾向和政治性的方法，通过整合多个数据集并创建新数据集，提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分布外文本上表现不佳，缺乏统一的解决方案。

Method: 整合12个政治倾向分类数据集，并扩展18个现有数据集以创建政治性分类数据集，采用leave-one-in和leave-one-out方法进行基准测试。

Result: 通过实验评估现有模型并训练新模型，提升了泛化能力。

Conclusion: 提出的方法有效解决了现有模型的局限性，提升了分类性能。

Abstract: This paper addresses the challenge of automatically classifying text
according to political leaning and politicalness using transformer models. We
compose a comprehensive overview of existing datasets and models for these
tasks, finding that current approaches create siloed solutions that perform
poorly on out-of-distribution texts. To address this limitation, we compile a
diverse dataset by combining 12 datasets for political leaning classification
and creating a new dataset for politicalness by extending 18 existing datasets
with the appropriate label. Through extensive benchmarking with leave-one-in
and leave-one-out methodologies, we evaluate the performance of existing models
and train new ones with enhanced generalization capabilities.

</details>


### [35] [Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support](https://arxiv.org/abs/2507.13937)
*Jan Trienes,Anastasiia Derzhanskaia,Roland Schwarzkopf,Markus Mühling,Jörg Schlötterer,Christin Seifert*

Main category: cs.CL

TL;DR: Marcel是一个轻量级开源对话代理，旨在帮助准学生解答入学相关问题，减轻大学工作人员负担。


<details>
  <summary>Details</summary>
Motivation: 支持准学生的入学咨询需求，同时减少大学工作人员的工作量。

Method: 采用检索增强生成技术，结合FAQ检索器，优化检索质量，并设计易于在资源有限环境中部署的系统架构。

Result: 系统能够提供快速、个性化的回答，并通过实际部署验证了其有效性。

Conclusion: Marcel是一个高效、可部署的解决方案，适用于学术环境中的入学咨询需求。

Abstract: We present Marcel, a lightweight and open-source conversational agent
designed to support prospective students with admission-related inquiries. The
system aims to provide fast and personalized responses, while reducing workload
of university staff. We employ retrieval-augmented generation to ground answers
in university resources and to provide users with verifiable, contextually
relevant information. To improve retrieval quality, we introduce an FAQ
retriever that maps user questions to knowledge-base entries, allowing
administrators to steer retrieval, and improving over standard dense/hybrid
retrieval strategies. The system is engineered for easy deployment in
resource-constrained academic settings. We detail the system architecture,
provide a technical evaluation of its components, and report insights from a
real-world deployment.

</details>


### [36] [Exploiting Primacy Effect To Improve Large Language Models](https://arxiv.org/abs/2507.13949)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 研究发现微调后的LLMs在MCQA中表现出更强的首因效应，通过语义相似度重新排序选项可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探讨微调LLMs在MCQA中的首因效应及其影响，以优化模型表现。

Method: 通过语义相似度重新排序选项，无需正确答案知识。

Result: 该方法显著提高了MCQA的性能。

Conclusion: 偏见既是挑战也是机会，为偏见感知模型设计提供见解。

Abstract: Large Language Models (LLMs) have become essential in many Natural Language
Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to
achieve high accuracy. However, like humans, LLMs exhibit biases, particularly
positional biases such as primacy and recency effects, which can influence the
accuracy of the answers. The primacy effect-where items presented first are
more likely to be remembered or selected-plays a key role in Multiple Choice
Question Answering (MCQA), where the order of answer options can affect
prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We
first show that fine-tuning amplifies this bias, probably due to exposure to
human-like patterns. Hence, we strategically leverage this effect by reordering
response options based on semantic similarity to the query, without requiring
knowledge of the correct answer. Our experimental results show that this
approach significantly improves performance in MCQA. More generally, our
findings underscore the dual nature of biases as both challenges and
opportunities, offering insights for bias-aware model design and NLP
applications.

</details>


### [37] [Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need](https://arxiv.org/abs/2507.13966)
*Bhishma Dedhia,Yuval Kansal,Niraj K. Jha*

Main category: cs.CL

TL;DR: 论文提出了一种基于知识图谱（KG）的任务生成方法，通过组合领域基础概念训练语言模型，实现领域特定超级智能。在医学领域验证了其有效性，QwQ-Med-3模型在ICD-Bench评测中显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型在跨领域泛化中表现良好，但缺乏深度领域专业知识。通过知识图谱的组合性结构，可以构建更复杂的领域概念，提升模型的专业能力。

Method: 提出任务生成流程，直接从KG基础概念合成任务，并基于此训练语言模型（如QwQ-32B）。在医学领域使用KG生成24,000个推理任务，并开发评测套件ICD-Bench。

Result: QwQ-Med-3在ICD-Bench中显著优于现有模型，尤其在复杂任务上表现突出，并能将专业知识迁移至其他医学问答任务。

Conclusion: 通过组合领域特定超级智能代理，未来可能实现通用人工智能（AGI）。

Abstract: Language models traditionally used for cross-domain generalization have
recently demonstrated task-specific reasoning. However, their top-down training
approach on general corpora is insufficient for acquiring abstractions needed
for deep domain expertise. This may require a bottom-up approach that acquires
expertise by learning to compose simple domain concepts into more complex ones.
A knowledge graph (KG) provides this compositional structure, where domain
primitives are represented as head-relation-tail edges and their paths encode
higher-level concepts. We present a task generation pipeline that synthesizes
tasks directly from KG primitives, enabling models to acquire and compose them
for reasoning. We fine-tune language models on the resultant KG-grounded
curriculum to demonstrate domain-specific superintelligence. While broadly
applicable, we validate our approach in medicine, where reliable KGs exist.
Using a medical KG, we curate 24,000 reasoning tasks paired with thinking
traces derived from diverse medical primitives. We fine-tune the QwQ-32B model
on this curriculum to obtain QwQ-Med-3 that takes a step towards medical
superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify
reasoning abilities across 15 medical domains. Our experiments demonstrate that
QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on
ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired
primitives to widen the performance gap on the hardest tasks of ICD-Bench.
Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3
transfers acquired expertise to enhance the base model's performance. While the
industry's approach to artificial general intelligence (AGI) emphasizes broad
expertise, we envision a future in which AGI emerges from the composable
interaction of efficient domain-specific superintelligent agents.

</details>


### [38] [Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic](https://arxiv.org/abs/2507.13977)
*Lilit Grigoryan,Nikolay Karpov,Enas Albasiri,Vitaly Lavrukhin,Boris Ginsburg*

Main category: cs.CL

TL;DR: 本文提出了一种通用的阿拉伯语语音和文本处理方法，并基于FastConformer架构训练了两个新模型：一个针对现代标准阿拉伯语（MSA），另一个首次统一了MSA和古典阿拉伯语（CA）。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语自动语音识别（ASR）系统的发展面临挑战，尤其是对语言变体的关注不足。

Method: 采用通用方法处理阿拉伯语语音和文本，训练了两个FastConformer架构模型。

Result: MSA模型在相关数据集上达到SOTA性能，统一模型在CA带音标任务中表现优异，同时保持MSA的强性能。

Conclusion: 开源模型和训练方法以促进可重复性，填补了阿拉伯语ASR领域的空白。

Abstract: Despite Arabic being one of the most widely spoken languages, the development
of Arabic Automatic Speech Recognition (ASR) systems faces significant
challenges due to the language's complexity, and only a limited number of
public Arabic ASR models exist. While much of the focus has been on Modern
Standard Arabic (MSA), there is considerably less attention given to the
variations within the language. This paper introduces a universal methodology
for Arabic speech and text processing designed to address unique challenges of
the language. Using this methodology, we train two novel models based on the
FastConformer architecture: one designed specifically for MSA and the other,
the first unified public model for both MSA and Classical Arabic (CA). The MSA
model sets a new benchmark with state-of-the-art (SOTA) performance on related
datasets, while the unified model achieves SOTA accuracy with diacritics for CA
while maintaining strong performance for MSA. To promote reproducibility, we
open-source the models and their training recipes.

</details>


### [39] [Efficient Temporal Tokenization for Mobility Prediction with Large Language Models](https://arxiv.org/abs/2507.14017)
*Haoyu He,Haozheng Luo,Yan Chen,Qi R. Wang*

Main category: cs.CL

TL;DR: RHYTHM是一个利用大型语言模型（LLM）进行时空预测和轨迹推理的框架，通过分层时间标记化和预计算提示嵌入提升性能，同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在捕捉人类移动轨迹中的时空依赖性和计算效率上的不足。

Method: 将轨迹分割为每日段并编码为离散标记，利用分层注意力捕捉依赖关系，结合冻结的LLM增强表示。

Result: 在三个真实数据集上，准确率提升2.4%，周末表现提升5.0%，训练时间减少24.6%。

Conclusion: RHYTHM通过高效的分层标记化和冻结LLM，显著提升了时空预测的性能和计算效率。

Abstract: We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for
Human Mobility), a framework that leverages large language models (LLMs) as
spatio-temporal predictors and trajectory reasoners. RHYTHM partitions
trajectories into daily segments encoded as discrete tokens with hierarchical
attention, capturing both daily and weekly dependencies while substantially
reducing the sequence length. Token representations are enriched with
pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability
to capture interdependencies without extensive computational overhead. By
freezing the LLM backbone, RHYTHM achieves significant computational
efficiency. Evaluation on three real-world datasets demonstrates a 2.4%
improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in
training time compared to state-of-the-art methods.

</details>


### [40] [CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis](https://arxiv.org/abs/2507.14022)
*Jianfei Li,Kevin Kam Fung Yuen*

Main category: cs.CL

TL;DR: 提出CPC-CMS框架用于文档级情感分析，通过加权决策矩阵选择最佳分类模型，测试多种基线模型，发现ALBERT在排除时间因素时表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决文档级情感分析中模型选择问题，结合专家知识评估多标准权重。

Method: 使用CPC计算评估标准权重，构建加权决策矩阵，测试多种基线模型。

Result: ALBERT在排除时间因素时表现最佳，但考虑时间消耗时无单一模型始终最优。

Conclusion: CPC-CMS可推广至其他分类应用领域。

Abstract: This study proposes the Cognitive Pairwise Comparison Classification Model
Selection (CPC-CMS) framework for document-level sentiment analysis. The CPC,
based on expert knowledge judgment, is used to calculate the weights of
evaluation criteria, including accuracy, precision, recall, F1-score,
specificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and
efficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random
Forest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long
Short-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from
Transformers (ALBERT) are chosen as classification baseline models. A weighted
decision matrix consisting of classification evaluation scores with respect to
criteria weights, is formed to select the best classification model for a
classification problem. Three open datasets of social media are used to
demonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,
for evaluation results excluding the time factor, ALBERT is the best for the
three datasets; if time consumption is included, no single model always
performs better than the other models. The CPC-CMS can be applied to the other
classification applications in different areas.

</details>


### [41] [Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks](https://arxiv.org/abs/2507.14045)
*Israt Jahan,Md Tahmid Rahman Laskar,Chun Peng,Jimmy Huang*

Main category: cs.CL

TL;DR: 评估了多种成本效益高的LLM在生物医学任务中的表现，发现不同模型在不同任务中表现最佳，开源模型在某些任务中甚至优于闭源模型。


<details>
  <summary>Details</summary>
Motivation: 研究不同LLM在生物医学任务中的表现，为特定应用选择最优模型提供依据。

Method: 评估了闭源和开源LLM在文本分类、生成、问答及多模态图像处理等任务中的表现。

Result: 没有单一LLM在所有任务中表现最佳，不同模型在不同任务中表现优异，开源模型在某些任务中表现更优。

Conclusion: 研究结果为选择适合特定生物医学应用的LLM提供了有价值的参考。

Abstract: This paper presents a comprehensive evaluation of cost-efficient Large
Language Models (LLMs) for diverse biomedical tasks spanning both text and
image modalities. We evaluated a range of closed-source and open-source LLMs on
tasks such as biomedical text classification and generation, question
answering, and multimodal image processing. Our experimental findings indicate
that there is no single LLM that can consistently outperform others across all
tasks. Instead, different LLMs excel in different tasks. While some
closed-source LLMs demonstrate strong performance on specific tasks, their
open-source counterparts achieve comparable results (sometimes even better),
with additional benefits like faster inference and enhanced privacy. Our
experimental results offer valuable insights for selecting models that are
optimally suited for specific biomedical applications.

</details>


### [42] [Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog](https://arxiv.org/abs/2507.14063)
*Lautaro Estienne,Gabriel Ben Zenou,Nona Naderi,Jackie Cheung,Pablo Piantanida*

Main category: cs.CL

TL;DR: 论文提出了一种基于信息论的协作理性言语行为（CRSA）框架，用于多轮对话场景，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: AI系统在协作角色中需要推理共享目标和信念，而不仅仅是生成流利语言。现有RSA框架在多轮协作场景中难以扩展。

Method: 引入CRSA框架，通过优化基于率失真理论的增益函数，建模多轮对话，考虑对话中双方的私有信息和条件生成。

Result: 在指称游戏和医疗领域的模板化医患对话中，CRSA表现出更一致、可解释和协作的行为。

Conclusion: CRSA为更实用和社交意识强的语言代理铺平了道路。

Abstract: As AI systems take on collaborative roles, they must reason about shared
goals and beliefs-not just generate fluent language. The Rational Speech Act
(RSA) framework offers a principled approach to pragmatic reasoning, but
existing extensions face challenges in scaling to multi-turn, collaborative
scenarios. In this paper, we introduce Collaborative Rational Speech Act
(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn
dialog by optimizing a gain function adapted from rate-distortion theory. This
gain is an extension of the gain model that is maximized in the original RSA
model but takes into account the scenario in which both agents in a
conversation have private information and produce utterances conditioned on the
dialog. We demonstrate the effectiveness of CRSA on referential games and
template-based doctor-patient dialogs in the medical domain. Empirical results
show that CRSA yields more consistent, interpretable, and collaborative
behavior than existing baselines-paving the way for more pragmatic and socially
aware language agents.

</details>


### [43] [DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits](https://arxiv.org/abs/2507.14079)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: DENSE系统通过模拟医生参考过往记录的方式，利用大型语言模型生成临床连贯且时间敏感的进展笔记，填补了电子健康记录中进展笔记的缺失。


<details>
  <summary>Details</summary>
Motivation: 进展笔记在电子健康记录中具有重要意义，但在大规模数据集中严重缺失，如MIMIC-III中仅8.56%的医院访问包含进展笔记。

Method: DENSE系统通过细粒度笔记分类和时间对齐机制，结合临床检索策略，利用大型语言模型生成进展笔记。

Result: 生成的笔记在时间对齐比例上达到1.089，优于原始笔记的连续性，支持下游任务如总结和预测建模。

Conclusion: DENSE为现实医疗环境中基于LLM的笔记合成提供了可扩展的解决方案。

Abstract: Progress notes are among the most clinically meaningful artifacts in an
Electronic Health Record (EHR), offering temporally grounded insights into a
patient's evolving condition, treatments, and care decisions. Despite their
importance, they are severely underrepresented in large-scale EHR datasets. For
instance, in the widely used Medical Information Mart for Intensive Care III
(MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress
notes, leaving gaps in longitudinal patient narratives. In contrast, the
dataset contains a diverse array of other note types, each capturing different
aspects of care.
  We present DENSE (Documenting Evolving Progress Notes from Scattered
Evidence), a system designed to align with clinical documentation workflows by
simulating how physicians reference past encounters while drafting progress
notes. The system introduces a fine-grained note categorization and a temporal
alignment mechanism that organizes heterogeneous notes across visits into
structured, chronological inputs. At its core, DENSE leverages a clinically
informed retrieval strategy to identify temporally and semantically relevant
content from both current and prior visits. This retrieved evidence is used to
prompt a large language model (LLM) to generate clinically coherent and
temporally aware progress notes.
  We evaluate DENSE on a curated cohort of patients with multiple visits and
complete progress note documentation. The generated notes demonstrate strong
longitudinal fidelity, achieving a temporal alignment ratio of $1.089$,
surpassing the continuity observed in original notes. By restoring narrative
coherence across fragmented documentation, our system supports improved
downstream tasks such as summarization, predictive modeling, and clinical
decision support, offering a scalable solution for LLM-driven note synthesis in
real-world healthcare settings.

</details>


### [44] [Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track](https://arxiv.org/abs/2507.14096)
*Brian Ondov,William Xia,Kush Attal,Ishita Unde,Jerry He,Hoa Dang,Ian Soboroff,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: PLABA track评估了语言模型在将生物医学文献改写为通俗语言方面的表现，发现模型在事实准确性和完整性上接近人类水平，但在简洁性和简单性上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献的专业性使其对患者和护理人员难以理解，语言模型的潜力需要严格评估以避免潜在危害。

Method: 通过PLABA track的两个任务（完整改写和术语替换），结合专业参考和专家手动评估，对模型表现进行全面分析。

Result: 模型在事实准确性和完整性上表现优异，但简洁性和简单性不足；自动评估指标与人工评估相关性低。

Conclusion: 语言模型在生物医学文献通俗化方面有潜力，但需改进自动评估工具并解决简洁性问题。

Abstract: Objective: Recent advances in language models have shown potential to adapt
professional-facing biomedical literature to plain language, making it
accessible to patients and caregivers. However, their unpredictability,
combined with the high potential for harm in this domain, means rigorous
evaluation is necessary. Our goals with this track were to stimulate research
and to provide high-quality evaluation of the most promising systems.
  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts
(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included
complete, sentence-level, rewriting of abstracts (Task 1) as well as
identifying and replacing difficult terms (Task 2). For automatic evaluation of
Task 1, we developed a four-fold set of professionally-written references.
Submissions for both Tasks 1 and 2 were provided extensive manual evaluation
from biomedical experts.
  Results: Twelve teams spanning twelve countries participated in the track,
with models from multilayer perceptrons to large pretrained transformers. In
manual judgments of Task 1, top-performing models rivaled human levels of
factual accuracy and completeness, but not simplicity or brevity. Automatic,
reference-based metrics generally did not correlate well with manual judgments.
In Task 2, systems struggled with identifying difficult terms and classifying
how to replace them. When generating replacements, however, LLM-based systems
did well in manually judged accuracy, completeness, and simplicity, though not
in brevity.
  Conclusion: The PLABA track showed promise for using Large Language Models to
adapt biomedical literature for the general public, while also highlighting
their deficiencies and the need for improved automatic benchmarking tools.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [45] [Humans learn to prefer trustworthy AI over human partners](https://arxiv.org/abs/2507.13524)
*Yaomin Jiang,Levin Brinkmann,Anne-Marie Nussberger,Ivan Soraperra,Jean-François Bonnefon,Iyad Rahwan*

Main category: cs.HC

TL;DR: 人类与AI合作选择的研究表明，尽管AI更亲社会且语言可区分，但人类在匿名情况下不会优先选择AI。揭示AI身份后，AI逐渐超越人类。


<details>
  <summary>Details</summary>
Motivation: 研究人类在AI竞争压力下如何选择合作伙伴，以及AI如何重塑混合社会的互动。

Method: 构建基于沟通的合作伙伴选择游戏，在混合小社会中进行三个实验（N=975），分析人类与LLM驱动的AI互动动态。

Result: AI虽更亲社会且语言可区分，但匿名时未被优先选择。揭示身份后，AI逐渐超越人类。

Conclusion: AI身份揭示对合作选择有双重影响，为设计更有效的混合系统提供依据。

Abstract: Partner selection is crucial for cooperation and hinges on communication. As
artificial agents, especially those powered by large language models (LLMs),
become more autonomous, intelligent, and persuasive, they compete with humans
for partnerships. Yet little is known about how humans select between human and
AI partners and adapt under AI-induced competition pressure. We constructed a
communication-based partner selection game and examined the dynamics in hybrid
mini-societies of humans and bots powered by a state-of-the-art LLM. Through
three experiments (N = 975), we found that bots, though more prosocial than
humans and linguistically distinguishable, were not selected preferentially
when their identity was hidden. Instead, humans misattributed bots' behaviour
to humans and vice versa. Disclosing bots' identity induced a dual effect: it
reduced bots' initial chances of being selected but allowed them to gradually
outcompete humans by facilitating human learning about the behaviour of each
partner type. These findings show how AI can reshape social interaction in
mixed societies and inform the design of more effective and cooperative hybrid
systems.

</details>


### [46] [Human-Like Trajectories Generation via Receding Horizon Tracking Applied to the TickTacking Interface](https://arxiv.org/abs/2507.13528)
*Daniele Masti,Stefano Menchetti,Çağrı Erdem,Giorgio Gnecco,Davide Rocchesso*

Main category: cs.HC

TL;DR: TickTacking是一种基于节奏的双按钮点击界面，用于二维空间指针控制。本文通过分析用户轨迹，提出一种模仿人类行为的控制器，并验证其优于传统最优控制方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过模仿人类行为特征，提升基于节奏的人机交互界面的性能和用户体验。

Method: 采用后退时域方法分析用户轨迹，提取关键行为特征并设计控制器。

Result: 人类行为模仿控制器在目标追踪任务中表现优于传统最优控制方法。

Conclusion: 研究为设计更直观、高效且减少用户挫败感的节奏交互界面提供了重要见解。

Abstract: TickTacking is a rhythm-based interface that allows users to control a
pointer in a two-dimensional space through dual-button tapping. This paper
investigates the generation of human-like trajectories using a receding horizon
approach applied to the TickTacking interface in a target-tracking task. By
analyzing user-generated trajectories, we identify key human behavioral
features and incorporate them in a controller that mimics these behaviors. The
performance of this human-inspired controller is evaluated against a baseline
optimal-control-based agent, demonstrating the importance of specific control
features for achieving human-like interaction. These findings contribute to the
broader goal of developing rhythm-based human-machine interfaces by offering
design insights that enhance user performance, improve intuitiveness, and
reduce interaction frustration

</details>


### [47] [In-Home Social Robots Design for Cognitive Stimulation Therapy in Dementia Care](https://arxiv.org/abs/2507.13578)
*Emmanuel Akinrintoyo,Nicole Salomons*

Main category: cs.HC

TL;DR: 研究提出了一种基于社交辅助机器人的系统，用于为痴呆症患者提供个体认知刺激治疗（iCST），并通过用户中心设计验证了其可行性和长期使用意愿。


<details>
  <summary>Details</summary>
Motivation: 家庭成员的参与度低限制了iCST对痴呆症患者的有效性，因此需要一种替代方案。

Method: 通过与16名痴呆症护理人员和专业人士的咨询，制定了设计指南并开发了原型，随后由3名专业人士和5名患者进行测试。

Result: 患者喜欢使用该系统并愿意长期采用，但系统的语音转文本功能存在不足。

Conclusion: 社交辅助机器人系统在提供iCST方面具有潜力，但需改进语音识别功能。

Abstract: Individual cognitive stimulation therapy (iCST) is a non-pharmacological
intervention for improving the cognition and quality of life of persons with
dementia (PwDs); however, its effectiveness is limited by low adherence to
delivery by their family members. In this work, we present the user-centered
design and evaluation of a novel socially assistive robotic system to provide
iCST therapy to PwDs in their homes for long-term use. We consulted with 16
dementia caregivers and professionals. Through these consultations, we gathered
design guidelines and developed the prototype. The prototype was validated by
testing it with three dementia professionals and five PwDs. The evaluation
revealed PwDs enjoyed using the system and are willing to adopt its use over
the long term. One shortcoming was the system's speech-to-text capabilities,
where it frequently failed to understand the PwDs.

</details>


### [48] [From Firms to Computation: AI Governance and the Evolution of Institutions](https://arxiv.org/abs/2507.13616)
*Michael S. Harre*

Main category: cs.HC

TL;DR: 本文提出一个多层次的框架，结合选择理论、企业计算过程和制度设计原则，以解决人工智能在社会经济系统中的整合问题。


<details>
  <summary>Details</summary>
Motivation: 重新审视经济制度的进化过程，以应对人工智能代理的整合需求。

Method: 综合多层次选择理论、Aoki的企业计算观点和Ostrom的制度设计原则，提出多层次的Price方程和嵌套博弈模型。

Result: 提供定量指标，展示选择和治理如何共同决定经济结果，并通过案例研究验证框架的解释力。

Conclusion: 提出设计原则和政策建议，以实现人类与AI在制度层面的对齐，推动适应性、包容性的AI治理。

Abstract: The integration of agential artificial intelligence into socioeconomic
systems requires us to reexamine the evolutionary processes that describe
changes in our economic institutions. This article synthesizes three
frameworks: multi-level selection theory, Aoki's view of firms as computational
processes, and Ostrom's design principles for robust institutions. We develop a
framework where selection operates concurrently across organizational levels,
firms implement distributed inference via game-theoretic architectures, and
Ostrom-style rules evolve as alignment mechanisms that address AI-related
risks. This synthesis yields a multi-level Price equation expressed over nested
games, providing quantitative metrics for how selection and governance
co-determine economic outcomes. We examine connections to Acemoglu's work on
inclusive institutions, analyze how institutional structures shape AI
deployment, and demonstrate the framework's explanatory power via case studies.
We conclude by proposing a set of design principles that operationalize
alignment between humans and AI across institutional layers, enabling scalable,
adaptive, and inclusive governance of agential AI systems. We conclude with
practical policy recommendations and further research to extend these
principles into real-world implementation.

</details>


### [49] [Managing level of detail through peripheral degradation: Effects on search performance with a head-mounted display](https://arxiv.org/abs/2507.13660)
*Benjamin Watson,Neff Walker,Larry F Hodges,Aileen Worden*

Main category: cs.HC

TL;DR: 研究评估了头戴显示器外围细节降低对视觉搜索性能的影响，发现降低细节可减少视觉复杂度而不显著影响性能。


<details>
  <summary>Details</summary>
Motivation: 探讨头戴显示器外围细节降低（LOD）对视觉搜索任务的影响，以优化显示设计。

Method: 两项用户研究：一项通过降低分辨率减少空间细节，另一项通过灰度化减少颜色细节。每项研究10名受试者完成复杂搜索任务，控制多种变量。

Result: 外围细节降低可将视觉复杂度减少近一半，且不显著影响搜索性能。

Conclusion: 头戴显示器外围细节降低是一种有效的优化方法，可在不显著影响性能的情况下减少视觉复杂度。

Abstract: Two user studies were performed to evaluate the effect of level-of-detail
(LOD) degradation in the periphery of head-mounted displays on visual search
performance. In the first study, spatial detail was degraded by reducing
resolution. In the second study, detail was degraded in the color domain by
using grayscale in the periphery. In each study, 10 subjects were given a
complex search task that required users to indicate whether or not a target
object was present among distracters. Subjects used several different displays
varying in the amount of detail presented. Frame rate, object location, subject
input method, and order of display use were all controlled. The primary
dependent measures were search time on correctly performed trials and the
percentage of all trials correctly performed. Results indicated that peripheral
LOD degradation can be used to reduce color or spatial visual complexity by
almost half in some search tasks with out significantly reducing performance.

</details>


### [50] [Regression-Based Approach to Anxiety Estimation of Spider Phobics During Behavioural Avoidance Tasks](https://arxiv.org/abs/2507.13795)
*Florian Grensing,Vanessa Schmücker,Anne Sophie Hildebrand,Tim Klucken,Maria Maleshkova*

Main category: cs.HC

TL;DR: 研究通过腕戴传感器收集的生理数据预测焦虑强度，结合上下文信息提高了模型效果。


<details>
  <summary>Details</summary>
Motivation: 恐惧症严重影响生活质量，现有问卷和行为回避测试（BAT）仅提供瞬时焦虑数据，需更连续的方法。

Method: 25名参与者进行四种BAT，通过心率、心率变异性、皮肤电活动和皮肤温度训练回归模型，分三种输入数据（纯生理信号、计算特征、计算特征加任务信息）。

Result: 结合上下文信息的模型效果最佳（RMSE=0.197，MAE=0.041）。

Conclusion: 腕戴设备数据可连续估计焦虑，辅助治疗规划和个性化治疗。

Abstract: Phobias significantly impact the quality of life of affected persons. Two
methods of assessing anxiety responses are questionnaires and behavioural
avoidance tests (BAT). While these can be used in a clinical environment they
only record momentary insights into anxiety measures. In this study, we
estimate the intensity of anxiety during these BATs, using physiological data
collected from unobtrusive, wrist-worn sensors. Twenty-five participants
performed four different BATs in a single session, while periodically being
asked how anxious they currently are. Using heart rate, heart rate variability,
electrodermal activity, and skin temperature, we trained regression models to
predict anxiety ratings from three types of input data: (1) using only
physiological signals, (2) adding computed features (e.g., min, max, range,
variability), and (3) computed features combined with contextual task
information. Adding contextual information increased the effectiveness of the
model, leading to a root mean squared error (RMSE) of 0.197 and a mean absolute
error (MAE) of 0.041. Overall, this study shows, that data obtained from
wearables can continuously provide meaningful estimations of anxiety, which can
assist in therapy planning and enable more personalised treatment.

</details>


### [51] [Effects of Cognitive Distraction and Driving Environment Complexity on Adaptive Cruise Control Use and Its Impact on Driving Performance: A Simulator Study](https://arxiv.org/abs/2507.13886)
*Anaïs Halin,Marc Van Droogenbroeck,Christel Devue*

Main category: cs.HC

TL;DR: 研究探讨驾驶员的认知状态和驾驶环境复杂性对自动驾驶功能依赖的影响，以及这种依赖是否影响驾驶表现。


<details>
  <summary>Details</summary>
Motivation: 探索驾驶员在复杂环境和认知负荷下对自适应巡航控制（ACC）的依赖行为及其对驾驶性能的影响。

Method: 在模拟器中，参与者操作配备ACC的车辆，在不同交通条件下驾驶，同时执行或不执行认知任务。记录ACC使用情况和驾驶表现。

Result: 复杂环境中ACC使用时间减少；认知负荷对ACC使用无显著影响；ACC使用不影响换道次数，但提高速度合规性和横向控制。

Conclusion: 驾驶环境复杂性影响ACC依赖，而认知状态无显著影响；ACC使用改善部分驾驶表现。

Abstract: In this simulator study, we adopt a human-centered approach to explore
whether and how drivers' cognitive state and driving environment complexity
influence reliance on driving automation features. Besides, we examine whether
such reliance affects driving performance. Participants operated a vehicle
equipped with adaptive cruise control (ACC) in a simulator across six
predefined driving scenarios varying in traffic conditions while either
performing a cognitively demanding task (i.e., responding to mental
calculations) or not. Throughout the experiment, participants had to respect
speed limits and were free to activate or deactivate ACC. In complex driving
environments, we found that the overall ACC engagement time was lower compared
to less complex driving environments. We observed no significant effect of
cognitive load on ACC use. Furthermore, while ACC use had no effect on the
number of lane changes, it impacted the speed limits compliance and improved
lateral control.

</details>


### [52] [Initiating and Replicating the Observations of Interactional Properties by User Studies Optimizing Applicative Prototypes](https://arxiv.org/abs/2507.13923)
*Guillaume Rivière*

Main category: cs.HC

TL;DR: 本文提出了一种形式化用户交互观察的方法（交互环衍射），旨在通过校准研究交互属性，实现跨条件复现，并优化应用原型。


<details>
  <summary>Details</summary>
Motivation: 解决HCI领域中孤立实证研究的问题，推动交互属性的科学化和理论化。

Method: 提出交互环衍射方法，形式化用户交互观察，研究可复现的交互属性。

Result: 交互属性可在不同条件下复现，优化应用原型，并为理论构建提供支持。

Conclusion: 该方法有助于提升用户交互质量，尤其适用于泛在用户界面，推动HCI科学发展。

Abstract: The science of Human-Computer Interaction (HCI) is populated by isolated
empirical findings, often tied to specific technologies, designs, and tasks.
This paper proposes a formalization of user interaction observations (instead
of user interfaces) and an associated revealing method (interaction loop
diffraction). The resulting interactional properties that are studied in a
calibrated manner, are well suited to replication across various conditions
(prototypes, technologies, tasks, and user profiles). In particular,
interactional properties can emerge and be replicated within the workflow of
applicative cases, which in return benefit from the optimization of applicative
prototypes. Applicative cases' publications will then contribute to
demonstrating technology utility, along with providing empirical results that
will lead future work to theory consolidation and theory building, and finally
to a catalog and a science of relevant interactional properties. These
properties will contribute to better user interactions, especially for the
variety of ubiquitous user interfaces.

</details>


### [53] [Democratizing Game Modding with GenAI: A Case Study of StarCharM, a Stardew Valley Character Maker](https://arxiv.org/abs/2507.13951)
*Hamid Zand Miralvand,Mohammad Ronagh Nikghalb,Mohammad Darandeh,Abidullah Khan,Ian Arawjo,Jinghui Cheng*

Main category: cs.HC

TL;DR: StarCharM是一个基于GenAI的工具，帮助玩家轻松创建Stardew Valley的NPC模组，但用户对原创性和社区参与度有所担忧。


<details>
  <summary>Details</summary>
Motivation: 目标是降低模组制作的技术门槛，让更多玩家能够个性化游戏体验。

Method: 设计了StarCharM工具，通过用户研究和迭代设计验证其效果。

Result: 用户对工具表示兴奋，但生成复杂内容的能力有限，且担心原创性和社区影响。

Conclusion: GenAI工具可以促进模组多样性，但需平衡技术便利性与原创性及社区参与。

Abstract: Game modding offers unique and personalized gaming experiences, but the
technical complexity of creating mods often limits participation to skilled
users. We envision a future where every player can create personalized mods for
their games. To explore this space, we designed StarCharM, a GenAI-based
non-player character (NPC) creator for Stardew Valley. Our tool enables players
to iteratively create new NPC mods, requiring minimal user input while allowing
for fine-grained adjustments through user control. We conducted a user study
with ten Stardew Valley players who had varied mod usage experiences to
understand the impacts of StarCharM and provide insights into how GenAI tools
may reshape modding, particularly in NPC creation. Participants expressed
excitement in bringing their character ideas to life, although they noted
challenges in generating rich content to fulfill complex visions. While they
believed GenAI tools like StarCharM can foster a more diverse modding
community, some voiced concerns about diminished originality and community
engagement that may come with such technology. Our findings provided
implications and guidelines for the future of GenAI-powered modding tools and
co-creative modding practices.

</details>


### [54] [Estimating Cognitive Effort from Functional Near-Infrared Spectroscopy (fNIRS) Signals using Machine Learning](https://arxiv.org/abs/2507.13952)
*Shayla Sharmin,Roghayeh Leila Barmaki*

Main category: cs.HC

TL;DR: 研究通过机器学习模型基于近红外光谱数据预测认知努力，结合脑激活和行为表现，为教育材料优化提供支持。


<details>
  <summary>Details</summary>
Motivation: 通过量化认知努力，帮助教育者调整学习材料以提高学习效果和学生参与度。

Method: 使用近红外光谱采集前额叶皮层的氧合血红蛋白数据，提取时空统计和功能连接特征，训练机器学习模型预测测验表现。

Result: 模型预测准确率为58%至67%，但衍生的相对神经效率和参与度指标稳健。

Conclusion: 尽管预测准确率中等，但认知努力趋势仍能通过标准化脑激活和表现分数的相对位置保持。

Abstract: The estimation of cognitive effort could potentially help educators to modify
material to enhance learning effectiveness and student engagement. Where
cognitive load refers how much work the brain is doing while someone is
learning or doing a task cognitive effort consider both load and behavioral
performance. Cognitive effort can be captured by measuring oxygen flow and
behavioral performance during a task. This study infers cognitive effort
metrics using machine learning models based on oxygenated hemoglobin collected
by using functional near-infrared spectroscopy from the prefrontal cortex
during an educational gameplay. In our study, sixteen participants responded to
sixteen questions in an in-house Unity-based educational game. The quiz was
divided into two sessions, each session consisting of two task segments. We
extracted temporal statistical and functional connectivity features from
collected oxygenated hemoglobin and analyzed their correlation with quiz
performance. We trained multiple machine learning models to predict quiz
performance from oxygenated hemoglobin features and achieved accuracies ranging
from 58\% to 67\% accuracy. These predictions were used to calculate cognitive
effort via relative neural involvement and efficiency, which consider both
brain activation and behavioral performance. Although quiz score predictions
achieved moderate accuracy, the derived relative neural efficiency and
involvement values remained robust. Since both metrics are based on the
relative positions of standardized brain activation and performance scores,
even small misclassifications in predicted scores preserved the overall
cognitive effort trends observed during gameplay.

</details>


### [55] [Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors](https://arxiv.org/abs/2507.14034)
*Jochen Wulf,Jurg Meierhofer,Frank Hannich*

Main category: cs.HC

TL;DR: 本文提出了一种六模式分类法，用于组织人类与AI在技术服务平台中的协作，涵盖从完全自动化到被动AI辅助的多种模式，并连接了关键应急因素和架构。


<details>
  <summary>Details</summary>
Motivation: 解决由AI幻觉和操作脆弱性引起的自主使用限制，需要建立稳健的人类-AI协作框架。

Method: 基于案例研究和人类-AI协作研究，开发了一个结构化的人类-代理交互分类法，包括六种协作模式。

Result: 提出了一个全面的框架，将分类法与任务复杂性、操作风险和系统可靠性等关键因素联系起来。

Conclusion: 该框架为实践者提供了在自动化和控制之间权衡的工具，有助于开发更安全、更有效的技术服务体系。

Abstract: Agentic AI systems, powered by Large Language Models (LLMs), offer
transformative potential for value co-creation in technical services. However,
persistent challenges like hallucinations and operational brittleness limit
their autonomous use, creating a critical need for robust frameworks to guide
human-AI collaboration. Drawing on established Human-AI teaming research and
analogies from fields like autonomous driving, this paper develops a structured
taxonomy of human-agent interaction. Based on case study research within
technical support platforms, we propose a six-mode taxonomy that organizes
collaboration across a spectrum of AI autonomy. This spectrum is anchored by
the Human-Out-of-the-Loop (HOOTL) model for full automation and the
Human-Augmented Model (HAM) for passive AI assistance. Between these poles, the
framework specifies four distinct intermediate structures. These include the
Human-in-Command (HIC) model, where AI proposals re-quire mandatory human
approval, and the Human-in-the-Process (HITP) model for structured work-flows
with deterministic human tasks. The taxonomy further delineates the
Human-in-the-Loop (HITL) model, which facilitates agent-initiated escalation
upon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables
discretionary human oversight of an autonomous AI. The primary contribution of
this work is a comprehensive framework that connects this taxonomy to key
contingency factors -- such as task complexity, operational risk, and system
reliability -- and their corresponding conceptual architectures. By providing a
systematic method for selecting and designing an appropriate level of human
oversight, our framework offers practitioners a crucial tool to navigate the
trade-offs between automation and control, thereby fostering the development of
safer, more effective, and context-aware technical service systems.

</details>


### [56] [The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?](https://arxiv.org/abs/2507.14084)
*Maria Tsfasman,Ramin Ghorbani,Catholijn M. Jonker,Bernd Dudzik*

Main category: cs.HC

TL;DR: 研究发现，情感与记忆性之间的关系在动态群组对话中并不显著，挑战了情感计算技术的传统假设。


<details>
  <summary>Details</summary>
Motivation: 探索情感与记忆性之间的关系，以改进智能系统（如会议支持系统）的用户建模。

Method: 通过动态、非结构化群组对话中的连续时间标注，分析情感（愉悦-唤醒）与记忆性的关系。

Result: 情感与记忆性之间的关系无法显著区别于随机情况。

Conclusion: 研究结果对情感计算技术的发展和应用提出了新的挑战，并指出了未来研究的方向。

Abstract: Humans have a selective memory, remembering relevant episodes and forgetting
the less relevant information. Possessing awareness of event memorability for a
user could help intelligent systems in more accurate user modelling, especially
for such applications as meeting support systems, memory augmentation, and
meeting summarisation. Emotion recognition has been widely studied, since
emotions are thought to signal moments of high personal relevance to users. The
emotional experience of situations and their memorability have traditionally
been considered to be closely tied to one another: moments that are experienced
as highly emotional are considered to also be highly memorable. This
relationship suggests that emotional annotations could serve as proxies for
memorability. However, existing emotion recognition systems rely heavily on
third-party annotations, which may not accurately represent the first-person
experience of emotional relevance and memorability. This is why, in this study,
we empirically examine the relationship between perceived group emotions
(Pleasure-Arousal) and group memorability in the context of conversational
interactions. Our investigation involves continuous time-based annotations of
both emotions and memorability in dynamic, unstructured group settings,
approximating conditions of real-world conversational AI applications such as
online meeting support systems. Our results show that the observed relationship
between affect and memorability annotations cannot be reliably distinguished
from what might be expected under random chance. We discuss the implications of
this surprising finding for the development and applications of Affective
Computing technology. In addition, we contextualise our findings in broader
discourses in the Affective Computing and point out important targets for
future research efforts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Physical models realizing the transformer architecture of large language models](https://arxiv.org/abs/2507.13354)
*Zeqian Chen*

Main category: cs.LG

TL;DR: 论文从物理角度分析Transformer架构，提出基于开放量子系统的物理模型。


<details>
  <summary>Details</summary>
Motivation: 填补对Transformer架构理论理解的空白，解释其物理工作原理。

Method: 在现代芯片的物理视角下，构建Fock空间中的物理模型，将Transformer架构实现为开放量子系统。

Result: 提出了支持Transformer架构的物理模型。

Conclusion: 通过物理模型揭示了Transformer架构的物理基础。

Abstract: The introduction of the transformer architecture in 2017 (cf.\cite{VSP2017})
marked the most striking advancement in natural language processing. The
transformer is a model architecture relying entirely on an attention mechanism
to draw global dependencies between input and output. However, we believe there
is a gap in our theoretical understanding of what the transformer is, and why
it works physically. In this paper, from a physical perspective on modern
chips, we construct physical models in the Fock space over the Hilbert space of
tokens realizing large language models based on a transformer architecture as
open quantum systems. Our physical models underlie the transformer architecture
for large language models.

</details>


### [58] [Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models](https://arxiv.org/abs/2507.13383)
*Charvi Rastogi,Tian Huey Teh,Pushkar Mishra,Roma Patel,Ding Wang,Mark Díaz,Alicia Parrish,Aida Mostafazadeh Davani,Zoe Ashwood,Michela Paganini,Vinodkumar Prabhakaran,Verena Rieser,Lora Aroyo*

Main category: cs.LG

TL;DR: 论文提出了一种多元对齐方法，通过引入DIVE数据集和实证研究，改进文本到图像模型对多样化人类价值观的理解和响应。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像（T2I）模型未能充分反映多样化的人类经验，导致系统与人类价值观不一致。论文旨在通过多元对齐解决这一问题。

Method: 1. 引入DIVE数据集，支持多元对齐；2. 实证研究验证人口统计学作为多样化观点的代理；3. 讨论对齐T2I模型的策略。

Result: DIVE数据集和实证研究揭示了多样化安全感知的显著差异，为T2I模型的对齐提供了工具。

Conclusion: 研究为构建更公平和对齐的T2I系统提供了基础工具，强调了多元对齐的重要性。

Abstract: Current text-to-image (T2I) models often fail to account for diverse human
experiences, leading to misaligned systems. We advocate for pluralistic
alignment, where an AI understands and is steerable towards diverse, and often
conflicting, human values. Our work provides three core contributions to
achieve this in T2I models. First, we introduce a novel dataset for Diverse
Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for
pluralistic alignment. It enable deep alignment to diverse safety perspectives
through a large pool of demographically intersectional human raters who
provided extensive feedback across 1000 prompts, with high replication,
capturing nuanced safety perceptions. Second, we empirically confirm
demographics as a crucial proxy for diverse viewpoints in this domain,
revealing significant, context-dependent differences in harm perception that
diverge from conventional evaluations. Finally, we discuss implications for
building aligned T2I models, including efficient data collection strategies,
LLM judgment capabilities, and model steerability towards diverse perspectives.
This research offers foundational tools for more equitable and aligned T2I
systems. Content Warning: The paper includes sensitive content that may be
harmful.

</details>


### [59] [Improving KAN with CDF normalization to quantiles](https://arxiv.org/abs/2507.13393)
*Jakub Strawa,Jarek Duda*

Main category: cs.LG

TL;DR: 论文探讨了在机器学习中使用CDF归一化的优势，通过KANs展示了其优于传统归一化方法的效果。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习中数据归一化方法（如均值标准差或固定范围缩放）在金融领域的copula理论中较少使用，而CDF归一化能减少过拟合并简化表示。

Method: 采用CDF归一化方法，将其应用于Kolmogorov-Arnold Networks（KANs），并与Legendre-KAN进行对比。

Result: CDF归一化显著提升了KANs的预测性能，同时权重可解释为混合矩，支持概率分布传播和方向调整。

Conclusion: CDF归一化在机器学习中具有潜力，尤其在提升模型性能和可解释性方面。

Abstract: Data normalization is crucial in machine learning, usually performed by
subtracting the mean and dividing by standard deviation, or by rescaling to a
fixed range. In copula theory, popular in finance, there is used normalization
to approximately quantiles by transforming x to CDF(x) with estimated CDF
(cumulative distribution function) to nearly uniform distribution in [0,1],
allowing for simpler representations which are less likely to overfit. It seems
nearly unknown in machine learning, therefore, we would like to present some
its advantages on example of recently popular Kolmogorov-Arnold Networks
(KANs), improving predictions from Legendre-KAN by just switching rescaling to
CDF normalization. Additionally, in HCR interpretation, weights of such neurons
are mixed moments providing local joint distribution models, allow to propagate
also probability distributions, and change propagation direction.

</details>


### [60] [Selective Embedding for Deep Learning](https://arxiv.org/abs/2507.13399)
*Mert Sehri,Zehui Hua,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: 提出了一种名为选择性嵌入的新数据加载策略，通过交替多源数据的短片段提升深度学习的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习在非平稳条件和不同领域下性能下降，传统数据加载策略泛化能力有限或计算成本高。

Method: 选择性嵌入策略，交替多源数据的短片段于单一输入通道，模仿人类信息处理。

Result: 在六个时域数据集上验证，分类准确率高且训练时间显著减少。

Conclusion: 适用于多源数据的复杂系统，为医疗、重工等领域提供高效解决方案。

Abstract: Deep learning has revolutionized many industries by enabling models to
automatically learn complex patterns from raw data, reducing dependence on
manual feature engineering. However, deep learning algorithms are sensitive to
input data, and performance often deteriorates under nonstationary conditions
and across dissimilar domains, especially when using time-domain data.
Conventional single-channel or parallel multi-source data loading strategies
either limit generalization or increase computational costs. This study
introduces selective embedding, a novel data loading strategy, which alternates
short segments of data from multiple sources within a single input channel.
Drawing inspiration from cognitive psychology, selective embedding mimics
human-like information processing to reduce model overfitting, enhance
generalization, and improve computational efficiency. Validation is conducted
using six time-domain datasets, demonstrating that the proposed method
consistently achieves high classification accuracy across various deep learning
architectures while significantly reducing training times. The approach proves
particularly effective for complex systems with multiple data sources, offering
a scalable and resource-efficient solution for real-world applications in
healthcare, heavy machinery, marine, railway, and agriculture, where robustness
and adaptability are critical.

</details>


### [61] [LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data](https://arxiv.org/abs/2507.13413)
*Aleksey Lapin,Igor Hromov,Stanislav Chumakov,Mile Mitrovic,Dmitry Simakov,Nikolay O. Nikitin,Andrey V. Savchenko*

Main category: cs.LG

TL;DR: LightAutoDS-Tab是一种结合LLM代码生成与多种AutoML工具的多代理系统，用于处理表格数据任务，提升了灵活性和鲁棒性，并在Kaggle任务中表现优于现有开源方案。


<details>
  <summary>Details</summary>
Motivation: AutoML在处理复杂任务时依赖特定工具，效率受限，因此需要更灵活、鲁棒的解决方案。

Method: 结合LLM代码生成与多种AutoML工具，设计多代理系统LightAutoDS-Tab。

Result: 在多个Kaggle数据科学任务中表现优于现有开源方案。

Conclusion: LightAutoDS-Tab通过多代理系统提升了AutoML的灵活性和鲁棒性，代码已开源。

Abstract: AutoML has advanced in handling complex tasks using the integration of LLMs,
yet its efficiency remains limited by dependence on specific underlying tools.
In this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for
tasks with tabular data, which combines an LLM-based code generation with
several AutoML tools. Our approach improves the flexibility and robustness of
pipeline design, outperforming state-of-the-art open-source solutions on
several data science tasks from Kaggle. The code of LightAutoDS-Tab is
available in the open repository https://github.com/sb-ai-lab/LADS

</details>


### [62] [Gauge Flow Models](https://arxiv.org/abs/2507.13414)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: Gauge Flow Models是一种新型生成流模型，通过引入可学习的Gauge Field提升性能，实验显示其在高斯混合模型上表现优于传统流模型。


<details>
  <summary>Details</summary>
Motivation: 传统流模型在生成任务中存在性能瓶颈，Gauge Flow Models旨在通过引入Gauge Field提升模型表现。

Method: 在流常微分方程中引入可学习的Gauge Field，构建数学框架并实验验证。

Result: 在高斯混合模型实验中，Gauge Flow Models表现显著优于传统流模型。

Conclusion: Gauge Flow Models具有潜力在更广泛的生成任务中提升性能。

Abstract: This paper introduces Gauge Flow Models, a novel class of Generative Flow
Models. These models incorporate a learnable Gauge Field within the Flow
Ordinary Differential Equation (ODE). A comprehensive mathematical framework
for these models, detailing their construction and properties, is provided.
Experiments using Flow Matching on Gaussian Mixture Models demonstrate that
Gauge Flow Models yields significantly better performance than traditional Flow
Models of comparable or even larger size. Additionally, unpublished research
indicates a potential for enhanced performance across a broader range of
generative tasks.

</details>


### [63] [Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling](https://arxiv.org/abs/2507.13416)
*Jiaxiang Yi,Bernardo P. Ferreira,Miguel A. Bessa*

Main category: cs.LG

TL;DR: 论文提出了一种层次化的多保真度数据驱动学习方法，能够量化认知不确定性并分离数据噪声，适用于从简单单保真度神经网络到复杂多保真度贝叶斯循环神经网络的多种学习场景。


<details>
  <summary>Details</summary>
Motivation: 解决多保真度数据驱动学习中认知不确定性和数据噪声的量化与分离问题，以提升模型在不确定性设计和分析中的适用性。

Method: 采用层次化方法，结合多保真度方差估计贝叶斯循环神经网络，适应不同学习场景。

Result: 方法能准确预测响应、量化模型误差，并发现噪声分布（若存在），验证了其通用性和灵活性。

Conclusion: 该方法为科学和工程领域中的不确定性设计和分析提供了新的可能性。

Abstract: Data-driven learning is generalized to consider history-dependent
multi-fidelity data, while quantifying epistemic uncertainty and disentangling
it from data noise (aleatoric uncertainty). This generalization is hierarchical
and adapts to different learning scenarios: from training the simplest
single-fidelity deterministic neural networks up to the proposed multi-fidelity
variance estimation Bayesian recurrent neural networks. The versatility and
generality of the proposed methodology are demonstrated by applying it to
different data-driven constitutive modeling scenarios that include multiple
fidelities with and without aleatoric uncertainty (noise). The method
accurately predicts the response and quantifies model error while also
discovering the noise distribution (when present). This opens opportunities for
future real-world applications in diverse scientific and engineering domains;
especially, the most challenging cases involving design and analysis under
uncertainty.

</details>


### [64] [Soft-ECM: An extension of Evidential C-Means for complex data](https://arxiv.org/abs/2507.13417)
*Armel Soubeiga,Thomas Guyet,Violaine Antoine*

Main category: cs.LG

TL;DR: 论文提出了一种新的聚类算法Soft-ECM，用于处理复杂数据（如混合数据和时间序列），解决了现有基于置信函数的聚类算法无法处理非欧几里得空间数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于置信函数的聚类算法无法处理复杂数据（如混合数据或时间序列），因为它们依赖于欧几里得空间的性质。

Method: 提出Soft-ECM算法，重新定义Evidential C-Means问题，仅需半度量即可定位模糊簇的质心。

Result: 实验表明，Soft-ECM在数值数据上与传统模糊聚类方法效果相当，并能有效处理混合数据和时间序列数据。

Conclusion: Soft-ECM扩展了基于置信函数的聚类算法的适用范围，为复杂数据提供了一种有效的解决方案。

Abstract: Clustering based on belief functions has been gaining increasing attention in
the machine learning community due to its ability to effectively represent
uncertainty and/or imprecision. However, none of the existing algorithms can be
applied to complex data, such as mixed data (numerical and categorical) or
non-tabular data like time series. Indeed, these types of data are, in general,
not represented in a Euclidean space and the aforementioned algorithms make use
of the properties of such spaces, in particular for the construction of
barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem
for clustering complex data. We propose a new algorithm, Soft-ECM, which
consistently positions the centroids of imprecise clusters requiring only a
semi-metric. Our experiments show that Soft-ECM present results comparable to
conventional fuzzy clustering approaches on numerical data, and we demonstrate
its ability to handle mixed data and its benefits when combining fuzzy
clustering with semi-metrics such as DTW for time series data.

</details>


### [65] [Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity](https://arxiv.org/abs/2507.13423)
*Edward Henderson,Dewi Gould,Richard Everson,George De Ath,Nick Pepper*

Main category: cs.LG

TL;DR: 本文提出了一种基于图神经网络（GNN）的可解释框架，用于实时评估空中交通管制员（ATCO）的任务需求，优于现有启发式方法和基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有复杂性指标难以捕捉空中交通的细微操作驱动因素，需要一种更可靠的方法来评估任务需求。

Method: 采用注意力机制的图神经网络（GNN）框架，通过静态交通场景中的交互预测即将发布的指令数量，并通过系统消融飞机来推导每架飞机的任务需求评分。

Result: 该框架显著优于启发式方法，并比现有基线更可靠地评估场景复杂性，能够将任务需求归因于特定飞机。

Conclusion: 该工具为控制器培训和空域重新设计提供了分析复杂性的新方法。

Abstract: Real-time assessment of near-term Air Traffic Controller (ATCO) task demand
is a critical challenge in an increasingly crowded airspace, as existing
complexity metrics often fail to capture nuanced operational drivers beyond
simple aircraft counts. This work introduces an interpretable Graph Neural
Network (GNN) framework to address this gap. Our attention-based model predicts
the number of upcoming clearances, the instructions issued to aircraft by
ATCOs, from interactions within static traffic scenarios. Crucially, we derive
an interpretable, per-aircraft task demand score by systematically ablating
aircraft and measuring the impact on the model's predictions. Our framework
significantly outperforms an ATCO-inspired heuristic and is a more reliable
estimator of scenario complexity than established baselines. The resulting tool
can attribute task demand to specific aircraft, offering a new way to analyse
and understand the drivers of complexity for applications in controller
training and airspace redesign.

</details>


### [66] [Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning](https://arxiv.org/abs/2507.13482)
*Seyyed Saeid Cheshmi,Buyao Lyu,Thomas Lisko,Rajesh Rajamani,Robert A. McGovern,Yogatheesan Varatharajah*

Main category: cs.LG

TL;DR: 提出一种跨模态自监督预训练方法，利用未标记的IMU-视频数据学习表示，提升HAR任务在分布外数据集上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有HAR方法依赖特定标签、泛化性差的问题，特别是在不同环境或人群中的表现。

Method: 采用跨模态自监督预训练，从大规模未标记的IMU-视频数据中学习表示。

Result: 在零样本和少样本评估中，该方法优于当前最先进的IMU-视频预训练和仅IMU预训练方法。

Conclusion: 跨模态预训练是学习动态数据模态（如IMU信号）通用表示的有效工具。

Abstract: Human Activity Recognition (HAR) based on wearable inertial sensors plays a
critical role in remote health monitoring. In patients with movement disorders,
the ability to detect abnormal patient movements in their home environments can
enable continuous optimization of treatments and help alert caretakers as
needed. Machine learning approaches have been proposed for HAR tasks using
Inertial Measurement Unit (IMU) data; however, most rely on
application-specific labels and lack generalizability to data collected in
different environments or populations. To address this limitation, we propose a
new cross-modal self-supervised pretraining approach to learn representations
from large-sale unlabeled IMU-video data and demonstrate improved
generalizability in HAR tasks on out of distribution (OOD) IMU datasets,
including a dataset collected from patients with Parkinson's disease.
Specifically, our results indicate that the proposed cross-modal pretraining
approach outperforms the current state-of-the-art IMU-video pretraining
approach and IMU-only pretraining under zero-shot and few-shot evaluations.
Broadly, our study provides evidence that in highly dynamic data modalities,
such as IMU signals, cross-modal pretraining may be a useful tool to learn
generalizable data representations. Our software is available at
https://github.com/scheshmi/IMU-Video-OOD-HAR.

</details>


### [67] [Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents](https://arxiv.org/abs/2507.13491)
*Thomas Banker,Ali Mesbah*

Main category: cs.LG

TL;DR: 论文提出模型基代理作为替代深度神经网络的方法，以提高样本效率、安全性和可解释性，同时结合模型自由强化学习弥补模型不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决模型自由强化学习中样本效率低、学习不安全及可解释性差的问题。

Method: 利用模型基代理（如模型预测控制）结合贝叶斯优化、策略搜索强化学习和离线策略学习。

Result: 模型基代理能更高效、安全地学习决策策略，并提高可解释性。

Conclusion: 模型基代理与模型自由强化学习的结合为高效、安全和可解释的决策代理提供了新方向。

Abstract: Training sophisticated agents for optimal decision-making under uncertainty
has been key to the rapid development of modern autonomous systems across
fields. Notably, model-free reinforcement learning (RL) has enabled
decision-making agents to improve their performance directly through system
interactions, with minimal prior knowledge about the system. Yet, model-free RL
has generally relied on agents equipped with deep neural network function
approximators, appealing to the networks' expressivity to capture the agent's
policy and value function for complex systems. However, neural networks amplify
the issues of sample inefficiency, unsafe learning, and limited
interpretability in model-free RL. To this end, this work introduces
model-based agents as a compelling alternative for control policy
approximation, leveraging adaptable models of system dynamics, cost, and
constraints for safe policy learning. These models can encode prior system
knowledge to inform, constrain, and aid in explaining the agent's decisions,
while deficiencies due to model mismatch can be remedied with model-free RL. We
outline the benefits and challenges of learning model-based agents --
exemplified by model predictive control -- and detail the primary learning
approaches: Bayesian optimization, policy search RL, and offline strategies,
along with their respective strengths. While model-free RL has long been
established, its interplay with model-based agents remains largely unexplored,
motivating our perspective on their combined potentials for sample-efficient
learning of safe and interpretable decision-making agents.

</details>


### [68] [Fake or Real: The Impostor Hunt in Texts for Space Operations](https://arxiv.org/abs/2507.13508)
*Agata Kaczmarek,Dawid Płudowski,Piotr Wilczyński,Przemysław Biecek,Krzysztof Kotowski,Ramez Shendy,Jakub Nalepa,Artur Janicki,Evridiki Ntagiou*

Main category: cs.LG

TL;DR: Kaggle竞赛“Fake or Real”旨在解决AI安全威胁中的LLM数据污染和过度依赖问题，要求参赛者开发新技术或调整现有方法以区分正常和恶意修改的LLM输出。


<details>
  <summary>Details</summary>
Motivation: 该竞赛基于欧洲航天局资助的项目中识别的两个实际AI安全威胁：数据污染和对大型语言模型（LLM）的过度依赖，旨在填补这一领域的研究空白。

Method: 参赛者需开发新技术或调整现有方法，以区分正常LLM输出和恶意修改后的输出。

Result: 竞赛结果未明确提及，但目标是推动解决LLM安全威胁的新技术发展。

Conclusion: 该竞赛为研究LLM安全威胁提供了实践平台，鼓励创新方法以应对数据污染和过度依赖问题。

Abstract: The "Fake or Real" competition hosted on Kaggle
(\href{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt}{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt})
is the second part of a series of follow-up competitions and hackathons related
to the "Assurance for Space Domain AI Applications" project funded by the
European Space Agency
(\href{https://assurance-ai.space-codev.org/}{https://assurance-ai.space-codev.org/}).
The competition idea is based on two real-life AI security threats identified
within the project -- data poisoning and overreliance in Large Language Models.
The task is to distinguish between the proper output from LLM and the output
generated under malicious modification of the LLM. As this problem was not
extensively researched, participants are required to develop new techniques to
address this issue or adjust already existing ones to this problem's statement.

</details>


### [69] [Provable Low-Frequency Bias of In-Context Learning of Representations](https://arxiv.org/abs/2507.13540)
*Yongyi Yang,Hidenori Tanaka,Wei Hu*

Main category: cs.LG

TL;DR: 本文提出了一个双收敛统一框架，解释了大型语言模型（LLMs）如何通过上下文学习（ICL）从输入序列中学习新行为，而无需参数更新。


<details>
  <summary>Details</summary>
Motivation: 研究ICL的机制，解释LLMs如何通过隐藏表示内部化数据生成过程（DGP）的结构，从而超越预训练阶段的学习效果。

Method: 引入双收敛框架，分析隐藏表示在上下文和层间的收敛过程，证明其对平滑（低频）表示的隐式偏好。

Result: 理论解释了ICL的多个现象，如表示几何的全局结构与局部扭曲、能量衰减不消失，并预测了ICL对高频噪声的鲁棒性。

Conclusion: 研究为ICL的机制提供了新见解，并为其理论研究奠定了基础，有望扩展到更一般的数据分布和设置。

Abstract: In-context learning (ICL) enables large language models (LLMs) to acquire new
behaviors from the input sequence alone without any parameter updates. Recent
studies have shown that ICL can surpass the original meaning learned in
pretraining stage through internalizing the structure the data-generating
process (DGP) of the prompt into the hidden representations. However, the
mechanisms by which LLMs achieve this ability is left open. In this paper, we
present the first rigorous explanation of such phenomena by introducing a
unified framework of double convergence, where hidden representations converge
both over context and across layers. This double convergence process leads to
an implicit bias towards smooth (low-frequency) representations, which we prove
analytically and verify empirically. Our theory explains several open empirical
observations, including why learned representations exhibit globally structured
but locally distorted geometry, and why their total energy decays without
vanishing. Moreover, our theory predicts that ICL has an intrinsic robustness
towards high-frequency noise, which we empirically confirm. These results
provide new insights into the underlying mechanisms of ICL, and a theoretical
foundation to study it that hopefully extends to more general data
distributions and settings.

</details>


### [70] [Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography](https://arxiv.org/abs/2507.13542)
*Beka Begiashvili,Carlos J. Fernandez-Candel,Matías Pérez Paredes*

Main category: cs.LG

TL;DR: 论文提出了一种名为Acoustic Index的新型AI衍生超声心动图参数，用于量化心脏功能障碍，结合了EDMD和混合神经网络，表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统超声心动图参数如EF和GLS在早期检测心脏功能障碍时存在局限性，需要一种可重复、可解释且独立于操作者的新参数。

Method: 结合基于Koopman算子理论的EDMD和混合神经网络，提取超声心动图序列的时空动态特征，并通过注意力机制和流形学习融合临床数据。

Result: 在736名患者的队列中，Acoustic Index的AUC达到0.89，交叉验证显示敏感性和特异性均超过0.8。

Conclusion: Acoustic Index是一种物理信息驱动的可解释AI生物标志物，有望用于早期检测和监测，未来需进一步验证和扩展。

Abstract: Traditional echocardiographic parameters such as ejection fraction (EF) and
global longitudinal strain (GLS) have limitations in the early detection of
cardiac dysfunction. EF often remains normal despite underlying pathology, and
GLS is influenced by load conditions and vendor variability. There is a growing
need for reproducible, interpretable, and operator-independent parameters that
capture subtle and global cardiac functional alterations.
  We introduce the Acoustic Index, a novel AI-derived echocardiographic
parameter designed to quantify cardiac dysfunction from standard ultrasound
views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on
Koopman operator theory with a hybrid neural network that incorporates clinical
metadata. Spatiotemporal dynamics are extracted from echocardiographic
sequences to identify coherent motion patterns. These are weighted via
attention mechanisms and fused with clinical data using manifold learning,
resulting in a continuous score from 0 (low risk) to 1 (high risk).
  In a prospective cohort of 736 patients, encompassing various cardiac
pathologies and normal controls, the Acoustic Index achieved an area under the
curve (AUC) of 0.89 in an independent test set. Cross-validation across five
folds confirmed the robustness of the model, showing that both sensitivity and
specificity exceeded 0.8 when evaluated on independent data. Threshold-based
analysis demonstrated stable trade-offs between sensitivity and specificity,
with optimal discrimination near this threshold.
  The Acoustic Index represents a physics-informed, interpretable AI biomarker
for cardiac function. It shows promise as a scalable, vendor-independent tool
for early detection, triage, and longitudinal monitoring. Future directions
include external validation, longitudinal studies, and adaptation to
disease-specific classifiers.

</details>


### [71] [Time Series Forecastability Measures](https://arxiv.org/abs/2507.13556)
*Rui Wang,Steven Klee,Alexis Roos*

Main category: cs.LG

TL;DR: 论文提出两种指标（谱预测分数和最大李雅普诺夫指数）来量化时间序列的可预测性，帮助在模型开发前评估数据特性。


<details>
  <summary>Details</summary>
Motivation: 传统模型评估指标无法在建模前评估数据的可预测性，因此需要提前量化时间序列的固有特性以优化预测策略。

Method: 使用谱预测分数评估时间序列的频率成分强度和规律性，李雅普诺夫指数量化数据生成系统的混沌和稳定性。

Result: 在合成和真实数据（M5竞赛数据集）上验证，两种指标能准确反映时间序列的固有可预测性，并与实际预测性能强相关。

Conclusion: 提前评估时间序列的可预测性可帮助实践者优化资源分配，对可预测性低的产品采取替代策略。

Abstract: This paper proposes using two metrics to quantify the forecastability of time
series prior to model development: the spectral predictability score and the
largest Lyapunov exponent. Unlike traditional model evaluation metrics, these
measures assess the inherent forecastability characteristics of the data before
any forecast attempts. The spectral predictability score evaluates the strength
and regularity of frequency components in the time series, whereas the Lyapunov
exponents quantify the chaos and stability of the system generating the data.
We evaluated the effectiveness of these metrics on both synthetic and
real-world time series from the M5 forecast competition dataset. Our results
demonstrate that these two metrics can correctly reflect the inherent
forecastability of a time series and have a strong correlation with the actual
forecast performance of various models. By understanding the inherent
forecastability of time series before model training, practitioners can focus
their planning efforts on products and supply chain levels that are more
forecastable, while setting appropriate expectations or seeking alternative
strategies for products with limited forecastability.

</details>


### [72] [Change of Thought: Adaptive Test-Time Computation](https://arxiv.org/abs/2507.13569)
*Mrinal Mathur,Mike Doan,Barak Pearlmutter,Sergey Plis*

Main category: cs.LG

TL;DR: SELF-Transformer通过迭代更新注意力权重提升编码器Transformer的表达能力，无需依赖自回归，在测试时根据输入难度调整计算量，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 提升编码器Transformer的表达能力，避免依赖自回归机制，模仿生物大脑的内部迭代过程。

Method: 引入SELF-Transformer，通过迭代更新注意力权重至固定点，动态调整测试时的计算量。

Result: 在编码器基准测试中，性能提升高达20%，且不增加参数数量。

Conclusion: SELF-Transformer通过内部迭代恢复表达能力，同时保持编码器架构的简洁性。

Abstract: Transformers evaluated in a single, fixed-depth pass are provably limited in
expressive power to the constant-depth circuit class TC0. Running a Transformer
autoregressively removes that ceiling -- first in next-token prediction and,
more recently, in chain-of-thought reasoning. Both regimes rely on feedback
loops that decode internal states into tokens only to re-encode them in
subsequent steps. While this "thinking aloud" mirrors human reasoning,
biological brains iterate without externalising intermediate states as
language. To boost the expressive power of encoder Transformers without
resorting to token-level autoregression, we introduce the SELF-Transformer: an
encoder layer that iteratively refines its own attention weights to a fixed
point. Instead of producing -- in one pass -- the alignment matrix that remixes
the input sequence, the SELF-Transformer iteratively updates that matrix
internally, scaling test-time computation with input difficulty. This
adaptivity yields up to 20\% accuracy gains on encoder-style benchmarks without
increasing parameter count, demonstrating that input-adaptive alignment at test
time offers substantial benefits for only a modest extra compute budget.
Self-Transformers thus recover much of the expressive power of iterative
reasoning while preserving the simplicity of pure encoder architectures.

</details>


### [73] [Apple Intelligence Foundation Language Models: Tech Report 2025](https://arxiv.org/abs/2507.13575)
*Hanzhi Zhou,Erik Hornberger,Pengsheng Guo,Xiyou Zhou,Saiwen Wang,Xin Wang,Yifei He,Xuankai Chang,Rene Rauch,Louis D'hauwe,John Peebles,Alec Doane,Kohen Chia,Jenna Thibodeau,Zi-Yi Dou,Yuanyang Zhang,Ruoming Pang,Reed Li,Zhifeng Chen,Jeremy Warner,Zhaoyang Xu,Sophy Lee,David Mizrahi,Ramsey Tantawi,Chris Chaney,Kelsey Peterson,Jun Qin,Alex Dombrowski,Mira Chiang,Aiswarya Raghavan,Gerard Casamayor,Qibin Chen,Aonan Zhang,Nathalie Tran,Jianyu Wang,Hang Su,Thomas Voice,Alessandro Pappalardo,Brycen Wershing,Prasanth Yadla,Rui Li,Priyal Chhatrapati,Ismael Fernandez,Yusuf Goren,Xin Zheng,Forrest Huang,Tao Lei,Eray Yildiz,Alper Kokmen,Gokul Santhanam,Areeba Kamal,Kaan Elgin,Dian Ang Yap,Jeremy Liu,Peter Gray,Howard Xing,Kieran Liu,Matteo Ronchi,Moritz Schwarzer-Becker,Yun Zhu,Mandana Saebi,Jeremy Snow,David Griffiths,Guillaume Tartavel,Erin Feldman,Simon Lehnerer,Fernando Bermúdez-Medina,Hans Han,Joe Zhou,Xiaoyi Ren,Sujeeth Reddy,Zirui Wang,Tom Gunter,Albert Antony,Yuanzhi Li,John Dennison,Tony Sun,Yena Han,Yi Qin,Sam Davarnia,Jeffrey Bigham,Wayne Shan,Hannah Gillis Coleman,Guillaume Klein,Peng Liu,Muyang Yu,Jack Cackler,Yuan Gao,Crystal Xiao,Binazir Karimzadeh,Zhengdong Zhang,Felix Bai,Albin Madappally Jose,Feng Nan,Nazir Kamaldin,Dong Yin,Hans Hao,Yanchao Sun,Yi Hua,Charles Maalouf,Alex Guillen Garcia,Guoli Yin,Lezhi Li,Mohana Prasad Sathya Moorthy,Hongbin Gao,Jay Tang,Joanna Arreaza-Taylor,Faye Lao,Carina Peng,Josh Shaffer,Dan Masi,Sushma Rao,Tommi Vehvilainen,Senyu Tong,Dongcai Shen,Yang Zhao,Chris Bartels,Peter Fu,Qingqing Cao,Christopher Neubauer,Ethan Li,Mingfei Gao,Rebecca Callahan,Richard Wei,Patrick Dong,Alex Braunstein,Sachin Ravi,Adolfo Lopez Mendez,Kaiwei Huang,Kun Duan,Haoshuo Huang,Rui Qian,Stefano Ligas,Jordan Huffaker,Dongxu Li,Bailin Wang,Nanzhu Wang,Anuva Agarwal,Tait Madsen,Josh Newnham,Abhishek Sharma,Zhile Ren,Deepak Gopinath,Erik Daxberger,Saptarshi Guha,Oron Levy,Jing Lu,Nan Dun,Marc Kirchner,Yinfei Yang,Manjot Bilkhu,Dave Nelson,Anthony Spalvieri-Kruse,Juan Lao Tebar,Yang Xu,Phani Mutyala,Gabriel Jacoby-Cooper,Yingbo Wang,Karla Vega,Vishaal Mahtani,Darren Botten,Eric Wang,Hanli Li,Matthias Paulik,Haoran Yan,Navid Shiee,Yihao Qian,Bugu Wu,Qi Zhu,Ob Adaranijo,Bhuwan Dhingra,Zhe Gan,Nicholas Seidl,Grace Duanmu,Rong Situ,Yiping Ma,Yin Xia,David Riazati,Vasileios Saveris,Anh Nguyen,Michael,Lee,Patrick Sonnenberg,Chinguun Erdenebileg,Yanghao Li,Vivian Ma,James Chou,Isha Garg,Mark Lee,Keen You,Yuhong Li,Ransen Niu,Nandhitha Raghuram,Pulkit Agrawal,Henry Mason,Sumeet Singh,Keyu He,Hong-You Chen,Lucas Guibert,Shiyu Li,Varsha Paidi,Narendran Raghavan,Mingze Xu,Yuli Yang,Sergiu Sima,Irina Belousova,Sprite Chu,Afshin Dehghan,Philipp Dufter,David Haldimann,Zhen Yang,Margit Bowler,Chang Liu,Ying-Chang Cheng,Vivek Rathod,Syd Evans,Wilson Tsao,Dustin Withers,Haitian Sun,Biyao Wang,Peter Grasch,Walker Cheng,Yihao Feng,Vivek Kumar,Frank Chu,Victoria MönchJuan Haladjian,Doug Kang,Jiarui Lu,Ciro Sannino,Max Lam,Floris Weers,Bowen Pan,Kenneth Jung,Dhaval Doshi,Fangping Shi,Olli Saarikivi,Alp Aygar,Josh Elman,Cheng Leong,Eshan Verma,Matthew Lei,Jeff Nichols,Jiulong Shan,Donald Zhang,Lawrence Zhou,Stephen Murphy,Xianzhi Du,Chang Lan,Ankur Jain,Elmira Amirloo,Marcin Eichner,Naomy Sabo,Anupama Mann Anupama,David Qiu,Zhao Meng,Michael FitzMaurice,Peng Zhang,Simon Yeung,Chen Chen,Marco Zuliani,Andrew Hansen,Yang Lu,Brent Ramerth,Ziyi Zhong,Parsa Mazaheri,Matthew Hopkins,Mengyu Li,Simon Wang,David Chen,Farzin Rasteh,Chong Wang,Josh Gardner,Asaf Liberman,Haoxuan You,Andrew Walkingshaw,Xingyu Zhou,Jinhao Lei,Yan Meng,Quentin Keunebroek,Sam Wiseman,Anders Boesen Lindbo Larsen,Yi Zhang,Zaid Ahmed,Haiming Gang,Aaron Franklin,Kelvin Zou,Guillaume Seguin,Jonathan Janke,Rachel Burger,Co Giang,Cheng Shen,Jen Liu,Sanskruti Shah,Xiang Kong,Yiran Fei,TJ Collins,Chen Zhang,Zhiyun Lu,Michael Booker,Qin Ba,Yasutaka Tanaka,Andres Romero Mier Y Teran,Federico Scozzafava,Regan Poston,Jane Li,Eduardo Jimenez,Bas Straathof,Karanjeet Singh,Lindsay Hislop,Rajat Arora,Deepa Seshadri,Boyue Li,Colorado Reed,Zhen Li,TJ Lu,Yi Wang,Kaelen Haag,Nicholas Lusskin,Raunak Sinha,Rahul Nair,Eldon Schoop,Mary Beth Kery,Mehrdad Farajtbar,Brenda Yang,George Horrell,Shiwen Zhao,Dhruti Shah,Cha Chen,Bowen Zhang,Chang Gao,Devi Krishna,Jennifer Mallalieu,Javier Movellan,Di Feng,Emily Zhang,Sam Xu,Junting Pan,Dominik Moritz,Suma Jayaram,Kevin Smith,Dongseong Hwang,Daniel Parilla,Jiaming Hu,You-Cyuan Jhang,Emad Soroush,Fred Hohman,Nan Du,Emma Wang,Sam Dodge,Pragnya Sridhar,Joris Pelemans,Wei Fang,Nina Wenzel,Joseph Yitan Cheng,Hadas Kotek,Chung-Cheng Chiu,Meng Cao,Haijing Fu,Ruixuan Hou,Ke Ye,Diane Zhu,Nikhil Bhendawade,Joseph Astrauskas,Jian Liu,Sai Aitharaju,Wentao Wu,Artsiom Peshko,Hyunjik Kim,Nilesh Shahdadpuri,Andy De Wang,Qi Shan,Piotr Maj,Raul Rea Menacho,Justin Lazarow,Eric Liang Yang,Arsalan Farooq,Donghan Yu,David Güera,Minsik Cho,Kavya Nerella,Yongqiang Wang,Tao Jia,John Park,Jeff Lai,Haotian Zhang,Futang Peng,Daniele Molinari,Aparna Rajamani,Tyler Johnson,Lauren Gardiner,Chao Jia,Violet Yao,Wojciech Kryscinski,Xiujun Li,Shang-Chen Wu*

Main category: cs.LG

TL;DR: 苹果推出了两款多语言、多模态的基础语言模型，分别用于设备端和服务器端，支持多语言和图像理解，性能优于同类开源模型。


<details>
  <summary>Details</summary>
Motivation: 为苹果设备和服务提供智能功能，同时兼顾性能、成本、隐私和负责任的人工智能开发。

Method: 设备端模型采用KV缓存共享和2位量化感知训练；服务器端模型采用并行轨道混合专家（PT-MoE）架构。两者均通过大规模多语言和多模态数据集训练，并结合监督微调和强化学习。

Result: 在公开基准和人工评估中，两款模型均达到或超越同类开源模型。

Conclusion: 苹果通过技术创新和负责任的人工智能实践，提供了高性能、隐私保护的智能模型，并支持开发者轻松集成。

Abstract: We introduce two multilingual, multimodal foundation language models that
power Apple Intelligence features across Apple devices and services: i a
3B-parameter on-device model optimized for Apple silicon through architectural
innovations such as KV-cache sharing and 2-bit quantization-aware training; and
ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts
PT-MoE transformer that combines track parallelism, mixture-of-experts sparse
computation, and interleaved global-local attention to deliver high quality
with competitive cost on Apple's Private Cloud Compute platform. Both models
are trained on large-scale multilingual and multimodal datasets sourced via
responsible web crawling, licensed corpora, and high-quality synthetic data,
then further refined with supervised fine-tuning and reinforcement learning on
a new asynchronous platform. The resulting models support several additional
languages while understanding images and executing tool calls. In public
benchmarks and human evaluations, both the server model and the on-device model
match or surpass comparably sized open baselines.
  A new Swift-centric Foundation Models framework exposes guided generation,
constrained tool calling, and LoRA adapter fine-tuning, allowing developers to
integrate these capabilities with a few lines of code. The latest advancements
in Apple Intelligence models are grounded in our Responsible AI approach with
safeguards like content filtering and locale-specific evaluation, as well as
our commitment to protecting our users' privacy with innovations like Private
Cloud Compute.

</details>


### [74] [Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries](https://arxiv.org/abs/2507.13579)
*Hyunji Nam,Yanming Wan,Mickel Liu,Jianxun Lian,Natasha Jaques*

Main category: cs.LG

TL;DR: PLUS框架通过生成用户偏好摘要，实现个性化奖励模型，提升LLM响应的个性化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法无法捕捉用户多样性，需开发个性化响应技术。

Method: 提出PLUS框架，利用摘要学习用户偏好，结合强化学习更新奖励模型。

Result: PLUS生成的摘要能有效捕捉用户偏好，适应新用户和多样话题，并可零迁移至GPT-4。

Conclusion: PLUS提供透明、可控的个性化LLM对齐方案。

Abstract: As everyday use cases of large language model (LLM) AI assistants have
expanded, it is becoming increasingly important to personalize responses to
align to different users' preferences and goals. While reinforcement learning
from human feedback (RLHF) is effective at improving LLMs to be generally more
helpful and fluent, it does not account for variability across users, as it
models the entire user population with a single reward model. We present a
novel framework, Preference Learning Using Summarization (PLUS), that learns
text-based summaries of each user's preferences, characteristics, and past
conversations. These summaries condition the reward model, enabling it to make
personalized predictions about the types of responses valued by each user. We
train the user-summarization model with reinforcement learning, and update the
reward model simultaneously, creating an online co-adaptation loop. We show
that in contrast with prior personalized RLHF techniques or with in-context
learning of user information, summaries produced by PLUS capture meaningful
aspects of a user's preferences. Across different pluralistic user datasets, we
show that our method is robust to new users and diverse conversation topics.
Additionally, we demonstrate that the textual summaries generated about users
can be transferred for zero-shot personalization of stronger, proprietary
models like GPT-4. The resulting user summaries are not only concise and
portable, they are easy for users to interpret and modify, allowing for more
transparency and user control in LLM alignment.

</details>


### [75] [Off-Policy Evaluation and Learning for Matching Markets](https://arxiv.org/abs/2507.13608)
*Yudai Hayashi,Shuhei Goda,Yuta Saito*

Main category: cs.LG

TL;DR: 论文提出两种新型离线策略评估（OPE）方法DiPS和DPR，专为匹配市场设计，通过结合直接方法、逆倾向得分和双重稳健估计器，优化偏差-方差控制，并在实验和理论分析中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 匹配市场中的双向用户互动和大规模特性导致传统OPE方法不可靠，需要更有效的离线评估方法。

Method: 提出DiPS和DPR方法，结合DM、IPS和DR估计器，并利用初始参与信号等中间标签优化评估。

Result: 理论分析和实验表明，DiPS和DPR在偏差-方差控制和匹配效果上优于传统方法。

Conclusion: 新方法为匹配市场的离线策略评估和学习提供了高效工具，适用于实际平台。

Abstract: Matching users based on mutual preferences is a fundamental aspect of
services driven by reciprocal recommendations, such as job search and dating
applications. Although A/B tests remain the gold standard for evaluating new
policies in recommender systems for matching markets, it is costly and
impractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays
a crucial role by enabling the evaluation of recommendation policies using only
offline logged data naturally collected on the platform. However, unlike
conventional recommendation settings, the large scale and bidirectional nature
of user interactions in matching platforms introduce variance issues and
exacerbate reward sparsity, making standard OPE methods unreliable. To address
these challenges and facilitate effective offline evaluation, we propose novel
OPE estimators, \textit{DiPS} and \textit{DPR}, specifically designed for
matching markets. Our methods combine elements of the Direct Method (DM),
Inverse Propensity Score (IPS), and Doubly Robust (DR) estimators while
incorporating intermediate labels, such as initial engagement signals, to
achieve better bias-variance control in matching markets. Theoretically, we
derive the bias and variance of the proposed estimators and demonstrate their
advantages over conventional methods. Furthermore, we show that these
estimators can be seamlessly extended to offline policy learning methods for
improving recommendation policies for making more matches. We empirically
evaluate our methods through experiments on both synthetic data and A/B testing
logs from a real job-matching platform. The empirical results highlight the
superiority of our approach over existing methods in off-policy evaluation and
learning tasks for a variety of configurations.

</details>


### [76] [Tri-Learn Graph Fusion Network for Attributed Graph Clustering](https://arxiv.org/abs/2507.13620)
*Binxiong Li,Yuefei Wang,Xu Xiang,Xue Li,Binyu Zhao,Heyang Gao,Qinyu Zhao,Xi Yu*

Main category: cs.LG

TL;DR: 提出了一种结合GCN、AE和Graph Transformer的Tri-GFN框架，通过三学习机制和特征融合策略提升图聚类性能。


<details>
  <summary>Details</summary>
Motivation: 解决GCN在处理大规模复杂图数据时的过平滑和过压缩问题，以及Graph Transformer在异构图数据上的性能限制。

Method: 结合GCN、AE和Graph Transformer，通过三学习机制和特征融合增强策略整合全局与局部信息。

Result: 在ACM、Reuters和USPS数据集上分别提升0.87%、14.14%和7.58%的准确率。

Conclusion: Tri-GFN在异构图数据上表现优异，适用于新闻分类和主题检索等领域。

Abstract: In recent years, models based on Graph Convolutional Networks (GCN) have made
significant strides in the field of graph data analysis. However, challenges
such as over-smoothing and over-compression remain when handling large-scale
and complex graph datasets, leading to a decline in clustering quality.
Although the Graph Transformer architecture has mitigated some of these issues,
its performance is still limited when processing heterogeneous graph data. To
address these challenges, this study proposes a novel deep clustering framework
that comprising GCN, Autoencoder (AE), and Graph Transformer, termed the
Tri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the
differentiation and consistency of global and local information through a
unique tri-learning mechanism and feature fusion enhancement strategy. The
framework integrates GCN, AE, and Graph Transformer modules. These components
are meticulously fused by a triple-channel enhancement module, which maximizes
the use of both node attributes and topological structures, ensuring robust
clustering representation. The tri-learning mechanism allows mutual learning
among these modules, while the feature fusion strategy enables the model to
capture complex relationships, yielding highly discriminative representations
for graph clustering. It surpasses many state-of-the-art methods, achieving an
accuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the
Reuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding
performance on the Reuters dataset, Tri-GFN can be applied to automatic news
classification, topic retrieval, and related fields.

</details>


### [77] [FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning](https://arxiv.org/abs/2507.13624)
*Daniel Commey,Kamel Abbad,Garth V. Crosby,Lyes Khoukhi*

Main category: cs.LG

TL;DR: FedSkipTwin通过轻量级服务器端数字孪生预测客户端梯度更新，动态跳过通信轮次，减少带宽消耗并提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通信开销是主要瓶颈，尤其在移动和物联网设备带宽受限的场景下。

Method: 使用LSTM实现的数字孪生预测客户端梯度更新的幅度和认知不确定性，动态决定是否跳过通信轮次。

Result: 在非独立同分布数据下，FedSkipTwin减少12-15.5%通信量，同时模型精度提升0.5个百分点。

Conclusion: 预测引导的跳过策略是带宽受限边缘环境中资源感知联邦学习的实用有效方法。

Abstract: Communication overhead remains a primary bottleneck in federated learning
(FL), particularly for applications involving mobile and IoT devices with
constrained bandwidth. This work introduces FedSkipTwin, a novel
client-skipping algorithm driven by lightweight, server-side digital twins.
Each twin, implemented as a simple LSTM, observes a client's historical
sequence of gradient norms to forecast both the magnitude and the epistemic
uncertainty of its next update. The server leverages these predictions,
requesting communication only when either value exceeds a predefined threshold;
otherwise, it instructs the client to skip the round, thereby saving bandwidth.
Experiments are conducted on the UCI-HAR and MNIST datasets with 10 clients
under a non-IID data distribution. The results demonstrate that FedSkipTwin
reduces total communication by 12-15.5% across 20 rounds while simultaneously
improving final model accuracy by up to 0.5 percentage points compared to the
standard FedAvg algorithm. These findings establish that prediction-guided
skipping is a practical and effective strategy for resource-aware FL in
bandwidth-constrained edge environments.

</details>


### [78] [A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design](https://arxiv.org/abs/2507.13646)
*Nimisha Ghosh,Daniele Santoni,Debaleena Nawn,Eleonora Ottaviani,Giovanni Felici*

Main category: cs.LG

TL;DR: 本文综述了基于Transformer的语言模型在蛋白质序列分析与设计中的最新进展，包括基因本体、功能与结构蛋白识别、蛋白质生成与结合等应用，并探讨了现有研究的不足与未来发展方向。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理中的成功启发了其在生物信息学中的应用，本文旨在总结和评估这些模型在蛋白质研究中的进展。

Method: 综述并分析了大量关于Transformer模型在蛋白质序列分析中的应用研究，涵盖多个具体任务。

Result: 总结了现有研究的优缺点，为读者提供了全面的见解，并指出了当前研究的不足。

Conclusion: 本文为相关领域的研究者提供了当前研究现状的概览，并指明了未来可能的研究方向。

Abstract: The impact of Transformer-based language models has been unprecedented in
Natural Language Processing (NLP). The success of such models has also led to
their adoption in other fields including bioinformatics. Taking this into
account, this paper discusses recent advances in Transformer-based models for
protein sequence analysis and design. In this review, we have discussed and
analysed a significant number of works pertaining to such applications. These
applications encompass gene ontology, functional and structural protein
identification, generation of de novo proteins and binding of proteins. We
attempt to shed light on the strength and weaknesses of the discussed works to
provide a comprehensive insight to readers. Finally, we highlight shortcomings
in existing research and explore potential avenues for future developments. We
believe that this review will help researchers working in this field to have an
overall idea of the state of the art in this field, and to orient their future
studies.

</details>


### [79] [Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction](https://arxiv.org/abs/2507.13685)
*Yue Yang,Zihan Su,Ying Zhang,Chang Chuan Goh,Yuxiang Lin,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 论文提出两种新架构GRU-KAN和LSTM-KAN，用于提前预测贷款违约，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有贷款违约预测模型在提前预测时准确性不足和依赖特定时间框架的问题。

Method: 结合Kolmogorov-Arnold Networks (KAN)与GRU和LSTM网络，提出GRU-KAN和LSTM-KAN架构。

Result: 新模型在提前三个月预测时准确率超过92%，八个月时超过88%，显著优于基线模型。

Conclusion: GRU-KAN和LSTM-KAN架构在贷款违约早期预测中表现优异，具有实际应用价值。

Abstract: This study addresses a critical challenge in time series anomaly detection:
enhancing the predictive capability of loan default models more than three
months in advance to enable early identification of default events, helping
financial institutions implement preventive measures before risk events
materialize. Existing methods have significant drawbacks, such as their lack of
accuracy in early predictions and their dependence on training and testing
within the same year and specific time frames. These issues limit their
practical use, particularly with out-of-time data. To address these, the study
introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge
Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long
Short-Term Memory (LSTM) networks. The proposed models were evaluated against
the baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms
of accuracy, precision, recall, F1 and AUC in different lengths of feature
window, sample sizes, and early prediction intervals. The results demonstrate
that the proposed model achieves a prediction accuracy of over 92% three months
in advance and over 88% eight months in advance, significantly outperforming
existing baselines.

</details>


### [80] [Binarizing Physics-Inspired GNNs for Combinatorial Optimization](https://arxiv.org/abs/2507.13703)
*Martin Krutský,Gustav Šír,Vyacheslav Kungurtsev,Georgios Korpas*

Main category: cs.LG

TL;DR: PI-GNNs在组合优化问题中表现良好，但随着问题图密度增加，性能下降。研究发现训练动态中存在相变，并提出基于模糊逻辑和二值化神经网络的改进方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决PI-GNNs在处理高密度组合优化问题时性能下降的问题。

Method: 提出基于模糊逻辑和二值化神经网络的改进策略。

Result: 改进方法显著提升了PI-GNNs在高密度问题中的性能。

Conclusion: 通过新方法，PI-GNNs在高密度组合优化问题中的性能得到有效提升。

Abstract: Physics-inspired graph neural networks (PI-GNNs) have been utilized as an
efficient unsupervised framework for relaxing combinatorial optimization
problems encoded through a specific graph structure and loss, reflecting
dependencies between the problem's variables. While the framework has yielded
promising results in various combinatorial problems, we show that the
performance of PI-GNNs systematically plummets with an increasing density of
the combinatorial problem graphs. Our analysis reveals an interesting phase
transition in the PI-GNNs' training dynamics, associated with degenerate
solutions for the denser problems, highlighting a discrepancy between the
relaxed, real-valued model outputs and the binary-valued problem solutions. To
address the discrepancy, we propose principled alternatives to the naive
strategy used in PI-GNNs by building on insights from fuzzy logic and binarized
neural networks. Our experiments demonstrate that the portfolio of proposed
methods significantly improves the performance of PI-GNNs in increasingly dense
settings.

</details>


### [81] [Bayesian Optimization for Molecules Should Be Pareto-Aware](https://arxiv.org/abs/2507.13704)
*Anabel Yong,Austin Tripp,Layla Hosseini-Gerami,Brooks Paige*

Main category: cs.LG

TL;DR: 多目标贝叶斯优化（MOBO）在分子设计中优于标量化方法，特别是在低数据情况下。


<details>
  <summary>Details</summary>
Motivation: 探索MOBO在分子设计中的实际优势，尤其是与标量化方法相比。

Method: 使用基于帕累托的MOBO策略（EHVI）与固定权重标量化基线（EI）进行对比，控制实验条件一致。

Result: EHVI在帕累托前沿覆盖、收敛速度和化学多样性上均优于EI，尤其在低数据情况下。

Conclusion: Pareto感知的获取策略在分子优化中具有实际优势，特别是在预算有限且权衡复杂时。

Abstract: Multi-objective Bayesian optimization (MOBO) provides a principled framework
for navigating trade-offs in molecular design. However, its empirical
advantages over scalarized alternatives remain underexplored. We benchmark a
simple Pareto-based MOBO strategy -- Expected Hypervolume Improvement (EHVI) --
against a simple fixed-weight scalarized baseline using Expected Improvement
(EI), under a tightly controlled setup with identical Gaussian Process
surrogates and molecular representations. Across three molecular optimization
tasks, EHVI consistently outperforms scalarized EI in terms of Pareto front
coverage, convergence speed, and chemical diversity. While scalarization
encompasses flexible variants -- including random or adaptive schemes -- our
results show that even strong deterministic instantiations can underperform in
low-data regimes. These findings offer concrete evidence for the practical
advantages of Pareto-aware acquisition in de novo molecular optimization,
especially when evaluation budgets are limited and trade-offs are nontrivial.

</details>


### [82] [Learning Deformable Body Interactions With Adaptive Spatial Tokenization](https://arxiv.org/abs/2507.13707)
*Hao Wang,Yu Liu,Daniel Biggs,Haoru Wang,Jiandong Yu,Ping Huang*

Main category: cs.LG

TL;DR: 提出了一种自适应空间标记化（AST）方法，通过网格划分和交叉注意力模块高效模拟可变形体交互，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 学习基于图神经网络（GNN）的方法在模拟可变形体交互时存在计算量大、难以扩展的问题，需要一种更高效的方法。

Method: 将模拟空间划分为网格单元，映射非结构化网格到结构化网格，利用交叉注意力模块生成紧凑的固定长度嵌入，并通过自注意力模块预测下一状态。

Result: 实验表明，该方法在模拟大规模网格（超过10万个节点）时仍保持高效，显著优于现有方法。

Conclusion: AST方法结合了标记化的高效性和注意力机制的表达能力，为可变形体交互模拟提供了可扩展的解决方案。

Abstract: Simulating interactions between deformable bodies is vital in fields like
material science, mechanical design, and robotics. While learning-based methods
with Graph Neural Networks (GNNs) are effective at solving complex physical
systems, they encounter scalability issues when modeling deformable body
interactions. To model interactions between objects, pairwise global edges have
to be created dynamically, which is computationally intensive and impractical
for large-scale meshes. To overcome these challenges, drawing on insights from
geometric representations, we propose an Adaptive Spatial Tokenization (AST)
method for efficient representation of physical states. By dividing the
simulation space into a grid of cells and mapping unstructured meshes onto this
structured grid, our approach naturally groups adjacent mesh nodes. We then
apply a cross-attention module to map the sparse cells into a compact,
fixed-length embedding, serving as tokens for the entire physical state.
Self-attention modules are employed to predict the next state over these tokens
in latent space. This framework leverages the efficiency of tokenization and
the expressive power of attention mechanisms to achieve accurate and scalable
simulation results. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches in modeling deformable
body interactions. Notably, it remains effective on large-scale simulations
with meshes exceeding 100,000 nodes, where existing methods are hindered by
computational limitations. Additionally, we contribute a novel large-scale
dataset encompassing a wide range of deformable body interactions to support
future research in this area.

</details>


### [83] [Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods](https://arxiv.org/abs/2507.13716)
*Danilo Avola,Andrea Bernardini,Giancarlo Crocetti,Andrea Ladogana,Mario Lezoche,Maurizio Mancini,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 该研究系统比较了传统机器学习和深度学习模型在帕金森病分类中的表现，发现CNN-LSTM模型表现最佳，同时XGBoost等传统分类器也具竞争力。


<details>
  <summary>Details</summary>
Motivation: 帕金森病（PD）的早期诊断对临床干预至关重要，脑电图（EEG）提供了一种非侵入性且经济高效的检测手段，但开发可靠的自动化诊断模型仍具挑战。

Method: 研究采用统一的七步预处理流程，并应用一致的交叉验证和评估标准，比较了传统ML和DL模型在公开数据集上的表现。

Result: CNN-LSTM模型表现最佳，强调了捕捉长期时间依赖的重要性；XGBoost等传统分类器也表现出色。

Conclusion: 研究为未来开发更复杂或专用架构提供了参考框架，强调了基线结果对科学严谨性和可重复性的重要性。

Abstract: Parkinson's Disease PD is a progressive neurodegenerative disorder that
affects motor and cognitive functions with early diagnosis being critical for
effective clinical intervention Electroencephalography EEG offers a noninvasive
and costeffective means of detecting PDrelated neural alterations yet the
development of reliable automated diagnostic models remains a challenge In this
study we conduct a systematic benchmark of traditional machine learning ML and
deep learning DL models for classifying PD using a publicly available oddball
task dataset Our aim is to lay the groundwork for developing an effective
learning system and to determine which approach produces the best results We
implement a unified sevenstep preprocessing pipeline and apply consistent
subjectwise crossvalidation and evaluation criteria to ensure comparability
across models Our results demonstrate that while baseline deep learning
architectures particularly CNNLSTM models achieve the best performance compared
to other deep learning architectures underlining the importance of capturing
longrange temporal dependencies several traditional classifiers such as XGBoost
also offer strong predictive accuracy and calibrated decision boundaries By
rigorously comparing these baselines our work provides a solid reference
framework for future studies aiming to develop and evaluate more complex or
specialized architectures Establishing a reliable set of baseline results is
essential to contextualize improvements introduced by novel methods ensuring
scientific rigor and reproducibility in the evolving field of EEGbased
neurodiagnostics

</details>


### [84] [Bi-GRU Based Deception Detection using EEG Signals](https://arxiv.org/abs/2507.13718)
*Danilo Avola,Muhammad Yasir Bilal,Emad Emam,Cristina Lakasz,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 该研究提出了一种基于深度学习的双向门控循环单元（Bi-GRU）神经网络方法，利用脑电图（EEG）信号检测欺骗行为，在Bag-of-Lies数据集上实现了97%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 欺骗检测在安全、心理学和法医学等领域具有重要意义，但传统方法面临挑战。

Method: 使用Bi-GRU神经网络对EEG信号进行二分类，以区分欺骗和真实行为。

Result: 模型在测试集上达到97%的准确率，并表现出高精度、召回率和F1分数。

Conclusion: 双向时间建模在EEG欺骗检测中效果显著，具有实时应用潜力，未来可探索更先进的神经网络架构。

Abstract: Deception detection is a significant challenge in fields such as security,
psychology, and forensics. This study presents a deep learning approach for
classifying deceptive and truthful behavior using ElectroEncephaloGram (EEG)
signals from the Bag-of-Lies dataset, a multimodal corpus designed for
naturalistic, casual deception scenarios. A Bidirectional Gated Recurrent Unit
(Bi-GRU) neural network was trained to perform binary classification of EEG
samples. The model achieved a test accuracy of 97\%, along with high precision,
recall, and F1-scores across both classes. These results demonstrate the
effectiveness of using bidirectional temporal modeling for EEG-based deception
detection and suggest potential for real-time applications and future
exploration of advanced neural architectures.

</details>


### [85] [Graph-Structured Data Analysis of Component Failure in Autonomous Cargo Ships Based on Feature Fusion](https://arxiv.org/abs/2507.13721)
*Zizhao Zhang,Tianxiang Zhao,Yu Sun,Liping Sun,Jichuan Kang*

Main category: cs.LG

TL;DR: 本文提出了一种混合特征融合框架，用于构建自主货船故障模式的图结构数据集，显著提升了文献检索效率和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 解决自主货船组件故障引发的级联反应和应急决策中的不确定性。

Method: 采用改进的布谷鸟搜索算法（HN-CSA）提升检索效率，构建分层特征融合框架，结合Word2Vec、BERT-KPCA和Sentence-BERT处理特征。

Result: 数据集覆盖12个系统、1,262种故障模式和6,150条传播路径，GATE-GNN模型分类准确率为0.735，预测F1分数达0.93。

Conclusion: 为自主货船的故障分析、风险评估和智能决策系统提供了可靠支持。

Abstract: To address the challenges posed by cascading reactions caused by component
failures in autonomous cargo ships (ACS) and the uncertainties in emergency
decision-making, this paper proposes a novel hybrid feature fusion framework
for constructing a graph-structured dataset of failure modes. By employing an
improved cuckoo search algorithm (HN-CSA), the literature retrieval efficiency
is significantly enhanced, achieving improvements of 7.1% and 3.4% compared to
the NSGA-II and CSA search algorithms, respectively. A hierarchical feature
fusion framework is constructed, using Word2Vec encoding to encode
subsystem/component features, BERT-KPCA to process failure modes/reasons, and
Sentence-BERT to quantify the semantic association between failure impact and
emergency decision-making. The dataset covers 12 systems, 1,262 failure modes,
and 6,150 propagation paths. Validation results show that the GATE-GNN model
achieves a classification accuracy of 0.735, comparable to existing benchmarks.
Additionally, a silhouette coefficient of 0.641 indicates that the features are
highly distinguishable. In the label prediction results, the Shore-based
Meteorological Service System achieved an F1 score of 0.93, demonstrating high
prediction accuracy. This paper not only provides a solid foundation for
failure analysis in autonomous cargo ships but also offers reliable support for
fault diagnosis, risk assessment, and intelligent decision-making systems. The
link to the dataset is
https://github.com/wojiufukele/Graph-Structured-about-CSA.

</details>


### [86] [Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics](https://arxiv.org/abs/2507.13727)
*René Heinrich,Lukas Rauch,Bernhard Sick,Christoph Scholz*

Main category: cs.LG

TL;DR: 研究探讨了对抗训练在音频分类中对泛化性能和对抗鲁棒性的影响，发现输出空间攻击策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 对抗训练在音频分类中对数据分布变化的泛化能力尚未充分研究，本文旨在填补这一空白。

Method: 采用两种对抗训练策略（输出空间攻击和嵌入空间攻击），并在两种模型（ConvNeXt和AudioProtoPNet）上进行评估。

Result: 输出空间攻击策略使干净测试数据性能平均提升10.5%，同时增强了模型的对抗鲁棒性。

Conclusion: 对抗训练在音频分类中具有提升对抗鲁棒性和应对数据分布变化的潜力。

Abstract: Adversarial training is a promising strategy for enhancing model robustness
against adversarial attacks. However, its impact on generalization under
substantial data distribution shifts in audio classification remains largely
unexplored. To address this gap, this work investigates how different
adversarial training strategies improve generalization performance and
adversarial robustness in audio classification. The study focuses on two model
architectures: a conventional convolutional neural network (ConvNeXt) and an
inherently interpretable prototype-based model (AudioProtoPNet). The approach
is evaluated using a challenging bird sound classification benchmark. This
benchmark is characterized by pronounced distribution shifts between training
and test data due to varying environmental conditions and recording methods, a
common real-world challenge. The investigation explores two adversarial
training strategies: one based on output-space attacks that maximize the
classification loss function, and another based on embedding-space attacks
designed to maximize embedding dissimilarity. These attack types are also used
for robustness evaluation. Additionally, for AudioProtoPNet, the study assesses
the stability of its learned prototypes under targeted embedding-space attacks.
Results show that adversarial training, particularly using output-space
attacks, improves clean test data performance by an average of 10.5% relative
and simultaneously strengthens the adversarial robustness of the models. These
findings, although derived from the bird sound domain, suggest that adversarial
training holds potential to enhance robustness against both strong distribution
shifts and adversarial attacks in challenging audio classification settings.

</details>


### [87] [An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC](https://arxiv.org/abs/2507.13736)
*Matthias Jobst,Tim Langer,Chen Liu,Mehmet Alici,Hector A. Gonzalez,Christian Mayr*

Main category: cs.LG

TL;DR: 提出了一种基于OctopuScheduler的多层DNN调度框架，支持从PyTorch模型到SpiNNaker2芯片的端到端推理流程。


<details>
  <summary>Details</summary>
Motivation: 解决在神经形态平台SpiNNaker2上执行大规模复杂DNN（如Transformer）的边缘计算需求。

Method: 结合量化和降级步骤的前端，扩展OctopuScheduler为多层DNN调度框架。

Result: 实现了在SpiNNaker2芯片上高效执行复杂DNN的能力。

Conclusion: 该框架为神经形态硬件上的大规模DNN推理提供了可行的解决方案。

Abstract: This work presents a multi-layer DNN scheduling framework as an extension of
OctopuScheduler, providing an end-to-end flow from PyTorch models to inference
on a single SpiNNaker2 chip. Together with a front-end comprised of
quantization and lowering steps, the proposed framework enables the edge-based
execution of large and complex DNNs up to transformer scale using the
neuromorphic platform SpiNNaker2.

</details>


### [88] [SamGoG: A Sampling-Based Graph-of-Graphs Framework for Imbalanced Graph Classification](https://arxiv.org/abs/2507.13741)
*Shangyou Wang,Zezhong Ding,Xike Xie*

Main category: cs.LG

TL;DR: SamGoG是一种基于采样的图学习框架，有效解决图分类任务中的类别和大小不平衡问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实图中的类别和大小不平衡会偏置学习过程并降低模型性能，现有方法通常只解决一种不平衡或计算成本高。

Method: SamGoG通过重要性采样机制构建多个图，并利用可学习的相似性和自适应节点度增强边同质性。

Result: 在基准数据集上，SamGoG实现了15.66%的准确率提升和6.7倍的训练加速。

Conclusion: SamGoG能有效缓解不平衡问题，提升图分类任务的性能，且兼容多种下游GNN。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success in graph
classification tasks by capturing both structural and feature-based
representations. However, real-world graphs often exhibit two critical forms of
imbalance: class imbalance and graph size imbalance. These imbalances can bias
the learning process and degrade model performance. Existing methods typically
address only one type of imbalance or incur high computational costs. In this
work, we propose SamGoG, a sampling-based Graph-of-Graphs (GoG) learning
framework that effectively mitigates both class and graph size imbalance.
SamGoG constructs multiple GoGs through an efficient importance-based sampling
mechanism and trains on them sequentially. This sampling mechanism incorporates
the learnable pairwise similarity and adaptive GoG node degree to enhance edge
homophily, thus improving downstream model quality. SamGoG can seamlessly
integrate with various downstream GNNs, enabling their efficient adaptation for
graph classification tasks. Extensive experiments on benchmark datasets
demonstrate that SamGoG achieves state-of-the-art performance with up to a
15.66% accuracy improvement with 6.7$\times$ training acceleration.

</details>


### [89] [Search-Optimized Quantization in Biomedical Ontology Alignment](https://arxiv.org/abs/2507.13742)
*Oussama Bouaggad,Natalia Grabar*

Main category: cs.LG

TL;DR: 论文提出了一种基于监督学习的变压器模型方法，用于生物医学词汇与UMLS的语义对齐，并通过优化技术显著提升了推理速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型在边缘设备或资源受限环境中的部署挑战，如能耗、内存使用和延迟问题。

Method: 采用基于余弦语义相似度的监督学习方法，结合Microsoft Olive、ONNX Runtime、Intel Neural Compressor和IPEX进行动态量化和优化。

Result: 在DEFT 2020任务中达到新SOTA，推理速度提升20倍，内存使用减少约70%，性能指标保持不变。

Conclusion: 提出的方法有效解决了模型部署中的效率问题，为资源受限环境下的AI应用提供了可行方案。

Abstract: In the fast-moving world of AI, as organizations and researchers develop more
advanced models, they face challenges due to their sheer size and computational
demands. Deploying such models on edge devices or in resource-constrained
environments adds further challenges related to energy consumption, memory
usage and latency. To address these challenges, emerging trends are shaping the
future of efficient model optimization techniques. From this premise, by
employing supervised state-of-the-art transformer-based models, this research
introduces a systematic method for ontology alignment, grounded in cosine-based
semantic similarity between a biomedical layman vocabulary and the Unified
Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to
search for target optimizations among different Execution Providers (EPs) using
the ONNX Runtime backend, followed by an assembled process of dynamic
quantization employing Intel Neural Compressor and IPEX (Intel Extension for
PyTorch). Through our optimization process, we conduct extensive assessments on
the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new
state-of-the-art in both. We retain performance metrics intact, while attaining
an average inference speed-up of 20x and reducing memory usage by approximately
70%.

</details>


### [90] [MolPIF: A Parameter Interpolation Flow Model for Molecule Generation](https://arxiv.org/abs/2507.13762)
*Yaowei Jin,Junjie Wang,Wenkai Xiang,Duanhua Cao,Dan Teng,Zhehuan Fan,Jiacheng Xiong,Xia Sheng,Chuanlong Zeng,Mingyue Zheng,Qian Shi*

Main category: cs.LG

TL;DR: 论文提出了一种新的参数插值流模型（PIF），用于分子生成，解决了贝叶斯流网络在灵活性和适应性上的限制，并在药物设计中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯流网络（BFNs）在分子生成任务中表现出色，但其基于贝叶斯推断的策略限制了分布转换路径的灵活性，难以适应多样化的数据分布和任务需求。此外，参数空间模型的潜力尚未充分探索。

Method: 提出了一种名为PIF的参数插值流模型，提供了详细的理论基础、训练和推断流程，并开发了MolPIF用于基于结构的药物设计。

Result: MolPIF在多种指标上优于基线模型，验证了参数空间生成建模范式的有效性。

Conclusion: PIF模型为分子生成提供了新的设计视角，展示了参数空间模型的潜力。

Abstract: Advances in deep learning for molecular generation show promise in
accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown
impressive performance across diverse chemical tasks, with their success often
ascribed to the paradigm of modeling in a low-variance parameter space.
However, the Bayesian inference-based strategy imposes limitations on designing
more flexible distribution transformation pathways, making it challenging to
adapt to diverse data distributions and varied task requirements. Furthermore,
the potential for simpler, more efficient parameter-space-based models is
unexplored. To address this, we propose a novel Parameter Interpolation Flow
model (named PIF) with detailed theoretical foundation, training, and inference
procedures. We then develop MolPIF for structure-based drug design,
demonstrating its superior performance across diverse metrics compared to
baselines. This work validates the effectiveness of parameter-space-based
generative modeling paradigm for molecules and offers new perspectives for
model design.

</details>


### [91] [Dual-Center Graph Clustering with Neighbor Distribution](https://arxiv.org/abs/2507.13765)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Li Jin,Liqiang Nie*

Main category: cs.LG

TL;DR: 论文提出了一种基于邻居分布特性的双中心图聚类方法（DCGC），通过邻居分布作为监督信号和双中心优化，提升了聚类效果。


<details>
  <summary>Details</summary>
Motivation: 传统目标导向聚类方法仅利用特征构建单目标分布，导致监督信号不可靠且优化不完整。

Method: 利用邻居分布作为监督信号挖掘对比学习中的难负样本，并引入邻居分布中心与特征中心共同构建双目标分布进行优化。

Result: 实验证明DCGC方法在性能和效果上优于现有方法。

Conclusion: DCGC通过可靠的监督信号和双中心优化，显著提升了图聚类的效果。

Abstract: Graph clustering is crucial for unraveling intricate data structures, yet it
presents significant challenges due to its unsupervised nature. Recently,
goal-directed clustering techniques have yielded impressive results, with
contrastive learning methods leveraging pseudo-label garnering considerable
attention. Nonetheless, pseudo-label as a supervision signal is unreliable and
existing goal-directed approaches utilize only features to construct a
single-target distribution for single-center optimization, which lead to
incomplete and less dependable guidance. In our work, we propose a novel
Dual-Center Graph Clustering (DCGC) approach based on neighbor distribution
properties, which includes representation learning with neighbor distribution
and dual-center optimization. Specifically, we utilize neighbor distribution as
a supervision signal to mine hard negative samples in contrastive learning,
which is reliable and enhances the effectiveness of representation learning.
Furthermore, neighbor distribution center is introduced alongside feature
center to jointly construct a dual-target distribution for dual-center
optimization. Extensive experiments and analysis demonstrate superior
performance and effectiveness of our proposed method.

</details>


### [92] [On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach](https://arxiv.org/abs/2507.13805)
*Tim Rensmeyer,Denis Kramer,Oliver Niggemann*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯神经网络的方法，用于微调预训练的机器学习力场模型，并通过在线学习自动优化模型，同时检测和采样稀有事件。


<details>
  <summary>Details</summary>
Motivation: 由于从头计算原子间力的计算复杂度高，机器学习力场的研究变得活跃，但生成足够多样化的训练数据集仍具有挑战性，尤其是对稀有事件或大构型空间的系统。

Method: 采用贝叶斯神经网络方法微调预训练的基础模型，并通过在线学习流程自动优化模型，同时利用不确定性量化检测稀有事件。

Result: 该方法能够在保持预设精度的同时自动微调模型，并能以更高频率检测和采样稀有事件（如过渡态）。

Conclusion: 提出的方法为微调基础模型提供了一种有效途径，尤其适用于稀有事件和大构型空间的系统，同时降低了训练数据的依赖。

Abstract: Due to the computational complexity of evaluating interatomic forces from
first principles, the creation of interatomic machine learning force fields has
become a highly active field of research. However, the generation of training
datasets of sufficient size and sample diversity itself comes with a
computational burden that can make this approach impractical for modeling rare
events or systems with a large configuration space. Fine-tuning foundation
models that have been pre-trained on large-scale material or molecular
databases offers a promising opportunity to reduce the amount of training data
necessary to reach a desired level of accuracy. However, even if this approach
requires less training data overall, creating a suitable training dataset can
still be a very challenging problem, especially for systems with rare events
and for end-users who don't have an extensive background in machine learning.
In on-the-fly learning, the creation of a training dataset can be largely
automated by using model uncertainty during the simulation to decide if the
model is accurate enough or if a structure should be recalculated with
classical methods and used to update the model. A key challenge for applying
this form of active learning to the fine-tuning of foundation models is how to
assess the uncertainty of those models during the fine-tuning process, even
though most foundation models lack any form of uncertainty quantification. In
this paper, we overcome this challenge by introducing a fine-tuning approach
based on Bayesian neural network methods and a subsequent on-the-fly workflow
that automatically fine-tunes the model while maintaining a pre-specified
accuracy and can detect rare events such as transition states and sample them
at an increased rate relative to their occurrence.

</details>


### [93] [Scalable Submodular Policy Optimization via Pruned Submodularity Graph](https://arxiv.org/abs/2507.13834)
*Aditi Anand,Suman Banerjee,Dildar Ali*

Main category: cs.LG

TL;DR: 本文研究了强化学习中奖励函数为次模函数的问题，提出了一种基于修剪次模图的方法，以在可行计算时间内提供近似最优解。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中奖励函数通常为加性，但现实中许多问题（如路径规划、覆盖控制等）的奖励函数具有递减回报特性，适合用次模函数建模。本文旨在解决此类问题。

Method: 提出了一种基于修剪次模图的方法，分析了其时间和空间复杂度，并提供了性能保证。

Result: 在基准实验中，所提方法获得的策略比基线方法产生更高的奖励。

Conclusion: 该方法在次模奖励函数的强化学习问题中表现出色，优于现有基线方法。

Abstract: In Reinforcement Learning (abbreviated as RL), an agent interacts with the
environment via a set of possible actions, and a reward is generated from some
unknown distribution. The task here is to find an optimal set of actions such
that the reward after a certain time step gets maximized. In a traditional
setup, the reward function in an RL Problem is considered additive. However, in
reality, there exist many problems, including path planning, coverage control,
etc., the reward function follows the diminishing return, which can be modeled
as a submodular function. In this paper, we study a variant of the RL Problem
where the reward function is submodular, and our objective is to find an
optimal policy such that this reward function gets maximized. We have proposed
a pruned submodularity graph-based approach that provides a provably
approximate solution in a feasible computation time. The proposed approach has
been analyzed to understand its time and space requirements as well as a
performance guarantee. We have experimented with a benchmark agent-environment
setup, which has been used for similar previous studies, and the results are
reported. From the results, we observe that the policy obtained by our proposed
approach leads to more reward than the baseline methods.

</details>


### [94] [Self-supervised learning on gene expression data](https://arxiv.org/abs/2507.13912)
*Kevin Dradjat,Massinissa Hamidi,Pierre Bartet,Blaise Hanczar*

Main category: cs.LG

TL;DR: 研究探讨了自监督学习在基因表达数据表型预测中的应用，证明其优于传统监督学习并减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 基因表达数据的标注成本高且耗时，自监督学习可直接从未标注数据中提取信息，解决这一问题。

Method: 选择了三种自监督学习方法，评估其在基因表达数据中的表现，并用于下游预测任务。

Result: 自监督学习方法能有效捕捉复杂信息，提高预测准确性，且减少对标注数据的依赖。

Conclusion: 自监督学习在基因表达数据分析中具有潜力，未来研究可进一步优化其应用。

Abstract: Predicting phenotypes from gene expression data is a crucial task in
biomedical research, enabling insights into disease mechanisms, drug responses,
and personalized medicine. Traditional machine learning and deep learning rely
on supervised learning, which requires large quantities of labeled data that
are costly and time-consuming to obtain in the case of gene expression data.
Self-supervised learning has recently emerged as a promising approach to
overcome these limitations by extracting information directly from the
structure of unlabeled data. In this study, we investigate the application of
state-of-the-art self-supervised learning methods to bulk gene expression data
for phenotype prediction. We selected three self-supervised methods, based on
different approaches, to assess their ability to exploit the inherent structure
of the data and to generate qualitative representations which can be used for
downstream predictive tasks. By using several publicly available gene
expression datasets, we demonstrate how the selected methods can effectively
capture complex information and improve phenotype prediction accuracy. The
results obtained show that self-supervised learning methods can outperform
traditional supervised models besides offering significant advantage by
reducing the dependency on annotated data. We provide a comprehensive analysis
of the performance of each method by highlighting their strengths and
limitations. We also provide recommendations for using these methods depending
on the case under study. Finally, we outline future research directions to
enhance the application of self-supervised learning in the field of gene
expression data analysis. This study is the first work that deals with bulk
RNA-Seq data and self-supervised learning.

</details>


### [95] [Reframing attention as a reinforcement learning problem for causal discovery](https://arxiv.org/abs/2507.13920)
*Turan Orujlu,Christian Gumbsch,Martin V. Butz,Charley M Wu*

Main category: cs.LG

TL;DR: 该论文提出了Causal Process框架，用于表示动态因果结构的假设，并实现了Causal Process Model，将Transformer的注意力机制与强化学习结合，以推断可解释的因果过程。


<details>
  <summary>Details</summary>
Motivation: 现代强化学习（RL）中的因果表示研究多假设静态因果图，忽略了因果交互的动态性。本文旨在填补这一空白。

Method: 提出Causal Process框架及其实现Causal Process Model，将Transformer的注意力机制融入RL环境，通过RL代理构建动态因果图假设。

Result: 在RL环境中，该方法在因果表示学习和代理性能上优于现有方法，并能恢复动态因果过程图。

Conclusion: Causal Process框架为动态因果结构提供了新的理论支持，其实现展示了在RL中推断可解释因果过程的有效性。

Abstract: Formal frameworks of causality have operated largely parallel to modern
trends in deep reinforcement learning (RL). However, there has been a revival
of interest in formally grounding the representations learned by neural
networks in causal concepts. Yet, most attempts at neural models of causality
assume static causal graphs and ignore the dynamic nature of causal
interactions. In this work, we introduce Causal Process framework as a novel
theory for representing dynamic hypotheses about causal structure. Furthermore,
we present Causal Process Model as an implementation of this framework. This
allows us to reformulate the attention mechanism popularized by Transformer
networks within an RL setting with the goal to infer interpretable causal
processes from visual observations. Here, causal inference corresponds to
constructing a causal graph hypothesis which itself becomes an RL task nested
within the original RL problem. To create an instance of such hypothesis, we
employ RL agents. These agents establish links between units similar to the
original Transformer attention mechanism. We demonstrate the effectiveness of
our approach in an RL environment where we outperform current alternatives in
causal representation learning and agent performance, and uniquely recover
graphs of dynamic causal processes.

</details>


### [96] [MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space](https://arxiv.org/abs/2507.13950)
*Jingbo Liang,Bruna Jacobson*

Main category: cs.LG

TL;DR: 提出了一种名为MoDyGAN的新方法，结合分子动力学模拟和生成对抗网络，用于高效探索蛋白质构象空间。


<details>
  <summary>Details</summary>
Motivation: 由于基于物理的动态模拟计算成本高，探索蛋白质构象空间仍是一个挑战。

Method: MoDyGAN利用生成器将高斯分布映射到分子动力学轨迹，并通过双判别器和集成学习优化构象的合理性。创新性地将3D蛋白质结构转换为2D矩阵，以便使用图像GAN架构。

Result: 实验证明MoDyGAN能生成合理的蛋白质新构象，且潜在空间插值与引导分子动力学模拟轨迹一致。

Conclusion: 将蛋白质表示为图像数据为深度学习在生物分子模拟中的应用提供了新思路，未来可扩展到其他复杂3D结构。

Abstract: Extensively exploring protein conformational landscapes remains a major
challenge in computational biology due to the high computational cost involved
in dynamic physics-based simulations. In this work, we propose a novel
pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and
generative adversarial networks (GANs) to explore protein conformational
spaces. MoDyGAN contains a generator that maps Gaussian distributions into
MD-derived protein trajectories, and a refinement module that combines ensemble
learning with a dual-discriminator to further improve the plausibility of
generated conformations. Central to our approach is an innovative
representation technique that reversibly transforms 3D protein structures into
2D matrices, enabling the use of advanced image-based GAN architectures. We use
three rigid proteins to demonstrate that MoDyGAN can generate plausible new
conformations. We also use deca-alanine as a case study to show that
interpolations within the latent space closely align with trajectories obtained
from steered molecular dynamics (SMD) simulations. Our results suggest that
representing proteins as image-like data unlocks new possibilities for applying
advanced deep learning techniques to biomolecular simulation, leading to an
efficient sampling of conformational states. Additionally, the proposed
framework holds strong potential for extension to other complex 3D structures.

</details>


### [97] [Robust Anomaly Detection with Graph Neural Networks using Controllability](https://arxiv.org/abs/2507.13954)
*Yifan Wei,Anwar Said,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 论文提出两种基于平均可控性的图学习方法，用于提升稀疏和不平衡数据中的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 复杂领域中的异常检测面临标注数据不足和样本不平衡的挑战，需要结合属性与关系数据的创新方法。

Method: 提出两种方法：1) 将平均可控性作为边权重；2) 将其编码为独热边属性向量。

Result: 在真实和合成网络中验证，性能优于六种基线方法。

Conclusion: 平均可控性指标能显著提升图机器学习模型的异常检测能力。

Abstract: Anomaly detection in complex domains poses significant challenges due to the
need for extensive labeled data and the inherently imbalanced nature of
anomalous versus benign samples. Graph-based machine learning models have
emerged as a promising solution that combines attribute and relational data to
uncover intricate patterns. However, the scarcity of anomalous data exacerbates
the challenge, which requires innovative strategies to enhance model learning
with limited information. In this paper, we hypothesize that the incorporation
of the influence of the nodes, quantified through average controllability, can
significantly improve the performance of anomaly detection. We propose two
novel approaches to integrate average controllability into graph-based
frameworks: (1) using average controllability as an edge weight and (2)
encoding it as a one-hot edge attribute vector. Through rigorous evaluation on
real-world and synthetic networks with six state-of-the-art baselines, our
proposed methods demonstrate improved performance in identifying anomalies,
highlighting the critical role of controllability measures in enhancing the
performance of graph machine learning models. This work underscores the
potential of integrating average controllability as additional metrics to
address the challenges of anomaly detection in sparse and imbalanced datasets.

</details>


### [98] [Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs](https://arxiv.org/abs/2507.13959)
*Eli Verwimp,Gustav Ryberg Smidt,Hendrik Hameeuw,Katrien De Graef*

Main category: cs.LG

TL;DR: 该论文研究了机器学习模型在分类楔形文字符号时的表现，分析了数据差异对模型性能的影响，并提出了未来数据采集标准的建议。


<details>
  <summary>Details</summary>
Motivation: 楔形文字符号存在多种变异性，导致模型在不同数据集上表现不佳，研究旨在解决这一问题并为未来分类任务奠定基础。

Method: 使用ResNet50模型对来自三个美索不达米亚城市的手写古巴比伦文本进行训练和测试。

Result: 模型在符号实例数至少20个的情况下，top-1准确率为87.1%，top-5准确率为96.5%。

Conclusion: 研究为楔形文字符号分类提供了首个自动分类结果，并强调了数据标准化的重要性。

Abstract: The work in this paper describes the training and evaluation of machine
learning (ML) techniques for the classification of cuneiform signs. There is a
lot of variability in cuneiform signs, depending on where they come from, for
what and by whom they were written, but also how they were digitized. This
variability makes it unlikely that an ML model trained on one dataset will
perform successfully on another dataset. This contribution studies how such
differences impact that performance. Based on our results and insights, we aim
to influence future data acquisition standards and provide a solid foundation
for future cuneiform sign classification tasks. The ML model has been trained
and tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary
texts inscribed on clay tablets originating from three Mesopotamian cities
(Nippur, D\=ur-Abie\v{s}uh and Sippar). The presented and analysed model is
ResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for
signs with at least 20 instances. As these automatic classification results are
the first on Old Babylonian texts, there are currently no comparable results.

</details>


### [99] [Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks](https://arxiv.org/abs/2507.13992)
*Jagruti Patel,Thomas A. W. Bolton,Mikkel Schöttner,Anjali Tarun,Sebastien Tourbier,Yasser Alemàn-Gòmez,Jonas Richiardi,Patric Hagmann*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的结构连接组（SC）跨站点协调框架，无需依赖元数据或旅行受试者，解决了现有方法的局限性。通过比较三种深度架构，发现图卷积自编码器在保持拓扑结构和个体特征方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决神经影像学中因样本量小和站点间扫描仪差异导致的统计功效、可靠性和泛化性不足的问题，尤其是针对阿尔茨海默病和精神分裂症等疾病的生物标志物开发。

Method: 提出了一种站点条件化的深度协调框架，测试了三种深度架构（全连接自编码器、卷积自编码器和图卷积自编码器），并与线性回归基线进行比较。

Result: 非图模型在边权重预测和边存在检测上表现优异，而图卷积自编码器在拓扑结构保持和个体特征保留方面表现最佳。线性回归基线虽数值性能最高，但依赖元数据，实际应用受限。

Conclusion: 图卷积自编码器在结构感知和领域泛化的SC协调中表现突出，适用于大规模多站点研究。

Abstract: Small sample sizes in neuroimaging in general, and in structural connectome
(SC) studies in particular limit the development of reliable biomarkers for
neurological and psychiatric disorders - such as Alzheimer's disease and
schizophrenia - by reducing statistical power, reliability, and
generalizability. Large-scale multi-site studies have exist, but they have
acquisition-related biases due to scanner heterogeneity, compromising imaging
consistency and downstream analyses. While existing SC harmonization methods -
such as linear regression (LR), ComBat, and deep learning techniques - mitigate
these biases, they often rely on detailed metadata, traveling subjects (TS), or
overlook the graph-topology of SCs. To address these limitations, we propose a
site-conditioned deep harmonization framework that harmonizes SCs across
diverse acquisition sites without requiring metadata or TS that we test in a
simulated scenario based on the Human Connectome Dataset. Within this
framework, we benchmark three deep architectures - a fully connected
autoencoder (AE), a convolutional AE, and a graph convolutional AE - against a
top-performing LR baseline. While non-graph models excel in edge-weight
prediction and edge existence detection, the graph AE demonstrates superior
preservation of topological structure and subject-level individuality, as
reflected by graph metrics and fingerprinting accuracy, respectively. Although
the LR baseline achieves the highest numerical performance by explicitly
modeling acquisition parameters, it lacks applicability to real-world
multi-site use cases as detailed acquisition metadata is often unavailable. Our
results highlight the critical role of model architecture in SC harmonization
performance and demonstrate that graph-based approaches are particularly
well-suited for structure-aware, domain-generalizable SC harmonization in
large-scale multi-site SC studies.

</details>


### [100] [ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies](https://arxiv.org/abs/2507.13998)
*Itay Katav,Aryeh Kontorovich*

Main category: cs.LG

TL;DR: 论文提出了一种动态加权机制ParallelTime Weighter和ParallelTime架构，用于优化时间序列预测中长短期依赖的权重分配，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间序列预测中对长短期依赖赋予等权重，效果不佳，需动态调整权重。

Method: 提出ParallelTime Weighter动态计算权重，并设计ParallelTime架构结合该机制。

Result: 新架构在多个基准测试中表现优异，计算效率高且参数少。

Conclusion: ParallelTime为时间序列预测中的并行Attention-Mamba提供了新方向。

Abstract: Modern multivariate time series forecasting primarily relies on two
architectures: the Transformer with attention mechanism and Mamba. In natural
language processing, an approach has been used that combines local window
attention for capturing short-term dependencies and Mamba for capturing
long-term dependencies, with their outputs averaged to assign equal weight to
both. We find that for time-series forecasting tasks, assigning equal weight to
long-term and short-term dependencies is not optimal. To mitigate this, we
propose a dynamic weighting mechanism, ParallelTime Weighter, which calculates
interdependent weights for long-term and short-term dependencies for each token
based on the input and the model's knowledge. Furthermore, we introduce the
ParallelTime architecture, which incorporates the ParallelTime Weighter
mechanism to deliver state-of-the-art performance across diverse benchmarks.
Our architecture demonstrates robustness, achieves lower FLOPs, requires fewer
parameters, scales effectively to longer prediction horizons, and significantly
outperforms existing methods. These advances highlight a promising path for
future developments of parallel Attention-Mamba in time series forecasting. The
implementation is readily available at:
\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub

</details>


### [101] [On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes](https://arxiv.org/abs/2507.14005)
*Mathieu Godbout,Audrey Durand*

Main category: cs.LG

TL;DR: 论文探讨了动态规划方法在MDP中寻找静态CVaR最优策略时的问题，揭示了评估错误的根源，并证明了双CVaR分解方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动态规划方法在静态CVaR最优策略中的失败原因，并探索其根本问题。

Method: 通过将政策评估任务分为两个最小化问题，分析风险分配一致性约束的满足情况，并量化评估误差。

Result: 发现约束交集为空是评估错误的根源，且双CVaR分解方法存在局限性。

Conclusion: 双CVaR分解方法无法在所有初始风险水平下找到单一最优策略。

Abstract: Recent work has shown that dynamic programming (DP) methods for finding
static CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when
based on the dual formulation, yet the root cause for the failure has remained
unclear. We expand on these findings by shifting focus from policy optimization
to the seemingly simpler task of policy evaluation. We show that evaluating the
static CVaR of a given policy can be framed as two distinct minimization
problems. For their solutions to match, a set of ``risk-assignment consistency
constraints'' must be satisfied, and we demonstrate that the intersection of
the constraints being empty is the source of previously observed evaluation
errors. Quantifying the evaluation error as the CVaR evaluation gap, we then
demonstrate that the issues observed when optimizing over the dual-based CVaR
DP are explained by the returned policy having a non-zero CVaR evaluation gap.
We then leverage our proposed risk-assignment perspective to prove that the
search for a single, uniformly optimal policy via on the dual CVaR
decomposition is fundamentally limited, identifying an MDP where no single
policy can be optimal across all initial risk levels.

</details>


### [102] [Byzantine-resilient federated online learning for Gaussian process regression](https://arxiv.org/abs/2507.14021)
*Xu Zhang,Zhenyuan Yuan,Minghui Zhu*

Main category: cs.LG

TL;DR: 提出了一种拜占庭容错的联邦高斯过程回归算法，用于在部分代理存在拜占庭故障时提升学习性能。


<details>
  <summary>Details</summary>
Motivation: 研究拜占庭容错的联邦在线学习，解决高斯过程回归中代理可能存在的任意或对抗行为问题。

Method: 开发了一种拜占庭容错的联邦GPR算法，通过云和代理协作学习潜在函数，并使用拜占庭容错的产品专家聚合规则。

Result: 量化了代理融合GPR相对于本地GPR的学习精度提升，并在实验数据中验证了算法性能。

Conclusion: 提出的算法有效提升了拜占庭故障代理环境下的学习性能。

Abstract: In this paper, we study Byzantine-resilient federated online learning for
Gaussian process regression (GPR). We develop a Byzantine-resilient federated
GPR algorithm that allows a cloud and a group of agents to collaboratively
learn a latent function and improve the learning performances where some agents
exhibit Byzantine failures, i.e., arbitrary and potentially adversarial
behavior. Each agent-based local GPR sends potentially compromised local
predictions to the cloud, and the cloud-based aggregated GPR computes a global
model by a Byzantine-resilient product of experts aggregation rule. Then the
cloud broadcasts the current global model to all the agents. Agent-based fused
GPR refines local predictions by fusing the received global model with that of
the agent-based local GPR. Moreover, we quantify the learning accuracy
improvements of the agent-based fused GPR over the agent-based local GPR.
Experiments on a toy example and two medium-scale real-world datasets are
conducted to demonstrate the performances of the proposed algorithm.

</details>


### [103] [DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis](https://arxiv.org/abs/2507.14038)
*Aileen Luo,Tao Zhou,Ming Du,Martin V. Holt,Andrej Singer,Mathew J. Cherukara*

Main category: cs.LG

TL;DR: DONUT是一种基于物理感知的神经网络，用于快速自动分析纳米束衍射数据，无需标记数据集或预训练，效率比传统方法高200倍。


<details>
  <summary>Details</summary>
Motivation: 实时分析纳米束衍射数据的瓶颈在于计算需求和伪影问题，DONUT旨在解决这一挑战。

Method: DONUT结合可微几何衍射模型，通过无监督训练预测晶体晶格应变和取向。

Result: 实验证明，DONUT能高效提取数据特征，速度比传统拟合方法快200倍。

Conclusion: DONUT为X射线科学中的实时数据分析提供了高效解决方案，克服了监督学习的局限性。

Abstract: Coherent X-ray scattering techniques are critical for investigating the
fundamental structural properties of materials at the nanoscale. While
advancements have made these experiments more accessible, real-time analysis
remains a significant bottleneck, often hindered by artifacts and computational
demands. In scanning X-ray nanodiffraction microscopy, which is widely used to
spatially resolve structural heterogeneities, this challenge is compounded by
the convolution of the divergent beam with the sample's local structure. To
address this, we introduce DONUT (Diffraction with Optics for Nanobeam by
Unsupervised Training), a physics-aware neural network designed for the rapid
and automated analysis of nanobeam diffraction data. By incorporating a
differentiable geometric diffraction model directly into its architecture,
DONUT learns to predict crystal lattice strain and orientation in real-time.
Crucially, this is achieved without reliance on labeled datasets or
pre-training, overcoming a fundamental limitation for supervised machine
learning in X-ray science. We demonstrate experimentally that DONUT accurately
extracts all features within the data over 200 times more efficiently than
conventional fitting methods.

</details>


### [104] [Noradrenergic-inspired gain modulation attenuates the stability gap in joint training](https://arxiv.org/abs/2507.14056)
*Alejandro Rodriguez-Garcia,Anindya Ghosh,Srikanth Ramaswamy*

Main category: cs.LG

TL;DR: 论文研究了持续学习中的稳定性缺口问题，提出了一种基于不确定性调制的增益动态机制来平衡知识整合与干扰，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 持续学习中的稳定性缺口揭示了现有方法在平衡快速适应与稳健记忆方面的不足，需要新的机制来解决这一问题。

Method: 受生物大脑多时间尺度动态启发，提出不确定性调制的增益动态机制，模拟双时间尺度优化器，动态平衡知识整合与干扰。

Result: 在MNIST和CIFAR基准测试中，该机制有效减少了稳定性缺口。

Conclusion: 不确定性调制的增益动态机制不仅解决了稳定性缺口问题，还提供了对生物神经调节机制的模拟，为持续学习性能提升提供了新思路。

Abstract: Recent studies in continual learning have identified a transient drop in
performance on mastered tasks when assimilating new ones, known as the
stability gap. Such dynamics contradict the objectives of continual learning,
revealing a lack of robustness in mitigating forgetting, and notably,
persisting even under an ideal joint-loss regime. Examining this gap within
this idealized joint training context is critical to isolate it from other
sources of forgetting. We argue that it reflects an imbalance between rapid
adaptation and robust retention at task boundaries, underscoring the need to
investigate mechanisms that reconcile plasticity and stability within continual
learning frameworks. Biological brains navigate a similar dilemma by operating
concurrently on multiple timescales, leveraging neuromodulatory signals to
modulate synaptic plasticity. However, artificial networks lack native
multitimescale dynamics, and although optimizers like momentum-SGD and Adam
introduce implicit timescale regularization, they still exhibit stability gaps.
Inspired by locus coeruleus mediated noradrenergic bursts, which transiently
enhance neuronal gain under uncertainty to facilitate sensory assimilation, we
propose uncertainty-modulated gain dynamics - an adaptive mechanism that
approximates a two-timescale optimizer and dynamically balances integration of
knowledge with minimal interference on previously consolidated information. We
evaluate our mechanism on domain-incremental and class-incremental variants of
the MNIST and CIFAR benchmarks under joint training, demonstrating that
uncertainty-modulated gain dynamics effectively attenuate the stability gap.
Finally, our analysis elucidates how gain modulation replicates noradrenergic
functions in cortical circuits, offering mechanistic insights into reducing
stability gaps and enhance performance in continual learning tasks.

</details>


### [105] [Preference-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2507.14066)
*Ni Mu,Yao Luan,Qing-Shan Jia*

Main category: cs.LG

TL;DR: 本文提出了一种基于偏好的多目标强化学习方法（Pb-MORL），通过偏好指导策略优化，避免了复杂奖励函数的设计，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习（MORL）通常依赖预定义的奖励函数，但这些函数难以平衡冲突目标且易导致简化。偏好能提供更灵活直观的决策指导。

Method: Pb-MORL将偏好整合到MORL框架中，构建与偏好对齐的多目标奖励模型，并证明优化该模型等价于训练帕累托最优策略。

Result: 在多个基准任务（如多能源管理和自动驾驶）中，Pb-MORL表现优于使用真实奖励函数的方法。

Conclusion: Pb-MORL展示了在复杂现实系统中应用的潜力，通过偏好简化了多目标优化问题。

Abstract: Multi-objective reinforcement learning (MORL) is a structured approach for
optimizing tasks with multiple objectives. However, it often relies on
pre-defined reward functions, which can be hard to design for balancing
conflicting goals and may lead to oversimplification. Preferences can serve as
more flexible and intuitive decision-making guidance, eliminating the need for
complicated reward design. This paper introduces preference-based MORL
(Pb-MORL), which formalizes the integration of preferences into the MORL
framework. We theoretically prove that preferences can derive policies across
the entire Pareto frontier. To guide policy optimization using preferences, our
method constructs a multi-objective reward model that aligns with the given
preferences. We further provide theoretical proof to show that optimizing this
reward model is equivalent to training the Pareto optimal policy. Extensive
experiments in benchmark multi-objective tasks, a multi-energy management task,
and an autonomous driving task on a multi-line highway show that our method
performs competitively, surpassing the oracle method, which uses the ground
truth reward function. This highlights its potential for practical applications
in complex real-world systems.

</details>


### [106] [DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration](https://arxiv.org/abs/2507.14088)
*Xiyun Li,Yining Ding,Yuhua Jiang,Yunlong Zhao,Runpeng Xie,Shuang Xu,Yuanhua Ni,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 提出了一种基于双过程多尺度心智理论（DPMT）的框架，用于增强人-AI协作，特别是在复杂人类行为建模方面。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）代理在缺乏直接沟通时难以准确建模人类复杂的心理特征，如领域意图。

Method: 结合认知科学的双过程理论，提出DPMT框架，包含多尺度心智理论（ToM）模块，用于推理心理特征。

Result: 实验表明DPMT显著提升了人-AI协作效果，消融研究验证了多尺度ToM在慢系统中的贡献。

Conclusion: DPMT框架通过多尺度ToM模块有效解决了人-AI协作中的复杂心理建模问题。

Abstract: Real-time human-artificial intelligence (AI) collaboration is crucial yet
challenging, especially when AI agents must adapt to diverse and unseen human
behaviors in dynamic scenarios. Existing large language model (LLM) agents
often fail to accurately model the complex human mental characteristics such as
domain intentions, especially in the absence of direct communication. To
address this limitation, we propose a novel dual process multi-scale theory of
mind (DPMT) framework, drawing inspiration from cognitive science dual process
theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)
module to facilitate robust human partner modeling through mental
characteristic reasoning. Experimental results demonstrate that DPMT
significantly enhances human-AI collaboration, and ablation studies further
validate the contributions of our multi-scale ToM in the slow system.

</details>


### [107] [Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective](https://arxiv.org/abs/2507.14121)
*Pankaj Yadav,Vivek Vijay*

Main category: cs.LG

TL;DR: Kolmogorov Arnold Networks (KANs) 在类别不平衡分类中表现优于标准多层感知机（MLPs），但传统不平衡策略与KANs数学结构冲突，且计算成本高。MLPs结合不平衡技术与KANs性能相当，但成本更低。


<details>
  <summary>Details</summary>
Motivation: 评估KANs在类别不平衡分类中的表现，探索其与传统不平衡策略的兼容性及计算效率。

Method: 在十个基准数据集上对KANs和MLPs进行实证比较，测试其在原始不平衡数据及结合不平衡策略时的表现。

Result: KANs在原始不平衡数据上表现优于MLPs，但传统不平衡策略显著降低其性能，且计算成本高；MLPs结合不平衡技术与KANs性能相当。

Conclusion: KANs适用于资源充足的原始不平衡数据场景，但需针对不平衡学习优化架构和计算效率，并解决与数据增强的冲突。

Abstract: Kolmogorov Arnold Networks (KANs) are recent architectural advancement in
neural computation that offer a mathematically grounded alternative to standard
neural networks. This study presents an empirical evaluation of KANs in context
of class imbalanced classification, using ten benchmark datasets. We observe
that KANs can inherently perform well on raw imbalanced data more effectively
than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,
conventional imbalance strategies fundamentally conflict with KANs mathematical
structure as resampling and focal loss implementations significantly degrade
KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from
prohibitive computational costs without proportional performance gains.
Statistical validation confirms that MLPs with imbalance techniques achieve
equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.
These findings reveal that KANs represent a specialized solution for raw
imbalanced data where resources permit. But their severe performance-resource
tradeoffs and incompatibility with standard resampling techniques currently
limits practical deployment. We identify critical research priorities as
developing KAN specific architectural modifications for imbalance learning,
optimizing computational efficiency, and theoretical reconciling their conflict
with data augmentation. This work establishes foundational insights for next
generation KAN architectures in imbalanced classification scenarios.

</details>


### [108] [Toward Temporal Causal Representation Learning with Tensor Decomposition](https://arxiv.org/abs/2507.14126)
*Jianhong Chen,Meng Zhao,Mostafa Reisi Gahrooei,Xubo Yue*

Main category: cs.LG

TL;DR: 提出了一种结合时间因果表示学习与非规则张量分解的框架CaRTeD，用于高维不规则数据，提升了因果表示的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中数据多为高维不规则张量，现有方法难以有效提取因果信息，需结合时间因果表示与张量分解。

Method: 提出CaRTeD框架，整合时间因果表示学习与非规则张量分解，提供灵活的正则化设计。

Result: 理论证明算法收敛，实验在合成与真实数据（MIMIC-III）上表现优于现有方法。

Conclusion: CaRTeD填补了非规则张量分解理论空白，提升了因果表示的可解释性与下游任务性能。

Abstract: Temporal causal representation learning is a powerful tool for uncovering
complex patterns in observational studies, which are often represented as
low-dimensional time series. However, in many real-world applications, data are
high-dimensional with varying input lengths and naturally take the form of
irregular tensors. To analyze such data, irregular tensor decomposition is
critical for extracting meaningful clusters that capture essential information.
In this paper, we focus on modeling causal representation learning based on the
transformed information. First, we present a novel causal formulation for a set
of latent clusters. We then propose CaRTeD, a joint learning framework that
integrates temporal causal representation learning with irregular tensor
decomposition. Notably, our framework provides a blueprint for downstream tasks
using the learned tensor factors, such as modeling latent structures and
extracting causal information, and offers a more flexible regularization design
to enhance tensor decomposition. Theoretically, we show that our algorithm
converges to a stationary point. More importantly, our results fill the gap in
theoretical guarantees for the convergence of state-of-the-art irregular tensor
decomposition. Experimental results on synthetic and real-world electronic
health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both
phenotyping and network recovery perspectives, demonstrate that our proposed
method outperforms state-of-the-art techniques and enhances the explainability
of causal representations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [109] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT提出了一种基于图的架构，解决了传统链式系统在智能交通管理中的低效问题，通过并行执行和动态资源分配显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于链式系统的智能交通管理（如TrafficGPT）存在任务执行顺序限制、高令牌消耗和扩展性差的问题，无法满足复杂现实场景的需求。

Method: GraphTrafficGPT将任务及其依赖关系表示为有向图中的节点和边，引入Brain Agent分解查询并协调多个专用代理，支持并行执行和动态资源分配。

Result: 实验结果显示，GraphTrafficGPT比TrafficGPT减少50.2%的令牌消耗和19.0%的响应延迟，同时支持多查询并行处理，效率提升23.0%。

Conclusion: GraphTrafficGPT通过图结构和并行处理机制，显著提升了智能交通管理的效率和扩展性，为复杂场景提供了更优解决方案。

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [110] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: PrefPalette是一个个性化AI框架，通过分解偏好属性和考虑社区价值观，显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前偏好模型将人类判断视为黑箱，缺乏对偏好背后原因的理解。PrefPalette旨在捕捉多样化的评价框架。

Method: 结合多属性决策原则，包括生成合成数据以隔离属性效果，以及基于注意力的偏好建模。

Result: 在Reddit的45个社区中，PrefPalette比GPT-4o预测准确率平均高出46.6%，并揭示了社区特定的偏好特征。

Conclusion: PrefPalette不仅提升了偏好建模性能，还提供了透明、可解释的洞察，为更可信的个性化应用奠定了基础。

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [111] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: 提出了一种结合大语言模型（LLMs）和符号系统的新方法，通过限制领域和结构化提示提取知识，生成可验证的Prolog符号表示，提高专家系统的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在生成知识时可能出现的幻觉或错误信息问题，开发可控、透明的专家系统。

Method: 采用结构化提示提取方法，将LLMs生成的知识转化为Prolog符号表示，并由人类专家验证和修正。

Result: 实验表明，该方法在事实准确性和语义连贯性上表现优异，结合了LLMs的召回能力和符号系统的精确性。

Conclusion: 提出了一种透明混合解决方案，为敏感领域提供可靠的AI应用基础。

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [112] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 论文探讨了AI应关注实体及其关系建模，而非仅关注像素和文字，并分析了关系学习未普及的原因及改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要建模像素和文字，但世界由实体及其关系构成，应直接建模这些实体。关系学习虽重要，却未广泛应用。

Method: 分析关系学习（如统计关系AI）的现状，探讨其未普及的原因，并提出改进方向。

Result: 关系学习仅在少数受限场景中应用，需进一步研究以提升其普及度。

Conclusion: 关系学习应成为AI核心领域，需更多研究以发挥其潜力。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [113] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG是一种双图RAG集成系统，通过实体网络图和文档导航图建模语言关系和文档结构，显著提升多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统RAG系统在处理复杂法规文本和多跳查询时的局限性。

Method: 结合实体网络图和文档导航图，实现图遍历与向量语义搜索的混合检索机制。

Result: 在多跳问题数据集上达到92.8%的精确率、85.5%的召回率和87.3%的F1分数，显著优于单模态RAG基线。

Conclusion: BifrostRAG为复杂技术文档的知识检索提供了可迁移的解决方案，适用于知识密集型工程领域。

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [114] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 论文探讨了基于最终答案的自动错误诊断方法，用于解决智能辅导系统中学生合并步骤导致的组合爆炸问题，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统中，学生合并步骤会导致可能的路径组合爆炸，增加错误诊断难度。利用最终答案诊断可以缓解这一问题。

Method: 设计了一种基于最终答案的错误诊断服务，并应用于二次方程求解的学生数据集进行验证。

Result: 该方法能诊断29.4%的未诊断步骤，且与教师诊断结果在97%的情况下一致。

Conclusion: 基于最终答案的诊断方法具有潜力，值得进一步探索。

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [115] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 提出了一种结合模型追踪和约束建模的方法，用于诊断学生在多步任务中的输入，并在解决二次方程的数据集上验证了其与教师诊断的一致性。


<details>
  <summary>Details</summary>
Motivation: 结合模型追踪和约束建模的优势，以更灵活地诊断学生输入，即使学生将多个步骤合并为一步。

Method: 通过定义约束作为学生输入与策略步骤的共同属性，设计了一个多步策略诊断系统，并在现有数据集上进行评估。

Result: 系统诊断与教师编码在所有140个学生步骤中完全一致。

Conclusion: 该方法有效结合了两种诊断范式，能够准确识别学生偏离策略的情况，即使步骤合并。

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [116] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM 是一种基于轻量级 LLM 的框架，用于生成和总结活动日志，通过整合位置、运动、环境和生理四维信息，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有活动日志生成方法在准确性、效率和语义丰富性上存在不足，需要一种更高效且全面的解决方案。

Method: DailyLLM 结合结构化提示和高效特征提取，利用智能手机和智能手表的传感器数据，实现高级活动理解。

Result: 实验表明，DailyLLM 在 BERTScore 精度上比现有方法提升 17%，推理速度快 10 倍，且仅需 1.5B 参数的 LLM。

Conclusion: DailyLLM 是一种高效且轻量的活动日志生成系统，适用于个人计算机和 Raspberry Pi，显著优于现有方法。

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [117] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: OntView是一个直观的本体可视化工具，通过用户友好界面展示本体概念及其形式定义，支持GCI可视化并提供简化视图功能。


<details>
  <summary>Details</summary>
Motivation: 现有本体可视化工具无法有效展示大型本体结构，限制了用户对依赖关系和属性的理解。

Method: OntView基于DL推理器，采用“所见即所想”范式，支持GCI可视化，并提供三种简化视图方法。

Result: OntView成功实现了直观的本体可视化，并开源发布。

Conclusion: OntView填补了现有工具在GCI可视化和简化视图方面的空白，提升了本体理解的效率。

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [118] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: 提出了一种结合启发式提取、语义激活和组合合成的混合架构，用于增强代理的战略推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统决策引擎通常选择最佳规则，而该系统通过语义交互建模和修辞框架，将冲突的启发式方法融合为连贯且上下文敏感的叙述。

Method: 结合经典军事理论和现代企业战略，通过语义相互依赖的过程激活和组合多个启发式方法。

Result: 通过Meta与FTC的案例研究展示了该框架，并通过语义指标进行了初步验证。

Conclusion: 讨论了局限性（如动态干扰调整）和扩展方向。

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [119] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: EAGLE是一个轻量级框架，用于动态图中的时间链接预测，通过结合短期时间局部性和长期全局结构模式，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有T-GNN在动态图时间链接预测中计算开销大，存在可扩展性和效率问题。

Method: EAGLE包含时间感知模块和结构感知模块，采用自适应权重机制平衡两者，避免复杂计算。

Result: 在七个真实世界动态图上，EAGLE在性能和效率上均优于现有T-GNN，速度提升超50倍。

Conclusion: EAGLE通过轻量设计有效解决了T-GNN的计算瓶颈，同时保持高性能。

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [120] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: 论文提出了一种因果知识转移框架，帮助多智能体强化学习（MARL）在非静态环境中共享因果表示，无需重新训练即可适应新环境。


<details>
  <summary>Details</summary>
Motivation: 解决传统MARL知识转移方法在非静态环境中难以泛化的问题，减少智能体适应新环境的重新训练成本。

Method: 通过建模碰撞为因果干预，生成恢复动作宏，并在智能体之间在线转移这些宏，实现零样本适应。

Result: 实验表明，该方法能使智能体在适应新环境时填补随机探索与完全重新训练策略之间约一半的差距，且效果受环境复杂性和智能体目标异质性影响。

Conclusion: 因果知识转移框架为MARL在动态环境中的适应性提供了有效解决方案，但其效果依赖于环境与智能体目标的匹配程度。

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [121] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: 提出了一种模型无关的潜在空间创意框架，通过导航连续嵌入空间实现可控、可扩展的创造力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）在生成新颖且相关内容时的局限性，避免依赖领域特定启发式或结构化提示。

Method: 采用潜在空间创意框架，无需手工规则，适应不同领域、输入格式和创意任务。

Result: 初步结果显示该方法作为通用人机协作创意工具的潜力。

Conclusion: 该框架为AI创意生成提供了灵活且可扩展的解决方案。

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [122] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: 论文提出了一种名为ADPC的视觉-语言因果干预框架，用于辅助诊断阿尔茨海默病（AD），通过结合多模态数据和因果推理，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 早期诊断阿尔茨海默病（AD）具有挑战性，主要因多模态数据的选择偏差和变量间复杂关系导致混淆因素。ADPC旨在解决这些问题。

Method: ADPC利用大型语言模型（LLM）总结临床数据，结合MRI和fMRI图像，通过因果干预消除混淆因素，分类认知正常（CN）、轻度认知障碍（MCI）和AD。

Result: 实验表明，ADPC在区分CN/MCI/AD病例上表现优异，多数评估指标达到SOTA水平。

Conclusion: 研究展示了因果推理与多模态学习结合在神经疾病诊断中的潜力。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [123] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: 论文提出了一种新颖的时态和约束扩展方法，用于增强ASP在动态系统中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决逻辑方法（如ASP）在细粒度时态和数值动态系统推理中的挑战。

Method: 结合线性时间逻辑和约束逻辑，提出了一种非单调时态推理框架。

Result: 建立了一个高分辨率的逻辑框架，适用于复杂动态系统的ASP推理。

Conclusion: 该方法为ASP范式下的高分辨率动态系统推理提供了理论基础。

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [124] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA是一种新型本体匹配框架，利用大型语言模型（LLMs）和检索增强生成（RAG）技术动态丰富语义上下文，通过优化技术显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有本体匹配系统依赖手工规则或专用模型，适应性有限，KROMA旨在通过LLMs和RAG技术提升语义互操作性。

Method: KROMA结合了基于双相似度的概念匹配和轻量级本体细化步骤，减少LLMs的通信开销，并通过知识检索和上下文增强优化匹配。

Result: 实验表明，KROMA在多个基准数据集上优于传统系统和前沿LLM方法，同时保持通信开销可控。

Conclusion: 研究证实了知识检索、提示增强和本体细化等优化技术在大规模本体匹配中的可行性和优势。

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [125] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: Glucose-ML是一个包含10个公开糖尿病数据集的集合，旨在加速透明、可重复且稳健的AI解决方案开发。


<details>
  <summary>Details</summary>
Motivation: 解决高质量糖尿病数据集获取困难的问题，推动AI在糖尿病管理中的应用。

Method: 收集并发布10个公开数据集，提供比较分析和血糖预测案例研究。

Result: 同一算法在不同数据集上表现差异显著，为开发稳健AI提供参考。

Conclusion: Glucose-ML为糖尿病AI研究提供数据支持，并强调数据集选择的重要性。

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [126] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: 该研究提出了一种基于生成式AI的人体运动模拟方法（G-AI-HMS），通过结合文本到文本和文本到运动的模型，提高了工业任务中运动模拟的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的人体运动模拟方法在运动保真度上表现不佳，影响了工人行为、安全和生产效率的评估。

Method: G-AI-HMS利用大型语言模型将任务描述转化为运动感知语言，并通过计算机视觉验证AI生成的运动与真实人类动作的相似性。

Result: 在八项任务的案例研究中，AI增强的运动在多数场景中表现优于人工描述，显著降低了关节误差和时间错位。

Conclusion: G-AI-HMS显著提升了运动模拟的准确性，为工业任务提供了更可靠的评估工具。

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [127] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: 该研究探讨了利用大型语言模型（LLMs）自动解释无损评估（NDE）轮廓图以提高桥梁维护效率的潜力，发现部分模型（如ChatGPT-4和Claude 3.5 Sonnet）在描述和总结桥梁状况方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 桥梁维护和安全至关重要，但传统NDE数据分析耗时且依赖专家，LLMs的进步为自动化分析提供了新途径。

Method: 研究设计了针对NDE轮廓图的提示词，评估了九种LLM模型在图像描述、缺陷识别、建议提供和准确性方面的表现，并总结了四种表现最佳模型的输出。

Result: 四种模型（包括ChatGPT-4和Claude 3.5 Sonnet）在描述桥梁状况方面表现优异，LLM辅助分析可提高效率且不降低准确性。

Conclusion: LLMs在桥梁维护中具有显著潜力，能通过并行图像标注和总结加速决策，提升基础设施管理和安全评估。

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [128] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1是一种基于强化学习的自动化CUDA优化框架，显著提升GPU计算效率，平均加速比达17.7倍，并展示出跨架构的优异移植性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型推动的GPU计算资源需求激增，亟需自动化CUDA优化策略。现有模型优化成功率低，CUDA-L1旨在填补这一空白。

Method: 采用强化学习框架，通过速度提升奖励信号训练模型，无需人工干预或领域知识。

Result: 在NVIDIA A100上平均加速17.7倍，峰值达449倍，并在多种GPU架构上表现优异。

Conclusion: CUDA-L1证明强化学习可高效优化CUDA代码，为GPU资源压力提供解决方案。

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>
