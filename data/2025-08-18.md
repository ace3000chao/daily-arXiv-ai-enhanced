<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.HC](#cs.HC) [Total: 27]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: A2HCoder是一个基于大语言模型的层次化算法到HDL编码代理，旨在高效可靠地实现算法到硬件的翻译，通过模块化和分步翻译减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统对超低延迟和低功耗的需求日益增长，但算法设计与硬件实现之间存在显著差距，传统方法需要大量专业知识和时间。

Method: A2HCoder采用层次化框架，水平维度分解算法为模块，垂直维度分步翻译并利用外部工具链调试和合成。

Result: 在5G无线通信领域的实际部署中验证了A2HCoder的实用性、可靠性和高效性。

Conclusion: A2HCoder通过模块化和分步翻译有效解决了算法到硬件翻译的挑战，提升了效率和可靠性。

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [2] [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
*Sihan Chen,John P. Lalor,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: PersonaTwin是一个多层次的提示条件框架，通过整合人口统计、行为和心理测量数据，构建自适应数字孪生，显著提升了用户模拟的真实性和情感细腻度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在捕捉用户多维细微差异方面存在不足，需要一种更精确的用户建模方法。

Method: 引入PersonaTwin框架，结合多种数据源，并通过医疗健康领域的8,500人数据集进行系统评估。

Result: 实验表明，PersonaTwin在模拟保真度上达到与oracle设置相当的水平，且下游模型在预测和公平性指标上接近基于真实个体的模型。

Conclusion: PersonaTwin展示了LLM数字孪生方法在个性化用户建模和行为分析中的潜力。

Abstract: While large language models (LLMs) afford new possibilities for user modeling
and approximation of human behaviors, they often fail to capture the
multidimensional nuances of individual users. In this work, we introduce
PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive
digital twins by integrating demographic, behavioral, and psychometric data.
Using a comprehensive data set in the healthcare context of more than 8,500
individuals, we systematically benchmark PersonaTwin against standard LLM
outputs, and our rigorous evaluation unites state-of-the-art text similarity
metrics with dedicated demographic parity assessments, ensuring that generated
responses remain accurate and unbiased. Experimental results show that our
framework produces simulation fidelity on par with oracle settings. Moreover,
downstream models trained on persona-twins approximate models trained on
individuals in terms of prediction and fairness metrics across both
GPT-4o-based and Llama-based models. Together, these findings underscore the
potential for LLM digital twin-based approaches in producing realistic and
emotionally nuanced user simulations, offering a powerful tool for personalized
digital user modeling and behavior analysis.

</details>


### [3] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: 介绍了两个开源推理模型gpt-oss-120b和gpt-oss-20b，采用混合专家架构，结合蒸馏和强化学习训练，具备强大的代理能力，并在数学、编码和安全基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 推动推理模型的准确性和推理成本的前沿，同时提供开源资源以促进广泛使用和进一步研究。

Method: 使用高效的混合专家Transformer架构，结合大规模蒸馏和强化学习训练，优化代理能力。

Result: 在数学、编码和安全基准测试中表现优异，模型权重及相关资源以Apache 2.0许可证开源。

Conclusion: gpt-oss-120b和gpt-oss-20b为开源社区提供了高性能的推理模型，支持广泛的研究和应用。

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [4] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: 研究构建了一个计算框架，用于从新闻文章中自动提取公司风险因素，比较了不同机器学习模型的性能，发现微调预训练语言模型优于零样本或少样本提示的LLMs。


<details>
  <summary>Details</summary>
Motivation: 识别公司风险对投资者和金融市场的健康至关重要，但现有方法可能不足。

Method: 提出包含七个方面的风险因素分类模式，标注744篇新闻文章，并测试多种机器学习模型。

Result: 微调预训练语言模型在大多数风险因素上表现优于LLMs（如LLaMA-2）。

Conclusion: 从新闻中识别风险因素能为公司和行业运营提供深入洞察。

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [5] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: Rule2Text框架利用大语言模型（LLM）为知识图谱的逻辑规则生成自然语言解释，提升可访问性和可用性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱的逻辑规则因复杂性和标签习惯难以理解，需改进解释方式。

Method: 结合多种LLM和提示策略（如零样本、少样本、思维链推理），并通过人类评估和LLM评估框架验证性能。

Result: 微调后的Zephyr模型在解释质量上显著提升，尤其在领域特定数据集中表现突出。

Conclusion: Rule2Text框架有效提升知识图谱规则的解释质量，支持类型推断模块，代码和数据已开源。

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [6] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: MDMs是一种新兴的非自回归生成模型，通过验证器辅助的推理时间扩展方法提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在离散数据生成中表现优异，但需要进一步提升生成质量。

Method: 提出基于验证器的推理时间扩展方法，结合预训练嵌入模型优化生成过程。

Result: 实验证明MDMs在文本风格转换任务中优于自回归模型，生成质量显著提升。

Conclusion: MDMs结合验证器是一种高效的非自回归生成框架，具有广泛应用潜力。

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [7] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 论文指出当前AI安全框架未充分考虑未成年人需求，提出SproutBench评估套件，揭示LLMs在未成年人使用中的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全框架主要针对成人，忽视了未成年人在认知、情感和社会层面的独特脆弱性，需重新评估。

Method: 引入SproutBench评估套件，包含1,283个针对性测试提示，评估47种LLMs的安全性。

Result: 发现LLMs在未成年人使用中存在显著安全隐患，如情感依赖和隐私风险，并揭示安全性与风险预防的相关性。

Conclusion: 研究为儿童中心AI设计提供了实用指南，强调需改进现有安全框架。

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [8] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: 研究探讨了大语言模型（LLM）在跨语言知识迁移中的问题，通过训练小型Transformer模型，揭示了跨语言统一表示的重要性及其影响因素。


<details>
  <summary>Details</summary>
Motivation: LLM在跨语言知识迁移中存在幻觉问题，即在不同语言间表达相同事实时表现不佳。研究旨在通过控制实验揭示这一现象的原因和动态。

Method: 通过训练小型Transformer模型，使用合成的多语言数据集，研究模型在不同语言间的事实表示方式及其影响因素。

Result: 研究发现，跨语言统一表示对知识迁移至关重要，且统一程度受事实与训练数据语言间的互信息及语言提取难度影响。

Conclusion: 研究通过控制实验揭示了跨语言迁移的动态机制，并提出了改进LLM跨语言迁移的新方向。

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [9] [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
*Andrew Wang,Sophia Hager,Adi Asija,Daniel Khashabi,Nicholas Andrews*

Main category: cs.CL

TL;DR: 论文研究了语言模型代理在复杂任务中面对外部失败时的备份计划能力，发现当前模型难以适应环境反馈并调整行动。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型代理在复杂任务中面对外部失败时的表现，以评估其备份计划能力。

Method: 设计了一个专门的规划基准测试，通过函数调用组合解决问题，并引入外部失败（如函数不可用）。

Result: 发现语言模型代理难以适应环境反馈并制定备份计划，即使搜索空间受限。

Conclusion: 当前生成模型在适应环境反馈方面存在挑战，未来研究需关注改进方向。

Abstract: As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

</details>


### [10] [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
*Martin Pavlíček,Tomáš Filip,Petr Sosík*

Main category: cs.CL

TL;DR: 论文提出了一种可重用、细粒度且主题无关的框架，用于评估大语言模型（LLM）中与极化相关的偏见，并通过俄罗斯-乌克兰战争的案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在下游任务中表现出偏见，尤其是在敏感话题上，但现有研究在偏见检测和缓解方面仍有不足。

Method: 结合极化敏感的情感指标和合成的平衡数据集，使用预定义的语义类别评估LLM的偏见。

Result: 研究发现LLM对乌克兰的情感普遍更积极，且在不同语义类别中存在显著差异。

Conclusion: 该框架支持自动化数据集生成和细粒度偏见评估，适用于多种极化驱动场景，并与其他偏见评估策略正交。

Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks,
especially when dealing with sensitive topics such as political discourse,
gender identity, ethnic relations, or national stereotypes. Although
significant progress has been made in bias detection and mitigation techniques,
certain challenges remain underexplored. This study proposes a reusable,
granular, and topic-agnostic framework to evaluate polarisation-related biases
in LLM (both open-source and closed-source). Our approach combines
polarisation-sensitive sentiment metrics with a synthetically generated
balanced dataset of conflict-related statements, using a predefined set of
semantic categories.
  As a case study, we created a synthetic dataset that focusses on the
Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,
Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with
a general trend for more positive sentiment toward Ukraine, the framework
allowed fine-grained analysis with considerable variation between semantic
categories, uncovering divergent behavioural patterns among models. Adaptation
to prompt modifications showed further bias towards preconceived language and
citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained
bias assessment, is applicable to a variety of polarisation-driven scenarios
and topics, and is orthogonal to many other bias-evaluation strategies.

</details>


### [11] [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
*Nicolas Goulet,Alexandre Blondin Massé,Moussa Abdendi*

Main category: cs.CL

TL;DR: 将数字词典嵌入AMR有向图中，利用预训练语言模型，并通过保形变换简化图，分析其性质与符号接地问题的关系。


<details>
  <summary>Details</summary>
Motivation: 探索如何将数字词典嵌入AMR有向图中，以解决语义表示中的符号接地问题。

Method: 使用预训练语言模型将数字词典嵌入AMR有向图，通过保形变换简化图。

Result: 分析了简化后的有向图性质，并讨论了其与符号接地问题的关联。

Conclusion: 该方法为语义表示和符号接地问题提供了新的研究视角。

Abstract: Abstract meaning representation (AMR) is a semantic formalism used to
represent the meaning of sentences as directed acyclic graphs. In this paper,
we describe how real digital dictionaries can be embedded into AMR directed
graphs (digraphs), using state-of-the-art pre-trained large language models.
Then, we reduce those graphs in a confluent manner, i.e. with transformations
that preserve their circuit space. Finally, the properties of these reduces
digraphs are analyzed and discussed in relation to the symbol grounding
problem.

</details>


### [12] [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
*Lorenzo Jaime Yu Flores,Junyi Shen,Xiaoyuan Gu*

Main category: cs.CL

TL;DR: 论文提出了一种名为RAMP的多智能体框架，用于营销任务中的受众筛选，通过迭代规划、工具调用、输出验证和生成改进建议来提高质量。结合长期记忆存储，该方法在评估查询中准确率提升了28个百分点。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在复杂任务中表现出色，但其在真实应用中的可靠性研究有限。本文旨在通过多智能体框架解决营销中的受众筛选问题。

Method: 提出RAMP框架，结合迭代规划、工具调用、输出验证和长期记忆存储（客户特定事实和过去查询的知识库）。

Result: 在88个评估查询中准确率提升28个百分点；模糊查询中，迭代验证和反思显著提高了召回率（约+20个百分点）和用户满意度。

Conclusion: 研究为在动态行业环境中部署可靠的LLM系统提供了实用见解。

Abstract: Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

</details>


### [13] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: MoNaCo是一个包含1,315个自然且复杂问题的基准测试，旨在评估大型语言模型（LLMs）在解决耗时信息查询问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试缺乏自然且耗时的问题，MoNaCo填补了这一空白。

Method: 通过分解标注流程，手动构建并回答大量自然耗时问题。

Result: 前沿LLMs在MoNaCo上最高仅达到61.2% F1，表现受限于低召回率和幻觉问题。

Conclusion: MoNaCo为跟踪模型在处理复杂信息查询问题上的进展提供了有效资源。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [14] [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
*Hikaru Asano,Hiroki Ouchi,Akira Kasuga,Ryo Yonetani*

Main category: cs.CL

TL;DR: MobQA是一个用于评估大型语言模型（LLMs）对人类移动数据语义理解能力的基准数据集，通过自然语言问答形式测试模型对GPS轨迹的语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有模型在预测人类移动模式方面表现优异，但其对模式背后原因或语义的理解能力尚不明确。MobQA旨在填补这一空白。

Method: MobQA包含5,800个高质量问答对，涵盖三种问题类型：事实检索、多选推理和自由解释，均需时空和语义推理。

Result: 评估显示，LLMs在事实检索上表现优异，但在语义推理和解释问题上存在显著局限，轨迹长度对模型效果影响较大。

Conclusion: MobQA揭示了当前LLMs在语义移动理解方面的成就与局限，为未来研究提供了方向。

Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

</details>


### [15] [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
*Anusha M D,Deepthi Vikram,Bharathi Raja Chakravarthi,Parameshwar R Hegde*

Main category: cs.CL

TL;DR: 本研究为低资源德拉威语Tulu构建了首个社交媒体混合代码的冒犯性语言识别基准数据集，评估了多种深度学习模型，发现BiGRU表现最佳，而多语言预训练模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: Tulu作为一种低资源语言，缺乏计算资源，但其数字存在日益增长，需要建立冒犯性语言识别的基准数据集。

Method: 从YouTube评论中收集并标注了3,845条混合代码的Tulu内容，评估了GRU、LSTM、BiGRU、BiLSTM、CNN、注意力机制及Transformer模型。

Result: BiGRU模型表现最佳（82%准确率，0.81宏F1分数），Transformer模型表现不佳。

Conclusion: 本研究为Tulu及其他低资源混合代码语言的NLP研究奠定了基础。

Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

</details>


### [16] [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
*Tao Wu,Jingyuan Chen,Wang Lin,Jian Zhan,Mengze Li,Kun Kuang,Fei Wu*

Main category: cs.CL

TL;DR: 论文提出了一种个性化干扰项生成方法，通过分析学生的历史答题记录生成定制化干扰项，以更精准地诊断个体学生的错误推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的干扰项生成方法只能捕捉群体层面的错误模式，无法满足个体学生的诊断需求。

Method: 采用无训练的两阶段框架：第一阶段通过蒙特卡洛树搜索（MCTS）从学生错误答案中重建推理轨迹；第二阶段基于此轨迹生成个性化干扰项。

Result: 实验表明，该方法在140名学生中生成个性化干扰项的效果最佳，并能推广到群体层面。

Conclusion: 该方法通过个性化干扰项生成，显著提升了对学生错误推理的诊断能力，具有鲁棒性和适应性。

Abstract: Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

</details>


### [17] [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
*Chenyang Le,Yinfeng Xia,Huiyan Li,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: 提出了一种创新的寄生双尺度方法（Parasitic Dual-Scale Approach），结合增强的推测采样、模型压缩和知识蒸馏技术，显著提升了多语言语音翻译模型的推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前多语言语音翻译模型参数量大，难以在本地部署中平衡推理效率和性能。

Method: 基于Whisper Medium模型，改进为whisperM2M，并集成KVSPN模块，结合推测采样、压缩和蒸馏技术。

Result: 在六种流行语言上实现SOTA性能，KVSPN模块带来40%的速度提升且BLEU分数无下降，整体速度提升2.6倍。

Conclusion: 该方法在保持性能的同时显著提升推理效率，适用于本地部署场景。

Abstract: Recent advancements in speech-to-text translation have led to the development
of multilingual models capable of handling multiple language pairs
simultaneously. However, these unified models often suffer from large parameter
sizes, making it challenging to balance inference efficiency and performance,
particularly in local deployment scenarios. We propose an innovative Parasitic
Dual-Scale Approach, which combines an enhanced speculative sampling method
with model compression and knowledge distillation techniques. Building on the
Whisper Medium model, we enhance it for multilingual speech translation into
whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art
(SOTA) performance across six popular languages with improved inference
efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation.
Combined with distillation methods, it represents a 2.6$\times$ speedup over
the original Whisper Medium with superior performance.

</details>


### [18] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: E-CaTCH是一个可解释且可扩展的框架，用于检测社交媒体上的多模态虚假信息，通过聚类、特征提取、跨模态对齐和时间建模，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 检测多模态虚假信息面临模态不一致、时间模式变化和类别不平衡的挑战，现有方法未能捕捉事件级结构。

Method: E-CaTCH通过聚类伪事件、提取文本和视觉特征、跨模态对齐、时间建模（LSTM增强）和事件级分类来检测虚假信息。

Result: 在多个数据集上，E-CaTCH表现优于现有基线，并展示了跨数据集的鲁棒性和通用性。

Conclusion: E-CaTCH通过事件级建模和跨模态对齐，有效解决了虚假信息检测的挑战，具有实际应用价值。

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [19] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: 提出了一种基于超图的跨粒度信息整合方法HGRAG，用于多跳问答任务，显著提升了问答性能和检索效率。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成方法在多跳问答中因忽略知识的结构关联而效果受限，而基于知识图的方法又过度依赖结构信息。HGRAG旨在通过超图整合结构和语义信息。

Method: 构建实体超图，节点为细粒度实体，超边为粗粒度段落；设计超图检索方法，结合实体和段落相似性；通过检索增强模块优化结果。

Result: 在基准数据集上表现优于现有方法，问答性能提升，检索效率提高6倍。

Conclusion: HGRAG通过跨粒度信息整合，有效解决了多跳问答中的知识关联问题，显著提升了性能。

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [20] [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
*Mukund Choudhary,KV Aditya Srivatsa,Gaurja Aeron,Antara Raaghavi Bhattacharya,Dang Khoa Dang Dinh,Ikhlasul Akmal Hanif,Daria Kotova,Ekaterina Kochmar,Monojit Choudhury*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在语言学谜题上表现不佳，尤其是在形态复杂度高的语言中。通过将单词分解为词素可提升表现，表明需要更智能的语言特定分词器。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在低资源语言中的语言学推理能力，揭示其弱点。

Method: 分析629个问题，覆盖41种低资源语言，标记语言特征以识别模型表现。

Result: LLMs在形态复杂度高的谜题中表现差，但在与英语相似的语言特征中表现较好；词素分解可提升表现。

Conclusion: 研究揭示了LLMs在语言学推理和低资源语言建模中的挑战，建议改进分词器设计。

Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks,
but their performance on linguistics puzzles remains consistently poor. These
puzzles, often derived from Linguistics Olympiad (LO) contests, provide a
minimal contamination environment to assess LLMs' linguistic reasoning
abilities across low-resource languages. This work analyses LLMs' performance
on 629 problems across 41 low-resource languages by labelling each with
linguistically informed features to unveil weaknesses. Our analyses show that
LLMs struggle with puzzles involving higher morphological complexity and
perform better on puzzles involving linguistic features that are also found in
English. We also show that splitting words into morphemes as a pre-processing
step improves solvability, indicating a need for more informed and
language-specific tokenisers. These findings thus offer insights into some
challenges in linguistic reasoning and modelling of low-resource languages.

</details>


### [21] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: LETToT框架通过专家推理结构评估旅游领域的大语言模型（LLMs），无需标注数据，效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法成本高且存在幻觉问题，需要一种无需标注数据的领域特定评估方法。

Method: 通过专家反馈优化层次化思维树（ToT），并将其应用于不同规模的LLMs评估。

Result: 专家ToT相对基线质量提升4.99-14.15%；小规模模型通过增强推理能力缩小与大规模模型的差距。

Conclusion: LETToT为领域特定LLM评估提供了一种可扩展、无需标注的替代方案。

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [22] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: 论文介绍了TOXIFRENCH，一个法语毒性内容检测的新基准数据集，并提出了一种动态加权损失的CoT微调策略，使得小型语言模型在性能上超越大型模型。


<details>
  <summary>Details</summary>
Motivation: 法语毒性检测因缺乏大规模数据集而发展不足，需要构建高质量数据集并提升模型性能。

Method: 通过半自动标注流程构建TOXIFRENCH数据集，并提出动态加权损失的CoT微调策略。

Result: 4B模型在毒性检测任务中表现最佳，F1分数提升13%，并展示了跨语言能力。

Conclusion: 该方法可推广至其他语言和安全关键任务，小型模型在特定任务中可能优于大型模型。

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [23] [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
*Arya VarastehNezhad,Reza Tavasoli,Soroush Elyasi,MohammadHossein LotfiNia,Hamed Farbeh*

Main category: cs.CL

TL;DR: 研究分析了8种LLM对抑郁、焦虑和压力问题的情感回复，发现不同模型和心理健康问题类型显著影响情感表达，而用户人口统计特征影响较小。


<details>
  <summary>Details</summary>
Motivation: 调查LLM在心理健康问题回复中的情感表达差异，以评估其对用户体验的潜在影响。

Method: 使用6种用户画像对8种LLM提问20个心理健康问题，生成2,880个回答，并通过情感分析工具评分。

Result: 不同LLM的情感表达差异显著，心理健康问题类型（如焦虑、抑郁、压力）对情感影响较大，而人口统计特征影响较小。

Conclusion: LLM的选择对心理健康应用至关重要，因其情感表达可能显著影响用户体验和结果。

Abstract: Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

</details>


### [24] [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
*Utsav Maskey,Sumit Yadav,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 论文提出SafeConstellations方法，通过跟踪任务特定的轨迹模式，减少LLMs的过度拒绝行为，提升实用性。


<details>
  <summary>Details</summary>
Motivation: LLMs的安全机制导致模型拒绝看似有害但实际无害的指令，降低了生产应用中的实用性。

Method: 通过嵌入空间中的轨迹分析，提出SafeConstellations方法，选择性引导模型行为。

Result: 方法将过度拒绝率降低高达73%，且对实用性影响最小。

Conclusion: SafeConstellations为减少过度拒绝提供了一种有效且原则性的解决方案。

Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms
cause models to reject benign instructions that superficially resemble harmful
content. This phenomena diminishes utility in production applications that
repeatedly rely on common prompt templates or applications that frequently rely
on LLMs for specific tasks (e.g. sentiment analysis, language translation).
Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse
responses to harmful instructions when those instructions are reframed to
appear as benign tasks. Our mechanistic analysis reveal that LLMs follow
distinct "constellation" patterns in embedding space as representations
traverse layers, with each task maintaining consistent trajectories that shift
predictably between refusal and non-refusal cases. We introduce
SafeConstellations, an inference-time trajectory-shifting approach that tracks
task-specific trajectory patterns and guides representations toward non-refusal
pathways. By selectively guiding model behavior only on tasks prone to
over-refusal, and by preserving general model behavior, our method reduces
over-refusal rates by up to 73% with minimal impact on utility-offering a
principled approach to mitigating over-refusals.

</details>


### [25] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: SGSimEval是一个用于自动调查生成（ASG）的综合评估基准，通过结合大纲、内容和参考的评估，以及LLM评分和定量指标，提供多方面的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在偏见、缺乏人类偏好和过度依赖LLMs-as-judges的问题，需要更全面的评估方法。

Method: 提出SGSimEval，整合大纲、内容和参考的评估，结合LLM评分和定量指标，并引入人类偏好指标。

Result: 实验显示ASG系统在大纲生成上表现接近人类，但在内容和参考生成上仍有改进空间，评估指标与人类评估一致性强。

Conclusion: SGSimEval为ASG提供了更全面的评估方法，未来需进一步优化内容和参考生成。

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [26] [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
*Sahil Sk,Debasish Dhal,Sonal Khosla,Sk Shahid,Sambit Shekhar,Akash Dhaka,Shantipriya Parida,Dilip K. Prasad,Ondřej Bojar*

Main category: cs.CL

TL;DR: 研究探讨了4位组缩放量化（GSQ）和生成预训练变换器量化（GPTQ）对LLaMA 1B、Qwen 0.5B和PHI 1.5B模型的影响，评估了其在多个NLP任务中的表现和效率。


<details>
  <summary>Details</summary>
Motivation: 通过量化技术减少大型语言模型的内存占用和计算成本，同时保持性能，以提高其可访问性。

Method: 应用GSQ和GPTQ技术对三种模型进行4位量化，并在MS MARCO、BoolQ和GSM8K数据集上评估其准确性和效率。

Result: 研究分析了量化对模型性能的影响，包括准确性、推理延迟和吞吐量，为实际部署提供了参考。

Conclusion: 量化技术在模型压缩和性能之间存在权衡，研究结果为用户选择合适的技术提供了依据，并为未来实验设定了基准。

Abstract: Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

</details>


### [27] [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
*Haitong Luo,Weiyao Zhang,Suhang Wang,Wenji Zou,Chungang Lin,Xuying Meng,Yujun Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种基于信号处理的新方法（SpecDetect和SpecDetect++），通过分析文本生成过程中的频谱特性来检测LLM生成的文本，性能优于现有方法且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法依赖表面统计特征，忽略了文本生成过程的信号特性，需要更可靠和高效的检测手段。

Method: 将检测问题转化为信号处理问题，利用全局离散傅里叶变换（DFT）和局部短时傅里叶变换（STFT）分析文本的频谱能量特性。

Result: 实验表明，该方法在性能和效率上均优于现有技术，运行时间减少近一半。

Conclusion: 信号处理技术为LLM生成文本检测提供了高效且可解释的新途径。

Abstract: The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

</details>


### [28] [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
*Sylvio Rüdian,Yassin Elsir,Marvin Kretschmer,Sabine Cayrou,Niels Pinkwart*

Main category: cs.CL

TL;DR: 论文探讨了利用大语言模型Llama 3.1从学生提交的语言学习课程中提取反馈指标的方法，并验证了其与人工评分的强相关性。


<details>
  <summary>Details</summary>
Motivation: 自动化反馈生成可以提升学生学习效率并帮助教师优化时间，但需要先提取高质量的反馈指标。

Method: 使用Llama 3.1从学生提交中提取反馈指标，并与人工评分进行对比分析。

Result: 发现LLM生成的指标与人工评分在多种反馈标准上具有显著强相关性。

Conclusion: 该方法为利用LLMs提取反馈指标提供了基础，未来可用于生成透明且可解释的形成性反馈。

Abstract: Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

</details>


### [29] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 本文系统评估了5种提高提示鲁棒性的方法，在统一框架下测试了8个模型和52个任务，并扩展到前沿模型GPT-4.1和DeepSeek V3。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）对提示的细微变化敏感，需系统性评估提升鲁棒性的方法。

Method: 在统一实验框架下，评估5种鲁棒性方法，涵盖微调和上下文学习范式，测试其在52个任务和8个模型上的表现。

Result: 提供了不同鲁棒性方法的相对有效性，帮助实践者在实际应用中实现稳定性能。

Conclusion: 研究为提升LLM在真实场景中的鲁棒性提供了实用指导。

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [30] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: 提出了一种结合推理与检索增强生成（RAG）的轻量级语言模型架构，适用于资源受限或安全环境。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统依赖大规模模型和外部API的问题，满足对高效且隐私保护的本地部署需求。

Method: 集成密集检索器与微调的Qwen2.5-Instruct模型，利用合成查询生成和前沿模型推理轨迹，探索文档压缩与数据设计对性能的影响。

Result: 领域特定微调显著提升答案准确性和一致性，接近前沿模型性能，适合本地部署。

Conclusion: 该方法在轻量级架构中实现了高性能，支持跨领域复现和适应。

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [31] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 提出了一种基于梯度优化和正则化的新方法，用于生成神经网络预测的提取性解释，适用于文本和图像输入。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络模型的发展，解释其预测的需求日益增长。

Method: 通过梯度优化和新的正则化方案，掩蔽输入中与预测无关的部分，确保解释的充分性、全面性和紧凑性。

Result: 方法在文本和图像分类任务中均能生成高质量的解释。

Conclusion: 该方法无需专门训练模型，仅基于分类器即可实现解释生成，且适用于多种输入类型。

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [32] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 提出了一种端到端可微分的训练范式，用于稳定训练基于理性化Transformer的分类器，简化了传统的三玩家游戏方法，并扩展到生成类别相关的理性化结果。


<details>
  <summary>Details</summary>
Motivation: 解决现有理性化模型训练中的不稳定问题，并提升理性化结果与人类标注的对齐性。

Method: 通过单一模型同时实现理性化选择、分类和互补分类功能，简化训练流程，并引入类别相关的理性化参数化和正则化。

Result: 显著提升了理性化结果与人类标注的对齐性，达到最先进水平，且无需显式监督。

Conclusion: 该方法简化了训练流程，提高了稳定性和性能，为理性化模型提供了更高效的解决方案。

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [33] [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
*Shangrui Nie,Florian Mai,David Kaczér,Charles Welch,Zhixue Zhao,Lucie Flek*

Main category: cs.CL

TL;DR: 通过微调模型回答价值观调查问题，可以显著改变其在下游任务中的行为，实现价值对齐。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过简单方法（如微调价值观调查）调整大型语言模型的隐含价值观，避免大量训练数据的需求。

Method: 构建价值观基线，微调模型回答价值观调查问题，评估其对领域内和领域外任务行为的影响。

Result: 微调不仅能改变模型对调查问题的回答，还能显著影响其在下游任务中的行为。

Conclusion: 简单微调方法可有效调整模型价值观，实现价值对齐。

Abstract: Large language models implicitly encode preferences over human values, yet
steering them often requires large training data. In this work, we investigate
a simple approach: Can we reliably modify a model's value system in downstream
behavior by training it to answer value survey questions accordingly? We first
construct value profiles of several open-source LLMs by asking them to rate a
series of value-related descriptions spanning 20 distinct human values, which
we use as a baseline for subsequent experiments. We then investigate whether
the value system of a model can be governed by fine-tuning on the value
surveys. We evaluate the effect of finetuning on the model's behavior in two
ways; first, we assess how answers change on in-domain, held-out survey
questions. Second, we evaluate whether the model's behavior changes in
out-of-domain settings (situational scenarios). To this end, we construct a
contextualized moral judgment dataset based on Reddit posts and evaluate
changes in the model's behavior in text-based adventure games. We demonstrate
that our simple approach can not only change the model's answers to in-domain
survey questions, but also produces substantial shifts (value alignment) in
implicit downstream task behavior.

</details>


### [34] [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
*Shivam Dubey*

Main category: cs.CL

TL;DR: HumorPlanSearch通过模块化流程提升LLM生成幽默的质量，结合上下文建模和多信号评估，显著提高幽默生成分数。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成幽默时因缺乏上下文和文化敏感性导致的通用、重复或不合时宜的问题。

Method: 提出HumorPlanSearch，包括Plan-Search、Humor Chain-of-Thought模板、知识图谱、新颖性过滤和迭代修订循环。

Result: 实验显示，完整流程（知识图谱+修订）将平均幽默生成分数（HGS）提升15.4%（p < 0.05）。

Conclusion: HumorPlanSearch通过全程上下文建模，推动AI幽默生成向更具连贯性、适应性和文化敏感性的方向发展。

Abstract: Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

</details>


### [35] [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
*Aditi Dutta,Susan Banducci*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在区分反性别歧视言论与性别歧视言论时的困难，发现模型常将反性别歧视言论误判为有害内容，尤其在政治敏感事件中。


<details>
  <summary>Details</summary>
Motivation: 反性别歧视言论在塑造在线民主辩论中至关重要，但自动化内容审核系统可能难以区分其与性别歧视言论。

Method: 分析了五种LLMs对英国2022年涉及女性议员的推文（性别歧视、反性别歧视和中性）的分类表现。

Result: 模型常将反性别歧视言论误判为有害，尤其在政治敏感事件中，可能导致边缘化声音被压制。

Conclusion: 建议审核设计需超越二元分类，整合人工审核，并在训练数据中明确包含反性别歧视言论。

Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist
gendered abuse and sexism, plays a vital role in shaping democratic debate
online. Yet automated content moderation systems, increasingly powered by large
language models (LLMs), may struggle to distinguish such resistance from the
sexism it opposes. This study examines how five LLMs classify sexist,
anti-sexist, and neutral political tweets from the UK, focusing on
high-salience trigger events involving female Members of Parliament in the year
2022. Our analysis show that models frequently misclassify anti-sexist speech
as harmful, particularly during politically charged events where rhetorical
styles of harm and resistance converge. These errors risk silencing those who
challenge sexism, with disproportionate consequences for marginalised voices.
We argue that moderation design must move beyond binary harmful/not-harmful
schemas, integrate human-in-the-loop review during sensitive events, and
explicitly include counter-speech in training data. By linking feminist
scholarship, event-based analysis, and model evaluation, this work highlights
the sociotechnical challenges of safeguarding resistance speech in digital
political spaces.

</details>


### [36] [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
*Bowen Zhang,Zixin Song,Chunquan Chen,Qian-Wen Zhang,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: CoDiEmb框架通过任务专用目标、动态采样和模型融合策略，解决了信息检索（IR）和语义文本相似性（STS）联合训练中的负迁移问题，显著提升了嵌入空间的几何特性。


<details>
  <summary>Details</summary>
Motivation: 联合训练IR和STS任务时，负迁移问题导致性能下降，需要系统性地解耦任务特定的学习信号。

Method: CoDiEmb框架包含任务专用目标与动态采样、delta引导的模型融合策略，以及高效的单阶段训练流程。

Result: 在15个标准基准测试中，CoDiEmb不仅缓解了跨任务性能折衷，还改善了嵌入空间的几何特性。

Conclusion: CoDiEmb通过创新设计有效解决了IR和STS联合训练的挑战，为统一文本嵌入学习提供了新思路。

Abstract: Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

</details>


### [37] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究探讨了如何通过结构化提示（JSON格式）提升轻量级LLM在情感分析中的性能，结果显示JSON格式优于自然语言格式，且无需微调即可部署。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析多基于文本，但营销理论指出客户评价还受其他参考点影响，因此研究补充信息对LLM情感分析的影响。

Method: 比较自然语言和JSON格式提示，使用3B参数轻量级模型，在Yelp的餐厅和夜生活类别上进行实验。

Result: JSON格式提示性能优于基线，Macro-F1提升1.6%和4%，RMSE降低16%和9.1%，适合资源受限设备。

Conclusion: 结构化提示可使小模型达到竞争性能，为大规模模型部署提供实用替代方案。

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [38] [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
*Monika Jotautaitė,Lucius Caviola,David A. Brewster,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）是否表现出物种主义偏见，并通过三个范式系统评估其对非人类动物的道德评价。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，研究其伦理倾向，尤其是物种主义偏见，对AI公平性和道德对齐至关重要。

Method: 通过三个范式评估：SpeciesismBench基准测试、心理学测量比较人类与模型反应、文本生成任务分析物种主义合理化。

Result: LLMs能识别物种主义言论但很少谴责，表现出较低的显性物种主义，但在直接权衡中更倾向于人类。模型可能基于认知能力而非物种本身做决策。

Conclusion: LLMs反映了混合的人类观点，但仍复制了动物剥削的文化规范。建议扩展AI公平性框架以明确包含非人类道德主体，减少物种主义偏见。

Abstract: As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

</details>


### [39] [Language models align with brain regions that represent concepts across modalities](https://arxiv.org/abs/2508.11536)
*Maria Ryskina,Greta Tuckute,Alexander Fung,Ashley Malkin,Evelina Fedorenko*

Main category: cs.CL

TL;DR: 研究探讨了语言模型（LMs）与大脑对齐的关系，发现语言模型可能在内部表征跨模态的概念意义。


<details>
  <summary>Details</summary>
Motivation: 认知科学和神经科学长期面临区分语言表征与概念意义表征的挑战，语言模型也遇到同样问题。

Method: 通过两个神经指标（大脑激活水平和跨模态意义一致性）分析LM-大脑对齐，使用fMRI数据集。

Result: 实验表明，语言模型在意义一致性更强的脑区预测信号更好，即使这些区域对语言处理不敏感。

Conclusion: 语言模型可能内部表征跨模态的概念意义。

Abstract: Cognitive science and neuroscience have long faced the challenge of
disentangling representations of language from representations of conceptual
meaning. As the same problem arises in today's language models (LMs), we
investigate the relationship between LM--brain alignment and two neural
metrics: (1) the level of brain activation during processing of sentences,
targeting linguistic processing, and (2) a novel measure of meaning consistency
across input modalities, which quantifies how consistently a brain region
responds to the same concept across paradigms (sentence, word cloud, image)
using an fMRI dataset (Pereira et al., 2018). Our experiments show that both
language-only and language-vision models predict the signal better in more
meaning-consistent areas of the brain, even when these areas are not strongly
sensitive to language processing, suggesting that LMs might internally
represent cross-modal conceptual meaning.

</details>


### [40] [AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment](https://arxiv.org/abs/2508.11567)
*Jinpeng Hu,Ao Wang,Qianqian Xie,Hui Ma,Zhuo Li,Dan Guo*

Main category: cs.CL

TL;DR: 提出了一种基于多智能体的心理健康评估框架，通过模拟医患对话动态提取信息，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统心理健康评估依赖专业人员且静态文本分析能力有限，无法捕捉动态交互中的深层信息。

Method: 采用多智能体框架，包括提问、评估、评分和更新模块，结合自适应提问机制和树状记忆结构。

Result: 在DAIC-WOZ数据集上表现优于现有方法。

Conclusion: 该框架能有效提升心理健康评估的动态信息提取和上下文跟踪能力。

Abstract: Mental health assessment is crucial for early intervention and effective
treatment, yet traditional clinician-based approaches are limited by the
shortage of qualified professionals. Recent advances in artificial intelligence
have sparked growing interest in automated psychological assessment, yet most
existing approaches are constrained by their reliance on static text analysis,
limiting their ability to capture deeper and more informative insights that
emerge through dynamic interaction and iterative questioning. Therefore, in
this paper, we propose a multi-agent framework for mental health evaluation
that simulates clinical doctor-patient dialogues, with specialized agents
assigned to questioning, adequacy evaluation, scoring, and updating. We
introduce an adaptive questioning mechanism in which an evaluation agent
assesses the adequacy of user responses to determine the necessity of
generating targeted follow-up queries to address ambiguity and missing
information. Additionally, we employ a tree-structured memory in which the root
node encodes the user's basic information, while child nodes (e.g., topic and
statement) organize key information according to distinct symptom categories
and interaction turns. This memory is dynamically updated throughout the
interaction to reduce redundant questioning and further enhance the information
extraction and contextual tracking capabilities. Experimental results on the
DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which
achieves better performance than existing approaches.

</details>


### [41] [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
*Qiguang Chen,Dengyun Peng,Jinhao Liu,HuiKang Su,Jiannan Guan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: DR. SAF框架通过动态调整推理深度，显著提升大语言模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决长链推理（CoT）中的冗余问题，提高计算效率，避免实时应用中的延迟。

Method: 引入DR. SAF框架，包含边界自感知对齐、自适应奖励管理和边界保护机制。

Result: 减少49.27%的响应token，提升6.59倍token效率，训练时间减少5倍，极端训练下准确率提升16%。

Conclusion: DR. SAF在资源受限环境中高效平衡效率与准确性，优于传统方法。

Abstract: Recent advancements in large language models (LLMs) have greatly improved
their capabilities on complex reasoning tasks through Long Chain-of-Thought
(CoT). However, this approach often results in substantial redundancy,
impairing computational efficiency and causing significant delays in real-time
applications. To improve the efficiency, current methods often rely on
human-defined difficulty priors, which do not align with the LLM's self-awared
difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic
Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to
dynamically assess and adjust their reasoning depth in response to problem
complexity. DR. SAF integrates three key components: Boundary Self-Awareness
Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.
These components allow models to optimize their reasoning processes, balancing
efficiency and accuracy without compromising performance. Our experimental
results demonstrate that DR. SAF achieves a 49.27% reduction in total response
tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain
in token efficiency and a 5x reduction in training time, making it well-suited
to resource-limited settings. During extreme training, DR. SAF can even surpass
traditional instruction-based models in token efficiency with more than 16%
accuracy improvement.

</details>


### [42] [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598)
*Greta Tuckute,Klemen Kotar,Evelina Fedorenko,Daniel L. K. Yamins*

Main category: cs.CL

TL;DR: AuriStream是一个受生物启发的两阶段语音编码模型，模拟人类听觉处理层次结构，在语音任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发更接近人类听觉处理的模型，以高效处理多种语音任务。

Method: 第一阶段将原始音频转换为基于人类耳蜗的时频表示，提取离散的耳蜗标记；第二阶段对耳蜗标记应用自回归序列模型。

Result: AuriStream在SUPERB语音任务中表现优异，能够学习有意义的音素和词汇表示，并生成可解码的音频延续。

Conclusion: AuriStream为语音表示学习提供了一个高效的两阶段框架，推动了更接近人类听觉的模型发展。

Abstract: We introduce AuriStream, a biologically inspired model for encoding speech
via a two-stage framework inspired by the human auditory processing hierarchy.
The first stage transforms raw audio into a time-frequency representation based
on the human cochlea, from which we extract discrete \textbf{cochlear tokens}.
The second stage applies an autoregressive sequence model over the cochlear
tokens. AuriStream learns meaningful phoneme and word representations, and
state-of-the-art lexical semantics. AuriStream shows competitive performance on
diverse downstream SUPERB speech tasks. Complementing AuriStream's strong
representational capabilities, it generates continuations of audio which can be
visualized in a spectrogram space and decoded back into audio, providing
insights into the model's predictions. In summary, we present a two-stage
framework for speech representation learning to advance the development of more
human-like models that efficiently handle a range of speech-based tasks.

</details>


### [43] [Dataset Creation for Visual Entailment using Generative AI](https://arxiv.org/abs/2508.11605)
*Rob Reijtenbach,Suzan Verberne,Gijs Wijnholds*

Main category: cs.CL

TL;DR: 提出并验证了一种用于视觉蕴含模型训练的新合成数据集，基于SNLI数据集生成图像，性能接近真实数据。


<details>
  <summary>Details</summary>
Motivation: 现有视觉蕴含数据集规模小且稀疏，手动创建耗时耗力。

Method: 利用SNLI文本前提通过Stable Diffusion生成图像，构建合成数据集，并通过CLIP特征向量训练视觉蕴含分类器进行验证。

Result: 合成数据训练的性能略低于真实数据（SNLI-VE F-score 0.686 vs 0.703；SICK-VTE 0.384 vs 0.400）。

Conclusion: 在数据稀疏场景下，合成数据是训练视觉蕴含模型的有效替代方案。

Abstract: In this paper we present and validate a new synthetic dataset for training
visual entailment models. Existing datasets for visual entailment are small and
sparse compared to datasets for textual entailment. Manually creating datasets
is labor-intensive. We base our synthetic dataset on the SNLI dataset for
textual entailment. We take the premise text from SNLI as input prompts in a
generative image model, Stable Diffusion, creating an image to replace each
textual premise. We evaluate our dataset both intrinsically and extrinsically.
For extrinsic evaluation, we evaluate the validity of the generated images by
using them as training data for a visual entailment classifier based on CLIP
feature vectors. We find that synthetic training data only leads to a slight
drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when
trained on real data. We also compare the quality of our generated training
data to original training data on another dataset: SICK-VTE. Again, there is
only a slight drop in F-score: from 0.400 to 0.384. These results indicate that
in settings with data sparsity, synthetic data can be a promising solution for
training visual entailment models.

</details>


### [44] [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
*Christopher J. Agostino*

Main category: cs.CL

TL;DR: TinyTim是一个基于James Joyce的《Finnegans Wake》微调的大型语言模型家族，其V1版本在生成文本时表现出高词汇多样性和低语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过专门的语言模型作为创造性架构中的发散知识源，支持自动化发现机制。

Method: 对《Finnegans Wake》进行微调，并通过定量评估与基线模型对比。

Result: TinyTim V1生成文本的词汇多样性高，但语义连贯性低。

Conclusion: 这类专门化模型可作为创造性架构中的发散知识源，推动自动化发现。

Abstract: This work introduces TinyTim, a family of large language models fine-tuned on
James Joyce's `Finnegans Wake'. Through quantitative evaluation against
baseline models, we demonstrate that TinyTim V1 produces a statistically
distinct generative profile characterized by high lexical diversity and low
semantic coherence. These findings are interpreted through theories of
creativity and complex problem-solving, arguing that such specialized models
can function as divergent knowledge sources within more extensive creative
architectures, powering automated discovery mechanisms in diverse settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [45] [Grounding Rule-Based Argumentation Using Datalog](https://arxiv.org/abs/2508.10976)
*Martin Diller,Sarah Alice Gaggl,Philipp Hanisch,Giuseppina Monterosso,Fritz Rauschenbach*

Main category: cs.AI

TL;DR: ASPIC+框架缺乏对一阶规则推理的智能基础处理，本文提出了一种基于Datalog的智能基础方法，并通过实验验证了其可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有ASPIC+框架仅支持命题规则推理，一阶规则需要基础化处理，但传统方法会导致输入理论规模指数级增长，缺乏专用解决方案。

Method: 将一阶ASPIC+实例转化为Datalog程序，利用Datalog引擎获取基础替换，并针对ASPIC+形式化提出简化方法以减少不必要的基础化。

Result: 提出了一种智能基础化方法，有效控制基础化规模并保持推理正确性，原型实现验证了其可扩展性。

Conclusion: 本文方法为一阶ASPIC+推理提供了高效的基础化解决方案，并通过实验证明了其可行性。

Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for
AI. Although first-order rules are commonly used in ASPIC+ examples, most
existing approaches to reason over rule-based argumentation only support
propositional rules. To enable reasoning over first-order instances, a
preliminary grounding step is required. As groundings can lead to an
exponential increase in the size of the input theories, intelligent procedures
are needed. However, there is a lack of dedicated solutions for ASPIC+.
Therefore, we propose an intelligent grounding procedure that keeps the size of
the grounding manageable while preserving the correctness of the reasoning
process. To this end, we translate the first-order ASPIC+ instance into a
Datalog program and query a Datalog engine to obtain ground substitutions to
perform the grounding of rules and contraries. Additionally, we propose
simplifications specific to the ASPIC+ formalism to avoid grounding of rules
that have no influence on the reasoning process. Finally, we performed an
empirical evaluation of a prototypical implementation to show scalability.

</details>


### [46] [From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching](https://arxiv.org/abs/2508.11070)
*Zahra Khotanlou,Kate Larson,Amir-Hossein Karimi*

Main category: cs.AI

TL;DR: 该论文提出了一种多主体算法追索框架，解决多追索寻求者和提供者之间的交互问题，通过三层优化实现社会福祉最大化。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，算法追索通常涉及多主体交互和资源竞争，而现有研究多关注单主体场景，忽略了多主体系统的复杂性。

Method: 将多主体交互建模为带权二分图匹配问题，提出三层优化框架：基础匹配、容量再分配和成本感知优化。

Result: 实验验证表明，该框架能在系统设置最小修改下实现接近最优的社会福祉。

Conclusion: 该研究将算法追索从个体推荐扩展到系统设计，为社会福祉提升提供了可行路径。

Abstract: Decision makers are increasingly relying on machine learning in sensitive
situations. In such settings, algorithmic recourse aims to provide individuals
with actionable and minimally costly steps to reverse unfavorable AI-driven
decisions. While existing research predominantly focuses on single-individual
(i.e., seeker) and single-model (i.e., provider) scenarios, real-world
applications often involve multiple interacting stakeholders. Optimizing
outcomes for seekers under an individual welfare approach overlooks the
inherently multi-agent nature of real-world systems, where individuals interact
and compete for limited resources. To address this, we introduce a novel
framework for multi-agent algorithmic recourse that accounts for multiple
recourse seekers and recourse providers. We model this many-to-many interaction
as a capacitated weighted bipartite matching problem, where matches are guided
by both recourse cost and provider capacity. Edge weights, reflecting recourse
costs, are optimized for social welfare while quantifying the welfare gap
between individual welfare and this collectively feasible outcome. We propose a
three-layer optimization framework: (1) basic capacitated matching, (2) optimal
capacity redistribution to minimize the welfare gap, and (3) cost-aware
optimization balancing welfare maximization with capacity adjustment costs.
Experimental validation on synthetic and real-world datasets demonstrates that
our framework enables the many-to-many algorithmic recourse to achieve
near-optimal welfare with minimum modification in system settings. This work
extends algorithmic recourse from individual recommendations to system-level
design, providing a tractable path toward higher social welfare while
maintaining individual actionability.

</details>


### [47] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: 提出了一种基于数据驱动的逆优化器和PPO框架的自动治疗计划方法，显著提高了头颈部癌症质子PBS治疗计划的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统质子PBS治疗计划需要大量人工调整和计算密集型逆优化，效率低下且依赖经验。

Method: 结合L2O逆优化器和PPO框架，利用Transformer处理长上下文，自动调整目标参数并生成高质量计划。

Result: 相比L-BFGSB，L2O逆优化器效率提升36.41%，有效性提升22.97%，计划生成时间平均2.55小时。

Conclusion: 该方法在多种临床条件下生成与人工计划相当或更优的治疗计划，显著提升了自动化水平。

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [48] [On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation](https://arxiv.org/abs/2508.11182)
*Matti Berthold,Lydia Blümel,Anna Rapberger*

Main category: cs.AI

TL;DR: 本文扩展了假设基础论证（ABA）中可接受性概念的研究，引入了强和弱可接受性及其相关语义，并探讨了它们在非平坦ABA中的性质。


<details>
  <summary>Details</summary>
Motivation: 研究ABA中标准可接受性概念的替代方案（强和弱可接受性），以丰富论证理论。

Method: 使用抽象双极集合基础论证框架（BSAFs）作为形式工具，分析强和弱可接受性在非平坦ABA中的性质。

Result: 证明了强和弱可接受性在非平坦ABA中保持模块化性质，但也存在一些缺陷。

Conclusion: 强和弱可接受性为ABA提供了新的语义选择，但需进一步研究以解决其局限性。

Abstract: In this work, we broaden the investigation of admissibility notions in the
context of assumption-based argumentation (ABA). More specifically, we study
two prominent alternatives to the standard notion of admissibility from
abstract argumentation, namely strong and weak admissibility, and introduce the
respective preferred, complete and grounded semantics for general (sometimes
called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation
frameworks (BSAFs) as formal playground since they concisely capture the
relations between assumptions and are expressive enough to represent general
non-flat ABA frameworks, as recently shown. While weak admissibility has been
recently investigated for a restricted fragment of ABA in which assumptions
cannot be derived (flat ABA), strong admissibility has not been investigated
for ABA so far. We introduce strong admissibility for ABA and investigate
desirable properties. We furthermore extend the recent investigations of weak
admissibility in the flat ABA fragment to the non-flat case. We show that the
central modularization property is maintained under classical, strong, and weak
admissibility. We also show that strong and weakly admissible semantics in
non-flat ABA share some of the shortcomings of standard admissible semantics
and discuss ways to address these.

</details>


### [49] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: 论文提出新数据集评估大型推理模型（LRMs）在解决不完整问题时的表现，发现其无法主动请求信息，并揭示其过度思考和幻觉行为。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估LRMs在解决定义明确问题时的能力，而忽略了其作为真正智能代理应具备的主动请求信息的能力。

Method: 提出包含两类不完整问题的新数据集，并系统评估LRMs的表现。

Result: LRMs无法主动请求信息，且表现出过度思考和幻觉行为。监督微调在提升此能力方面具有潜力与挑战。

Conclusion: 研究为开发真正智能的LRMs提供了新视角，强调其不仅需解决问题，还需具备主动交互能力。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [50] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: SAGE提出了一种动态知识图谱嵌入框架，通过自适应调整嵌入维度和动态蒸馏机制，显著提升了动态知识图谱的嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的知识图谱是动态演化的，现有方法未能充分考虑更新规模的变化，缺乏系统性的评估。

Method: SAGE根据更新规模确定嵌入维度并扩展嵌入空间，采用动态蒸馏机制平衡新旧知识。

Result: 在七个基准测试中，SAGE表现优于现有基线，MRR、H@1和H@10分别提升1.38%、1.25%和1.6%。

Conclusion: SAGE证明了自适应嵌入维度在动态知识图谱嵌入中的重要性，其代码已开源。

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


### [51] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: CRAFT-GUI提出了一种基于GRPO的课程学习框架，通过考虑任务难度差异和细粒度奖励设计，显著提升了GUI交互任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在GUI环境中忽视了任务难度差异和奖励信号的粗糙性，限制了学习效率和适应性。

Method: 采用GRPO框架，结合课程学习和细粒度奖励函数（规则信号与模型评估结合）。

Result: 在公开和内部基准测试中分别提升5.6%和10.3%，优于现有方法。

Conclusion: 结合强化学习与课程学习能有效提升GUI交互任务的性能。

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [52] [AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager](https://arxiv.org/abs/2508.11416)
*Xuhua Zhao,Yuxuan Xie,Caihua Chen,Yuxiang Sun*

Main category: cs.AI

TL;DR: 论文提出AIM-Bench基准，评估LLM代理在不确定供应链管理中的决策行为，发现其存在类似人类的决策偏差，并探讨了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在不确定库存决策中的能力和潜在偏差，以解决其在现实问题中的应用局限性。

Method: 通过AIM-Bench基准，设计多样化的库存补充实验，评估LLM代理的决策行为，并测试认知反思和信息共享等缓解策略。

Result: 不同LLM代理表现出类似人类的决策偏差，认知反思和信息共享能有效缓解部分偏差。

Conclusion: 需谨慎考虑LLM在库存决策中的潜在偏差，研究为开发以人为本的供应链决策支持系统提供了方向。

Abstract: Recent advances in mathematical reasoning and the long-term planning
capabilities of large language models (LLMs) have precipitated the development
of agents, which are being increasingly leveraged in business operations
processes. Decision models to optimize inventory levels are one of the core
elements of operations management. However, the capabilities of the LLM agent
in making inventory decisions in uncertain contexts, as well as the
decision-making biases (e.g. framing effect, etc.) of the agent, remain largely
unexplored. This prompts concerns regarding the capacity of LLM agents to
effectively address real-world problems, as well as the potential implications
of biases that may be present. To address this gap, we introduce AIM-Bench, a
novel benchmark designed to assess the decision-making behaviour of LLM agents
in uncertain supply chain management scenarios through a diverse series of
inventory replenishment experiments. Our results reveal that different LLMs
typically exhibit varying degrees of decision bias that are similar to those
observed in human beings. In addition, we explored strategies to mitigate the
pull-to-centre effect and the bullwhip effect, namely cognitive reflection and
implementation of information sharing. These findings underscore the need for
careful consideration of the potential biases in deploying LLMs in Inventory
decision-making scenarios. We hope that these insights will pave the way for
mitigating human decision bias and developing human-centred decision support
systems for supply chains.

</details>


### [53] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: Inclusion Arena是一个实时排行榜，通过从AI应用中收集的人类反馈对LLMs和MLLMs进行排名，弥补了静态数据集和通用领域提示的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准和排行榜依赖静态数据集或通用领域提示，无法反映模型在实际应用中的性能。

Method: 平台整合了成对模型比较到用户自然交互中，并采用改进的Bradley-Terry模型，包括冷启动机制（Placement Matches）和智能比较策略（Proximity Sampling）。

Result: Inclusion Arena提供可靠且稳定的排名，数据传递性更高，并显著减少恶意操纵风险。

Conclusion: 该平台旨在加速开发真正适用于实际用户场景的LLMs和MLLMs。

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


### [54] [Landmark-Assisted Monte Carlo Planning](https://arxiv.org/abs/2508.11493)
*David H. Chan,Mark Roberts,Dana S. Nau*

Main category: cs.AI

TL;DR: 论文研究了概率性地标在随机规划中的应用，通过调整UCT算法以利用地标作为子目标分解MDP，结果显示地标能显著提升在线概率规划的性能。


<details>
  <summary>Details</summary>
Motivation: 地标在经典规划中已有重要应用，但在随机领域很少使用，作者希望探索其在随机规划中的潜力。

Method: 提出概率性地标的概念，并调整UCT算法以平衡贪婪地标达成与最终目标达成的关系。

Result: 在基准测试中，合适的地标能显著提升UCT算法的性能，但最佳平衡点因问题而异。

Conclusion: 地标可以为解决MDP的随时算法提供有效指导。

Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in
every solution plan$\unicode{x2013}$have contributed to major advancements in
classical planning, but they have seldom been used in stochastic domains. We
formalize probabilistic landmarks and adapt the UCT algorithm to leverage them
as subgoals to decompose MDPs; core to the adaptation is balancing between
greedy landmark achievement and final goal achievement. Our results in
benchmark domains show that well-chosen landmarks can significantly improve the
performance of UCT in online probabilistic planning, while the best balance of
greedy versus long-term goal achievement is problem-dependent. The results
suggest that landmarks can provide helpful guidance for anytime algorithms
solving MDPs.

</details>


### [55] [Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models](https://arxiv.org/abs/2508.11524)
*Wenkai Yu,Jianhang Tang,Yang Zhang,Shanjiang Tang,Kebing Jin,Hankz Hankui Zhuo*

Main category: cs.AI

TL;DR: 论文提出了一种结合LLM和问题分解的新型规划器，通过LLM4Inspire和LLM4Predict两种范式辅助分解大规模规划问题，实验证明LLM4Predict（结合领域知识）比LLM4Inspire（通用知识）更有效。


<details>
  <summary>Details</summary>
Motivation: 解决大规模规划问题中的状态空间爆炸问题，并探索如何结合LLM与领域知识以生成有效规划。

Method: 提出LLM辅助规划器，先分解问题为子任务，再通过LLM4Inspire（启发式指导）和LLM4Predict（领域知识推断）辅助分解。

Result: 实验验证了规划器的有效性，LLM4Predict在修剪搜索空间时表现优于LLM4Inspire。

Conclusion: 结合领域知识的LLM（LLM4Predict）在大规模规划问题中更具潜力。

Abstract: Addressing large-scale planning problems has become one of the central
challenges in the planning community, deriving from the state-space explosion
caused by growing objects and actions. Recently, researchers have explored the
effectiveness of leveraging Large Language Models (LLMs) to generate helpful
actions and states to prune the search space. However, prior works have largely
overlooked integrating LLMs with domain-specific knowledge to ensure valid
plans. In this paper, we propose a novel LLM-assisted planner integrated with
problem decomposition, which first decomposes large planning problems into
multiple simpler sub-tasks. Then we explore two novel paradigms to utilize
LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where
LLM4Inspire provides heuristic guidance according to general knowledge and
LLM4Predict employs domain-specific knowledge to infer intermediate conditions.
We empirically validate the effectiveness of our planner across multiple
domains, demonstrating the ability of search space partition when solving
large-scale planning problems. The experimental results show that LLMs
effectively locate feasible solutions when pruning the search space, where
infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds
particular promise compared with LLM4Inspire, which offers general knowledge
within LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification](https://arxiv.org/abs/2508.10926)
*DongSeong-Yoon*

Main category: cs.LG

TL;DR: 论文提出了一种基于合作博弈的多准则投票集成方法，以解决现有方法仅考虑单一评价标准的问题，从而提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有投票集成方法仅考虑单一评价标准，无法充分利用分类器的多种先验信息，限制了模型性能的提升。

Method: 通过合作博弈在多准则情境下进行决策，同时考虑分类器的多种先验信息，实现合理的权重分配。

Result: 在Open-ML-CC18数据集上的实验表明，该方法优于其他权重分配方法。

Conclusion: 提出的多准则投票集成方法能有效利用多种信息，显著提升模型性能。

Abstract: Since the Fourth Industrial Revolution, AI technology has been widely used in
many fields, but there are several limitations that need to be overcome,
including overfitting/underfitting, class imbalance, and the limitations of
representation (hypothesis space) due to the characteristics of different
models. As a method to overcome these problems, ensemble, commonly known as
model combining, is being extensively used in the field of machine learning.
Among ensemble learning methods, voting ensembles have been studied with
various weighting methods, showing performance improvements. However, the
existing methods that reflect the pre-information of classifiers in weights
consider only one evaluation criterion, which limits the reflection of various
information that should be considered in a model realistically. Therefore, this
paper proposes a method of making decisions considering various information
through cooperative games in multi-criteria situations. Using this method,
various types of information known beforehand in classifiers can be
simultaneously considered and reflected, leading to appropriate weight
distribution and performance improvement. The machine learning algorithms were
applied to the Open-ML-CC18 dataset and compared with existing ensemble
weighting methods. The experimental results showed superior performance
compared to other weighting methods.

</details>


### [57] [Apriel-Nemotron-15B-Thinker](https://arxiv.org/abs/2508.10948)
*Shruthan Radhakrishna,Soham Parikh,Gopal Sarda,Anil Turkkan,Quaizar Vohra,Raymond Li,Dhruv Jhamb,Kelechi Ogueji,Aanjaneya Shukla,Oluwanifemi Bamgbose,Toby Liang,Luke Kumar,Oleksiy Ostapenko,Shiva Krishna Reddy Malay,Aman Tiwari,Tara Bogavelli,Vikas Yadav,Jash Mehta,Saloni Mittal,Akshay Kalkunte,Pulkit Pattnaik,Khalil Slimi,Anirudh Sreeram,Jishnu Nair,Akintunde Oladipo,Shashank Maiya,Khyati Mahajan,Rishabh Maheshwary,Masoud Hashemi,Sai Rajeswar Mudumba,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sebastien Paquet,Sagar Davasam,Srinivas Sunkara*

Main category: cs.LG

TL;DR: Apriel-Nemotron-15B-Thinker是一个15B参数模型，性能媲美32B参数模型，但内存占用减半。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在内存和计算成本上的限制，使其更适合企业应用。

Method: 采用四阶段训练流程：基础模型扩展、持续预训练、监督微调和GRPO强化学习。

Result: 在多项基准测试中表现优于或等同于32B参数模型。

Conclusion: Apriel-Nemotron-15B-Thinker在性能和效率上取得了平衡，适合实际企业部署。

Abstract: While large language models (LLMs) have achieved remarkable reasoning
capabilities across domains like code, math and other enterprise tasks, their
significant memory and computational costs often preclude their use in
practical enterprise settings. To this end, we introduce
Apriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow
Apriel SLM series that achieves performance against medium sized
state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while
maintaining only half the memory footprint of those alternatives.
Apriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline
including 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised
Fine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive
evaluations across a diverse suite of benchmarks consistently demonstrate that
our Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its
32-billion parameter counterparts, despite being less than half their size.

</details>


### [58] [Towards Efficient Prompt-based Continual Learning in Distributed Medical AI](https://arxiv.org/abs/2508.10954)
*Gyutae Oh,Jitae Shin*

Main category: cs.LG

TL;DR: 论文提出了一种基于提示的持续学习方法（PCL），用于解决医学领域数据共享受限的问题，通过统一提示池和最小扩展策略，显著提升了分类准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 医学领域的数据共享受到伦理、社会和制度限制，传统集中式学习难以实现，且传统训练方法容易过拟合新数据并遗忘旧知识。

Method: 采用基于提示的持续学习方法（PCL），通过统一提示池和最小扩展策略，结合新的正则化项平衡知识保留和适应。

Result: 在三个糖尿病视网膜病变数据集上的实验表明，PCL方法比现有方法提高了至少10%的分类准确率和9个点的F1分数，同时降低了推理成本。

Conclusion: PCL方法有望推动可持续的医学AI发展，支持实时诊断、患者监测和远程医疗应用。

Abstract: Modern AI models achieve state-of-the-art performance with large-scale,
high-quality datasets; however, ethical, social, and institutional constraints
in the medical domain severely restrict data sharing, rendering centralized
learning nearly impossible. Each institution must incrementally update models
using only local data. Traditional training overfits new samples and suffers
from catastrophic forgetting, losing previously acquired knowledge. Medical
data distributions also shift due to varying diagnostic equipment and
demographics. Although continual learning (CL) has advanced, most methods
address natural images, leaving medical-domain-specific CL underexplored. We
propose a prompt-based continual learning (PCL) approach featuring a unified
prompt pool with a minimal expansion strategy: by expanding and freezing a
subset of prompts, our method reduces computational overhead, and a novel
regularization term balances retention and adaptation. Experiments on three
diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy
Detection show our model improves final classification accuracy by at least 10%
and F1-score by 9 points over state-of-the-art approaches while lowering
inference cost. We anticipate this study will drive sustainable medical AI
advances, enabling real-time diagnosis, patient monitoring, and telemedicine
applications in distributed healthcare. Code will be released upon acceptance

</details>


### [59] [Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis](https://arxiv.org/abs/2508.10967)
*Xinyi Li,Sai Wang,Yutian Lin,Yu Wu,Yi Yang*

Main category: cs.LG

TL;DR: Retro-Expert是一个可解释的逆合成预测框架，结合大型语言模型和专业模型，通过强化学习实现协作推理，提供化学逻辑的自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖静态模式匹配，缺乏逻辑决策能力，导致黑箱决策。

Method: 结合专业模型（浅层推理）、大型语言模型（关键推理）和强化学习（优化决策策略），生成可解释的推理路径。

Result: 实验表明Retro-Expert在多个指标上超越现有模型，并提供专家认可的解释。

Conclusion: Retro-Expert不仅提升了预测性能，还通过可解释性弥合了AI预测与化学实践之间的差距。

Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a
given product molecule, which is a fundamental task in chemical synthesis.
However, existing models rely on static pattern-matching paradigm, which limits
their ability to perform effective logic decision-making, leading to black-box
decision-making. Building on this, we propose Retro-Expert, an interpretable
retrosynthesis framework that performs collaborative reasoning by combining the
complementary reasoning strengths of Large Language Models and specialized
models via reinforcement learning. It outputs natural language explanations
grounded in chemical logic through three components: (1) specialized models
perform shallow reasoning to construct high-quality chemical decision space,
(2) LLM-driven critical reasoning to generate predictions and corresponding
interpretable reasoning path, and (3) reinforcement learning optimizing
interpretable decision policy. Experiments show that Retro-Expert not only
surpasses both LLM-based and specialized models across different metrics but
also provides expert-aligned explanations that bridge the gap between AI
predictions and actionable chemical insights.

</details>


### [60] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: BeyondWeb框架通过优化合成数据生成，显著提升预训练性能，超越现有合成数据集。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型预训练中数据量扩展带来的收益递减问题，探索合成数据的潜力。

Method: 开发BeyondWeb框架，生成高质量合成数据，并分析其优化因素。

Result: BeyondWeb在14项基准测试中平均提升5.1pp和2.6pp，训练速度更快，小模型表现优于大模型。

Conclusion: 高质量合成数据需多因素联合优化，BeyondWeb展示了其潜力。

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [61] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为M&C的模型选择框架，帮助用户从模型平台中选择最适合目标数据域的预训练T2I模型，无需对所有模型进行微调。


<details>
  <summary>Details</summary>
Motivation: 当前公开的预训练T2I模型虽然促进了模型的民主化，但用户面临如何选择最适合目标数据域的模型的挑战。

Method: M&C框架的核心是一个匹配图，包含模型和数据集节点，以及模型-数据和数据-数据对的边，通过图嵌入特征预测最佳微调模型。

Result: 在10个T2I模型和32个数据集上的评估显示，M&C在61.3%的情况下成功预测最佳模型，其余情况下也能选择接近最优的模型。

Conclusion: M&C为预训练T2I模型的模型选择问题提供了高效解决方案。

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [62] [CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention](https://arxiv.org/abs/2508.11016)
*Qingbin Li,Rongkun Xue,Jie Wang,Ming Zhou,Zhi Li,Xiaofeng Ji,Yongqi Wang,Miao Liu,Zheming Yang,Minghui Qiu,Jing Yang*

Main category: cs.LG

TL;DR: CURE框架通过两阶段方法（高熵关键令牌重新生成和静态初始状态采样）解决RLVR中的熵崩溃问题，提升LLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR中静态初始状态采样导致的熵崩溃和低多样性问题。

Method: 两阶段框架：1. 高熵关键令牌重新生成以增加探索；2. 静态初始状态采样以增强利用。

Result: 在数学推理任务中性能提升5%，同时保持高熵水平。

Conclusion: CURE在熵和准确性上均达到最优性能，验证了其有效性。

Abstract: Recent advances in Reinforcement Learning with Verified Reward (RLVR) have
driven the emergence of more sophisticated cognitive behaviors in large
language models (LLMs), thereby enhancing their reasoning capabilities.
However, in prior RLVR pipelines, the repeated use of static initial-state
sampling drawn exactly from the dataset distribution during each sampling phase
produced overly deterministic, low diversity model behavior, which manifested
as rapid entropy collapse and hindered sustained performance gains during
prolonged training. To address this issue, we introduce CURE
(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a
two-stage framework that balances exploration and exploitation. Specifically,
in the first stage, to deliberately steer the model toward novel yet coherent
contexts, we re-generate at high-entropy critical tokens and jointly optimize
the original and the branched trajectories. The further comparison with vanilla
DAPO shows that the regeneration process achieves a better performance on math
reasoning tasks while sustaining a high-level entropy degree for exploration.
In the second stage, we continue training with static initial-state sampling by
DAPO, intentionally placing the model in a familiar state to gradually
strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,
compared to other RLVR methods, CURE achieves a 5% performance gain across six
math benchmarks, establishing state-of-the-art performance in both entropy and
accuracy. A series of experiments further validate the effectiveness of our
approach. Code is available at https://github.com/CURE-Project/CURE.

</details>


### [63] [Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis](https://arxiv.org/abs/2508.11020)
*Aakash Kumar,Emanuele Natale*

Main category: cs.LG

TL;DR: 本文通过基于随机子集和问题的理论结果，将强彩票假设（SLTH）扩展到量化网络，证明了在量化设置中可以精确表示目标离散神经网络，并给出了初始网络过参数化的最优界限。


<details>
  <summary>Details</summary>
Motivation: 量化是提高神经网络效率的关键技术，但现有理论主要针对连续设置，无法直接应用于量化网络。本文旨在填补这一理论空白。

Method: 基于Borgs等人的数分割问题理论结果，推导出量化设置下随机子集和问题的新理论，并扩展SLTH框架到有限精度网络。

Result: 在量化设置中，目标离散神经网络可以精确表示，且初始网络的过参数化界限是最优的。

Conclusion: 本文为量化神经网络提供了新的理论基础，扩展了SLTH的应用范围，并证明了量化网络的高效表示能力。

Abstract: Quantization is an essential technique for making neural networks more
efficient, yet our theoretical understanding of it remains limited. Previous
works demonstrated that extremely low-precision networks, such as binary
networks, can be constructed by pruning large, randomly-initialized networks,
and showed that the ratio between the size of the original and the pruned
networks is at most polylogarithmic.
  The specific pruning method they employed inspired a line of theoretical work
known as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights
from the Random Subset Sum Problem. However, these results primarily address
the continuous setting and cannot be applied to extend SLTH results to the
quantized setting.
  In this work, we build on foundational results by Borgs et al. on the Number
Partitioning Problem to derive new theoretical results for the Random Subset
Sum Problem in a quantized setting.
  Using these results, we then extend the SLTH framework to finite-precision
networks. While prior work on SLTH showed that pruning allows approximation of
a certain class of neural networks, we demonstrate that, in the quantized
setting, the analogous class of target discrete neural networks can be
represented exactly, and we prove optimal bounds on the necessary
overparameterization of the initial network as a function of the precision of
the target network.

</details>


### [64] [Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks](https://arxiv.org/abs/2508.11025)
*Laura Lützow,Michael Eichelbeck,Mykel J. Kochenderfer,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出了一种名为zono-conformal prediction的新方法，通过构建预测zonotopes来解决现有方法的计算和数据密集型问题，并在多维输出中捕捉依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法计算成本高、数据需求大，且多维度输出依赖关系捕捉能力有限。

Method: 引入zono-conformal prediction，通过线性程序构建预测zonotopes，适用于非线性基础预测器（如神经网络）。

Result: 实验表明，zono-conformal prediction比现有方法更少保守，同时保持类似的测试数据覆盖率。

Conclusion: zono-conformal prediction提供了一种高效且数据友好的不确定性量化方法，适用于回归和分类任务。

Abstract: Conformal prediction is a popular uncertainty quantification method that
augments a base predictor with prediction sets with statistically valid
coverage guarantees. However, current methods are often computationally
expensive and data-intensive, as they require constructing an uncertainty model
before calibration. Moreover, existing approaches typically represent the
prediction sets with intervals, which limits their ability to capture
dependencies in multi-dimensional outputs. We address these limitations by
introducing zono-conformal prediction, a novel approach inspired by interval
predictor models and reachset-conformant identification that constructs
prediction zonotopes with assured coverage. By placing zonotopic uncertainty
sets directly into the model of the base predictor, zono-conformal predictors
can be identified via a single, data-efficient linear program. While we can
apply zono-conformal prediction to arbitrary nonlinear base predictors, we
focus on feed-forward neural networks in this work. Aside from regression
tasks, we also construct optimal zono-conformal predictors in classification
settings where the output of an uncertain predictor is a set of possible
classes. We provide probabilistic coverage guarantees and present methods for
detecting outliers in the identification data. In extensive numerical
experiments, we show that zono-conformal predictors are less conservative than
interval predictor models and standard conformal prediction methods, while
achieving a similar coverage over the test data.

</details>


### [65] [Learning with Confidence](https://arxiv.org/abs/2508.11037)
*Oliver Ethan Richardson*

Main category: cs.LG

TL;DR: 论文探讨了学习或更新信念中的信心概念，区分了它与概率或似然的不同，并提出了两种测量方法。


<details>
  <summary>Details</summary>
Motivation: 研究学习过程中信心的作用及其与概率的区别，以统一文献中的相关概念。

Method: 通过公理化定义信心学习，提出两种连续测量方法，并证明其普适性。在附加假设下，推导出更简洁的向量场和损失函数表示。

Result: 证明了信心可以统一表示为连续测量，并扩展了复合观察的语言。贝叶斯规则被特例化为线性期望的优化学习者。

Conclusion: 信心是学习中的独立概念，可通过公理化方法统一表示，并扩展了学习理论的语言。

Abstract: We characterize a notion of confidence that arises in learning or updating
beliefs: the amount of trust one has in incoming information and its impact on
the belief state. This learner's confidence can be used alongside (and is
easily mistaken for) probability or likelihood, but it is fundamentally a
different concept -- one that captures many familiar concepts in the
literature, including learning rates and number of training epochs, Shafer's
weight of evidence, and Kalman gain. We formally axiomatize what it means to
learn with confidence, give two canonical ways of measuring confidence on a
continuum, and prove that confidence can always be represented in this way.
Under additional assumptions, we derive more compact representations of
confidence-based learning in terms of vector fields and loss functions. These
representations induce an extended language of compound "parallel"
observations. We characterize Bayes Rule as the special case of an optimizing
learner whose loss representation is a linear expectation.

</details>


### [66] [Conditional Independence Estimates for the Generalized Nonparanormal](https://arxiv.org/abs/2508.11050)
*Ujas Shah,Manuel Lladser,Rebecca Morrison*

Main category: cs.LG

TL;DR: 论文提出了一种广义非正态分布（广义非正态正态）的理论，证明在满足特定条件下，可以从精度矩阵推断条件独立结构，并提供了高效算法。


<details>
  <summary>Details</summary>
Motivation: 研究非高斯分布中如何推断变量间的条件独立结构，扩展高斯分布的相关理论。

Method: 基于高斯分布的对角变换，定义广义非正态正态分布，并提出一种高效算法从数据中恢复条件独立结构。

Result: 通过合成实验和真实数据验证了算法的有效性。

Conclusion: 广义非正态正态分布为推断非高斯数据的条件独立结构提供了新方法。

Abstract: For general non-Gaussian distributions, the covariance and precision matrices
do not encode the independence structure of the variables, as they do for the
multivariate Gaussian. This paper builds on previous work to show that for a
class of non-Gaussian distributions -- those derived from diagonal
transformations of a Gaussian -- information about the conditional independence
structure can still be inferred from the precision matrix, provided the data
meet certain criteria, analogous to the Gaussian case. We call such
transformations of the Gaussian as the generalized nonparanormal. The functions
that define these transformations are, in a broad sense, arbitrary. We also
provide a simple and computationally efficient algorithm that leverages this
theory to recover conditional independence structure from the generalized
nonparanormal data. The effectiveness of the proposed algorithm is demonstrated
via synthetic experiments and applications to real-world data.

</details>


### [67] [SHLIME: Foiling adversarial attacks fooling SHAP and LIME](https://arxiv.org/abs/2508.11053)
*Sam Chauhan,Estelle Duguet,Karthik Ramakrishnan,Hugh Van Deventer,Jack Kruger,Ranjan Subbaraman*

Main category: cs.LG

TL;DR: 该论文研究了LIME和SHAP等后解释方法在对抗性操纵下的脆弱性，并提出了改进其鲁棒性的策略。


<details>
  <summary>Details</summary>
Motivation: 后解释方法（如LIME和SHAP）被广泛用于评估模型偏见，但它们易受对抗性操纵，可能掩盖有害偏见。

Method: 通过复制COMPAS实验建立基线，引入模块化测试框架，系统评估增强和集成解释方法在不同性能分类器上的表现。

Result: 研究发现某些LIME/SHAP集成配置能显著提高偏见检测能力，增强高风险机器学习系统的透明度。

Conclusion: 论文强调了改进后解释方法鲁棒性的重要性，并展示了集成方法在提升透明度方面的潜力。

Abstract: Post hoc explanation methods, such as LIME and SHAP, provide interpretable
insights into black-box classifiers and are increasingly used to assess model
biases and generalizability. However, these methods are vulnerable to
adversarial manipulation, potentially concealing harmful biases. Building on
the work of Slack et al. (2020), we investigate the susceptibility of LIME and
SHAP to biased models and evaluate strategies for improving robustness. We
first replicate the original COMPAS experiment to validate prior findings and
establish a baseline. We then introduce a modular testing framework enabling
systematic evaluation of augmented and ensemble explanation approaches across
classifiers of varying performance. Using this framework, we assess multiple
LIME/SHAP ensemble configurations on out-of-distribution models, comparing
their resistance to bias concealment against the original methods. Our results
identify configurations that substantially improve bias detection, highlighting
their potential for enhancing transparency in the deployment of high-stakes
machine learning systems.

</details>


### [68] [Abundance-Aware Set Transformer for Microbiome Sample Embedding](https://arxiv.org/abs/2508.11075)
*Hyunwoo Yoo,Gail Rosen*

Main category: cs.LG

TL;DR: 提出了一种基于丰度感知的Set Transformer方法，用于构建微生物组样本的固定大小嵌入，优于传统平均池化和未加权方法。


<details>
  <summary>Details</summary>
Motivation: 现有微生物组样本表示方法多依赖序列嵌入的简单平均，忽略了分类群丰度的生物学重要性。

Method: 通过按相对丰度加权序列嵌入，提出丰度感知的Set Transformer变体，无需修改模型架构。

Result: 在真实微生物组分类任务中表现优于平均池化和未加权Set Transformer，部分任务达到完美性能。

Conclusion: 丰度感知聚合方法为微生物组表示提供了更稳健且生物学信息更丰富的解决方案。

Abstract: Microbiome sample representation to input into LLMs is essential for
downstream tasks such as phenotype prediction and environmental classification.
While prior studies have explored embedding-based representations of each
microbiome sample, most rely on simple averaging over sequence embeddings,
often overlooking the biological importance of taxa abundance. In this work, we
propose an abundance-aware variant of the Set Transformer to construct
fixed-size sample-level embeddings by weighting sequence embeddings according
to their relative abundance. Without modifying the model architecture, we
replicate embedding vectors proportional to their abundance and apply
self-attention-based aggregation. Our method outperforms average pooling and
unweighted Set Transformers on real-world microbiome classification tasks,
achieving perfect performance in some cases. These results demonstrate the
utility of abundance-aware aggregation for robust and biologically informed
microbiome representation. To the best of our knowledge, this is one of the
first approaches to integrate sequence-level abundance into Transformer-based
sample embeddings.

</details>


### [69] [A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora](https://arxiv.org/abs/2508.11084)
*Thanasis Schoinas,Ghulam Qadir*

Main category: cs.LG

TL;DR: 本文提出了一种经济可行的预测编码方法，通过数据管理流程将即时消息分组为日聊，结合特征选择和逻辑回归分类器，并通过降维提升性能。


<details>
  <summary>Details</summary>
Motivation: 即时消息因其非正式性和小规模特性，在预测编码中带来额外挑战，需要一种经济高效的解决方案。

Method: 采用数据管理流程将消息分组为日聊，进行特征选择和逻辑回归分类，并通过降维优化性能。

Result: 在Instant Bloomberg数据集上测试，方法表现良好，同时展示了成本节约的示例。

Conclusion: 该方法为即时消息的预测编码提供了一种经济可行的解决方案，并通过降维提升了性能。

Abstract: Predictive coding, the term used in the legal industry for document
classification using machine learning, presents additional challenges when the
dataset comprises instant messages, due to their informal nature and smaller
sizes. In this paper, we exploit a data management workflow to group messages
into day chats, followed by feature selection and a logistic regression
classifier to provide an economically feasible predictive coding solution. We
also improve the solution's baseline model performance by dimensionality
reduction, with focus on quantitative features. We test our methodology on an
Instant Bloomberg dataset, rich in quantitative information. In parallel, we
provide an example of the cost savings of our approach.

</details>


### [70] [Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation](https://arxiv.org/abs/2508.11086)
*Emily Liu,Kuan Han,Minfeng Zhan,Bocheng Zhao,Guanyu Mu,Yang Song*

Main category: cs.LG

TL;DR: 提出了一种基于相对优势的去偏框架，通过比较观看时间与用户和物品组的参考分布，生成基于分位数的偏好信号，显著提升了推荐准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 原始观看时间受视频时长、流行度和用户行为等混杂因素影响，可能导致推荐模型偏差。

Method: 采用两阶段架构，分离分布估计和偏好学习，并引入分布嵌入参数化观看时间分位数。

Result: 离线和在线实验均显示推荐准确性和鲁棒性显著优于现有基线方法。

Conclusion: 相对优势去偏框架有效解决了观看时间偏差问题，提升了推荐系统的性能。

Abstract: Watch time is widely used as a proxy for user satisfaction in video
recommendation platforms. However, raw watch times are influenced by
confounding factors such as video duration, popularity, and individual user
behaviors, potentially distorting preference signals and resulting in biased
recommendation models. We propose a novel relative advantage debiasing
framework that corrects watch time by comparing it to empirically derived
reference distributions conditioned on user and item groups. This approach
yields a quantile-based preference signal and introduces a two-stage
architecture that explicitly separates distribution estimation from preference
learning. Additionally, we present distributional embeddings to efficiently
parameterize watch-time quantiles without requiring online sampling or storage
of historical data. Both offline and online experiments demonstrate significant
improvements in recommendation accuracy and robustness compared to existing
baseline methods.

</details>


### [71] [Compressive Meta-Learning](https://arxiv.org/abs/2508.11090)
*Daniel Mas Montserrat,David Bonet,Maria Perera,Xavier Giró-i-Nieto,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经网络的压缩元学习框架，用于改进压缩学习中的编码和解码阶段，提高了速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模的快速扩大，需要高效且快速的参数学习技术。现有的压缩学习方法未能充分利用数据的底层结构。

Method: 通过神经网络元学习压缩学习的编码和解码阶段，应用于压缩PCA、压缩岭回归、压缩k-means和自编码器。

Result: 提出的框架在速度和准确性上优于现有方法。

Conclusion: 压缩元学习框架为高效、隐私友好的学习提供了新思路。

Abstract: The rapid expansion in the size of new datasets has created a need for fast
and efficient parameter-learning techniques. Compressive learning is a
framework that enables efficient processing by using random, non-linear
features to project large-scale databases onto compact, information-preserving
representations whose dimensionality is independent of the number of samples
and can be easily stored, transferred, and processed. These database-level
summaries are then used to decode parameters of interest from the underlying
data distribution without requiring access to the original samples, offering an
efficient and privacy-friendly learning framework. However, both the encoding
and decoding techniques are typically randomized and data-independent, failing
to exploit the underlying structure of the data. In this work, we propose a
framework that meta-learns both the encoding and decoding stages of compressive
learning methods by using neural networks that provide faster and more accurate
systems than the current state-of-the-art approaches. To demonstrate the
potential of the presented Compressive Meta-Learning framework, we explore
multiple applications -- including neural network-based compressive PCA,
compressive ridge regression, compressive k-means, and autoencoders.

</details>


### [72] [Predictive Multimodal Modeling of Diagnoses and Treatments in EHR](https://arxiv.org/abs/2508.11092)
*Cindy Shih-Ting Huang,Clarence Boon Liang Ng,Marek Rei*

Main category: cs.LG

TL;DR: 本文提出了一种多模态系统，用于早期预测ICD代码分配，通过融合临床记录和电子健康记录中的表格事件，结合预训练编码器和跨模态注意力机制，提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注出院后文档分类，而早期预测ICD代码可用于识别健康风险、优化治疗和资源分配。

Method: 提出多模态系统，融合临床记录和表格事件，使用预训练编码器、特征池化和跨模态注意力机制，并引入加权时间损失函数。

Result: 实验表明，该方法优于当前最先进系统，提升了早期预测性能。

Conclusion: 多模态融合和加权时间损失策略有效提升了ICD代码的早期预测能力。

Abstract: While the ICD code assignment problem has been widely studied, most works
have focused on post-discharge document classification. Models for early
forecasting of this information could be used for identifying health risks,
suggesting effective treatments, or optimizing resource allocation. To address
the challenge of predictive modeling using the limited information at the
beginning of a patient stay, we propose a multimodal system to fuse clinical
notes and tabular events captured in electronic health records. The model
integrates pre-trained encoders, feature pooling, and cross-modal attention to
learn optimal representations across modalities and balance their presence at
every temporal point. Moreover, we present a weighted temporal loss that
adjusts its contribution at each point in time. Experiments show that these
strategies enhance the early prediction model, outperforming the current
state-of-the-art systems.

</details>


### [73] [Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation](https://arxiv.org/abs/2508.11105)
*Sajjad Saed,Babak Teimourpour*

Main category: cs.LG

TL;DR: FGAT框架结合图神经网络和注意力机制，通过分层图结构同时建模服装兼容性和用户偏好，显著提升时尚推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 时尚产业快速扩张导致用户难以找到兼容商品，现有研究独立处理服装兼容性和个性化推荐，忽略了复杂交互。

Method: 构建用户、服装和物品的三层分层图，整合视觉和文本特征，利用图注意力机制动态加权节点重要性。

Result: 在POG数据集上，FGAT在精度、HR、召回率、NDCG和准确率上优于基线模型HFGN。

Conclusion: 结合多模态特征、分层图结构和注意力机制，显著提升个性化时尚推荐的准确性和效率。

Abstract: The rapid expansion of the fashion industry and the growing variety of
products have made it challenging for users to find compatible items on
e-commerce platforms. Effective fashion recommendation systems are crucial for
filtering irrelevant items and suggesting suitable ones. However,
simultaneously addressing outfit compatibility and personalized recommendations
remains a significant challenge, as these aspects are often treated
independently in existing studies, often overlooking the complex interactions
between items and user preferences. This research introduces a new framework
named FGAT, inspired by the HFGN model, which leverages graph neural networks
and graph attention mechanisms to tackle this issue. The proposed framework
constructs a three-tier hierarchical graph of users, outfits, and items,
integrating visual and textual features to simultaneously model outfit
compatibility and user preferences. A graph attention mechanism dynamically
weights node importance during representation propagation, enabling the capture
of key interactions and generating precise representations for both user
preferences and outfit compatibility. Evaluated on the POG dataset, FGAT
outperforms baseline models such as HFGN, achieving improved results in
precision, HR, recall, NDCG, and accuracy.These results demonstrate that
combining multimodal visual-textual features with a hierarchical graph
structure and attention mechanisms significantly enhances the accuracy and
efficiency of personalized fashion recommendation systems.

</details>


### [74] [Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees](https://arxiv.org/abs/2508.11112)
*Jianhao Ma,Lin Xiao*

Main category: cs.LG

TL;DR: 论文研究了分段仿射正则化（PAR）在监督学习中的理论和应用，展示了其在过参数化情况下的量化特性、多种PAR的闭式近端映射及统计保证。


<details>
  <summary>Details</summary>
Motivation: 解决离散或量化变量的优化问题，提供基于连续优化的灵活建模和计算框架。

Method: 研究了PAR的理论基础，包括过参数化情况下的量化特性、闭式近端映射的推导，以及使用近端梯度法、加速变体和ADMM求解PAR正则化问题。

Result: 在过参数化情况下，PAR正则化损失函数的每个临界点都表现出高度量化特性；通过PAR可以近似经典正则化形式并获得类似的统计保证。

Conclusion: PAR为量化问题提供了有效的理论和计算框架，适用于多种正则化形式，并具有统计保证。

Abstract: Optimization problems over discrete or quantized variables are very
challenging in general due to the combinatorial nature of their search space.
Piecewise-affine regularization (PAR) provides a flexible modeling and
computational framework for quantization based on continuous optimization. In
this work, we focus on the setting of supervised learning and investigate the
theoretical foundations of PAR from optimization and statistical perspectives.
First, we show that in the overparameterized regime, where the number of
parameters exceeds the number of samples, every critical point of the
PAR-regularized loss function exhibits a high degree of quantization. Second,
we derive closed-form proximal mappings for various (convex, quasi-convex, and
non-convex) PARs and show how to solve PAR-regularized problems using the
proximal gradient method, its accelerated variant, and the Alternating
Direction Method of Multipliers. Third, we study statistical guarantees of
PAR-regularized linear regression problems; specifically, we can approximate
classical formulations of $\ell_1$-, squared $\ell_2$-, and nonconvex
regularizations using PAR and obtain similar statistical guarantees with
quantized solutions.

</details>


### [75] [CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets](https://arxiv.org/abs/2508.11144)
*Gauri Jain,Dominik Rothenhäusler,Kirk Bansak,Elisabeth Paulson*

Main category: cs.LG

TL;DR: 论文提出了一种名为CTRL的元学习方法，旨在解决多源数据下的预测问题，同时保持源间差异。


<details>
  <summary>Details</summary>
Motivation: 在多源数据（如不同地点或群体）的机器学习任务中，需要既保证整体准确性，又保持源间差异。例如，难民安置项目需要针对不同地点生成差异化预测。

Method: CTRL结合了跨域残差学习和自适应池化/聚类技术，以平衡数据量和数据质量。

Result: 在5个大规模数据集（包括瑞士国家庇护计划数据）上，CTRL在多个关键指标上优于现有基准方法。

Conclusion: CTRL是一种有效的元学习方法，能够同时提升整体准确性和保持源间异质性。

Abstract: Machine learning (ML) tasks often utilize large-scale data that is drawn from
several distinct sources, such as different locations, treatment arms, or
groups. In such settings, practitioners often desire predictions that not only
exhibit good overall accuracy, but also remain reliable within each source and
preserve the differences that matter across sources. For instance, several
asylum and refugee resettlement programs now use ML-based employment
predictions to guide where newly arriving families are placed within a host
country, which requires generating informative and differentiated predictions
for many and often small source locations. However, this task is made
challenging by several common characteristics of the data in these settings:
the presence of numerous distinct data sources, distributional shifts between
them, and substantial variation in sample sizes across sources. This paper
introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method
that combines the strengths of cross-domain residual learning and adaptive
pooling/clustering in order to simultaneously improve overall accuracy and
preserve source-level heterogeneity. We provide theoretical results that
clarify how our objective navigates the trade-off between data quantity and
data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5
large-scale datasets. This includes a dataset from the national asylum program
in Switzerland, where the algorithmic geographic assignment of asylum seekers
is currently being piloted. CTRL consistently outperforms the benchmarks across
several key metrics and when using a range of different base learners.

</details>


### [76] [Towards the Next-generation Bayesian Network Classifiers](https://arxiv.org/abs/2508.11145)
*Huan Zhang,Daokun Zhang,Kexin Meng,Geoffrey I. Webb*

Main category: cs.LG

TL;DR: 提出了一种通过学习特征值的分布表示来设计高阶贝叶斯网络分类器的新范式，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯网络分类器因参数爆炸和数据稀疏性问题，难以建模高阶特征依赖，限制了其在复杂数据上的表现。

Method: 通过分布表示学习特征值的语义相关性，并设计神经版本的KDB分类器（NeuralKDB），利用神经网络架构学习条件概率。

Result: 在60个UCI数据集上的实验表明，NeuralKDB在捕捉高阶特征依赖方面表现优异，显著优于传统贝叶斯网络分类器和其他竞争方法。

Conclusion: 分布表示学习为贝叶斯网络分类器的高阶依赖建模提供了有效解决方案，显著提升了分类性能。

Abstract: Bayesian network classifiers provide a feasible solution to tabular data
classification, with a number of merits like high time and memory efficiency,
and great explainability. However, due to the parameter explosion and data
sparsity issues, Bayesian network classifiers are restricted to low-order
feature dependency modeling, making them struggle in extrapolating the
occurrence probabilities of complex real-world data. In this paper, we propose
a novel paradigm to design high-order Bayesian network classifiers, by learning
distributional representations for feature values, as what has been done in
word embedding and graph representation learning. The learned distributional
representations are encoded with the semantic relatedness between different
features through their observed co-occurrence patterns in training data, which
then serve as a hallmark to extrapolate the occurrence probabilities of new
test samples. As a classifier design realization, we remake the K-dependence
Bayesian classifier (KDB) by extending it into a neural version, i.e.,
NeuralKDB, where a novel neural network architecture is designed to learn
distributional representations of feature values and parameterize the
conditional probabilities between interdependent features. A stochastic
gradient descent based algorithm is designed to train the NeuralKDB model
efficiently. Extensive classification experiments on 60 UCI datasets
demonstrate that the proposed NeuralKDB classifier excels in capturing
high-order feature dependencies and significantly outperforms the conventional
Bayesian network classifiers, as well as other competitive classifiers,
including two neural network based classifiers without distributional
representation learning.

</details>


### [77] [Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning](https://arxiv.org/abs/2508.11159)
*Heqiang Wang,Weihong Yang,Xiaoxiong Zhong,Jia Zhou,Fangming Liu,Weizhe Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为QQR的算法，用于解决多模态在线联邦学习（MMO-FL）中的模态数量和质量不平衡问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）设备生成的多模态数据存在数量和质量的动态不平衡，这对分布式学习框架提出了新的挑战。

Method: 提出了基于原型学习的QQR算法，用于在训练过程中动态平衡模态数量和质量。

Result: 在两种真实多模态数据集上的实验表明，QQR算法在模态不平衡条件下优于基准方法。

Conclusion: QQR算法有效解决了MMO-FL中的模态不平衡问题，提升了学习性能。

Abstract: The Internet of Things (IoT) ecosystem produces massive volumes of multimodal
data from diverse sources, including sensors, cameras, and microphones. With
advances in edge intelligence, IoT devices have evolved from simple data
acquisition units into computationally capable nodes, enabling localized
processing of heterogeneous multimodal data. This evolution necessitates
distributed learning paradigms that can efficiently handle such data.
Furthermore, the continuous nature of data generation and the limited storage
capacity of edge devices demand an online learning framework. Multimodal Online
Federated Learning (MMO-FL) has emerged as a promising approach to meet these
requirements. However, MMO-FL faces new challenges due to the inherent
instability of IoT devices, which often results in modality quantity and
quality imbalance (QQI) during data collection. In this work, we systematically
investigate the impact of QQI within the MMO-FL framework and present a
comprehensive theoretical analysis quantifying how both types of imbalance
degrade learning performance. To address these challenges, we propose the
Modality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning
based method designed to operate in parallel with the training process.
Extensive experiments on two real-world multimodal datasets show that the
proposed QQR algorithm consistently outperforms benchmarks under modality
imbalance conditions with promising learning performance.

</details>


### [78] [A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels](https://arxiv.org/abs/2508.11180)
*Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: 提出了一种半监督生成模型，结合信息瓶颈原则，解决多视图学习中的视图缺失和标签缺失问题，并在图像和多组学数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 多视图学习在真实数据中常面临视图缺失和标签缺失问题，现有方法无法充分利用无标签数据。

Method: 提出半监督生成模型，结合信息瓶颈原则，最大化无标签样本的似然，并在潜在空间中进行跨视图互信息最大化。

Result: 在视图缺失和标签有限的情况下，模型在图像和多组学数据上的预测和填补性能优于现有方法。

Conclusion: 该模型有效整合了有标签和无标签数据，提升了多视图学习的性能。

Abstract: Multi-view learning is widely applied to real-life datasets, such as multiple
omics biological data, but it often suffers from both missing views and missing
labels. Prior probabilistic approaches addressed the missing view problem by
using a product-of-experts scheme to aggregate representations from present
views and achieved superior performance over deterministic classifiers, using
the information bottleneck (IB) principle. However, the IB framework is
inherently fully supervised and cannot leverage unlabeled data. In this work,
we propose a semi-supervised generative model that utilizes both labeled and
unlabeled samples in a unified framework. Our method maximizes the likelihood
of unlabeled samples to learn a latent space shared with the IB on labeled
data. We also perform cross-view mutual information maximization in the latent
space to enhance the extraction of shared information across views. Compared to
existing approaches, our model achieves better predictive and imputation
performance on both image and multi-omics data with missing views and limited
labeled samples.

</details>


### [79] [Quantum-Boosted High-Fidelity Deep Learning](https://arxiv.org/abs/2508.11190)
*Feng-ao Wang,Shaobo Chen,Yao Xuan,Junwei Liu,Qi Gao,Hongdong Zhu,Junjie Hou,Lixin Yuan,Jinyu Cheng,Chenxin Yi,Hai Wei,Yin Ma,Tao Xu,Kai Wen,Yixue Li*

Main category: cs.LG

TL;DR: 论文提出了一种量子-经典混合架构QBM-VAE，利用量子处理器高效采样Boltzmann分布，解决了传统高斯先验在复杂生物数据中的局限性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统概率深度学习依赖高斯先验，无法准确捕捉复杂非高斯数据（如生物数据），限制了科学发现的准确性。Boltzmann分布更具表达力，但经典计算机难以实现。

Method: 提出QBM-VAE，结合量子处理器采样Boltzmann分布作为先验，构建深度生成模型，应用于大规模单细胞数据集。

Result: QBM-VAE在数据整合、细胞分类和轨迹推断等任务中优于传统高斯模型（如VAE和SCVI），并展示了量子优势。

Conclusion: QBM-VAE为深度学习引入物理先验，突破数据限制，展示了量子计算在大规模科学问题中的实用优势，并为混合量子AI模型提供了可转移的蓝图。

Abstract: A fundamental limitation of probabilistic deep learning is its predominant
reliance on Gaussian priors. This simplistic assumption prevents models from
accurately capturing the complex, non-Gaussian landscapes of natural data,
particularly in demanding domains like complex biological data, severely
hindering the fidelity of the model for scientific discovery. The
physically-grounded Boltzmann distribution offers a more expressive
alternative, but it is computationally intractable on classical computers. To
date, quantum approaches have been hampered by the insufficient qubit scale and
operational stability required for the iterative demands of deep learning.
Here, we bridge this gap by introducing the Quantum Boltzmann
Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable
hybrid quantum-classical architecture. Our framework leverages a quantum
processor for efficient sampling from the Boltzmann distribution, enabling its
use as a powerful prior within a deep generative model. Applied to
million-scale single-cell datasets from multiple sources, the QBM-VAE generates
a latent space that better preserves complex biological structures,
consistently outperforming conventional Gaussian-based deep learning models
like VAE and SCVI in essential tasks such as omics data integration, cell-type
classification, and trajectory inference. It also provides a typical example of
introducing a physics priori into deep learning to drive the model to acquire
scientific discovery capabilities that breaks through data limitations. This
work provides the demonstration of a practical quantum advantage in deep
learning on a large-scale scientific problem and offers a transferable
blueprint for developing hybrid quantum AI models.

</details>


### [80] [Meta-learning Structure-Preserving Dynamics](https://arxiv.org/abs/2508.11205)
*Cheng Jing,Uvini Balasuriya Mudiyanselage,Woojin Cho,Minju Jo,Anthony Gruber,Kookjin Lee*

Main category: cs.LG

TL;DR: 提出了一种基于调制的元学习框架，用于结构保持的动态建模，无需显式系统参数知识或重新训练，实现了参数化动态系统的可扩展和泛化学习。


<details>
  <summary>Details</summary>
Motivation: 现有结构保持的动态建模方法需要固定系统配置和显式参数知识，且重新训练成本高，限制了其在多查询或参数变化场景中的应用。元学习虽提供解决方案，但现有方法存在训练不稳定或泛化能力有限的问题。

Method: 引入基于调制的元学习框架，通过紧凑的潜在表示直接条件化结构保持模型，避免灰盒系统知识和显式优化。应用于参数化能量守恒和耗散系统，实现可扩展和泛化学习。

Result: 在标准基准问题上实验表明，该方法在少样本学习场景中实现准确预测，同时保持动态稳定性和参数空间中的有效泛化性能。

Conclusion: 提出的调制框架解决了结构保持模型在参数变化场景中的局限性，为动态系统建模提供了高效且泛化的解决方案。

Abstract: Structure-preserving approaches to dynamics modeling have demonstrated great
potential for modeling physical systems due to their strong inductive biases
that enforce conservation laws and dissipative behavior. However, the resulting
models are typically trained for fixed system configurations, requiring
explicit knowledge of system parameters as well as costly retraining for each
new set of parameters -- a major limitation in many-query or parameter-varying
scenarios. Meta-learning offers a potential solution, but existing approaches
like optimization-based meta-learning often suffer from training instability or
limited generalization capability. Inspired by ideas from computer vision, we
introduce a modulation-based meta-learning framework that directly conditions
structure-preserving models on compact latent representations of potentially
unknown system parameters, avoiding the need for gray-box system knowledge and
explicit optimization during adaptation. Through the application of novel
modulation strategies to parametric energy-conserving and dissipative systems,
we enable scalable and generalizable learning across parametric families of
dynamical systems. Experiments on standard benchmark problems demonstrate that
our approach achieves accurate predictions in few-shot learning settings,
without compromising on the essential physical constraints necessary for
dynamical stability and effective generalization performance across parameter
space.

</details>


### [81] [Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning](https://arxiv.org/abs/2508.11210)
*Minghui Sun,Matthew M. Engelhard,Benjamin A. Goldstein*

Main category: cs.LG

TL;DR: 该研究提出了一种名为“Borrowing From the Future (BFF)”的多模态对比框架，旨在通过利用后期数据提升早期儿科风险评估的预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管后期风险评估通常更精确，但临床需要尽早进行可靠的风险评估。

Method: BFF将每个时间窗口视为独立模态，利用后期数据（如儿童健康检查）的信息来监督早期（如产前/出生阶段）的学习。

Result: 在两个真实儿科结果预测任务中验证了BFF，显示早期风险评估性能的持续提升。

Conclusion: BFF框架通过多模态对比学习有效提升了早期风险评估的可靠性。

Abstract: Risk assessments for a pediatric population are often conducted across
multiple stages. For example, clinicians may evaluate risks prenatally, at
birth, and during Well-Child visits. Although predictions made at later stages
typically achieve higher precision, it is clinically desirable to make reliable
risk assessments as early as possible. Therefore, this study focuses on
improving prediction performance in early-stage risk assessments. Our solution,
\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal
framework that treats each time window as a distinct modality. In BFF, a model
is trained on all available data throughout the time while performing a risk
assessment using up-to-date information. This contrastive framework allows the
model to ``borrow'' informative signals from later stages (e.g., Well-Child
visits) to implicitly supervise the learning at earlier stages (e.g.,
prenatal/birth stages). We validate BFF on two real-world pediatric outcome
prediction tasks, demonstrating consistent improvements in early risk
assessments. The code is available at https://github.com/scotsun/bff.

</details>


### [82] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: 论文探讨了计算与认知中的因果抽象理论，提出基于因果抽象的计算实现框架，并讨论了表征在其中的作用。


<details>
  <summary>Details</summary>
Motivation: 研究系统如何通过计算实现表征，以及因果抽象理论如何为这一问题提供新的视角。

Method: 利用因果抽象理论，结合深度学习和人工神经网络的讨论，分析计算实现与表征的关系。

Result: 提出了一种基于因果抽象的计算实现框架，强调了表征在预测和泛化中的作用。

Conclusion: 因果抽象理论为计算与认知研究提供了有效工具，表征在预测和泛化中具有核心地位。

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [83] [Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM](https://arxiv.org/abs/2508.11215)
*Zicheng Guo,Shuqi Wu,Meixing Zhu,He Guandi*

Main category: cs.LG

TL;DR: 提出了一种基于CNN-LSTM混合架构的PM2.5浓度预测模型，结合CNN的空间特征提取和LSTM的时间依赖性建模，在多元数据集上表现优于传统方法，但计算资源需求高。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化加剧，准确预测PM2.5浓度对环境保护、公共卫生和城市管理至关重要。

Method: 使用CNN提取局部空间特征，LSTM建模时间序列依赖，基于北京工业区2010-2015年的多元数据集进行6小时PM2.5浓度预测。

Result: 模型RMSE为5.236，优于传统时间序列模型，展示了在空气污染预警系统中的潜力。

Conclusion: 模型计算资源需求高，需优化处理多元大气因素的能力，未来将提升可扩展性和支持更复杂的天气预测任务。

Abstract: With the intensification of global climate change, accurate prediction of air
quality indicators, especially PM2.5 concentration, has become increasingly
important in fields such as environmental protection, public health, and urban
management. To address this, we propose an air quality PM2.5 index prediction
model based on a hybrid CNN-LSTM architecture. The model effectively combines
Convolutional Neural Networks (CNN) for local spatial feature extraction and
Long Short-Term Memory (LSTM) networks for modeling temporal dependencies in
time series data. Using a multivariate dataset collected from an industrial
area in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5
concentration, temperature, dew point, pressure, wind direction, wind speed,
and precipitation -- the model predicts the average PM2.5 concentration over
6-hour intervals. Experimental results show that the model achieves a root mean
square error (RMSE) of 5.236, outperforming traditional time series models in
both accuracy and generalization. This demonstrates its strong potential in
real-world applications such as air pollution early warning systems. However,
due to the complexity of multivariate inputs, the model demands high
computational resources, and its ability to handle diverse atmospheric factors
still requires optimization. Future work will focus on enhancing scalability
and expanding support for more complex multivariate weather prediction tasks.

</details>


### [84] [Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories](https://arxiv.org/abs/2508.11235)
*William Alemanni,Arianna Burzacchi,Davide Colombi,Elena Giarratano*

Main category: cs.LG

TL;DR: 本文提出了一种改进的交互式投票地图匹配算法，用于高效处理不同采样率的轨迹数据，旨在高精度重建GPS轨迹。


<details>
  <summary>Details</summary>
Motivation: 目标是提高GPS轨迹重建的准确性，不受输入数据质量的限制，并扩展算法的适用性。

Method: 在原始算法基础上整合轨迹插补，采用距离限制的交互式投票策略降低计算复杂度，并改进以处理道路网络中的缺失数据。

Result: 改进后的算法保留了原始算法的核心优势，同时显著扩展了其在实际场景中的适用性。

Conclusion: 该算法能够高效处理不同采样率的轨迹数据，适用于任何OpenStreetMap覆盖的地理区域。

Abstract: This paper presents an enhanced version of the Interactive Voting-Based Map
Matching algorithm, designed to efficiently process trajectories with varying
sampling rates. The main aim is to reconstruct GPS trajectories with high
accuracy, independent of input data quality. Building upon the original
algorithm, developed exclusively for aligning GPS signals to road networks, we
extend its capabilities by integrating trajectory imputation. Our improvements
also include the implementation of a distance-bounded interactive voting
strategy to reduce computational complexity, as well as modifications to
address missing data in the road network. Furthermore, we incorporate a
custom-built asset derived from OpenStreetMap, enabling this approach to be
smoothly applied in any geographic region covered by OpenStreetMap's road
network. These advancements preserve the core strengths of the original
algorithm while significantly extending its applicability to diverse real-world
scenarios.

</details>


### [85] [Graph Neural Diffusion via Generalized Opinion Dynamics](https://arxiv.org/abs/2508.11249)
*Asela Hevapathige,Asiri Wijesinghe,Ahad N. Zehmakan*

Main category: cs.LG

TL;DR: GODNF框架通过统一多种意见动态模型，解决了现有扩散GNN在适应性、深度和理论理解上的局限，实现了高效、可解释的消息传播。


<details>
  <summary>Details</summary>
Motivation: 现有扩散GNN存在适应性差、深度受限和理论理解不足的问题，需要一种更灵活、高效且理论支持的框架。

Method: 提出GODNF，通过节点特定行为建模和动态邻居影响，统一多种意见动态模型，支持异构扩散和时序动态。

Result: 理论分析证明GODNF能建模多样收敛配置，实验显示其在节点分类和影响力估计任务上优于现有GNN。

Conclusion: GODNF为扩散GNN提供了一种高效、可解释且理论支持的解决方案，显著提升了性能。

Abstract: There has been a growing interest in developing diffusion-based Graph Neural
Networks (GNNs), building on the connections between message passing mechanisms
in GNNs and physical diffusion processes. However, existing methods suffer from
three critical limitations: (1) they rely on homogeneous diffusion with static
dynamics, limiting adaptability to diverse graph structures; (2) their depth is
constrained by computational overhead and diminishing interpretability; and (3)
theoretical understanding of their convergence behavior remains limited. To
address these challenges, we propose GODNF, a Generalized Opinion Dynamics
Neural Framework, which unifies multiple opinion dynamics models into a
principled, trainable diffusion mechanism. Our framework captures heterogeneous
diffusion patterns and temporal dynamics via node-specific behavior modeling
and dynamic neighborhood influence, while ensuring efficient and interpretable
message propagation even at deep layers. We provide a rigorous theoretical
analysis demonstrating GODNF's ability to model diverse convergence
configurations. Extensive empirical evaluations of node classification and
influence estimation tasks confirm GODNF's superiority over state-of-the-art
GNNs.

</details>


### [86] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种通过提示设计从封闭权重LLM中提取特征并训练轻量级公平分类器的框架，适用于高风险的公平性要求场景。


<details>
  <summary>Details</summary>
Motivation: 由于封闭权重LLM（如GPT-4）无法通过传统公平算法进行调优，需要一种新的方法来实现公平分类。

Method: 将LLM视为特征提取器，通过设计提示获取其概率预测的特征，再应用公平算法训练轻量级分类器。

Result: 在五个数据集上实验表明，该框架在准确性和公平性之间取得了良好平衡，且优于基于LLM嵌入或原始特征的分类器。

Conclusion: 该框架为封闭权重LLM提供了一种高效、数据友好的公平分类解决方案。

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [87] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 本文提出了一种名为RTE的训练框架，通过时间集成增强SNN的对抗鲁棒性，解决了时间子网络的脆弱性和对抗性漏洞跨时间传递的问题。


<details>
  <summary>Details</summary>
Motivation: SNN在能效和类脑计算方面具有潜力，但其对抗扰动的脆弱性尚未被充分理解。

Method: 提出RTE框架，通过统一损失函数和随机采样策略优化时间子网络的鲁棒性，并减少对抗扰动的跨时间传递。

Result: 实验表明RTE在多个基准测试中优于现有方法，重塑了SNN的内部鲁棒性景观。

Conclusion: 研究强调了时间结构在对抗学习中的重要性，为构建鲁棒的SNN提供了理论基础。

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [88] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: 论文提出HS-GPPT模型，通过光谱对齐优化图预训练和提示调整，解决现有方法在异质图上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图预训练方法依赖同质性低频知识，无法适应现实图中多样化的光谱分布。

Method: 采用混合光谱滤波器主干和局部-全局对比学习，设计提示图以对齐光谱分布。

Result: 实验验证了HS-GPPT在转导和归纳学习中的有效性。

Conclusion: HS-GPPT通过光谱对齐实现了跨同质性和异质性的高效知识迁移。

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


### [89] [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](https://arxiv.org/abs/2508.11338)
*Prathamesh Devadiga,Yashmitha Shailesh*

Main category: cs.LG

TL;DR: RegimeNAS是一种新颖的可微分架构搜索框架，专为加密货币交易设计，通过市场状态感知提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决静态深度学习模型在动态金融环境中的局限性。

Method: 采用贝叶斯搜索空间、动态激活的神经网络模块（波动、趋势、范围块）和多目标损失函数。

Result: 在加密货币数据上显著优于现有基准，平均绝对误差降低80.3%，收敛速度更快。

Conclusion: 将领域知识（如市场状态）嵌入NAS过程，对金融应用至关重要。

Abstract: We introduce RegimeNAS, a novel differentiable architecture search framework
specifically designed to enhance cryptocurrency trading performance by
explicitly integrating market regime awareness. Addressing the limitations of
static deep learning models in highly dynamic financial environments, RegimeNAS
features three core innovations: (1) a theoretically grounded Bayesian search
space optimizing architectures with provable convergence properties; (2)
specialized, dynamically activated neural modules (Volatility, Trend, and Range
blocks) tailored for distinct market conditions; and (3) a multi-objective loss
function incorporating market-specific penalties (e.g., volatility matching,
transition smoothness) alongside mathematically enforced Lipschitz stability
constraints. Regime identification leverages multi-head attention across
multiple timeframes for improved accuracy and uncertainty estimation. Rigorous
empirical evaluation on extensive real-world cryptocurrency data demonstrates
that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving
an 80.3% Mean Absolute Error reduction compared to the best traditional
recurrent baseline and converging substantially faster (9 vs. 50+ epochs).
Ablation studies and regime-specific analysis confirm the critical contribution
of each component, particularly the regime-aware adaptation mechanism. This
work underscores the imperative of embedding domain-specific knowledge, such as
market regimes, directly within the NAS process to develop robust and adaptive
models for challenging financial applications.

</details>


### [90] [Conformal Prediction Meets Long-tail Classification](https://arxiv.org/abs/2508.11345)
*Shuqi Liu,Jianguo Huang,Luke Ong*

Main category: cs.LG

TL;DR: 论文提出了一种名为TACP的方法，通过利用长尾结构减少尾部类别的覆盖率不足问题，并进一步提出sTACP扩展方法。


<details>
  <summary>Details</summary>
Motivation: 现有CP方法在长尾标签分布下，尾部类别的覆盖率不足，影响预测集的可靠性。

Method: 提出TACP方法，利用长尾结构缩小头尾覆盖率差距；进一步提出sTACP扩展方法，通过重加权机制改善覆盖率平衡。

Result: 理论分析和实验表明，TACP和sTACP能有效减少头尾覆盖率差距，并在多个长尾基准数据集上表现优异。

Conclusion: TACP和sTACP方法显著改善了长尾分布下的覆盖率平衡，提升了预测集的可靠性。

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
that converts a pretrained model's point prediction into a prediction set, with
the set size reflecting the model's confidence. Although existing CP methods
are guaranteed to achieve marginal coverage, they often exhibit imbalanced
coverage across classes under long-tail label distributions, tending to over
cover the head classes at the expense of under covering the remaining tail
classes. This under coverage is particularly concerning, as it undermines the
reliability of the prediction sets for minority classes, even with coverage
ensured on average. In this paper, we propose the Tail-Aware Conformal
Prediction (TACP) method to mitigate the under coverage of the tail classes by
utilizing the long-tail structure and narrowing the head-tail coverage gap.
Theoretical analysis shows that it consistently achieves a smaller head-tail
coverage gap than standard methods. To further improve coverage balance across
all classes, we introduce an extension of TACP: soft TACP (sTACP) via a
reweighting mechanism. The proposed framework can be combined with various
non-conformity scores, and experiments on multiple long-tail benchmark datasets
demonstrate the effectiveness of our methods.

</details>


### [91] [NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models](https://arxiv.org/abs/2508.11348)
*Xiaohan Bi,Binhang Qi,Hailong Sun,Xiang Gao,Yue Yu,Xiaojun Liang*

Main category: cs.LG

TL;DR: NeMo是一种可扩展且通用的模块化训练方法，适用于Transformer和CNN等多种架构，显著提高了模块分类精度并减少了模块大小。


<details>
  <summary>Details</summary>
Motivation: 随着DNN模型在现代软件系统中的广泛应用，训练成本高昂成为挑战。现有模块化方法难以适应多样化的DNN和大规模模型。

Method: NeMo在神经元级别操作，采用对比学习和复合损失函数，支持大规模模型的模块化训练。

Result: 实验表明，NeMo在模块分类精度上平均提升1.72%，模块大小减少58.10%，适用于CNN和Transformer模型。

Conclusion: NeMo为可扩展和通用的DNN模块化提供了有效解决方案，具有实际应用潜力。

Abstract: With the growing incorporation of deep neural network (DNN) models into
modern software systems, the prohibitive construction costs have become a
significant challenge. Model reuse has been widely applied to reduce training
costs, but indiscriminately reusing entire models may incur significant
inference overhead. Consequently, DNN modularization has gained attention,
enabling module reuse by decomposing DNN models. The emerging
modularizing-while-training (MwT) paradigm, which incorporates modularization
into training, outperforms modularizing-after-training approaches. However,
existing MwT methods focus on small-scale CNN models at the convolutional
kernel level and struggle with diverse DNNs and large-scale models,
particularly Transformer-based models. To address these limitations, we propose
NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron
level fundamental component common to all DNNs-ensuring applicability to
Transformers and various architectures. We design a contrastive learning-based
modular training method with an effective composite loss function, enabling
scalability to large-scale models. Comprehensive experiments on two
Transformer-based models and four CNN models across two classification datasets
demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show
average gains of 1.72% in module classification accuracy and 58.10% reduction
in module size, demonstrating efficacy across both CNN and large-scale
Transformer-based models. A case study on open-source projects shows NeMo's
potential benefits in practical scenarios, offering a promising approach for
scalable and generalizable DNN modularization.

</details>


### [92] [A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts](https://arxiv.org/abs/2508.11349)
*Angela John,Selvyn Allotey,Till Koebe,Alexandra Tyukavina,Ingmar Weber*

Main category: cs.LG

TL;DR: 研究通过卫星图像和辅助数据评估全球造林和再造林项目的数据可靠性，提出LDIS评分标准，发现多数项目存在地理数据不完整问题。


<details>
  <summary>Details</summary>
Motivation: 由于自愿碳市场中造林和再造林项目的数据可靠性存疑，研究旨在通过标准化方法增强项目透明度和问责。

Method: 利用卫星图像和辅助数据，对全球45,628个项目的地理边界和数据进行标准化评估，提出LDIS评分。

Result: 79%的种植点在地理数据完整性上存在问题，15%的项目缺乏机器可读的地理数据。

Conclusion: 研究为自愿碳市场提供了增强问责的工具，同时数据集可用于计算机视觉等任务。

Abstract: Afforestation and reforestation are popular strategies for mitigating climate
change by enhancing carbon sequestration. However, the effectiveness of these
efforts is often self-reported by project developers, or certified through
processes with limited external validation. This leads to concerns about data
reliability and project integrity. In response to increasing scrutiny of
voluntary carbon markets, this study presents a dataset on global afforestation
and reforestation efforts compiled from primary (meta-)information and
augmented with time-series satellite imagery and other secondary data. Our
dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.
Since any remote sensing-based validation effort relies on the integrity of a
planting site's geographic boundary, this dataset introduces a standardized
assessment of the provided site-level location information, which we summarize
in one easy-to-communicate key indicator: LDIS -- the Location Data Integrity
Score. We find that approximately 79\% of the georeferenced planting sites
monitored fail on at least 1 out of 10 LDIS indicators, while 15\% of the
monitored projects lack machine-readable georeferenced data in the first place.
In addition to enhancing accountability in the voluntary carbon market, the
presented dataset also holds value as training data for e.g. computer
vision-related tasks with millions of linked Sentinel-2 and Planetscope
satellite images.

</details>


### [93] [Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning](https://arxiv.org/abs/2508.11353)
*Han Zhou,Hongpeng Yin,Xuanhong Deng,Yuyu Huang,Hao Ren*

Main category: cs.LG

TL;DR: 论文提出了一种名为HGD的梯度下降算法，通过平衡不同类别的梯度范数来解决数据流中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的数据流常存在类别不平衡问题，现有方法如重采样和重加权效果有限，因此作者提出通过修改训练过程（特别是梯度下降技术）来解决这一问题。

Method: 提出HGD算法，通过平衡梯度范数来避免对小类别的欠拟合，实现平衡的在线学习。HGD无需数据缓冲区、额外参数或先验知识。

Result: 理论分析表明HGD具有次线性遗憾界，实验证明其在多种不平衡数据流场景中优于常用方法。

Conclusion: HGD是一种高效且通用的方法，适用于任何使用梯度下降优化的学习模型。

Abstract: Many real-world data are sequentially collected over time and often exhibit
skewed class distributions, resulting in imbalanced data streams. While
existing approaches have explored several strategies, such as resampling and
reweighting, for imbalanced data stream learning, our work distinguishes itself
by addressing the imbalance problem through training modification, particularly
focusing on gradient descent techniques. We introduce the harmonized gradient
descent (HGD) algorithm, which aims to equalize the norms of gradients across
different classes. By ensuring the gradient norm balance, HGD mitigates
under-fitting for minor classes and achieves balanced online learning. Notably,
HGD operates in a streamlined implementation process, requiring no data-buffer,
extra parameters, or prior knowledge, making it applicable to any learning
models utilizing gradient descent for optimization. Theoretical analysis, based
on a few common and mild assumptions, shows that HGD achieves a satisfied
sub-linear regret bound. The proposed algorithm are compared with the commonly
used online imbalance learning methods under several imbalanced data stream
scenarios. Extensive experimental evaluations demonstrate the efficiency and
effectiveness of HGD in learning imbalanced data streams.

</details>


### [94] [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://arxiv.org/abs/2508.11356)
*Jia Liu,ChangYi He,YingQiao Lin,MingMin Yang,FeiYang Shen,ShaoGuo Liu,TingTing Gao*

Main category: cs.LG

TL;DR: 论文提出了一种基于熵的机制（ETMR和EAR），用于改进测试时强化学习（TTRL），解决了高推理成本和估计偏差问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中表现优异，但仍依赖标注数据且无监督场景下适应性不足。测试时强化学习（TTRL）虽能自优化，但面临高推理成本和估计偏差的挑战。

Method: 引入熵机制，提出两种策略：Entropy-fork Tree Majority Rollout（ETMR）和Entropy-based Advantage Reshaping（EAR），以平衡探索与利用。

Result: 在AIME 2024基准测试中，Llama3.1-8B的Pass at 1指标相对提升68%，且仅消耗60%的推理预算。

Conclusion: 该方法有效优化了推理效率、多样性和估计稳健性之间的权衡，推动了无监督强化学习在开放域推理任务中的应用。

Abstract: Recent advancements in Large Language Models have yielded significant
improvements in complex reasoning tasks such as mathematics and programming.
However, these models remain heavily dependent on annotated data and exhibit
limited adaptability in unsupervised scenarios. To address these limitations,
test-time reinforcement learning (TTRL) has been proposed, which enables
self-optimization by leveraging model-generated pseudo-labels. Despite its
promise, TTRL faces several key challenges, including high inference costs due
to parallel rollouts and early-stage estimation bias that fosters
overconfidence, reducing output diversity and causing performance plateaus. To
address these challenges, we introduce an entropy-based mechanism to enhance
the exploration-exploitation balance in test-time reinforcement learning
through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and
Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our
approach enables Llama3.1-8B to achieve a 68 percent relative improvement in
Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of
the rollout tokens budget. This highlights our method's ability to effectively
optimize the trade-off between inference efficiency, diversity, and estimation
robustness, thereby advancing unsupervised reinforcement learning for
open-domain reasoning tasks.

</details>


### [95] [PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding](https://arxiv.org/abs/2508.11357)
*Changhong Jing,Yan Liu,Shuqiang Wang,Bruce X. B. Yu,Gong Chen,Zhejing Hu,Zhi Zhang,Yanyan Shen*

Main category: cs.LG

TL;DR: PTSM框架通过双分支掩码机制和正交子空间分解，实现了跨被试EEG解码的零样本泛化，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决跨被试EEG解码中的被试间变异性大和共享表征稀缺问题。

Method: 采用双分支掩码机制学习个性化与共享时空模式，并通过信息论约束分解潜在嵌入。

Result: 在运动想象数据集上实现零样本泛化，性能超越现有方法。

Conclusion: PTSM通过解耦神经表征，实现了个性化和可迁移的解码。

Abstract: Cross-subject electroencephalography (EEG) decoding remains a fundamental
challenge in brain-computer interface (BCI) research due to substantial
inter-subject variability and the scarcity of subject-invariant
representations. This paper proposed PTSM (Physiology-aware and Task-invariant
Spatio-temporal Modeling), a novel framework for interpretable and robust EEG
decoding across unseen subjects. PTSM employs a dual-branch masking mechanism
that independently learns personalized and shared spatio-temporal patterns,
enabling the model to preserve individual-specific neural characteristics while
extracting task-relevant, population-shared features. The masks are factorized
across temporal and spatial dimensions, allowing fine-grained modulation of
dynamic EEG patterns with low computational overhead. To further address
representational entanglement, PTSM enforces information-theoretic constraints
that decompose latent embeddings into orthogonal task-related and
subject-related subspaces. The model is trained end-to-end via a
multi-objective loss integrating classification, contrastive, and
disentanglement objectives. Extensive experiments on cross-subject motor
imagery datasets demonstrate that PTSM achieves strong zero-shot
generalization, outperforming state-of-the-art baselines without
subject-specific calibration. Results highlight the efficacy of disentangled
neural representations for achieving both personalized and transferable
decoding in non-stationary neurophysiological settings.

</details>


### [96] [Fusing Rewards and Preferences in Reinforcement Learning](https://arxiv.org/abs/2508.11363)
*Sadegh Khorasani,Saber Salehkaleybar,Negar Kiyavash,Matthias Grossglauser*

Main category: cs.LG

TL;DR: DFA是一种强化学习算法，结合个体奖励和成对偏好，直接利用策略的对数概率建模偏好概率，避免单独奖励建模步骤。实验表明DFA在控制环境中表现优于或匹配SAC，且在偏好数据下优于RLHF基线。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通常依赖单一奖励信号或偏好数据，缺乏将两者有效结合的算法。DFA旨在融合个体奖励和偏好数据，提升学习效率和稳定性。

Method: DFA通过策略的对数概率直接建模偏好概率，避免单独奖励建模。偏好数据可来自人工标注或在线合成，基于Bradley-Terry模型优化偏好损失。

Result: DFA在六种控制环境中表现优于或匹配SAC，训练更稳定。在半合成偏好数据下，DFA优于RLHF基线，接近真实奖励的性能。

Conclusion: DFA通过融合个体奖励和偏好数据，提供了一种高效且稳定的强化学习算法，适用于多种任务。

Abstract: We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that
fuses both individual rewards and pairwise preferences (if available) into a
single update rule. DFA uses the policy's log-probabilities directly to model
the preference probability, avoiding a separate reward-modeling step.
Preferences can be provided by human-annotators (at state-level or
trajectory-level) or be synthesized online from Q-values stored in an
off-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing
DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)
policy. Our simulation results show that DFA trained on generated preferences
matches or exceeds SAC on six control environments and demonstrates a more
stable training process. With only a semi-synthetic preference dataset under
Bradley-Terry model, our algorithm outperforms reward-modeling reinforcement
learning from human feedback (RLHF) baselines in a stochastic GridWorld and
approaches the performance of an oracle with true rewards.

</details>


### [97] [Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization](https://arxiv.org/abs/2508.11365)
*Jayanta Mandi,Ali İrfan Mahmutoğulları,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 决策聚焦学习（DFL）通过直接最小化决策后悔来训练机器学习模型，提出了一种新的方法：即使使用可微优化层，也最小化替代损失，以提高效率和效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有梯度DFL方法在LP问题中梯度为零的问题，提升决策质量。

Method: 提出最小化替代损失的方法，结合DYS-Net高效计算近似解和梯度。

Result: 实验表明，该方法在决策后悔上优于或媲美现有方法，同时显著减少训练时间。

Conclusion: 最小化替代损失结合DYS-Net是一种高效且有效的DFL方法。

Abstract: Decision-focused learning (DFL) trains a machine learning (ML) model to
predict parameters of an optimization problem, to directly minimize decision
regret, i.e., maximize decision quality. Gradient-based DFL requires computing
the derivative of the solution to the optimization problem with respect to the
predicted parameters. However, for many optimization problems, such as linear
programs (LPs), the gradient of the regret with respect to the predicted
parameters is zero almost everywhere. Existing gradient-based DFL approaches
for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP
into a differentiable optimization problem by adding a quadratic regularizer
and then minimizing the regret directly or (b) minimizing surrogate losses that
have informative (sub)gradients. In this paper, we show that the former
approach still results in zero gradients, because even after smoothing the
regret remains constant across large regions of the parameter space. To address
this, we propose minimizing surrogate losses -- even when a differentiable
optimization layer is used and regret can be minimized directly. Our
experiments demonstrate that minimizing surrogate losses allows differentiable
optimization layers to achieve regret comparable to or better than
surrogate-loss based DFL methods. Further, we demonstrate that this also holds
for DYS-Net, a recently proposed differentiable optimization technique for LPs,
that computes approximate solutions and gradients through operations that can
be performed using feedforward neural network layers. Because DYS-Net executes
the forward and the backward pass very efficiently, by minimizing surrogate
losses using DYS-Net, we are able to attain regret on par with the
state-of-the-art while reducing training time by a significant margin.

</details>


### [98] [A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting](https://arxiv.org/abs/2508.11390)
*Michael Banf,Dominik Filipiak,Max Schattauer,Liliya Imasheva*

Main category: cs.LG

TL;DR: 论文提出了一种基于Forman-Ricci曲率的结构提升策略，用于解决图神经网络中长距离信息传递的失真问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂交互（如社交或生物网络）需要更高阶的拓扑表示，而传统图神经网络难以有效处理这些结构。

Method: 利用Forman-Ricci曲率定义边基网络特性，将数据从基本图形式提升到更富表达力的拓扑结构，再应用GNN模型。

Result: 该方法通过曲率揭示了图的局部和全局特性（如网络主干），并有效缓解了信息传递中的过压缩问题。

Conclusion: 提出的曲率提升策略为高阶拓扑学习提供了新思路，解决了图神经网络中的信息失真挑战。

Abstract: Graph Neural Networks are highly effective at learning from relational data,
leveraging node and edge features while maintaining the symmetries inherent to
graph structures. However, many real-world systems, such as social or
biological networks, exhibit complex interactions that are more naturally
represented by higher-order topological domains. The emerging field of
Geometric and Topological Deep Learning addresses this challenge by introducing
methods that utilize and benefit from higher-order structures. Central to TDL
is the concept of lifting, which transforms data representations from basic
graph forms to more expressive topologies before the application of GNN models
for learning. In this work, we propose a structural lifting strategy using
Forman-Ricci curvature, which defines an edge-based network characteristic
based on Riemannian geometry. Curvature reveals local and global properties of
a graph, such as a network's backbones, i.e. coarse, structure-preserving graph
geometries that form connections between major communities - most suitably
represented as hyperedges to model information flows between clusters across
large distances in the network. To this end, our approach provides a remedy to
the problem of information distortion in message passing across long distances
and graph bottlenecks - a phenomenon known in graph learning as over-squashing.

</details>


### [99] [On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting](https://arxiv.org/abs/2508.11408)
*Wenhao Zhang,Yuexiang Xie,Yuchang Sun,Yanxi Chen,Guoyin Wang,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: CHORD框架通过动态加权统一SFT和RL，避免破坏模型模式，提升学习效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决现有SFT和RL结合方法可能导致模型模式破坏和过拟合专家数据的问题。

Method: 提出CHORD框架，将SFT作为动态加权辅助目标融入RL过程，采用全局系数和令牌级权重函数。

Result: 实验证明CHORD在基准测试中表现优于基线，实现了稳定高效的学习。

Conclusion: CHORD有效协调专家数据和探索，为未来研究提供新思路。

Abstract: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two
prominent post-training paradigms for refining the capabilities and aligning
the behavior of Large Language Models (LLMs). Existing approaches that
integrate SFT and RL often face the risk of disrupting established model
patterns and inducing overfitting to expert data. To address this, we present a
novel investigation into the unified view of SFT and RL through an off-policy
versus on-policy lens. We propose CHORD, a framework for the Controllable
Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic
Weighting, which reframes SFT not as a separate stage but as a dynamically
weighted auxiliary objective within the on-policy RL process. Based on an
analysis of off-policy expert data's influence at both holistic and granular
levels, we incorporate a dual-control mechanism in CHORD. Specifically, the
framework first employs a global coefficient to holistically guide the
transition from off-policy imitation to on-policy exploration, and then applies
a token-wise weighting function that enables granular learning from expert
tokens, which preserves on-policy exploration and mitigates disruption from
off-policy data. We conduct extensive experiments on widely used benchmarks,
providing empirical evidence that CHORD achieves a stable and efficient
learning process. By effectively harmonizing off-policy expert data with
on-policy exploration, CHORD demonstrates significant improvements over
baselines. We release the implementation at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to
inspire further research.

</details>


### [100] [Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space](https://arxiv.org/abs/2508.11424)
*Yinghua Yao,Yuangang Pan,Xixian Chen*

Main category: cs.LG

TL;DR: LEAD是一种序列-结构协同设计框架，通过在共享潜在空间中优化抗体序列和结构，显著提高了开发性能并减少了查询消耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法在原始数据空间中优化互补决定区（CDRs）效率低下，导致评估成本过高。

Method: 提出LEAD框架，在共享潜在空间中优化序列和结构，并设计黑盒指导策略以适应非可微分属性评估器。

Result: LEAD在单目标和多目标优化中表现优异，查询消耗减少一半，性能超越基线方法。

Conclusion: LEAD突破了现有方法的限制，实现了高效且同步的多模态设计优化。

Abstract: Advancements in deep generative models have enabled the joint modeling of
antibody sequence and structure, given the antigen-antibody complex as context.
However, existing approaches for optimizing complementarity-determining regions
(CDRs) to improve developability properties operate in the raw data space,
leading to excessively costly evaluations due to the inefficient search
process. To address this, we propose LatEnt blAck-box Design (LEAD), a
sequence-structure co-design framework that optimizes both sequence and
structure within their shared latent space. Optimizing shared latent codes can
not only break through the limitations of existing methods, but also ensure
synchronization of different modality designs. Particularly, we design a
black-box guidance strategy to accommodate real-world scenarios where many
property evaluators are non-differentiable. Experimental results demonstrate
that our LEAD achieves superior optimization performance for both single and
multi-property objectives. Notably, LEAD reduces query consumption by a half
while surpassing baseline methods in property optimization. The code is
available at https://github.com/EvaFlower/LatEnt-blAck-box-Design.

</details>


### [101] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: 论文提出了一种通过收缩理论提升卷积神经常微分方程（NODEs）鲁棒性的方法，并通过正则化技术实现。


<details>
  <summary>Details</summary>
Motivation: 神经网络对输入噪声和对抗攻击脆弱，需要提升其鲁棒性。

Method: 利用收缩理论，通过正则化项（涉及系统动态的雅可比矩阵）或权重正则化项（针对特定激活函数的NODEs）诱导收缩性。

Result: 在MNIST和FashionMNIST数据集上，该方法有效提升了模型对噪声和攻击的鲁棒性。

Conclusion: 收缩性正则化是提升NODEs鲁棒性的有效方法，且可通过计算高效的方式实现。

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


### [102] [Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity](https://arxiv.org/abs/2508.11436)
*Mayssa Soussia,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: 论文提出了一种名为mCOCO的新框架，利用Reservoir Computing（RC）从BOLD信号中学习群体级别的功能性连接脑模板（CBT），解决了现有方法在可解释性、计算成本和认知能力建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如传统机器学习和图神经网络）在生成连接脑模板时存在可解释性差、计算成本高以及忽略认知能力的问题，因此需要一种更高效且全面的解决方案。

Method: mCOCO框架分为两个阶段：1) 将BOLD信号映射到储备池中生成个体功能连接组，再聚合为群体级别CBT；2) 通过认知储备池整合多感官输入，赋予CBT认知特性。

Result: 实验表明，mCOCO在中心性、区分性、拓扑合理性和多感官记忆保留方面显著优于基于GNN的CBT。

Conclusion: mCOCO框架为功能性连接研究提供了一种高效、可解释且具有认知能力的解决方案。

Abstract: The generation of connectional brain templates (CBTs) has recently garnered
significant attention for its potential to identify unique connectivity
patterns shared across individuals. However, existing methods for CBT learning
such as conventional machine learning and graph neural networks (GNNs) are
hindered by several limitations. These include: (i) poor interpretability due
to their black-box nature, (ii) high computational cost, and (iii) an exclusive
focus on structure and topology, overlooking the cognitive capacity of the
generated CBT. To address these challenges, we introduce mCOCO (multi-sensory
COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)
to learn population-level functional CBT from BOLD
(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow
for tracking state changes over time, enhancing interpretability and enabling
the modeling of brain-like dynamics, as demonstrated in prior literature. By
integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO
captures not only structure and topology but also how brain regions process
information and adapt to cognitive tasks such as sensory processing, all in a
computationally efficient manner. Our mCOCO framework consists of two phases:
(1) mapping BOLD signals into the reservoir to derive individual functional
connectomes, which are then aggregated into a group-level CBT - an approach, to
the best of our knowledge, not previously explored in functional connectivity
studies - and (2) incorporating multi-sensory inputs through a cognitive
reservoir, endowing the CBT with cognitive traits. Extensive evaluations show
that our mCOCO-based template significantly outperforms GNN-based CBT in terms
of centeredness, discriminativeness, topological soundness, and multi-sensory
memory retention. Our source code is available at
https://github.com/basiralab/mCOCO.

</details>


### [103] [Informative Post-Hoc Explanations Only Exist for Simple Functions](https://arxiv.org/abs/2508.11441)
*Eric Günther,Balázs Szabados,Robi Bhattacharjee,Sebastian Bordt,Ulrike von Luxburg*

Main category: cs.LG

TL;DR: 论文提出了一种基于学习理论的框架，用于评估解释算法是否能提供关于决策函数的信息，并证明许多流行算法在复杂模型下无效。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证局部后解释算法是否能真正揭示复杂机器学习模型的行为，并提供理论支持。

Method: 方法是通过定义一个“信息性”解释的标准（即减少可能决策函数的复杂性），并分析不同解释算法在此标准下的表现。

Result: 结果显示许多流行算法（如梯度解释、SHAP等）在复杂模型下无法提供信息性解释，并提出了改进条件。

Conclusion: 结论是解释算法的有效性依赖于模型和解释方法的匹配，这对AI的高风险应用和监管有重要影响。

Abstract: Many researchers have suggested that local post-hoc explanation algorithms
can be used to gain insights into the behavior of complex machine learning
models. However, theoretical guarantees about such algorithms only exist for
simple decision functions, and it is unclear whether and under which
assumptions similar results might exist for complex models. In this paper, we
introduce a general, learning-theory-based framework for what it means for an
explanation to provide information about a decision function. We call an
explanation informative if it serves to reduce the complexity of the space of
plausible decision functions. With this approach, we show that many popular
explanation algorithms are not informative when applied to complex decision
functions, providing a rigorous mathematical rejection of the idea that it
should be possible to explain any model. We then derive conditions under which
different explanation algorithms become informative. These are often stronger
than what one might expect. For example, gradient explanations and
counterfactual explanations are non-informative with respect to the space of
differentiable functions, and SHAP and anchor explanations are not informative
with respect to the space of decision trees. Based on these results, we discuss
how explanation algorithms can be modified to become informative. While the
proposed analysis of explanation algorithms is mathematical, we argue that it
holds strong implications for the practical applicability of these algorithms,
particularly for auditing, regulation, and high-risk applications of AI.

</details>


### [104] [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](https://arxiv.org/abs/2508.11460)
*Aurora Grefsrud,Nello Blaser,Trygve Buanes*

Main category: cs.LG

TL;DR: 该研究通过贝叶斯推断框架和合成数据集测试，评估了六种概率机器学习算法在类别概率和不确定性估计中的表现，发现深度学习算法在分布外数据上的不确定性估计存在不足。


<details>
  <summary>Details</summary>
Motivation: 随着数据模型复杂度增加（如深度学习），不确定性量化变得困难，需要验证不同算法在不确定性估计中的表现。

Method: 使用近似贝叶斯推断框架和合成分类数据集，测试六种算法（包括神经网络集成、蒙特卡洛Dropout等）的不确定性估计能力。

Result: 所有算法在校准性上表现良好，但深度学习算法在分布外数据上的不确定性估计不一致。

Conclusion: 研究为开发新的不确定性估计方法提供了参考，强调了深度学习算法在分布外数据上的局限性。

Abstract: Rigorous statistical methods, including parameter estimation with
accompanying uncertainties, underpin the validity of scientific discovery,
especially in the natural sciences. With increasingly complex data models such
as deep learning techniques, uncertainty quantification has become exceedingly
difficult and a plethora of techniques have been proposed. In this case study,
we use the unifying framework of approximate Bayesian inference combined with
empirical tests on carefully created synthetic classification datasets to
investigate qualitative properties of six different probabilistic machine
learning algorithms for class probability and uncertainty estimation: (i) a
neural network ensemble, (ii) neural network ensemble with conflictual loss,
(iii) evidential deep learning, (iv) a single neural network with Monte Carlo
Dropout, (v) Gaussian process classification and (vi) a Dirichlet process
mixture model. We check if the algorithms produce uncertainty estimates which
reflect commonly desired properties, such as being well calibrated and
exhibiting an increase in uncertainty for out-of-distribution data points. Our
results indicate that all algorithms are well calibrated, but none of the deep
learning based algorithms provide uncertainties that consistently reflect lack
of experimental evidence for out-of-distribution data points. We hope our study
may serve as a clarifying example for researchers developing new methods of
uncertainty estimation for scientific data-driven modeling.

</details>


### [105] [Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection](https://arxiv.org/abs/2508.11504)
*Andrea Castellani,Zacharias Papadovasilakis,Giorgos Papoutsoglou,Mary Cole,Brian Bautsch,Tobias Rodemann,Ioannis Tsamardinos,Angela Harden*

Main category: cs.LG

TL;DR: 该研究利用AutoML和可解释AI分析车祸严重性，识别出17个关键风险因素，并强调环境因素的重要性。


<details>
  <summary>Details</summary>
Motivation: 全球车祸是伤害和死亡的主要原因，需数据驱动方法以理解和减轻车祸严重性。

Method: 使用JADBio AutoML平台构建预测模型，结合SHAP解释特征贡献，最终采用Ridge Logistic Regression模型。

Result: 模型AUC-ROC为85.6%（训练集）和84.9%（测试集），识别出17个关键特征，环境因素比传统因素（如酒精）更具影响力。

Conclusion: 研究提供了可扩展框架，支持数据驱动的交通安全政策，强调方法严谨性和可解释性。

Abstract: Motor vehicle crashes remain a leading cause of injury and death worldwide,
necessitating data-driven approaches to understand and mitigate crash severity.
This study introduces a curated dataset of more than 3 million people involved
in accidents in Ohio over six years (2017-2022), aggregated to more than 2.3
million vehicle-level records for predictive analysis. The primary contribution
is a transparent and reproducible methodology that combines Automated Machine
Learning (AutoML) and explainable artificial intelligence (AI) to identify and
interpret key risk factors associated with severe crashes. Using the JADBio
AutoML platform, predictive models were constructed to distinguish between
severe and non-severe crash outcomes. The models underwent rigorous feature
selection across stratified training subsets, and their outputs were
interpreted using SHapley Additive exPlanations (SHAP) to quantify the
contribution of individual features. A final Ridge Logistic Regression model
achieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test
set, with 17 features consistently identified as the most influential
predictors. Key features spanned demographic, environmental, vehicle, human,
and operational categories, including location type, posted speed, minimum
occupant age, and pre-crash action. Notably, certain traditionally emphasized
factors, such as alcohol or drug impairment, were less influential in the final
model compared to environmental and contextual variables. Emphasizing
methodological rigor and interpretability over mere predictive performance,
this study offers a scalable framework to support Vision Zero with aligned
interventions and advanced data-informed traffic safety policy.

</details>


### [106] [Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies](https://arxiv.org/abs/2508.11513)
*Fanzhen Liu,Xiaoxiao Ma,Jian Yang,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Quan Z. Sheng,Jia Wu*

Main category: cs.LG

TL;DR: GraphOracle是一个新型自解释图神经网络框架，旨在生成和评估类级解释，解决了现有方法在类级解释上的不足。


<details>
  <summary>Details</summary>
Motivation: 增强图神经网络（GNNs）的可解释性，确保其安全公平部署，并解决现有自解释GNNs在类级解释上的局限性。

Method: GraphOracle联合学习GNN分类器和一组结构化稀疏子图，通过掩码评估策略验证图-子图-预测依赖关系。

Result: GraphOracle在多个图分类任务中表现出更高的保真度、可解释性和可扩展性，优于ProtGNN和PGIB。

Conclusion: GraphOracle通过熵正则化子图选择和轻量级随机游走提取，成为GNNs中忠实类级自解释的实用解决方案。

Abstract: Enhancing the interpretability of graph neural networks (GNNs) is crucial to
ensure their safe and fair deployment. Recent work has introduced
self-explainable GNNs that generate explanations as part of training, improving
both faithfulness and efficiency. Some of these models, such as ProtGNN and
PGIB, learn class-specific prototypes, offering a potential pathway toward
class-level explanations. However, their evaluations focus solely on
instance-level explanations, leaving open the question of whether these
prototypes meaningfully generalize across instances of the same class. In this
paper, we introduce GraphOracle, a novel self-explainable GNN framework
designed to generate and evaluate class-level explanations for GNNs. Our model
jointly learns a GNN classifier and a set of structured, sparse subgraphs that
are discriminative for each class. We propose a novel integrated training that
captures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependencies
efficiently and faithfully, validated through a masking-based evaluation
strategy. This strategy enables us to retroactively assess whether prior
methods like ProtGNN and PGIB deliver effective class-level explanations. Our
results show that they do not. In contrast, GraphOracle achieves superior
fidelity, explainability, and scalability across a range of graph
classification tasks. We further demonstrate that GraphOracle avoids the
computational bottlenecks of previous methods$\unicode{x2014}$like Monte Carlo
Tree Search$\unicode{x2014}$by using entropy-regularized subgraph selection and
lightweight random walk extraction, enabling faster and more scalable training.
These findings position GraphOracle as a practical and principled solution for
faithful class-level self-explainability in GNNs.

</details>


### [107] [DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality](https://arxiv.org/abs/2508.11514)
*Qitong Chu,Yufeng Yue,Danya Yao,Huaxin Pei*

Main category: cs.LG

TL;DR: 提出了一种双空间引导的测试框架，通过协调场景参数空间和智能体行为空间，生成兼顾多样性和关键性的测试场景。


<details>
  <summary>Details</summary>
Motivation: 动态环境中决策智能体的部署增加，对安全验证的需求上升，但现有方法在高维场景空间中难以平衡多样性和关键性。

Method: 采用分层表示框架，结合降维和多维子空间评估，在参数空间中定位多样和关键子空间；在行为空间中利用交互数据量化行为关键性和多样性，支持生成模式切换。

Result: 实验表明，该框架在五种决策智能体上平均提高了56.23%的关键场景生成效率，并在新指标下表现出更高的多样性。

Conclusion: 双空间引导框架有效解决了高维场景空间中的多样性和关键性平衡问题，优于现有基线方法。

Abstract: The growing deployment of decision-making agents in dynamic environments
increases the demand for safety verification. While critical testing scenario
generation has emerged as an appealing verification methodology, effectively
balancing diversity and criticality remains a key challenge for existing
methods, particularly due to local optima entrapment in high-dimensional
scenario spaces. To address this limitation, we propose a dual-space guided
testing framework that coordinates scenario parameter space and agent behavior
space, aiming to generate testing scenarios considering diversity and
criticality. Specifically, in the scenario parameter space, a hierarchical
representation framework combines dimensionality reduction and
multi-dimensional subspace evaluation to efficiently localize diverse and
critical subspaces. This guides dynamic coordination between two generation
modes: local perturbation and global exploration, optimizing critical scenario
quantity and diversity. Complementarily, in the agent behavior space,
agent-environment interaction data are leveraged to quantify behavioral
criticality/diversity and adaptively support generation mode switching, forming
a closed feedback loop that continuously enhances scenario characterization and
exploration within the parameter space. Experiments show our framework improves
critical scenario generation by an average of 56.23\% and demonstrates greater
diversity under novel parameter-behavior co-driven metrics when tested on five
decision-making agents, outperforming state-of-the-art baselines.

</details>


### [108] [Finite-Width Neural Tangent Kernels from Feynman Diagrams](https://arxiv.org/abs/2508.11522)
*Max Guillen,Philipp Misof,Jan E. Gerken*

Main category: cs.LG

TL;DR: 论文提出了一种使用费曼图计算有限宽度修正的方法，以分析神经切核（NTK）在深度神经网络中的统计特性，并验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 在无限宽度极限下，NTK的计算虽然简单，但忽略了训练中的关键特性（如NTK演化和特征学习）。为了更准确地描述有限宽度网络的训练动态，需要引入有限宽度修正。

Method: 通过费曼图计算有限宽度修正，简化代数操作，并推导出涉及预激活、NTK及其高阶导数张量的层间递归关系。

Result: 扩展了深度网络的稳定性结果，证明了ReLU等尺度不变非线性在NTK Gram矩阵对角线上无有限宽度修正，并通过数值实验验证了结果。

Conclusion: 费曼图方法为有限宽度NTK统计提供了高效的计算框架，有助于更全面地理解深度神经网络的训练动态。

Abstract: Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,
non-linear neural networks. In the infinite-width limit, NTKs can easily be
computed for most common architectures, yielding full analytic control over the
training dynamics. However, at infinite width, important properties of training
such as NTK evolution or feature learning are absent. Nevertheless, finite
width effects can be included by computing corrections to the Gaussian
statistics at infinite width. We introduce Feynman diagrams for computing
finite-width corrections to NTK statistics. These dramatically simplify the
necessary algebraic manipulations and enable the computation of layer-wise
recursive relations for arbitrary statistics involving preactivations, NTKs and
certain higher-derivative tensors (dNTK and ddNTK) required to predict the
training dynamics at leading order. We demonstrate the feasibility of our
framework by extending stability results for deep networks from preactivations
to NTKs and proving the absence of finite-width corrections for scale-invariant
nonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We
validate our results with numerical experiments.

</details>


### [109] [Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2508.11528)
*Juhi Soni,Markus Lange-Hegermann,Stefan Windmann*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息扩散模型的无监督异常检测方法，用于多元时间序列数据，通过加权物理信息损失提升性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列领域已展现出有效性，但现有方法缺乏物理依赖性的考虑，因此提出结合物理信息的改进方法。

Method: 使用加权物理信息损失训练扩散模型，学习多元时间序列数据的物理依赖性时间分布。

Result: 实验表明，该方法在异常检测中提高了F1分数，生成数据多样性和对数似然性更优，优于基线方法和现有物理信息模型。

Conclusion: 物理信息训练显著提升了扩散模型的性能，尤其在合成和部分真实数据集上表现突出。

Abstract: We propose an unsupervised anomaly detection approach based on a
physics-informed diffusion model for multivariate time series data. Over the
past years, diffusion model has demonstrated its effectiveness in forecasting,
imputation, generation, and anomaly detection in the time series domain. In
this paper, we present a new approach for learning the physics-dependent
temporal distribution of multivariate time series data using a weighted
physics-informed loss during diffusion model training. A weighted
physics-informed loss is constructed using a static weight schedule. This
approach enables a diffusion model to accurately approximate underlying data
distribution, which can influence the unsupervised anomaly detection
performance. Our experiments on synthetic and real-world datasets show that
physics-informed training improves the F1 score in anomaly detection; it
generates better data diversity and log-likelihood. Our model outperforms
baseline approaches, additionally, it surpasses prior physics-informed work and
purely data-driven diffusion models on a synthetic dataset and one real-world
dataset while remaining competitive on others.

</details>


### [110] [A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow](https://arxiv.org/abs/2508.11529)
*George Paterakis,Andrea Castellani,George Papoutsoglou,Tobias Rodemann,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: 本文提出了一种名为HXAI（Holistic Explainable AI）的用户中心框架，通过嵌入解释到数据分析的每个阶段，并针对用户需求定制解释，以提高AI模型的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型被视为“黑箱”，传统解释方法仅关注单个预测，忽略了上下游决策和质量检查。HXAI旨在解决这一问题，提供全面的解释框架。

Method: HXAI将六个组件（数据、分析设置、学习过程、模型输出、模型质量、沟通渠道）统一为一个分类法，并通过112项问题库和用户调查，结合人类解释理论、人机交互原则和实证研究，设计清晰、可操作的解释。

Result: HXAI提供了一个全面的分类法，减少了术语歧义，并支持对现有工具链的严格覆盖分析。此外，展示了基于大语言模型的AI代理如何协调多种解释技术，生成针对特定利益相关者的叙述。

Conclusion: HXAI通过跨学科融合、实际项目经验和文献综述，提出了一种新颖的端到端透明性、可信度和负责任AI部署视角。

Abstract: Artificial intelligence is reshaping science and industry, yet many users
still regard its models as opaque "black boxes". Conventional explainable
artificial-intelligence methods clarify individual predictions but overlook the
upstream decisions and downstream quality checks that determine whether
insights can be trusted. In this work, we present Holistic Explainable
Artificial Intelligence (HXAI), a user-centric framework that embeds
explanation into every stage of the data-analysis workflow and tailors those
explanations to users. HXAI unifies six components (data, analysis set-up,
learning process, model output, model quality, communication channel) into a
single taxonomy and aligns each component with the needs of domain experts,
data analysts and data scientists. A 112-item question bank covers these needs;
our survey of contemporary tools highlights critical coverage gaps. Grounded in
theories of human explanation, principles from human-computer interaction and
findings from empirical user studies, HXAI identifies the characteristics that
make explanations clear, actionable and cognitively manageable. A comprehensive
taxonomy operationalises these insights, reducing terminological ambiguity and
enabling rigorous coverage analysis of existing toolchains. We further
demonstrate how AI agents that embed large-language models can orchestrate
diverse explanation techniques, translating technical artifacts into
stakeholder-specific narratives that bridge the gap between AI developers and
domain experts. Departing from traditional surveys or perspective articles,
this work melds concepts from multiple disciplines, lessons from real-world
projects and a critical synthesis of the literature to advance a novel,
end-to-end viewpoint on transparency, trustworthiness and responsible AI
deployment.

</details>


### [111] [DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning](https://arxiv.org/abs/2508.11530)
*Lianshuai Guo,Zhongzheng Yuan,Xunkai Li,Yinlin Zhu,Meixia Qu,Wenyu Wang*

Main category: cs.LG

TL;DR: DFed-SST是一种去中心化的联邦图学习框架，通过自适应通信机制优化客户端间的拓扑结构，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化联邦学习（DFL）方法未能充分利用本地子图的拓扑信息，而联邦图学习（FGL）主要采用中心化架构，无法发挥去中心化的优势。

Method: 提出DFed-SST框架，采用双拓扑自适应通信机制，动态构建和优化客户端间的通信拓扑。

Result: 在八个真实数据集上，DFed-SST平均准确率比基线方法提高了3.26%。

Conclusion: DFed-SST有效解决了异构性问题，为去中心化联邦图学习提供了高效解决方案。

Abstract: Decentralized Federated Learning (DFL) has emerged as a robust distributed
paradigm that circumvents the single-point-of-failure and communication
bottleneck risks of centralized architectures. However, a significant challenge
arises as existing DFL optimization strategies, primarily designed for tasks
such as computer vision, fail to address the unique topological information
inherent in the local subgraph. Notably, while Federated Graph Learning (FGL)
is tailored for graph data, it is predominantly implemented in a centralized
server-client model, failing to leverage the benefits of decentralization.To
bridge this gap, we propose DFed-SST, a decentralized federated graph learning
framework with adaptive communication. The core of our method is a
dual-topology adaptive communication mechanism that leverages the unique
topological features of each client's local subgraph to dynamically construct
and optimize the inter-client communication topology. This allows our framework
to guide model aggregation efficiently in the face of heterogeneity. Extensive
experiments on eight real-world datasets consistently demonstrate the
superiority of DFed-SST, achieving 3.26% improvement in average accuracy over
baseline methods.

</details>


### [112] [Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models](https://arxiv.org/abs/2508.11542)
*Nicole Aretz,Karen Willcox*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动的嵌套算子推断方法，用于从高维动态系统的快照数据中学习物理信息降阶模型。该方法通过利用降阶空间内的层次结构，优先考虑主导模式的相互作用，显著降低了误差。


<details>
  <summary>Details</summary>
Motivation: 传统算子推断方法在降阶模型学习中可能无法充分利用降阶空间的层次结构，导致误差较大。本文旨在通过嵌套方法优化初始猜测，提高模型精度。

Method: 采用嵌套算子推断方法，通过迭代构建初始猜测，优先考虑主导模式的相互作用。支持动态基和模型形式的更新。

Result: 在立方热传导问题中，嵌套方法误差比标准方法小四倍；在格陵兰冰盖模型中，平均误差为3%，计算速度提升超过19,000倍。

Conclusion: 嵌套算子推断方法显著提高了降阶模型的精度和计算效率，适用于动态基和模型形式更新的场景。

Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.

</details>


### [113] [SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling](https://arxiv.org/abs/2508.11553)
*Jinghui Wang,Shaojie Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Xiaojiang Zhang,Minglei Zhang,Jiarong Zhang,Wenhao Zhuang,Yuchen Cao,Wankang Bao,Haimo Li,Zheng Lin,Huiming Wang,Haoyang Huang,Zongxian Feng,Zizheng Zhan,Ken Deng,Wen Xiang,Huaixi Tang,Kun Wu,Mengtong Li,Mengfei Xie,Junyi Peng,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SeamlessFlow是一个基于服务器的强化学习框架，解决了工业规模RL中的两个核心挑战：解耦训练与复杂执行流，以及最大化GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 解决工业规模RL中训练与执行流耦合、GPU利用率低的问题。

Method: 引入数据平面解耦训练与代理执行，采用标签驱动调度和时空复用管道优化资源利用。

Result: 实现了稳定性和高性能，适用于多代理和复杂RL任务。

Conclusion: SeamlessFlow通过创新设计，为大规模RL部署提供了高效稳定的解决方案。

Abstract: We introduce SeamlessFlow, a server based reinforcement learning (RL)
framework that addresses two core challenges in industrial scale RL: (1)
decoupling RL training from the complex execution flow of agents; (2)
maximizing GPU utilization with minimal idle time while preserving the
stability and scalability required for large-scale deployments. First,
SeamlessFlow introduces a data plane that decouples the RL trainer from
diverse, complex agent implementations while sustaining high throughput. A
central trajectory manager maintains complete interaction histories and
supports partial rollout, allowing rollout to pause for weight updates and
resume seamlessly, keeping agents unaware of service interruptions. Second, we
propose a tag driven scheduling paradigm that abstracts hardware into
capability tagged resources, unifying colocated and disaggregated
architectures. Based on this, SeamlessFlow introduces a spatiotemporal
multiplexing pipeline that dynamically reassigns idle training nodes to rollout
in a train rollout separated setup, eliminating pipeline bubbles and fully
exploiting heterogeneous cluster resources. By combining these innovations,
SeamlessFlow delivers both stability and high performance, making it well
suited for multi agent, long horizon, and other complex RL tasks.

</details>


### [114] [Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective](https://arxiv.org/abs/2508.11618)
*Jungang Chen,Seyyed A. Hosseini*

Main category: cs.LG

TL;DR: 提出了一种基于马尔可夫游戏的范式，研究不同联盟结构对碳捕集与封存（CCS）项目中利益相关者目标的影响。


<details>
  <summary>Details</summary>
Motivation: CCS项目涉及多方利益相关者，各自目标不同且地质条件复杂，需探讨独立优化还是协作更有效。

Method: 将多利益相关者、多站点问题建模为带安全约束的多智能体强化学习问题，并利用E2C框架的替代模型降低计算成本。

Result: 框架能有效优化多目标利益相关者参与的CO2封存管理。

Conclusion: 协作联盟结构对CCS项目的优化管理至关重要。

Abstract: Carbon capture and storage (CCS) projects typically involve a diverse array
of stakeholders or players from public, private, and regulatory sectors, each
with different objectives and responsibilities. Given the complexity, scale,
and long-term nature of CCS operations, determining whether individual
stakeholders can independently maximize their interests or whether
collaborative coalition agreements are needed remains a central question for
effective CCS project planning and management. CCS projects are often
implemented in geologically connected sites, where shared geological features
such as pressure space and reservoir pore capacity can lead to competitive
behavior among stakeholders. Furthermore, CO2 storage sites are often located
in geologically mature basins that previously served as sites for hydrocarbon
extraction or wastewater disposal in order to leverage existing
infrastructures, which makes unilateral optimization even more complicated and
unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively
investigate how different coalition structures affect the goals of
stakeholders. We frame this multi-stakeholder multi-site problem as a
multi-agent reinforcement learning problem with safety constraints. Our
approach enables agents to learn optimal strategies while compliant with safety
regulations. We present an example where multiple operators are injecting CO2
into their respective project areas in a geologically connected basin. To
address the high computational cost of repeated simulations of high-fidelity
models, a previously developed surrogate model based on the Embed-to-Control
(E2C) framework is employed. Our results demonstrate the effectiveness of the
proposed framework in addressing optimal management of CO2 storage when
multiple stakeholders with various objectives and goals are involved.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [115] [How do Data Journalists Design Maps to Tell Stories?](https://arxiv.org/abs/2508.10903)
*Arlindo Gomes,Emilly Brito,Luis Morais,Nivan Ferreira*

Main category: cs.HC

TL;DR: 论文研究了新闻地图的设计过程和设计空间，通过分析462个新闻地图和采访数据记者，提出了包含八个维度的设计空间，并总结了常见的设计理由和实践中的不足。


<details>
  <summary>Details</summary>
Motivation: 新闻地图在传达空间背景和叙述故事中至关重要，但设计挑战多，如平衡美学、读者数据素养、时间压力和技术能力。研究旨在深入了解新闻机构的地图设计过程。

Method: 收集并分析了462个新闻地图，构建了包含八个维度的设计空间；通过半结构化访谈四位数据记者，总结设计理由和实践中的不足。

Result: 提出了一个包含八个维度的设计空间，涉及文章属性和地图的视觉/交互特征；访谈揭示了常见设计理由和实践中的潜在问题。

Conclusion: 研究为研究人员和记者提供了实证数据，以设计和研究新闻地图，填补了当前实践的不足。

Abstract: Maps are essential to news media as they provide a familiar way to convey
spatial context and present engaging narratives. However, the design of
journalistic maps may be challenging, as editorial teams need to balance
multiple aspects, such as aesthetics, the audience's expected data literacy,
tight publication deadlines, and the team's technical skills. Data journalists
often come from multiple areas and lack a cartography, data visualization, and
data science background, limiting their competence in creating maps. While
previous studies have examined spatial visualizations in data stories, this
research seeks to gain a deeper understanding of the map design process
employed by news outlets. To achieve this, we strive to answer two specific
research questions: what is the design space of journalistic maps? and how do
editorial teams produce journalistic map articles? To answer the first one, we
collected and analyzed a large corpus of 462 journalistic maps used in news
articles from five major news outlets published over three months. As a result,
we created a design space comprised of eight dimensions that involved both
properties describing the articles' aspects and the visual/interactive features
of maps. We approach the second research question via semi-structured
interviews with four data journalists who create data-driven articles daily.
Through these interviews, we identified the most common design rationales made
by editorial teams and potential gaps in current practices. We also collected
the practitioners' feedback on our design space to externally validate it. With
these results, we aim to provide researchers and journalists with empirical
data to design and study journalistic maps.

</details>


### [116] [Designing for Engaging Communication Between Parents and Young Adult Children Through Shared Music Experiences](https://arxiv.org/abs/2508.10907)
*Euihyeok Lee,Souneil Park,Jin Yu,Seungchul Lee,Seungwoo Kang*

Main category: cs.HC

TL;DR: 论文提出通过音乐促进异地父母与成年子女的社交互动，开发了DJ-Fam应用，并通过实验验证其效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用音乐增强异地父母与成年子女的日常互动，弥补传统沟通方式的不足。

Method: 探索现有沟通需求，开发DJ-Fam应用，通过实验验证其效果。

Result: DJ-Fam显著增加了沟通频率和话题多样性，参与者满意度高。

Conclusion: DJ-Fam通过音乐有效促进了亲子互动和关系改善。

Abstract: This paper aims to foster social interaction between parents and young adult
children living apart via music. Our approach transforms their music-listening
moment into an opportunity to listen to the other's favorite songs and enrich
interaction in their daily lives. To this end, we explore the current practice
and needs of parent-child communication and the experience and perception of
music-mediated interaction. Based on the findings, we developed DJ-Fam, a
mobile application that enables parents and children to listen to their
favorite songs and use them as conversation starters to foster parent-child
interaction. From our deployment study with seven families over four weeks in
South Korea, we show the potential of DJ-Fam to influence parent-child
interaction and their mutual understanding and relationship positively.
Specifically, DJ-Fam considerably increases the frequency of communication and
diversifies the communication channels and topics, all of which are
satisfactory to the participants.

</details>


### [117] [Uncovering Latent Connections in Indigenous Heritage: Semantic Pipelines for Cultural Preservation in Brazil](https://arxiv.org/abs/2508.10911)
*Luis Vitor Zerkowski,Nina S. T. Hirata*

Main category: cs.HC

TL;DR: 论文探讨了如何利用人工智能技术提升巴西土著文化遗产的在线保存与展示，通过视觉和文本两种语义管道增强数据的可访问性和探索性。


<details>
  <summary>Details</summary>
Motivation: 面对系统性边缘化和城市发展，土著社区的文化遗产保护面临挑战，需要创新方法来增强文化遗产的保存和传播。

Method: 开发了两种语义管道：视觉管道建模图像相似性，文本管道捕捉描述中的语义关系，并将其整合到交互式可视化工具中。

Result: 系统支持策展任务、促进公众参与，并揭示了藏品中的潜在联系，展示了AI在文化保护中的伦理贡献。

Conclusion: 该研究证明了AI可以以伦理方式为文化遗产保护实践做出贡献，提供了一种创新的技术解决方案。

Abstract: Indigenous communities face ongoing challenges in preserving their cultural
heritage, particularly in the face of systemic marginalization and urban
development. In Brazil, the Museu Nacional dos Povos Indigenas through the
Tainacan platform hosts the country's largest online collection of Indigenous
objects and iconographies, providing a critical resource for cultural
engagement. Using publicly available data from this repository, we present a
data-driven initiative that applies artificial intelligence to enhance
accessibility, interpretation, and exploration. We develop two semantic
pipelines: a visual pipeline that models image-based similarity and a textual
pipeline that captures semantic relationships from item descriptions. These
embedding spaces are projected into two dimensions and integrated into an
interactive visualization tool we also developed. In addition to
similarity-based navigation, users can explore the collection through temporal
and geographic lenses, enabling both semantic and contextualized perspectives.
The system supports curatorial tasks, aids public engagement, and reveals
latent connections within the collection. This work demonstrates how AI can
ethically contribute to cultural preservation practices.

</details>


### [118] [Generation and Evaluation in the Human Invention Process through the Lens of Game Design](https://arxiv.org/abs/2508.10914)
*Katherine M. Collins,Graham Todd,Cedegao E. Zhang,Adrian Weller,Julian Togelius,Junyi Chu,Lionel Wong,Thomas L. Griffiths,Joshua B. Tenenbaum*

Main category: cs.HC

TL;DR: 研究探讨了人类通过游戏设计展现的创新能力，分析了新手游戏设计中的认知机制，发现基于模型的质量评估是关键。


<details>
  <summary>Details</summary>
Motivation: 探索人类如何通过创造和评估规则来展现智能，特别是在低风险的日常游戏设计中。

Method: 分析了450多个由新手设计的游戏，研究了基于联想和模型评估的认知机制。

Result: 生成游戏的最佳描述是基于群体水平游戏质量的模型评估。

Conclusion: 人类创新不仅依赖于提议，还依赖于评估，研究为开放式创新提供了计算工具。

Abstract: The human ability to learn rules and solve problems has been a central
concern of cognitive science research since the field's earliest days. But we
do not just follow rules and solve problems given to us by others: we modify
those rules, create new problems, and set new goals and tasks for ourselves and
others. Arguably, even more than rule following and problem solving, human
intelligence is about creatively breaking and stretching the rules, changing
the game, and inventing new problems worth thinking about. Creating a good rule
or a good problem depends not just on the ideas one can think up but on how one
evaluates such proposals. Here, we study invention through the lens of game
design. We focus particularly on the early stages of novice, "everyday" game
creation, where the stakes are low. We draw on a dataset of over 450 human
created games, created by participants who saw an initial seed set of
two-player grid-based strategy games. We consider two different cognitive
mechanisms that may be at work during the early processes of intuitive game
invention: an associative proposal based on previous games one has seen and
compute-bounded model-based evaluation that an everyday game creator may use to
refine their initial draft proposals. In our preliminary work, we conduct a
model-based analysis of how people invented new games based on prior experience
and find that generated games are best described by a model which incorporates
model-based estimates of game quality at a population level. Our work points to
how human invention is based not only on what people propose, but how they
evaluate and offers a computational toolkit to scale empirical studies of
model-based simulation in open-ended human innovation.

</details>


### [119] [Multimodal Quantitative Measures for Multiparty Behaviour Evaluation](https://arxiv.org/abs/2508.10916)
*Ojas Shirekar,Wim Pouw,Chenxu Hao,Vrushank Phadnis,Thabo Beeler,Chirag Raman*

Main category: cs.HC

TL;DR: 提出了一种用于评估多参与者社交行为的统一框架，涵盖同步性、时间对齐和结构相似性三个维度，并通过实验验证了其敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标忽视了多参与者交互中的上下文协调动态，需要一种更全面的评估方法。

Method: 采用交叉递归量化分析（CRQA）、多尺度经验模态分解的节拍一致性（Beat Consistency）和软动态时间规整（Soft-DTW）三种方法。

Result: 实验表明，三种理论驱动的扰动（手势运动阻尼、语音-手势延迟、音高方差减少）对指标有可预测的影响。

Conclusion: 该框架为评估和优化社交智能代理提供了可靠工具。

Abstract: Digital humans are emerging as autonomous agents in multiparty interactions,
yet existing evaluation metrics largely ignore contextual coordination
dynamics. We introduce a unified, intervention-driven framework for objective
assessment of multiparty social behaviour in skeletal motion data, spanning
three complementary dimensions: (1) synchrony via Cross-Recurrence
Quantification Analysis, (2) temporal alignment via Multiscale Empirical Mode
Decompositionbased Beat Consistency, and (3) structural similarity via Soft
Dynamic Time Warping. We validate metric sensitivity through three
theory-driven perturbations -- gesture kinematic dampening, uniform
speech-gesture delays, and prosodic pitch-variance reduction-applied to
$\approx 145$ 30-second thin slices of group interactions from the DnD dataset.
Mixed-effects analyses reveal predictable, joint-independent shifts: dampening
increases CRQA determinism and reduces beat consistency, delays weaken
cross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A
complementary perception study ($N=27$) compares judgments of full-video and
skeleton-only renderings to quantify representation effects. Our three measures
deliver orthogonal insights into spatial structure, timing alignment, and
behavioural variability. Thereby forming a robust toolkit for evaluating and
refining socially intelligent agents. Code available on
\href{https://github.com/tapri-lab/gig-interveners}{GitHub}.

</details>


### [120] [Managing the unexpected: Operator behavioural data and its value in predicting correct alarm responses](https://arxiv.org/abs/2508.10917)
*Chidera W. Amazu,Joseph Mietkiewicz,Ammar N. Abbas,Gabriele Baldissone,Davide Fissore,Micaela Demichela,Anders L. Madsen,Maria Chiara Leva*

Main category: cs.HC

TL;DR: 研究探讨了如何利用实时数据和操作员-系统交互数据预测操作员行为，而无需侵入性生理测量工具。


<details>
  <summary>Details</summary>
Motivation: 侵入性生理测量工具（如眼动仪和EEG帽）在日常操作中可能不适用，因此需要非侵入性方法来预测操作员行为和响应结果。

Method: 使用甲醛生产厂模拟器和四种人机实验配置获取数据，采用逐步逻辑回归和贝叶斯网络模型进行分析。

Result: 研究确定了一些预测性指标，可作为警报响应场景中系统性能的先兆或预测因子。

Conclusion: 实时获取相关行为指标有助于决策者预测结果并为操作员提供及时支持。

Abstract: Data from psychophysiological measures can offer new insight into control
room operators' behaviour, cognition, and mental workload status. This can be
particularly helpful when combined with appraisal of capacity to respond to
possible critical plant conditions (i.e. critical alarms response scenarios).
However, wearable physiological measurement tools such as eye tracking and EEG
caps can be perceived as intrusive and not suitable for usage in daily
operations. Therefore, this article examines the potential of using real-time
data from process and operator-system interactions during abnormal scenarios
that can be recorded and retrieved from the distributed control system's
historian or process log, and their capacity to provide insight into operator
behavior and predict their response outcomes, without intruding on daily tasks.
Data for this study were obtained from a design of experiment using a
formaldehyde production plant simulator and four human-in-the-loop experimental
support configurations. A comparison between the different configurations in
terms of both behaviour and performance is presented in this paper. A step-wise
logistic regression and a Bayesian network models were used to achieve this
objective. The results identified some predictive metrics and the paper discuss
their value as precursor or predictor of overall system performance in alarm
response scenarios. Knowledge of relevant and predictive behavioural metrics
accessible in real time can better equip decision-makers to predict outcomes
and provide timely support measures for operators.

</details>


### [121] [Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?](https://arxiv.org/abs/2508.10919)
*Mohammed Saqr,Kamila Misiejuk,Sonsoles López-Pernas*

Main category: cs.HC

TL;DR: 研究探讨了人类与AI在解决复杂问题时的互动模式，发现当前LLMs更倾向于指令遵循而非认知协作，缺乏协同效应。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注语言学习，忽视了认知任务中协作的动态与演化，本研究填补了这一空白。

Method: 通过定性编码、转移网络分析、序列分析等方法分析学生与AI的互动模式。

Result: 发现互动以指令模式为主，缺乏协同；问题复杂度与成绩无显著关联。

Conclusion: 当前LLMs更适合指令执行而非认知协作，需设计更注重认知对齐的AI系统。

Abstract: While research on human-AI collaboration exists, it mainly examined language
learning and used traditional counting methods with little attention to
evolution and dynamics of collaboration on cognitively demanding tasks. This
study examines human-AI interactions while solving a complex problem.
Student-AI interactions were qualitatively coded and analyzed with transition
network analysis, sequence analysis and partial correlation networks as well as
comparison of frequencies using chi-square and Person-residual shaded Mosaic
plots to map interaction patterns, their evolution, and their relationship to
problem complexity and student performance. Findings reveal a dominant
Instructive pattern with interactions characterized by iterative ordering
rather than collaborative negotiation. Oftentimes, students engaged in long
threads that showed misalignment between their prompts and AI output that
exemplified a lack of synergy that challenges the prevailing assumptions about
LLMs as collaborative partners. We also found no significant correlations
between assignment complexity, prompt length, and student grades suggesting a
lack of cognitive depth, or effect of problem difficulty. Our study indicates
that the current LLMs, optimized for instruction-following rather than
cognitive partnership, compound their capability to act as cognitively
stimulating or aligned collaborators. Implications for designing AI systems
that prioritize cognitive alignment and collaboration are discussed.

</details>


### [122] [GhostObjects: Instructing Robots by Manipulating Spatially Aligned Virtual Twins in Augmented Reality](https://arxiv.org/abs/2508.11022)
*Lauren W. Wang,Parastoo Abtahi*

Main category: cs.HC

TL;DR: 通过AR中的GhostObjects实现机器人指令交互，替代传统编程或遥控方式。


<details>
  <summary>Details</summary>
Motivation: 机器人自主操作能力提升，但仍需人类个性化指令交互。

Method: 利用AR中的GhostObjects（物理对象的虚拟孪生体）进行直接操作，支持多对象选择和复位功能。

Result: 用户能精确指定物理目标和空间参数，支持复杂任务。

Conclusion: GhostObjects提供了一种更直观、灵活的机器人指令交互方式。

Abstract: Robots are increasingly capable of autonomous operations, yet human
interaction remains essential for issuing personalized instructions. Instead of
directly controlling robots through Programming by Demonstration (PbD) or
teleoperation, we propose giving instructions by interacting with
GhostObjects-world-aligned, life-size virtual twins of physical objects-in
augmented reality (AR). By direct manipulation of GhostObjects, users can
precisely specify physical goals and spatial parameters, with features
including real-world lasso selection of multiple objects and snapping back to
default positions, enabling tasks beyond simple pick-and-place.

</details>


### [123] [Families' Vision of Generative AI Agents for Household Safety Against Digital and Physical Threats](https://arxiv.org/abs/2508.11030)
*Zikai Wen,Lanjing Liu,Yaxing Yao*

Main category: cs.HC

TL;DR: 研究探讨了家庭如何通过多个人工智能代理利用生成式AI提升家庭安全，发现家庭倾向于将安全任务分配给不同角色代理，并提出了隐私保护设计原则。


<details>
  <summary>Details</summary>
Motivation: 随着家庭在数字和物理环境中面临的安全挑战日益复杂，研究旨在探索生成式AI如何通过多代理系统支持家庭安全。

Method: 采用两阶段定性研究，包括13对亲子家庭的访谈和协作会议，分析家庭对生成式AI的认知及AI代理的潜在应用。

Result: 家庭倾向于将安全任务分配给不同角色代理（如家庭经理、私人导师、家庭治疗师），并强调隐私边界、代际信任差异和家庭沟通的重要性。

Conclusion: 研究提出了一种多代理系统设计，包含四项隐私保护原则，以平衡家庭环境中的安全、隐私和自主性。

Abstract: As families face increasingly complex safety challenges in digital and
physical environments, generative AI (GenAI) presents new opportunities to
support household safety through multiple specialized AI agents. Through a
two-phase qualitative study consisting of individual interviews and
collaborative sessions with 13 parent-child dyads, we explored families'
conceptualizations of GenAI and their envisioned use of AI agents in daily
family life. Our findings reveal that families preferred to distribute
safety-related support across multiple AI agents, each embodying a familiar
caregiving role: a household manager coordinating routine tasks and mitigating
risks such as digital fraud and home accidents; a private tutor providing
personalized educational support, including safety education; and a family
therapist offering emotional support to address sensitive safety issues such as
cyberbullying and digital harassment. Families emphasized the need for
agent-specific privacy boundaries, recognized generational differences in trust
toward AI agents, and stressed the importance of maintaining open family
communication alongside the assistance of AI agents. Based on these findings,
we propose a multi-agent system design featuring four privacy-preserving
principles: memory segregation, conversational consent, selective data sharing,
and progressive memory management to help balance safety, privacy, and autonomy
within family contexts.

</details>


### [124] [AI That Helps Us Help Each Other: A Proactive System for Scaffolding Mentor-Novice Collaboration in Entrepreneurship Coaching](https://arxiv.org/abs/2508.11052)
*Evey Jiaxin Huang,Matthew Easterday,Elizabeth Gerber*

Main category: cs.HC

TL;DR: 论文提出了一种结合领域特定认知模型与大型语言模型（LLM）的人机协作系统，用于支持创业新手和导师的元认知能力，提升会议质量和情感共鸣。


<details>
  <summary>Details</summary>
Motivation: 创业新手在应对开放式问题时面临元认知挑战，导师则因时间和资源有限难以提供个性化支持。

Method: 系统结合认知模型与LLM，主动提出问题以挑战新手思维，并帮助导师规划会议。导师可调整认知模型以满足需求。

Result: 实地部署显示，系统提升了新手的元认知能力、导师的情感共鸣策略，并改善了会议质量，但也揭示了AI信任等问题。

Conclusion: 论文为复杂领域中的AI系统设计提供了原则，适用于医疗、教育等类似领域。

Abstract: Entrepreneurship requires navigating open-ended, ill-defined problems:
identifying risks, challenging assumptions, and making strategic decisions
under deep uncertainty. Novice founders often struggle with these metacognitive
demands, while mentors face limited time and visibility to provide tailored
support. We present a human-AI coaching system that combines a domain-specific
cognitive model of entrepreneurial risk with a large language model (LLM) to
proactively scaffold both novice and mentor thinking. The system proactively
poses diagnostic questions that challenge novices' thinking and helps both
novices and mentors plan for more focused and emotionally attuned meetings.
Critically, mentors can inspect and modify the underlying cognitive model,
shaping the logic of the system to reflect their evolving needs. Through an
exploratory field deployment, we found that using the system supported novice
metacognition, helped mentors plan emotionally attuned strategies, and improved
meeting depth, intentionality, and focus--while also surfaced key tensions
around trust, misdiagnosis, and expectations of AI. We contribute design
principles for proactive AI systems that scaffold metacognition and human-human
collaboration in complex, ill-defined domains, offering implications for
similar domains like healthcare, education, and knowledge work.

</details>


### [125] [Stories and Systems: Educational Interactive Storytelling to Teach Media Literacy and Systemic Thinking](https://arxiv.org/abs/2508.11059)
*Christian Roth,Rahmin Bender-Salazar,Breanne Pitt*

Main category: cs.HC

TL;DR: 本文探讨了交互式数字叙事（IDNs）如何帮助学习者培养应对复杂社会挑战（如气候变化、疫情和社会不平等）所需的批判性素养，提出了Systemic Learning IDNs和CLASS框架。


<details>
  <summary>Details</summary>
Motivation: 数字技术虽提供了广泛的叙事和数据访问，但也导致错误信息和问题简化。IDNs能通过非线性互动故事促进深度理解和参与。

Method: 提出CLASS框架，整合系统思维、设计思维和叙事，应用于两个案例（商业叙事模拟和教育原型）。

Result: 通过叙事、系统映射和参与式设计，IDNs成为变革性系统导向学习的强大工具。

Conclusion: IDNs在复杂世界中具有潜力，CLASS框架为设计和实施提供了实用指导。

Abstract: This paper explores how Interactive Digital Narratives (IDNs) can support
learners in developing the critical literacies needed to address complex
societal challenges, so-called wicked problems, such as climate change,
pandemics, and social inequality. While digital technologies offer broad access
to narratives and data, they also contribute to misinformation and the
oversimplification of interconnected issues. IDNs enable learners to navigate
nonlinear, interactive stories, fostering deeper understanding and engagement.
We introduce Systemic Learning IDNs: interactive narrative experiences
explicitly designed to help learners explore and reflect on complex systems and
interdependencies. To guide their creation and use, we propose the CLASS
framework, a structured model that integrates systems thinking, design
thinking, and storytelling. This transdisciplinary approach supports learners
in developing curiosity, critical thinking, and collaborative problem-solving.
Focusing on the classroom context, we apply CLASS to two cases, one commercial
narrative simulation and one educational prototype, offering a comparative
analysis and practical recommendations for future design and implementation. By
combining narrative, systems mapping, and participatory design, this paper
highlights how IDNs can become powerful tools for transformative,
systems-oriented learning in an increasingly complex world.

</details>


### [126] [Human-in-the-Loop Systems for Adaptive Learning Using Generative AI](https://arxiv.org/abs/2508.11062)
*Bhavishya Tarun,Haoze Du,Dinesh Kannan,Edward F. Gehringer*

Main category: cs.HC

TL;DR: 论文提出了一种基于人机交互（HITL）的方法，利用生成式AI增强个性化学习，通过学生反馈实时调整AI生成的内容，提升学习效果和参与度。


<details>
  <summary>Details</summary>
Motivation: 旨在通过学生直接参与AI生成内容的修改和反馈，实现更个性化的学习体验，特别是在STEM教育中提高学生的理解和信心。

Method: 采用反馈标签技术和提示工程，结合检索增强生成（RAG）系统，实时检索和调整教育内容。

Result: 初步研究表明，与传统AI工具相比，该方法显著提升了学生的学习效果和自信心。

Conclusion: 该研究展示了AI通过迭代优化和反馈驱动，在构建动态个性化学习环境中的潜力。

Abstract: A Human-in-the-Loop (HITL) approach leverages generative AI to enhance
personalized learning by directly integrating student feedback into
AI-generated solutions. Students critique and modify AI responses using
predefined feedback tags, fostering deeper engagement and understanding. This
empowers students to actively shape their learning, with AI serving as an
adaptive partner. The system uses a tagging technique and prompt engineering to
personalize content, informing a Retrieval-Augmented Generation (RAG) system to
retrieve relevant educational material and adjust explanations in real time.
This builds on existing research in adaptive learning, demonstrating how
student-driven feedback loops can modify AI-generated responses for improved
student retention and engagement, particularly in STEM education. Preliminary
findings from a study with STEM students indicate improved learning outcomes
and confidence compared to traditional AI tools. This work highlights AI's
potential to create dynamic, feedback-driven, and personalized learning
environments through iterative refinement.

</details>


### [127] [DriveSimQuest: A VR Driving Simulator and Research Platform on Meta Quest with Unity](https://arxiv.org/abs/2508.11072)
*Nishanth Chidambaram,Weichen Liu,Manas Satish Bedmutha,Nadir Weibel,Chen Chen*

Main category: cs.HC

TL;DR: DriveSimQuest是一个基于Meta Quest Pro和Unity的VR驾驶模拟器，能够实时捕捉多种行为信号，为研究驾驶行为和设计辅助系统提供便捷平台。


<details>
  <summary>Details</summary>
Motivation: 现有VR驾驶模拟器通常仅能追踪眼球运动，且设备笨重、架构复杂，限制了交互研究和实践。

Method: 基于Meta Quest Pro和Unity开发，支持实时捕捉视线、面部表情、手部活动和全身动作。

Result: DriveSimQuest提供了一个易于部署的平台，支持研究驾驶者的情感状态和行为。

Conclusion: 该平台为未来情境感知驾驶辅助系统的设计提供了初步支持。

Abstract: Using head-mounted Virtual Reality (VR) displays to simulate driving is
critical to studying driving behavior and designing driver assistance systems.
But existing VR driving simulators are often limited to tracking only eye
movements. The bulky outside-in tracking setup and Unreal-based architecture
also present significant engineering challenges for interaction researchers and
practitioners. We present DriveSimQuest, a VR driving simulator and research
platform built on the Meta Quest Pro and Unity, capable of capturing rich
behavioral signals such as gaze, facial expressions, hand activities, and
full-body gestures in real-time. DriveSimQuest offers a preliminary,
easy-to-deploy platform that supports researchers and practitioners in studying
drivers' affective states and behaviors, and in designing future context-aware
driving assistance systems.

</details>


### [128] [Toward Needs-Conscious Design: Co-Designing a Human-Centered Framework for AI-Mediated Communication](https://arxiv.org/abs/2508.11149)
*Robert Wolfe,Aayushi Dangol,JaeWon Kim,Alexis Hiniker*

Main category: cs.HC

TL;DR: 论文提出了一种基于非暴力沟通原则的AI中介沟通框架“需求意识设计”，通过访谈和日记研究定义了其三大支柱，并探讨了AI沟通中的“共情迷雾”问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过非暴力沟通原则设计AI中介沟通工具，以促进人际关系而非替代或模糊它。

Method: 通过访谈14名认证NVC培训师和13名普通用户的日记研究及共同设计，定义了需求意识设计的三大支柱。

Result: 提出了“共情迷雾”现象，并提供了设计概念和指导问题，以支持用户知情同意的需求意识设计。

Conclusion: 需求意识设计为利用AI促进人际联系而非替代提供了基础。

Abstract: We introduce Needs-Conscious Design, a human-centered framework for
AI-mediated communication that builds on the principles of Nonviolent
Communication (NVC). We conducted an interview study with N=14 certified NVC
trainers and a diary study and co-design with N=13 lay users of online
communication technologies to understand how NVC might inform design that
centers human relationships. We define three pillars of Needs-Conscious Design:
Intentionality, Presence, and Receptiveness to Needs. Drawing on participant
co-designs, we provide design concepts and illustrative examples for each of
these pillars. We further describe a problematic emergent property of
AI-mediated communication identified by participants, which we call Empathy
Fog, and which is characterized by uncertainty over how much empathy,
attention, and effort a user has actually invested via an AI-facilitated online
interaction. Finally, because even well-intentioned designs may alter user
behavior and process emotional data, we provide guiding questions for
consentful Needs-Conscious Design, applying an affirmative consent framework
used in social media contexts. Needs-Conscious Design offers a foundation for
leveraging AI to facilitate human connection, rather than replacing or
obscuring it.

</details>


### [129] [From Misunderstandings to Learning Opportunities: Leveraging Generative AI in Discussion Forums to Support Student Learning](https://arxiv.org/abs/2508.11150)
*Stanislav Pozdniakov,Jonathan Brazil,Oleksandra Poquet,Stephan Krusche,Santiago Berrezueta-Guzman,Shazia Sadiq,Hassan Khosravi*

Main category: cs.HC

TL;DR: 论文探讨了在大课堂讨论论坛中，如何利用大型语言模型（LLMs）和检索增强生成（RAG）技术识别学生常见误解，并提出解决方法。通过真实课程数据验证，方法被证明有效且受到教师认可。


<details>
  <summary>Details</summary>
Motivation: 解决大课堂讨论论坛中学生误解识别和教师有效干预的挑战。

Method: 结合大型语言模型（LLMs）和检索增强生成（RAG）技术，提出Misunderstanding to Mastery（M2M）方法。

Result: 在三个计算机科学课程中验证，教师认为方法有效且实用，但也提出了改进需求。

Conclusion: M2M方法在识别学生误解和生成可操作见解方面具有潜力，但需进一步优化和考虑伦理问题。

Abstract: In the contemporary educational landscape, particularly in large classroom
settings, discussion forums have become a crucial tool for promoting
interaction and addressing student queries. These forums foster a collaborative
learning environment where students engage with both the teaching team and
their peers. However, the sheer volume of content generated in these forums
poses two significant interconnected challenges: How can we effectively
identify common misunderstandings that arise in student discussions? And once
identified, how can instructors use these insights to address them effectively?
This paper explores the approach to integrating large language models (LLMs)
and Retrieval-Augmented Generation (RAG) to tackle these challenges. We then
demonstrate the approach Misunderstanding to Mastery (M2M) with authentic data
from three computer science courses, involving 1355 students with 2878 unique
posts, followed by an evaluation with five instructors teaching these courses.
Results show that instructors found the approach promising and valuable for
teaching, effectively identifying misunderstandings and generating actionable
insights. Instructors highlighted the need for more fine-grained groupings,
clearer metrics, validation of the created resources, and ethical
considerations around data anonymity.

</details>


### [130] [Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas](https://arxiv.org/abs/2508.11278)
*Francesco Sovrano,Gabriele Dominici,Rita Sevastjanova,Alessandra Stramiglio,Alberto Bacchelli*

Main category: cs.HC

TL;DR: 该论文提出了一个动态基准框架，用于评估通用人工智能（GPAI）在软件工程中是否存在数据诱导的认知偏差，发现GPAI系统普遍依赖浅层语言启发式而非深度推理。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨GPAI系统是否因训练数据而表现出人类认知偏差，从而在软件工程中引发潜在风险。

Method: 方法包括开发动态基准框架，通过16个手工任务和AI生成的任务变体，测试GPAI系统对偏差诱导线索的敏感性。

Result: 结果显示，所有测试的GPAI系统（如GPT、LLaMA、DeepSeek）均表现出认知偏差（5.9%至35%），且偏差敏感性随任务复杂度增加而显著上升（最高49%）。

Conclusion: 结论指出GPAI系统在软件工程中存在认知偏差风险，需进一步研究和改进以降低实际部署中的潜在危害。

Abstract: Human cognitive biases in software engineering can lead to costly errors.
While general-purpose AI (GPAI) systems may help mitigate these biases due to
their non-human nature, their training on human-generated data raises a
critical question: Do GPAI systems themselves exhibit cognitive biases?
  To investigate this, we present the first dynamic benchmarking framework to
evaluate data-induced cognitive biases in GPAI within software engineering
workflows. Starting with a seed set of 16 hand-crafted realistic tasks, each
featuring one of 8 cognitive biases (e.g., anchoring, framing) and
corresponding unbiased variants, we test whether bias-inducing linguistic cues
unrelated to task logic can lead GPAI systems from correct to incorrect
conclusions.
  To scale the benchmark and ensure realism, we develop an on-demand
augmentation pipeline relying on GPAI systems to generate task variants that
preserve bias-inducing cues while varying surface details. This pipeline
ensures correctness (88--99% on average, according to human evaluation),
promotes diversity, and controls reasoning complexity by leveraging
Prolog-based reasoning and LLM-as-a-judge validation. It also verifies that the
embedded biases are both harmful and undetectable by logic-based, unbiased
reasoners.
  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent
tendency to rely on shallow linguistic heuristics over deep reasoning. All
systems exhibit cognitive biases (ranging from 5.9% to 35% across types), with
bias sensitivity increasing sharply with task complexity (up to 49%),
highlighting critical risks in real-world software engineering deployments.

</details>


### [131] [GulliVR: A Walking-Oriented Technique for Navigation in Virtual Reality Games Based on Virtual Body Resizing](https://arxiv.org/abs/2508.11304)
*Andrey Krekhov,Sebastian Cmentowski,Katharina Emmerich,Maic Masuch,Jens Krüger*

Main category: cs.HC

TL;DR: 论文提出了一种通过将玩家变成巨人来增强虚拟现实游戏中物理行走体验的导航方法，相比传统的传送技术，显著提升了存在感和行走距离。


<details>
  <summary>Details</summary>
Motivation: 现代虚拟现实系统虽然支持房间尺度的运动，但实际空间限制了探索广阔虚拟环境的能力。开发者通常使用传送技术绕过这一限制，但这往往导致玩家停止物理行走。

Method: 提出了一种导航隐喻，通过按需将玩家变成巨人，按比例增加模拟的视距，从而覆盖大距离移动，同时避免晕动症并产生微型世界的感觉。

Result: 评估表明，相比传送技术，该方法显著提升了玩家的存在感和行走距离。

Conclusion: 该方法为游戏设计提供了新的启示，强调了物理行走的重要性。

Abstract: Virtual reality games are often centered around our feeling of "being there".
That presence can be significantly enhanced by supporting physical walking.
Although modern virtual reality systems enable room-scale motions, the size of
our living rooms is not enough to explore vast virtual environments. Developers
bypass that limitation by adding virtual navigation such as teleportation.
Although such techniques are intended (or designed) to extend but not replace
natural walking, what we often observe are nonmoving players beaming to a
location that is one real step ahead. Our navigation metaphor emphasizes
physical walking by promoting players into giants on demand to cover large
distances. In contrast to flying, our technique proportionally increases the
modeled eye distance, preventing cybersickness and creating the feeling of
being in a miniature world. Our evaluations underpin a significantly increased
presence and walking distance compared to the teleportation approach. Finally,
we derive a set of game design implications related to the integration of our
technique.

</details>


### [132] [Outpace Reality: A Novel Augmented-Walking Technique for Virtual Reality Games](https://arxiv.org/abs/2508.11314)
*Sebastian Cmentowski,Fabian Kievelitz,Jens Krüger*

Main category: cs.HC

TL;DR: 提出了一种虚拟隧道技术，通过隐藏视觉流来避免晕动症，同时增强物理活动和沉浸感。


<details>
  <summary>Details</summary>
Motivation: 解决虚拟环境中物理行走空间不足的问题，同时避免因视觉流增加导致的晕动症。

Method: 设计了一种虚拟隧道，外部显示完整距离，内部实际距离较短，通过真实行走覆盖距离，同时隧道墙壁上的窗口揭示实际加速运动。

Result: 避免了晕动症，增强了物理活动，并保持了沉浸感。

Conclusion: 讨论了该运动技术的设计考虑和局限性。

Abstract: The size of most virtual environments exceeds the tracking space available
for physical walking. One solution to this disparity is to extend the available
walking range by augmenting users' actual movements. However, the resulting
increase in visual flow can easily cause cybersickness. Therefore, we present a
novel augmented-walking approach for virtual reality games. Our core concept is
a virtual tunnel that spans the entire travel distance when viewed from the
outside. However, its interior is only a fraction as long, allowing users to
cover the distance by real walking. Whereas the tunnel hides the visual flow
from the applied movement acceleration, windows on the tunnel's walls still
reveal the actual expedited motion. Our evaluation reveals that our approach
avoids cybersickness while enhancing physical activity and preserving presence.
We finish our paper with a discussion of the design considerations and
limitations of our proposed locomotion technique.

</details>


### [133] [The User-first Approach to AI Ethics: Preferences for Ethical Principles in AI Systems across Cultures and Contexts](https://arxiv.org/abs/2508.11327)
*Benjamin J. Carroll,Jianlong Zhou,Paul F. Burke,Sabine Ammon*

Main category: cs.HC

TL;DR: 用户对AI伦理原则的偏好存在文化差异，隐私、公正与透明是普遍优先项，但需结合文化和情境定制伦理实践。


<details>
  <summary>Details</summary>
Motivation: 探讨用户对AI伦理原则的实际偏好，填补现有文献中用户视角的空白。

Method: 在四个国家进行离散选择实验，量化用户对11项伦理原则的偏好，并采用潜在类别分析。

Result: 用户偏好隐私、公正与透明，但存在文化差异；发现四类用户群体，最大群体对伦理漠然并依赖监管。

Conclusion: 研究为定制化伦理实践、加强监管机制及推进用户为中心的AI伦理提供了依据。

Abstract: As AI systems increasingly permeate everyday life, designers and developers
face mounting pressure to balance innovation with ethical design choices. To
date, the operationalisation of AI ethics has predominantly depended on
frameworks that prescribe which ethical principles should be embedded within AI
systems. However, the extent to which users value these principles remains
largely unexplored in the existing literature. In a discrete choice experiment
conducted in four countries, we quantify user preferences for 11 ethical
principles. Our findings indicate that, while users generally prioritise
privacy, justice & fairness, and transparency, their preferences exhibit
significant variation based on culture and application context. Latent class
analysis further revealed four distinct user cohorts, the largest of which is
ethically disengaged and defers to regulatory oversight. Our findings offer (1)
empirical evidence of uneven user prioritisation of AI ethics principles, (2)
actionable guidance for operationalising ethics tailored to culture and
context, (3) support for the development of robust regulatory mechanisms, and
(4) a foundation for advancing a user-centred approach to AI ethics, motivated
independently from abstract moral theory.

</details>


### [134] [Towards Smart Workplaces: Understanding Mood-Influencing Factors of the Physical Workspace in Collaborative Group Settings](https://arxiv.org/abs/2508.11335)
*Tzu-Hui Wu,Sebastian Cmentowski,Yunyin Lou,Jun Hu,Regina Bernhaupt*

Main category: cs.HC

TL;DR: 研究探讨了物理工作空间如何影响团队情绪，并探索了智能技术（如自适应环境照明）在情绪调节中的潜力。


<details>
  <summary>Details</summary>
Motivation: 团队情绪对工作体验、团队表现和创造力至关重要，但维持积极氛围具有挑战性。智能技术可能提供支持，但物理工作空间与团队情绪的关系尚不明确。

Method: 进行了定性用户研究（8个工作小组，共26名参与者），探讨物理工作空间对团队情绪的影响及员工对智能情绪感知技术的看法。

Result: 研究发现影响团队情绪的关键因素，以及参与者对支持性技术的期望（如保护隐私和自主权）。

Conclusion: 研究强调了自适应工作空间的潜力，同时指出需以人为中心设计技术干预，以促进团队福祉。

Abstract: Group mood plays a crucial role in shaping workspace experiences, influencing
group dynamics, team performance, and creativity. The perceived group mood
depends on many, often subconscious, aspects such as individual emotional
states or group life, which make it challenging to maintain a positive
atmosphere. Intelligent technology could support mood regulation in physical
office environments, for example, as adaptive ambient lighting for mood
regulation. However, little is known about the relationship between the
physical workspace and group mood dynamics. To address this knowledge gap, we
conducted a qualitative user study (N=8 workgroups and overall 26 participants)
to explore how the physical workspace shapes group mood experiences and
investigate employees' perspectives on intelligent mood-aware technologies. Our
findings reveal key factors influencing group mood, and participants'
expectations for supportive technology to preserve privacy and autonomy. Our
work highlights the potential of adaptive and responsive workspaces while also
emphasizing the need for human-centered, technology-driven interventions that
benefit group well-being.

</details>


### [135] [Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis](https://arxiv.org/abs/2508.11398)
*Mithat Can Ozgun,Jiahuan Pei,Koen Hindriks,Lucia Donatelli,Qingzhi Liu,Xin Sun,Junxiao Wang*

Main category: cs.HC

TL;DR: DSM5AgentFlow是一个基于LLM的代理工作流，旨在自主生成DSM-5 Level-1诊断问卷，填补了心理健康诊断领域的空白。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在心理健康诊断领域表现不佳，主要受限于数据稀缺和缺乏临床推理能力。

Method: 通过模拟治疗师与客户的对话，生成透明、逐步的诊断预测。

Result: 实验评估了LLM在对话真实性、诊断准确性和可解释性三个维度的表现。

Conclusion: 该工作流可作为心理健康诊断的补充工具，符合伦理和法律标准。

Abstract: LLM-based agents have emerged as transformative tools capable of executing
complex tasks through iterative planning and action, achieving significant
advancements in understanding and addressing user needs. Yet, their
effectiveness remains limited in specialized domains such as mental health
diagnosis, where they underperform compared to general applications. Current
approaches to integrating diagnostic capabilities into LLMs rely on scarce,
highly sensitive mental health datasets, which are challenging to acquire.
These methods also fail to emulate clinicians' proactive inquiry skills, lack
multi-turn conversational comprehension, and struggle to align outputs with
expert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the
first LLM-based agent workflow designed to autonomously generate DSM-5 Level-1
diagnostic questionnaires. By simulating therapist-client dialogues with
specific client profiles, the framework delivers transparent, step-by-step
disorder predictions, producing explainable and trustworthy results. This
workflow serves as a complementary tool for mental health diagnosis, ensuring
adherence to ethical and legal standards. Through comprehensive experiments, we
evaluate leading LLMs across three critical dimensions: conversational realism,
diagnostic accuracy, and explainability. Our datasets and implementations are
fully open-sourced.

</details>


### [136] [FACET:Teacher-Centred LLM-Based Multi-Agent Systems-Towards Personalized Educational Worksheets](https://arxiv.org/abs/2508.11401)
*Jana Gonnermann-Müller,Jennifer Haase,Konstantin Fackeldey,Sebastian Pokutta*

Main category: cs.HC

TL;DR: FACET框架是一个基于LLM的多智能体系统，旨在为教师生成个性化的课堂材料，整合认知和动机维度。


<details>
  <summary>Details</summary>
Motivation: 学生群体的异质性增加对教师提出了挑战，尤其是数学教育中认知、动机和情感差异对学习成果的影响。现有AI工具多为性能导向，缺乏对教师和教学需求的全面支持。

Method: FACET框架包含三个智能体：模拟学习者档案的学习者智能体、根据教学原则调整内容的教师智能体，以及提供质量保证的评估智能体。系统测试了八年级数学内容，并通过自动评估和教师反馈验证可行性。

Result: 十次内部评估显示生成材料与学习者档案高度匹配，教师反馈肯定了任务结构和适用性。

Conclusion: 多智能体LLM架构在异质课堂中具有可扩展性和情境感知潜力，未来可扩展至更丰富学习者档案和实际课堂试验。

Abstract: The increasing heterogeneity of student populations poses significant
challenges for teachers, particularly in mathematics education, where
cognitive, motivational, and emotional differences strongly influence learning
outcomes. While AI-driven personalization tools have emerged, most remain
performance-focused, offering limited support for teachers and neglecting
broader pedagogical needs. This paper presents the FACET framework, a
teacher-facing, large language model (LLM)-based multi-agent system designed to
generate individualized classroom materials that integrate both cognitive and
motivational dimensions of learner profiles. The framework comprises three
specialized agents: (1) learner agents that simulate diverse profiles
incorporating topic proficiency and intrinsic motivation, (2) a teacher agent
that adapts instructional content according to didactical principles, and (3)
an evaluator agent that provides automated quality assurance. We tested the
system using authentic grade 8 mathematics curriculum content and evaluated its
feasibility through a) automated agent-based assessment of output quality and
b) exploratory feedback from K-12 in-service teachers. Results from ten
internal evaluations highlighted high stability and alignment between generated
materials and learner profiles, and teacher feedback particularly highlighted
structure and suitability of tasks. The findings demonstrate the potential of
multi-agent LLM architectures to provide scalable, context-aware
personalization in heterogeneous classroom settings, and outline directions for
extending the framework to richer learner profiles and real-world classroom
trials.

</details>


### [137] [Towards Embodied Conversational Agents for Reducing Oral Exam Anxiety in Extended Reality](https://arxiv.org/abs/2508.11412)
*Jens Grubert,Yvonne Sedelmaier,Dieter Landes*

Main category: cs.HC

TL;DR: 探讨了在扩展现实（XR）环境中使用具身对话代理（ECAs）帮助学生准备口试的潜力，提出了一种结合真实感ECAs和实时大语言模型（LLMs）的系统概念。


<details>
  <summary>Details</summary>
Motivation: 口试是高等教育中普遍但心理压力大的评估形式，许多学生因焦虑影响表现，需要一种安全、可重复的练习方式。

Method: 提出整合真实感ECAs和实时LLMs的系统，支持心理安全、自适应和可重复的口试场景练习。

Result: 讨论了该系统的潜在益处和挑战。

Conclusion: XR环境中的ECAs和LLMs结合有望为学生提供有效的口试准备支持。

Abstract: Oral examinations are a prevalent but psychologically demanding form of
assessment in higher education. Many students experience intense anxiety, which
can impair cognitive performance and hinder academic success. This position
paper explores the potential of embodied conversational agents (ECAs) in
extended reality (XR) environments to support students preparing for oral
exams. We propose a system concept that integrates photorealistic ECAs with
real-time capable large language models (LLMs) to enable psychologically safe,
adaptive, and repeatable rehearsal of oral examination scenarios. We also
discuss the potential benefits and challenges of such an envisioned system.

</details>


### [138] [ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality](https://arxiv.org/abs/2508.11426)
*Steffen Hauck,Diar Abdlkarim,John Dudley,Per Ola Kristensson,Eyal Ofek,Jens Grubert*

Main category: cs.HC

TL;DR: 研究探讨了使用ReachVox（一种极简编码）在VR中辅助远程操作者与机械臂协作的效果。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，机器人运动路径的规划和理解是主要挑战，需要实时适应。

Method: 通过用户研究（n=20），比较ReachVox可视化与基于点的可达性检查的效果。

Result: 研究表明ReachVox可视化在协作中具有优势。

Conclusion: ReachVox是一种有效的工具，可提升人机协作的效率。

Abstract: Human-Robot-Collaboration can enhance workflows by leveraging the mutual
strengths of human operators and robots. Planning and understanding robot
movements remain major challenges in this domain. This problem is prevalent in
dynamic environments that might need constant robot motion path adaptation. In
this paper, we investigate whether a minimalistic encoding of the reachability
of a point near an object of interest, which we call ReachVox, can aid the
collaboration between a remote operator and a robotic arm in VR. Through a user
study (n=20), we indicate the strength of the visualization relative to a
point-based reachability check-up.

</details>


### [139] [Grand Challenge: Mediating Between Confirmatory and Exploratory Research Cultures in Health Sciences and Visual Analytics](https://arxiv.org/abs/2508.11544)
*Viktor von Wyl,Jürgen Bernard*

Main category: cs.HC

TL;DR: 论文探讨了健康科学与可视化分析研究合作中的障碍，提出了七个研究需求和行动，以促进跨学科合作。


<details>
  <summary>Details</summary>
Motivation: 健康科学与可视化分析研究在设计方法上存在差异，导致合作困难，如术语不一致、验证标准不同等。

Method: 通过识别七个研究需求和行动，构建了一个包含文化、标准和流程的框架。

Result: 提出了一个研究议程，旨在开发可靠、可重复且临床相关的数据驱动方法。

Conclusion: 通过文化适应、标准统一和流程整合，可以促进跨学科合作，提升研究效果。

Abstract: Collaboration between health science and visual analytics research is often
hindered by different, sometimes incompatible approaches to research design.
Health science often follows hypothesis-driven protocols, registered in
advance, and focuses on reproducibility and risk mitigation. Visual analytics,
in contrast, relies on iterative data exploration, prioritizing insight
generation and analytic refinement through user interaction. These differences
create challenges in interdisciplinary projects, including misaligned
terminology, unrealistic expectations about data readiness, divergent
validation norms, or conflicting explainability requirements. To address these
persistent tensions, we identify seven research needs and actions: (1)
guidelines for broader community adoption, (2) agreement on quality and
validation benchmarks, (3) frameworks for aligning research tasks, (4)
integrated workflows combining confirmatory and exploratory stages, (5) tools
for harmonizing terminology across disciplines, (6) dedicated bridging roles
for transdisciplinary work, and (7) cultural adaptation and mutual recognition.
We organize these needs in a framework with three areas: culture, standards,
and processes. They can constitute a research agenda for developing reliable,
reproducible, and clinically relevant data-centric methods.

</details>


### [140] [Adaptive Cardio Load Targets for Improving Fitness and Performance](https://arxiv.org/abs/2508.11613)
*Justin Phillips,Daniel Roggen,Cathy Speed,Robert Harle*

Main category: cs.HC

TL;DR: Cardio Load (CL)是Google在2024年推出的心血管工作量测量指标，基于心率储备，结合活动强度和时长。通过用户反馈和研究，推出个性化周目标功能，2025年9月后将在Fitbit应用中公测。


<details>
  <summary>Details</summary>
Motivation: 提供一种更全面的心血管工作量测量方法，结合活动强度和时长，帮助用户更好地管理训练负荷。

Method: 基于心率储备计算Cardio Load，并引入自适应和个性化的周目标设定。

Result: Cardio Load能够捕捉全天活动（包括锻炼和日常活动），并与Active Zone Minutes（AZMs）区分用途。

Conclusion: Cardio Load为用户提供了更灵活的训练负荷管理方式，适合追求性能提升的用户。

Abstract: Cardio Load, introduced by Google in 2024, is a measure of cardiovascular
work (also known as training load) resulting from all the user's activities
across the day. It is based on heart rate reserve and captures both activity
intensity and duration. Thanks to feedback from users and internal research, we
introduce adaptive and personalized targets which will be set weekly. This
feature will be available in the Public Preview of the Fitbit app after
September 2025. This white paper provides a comprehensive overview of Cardio
Load (CL) and how weekly CL targets are established, with examples shown to
illustrate the effect of varying CL on the weekly target. We compare Cardio
Load and Active Zone Minutes (AZMs), highlighting their distinct purposes, i.e.
AZMs for health guidelines and CL for performance measurement. We highlight
that CL is accumulated both during active workouts and incidental daily
activities, so users are able top-up their CL score with small bouts of
activity across the day.

</details>


### [141] [Grab-n-Go: On-the-Go Microgesture Recognition with Objects in Hand](https://arxiv.org/abs/2508.11620)
*Chi-Jung Lee,Jiaxin Li,Tianhong Catherine Yu,Ruidong Zhang,Vipin Gunda,François Guimbretière,Cheng Zhang*

Main category: cs.HC

TL;DR: Grab-n-Go是一种利用主动声学传感识别手持物体时细微手势的可穿戴设备，通过深度学习框架实现高精度识别。


<details>
  <summary>Details</summary>
Motivation: 随着计算设备融入日常生活，需要一种即使手被占用也能实现直观交互的方法。

Method: 使用单手腕带捕捉手势、握持姿势和物体几何信息，结合深度学习识别30种微手势。

Result: 在10名参与者和25种日常物体的研究中，平均识别准确率为92.0%。

Conclusion: Grab-n-Go展示了无需修改现有物体即可实现无缝交互的潜力。

Abstract: As computing devices become increasingly integrated into daily life, there is
a growing need for intuitive, always-available interaction methods, even when
users' hands are occupied. In this paper, we introduce Grab-n-Go, the first
wearable device that leverages active acoustic sensing to recognize subtle hand
microgestures while holding various objects. Unlike prior systems that focus
solely on free-hand gestures or basic hand-object activity recognition,
Grab-n-Go simultaneously captures information about hand microgestures,
grasping poses, and object geometries using a single wristband, enabling the
recognition of fine-grained hand movements occurring within activities
involving occupied hands. A deep learning framework processes these complex
signals to identify 30 distinct microgestures, with 6 microgestures for each of
the 5 grasping poses. In a user study with 10 participants and 25 everyday
objects, Grab-n-Go achieved an average recognition accuracy of 92.0%. A
follow-up study further validated Grab-n-Go's robustness against 10 more
challenging, deformable objects. These results underscore the potential of
Grab-n-Go to provide seamless, unobtrusive interactions without requiring
modifications to existing objects. The complete dataset, comprising data from
18 participants performing 30 microgestures with 35 distinct objects, is
publicly available at https://github.com/cjlisalee/Grab-n-Go_Data with the DOI:
https://doi.org/10.7298/7kbd-vv75.

</details>
