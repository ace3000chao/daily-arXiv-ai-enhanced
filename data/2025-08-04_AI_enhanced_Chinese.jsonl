{"id": "2508.00037", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00037", "abs": "https://arxiv.org/abs/2508.00037", "authors": ["Tong Nie", "Jian Sun", "Wei Ma"], "title": "Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion", "comment": "Accepted at IEEE Transactions on Industrial Informatics", "summary": "Networked urban systems facilitate the flow of people, resources, and\nservices, and are essential for economic and social interactions. These systems\noften involve complex processes with unknown governing rules, observed by\nsensor-based time series. To aid decision-making in industrial and engineering\ncontexts, data-driven predictive models are used to forecast spatiotemporal\ndynamics of urban systems. Current models such as graph neural networks have\nshown promise but face a trade-off between efficacy and efficiency due to\ncomputational demands. Hence, their applications in large-scale networks still\nrequire further efforts. This paper addresses this trade-off challenge by\ndrawing inspiration from physical laws to inform essential model designs that\nalign with fundamental principles and avoid architectural redundancy. By\nunderstanding both micro- and macro-processes, we present a principled\ninterpretable neural diffusion scheme based on Transformer-like structures\nwhose attention layers are induced by low-dimensional embeddings. The proposed\nscalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is\nvalidated on large-scale urban systems including traffic flow, solar power, and\nsmart meters, showing state-of-the-art performance and remarkable scalability.\nOur results constitute a fresh perspective on the dynamics prediction in\nlarge-scale urban networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5b9a\u5f8b\u542f\u53d1\u7684\u53ef\u6269\u5c55\u65f6\u7a7aTransformer\uff08ScaleSTF\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u9884\u6d4b\u5927\u89c4\u6a21\u57ce\u5e02\u7f51\u7edc\u4e2d\u7684\u52a8\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6548\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u57ce\u5e02\u7f51\u7edc\u7cfb\u7edf\u6d89\u53ca\u590d\u6742\u4e14\u89c4\u5219\u672a\u77e5\u7684\u52a8\u6001\u8fc7\u7a0b\uff0c\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff08\u5982\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u6548\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u5b9a\u5f8b\u548cTransformer\u7ed3\u6784\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f4e\u7ef4\u5d4c\u5165\u8bf1\u5bfc\u7684\u6ce8\u610f\u529b\u5c42\uff0c\u63d0\u51fa\u4e86\u7ebf\u6027\u590d\u6742\u5ea6\u7684ScaleSTF\u6a21\u578b\u3002", "result": "\u5728\u4ea4\u901a\u6d41\u91cf\u3001\u592a\u9633\u80fd\u53d1\u7535\u548c\u667a\u80fd\u7535\u8868\u7b49\u5927\u89c4\u6a21\u57ce\u5e02\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86ScaleSTF\u7684\u5148\u8fdb\u6027\u80fd\u548c\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "ScaleSTF\u4e3a\u5927\u89c4\u6a21\u57ce\u5e02\u7f51\u7edc\u52a8\u6001\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6548\u80fd\u4e0e\u6548\u7387\u4e0a\u7684\u5e73\u8861\u4f18\u52bf\u3002"}}
{"id": "2508.00039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00039", "abs": "https://arxiv.org/abs/2508.00039", "authors": ["Kaustav Chatterjee", "Joshua Q. Li", "Fatemeh Ansari", "Masud Rana Munna", "Kundan Parajulee", "Jared Schwennesen"], "title": "Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings", "comment": null, "summary": "Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose\nsafety risks to highway vehicles due to potential hang-ups. These crossings\ntypically result from post-construction railway track maintenance activities or\nnon-compliance with design guidelines for HRGC vertical alignments.\nConventional methods for measuring HRGC profiles are costly, time-consuming,\ntraffic-disruptive, and present safety challenges. To address these issues,\nthis research employed advanced, cost-effective techniques and innovative\nmodeling approaches for HRGC profile measurement. A novel hybrid deep learning\nframework combining Long Short-Term Memory (LSTM) and Transformer architectures\nwas developed by utilizing instrumentation and ground truth data.\nInstrumentation data were gathered using a highway testing vehicle equipped\nwith Inertial Measurement Unit (IMU) and Global Positioning System (GPS)\nsensors, while ground truth data were obtained via an industrial-standard\nwalking profiler. Field data was collected at the Red Rock Railroad Corridor in\nOklahoma. Three advanced deep learning models Transformer-LSTM sequential\n(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel\n(model 3) were evaluated to identify the most efficient architecture. Models 2\nand 3 outperformed the others and were deployed to generate 2D/3D HRGC\nprofiles. The deep learning models demonstrated significant potential to\nenhance highway and railroad safety by enabling rapid and accurate assessment\nof HRGC hang-up susceptibility.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LSTM\u548cTransformer\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u6d4b\u91cf\u94c1\u8def\u516c\u8def\u5e73\u4ea4\u9053\u53e3\uff08HRGC\uff09\u7684\u5782\u76f4\u5256\u9762\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u7684\u95ee\u9898\u3002", "motivation": "\u94c1\u8def\u516c\u8def\u5e73\u4ea4\u9053\u53e3\u7684\u5782\u76f4\u5256\u9762\u95ee\u9898\u53ef\u80fd\u5bfc\u81f4\u8f66\u8f86\u60ac\u6302\u98ce\u9669\uff0c\u4f20\u7edf\u6d4b\u91cf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u6210\u672c\u9ad8\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u91c7\u7528IMU\u548cGPS\u4f20\u611f\u5668\u91c7\u96c6\u6570\u636e\uff0c\u7ed3\u5408\u5de5\u4e1a\u6807\u51c6\u6b65\u884c\u5256\u9762\u4eea\u83b7\u53d6\u5730\u9762\u771f\u5b9e\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u4e09\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b2\u548c\u6a21\u578b3\u8868\u73b0\u6700\u4f73\uff0c\u6210\u529f\u751f\u6210\u4e862D/3D\u7684HRGC\u5256\u9762\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u63d0\u5347\u6d4b\u91cf\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728HRGC\u5256\u9762\u6d4b\u91cf\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u63d0\u5347\u516c\u8def\u548c\u94c1\u8def\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.00040", "categories": ["cs.LG", "math.PR", "stat.AP", "stat.ML", "60J20, 68T07"], "pdf": "https://arxiv.org/pdf/2508.00040", "abs": "https://arxiv.org/abs/2508.00040", "authors": ["Abhinav Das", "Stephan Schl\u00fcter"], "title": "Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting", "comment": null, "summary": "This work integrates Bayesian regime detection with conditional neural\nprocesses for 24-hour electricity price prediction in the German market. Our\nmethodology integrates regime detection using a disentangled sticky\nhierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to\ndaily electricity prices. Each identified regime is subsequently modeled by an\nindependent conditional neural process (CNP), trained to learn localized\nmappings from input contexts to 24-dimensional hourly price trajectories, with\nfinal predictions computed as regime-weighted mixtures of these CNP outputs. We\nrigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated\nauto-regressive (LEAR) models by integrating their forecasts into diverse\nbattery storage optimization frameworks, including price arbitrage, risk\nmanagement, grid services, and cost minimization. This operational utility\nassessment revealed complex performance trade-offs: LEAR often yielded superior\nabsolute profits or lower costs, while DNN showed exceptional optimality in\nspecific cost-minimization contexts. Recognizing that raw prediction accuracy\ndoesn't always translate to optimal operational outcomes, we employed TOPSIS as\na comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified\nLEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model\nemerged as the most balanced and preferred solution for 2021, 2022 and 2023.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8d1d\u53f6\u65af\u673a\u5236\u68c0\u6d4b\u548c\u6761\u4ef6\u795e\u7ecf\u8fc7\u7a0b\u7684\u65b9\u6cd5\uff08R-NP\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u5fb7\u56fd\u5e02\u573a\u768424\u5c0f\u65f6\u7535\u4ef7\uff0c\u5e76\u901a\u8fc7\u591a\u6807\u51c6\u8bc4\u4f30\uff08TOPSIS\uff09\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u7535\u529b\u5e02\u573a\u4ef7\u683c\u7684\u52a8\u6001\u6027\u548c\u590d\u6742\u6027\u9700\u8981\u66f4\u7cbe\u786e\u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u7535\u6c60\u5b58\u50a8\u4f18\u5316\u7b49\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528DS-HDP-HMM\u8fdb\u884c\u673a\u5236\u68c0\u6d4b\uff0c\u6bcf\u4e2a\u673a\u5236\u7531\u72ec\u7acb\u7684CNP\u5efa\u6a21\uff0c\u6700\u7ec8\u9884\u6d4b\u4e3a\u673a\u5236\u52a0\u6743\u7684CNP\u8f93\u51fa\u3002", "result": "R-NP\u57282021-2023\u5e74\u8868\u73b0\u51fa\u6700\u5e73\u8861\u7684\u6027\u80fd\uff0c\u4f18\u4e8eDNN\u548cLEAR\u6a21\u578b\u3002", "conclusion": "R-NP\u662f\u4e00\u79cd\u7efc\u5408\u6027\u80fd\u4f18\u8d8a\u7684\u7535\u4ef7\u9884\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.00041", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00041", "abs": "https://arxiv.org/abs/2508.00041", "authors": ["Yebo Wu", "Jingguang Li", "Zhijiang Guo", "Li Li"], "title": "Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages", "comment": null, "summary": "Federated fine-tuning enables Large Language Models (LLMs) to adapt to\ndownstream tasks while preserving data privacy, but its resource-intensive\nnature limits deployment on edge devices. In this paper, we introduce\nDevelopmental Federated Tuning (DevFT), a resource-efficient approach inspired\nby cognitive development that progressively builds a powerful LLM from a\ncompact foundation. DevFT decomposes the fine-tuning process into developmental\nstages, each optimizing submodels with increasing parameter capacity. Knowledge\nfrom earlier stages transfers to subsequent submodels, providing optimized\ninitialization parameters that prevent convergence to local minima and\naccelerate training. This paradigm mirrors human learning, gradually\nconstructing comprehensive knowledge structure while refining existing skills.\nTo efficiently build stage-specific submodels, DevFT introduces\ndeconfliction-guided layer grouping and differential-based layer fusion to\ndistill essential information and construct representative layers. Evaluations\nacross multiple benchmarks demonstrate that DevFT significantly outperforms\nstate-of-the-art methods, achieving up to 4.59$\\times$ faster convergence,\n10.67$\\times$ reduction in communication overhead, and 9.07% average\nperformance improvement, while maintaining compatibility with existing\napproaches.", "AI": {"tldr": "DevFT\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u8054\u90a6\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u6784\u5efaLLM\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5fae\u8c03\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u9690\u79c1\u3002", "method": "\u5206\u9636\u6bb5\u5fae\u8c03\uff0c\u9010\u6b65\u589e\u52a0\u5b50\u6a21\u578b\u53c2\u6570\u5bb9\u91cf\uff0c\u901a\u8fc7\u77e5\u8bc6\u8f6c\u79fb\u4f18\u5316\u521d\u59cb\u5316\u53c2\u6570\u3002\u91c7\u7528\u89e3\u51b2\u7a81\u5f15\u5bfc\u7684\u5c42\u5206\u7ec4\u548c\u57fa\u4e8e\u5dee\u5f02\u7684\u5c42\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u53474.59\u500d\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c1110.67\u500d\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u53479.07%\u3002", "conclusion": "DevFT\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u517c\u5bb9\u73b0\u6709\u65b9\u6cd5\u7684\u8054\u90a6\u5fae\u8c03\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002"}}
{"id": "2508.00079", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00079", "abs": "https://arxiv.org/abs/2508.00079", "authors": ["Oshayer Siddique", "J. M Areeb Uzair Alam", "Md Jobayer Rahman Rafy", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems", "comment": "Under review, 18 pages, 4 figures, 7 tables", "summary": "The discipline of physics stands as a cornerstone of human intellect, driving\nthe evolution of technology and deepening our understanding of the fundamental\nprinciples of the cosmos. Contemporary literature includes some works centered\non the task of solving physics problems - a crucial domain of natural language\nreasoning. In this paper, we evaluate the performance of frontier LLMs in\nsolving physics problems, both mathematical and descriptive. We also employ a\nplethora of inference-time techniques and agentic frameworks to improve the\nperformance of the models. This includes the verification of proposed solutions\nin a cumulative fashion by other, smaller LLM agents, and we perform a\ncomparative analysis of the performance that the techniques entail. There are\nsignificant improvements when the multi-agent framework is applied to problems\nthat the models initially perform poorly on. Furthermore, we introduce a new\nevaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small\nVAL}}$, consisting of 19,609 problems sourced from various physics textbooks\nand their corresponding correct solutions scraped from physics forums and\neducational websites. Our code and data are publicly available at\nhttps://github.com/areebuzair/PhysicsEval.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u524d\u6cbfLLMs\u5728\u89e3\u51b3\u7269\u7406\u95ee\u9898\uff08\u6570\u5b66\u548c\u63cf\u8ff0\u6027\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u591a\u4ee3\u7406\u6846\u67b6\u548c\u63a8\u7406\u6280\u672f\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u65b0\u7684\u7269\u7406\u95ee\u9898\u8bc4\u4f30\u57fa\u51c6${\\rm P{\\small HYSICS}E{\\small VAL}$\u3002", "motivation": "\u7269\u7406\u95ee\u9898\u662f\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7684\u5173\u952e\u9886\u57df\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8LLMs\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u591a\u4ee3\u7406\u6846\u67b6\u548c\u63a8\u7406\u6280\u672f\uff08\u5982\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\uff09\uff0c\u5e76\u5f15\u5165\u65b0\u57fa\u51c6${\\rm P{\\small HYSICS}E{\\small VAL}$\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u591a\u4ee3\u7406\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u521d\u59cb\u8868\u73b0\u8f83\u5dee\u7684\u7269\u7406\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u4e3aLLMs\u5728\u7269\u7406\u95ee\u9898\u89e3\u51b3\u4e0a\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u6cd5\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.00081", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00081", "abs": "https://arxiv.org/abs/2508.00081", "authors": ["Fred Mutisya", "Shikoh Gitau", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha"], "title": "Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench", "comment": null, "summary": "HealthBench, a benchmark designed to measure the capabilities of AI systems\nfor health better (Arora et al., 2025), has advanced medical language model\nevaluation through physician-crafted dialogues and transparent rubrics.\nHowever, its reliance on expert opinion, rather than high-tier clinical\nevidence, risks codifying regional biases and individual clinician\nidiosyncrasies, further compounded by potential biases in automated grading\nsystems. These limitations are particularly magnified in low- and middle-income\nsettings, where issues like sparse neglected tropical disease coverage and\nregion-specific guideline mismatches are prevalent.\n  The unique challenges of the African context, including data scarcity,\ninadequate infrastructure, and nascent regulatory frameworks, underscore the\nurgent need for more globally relevant and equitable benchmarks. To address\nthese shortcomings, we propose anchoring reward functions in version-controlled\nClinical Practice Guidelines (CPGs) that incorporate systematic reviews and\nGRADE evidence ratings.\n  Our roadmap outlines \"evidence-robust\" reinforcement learning via\nrubric-to-guideline linkage, evidence-weighted scoring, and contextual override\nlogic, complemented by a focus on ethical considerations and the integration of\ndelayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,\nwhile preserving HealthBench's transparency and physician engagement, we aim to\nfoster medical language models that are not only linguistically polished but\nalso clinically trustworthy, ethically sound, and globally relevant.", "AI": {"tldr": "HealthBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u533b\u7597AI\u7cfb\u7edf\u7684\u57fa\u51c6\uff0c\u4f46\u4f9d\u8d56\u4e13\u5bb6\u610f\u89c1\u53ef\u80fd\u5f15\u5165\u504f\u89c1\u3002\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\uff08CPGs\uff09\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5168\u7403\u9002\u7528\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "HealthBench\u867d\u63a8\u52a8\u4e86\u533b\u7597\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\uff0c\u4f46\u5176\u4f9d\u8d56\u4e13\u5bb6\u610f\u89c1\u53ef\u80fd\u56fa\u5316\u533a\u57df\u504f\u89c1\u548c\u4e2a\u4f53\u5dee\u5f02\uff0c\u5c24\u5176\u5728\u4f4e\u6536\u5165\u5730\u533a\u95ee\u9898\u66f4\u7a81\u51fa\u3002\u9700\u8981\u66f4\u5168\u7403\u5316\u548c\u516c\u5e73\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7248\u672c\u63a7\u5236CPGs\u7684\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u7cfb\u7edf\u7efc\u8ff0\u548cGRADE\u8bc1\u636e\u8bc4\u7ea7\uff0c\u901a\u8fc7\u8bc1\u636e\u52a0\u6743\u8bc4\u5206\u548c\u4e0a\u4e0b\u6587\u8986\u76d6\u903b\u8f91\u5b9e\u73b0\u201c\u8bc1\u636e\u7a33\u5065\u201d\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u6539\u8fdb\u540e\u7684\u65b9\u6cd5\u65e8\u5728\u63d0\u5347\u533b\u7597\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3001\u4f26\u7406\u5408\u89c4\u6027\u548c\u5168\u7403\u9002\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408CPGs\u548c\u900f\u660e\u6027\uff0c\u6539\u8fdbHealthBench\uff0c\u76ee\u6807\u662f\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u516c\u5e73\u4e14\u5168\u7403\u9002\u7528\u7684\u533b\u7597\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2508.00002", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00002", "abs": "https://arxiv.org/abs/2508.00002", "authors": ["Kaustav Bhattacharjee", "Jun Yuan", "Aritra Dasgupta"], "title": "ReVise: A Human-AI Interface for Incremental Algorithmic Recourse", "comment": "Conditionally accepted for the IEEE VIS 2025 Short Papers track", "summary": "The recent adoption of artificial intelligence in socio-technical systems\nraises concerns about the black-box nature of the resulting decisions in fields\nsuch as hiring, finance, admissions, etc. If data subjects -- such as job\napplicants, loan applicants, and students -- receive an unfavorable outcome,\nthey may be interested in algorithmic recourse, which involves updating certain\nfeatures to yield a more favorable result when re-evaluated by algorithmic\ndecision-making. Unfortunately, when individuals do not fully understand the\nincremental steps needed to change their circumstances, they risk following\nmisguided paths that can lead to significant, long-term adverse consequences.\nExisting recourse approaches focus exclusively on the final recourse goal but\nneglect the possible incremental steps to reach the goal with real-life\nconstraints, user preferences, and model artifacts. To address this gap, we\nformulate a visual analytic workflow for incremental recourse planning in\ncollaboration with AI/ML experts and contribute an interactive visualization\ninterface that helps data subjects efficiently navigate the recourse\nalternatives and make an informed decision. We present a usage scenario and\nsubjective feedback from observational studies with twelve graduate students\nusing a real-world dataset, which demonstrates that our approach can be\ninstrumental for data subjects in choosing a suitable recourse path.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89c6\u5316\u5206\u6790\u5de5\u4f5c\u6d41\uff0c\u5e2e\u52a9\u6570\u636e\u4e3b\u4f53\u5728\u7b97\u6cd5\u51b3\u7b56\u4e2d\u89c4\u5212\u589e\u91cf\u8865\u6551\u8def\u5f84\uff0c\u907f\u514d\u8bef\u5bfc\u6027\u9009\u62e9\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u5728\u62db\u8058\u3001\u91d1\u878d\u7b49\u9886\u57df\u7684\u9ed1\u7bb1\u51b3\u7b56\u53ef\u80fd\u5bf9\u6570\u636e\u4e3b\u4f53\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\uff0c\u73b0\u6709\u8865\u6551\u65b9\u6cd5\u5ffd\u89c6\u4e86\u589e\u91cf\u6b65\u9aa4\u548c\u5b9e\u9645\u7ea6\u675f\u3002", "method": "\u4e0eAI/ML\u4e13\u5bb6\u5408\u4f5c\uff0c\u8bbe\u8ba1\u4e86\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u754c\u9762\uff0c\u652f\u6301\u6570\u636e\u4e3b\u4f53\u63a2\u7d22\u8865\u6551\u8def\u5f84\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u96c6\u548c\u7528\u6237\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e2e\u52a9\u7528\u6237\u9009\u62e9\u5408\u9002\u8def\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6570\u636e\u4e3b\u4f53\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u3001\u5b9e\u7528\u7684\u7b97\u6cd5\u8865\u6551\u5de5\u5177\u3002"}}
{"id": "2508.00043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00043", "abs": "https://arxiv.org/abs/2508.00043", "authors": ["Nhut Truong", "Uri Hasson"], "title": "Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity", "comment": null, "summary": "Topographic neural networks are computational models that can simulate the\nspatial and functional organization of the brain. Topographic constraints in\nneural networks can be implemented in multiple ways, with potentially different\nimpacts on the representations learned by the network. The impact of such\ndifferent implementations has not been systematically examined. To this end,\nhere we compare topographic convolutional neural networks trained with two\nspatial constraints: Weight Similarity (WS), which pushes neighboring units to\ndevelop similar incoming weights, and Activation Similarity (AS), which\nenforces similarity in unit activations. We evaluate the resulting models on\nclassification accuracy, robustness to weight perturbations and input\ndegradation, and the spatial organization of learned representations. Compared\nto both AS and standard CNNs, WS provided three main advantages: i) improved\nrobustness to noise, also showing higher accuracy under weight corruption; ii)\ngreater input sensitivity, reflected in higher activation variance; and iii)\nstronger functional localization, with units showing similar activations\npositioned at closer distances. In addition, WS produced differences in\norientation tuning, symmetry sensitivity, and eccentricity profiles of units,\nindicating an influence of this spatial constraint on the representational\ngeometry of the network. Our findings suggest that during end-to-end training,\nWS constraints produce more robust representations than AS or non-topographic\nCNNs. These findings also suggest that weight-based spatial constraints can\nshape feature learning and functional organization in biophysical inspired\nmodels.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u7a7a\u95f4\u7ea6\u675f\uff08\u6743\u91cd\u76f8\u4f3c\u6027\u548c\u6fc0\u6d3b\u76f8\u4f3c\u6027\uff09\u5bf9\u5730\u5f62\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6743\u91cd\u76f8\u4f3c\u6027\u5728\u9c81\u68d2\u6027\u3001\u8f93\u5165\u654f\u611f\u6027\u548c\u529f\u80fd\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u5730\u5f62\u7ea6\u675f\u5bf9\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u8868\u793a\u7684\u5f71\u54cd\uff0c\u586b\u8865\u7cfb\u7edf\u6027\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6bd4\u8f83\u6743\u91cd\u76f8\u4f3c\u6027\uff08WS\uff09\u548c\u6fc0\u6d3b\u76f8\u4f3c\u6027\uff08AS\uff09\u4e24\u79cd\u7ea6\u675f\uff0c\u8bc4\u4f30\u5206\u7c7b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u8868\u793a\u7684\u7a7a\u95f4\u7ec4\u7ec7\u3002", "result": "WS\u5728\u566a\u58f0\u9c81\u68d2\u6027\u3001\u8f93\u5165\u654f\u611f\u6027\u548c\u529f\u80fd\u5b9a\u4f4d\u65b9\u9762\u4f18\u4e8eAS\u548c\u6807\u51c6CNN\uff0c\u5e76\u5f71\u54cd\u4e86\u7f51\u7edc\u8868\u793a\u51e0\u4f55\u3002", "conclusion": "\u6743\u91cd\u76f8\u4f3c\u6027\u7ea6\u675f\u80fd\u4ea7\u751f\u66f4\u9c81\u68d2\u7684\u8868\u793a\uff0c\u5e76\u5f71\u54cd\u751f\u7269\u7269\u7406\u542f\u53d1\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u5b66\u4e60\u548c\u529f\u80fd\u7ec4\u7ec7\u3002"}}
{"id": "2508.00086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00086", "abs": "https://arxiv.org/abs/2508.00086", "authors": ["Kelly Kendro", "Jeffrey Maloney", "Scott Jarvis"], "title": "Do LLMs produce texts with \"human-like\" lexical diversity?", "comment": "35 pages; includes abstract", "summary": "The degree to which LLMs produce writing that is truly human-like remains\nunclear despite the extensive empirical attention that this question has\nreceived. The present study addresses this question from the perspective of\nlexical diversity. Specifically, the study investigates patterns of lexical\ndiversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,\nand -4.5) in comparison with texts written by L1 and L2 English participants (n\n= 240) across four education levels. Six dimensions of lexical diversity were\nmeasured in each text: volume, abundance, variety-repetition, evenness,\ndisparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and\nSupport Vector Machines revealed that the LLM-generated texts differed\nsignificantly from human-written texts for each variable, with ChatGPT-o4 mini\nand -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated\nhigher levels of lexical diversity despite producing fewer tokens. The human\nwriters' lexical diversity did not differ across subgroups (i.e., education,\nlanguage status). Altogether, the results indicate that LLMs do not produce\nhuman-like texts in relation to lexical diversity, and the newer LLMs produce\nless human-like texts than older models. We discuss the implications of these\nresults for language pedagogy and related applications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cLLM\u751f\u6210\u7684\u6587\u672c\u5728\u8bcd\u6c47\u591a\u6837\u6027\u4e0a\u4e0e\u4eba\u7c7b\u5199\u4f5c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u8f83\u65b0\u7684\u6a21\u578b\uff08\u5982ChatGPT-4.5\uff09\u5dee\u5f02\u66f4\u5927\u3002", "motivation": "\u63a2\u8ba8LLM\u751f\u6210\u7684\u6587\u672c\u662f\u5426\u5728\u8bcd\u6c47\u591a\u6837\u6027\u4e0a\u4e0e\u4eba\u7c7b\u5199\u4f5c\u76f8\u4f3c\u3002", "method": "\u6bd4\u8f83\u56db\u4e2aChatGPT\u6a21\u578b\u4e0e240\u540dL1\u548cL2\u82f1\u8bed\u53c2\u4e0e\u8005\u7684\u6587\u672c\uff0c\u6d4b\u91cf\u516d\u4e2a\u8bcd\u6c47\u591a\u6837\u6027\u7ef4\u5ea6\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "LLM\u751f\u6210\u7684\u6587\u672c\u5728\u8bcd\u6c47\u591a\u6837\u6027\u4e0a\u4e0e\u4eba\u7c7b\u5199\u4f5c\u663e\u8457\u4e0d\u540c\uff0c\u8f83\u65b0\u6a21\u578b\u5dee\u5f02\u66f4\u5927\u3002", "conclusion": "LLM\u751f\u6210\u7684\u6587\u672c\u5728\u8bcd\u6c47\u591a\u6837\u6027\u4e0a\u4e0d\u5177\u4eba\u7c7b\u7279\u5f81\uff0c\u8f83\u65b0\u6a21\u578b\u66f4\u4e0d\u76f8\u4f3c\u3002"}}
{"id": "2508.00106", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00106", "abs": "https://arxiv.org/abs/2508.00106", "authors": ["Ernest Bonnah", "Luan Viet Nguyen", "Khaza Anuarul Hoque"], "title": "Hyperproperty-Constrained Secure Reinforcement Learning", "comment": "Accepted in IEEE/ACM MEMOCODE 2025", "summary": "Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a\ndomain-specific formal specification language known for its effectiveness in\ncompactly representing security, opacity, and concurrency properties for\nrobotics applications. This paper focuses on HyperTWTL-constrained secure\nreinforcement learning (SecRL). Although temporal logic-constrained safe\nreinforcement learning (SRL) is an evolving research problem with several\nexisting literature, there is a significant research gap in exploring\nsecurity-aware reinforcement learning (RL) using hyperproperties. Given the\ndynamics of an agent as a Markov Decision Process (MDP) and opacity/security\nconstraints formalized as HyperTWTL, we propose an approach for learning\nsecurity-aware optimal policies using dynamic Boltzmann softmax RL while\nsatisfying the HyperTWTL constraints. The effectiveness and scalability of our\nproposed approach are demonstrated using a pick-up and delivery robotic mission\ncase study. We also compare our results with two other baseline RL algorithms,\nshowing that our proposed method outperforms them.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHyperTWTL\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08SecRL\uff09\uff0c\u901a\u8fc7\u52a8\u6001Boltzmann softmax RL\u5b66\u4e60\u6ee1\u8db3HyperTWTL\u7ea6\u675f\u7684\u5b89\u5168\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u63a2\u7d22\u57fa\u4e8e\u8d85\u5c5e\u6027\u7684\u5b89\u5168\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u7a7a\u767d\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5982\u4f55\u5229\u7528HyperTWTL\u8868\u8fbe\u5b89\u5168\u548c\u4e0d\u900f\u660e\u6027\u7ea6\u675f\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u52a8\u6001\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u5e76\u4f7f\u7528HyperTWTL\u5f62\u5f0f\u5316\u4e0d\u900f\u660e\u6027\u548c\u5b89\u5168\u7ea6\u675f\uff0c\u63d0\u51fa\u52a8\u6001Boltzmann softmax RL\u65b9\u6cd5\u5b66\u4e60\u6ee1\u8db3\u7ea6\u675f\u7684\u6700\u4f18\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u673a\u5668\u4eba\u62fe\u53d6\u548c\u4ea4\u4ed8\u4efb\u52a1\u6848\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4f18\u4e8e\u4e24\u79cd\u57fa\u7ebfRL\u7b97\u6cd5\u3002", "conclusion": "HyperTWTL\u7ea6\u675f\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5b89\u5168\u611f\u77e5RL\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.00103", "categories": ["cs.HC", "cs.AI", "68T01", "H.5.0; I.2.0"], "pdf": "https://arxiv.org/pdf/2508.00103", "abs": "https://arxiv.org/abs/2508.00103", "authors": ["Guilherme Guerino", "Luiz Rodrigues", "Luana Bianchiniand Mariana Alves", "Marcelo Marinho", "Thomaz Veloso", "Valmir Macario", "Diego Dermeval", "Thales Vieira", "Ig Bittencourt", "Seiji Isotani"], "title": "A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app", "comment": "Article accepted in the International Journal of Human-Computer\n  Interaction", "summary": "Integrating Artificial Intelligence in Education (AIED) aims to enhance\nlearning experiences through technologies like Intelligent Tutoring Systems\n(ITS), offering personalized learning, increased engagement, and improved\nretention rates. However, AIED faces three main challenges: the critical role\nof teachers in the design process, the limitations and reliability of AI tools,\nand the accessibility of technological resources. Augmented Intelligence (AuI)\naddresses these challenges by enhancing human capabilities rather than\nreplacing them, allowing systems to suggest solutions. In contrast, humans\nprovide final assessments, thus improving AI over time. In this sense, this\nstudy focuses on designing, developing, and evaluating MathAIde, an ITS that\ncorrects mathematics exercises using computer vision and AI and provides\nfeedback based on photos of student work. The methodology included\nbrainstorming sessions with potential users, high-fidelity prototyping, A/B\ntesting, and a case study involving real-world classroom environments for\nteachers and students. Our research identified several design possibilities for\nimplementing AuI in ITSs, emphasizing a balance between user needs and\ntechnological feasibility. Prioritization and validation through prototyping\nand testing highlighted the importance of efficiency metrics, ultimately\nleading to a solution that offers pre-defined remediation alternatives for\nteachers. Real-world deployment demonstrated the usefulness of the proposed\nsolution. Our research contributes to the literature by providing a usable,\nteacher-centered design approach that involves teachers in all design phases.\nAs a practical implication, we highlight that the user-centered design approach\nincreases the usefulness and adoption potential of AIED systems, especially in\nresource-limited environments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6559\u80b2\u4e2d\u4eba\u5de5\u667a\u80fd\uff08AIED\uff09\u7684\u6574\u5408\uff0c\u63d0\u51fa\u901a\u8fc7\u589e\u5f3a\u667a\u80fd\uff08AuI\uff09\u89e3\u51b3\u6559\u5e08\u53c2\u4e0e\u3001AI\u5de5\u5177\u9650\u5236\u548c\u8d44\u6e90\u53ef\u53ca\u6027\u7b49\u6311\u6218\uff0c\u5e76\u5f00\u53d1\u4e86MathAIde\u7cfb\u7edf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u589e\u5f3a\u667a\u80fd\uff08AIED\uff09\u63d0\u5347\u5b66\u4e60\u4f53\u9a8c\uff0c\u540c\u65f6\u89e3\u51b3\u6559\u5e08\u53c2\u4e0e\u3001AI\u5de5\u5177\u9650\u5236\u548c\u8d44\u6e90\u53ef\u53ca\u6027\u7b49\u5173\u952e\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5305\u62ec\u8bbe\u8ba1\u3001\u5f00\u53d1\u548c\u8bc4\u4f30MathAIde\u7cfb\u7edf\uff0c\u91c7\u7528\u5934\u8111\u98ce\u66b4\u3001\u9ad8\u4fdd\u771f\u539f\u578b\u3001A/B\u6d4b\u8bd5\u548c\u771f\u5b9e\u8bfe\u5802\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u589e\u5f3a\u667a\u80fd\u5728ITS\u4e2d\u7684\u8bbe\u8ba1\u53ef\u80fd\u6027\uff0c\u5f3a\u8c03\u7528\u6237\u9700\u6c42\u4e0e\u6280\u672f\u53ef\u884c\u6027\u7684\u5e73\u8861\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u90e8\u7f72\u9a8c\u8bc1\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u8d21\u732e\u4e86\u4ee5\u6559\u5e08\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5f3a\u8c03\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u63d0\u5347AIED\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u91c7\u7eb3\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u3002"}}
{"id": "2508.00046", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00046", "abs": "https://arxiv.org/abs/2508.00046", "authors": ["Ruo Yu Tao", "Kaicheng Guo", "Cameron Allen", "George Konidaris"], "title": "Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains", "comment": "To appear at RLC 2025. 1 cover page, 10 pages, 3 reference pages + 13\n  pages for supplementary material", "summary": "Mitigating partial observability is a necessary but challenging task for\ngeneral reinforcement learning algorithms. To improve an algorithm's ability to\nmitigate partial observability, researchers need comprehensive benchmarks to\ngauge progress. Most algorithms tackling partial observability are only\nevaluated on benchmarks with simple forms of state aliasing, such as feature\nmasking and Gaussian noise. Such benchmarks do not represent the many forms of\npartial observability seen in real domains, like visual occlusion or unknown\nopponent intent. We argue that a partially observable benchmark should have two\nkey properties. The first is coverage in its forms of partial observability, to\nensure an algorithm's generalizability. The second is a large gap between the\nperformance of a agents with more or less state information, all other factors\nroughly equal. This gap implies that an environment is memory improvable: where\nperformance gains in a domain are from an algorithm's ability to cope with\npartial observability as opposed to other factors. We introduce best-practice\nguidelines for empirically benchmarking reinforcement learning under partial\nobservability, as well as the open-source library POBAX: Partially Observable\nBenchmarks in JAX. We characterize the types of partial observability present\nin various environments and select representative environments for our\nbenchmark. These environments include localization and mapping, visual control,\ngames, and more. Additionally, we show that these tasks are all memory\nimprovable and require hard-to-learn memory functions, providing a concrete\nsignal for partial observability research. This framework includes recommended\nhyperparameters as well as algorithm implementations for fast, out-of-the-box\nevaluation, as well as highly performant environments implemented in JAX for\nGPU-scalable experimentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u6027\u80fd\u7684\u57fa\u51c6\u6846\u67b6POBAX\uff0c\u5e76\u5f3a\u8c03\u4e86\u57fa\u51c6\u5e94\u5177\u5907\u7684\u4e24\u79cd\u5173\u952e\u7279\u6027\uff1a\u8986\u76d6\u591a\u79cd\u90e8\u5206\u53ef\u89c2\u6d4b\u5f62\u5f0f\u4ee5\u53ca\u8bb0\u5fc6\u53ef\u6539\u8fdb\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4ec5\u8bc4\u4f30\u7b80\u5355\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u5f62\u5f0f\uff08\u5982\u7279\u5f81\u63a9\u7801\u548c\u9ad8\u65af\u566a\u58f0\uff09\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u590d\u6742\u6027\uff08\u5982\u89c6\u89c9\u906e\u6321\u6216\u672a\u77e5\u5bf9\u624b\u610f\u56fe\uff09\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51faPOBAX\u5f00\u6e90\u5e93\uff0c\u5305\u542b\u591a\u79cd\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\uff08\u5982\u5b9a\u4f4d\u4e0e\u5730\u56fe\u3001\u89c6\u89c9\u63a7\u5236\u3001\u6e38\u620f\u7b49\uff09\uff0c\u5e76\u63d0\u4f9b\u63a8\u8350\u7684\u8d85\u53c2\u6570\u548c\u7b97\u6cd5\u5b9e\u73b0\u3002", "result": "\u8fd9\u4e9b\u4efb\u52a1\u5747\u5177\u6709\u8bb0\u5fc6\u53ef\u6539\u8fdb\u6027\uff0c\u4e14\u9700\u8981\u96be\u4ee5\u5b66\u4e60\u7684\u8bb0\u5fc6\u529f\u80fd\uff0c\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u4fe1\u53f7\u3002", "conclusion": "POBAX\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u5e76\u63a8\u52a8\u4e86\u7b97\u6cd5\u5728\u590d\u6742\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.00095", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00095", "abs": "https://arxiv.org/abs/2508.00095", "authors": ["Zachary K. Stine", "James E. Deitrick"], "title": "Semiotic Complexity and Its Epistemological Implications for Modeling Culture", "comment": "Preprint. Manuscript currently under review", "summary": "Greater theorizing of methods in the computational humanities is needed for\nepistemological and interpretive clarity, and therefore the maturation of the\nfield. In this paper, we frame such modeling work as engaging in translation\nwork from a cultural, linguistic domain into a computational, mathematical\ndomain, and back again. Translators benefit from articulating the theory of\ntheir translation process, and so do computational humanists in their work --\nto ensure internal consistency, avoid subtle yet consequential translation\nerrors, and facilitate interpretive transparency. Our contribution in this\npaper is to lay out a particularly consequential dimension of the lack of\ntheorizing and the sorts of translation errors that emerge in our modeling\npractices as a result. Along these lines we introduce the idea of semiotic\ncomplexity as the degree to which the meaning of some text may vary across\ninterpretive lenses, and make the case that dominant modeling practices --\nespecially around evaluation -- commit a translation error by treating\nsemiotically complex data as semiotically simple when it seems\nepistemologically convenient by conferring superficial clarity. We then lay out\nseveral recommendations for researchers to better account for these\nepistemological issues in their own work.", "AI": {"tldr": "\u8bba\u6587\u547c\u5401\u8ba1\u7b97\u4eba\u6587\u5b66\u79d1\u9700\u8981\u66f4\u591a\u65b9\u6cd5\u8bba\u7406\u8bba\u5316\uff0c\u4ee5\u63d0\u5347\u8ba4\u8bc6\u8bba\u548c\u89e3\u91ca\u6e05\u6670\u5ea6\u3002\u4f5c\u8005\u5c06\u5efa\u6a21\u5de5\u4f5c\u89c6\u4e3a\u6587\u5316\u8bed\u8a00\u9886\u57df\u4e0e\u8ba1\u7b97\u6570\u5b66\u9886\u57df\u4e4b\u95f4\u7684\u7ffb\u8bd1\uff0c\u5f3a\u8c03\u7406\u8bba\u5316\u7684\u91cd\u8981\u6027\u4ee5\u907f\u514d\u7ffb\u8bd1\u9519\u8bef\u3002", "motivation": "\u8ba1\u7b97\u4eba\u6587\u5b66\u79d1\u7f3a\u4e4f\u65b9\u6cd5\u8bba\u7406\u8bba\u5316\uff0c\u5bfc\u81f4\u8ba4\u8bc6\u8bba\u548c\u89e3\u91ca\u6a21\u7cca\uff0c\u5f71\u54cd\u9886\u57df\u6210\u719f\u3002", "method": "\u5c06\u5efa\u6a21\u5de5\u4f5c\u89c6\u4e3a\u7ffb\u8bd1\u8fc7\u7a0b\uff0c\u63d0\u51fa\u2018\u7b26\u53f7\u590d\u6742\u6027\u2019\u6982\u5ff5\uff0c\u5206\u6790\u5efa\u6a21\u5b9e\u8df5\u4e2d\u56e0\u5ffd\u89c6\u590d\u6742\u6027\u800c\u5bfc\u81f4\u7684\u7ffb\u8bd1\u9519\u8bef\u3002", "result": "\u6307\u51fa\u5f53\u524d\u5efa\u6a21\u5b9e\u8df5\uff08\u5c24\u5176\u662f\u8bc4\u4f30\uff09\u5c06\u7b26\u53f7\u590d\u6742\u6570\u636e\u7b80\u5316\u4e3a\u7b26\u53f7\u7b80\u5355\u6570\u636e\uff0c\u5bfc\u81f4\u8ba4\u8bc6\u8bba\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u5efa\u8bae\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u66f4\u597d\u5730\u5904\u7406\u7b26\u53f7\u590d\u6742\u6027\uff0c\u63d0\u5347\u5efa\u6a21\u7684\u900f\u660e\u5ea6\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.00116", "categories": ["cs.AI", "H.4.1; I.2.1"], "pdf": "https://arxiv.org/pdf/2508.00116", "abs": "https://arxiv.org/abs/2508.00116", "authors": ["Wil M. P. van der Aalst"], "title": "No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence", "comment": "10 pages, 4 figures, preprint keynote paper of the seventh\n  International Conference on Intelligent and Fuzzy Systems (INFUS 2025)", "summary": "The uptake of Artificial Intelligence (AI) impacts the way we work, interact,\ndo business, and conduct research. However, organizations struggle to apply AI\nsuccessfully in industrial settings where the focus is on end-to-end\noperational processes. Here, we consider generative, predictive, and\nprescriptive AI and elaborate on the challenges of diagnosing and improving\nsuch processes. We show that AI needs to be grounded using Object-Centric\nProcess Mining (OCPM). Process-related data are structured and\norganization-specific and, unlike text, processes are often highly dynamic.\nOCPM is the missing link connecting data and processes and enables different\nforms of AI. We use the term Process Intelligence (PI) to refer to the\namalgamation of process-centric data-driven techniques able to deal with a\nvariety of object and event types, enabling AI in an organizational context.\nThis paper explains why AI requires PI to improve operational processes and\nhighlights opportunities for successfully combining OCPM and generative,\npredictive, and prescriptive AI.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6311\u6218\uff0c\u63d0\u51fa\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u8fc7\u7a0b\u6316\u6398\uff08OCPM\uff09\u5b9e\u73b0\u8fc7\u7a0b\u667a\u80fd\uff08PI\uff09\uff0c\u4ee5\u652f\u6301\u751f\u6210\u3001\u9884\u6d4b\u548c\u89c4\u8303AI\u3002", "motivation": "\u7ec4\u7ec7\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u6210\u529f\u5e94\u7528AI\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u7aef\u5230\u7aef\u64cd\u4f5c\u8fc7\u7a0b\u7684\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u5bf9\u8c61\u4e2d\u5fc3\u8fc7\u7a0b\u6316\u6398\uff08OCPM\uff09\u4f5c\u4e3a\u8fde\u63a5\u6570\u636e\u548c\u8fc7\u7a0b\u7684\u6865\u6881\uff0c\u652f\u6301\u591a\u79cdAI\u5f62\u5f0f\u3002", "result": "OCPM\u662f\u5b9e\u73b0\u8fc7\u7a0b\u667a\u80fd\uff08PI\uff09\u7684\u5173\u952e\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301AI\u5728\u7ec4\u7ec7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "AI\u9700\u8981PI\u6765\u6539\u8fdb\u64cd\u4f5c\u8fc7\u7a0b\uff0cOCPM\u4e0e\u751f\u6210\u3001\u9884\u6d4b\u548c\u89c4\u8303AI\u7684\u7ed3\u5408\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2508.00107", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00107", "abs": "https://arxiv.org/abs/2508.00107", "authors": ["Jan Simson"], "title": "Decoupling Data and Tooling in Interactive Visualization", "comment": "Poster at IEEE VIS 2025", "summary": "Interactive data visualization is a major part of modern exploratory data\nanalysis, with web-based technologies enabling a rich ecosystem of both\nspecialized and general tools. However, current visualization tools often lack\nsupport for transformation or wrangling of data and are forced to re-implement\ntheir own solutions to load and ingest data. This redundancy creates\nsubstantial development overhead for tool creators, steeper learning curves for\nusers who must master different data handling interfaces across tools and a\ndegraded user experience as data handling is usually seen as an after-thought.\n  We propose a modular approach that separates data wrangling and loading\ncapabilities from visualization components. This architecture allows\nvisualization tools to concentrate on their core strengths while providing the\nopportunity to develop a unified, powerful interface for data handling. An\nadditional benefit of this approach is that it allows for multiple tools to\nexist and be used side by side. We demonstrate the feasibility of this approach\nby building an early prototype using web technologies to encapsulate\nvisualization tools and manage data flow between them.\n  We discuss future research directions, including downstream integrations with\nother tooling, such as IDEs, literate programming notebooks and applications,\nas well as incorporation of new technologies for efficient data\ntransformations. We seek input from the community to better understand the\nrequirements towards this approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u5c06\u6570\u636e\u6574\u7406\u548c\u52a0\u8f7d\u529f\u80fd\u4e0e\u53ef\u89c6\u5316\u7ec4\u4ef6\u5206\u79bb\uff0c\u4ee5\u51cf\u5c11\u5f00\u53d1\u5197\u4f59\u5e76\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u5f53\u524d\u53ef\u89c6\u5316\u5de5\u5177\u7f3a\u4e4f\u5bf9\u6570\u636e\u8f6c\u6362\u6216\u6574\u7406\u7684\u652f\u6301\uff0c\u5bfc\u81f4\u5f00\u53d1\u6210\u672c\u9ad8\u3001\u7528\u6237\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5206\u79bb\u6570\u636e\u6574\u7406\u548c\u53ef\u89c6\u5316\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u9a8c\u8bc1\u53ef\u884c\u6027\u3002", "result": "\u5c55\u793a\u4e86\u6a21\u5757\u5316\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u51cf\u5c11\u5f00\u53d1\u8d1f\u62c5\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u5e76\u652f\u6301\u591a\u5de5\u5177\u5e76\u884c\u4f7f\u7528\u3002"}}
{"id": "2508.00047", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00047", "abs": "https://arxiv.org/abs/2508.00047", "authors": ["Yuan-Cheng Yu", "Yen-Chieh Ouyang", "Chun-An Lin"], "title": "TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection", "comment": "11 pages, 2 figures", "summary": "Time-series anomaly detection plays a central role across a wide range of\napplication domains. With the increasing proliferation of the Internet of\nThings (IoT) and smart manufacturing, time-series data has dramatically\nincreased in both scale and dimensionality. This growth has exposed the\nlimitations of traditional statistical methods in handling the high\nheterogeneity and complexity of such data. Inspired by the recent success of\nlarge language models (LLMs) in multimodal tasks across language and vision\ndomains, we propose a novel unsupervised anomaly detection framework: A\nTri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly\nDetection (TriP-LLM). TriP-LLM integrates local and global temporal features\nthrough a tri-branch design-Patching, Selection, and Global-to encode the input\ntime series into patch-wise tokens, which are then processed by a frozen,\npretrained LLM. A lightweight patch-wise decoder reconstructs the input, from\nwhich anomaly scores are derived. We evaluate TriP-LLM on several public\nbenchmark datasets using PATE, a recently proposed threshold-free evaluation\nmetric, and conduct all comparisons within a unified open-source framework to\nensure fairness. Experimental results show that TriP-LLM consistently\noutperforms recent state-of-the-art methods across all datasets, demonstrating\nstrong detection capabilities. Furthermore, through extensive ablation studies,\nwe verify the substantial contribution of the LLM to the overall architecture.\nCompared to LLM-based approaches using Channel Independence (CI) patch\nprocessing, TriP-LLM achieves significantly lower memory consumption, making it\nmore suitable for GPU memory-constrained environments. All code and model\ncheckpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git", "AI": {"tldr": "TriP-LLM\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5206\u652f\u8bbe\u8ba1\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u65f6\u95f4\u7279\u5f81\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u53d7\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u542f\u53d1\uff0c\u63d0\u51fa\u4e86TriP-LLM\u3002", "method": "TriP-LLM\u91c7\u7528\u4e09\u5206\u652f\u8bbe\u8ba1\uff08Patching\u3001Selection\u3001Global\uff09\u5c06\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u4e3a\u8865\u4e01\u4ee4\u724c\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3LLM\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u91cd\u6784\u8f93\u5165\u4ee5\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cTriP-LLM\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5185\u5b58\u6d88\u8017\u663e\u8457\u4f4e\u4e8e\u57fa\u4e8eCI\u8865\u4e01\u5904\u7406\u7684LLM\u65b9\u6cd5\u3002", "conclusion": "TriP-LLM\u5c55\u793a\u4e86LLM\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u9002\u7528\u4e8eGPU\u5185\u5b58\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2508.00109", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00109", "abs": "https://arxiv.org/abs/2508.00109", "authors": ["Mingda Chen", "Yang Li", "Xilun Chen", "Adina Williams", "Gargi Ghosh", "Scott Yih"], "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality", "comment": null, "summary": "Long-form factuality evaluation assesses the ability of models to generate\naccurate, comprehensive responses to short prompts. Existing benchmarks often\nlack human verification, leading to potential quality issues. To address this\nlimitation, we introduce FACTORY, a large-scale, human-verified prompt set.\nDeveloped using a model-in-the-loop approach and refined by humans, FACTORY\nincludes challenging prompts that are fact-seeking, answerable, and\nunambiguous. We conduct human evaluations on 6 state-of-the-art language models\nusing FACTORY and existing datasets. Our results show that FACTORY is a\nchallenging benchmark: approximately 40% of the claims made in the responses of\nSOTA models are not factual, compared to only 10% for other datasets. Our\nanalysis identifies the strengths of FACTORY over prior benchmarks, emphasizing\nits reliability and the necessity for models to reason across long-tailed\nfacts.", "AI": {"tldr": "FACTORY\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u4eba\u5de5\u9a8c\u8bc1\u7684\u63d0\u793a\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u751f\u6210\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u73b0\u6709SOTA\u6a21\u578b\u5728FACTORY\u4e0a\u7684\u4e8b\u5b9e\u9519\u8bef\u7387\u9ad8\u8fbe40%\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u5b9e\u6027\u8bc4\u4f30\u57fa\u51c6\u7f3a\u4e4f\u4eba\u5de5\u9a8c\u8bc1\uff0c\u53ef\u80fd\u5bfc\u81f4\u8d28\u91cf\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u91c7\u7528\u6a21\u578b\u4e0e\u4eba\u5de5\u7ed3\u5408\u7684\u5f00\u53d1\u65b9\u5f0f\uff0c\u6784\u5efaFACTORY\u63d0\u793a\u96c6\uff0c\u5e76\u5bf96\u4e2aSOTA\u6a21\u578b\u8fdb\u884c\u4eba\u5de5\u8bc4\u4f30\u3002", "result": "FACTORY\u662f\u4e00\u4e2a\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0cSOTA\u6a21\u578b\u5728\u5176\u4e0a\u7684\u4e8b\u5b9e\u9519\u8bef\u7387\u4e3a40%\uff0c\u8fdc\u9ad8\u4e8e\u5176\u4ed6\u6570\u636e\u96c6\u768410%\u3002", "conclusion": "FACTORY\u56e0\u5176\u53ef\u9760\u6027\u548c\u5bf9\u957f\u5c3e\u4e8b\u5b9e\u63a8\u7406\u7684\u9700\u6c42\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002"}}
{"id": "2508.00129", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00129", "abs": "https://arxiv.org/abs/2508.00129", "authors": ["Agust\u00edn Borda", "Juan Bautista Cabral", "Gonzalo Giarda", "Diego Nicol\u00e1s Gimenez Irusta", "Paula Pacheco", "Alvaro Roy Schachner"], "title": "Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis", "comment": null, "summary": "In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem\nthat can greatly affect the results of a Multi-Criteria Decision Method against\na particular set of alternatives. It is therefore useful to have a mechanism\nthat allows one to measure the performance of a method on a set of\nalternatives. This idea could be taken further to build a global ranking of the\neffectiveness of different methods to solve a problem. In this paper, we\npresent three tests that detect the presence of Rank Reversals, along with\ntheir implementation in the Scikit-Criteria library. We also address the\ncomplications that arise when implementing these tests for general scenarios\nand the design considerations we made to handle them. We close with a\ndiscussion about how these additions could play a major role in the judgment of\nmulti-criteria decision methods for problem solving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u68c0\u6d4b\u591a\u51c6\u5219\u51b3\u7b56\u5206\u6790\u4e2d\u6392\u540d\u53cd\u8f6c\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728Scikit-Criteria\u5e93\u4e2d\u7684\u5b9e\u73b0\u53ca\u8bbe\u8ba1\u8003\u8651\u3002", "motivation": "\u6392\u540d\u53cd\u8f6c\u662f\u591a\u51c6\u5219\u51b3\u7b56\u5206\u6790\u4e2d\u7684\u4e25\u91cd\u95ee\u9898\uff0c\u5f71\u54cd\u51b3\u7b56\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u8bc4\u4f30\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u68c0\u6d4b\u6392\u540d\u53cd\u8f6c\uff0c\u5e76\u5728Scikit-Criteria\u5e93\u4e2d\u5b9e\u73b0\u3002", "result": "\u5b9e\u73b0\u4e86\u9488\u5bf9\u4e00\u822c\u573a\u666f\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u89e3\u51b3\u4e86\u76f8\u5173\u8bbe\u8ba1\u95ee\u9898\u3002", "conclusion": "\u8fd9\u4e9b\u6d4b\u8bd5\u65b9\u6cd5\u5728\u591a\u51c6\u5219\u51b3\u7b56\u65b9\u6cd5\u7684\u8bc4\u4f30\u4e2d\u53ef\u80fd\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2508.00140", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00140", "abs": "https://arxiv.org/abs/2508.00140", "authors": ["Zhanna Kaufman", "Madeline Endres", "Cindy Xiong Bearfield", "Yuriy Brun"], "title": "Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models", "comment": null, "summary": "Systems relying on ML have become ubiquitous, but so has biased behavior\nwithin them. Research shows that bias significantly affects stakeholders' trust\nin systems and how they use them. Further, stakeholders of different\nbackgrounds view and trust the same systems differently. Thus, how ML models'\nbehavior is explained plays a key role in comprehension and trust. We survey\nexplainability visualizations, creating a taxonomy of design characteristics.\nWe conduct user studies to evaluate five state-of-the-art visualization tools\n(LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how\ntaxonomy characteristics affect comprehension, bias perception, and trust for\nnon-expert ML users. Surprisingly, we find an inverse relationship between\ncomprehension and trust: the better users understand the models, the less they\ntrust them. We investigate the cause and find that this relationship is\nstrongly mediated by bias perception: more comprehensible visualizations\nincrease people's perception of bias, and increased bias perception reduces\ntrust. We confirm this relationship is causal: Manipulating explainability\nvisualizations to control comprehension, bias perception, and trust, we show\nthat visualization design can significantly (p < 0.001) increase comprehension,\nincrease perceived bias, and reduce trust. Conversely, reducing perceived model\nbias, either by improving model fairness or by adjusting visualization design,\nsignificantly increases trust even when comprehension remains high. Our work\nadvances understanding of how comprehension affects trust and systematically\ninvestigates visualization's role in facilitating responsible ML applications.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86ML\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u53ef\u89c6\u5316\u5bf9\u7528\u6237\u7406\u89e3\u548c\u4fe1\u4efb\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7406\u89e3\u8d8a\u6df1\uff0c\u4fe1\u4efb\u8d8a\u4f4e\uff0c\u539f\u56e0\u662f\u611f\u77e5\u5230\u7684\u504f\u89c1\u589e\u52a0\u3002", "motivation": "ML\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u884c\u4e3a\u666e\u904d\u5b58\u5728\uff0c\u5f71\u54cd\u7528\u6237\u4fe1\u4efb\uff0c\u4e0d\u540c\u80cc\u666f\u7684\u7528\u6237\u5bf9\u540c\u4e00\u7cfb\u7edf\u7684\u770b\u6cd5\u548c\u4fe1\u4efb\u5ea6\u4e0d\u540c\u3002", "method": "\u8c03\u67e5\u53ef\u89e3\u91ca\u6027\u53ef\u89c6\u5316\u5de5\u5177\uff08\u5982LIME\u3001SHAP\u7b49\uff09\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u5176\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u7684\u7406\u89e3\u3001\u504f\u89c1\u611f\u77e5\u548c\u4fe1\u4efb\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u7406\u89e3\u4e0e\u4fe1\u4efb\u5448\u8d1f\u76f8\u5173\uff0c\u53ef\u89c6\u5316\u8bbe\u8ba1\u80fd\u663e\u8457\u5f71\u54cd\u7406\u89e3\u3001\u504f\u89c1\u611f\u77e5\u548c\u4fe1\u4efb\u3002", "conclusion": "\u53ef\u89c6\u5316\u8bbe\u8ba1\u5728\u4fc3\u8fdb\u8d1f\u8d23\u4efbML\u5e94\u7528\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u51cf\u5c11\u504f\u89c1\u611f\u77e5\u53ef\u63d0\u9ad8\u4fe1\u4efb\u3002"}}
{"id": "2508.00078", "categories": ["cs.LG", "cs.AI", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2508.00078", "abs": "https://arxiv.org/abs/2508.00078", "authors": ["Imen Mahmoud", "Andrei Velichko"], "title": "Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization", "comment": "22 pages, 5 figures", "summary": "This study proposes a novel methodological framework integrating a LightGBM\nregression model and genetic algorithm (GA) optimization to systematically\nevaluate the contribution of COVID-19-related indicators to Bitcoin return\nprediction. The primary objective was not merely to forecast Bitcoin returns\nbut rather to determine whether including pandemic-related health data\nsignificantly enhances prediction accuracy. A comprehensive dataset comprising\ndaily Bitcoin returns and COVID-19 metrics (vaccination rates,\nhospitalizations, testing statistics) was constructed. Predictive models,\ntrained with and without COVID-19 features, were optimized using GA over 31\nindependent runs, allowing robust statistical assessment. Performance metrics\n(R2, RMSE, MAE) were statistically compared through distribution overlaps and\nMann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified\nindividual feature contributions. Results indicate that COVID-19 indicators\nsignificantly improved model performance, particularly in capturing extreme\nmarket fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly\nsignificant statistically). Among COVID-19 features, vaccination metrics,\nespecially the 75th percentile of fully vaccinated individuals, emerged as\ndominant predictors. The proposed methodology extends existing financial\nanalytics tools by incorporating public health signals, providing investors and\npolicymakers with refined indicators to navigate market uncertainty during\nsystemic crises.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LightGBM\u56de\u5f52\u6a21\u578b\u548c\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u8bc4\u4f30COVID-19\u76f8\u5173\u6307\u6807\u5bf9\u6bd4\u7279\u5e01\u56de\u62a5\u9884\u6d4b\u7684\u8d21\u732e\u3002\u7ed3\u679c\u8868\u660e\uff0c\u75ab\u60c5\u6307\u6807\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u75ab\u82d7\u63a5\u79cd\u6570\u636e\u3002", "motivation": "\u7814\u7a76\u7684\u4e3b\u8981\u76ee\u6807\u4e0d\u4ec5\u662f\u9884\u6d4b\u6bd4\u7279\u5e01\u56de\u62a5\uff0c\u800c\u662f\u786e\u5b9a\u5305\u542b\u75ab\u60c5\u76f8\u5173\u5065\u5eb7\u6570\u636e\u662f\u5426\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528LightGBM\u56de\u5f52\u6a21\u578b\u548c\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\uff0c\u6784\u5efa\u5305\u542b\u6bd4\u7279\u5e01\u56de\u62a5\u548cCOVID-19\u6307\u6807\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc731\u6b21\u72ec\u7acb\u8fd0\u884c\u8fdb\u884c\u7edf\u8ba1\u8bc4\u4f30\u3002", "result": "COVID-19\u6307\u6807\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff08R2\u589e\u52a040%\uff0cRMSE\u964d\u4f4e2%\uff09\uff0c\u75ab\u82d7\u63a5\u79cd\u6570\u636e\u662f\u4e3b\u8981\u9884\u6d4b\u56e0\u7d20\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u516c\u5171\u536b\u751f\u4fe1\u53f7\u6269\u5c55\u4e86\u91d1\u878d\u5206\u6790\u5de5\u5177\uff0c\u4e3a\u6295\u8d44\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5728\u7cfb\u7edf\u6027\u5371\u673a\u4e2d\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u5e02\u573a\u6307\u6807\u3002"}}
{"id": "2508.00121", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00121", "abs": "https://arxiv.org/abs/2508.00121", "authors": ["Xiao Zhang", "Johan bos"], "title": "Is neural semantic parsing good at ellipsis resolution, or isn't it?", "comment": "Accepted by 16th IWCS", "summary": "Neural semantic parsers have shown good overall performance for a variety of\nlinguistic phenomena, reaching semantic matching scores of more than 90%. But\nhow do such parsers perform on strongly context-sensitive phenomena, where\nlarge pieces of semantic information need to be duplicated to form a meaningful\nsemantic representation? A case in point is English verb phrase ellipsis, a\nconstruct where entire verb phrases can be abbreviated by a single auxiliary\nverb. Are the otherwise known as powerful semantic parsers able to deal with\nellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with\ntheir fully resolved meaning representation and used this as a challenge set\nfor a large battery of neural semantic parsers. Although these parsers\nperformed very well on the standard test set, they failed in the instances with\nellipsis. Data augmentation", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u8bed\u4e49\u89e3\u6790\u5668\u5728\u5f3a\u4e0a\u4e0b\u6587\u654f\u611f\u73b0\u8c61\uff08\u5982\u82f1\u8bed\u52a8\u8bcd\u77ed\u8bed\u7701\u7565\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c3d\u7ba1\u5728\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7701\u7565\u73b0\u8c61\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u8bed\u4e49\u89e3\u6790\u5668\u5728\u5904\u7406\u9700\u8981\u5927\u91cf\u8bed\u4e49\u4fe1\u606f\u590d\u5236\u7684\u5f3a\u4e0a\u4e0b\u6587\u654f\u611f\u73b0\u8c61\uff08\u5982\u52a8\u8bcd\u77ed\u8bed\u7701\u7565\uff09\u65f6\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b120\u4e2a\u7701\u7565\u6848\u4f8b\u53ca\u5176\u5b8c\u5168\u89e3\u6790\u7684\u8bed\u4e49\u8868\u793a\u7684\u8bed\u6599\u5e93\uff0c\u4f5c\u4e3a\u6311\u6218\u96c6\u6d4b\u8bd5\u591a\u79cd\u795e\u7ecf\u8bed\u4e49\u89e3\u6790\u5668\u3002", "result": "\u5c3d\u7ba1\u89e3\u6790\u5668\u5728\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff08\u8bed\u4e49\u5339\u914d\u5206\u6570\u8d85\u8fc790%\uff09\uff0c\u4f46\u5728\u7701\u7565\u6848\u4f8b\u4e2d\u8868\u73b0\u5931\u8d25\u3002", "conclusion": "\u795e\u7ecf\u8bed\u4e49\u89e3\u6790\u5668\u5728\u5904\u7406\u5f3a\u4e0a\u4e0b\u6587\u654f\u611f\u73b0\u8c61\uff08\u5982\u7701\u7565\uff09\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u6570\u636e\u589e\u5f3a\u53ef\u80fd\u662f\u4e00\u79cd\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2508.00137", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00137", "abs": "https://arxiv.org/abs/2508.00137", "authors": ["Shqiponja Ahmetaj", "George Konstantinidis", "Magdalena Ortiz", "Paolo Pareti", "Mantas Simkus"], "title": "SHACL Validation under Graph Updates (Extended Paper)", "comment": "Accepted at the International Semantic Web Conference (ISWC 2025)", "summary": "SHACL (SHApe Constraint Language) is a W3C standardized constraint language\nfor RDF graphs. In this paper, we study SHACL validation in RDF graphs under\nupdates. We present a SHACL-based update language that can capture intuitive\nand realistic modifications on RDF graphs and study the problem of static\nvalidation under such updates. This problem asks to verify whether every graph\nthat validates a SHACL specification will still do so after applying a given\nupdate sequence. More importantly, it provides a basis for further services for\nreasoning about evolving RDF graphs. Using a regression technique that embeds\nthe update actions into SHACL constraints, we show that static validation under\nupdates can be reduced to (un)satisfiability of constraints in (a minor\nextension of) SHACL. We analyze the computational complexity of the static\nvalidation problem for SHACL and some key fragments. Finally, we present a\nprototype implementation that performs static validation and other static\nanalysis tasks on SHACL constraints and demonstrate its behavior through\npreliminary experiments.", "AI": {"tldr": "\u7814\u7a76SHACL\u5728RDF\u56fe\u66f4\u65b0\u4e0b\u7684\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eSHACL\u7684\u66f4\u65b0\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u56de\u5f52\u6280\u672f\u5c06\u5176\u8f6c\u5316\u4e3aSHACL\u7ea6\u675f\u7684\uff08\u4e0d\uff09\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u3002", "motivation": "\u7814\u7a76RDF\u56fe\u5728\u66f4\u65b0\u540e\u662f\u5426\u4ecd\u6ee1\u8db3SHACL\u89c4\u8303\uff0c\u4e3a\u52a8\u6001RDF\u56fe\u63d0\u4f9b\u9a8c\u8bc1\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eSHACL\u7684\u66f4\u65b0\u8bed\u8a00\uff0c\u4f7f\u7528\u56de\u5f52\u6280\u672f\u5c06\u66f4\u65b0\u52a8\u4f5c\u5d4c\u5165SHACL\u7ea6\u675f\uff0c\u5206\u6790\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5c55\u793a\u4e86\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u53ef\u8f6c\u5316\u4e3aSHACL\u7ea6\u675f\u7684\uff08\u4e0d\uff09\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\uff0c\u4e3a\u52a8\u6001RDF\u56fe\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9a8c\u8bc1\u5de5\u5177\u3002"}}
{"id": "2508.00160", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.00160", "abs": "https://arxiv.org/abs/2508.00160", "authors": ["Ziqing Xu", "Nick Bryan-Kinns"], "title": "DeformTune: A Deformable XAI Music Prototype for Non-Musicians", "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "summary": "Many existing AI music generation tools rely on text prompts, complex\ninterfaces, or instrument-like controls, which may require musical or technical\nknowledge that non-musicians do not possess. This paper introduces DeformTune,\na prototype system that combines a tactile deformable interface with the\nMeasureVAE model to explore more intuitive, embodied, and explainable AI\ninteraction. We conducted a preliminary study with 11 adult participants\nwithout formal musical training to investigate their experience with\nAI-assisted music creation. Thematic analysis of their feedback revealed\nrecurring challenge--including unclear control mappings, limited expressive\nrange, and the need for guidance throughout use. We discuss several design\nopportunities for enhancing explainability of AI, including multimodal feedback\nand progressive interaction support. These findings contribute early insights\ntoward making AI music systems more explainable and empowering for novice\nusers.", "AI": {"tldr": "DeformTune\u662f\u4e00\u4e2a\u7ed3\u5408\u53ef\u53d8\u5f62\u89e6\u63a7\u754c\u9762\u4e0eMeasureVAE\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u4e3a\u975e\u97f3\u4e50\u4e13\u4e1a\u4eba\u58eb\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684AI\u97f3\u4e50\u521b\u4f5c\u4f53\u9a8c\u3002\u521d\u6b65\u7814\u7a76\u8868\u660e\uff0c\u7528\u6237\u9762\u4e34\u63a7\u5236\u6620\u5c04\u4e0d\u6e05\u6670\u7b49\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u589e\u5f3aAI\u53ef\u89e3\u91ca\u6027\u7684\u8bbe\u8ba1\u673a\u4f1a\u3002", "motivation": "\u73b0\u6709AI\u97f3\u4e50\u751f\u6210\u5de5\u5177\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u6216\u590d\u6742\u754c\u9762\uff0c\u5bf9\u975e\u97f3\u4e50\u4e13\u4e1a\u4eba\u58eb\u4e0d\u53cb\u597d\u3002DeformTune\u65e8\u5728\u901a\u8fc7\u89e6\u89c9\u754c\u9762\u548c\u76f4\u89c2\u4ea4\u4e92\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\u3002", "method": "\u7ed3\u5408\u53ef\u53d8\u5f62\u89e6\u63a7\u754c\u9762\u4e0eMeasureVAE\u6a21\u578b\uff0c\u901a\u8fc711\u540d\u65e0\u97f3\u4e50\u80cc\u666f\u7684\u6210\u4eba\u7528\u6237\u8fdb\u884c\u521d\u6b65\u7814\u7a76\uff0c\u91c7\u7528\u4e3b\u9898\u5206\u6790\u53cd\u9988\u3002", "result": "\u7528\u6237\u53cd\u9988\u663e\u793a\u63a7\u5236\u6620\u5c04\u4e0d\u6e05\u6670\u3001\u8868\u8fbe\u8303\u56f4\u6709\u9650\u7b49\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u53cd\u9988\u548c\u6e10\u8fdb\u5f0f\u4ea4\u4e92\u652f\u6301\u7b49\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347AI\u97f3\u4e50\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\u63d0\u4f9b\u4e86\u65e9\u671f\u89c1\u89e3\uff0c\u5c24\u5176\u9488\u5bf9\u65b0\u624b\u7528\u6237\u3002"}}
{"id": "2508.00098", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00098", "abs": "https://arxiv.org/abs/2508.00098", "authors": ["Ashkan Shakarami", "Yousef Yeganeh", "Azade Farshad", "Lorenzo Nicole", "Stefano Ghidoni", "Nassir Navab"], "title": "Stress-Aware Resilient Neural Training", "comment": "16 pages, 11 figures", "summary": "This paper introduces Stress-Aware Learning, a resilient neural training\nparadigm in which deep neural networks dynamically adjust their optimization\nbehavior - whether under stable training regimes or in settings with uncertain\ndynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)\nDeformation, inspired by structural fatigue in materials science. To\ninstantiate this concept, we propose Plastic Deformation Optimizer, a\nstress-aware mechanism that injects adaptive noise into model parameters\nwhenever an internal stress signal - reflecting stagnation in training loss and\naccuracy - indicates persistent optimization difficulty. This enables the model\nto escape sharp minima and converge toward flatter, more generalizable regions\nof the loss landscape. Experiments across six architectures, four optimizers,\nand seven vision benchmarks demonstrate improved robustness and generalization\nwith minimal computational overhead. The code and 3D visuals will be available\non GitHub: https://github.com/Stress-Aware-Learning/SAL.", "AI": {"tldr": "Stress-Aware Learning\u662f\u4e00\u79cd\u5f39\u6027\u795e\u7ecf\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f18\u5316\u884c\u4e3a\uff0c\u5e2e\u52a9\u6a21\u578b\u9003\u79bb\u5c16\u9510\u6700\u5c0f\u503c\uff0c\u6536\u655b\u5230\u66f4\u5e73\u5766\u3001\u6cdb\u5316\u6027\u66f4\u5f3a\u7684\u635f\u5931\u533a\u57df\u3002", "motivation": "\u53d7\u6750\u6599\u79d1\u5b66\u4e2d\u7ed3\u6784\u75b2\u52b3\u7684\u542f\u53d1\uff0c\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u56e0\u4f18\u5316\u56f0\u96be\u800c\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPlastic Deformation Optimizer\uff0c\u901a\u8fc7\u5185\u90e8\u5e94\u529b\u4fe1\u53f7\u68c0\u6d4b\u8bad\u7ec3\u505c\u6ede\uff0c\u5e76\u6ce8\u5165\u81ea\u9002\u5e94\u566a\u58f0\u4ee5\u8c03\u6574\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5728\u516d\u79cd\u67b6\u6784\u3001\u56db\u79cd\u4f18\u5316\u5668\u548c\u4e03\u4e2a\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u5c0f\u3002", "conclusion": "Stress-Aware Learning\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.00185", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00185", "abs": "https://arxiv.org/abs/2508.00185", "authors": ["Alper Yaman", "Jannik Schwab", "Christof Nitsche", "Abhirup Sinha", "Marco Huber"], "title": "Comparison of Large Language Models for Deployment Requirements", "comment": null, "summary": "Large Language Models (LLMs), such as Generative Pre-trained Transformers\n(GPTs) are revolutionizing the generation of human-like text, producing\ncontextually relevant and syntactically correct content. Despite challenges\nlike biases and hallucinations, these Artificial Intelligence (AI) models excel\nin tasks, such as content creation, translation, and code generation.\nFine-tuning and novel architectures, such as Mixture of Experts (MoE), address\nthese issues. Over the past two years, numerous open-source foundational and\nfine-tuned models have been introduced, complicating the selection of the\noptimal LLM for researchers and companies regarding licensing and hardware\nrequirements. To navigate the rapidly evolving LLM landscape and facilitate LLM\nselection, we present a comparative list of foundational and domain-specific\nmodels, focusing on features, such as release year, licensing, and hardware\nrequirements. This list is published on GitLab and will be continuously\nupdated.", "AI": {"tldr": "\u8be5\u8bba\u6587\u603b\u7ed3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\u73b0\u72b6\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u7279\u70b9\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6301\u7eed\u66f4\u65b0\u7684\u6a21\u578b\u5217\u8868\u4ee5\u5e2e\u52a9\u7814\u7a76\u8005\u548c\u4f01\u4e1a\u9009\u62e9\u9002\u5408\u7684\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u5f00\u6e90\u548c\u9886\u57df\u7279\u5b9aLLM\u6a21\u578b\u7684\u589e\u591a\uff0c\u7814\u7a76\u8005\u548c\u4f01\u4e1a\u5728\u9009\u62e9\u6a21\u578b\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u7efc\u5408\u6bd4\u8f83\u6a21\u578b\u7279\u6027\u3001\u8bb8\u53ef\u548c\u786c\u4ef6\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u6574\u7406\u548c\u6bd4\u8f83\u4e0d\u540cLLM\u6a21\u578b\u7684\u7279\u5f81\uff08\u5982\u53d1\u5e03\u65f6\u95f4\u3001\u8bb8\u53ef\u548c\u786c\u4ef6\u9700\u6c42\uff09\uff0c\u521b\u5efa\u5e76\u53d1\u5e03\u4e00\u4e2a\u6301\u7eed\u66f4\u65b0\u7684\u6a21\u578b\u5217\u8868\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5f00\u7684\u3001\u6301\u7eed\u66f4\u65b0\u7684LLM\u6a21\u578b\u6bd4\u8f83\u5217\u8868\uff0c\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u9009\u62e9\u9002\u5408\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u6a21\u578b\u7684\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5e76\u8ba1\u5212\u901a\u8fc7\u6301\u7eed\u66f4\u65b0\u6765\u9002\u5e94\u5feb\u901f\u53d1\u5c55\u7684LLM\u9886\u57df\u3002"}}
{"id": "2508.00138", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00138", "abs": "https://arxiv.org/abs/2508.00138", "authors": ["Rashid Mushkani", "Hugo Berard", "Toumadher Ammar", "Cassandre Chatonnier", "Shin Koseki"], "title": "Co-Producing AI: Toward an Augmented, Participatory Lifecycle", "comment": "Eighth AAAI/ACM Conference on AI, Ethics, and Society 2025", "summary": "Despite efforts to mitigate the inherent risks and biases of artificial\nintelligence (AI) algorithms, these algorithms can disproportionately impact\nculturally marginalized groups. A range of approaches has been proposed to\naddress or reduce these risks, including the development of ethical guidelines\nand principles for responsible AI, as well as technical solutions that promote\nalgorithmic fairness. Drawing on design justice, expansive learning theory, and\nrecent empirical work on participatory AI, we argue that mitigating these harms\nrequires a fundamental re-architecture of the AI production pipeline. This\nre-design should center co-production, diversity, equity, inclusion (DEI), and\nmultidisciplinary collaboration. We introduce an augmented AI lifecycle\nconsisting of five interconnected phases: co-framing, co-design,\nco-implementation, co-deployment, and co-maintenance. The lifecycle is informed\nby four multidisciplinary workshops and grounded in themes of distributed\nauthority and iterative knowledge exchange. Finally, we relate the proposed\nlifecycle to several leading ethical frameworks and outline key research\nquestions that remain for scaling participatory governance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bbe\u8ba1\u6b63\u4e49\u548c\u53c2\u4e0e\u5f0fAI\u7684AI\u751f\u547d\u5468\u671f\u91cd\u6784\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5171\u540c\u751f\u4ea7\u3001\u591a\u6837\u6027\u3001\u516c\u5e73\u6027\u548c\u591a\u5b66\u79d1\u5408\u4f5c\uff0c\u4ee5\u51cf\u5c11AI\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "AI\u7b97\u6cd5\u53ef\u80fd\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u4ea7\u751f\u4e0d\u6210\u6bd4\u4f8b\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4f26\u7406\u6307\u5357\u548c\u6280\u672f\u89e3\u51b3\u65b9\u6848\uff09\u672a\u80fd\u6839\u672c\u89e3\u51b3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4e94\u4e2a\u9636\u6bb5\u7684\u589e\u5f3aAI\u751f\u547d\u5468\u671f\uff08\u5171\u540c\u6846\u67b6\u3001\u5171\u540c\u8bbe\u8ba1\u3001\u5171\u540c\u5b9e\u65bd\u3001\u5171\u540c\u90e8\u7f72\u3001\u5171\u540c\u7ef4\u62a4\uff09\uff0c\u57fa\u4e8e\u591a\u5b66\u79d1\u7814\u8ba8\u4f1a\u548c\u5206\u5e03\u5f0f\u6743\u5a01\u7406\u5ff5\u3002", "result": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u8bbe\u8ba1\u6b63\u4e49\u548c\u53c2\u4e0e\u5f0fAI\uff0c\u4e3aAI\u751f\u4ea7\u6d41\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u67b6\u6784\u3002", "conclusion": "\u91cd\u6784AI\u751f\u547d\u5468\u671f\u662f\u51cf\u5c11AI\u5371\u5bb3\u7684\u5173\u952e\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u6269\u5c55\u53c2\u4e0e\u5f0f\u6cbb\u7406\u3002"}}
{"id": "2508.00178", "categories": ["cs.HC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00178", "abs": "https://arxiv.org/abs/2508.00178", "authors": ["Brian Houck", "Travis Lowdermilk", "Cody Beyer", "Steven Clarke", "Ben Hanrahan"], "title": "The SPACE of AI: Real-World Lessons on AI's Impact on Developers", "comment": null, "summary": "As artificial intelligence (AI) tools become increasingly embedded in\nsoftware development workflows, questions persist about their true impact on\ndeveloper productivity and experience. This paper presents findings from a\nmixed-methods study examining how developers perceive AI's influence across the\ndimensions of the SPACE framework: Satisfaction, Performance, Activity,\nCollaboration and Efficiency. Drawing on survey responses from over 500\ndevelopers and qualitative insights from interviews and observational studies,\nwe find that AI is broadly adopted and widely seen as enhancing productivity,\nparticularly for routine tasks. However, the benefits vary, depending on task\ncomplexity, individual usage patterns, and team-level adoption. Developers\nreport increased efficiency and satisfaction, with less evidence of impact on\ncollaboration. Organizational support and peer learning play key roles in\nmaximizing AI's value. These findings suggest that AI is augmenting developers\nrather than replacing them, and that effective integration depends as much on\nteam culture and support structures as on the tools themselves. We conclude\nwith practical recommendations for teams, organizations and researchers seeking\nto harness AI's potential in software engineering.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e7f\u6cdb\u91c7\u7528\uff0c\u63d0\u5347\u6548\u7387\u548c\u6ee1\u610f\u5ea6\uff0c\u4f46\u5bf9\u534f\u4f5c\u5f71\u54cd\u8f83\u5c0f\uff0c\u56e2\u961f\u6587\u5316\u548c\u652f\u6301\u7ed3\u6784\u662f\u5173\u952e\u3002", "motivation": "\u63a2\u8ba8AI\u5de5\u5177\u5bf9\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u548c\u4f53\u9a8c\u7684\u771f\u5b9e\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u5305\u62ec500\u591a\u540d\u5f00\u53d1\u8005\u7684\u8c03\u67e5\u3001\u8bbf\u8c08\u548c\u89c2\u5bdf\u7814\u7a76\u3002", "result": "AI\u666e\u904d\u63d0\u5347\u751f\u4ea7\u529b\uff0c\u5c24\u5176\u5bf9\u5e38\u89c4\u4efb\u52a1\uff0c\u4f46\u6548\u679c\u56e0\u4efb\u52a1\u590d\u6742\u5ea6\u3001\u4f7f\u7528\u6a21\u5f0f\u548c\u56e2\u961f\u91c7\u7528\u7a0b\u5ea6\u800c\u5f02\u3002", "conclusion": "AI\u662f\u5f00\u53d1\u8005\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u6709\u6548\u6574\u5408\u9700\u56e2\u961f\u6587\u5316\u548c\u652f\u6301\u7ed3\u6784\u914d\u5408\uff0c\u5e76\u63d0\u51fa\u5b9e\u8df5\u5efa\u8bae\u3002"}}
{"id": "2508.00117", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00117", "abs": "https://arxiv.org/abs/2508.00117", "authors": ["Md. Ehsanul Haque", "S. M. Jahidul Islam", "Shakil Mia", "Rumana Sharmin", "Ashikuzzaman", "Md Samir Morshed", "Md. Tahmidul Huque"], "title": "StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection", "comment": "Accepted and presented paper of THE 16th INTERNATIONAL IEEE\n  CONFERENCE ON COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT)\n  INDIA", "summary": "Liver diseases are a serious health concern in the world, which requires\nprecise and timely diagnosis to enhance the survival chances of patients. The\ncurrent literature implemented numerous machine learning and deep learning\nmodels to classify liver diseases, but most of them had some issues like high\nmisclassification error, poor interpretability, prohibitive computational\nexpense, and lack of good preprocessing strategies. In order to address these\ndrawbacks, we introduced StackLiverNet in this study; an interpretable stacked\nensemble model tailored to the liver disease detection task. The framework uses\nadvanced data preprocessing and feature selection technique to increase model\nrobustness and predictive ability. Random undersampling is performed to deal\nwith class imbalance and make the training balanced. StackLiverNet is an\nensemble of several hyperparameter-optimized base classifiers, whose\ncomplementary advantages are used through a LightGBM meta-model. The provided\nmodel demonstrates excellent performance, with the testing accuracy of 99.89%,\nCohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and\nefficient training and inference speeds that are amenable to clinical practice\n(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local\nInterpretable Model-Agnostic Explanations (LIME) are applied to generate\ntransparent explanations of individual predictions, revealing high\nconcentrations of Alkaline Phosphatase and moderate SGOT as important\nobservations of liver disease. Also, SHAP was used to rank features by their\nglobal contribution to predictions, while the Morris method confirmed the most\ninfluential features through sensitivity analysis.", "AI": {"tldr": "StackLiverNet\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5806\u53e0\u96c6\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u809d\u75c5\u68c0\u6d4b\uff0c\u901a\u8fc7\u9ad8\u7ea7\u6570\u636e\u9884\u5904\u7406\u548c\u7279\u5f81\u9009\u62e9\u63d0\u9ad8\u6027\u80fd\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u809d\u75c5\u5206\u7c7b\u6a21\u578b\u5b58\u5728\u9ad8\u8bef\u5206\u7c7b\u7387\u3001\u89e3\u91ca\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u968f\u673a\u6b20\u91c7\u6837\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u7ed3\u5408\u8d85\u53c2\u6570\u4f18\u5316\u7684\u57fa\u5206\u7c7b\u5668\u548cLightGBM\u5143\u6a21\u578b\u3002", "result": "\u6d4b\u8bd5\u51c6\u786e\u738799.89%\uff0cCohen Kappa 0.9974\uff0cAUC 0.9993\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u5feb\u3002", "conclusion": "StackLiverNet\u6027\u80fd\u4f18\u8d8a\uff0c\u9002\u5408\u4e34\u5e8a\u5b9e\u8df5\uff0c\u4e14\u901a\u8fc7LIME\u548cSHAP\u63d0\u4f9b\u900f\u660e\u89e3\u91ca\u3002"}}
{"id": "2508.00217", "categories": ["cs.CL", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00217", "abs": "https://arxiv.org/abs/2508.00217", "authors": ["Xiaofeng Wu", "Alan Ritter", "Wei Xu"], "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges", "comment": null, "summary": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8868\u683c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u8868\u683c\u7406\u89e3\u7684\u5206\u7c7b\u548c\u4efb\u52a1\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u8868\u683c\u56e0\u5176\u590d\u6742\u548c\u7075\u6d3b\u7684\u7ed3\u6784\u5728LLMs\u548cMLLMs\u4e2d\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u901a\u7528\u65b9\u6cd5\uff0c\u5bfc\u81f4\u7406\u89e3\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u8868\u683c\u8f93\u5165\u8868\u793a\u548c\u4ecb\u7ecd\u8868\u683c\u7406\u89e3\u4efb\u52a1\uff0c\u63d0\u51fa\u5173\u952e\u6982\u5ff5\u3002", "result": "\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u68c0\u7d22\u4efb\u52a1\u4e3b\u5bfc\u3001\u6a21\u578b\u5904\u7406\u590d\u6742\u8868\u683c\u7684\u56f0\u96be\u4ee5\u53ca\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3\u8868\u683c\u7406\u89e3\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u8868\u683c\u5904\u7406\u548c\u591a\u8868\u683c\u573a\u666f\u4e2d\u3002"}}
{"id": "2508.00143", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00143", "abs": "https://arxiv.org/abs/2508.00143", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Kenneth R. Koedinger"], "title": "Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation", "comment": "Accepted for presentation at NCME AIME-Con 2025", "summary": "Humans can be notoriously imperfect evaluators. They are often biased,\nunreliable, and unfit to define \"ground truth.\" Yet, given the surging need to\nproduce large amounts of training data in educational applications using AI,\ntraditional inter-rater reliability (IRR) metrics like Cohen's kappa remain\ncentral to validating labeled data. IRR remains a cornerstone of many machine\nlearning pipelines for educational data. Take, for example, the classification\nof tutors' moves in dialogues or labeling open responses in machine-graded\nassessments. This position paper argues that overreliance on human IRR as a\ngatekeeper for annotation quality hampers progress in classifying data in ways\nthat are valid and predictive in relation to improving learning. To address\nthis issue, we highlight five examples of complementary evaluation methods,\nsuch as multi-label annotation schemes, expert-based approaches, and\nclose-the-loop validity. We argue that these approaches are in a better\nposition to produce training data and subsequent models that produce improved\nstudent learning and more actionable insights than IRR approaches alone. We\nalso emphasize the importance of external validity, for example, by\nestablishing a procedure of validating tutor moves and demonstrating that it\nworks across many categories of tutor actions (e.g., providing hints). We call\non the field to rethink annotation quality and ground truth--prioritizing\nvalidity and educational impact over consensus alone.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u4f20\u7edf\u7684\u4eba\u7c7b\u8bc4\u5206\u4e00\u81f4\u6027\uff08IRR\uff09\u4f5c\u4e3a\u6807\u6ce8\u8d28\u91cf\u7684\u552f\u4e00\u6807\u51c6\u5b58\u5728\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e94\u79cd\u8865\u5145\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6559\u80b2AI\u6570\u636e\u7684\u6709\u6548\u6027\u548c\u9884\u6d4b\u6027\u3002", "motivation": "\u4eba\u7c7b\u8bc4\u4f30\u8005\u5b58\u5728\u504f\u89c1\u548c\u4e0d\u53ef\u9760\u6027\uff0c\u4f20\u7edfIRR\u6307\u6807\u65e0\u6cd5\u5b8c\u5168\u4fdd\u8bc1\u6807\u6ce8\u6570\u636e\u7684\u8d28\u91cf\uff0c\u5f71\u54cd\u4e86\u6559\u80b2AI\u6a21\u578b\u7684\u8fdb\u6b65\u3002", "method": "\u63d0\u51fa\u4e94\u79cd\u8865\u5145\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u591a\u6807\u7b7e\u6807\u6ce8\u65b9\u6848\u3001\u4e13\u5bb6\u8bc4\u4f30\u548c\u95ed\u73af\u9a8c\u8bc1\u7b49\uff0c\u5f3a\u8c03\u5916\u90e8\u6709\u6548\u6027\u7684\u91cd\u8981\u6027\u3002", "result": "\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\uff0c\u63d0\u5347\u5b66\u751f\u5b66\u4e60\u6548\u679c\u548c\u53ef\u64cd\u4f5c\u6027\u89c1\u89e3\u3002", "conclusion": "\u547c\u5401\u91cd\u65b0\u601d\u8003\u6807\u6ce8\u8d28\u91cf\u548c\u771f\u5b9e\u6807\u51c6\uff0c\u4f18\u5148\u8003\u8651\u6709\u6548\u6027\u548c\u6559\u80b2\u5f71\u54cd\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u5171\u8bc6\u3002"}}
{"id": "2508.00211", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00211", "abs": "https://arxiv.org/abs/2508.00211", "authors": ["Esen K. T\u00fct\u00fcnc\u00fc", "Mar Gonzalez-Franco", "Eric J. Gonzalez"], "title": "HandOver: Enabling Precise Selection & Manipulation of 3D Objects with Mouse and Hand Tracking", "comment": "11 pages, 10 figures", "summary": "We present HandOver, an extended reality (XR) interaction technique designed\nto unify the precision of traditional mouse input for object selection with the\nexpressiveness of hand-tracking for object manipulation. With HandOver, the\nmouse is used to drive a depth-aware 3D cursor enabling precise and restful\ntargeting -by hovering their hand over the mouse, the user can then seamlessly\ntransition into direct 3D manipulation of the target object. In a formal user\nstudy, we compare HandOver against two raybased techniques: traditional\nraycasting (Ray) and a hybrid method (Ray+Hand) in a 3D docking task. Results\nshow HandOver yields lower task errors across all distances, and moreover\nimproves interaction ergonomics as highlighted by a RULA posture analysis and\nself-reported measures (NASA-TLX). These findings illustrate the benefits of\nblending traditional precise input devices with the expressive gestural inputs\nafforded by hand-tracking in XR, leading to improved user comfort and task\nperformance. This blended paradigm yields a unified workflow allowing users to\nleverage the best of each input modality as they interact in immersive\nenvironments.", "AI": {"tldr": "HandOver\u662f\u4e00\u79cdXR\u4ea4\u4e92\u6280\u672f\uff0c\u7ed3\u5408\u9f20\u6807\u7684\u7cbe\u786e\u9009\u62e9\u548c\u624b\u90e8\u8ffd\u8e2a\u7684\u7075\u6d3b\u64cd\u4f5c\uff0c\u57283D\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5c04\u7ebf\u6280\u672f\u3002", "motivation": "\u89e3\u51b3XR\u73af\u5883\u4e2d\u7cbe\u786e\u9009\u62e9\u548c\u7075\u6d3b\u64cd\u4f5c\u7684\u7ed3\u5408\u95ee\u9898\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u4efb\u52a1\u6548\u7387\u3002", "method": "\u4f7f\u7528\u9f20\u6807\u9a71\u52a8\u6df1\u5ea6\u611f\u77e53D\u5149\u6807\u8fdb\u884c\u7cbe\u786e\u9009\u62e9\uff0c\u901a\u8fc7\u624b\u90e8\u60ac\u505c\u5207\u6362\u81f3\u76f4\u63a53D\u64cd\u4f5c\u3002", "result": "HandOver\u57283D\u4efb\u52a1\u4e2d\u8bef\u5dee\u66f4\u4f4e\uff0c\u4ea4\u4e92\u8212\u9002\u5ea6\u66f4\u9ad8\uff0c\u7528\u6237\u53cd\u9988\u66f4\u4f18\u3002", "conclusion": "\u7ed3\u5408\u4f20\u7edf\u8f93\u5165\u8bbe\u5907\u548c\u624b\u90e8\u8ffd\u8e2a\u6280\u672f\u80fd\u63d0\u5347XR\u4ea4\u4e92\u7684\u8212\u9002\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.00127", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00127", "abs": "https://arxiv.org/abs/2508.00127", "authors": ["Saleh Nikooroo", "Thomas Engel"], "title": "Structured Transformations for Stable and Interpretable Neural Computation", "comment": null, "summary": "Despite their impressive performance, contemporary neural networks often lack\nstructural safeguards that promote stable learning and interpretable behavior.\nIn this work, we introduce a reformulation of layer-level transformations that\ndeparts from the standard unconstrained affine paradigm. Each transformation is\ndecomposed into a structured linear operator and a residual corrective\ncomponent, enabling more disciplined signal propagation and improved training\ndynamics. Our formulation encourages internal consistency and supports stable\ninformation flow across depth, while remaining fully compatible with standard\nlearning objectives and backpropagation. Through a series of synthetic and\nreal-world experiments, we demonstrate that models constructed with these\nstructured transformations exhibit improved gradient conditioning, reduced\nsensitivity to perturbations, and layer-wise robustness. We further show that\nthese benefits persist across architectural scales and training regimes. This\nstudy serves as a foundation for a more principled class of neural\narchitectures that prioritize stability and transparency-offering new tools for\nreasoning about learning behavior without sacrificing expressive power.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u5c42\u53d8\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u7ebf\u6027\u64cd\u4f5c\u548c\u6b8b\u5dee\u6821\u6b63\u7ec4\u4ef6\uff0c\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u4ee3\u795e\u7ecf\u7f51\u7edc\u7f3a\u4e4f\u7ed3\u6784\u4fdd\u969c\uff0c\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u884c\u4e3a\u96be\u4ee5\u89e3\u91ca\u3002", "method": "\u5c06\u5c42\u53d8\u6362\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u7ebf\u6027\u64cd\u4f5c\u548c\u6b8b\u5dee\u6821\u6b63\u7ec4\u4ef6\uff0c\u652f\u6301\u7a33\u5b9a\u7684\u4fe1\u606f\u6d41\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6539\u5584\u4e86\u68af\u5ea6\u6761\u4ef6\u3001\u964d\u4f4e\u4e86\u5bf9\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u5c42\u95f4\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u66f4\u7a33\u5b9a\u548c\u900f\u660e\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2508.00220", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00220", "abs": "https://arxiv.org/abs/2508.00220", "authors": ["Rana Aref Salama", "Abdou Youssef", "Mona Diab"], "title": "Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform", "comment": null, "summary": "Wavelet transforms, a powerful mathematical tool, have been widely used in\ndifferent domains, including Signal and Image processing, to unravel intricate\npatterns, enhance data representation, and extract meaningful features from\ndata. Tangible results from their application suggest that Wavelet transforms\ncan be applied to NLP capturing a variety of linguistic and semantic\nproperties. In this paper, we empirically leverage the application of Discrete\nWavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase\nthe capabilities of DWT in analyzing embedding representations at different\nlevels of resolution and compressing them while maintaining their overall\nquality. We assess the effectiveness of DWT embeddings on semantic similarity\ntasks to show how DWT can be used to consolidate important semantic information\nin an embedding vector. We show the efficacy of the proposed paradigm using\ndifferent embedding models, including large language models, on downstream\ntasks. Our results show that DWT can reduce the dimensionality of embeddings by\n50-93% with almost no change in performance for semantic similarity tasks,\nwhile achieving superior accuracy in most downstream tasks. Our findings pave\nthe way for applying DWT to improve NLP applications.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\uff08DWT\uff09\u5728\u8bcd\u548c\u53e5\u5b50\u5d4c\u5165\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u5206\u8fa8\u7387\u5206\u6790\u548c\u538b\u7f29\u5d4c\u5165\u8868\u793a\u65b9\u9762\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fe1\u606f\u3002", "motivation": "\u5c0f\u6ce2\u53d8\u6362\u5728\u4fe1\u53f7\u548c\u56fe\u50cf\u5904\u7406\u4e2d\u5df2\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1DWT\u5728NLP\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u5c06DWT\u5e94\u7528\u4e8e\u8bcd\u548c\u53e5\u5b50\u5d4c\u5165\uff0c\u5206\u6790\u5176\u5728\u591a\u5206\u8fa8\u7387\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "result": "DWT\u80fd\u5c06\u5d4c\u5165\u7ef4\u5ea6\u51cf\u5c1150-93%\uff0c\u4e14\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\u6027\u80fd\u51e0\u4e4e\u4e0d\u53d8\uff0c\u591a\u6570\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "DWT\u4e3a\u6539\u8fdbNLP\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5d4c\u5165\u538b\u7f29\u548c\u8bed\u4e49\u4fe1\u606f\u4fdd\u7559\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00159", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.TH", "math.OC", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2508.00159", "abs": "https://arxiv.org/abs/2508.00159", "authors": ["Jobst Heitzig", "Ram Potham"], "title": "Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power", "comment": null, "summary": "Power is a key concept in AI safety: power-seeking as an instrumental goal,\nsudden or gradual disempowerment of humans, power balance in human-AI\ninteraction and international AI governance. At the same time, power as the\nability to pursue diverse goals is essential for wellbeing.\n  This paper explores the idea of promoting both safety and wellbeing by\nforcing AI agents explicitly to empower humans and to manage the power balance\nbetween humans and AI agents in a desirable way. Using a principled, partially\naxiomatic approach, we design a parametrizable and decomposable objective\nfunction that represents an inequality- and risk-averse long-term aggregate of\nhuman power. It takes into account humans' bounded rationality and social\nnorms, and, crucially, considers a wide variety of possible human goals.\n  We derive algorithms for computing that metric by backward induction or\napproximating it via a form of multi-agent reinforcement learning from a given\nworld model. We exemplify the consequences of (softly) maximizing this metric\nin a variety of paradigmatic situations and describe what instrumental\nsub-goals it will likely imply. Our cautious assessment is that softly\nmaximizing suitable aggregate metrics of human power might constitute a\nbeneficial objective for agentic AI systems that is safer than direct\nutility-based objectives.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u901a\u8fc7\u8bbe\u8ba1\u76ee\u6807\u51fd\u6570\uff0c\u4f7fAI\u7cfb\u7edf\u660e\u786e\u8d4b\u80fd\u4eba\u7c7b\u5e76\u7ba1\u7406\u4eba\u673a\u6743\u529b\u5e73\u8861\uff0c\u4ee5\u4fc3\u8fdb\u5b89\u5168\u548c\u798f\u7949\u3002", "motivation": "\u6743\u529b\u662fAI\u5b89\u5168\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u540c\u65f6\u6743\u529b\u4f5c\u4e3a\u8ffd\u6c42\u591a\u6837\u76ee\u6807\u7684\u80fd\u529b\u5bf9\u798f\u7949\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ba1\u7406\u6743\u529b\u5e73\u8861\u63d0\u5347\u5b89\u5168\u4e0e\u798f\u7949\u3002", "method": "\u91c7\u7528\u90e8\u5206\u516c\u7406\u5316\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u53ef\u53c2\u6570\u5316\u548c\u5206\u89e3\u7684\u76ee\u6807\u51fd\u6570\uff0c\u8003\u8651\u4eba\u7c7b\u6709\u9650\u7406\u6027\u3001\u793e\u4f1a\u89c4\u8303\u53ca\u591a\u6837\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u9006\u5411\u5f52\u7eb3\u6216\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8ba1\u7b97\u3002", "result": "\u5728\u591a\u79cd\u60c5\u5883\u4e0b\u5c55\u793a\u4e86\u6700\u5927\u5316\u4eba\u7c7b\u6743\u529b\u6307\u6807\u7684\u540e\u679c\u53ca\u6f5c\u5728\u5b50\u76ee\u6807\uff0c\u8868\u660e\u5176\u53ef\u80fd\u6bd4\u76f4\u63a5\u57fa\u4e8e\u6548\u7528\u7684\u76ee\u6807\u66f4\u5b89\u5168\u3002", "conclusion": "\u8f6f\u6700\u5927\u5316\u5408\u9002\u7684\u4eba\u7c7b\u6743\u529b\u6307\u6807\u53ef\u80fd\u662f\u6bd4\u76f4\u63a5\u6548\u7528\u76ee\u6807\u66f4\u5b89\u5168\u7684AI\u7cfb\u7edf\u76ee\u6807\u3002"}}
{"id": "2508.00233", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00233", "abs": "https://arxiv.org/abs/2508.00233", "authors": ["Douglas Markant", "Subham Sah", "Alireza Karduni", "Milad Rogha", "My Thai", "Wenwen Dou"], "title": "Correcting Misperceptions at a Glance: Using Data Visualizations to Reduce Political Sectarianism", "comment": "11 pages, 5 figures. IEEE VIS 2025", "summary": "Political sectarianism is fueled in part by misperceptions of political\nopponents: People commonly overestimate the support for extreme policies among\nmembers of the other party. Research suggests that correcting partisan\nmisperceptions by informing people about the actual views of outparty members\nmay reduce one's own expressed support for political extremism, including\npartisan violence and anti-democratic actions. The present study investigated\nhow correction effects depend on different representations of outparty views\ncommunicated through data visualizations. We conducted an experiment with U.S.\nbased participants from Prolific (N=239 Democrats, N=244 Republicans).\nParticipants made predictions about support for political violence and\nundemocratic practices among members of their political outparty. They were\nthen presented with data from an earlier survey on the actual views of outparty\nmembers. Some participants viewed only the average response (Mean-Only\ncondition), while other groups were shown visual representations of the range\nof views from 75% of the outparty (Mean+Interval condition) or the full\ndistribution of responses (Mean+Points condition). Compared to a control group\nthat was not informed about outparty views, we observed the strongest\ncorrection effects among participants in the Mean-only and Mean+Points\ncondition, while correction effects were weaker in the Mean+Interval condition.\nIn addition, participants who observed the full distribution of out-party views\n(Mean+Points condition) were most accurate at later recalling the degree of\nsupport among the outparty. Our findings suggest that data visualizations can\nbe an important tool for correcting pervasive distortions in beliefs about\nother groups. However, the way in which variability in outparty views is\nvisualized can significantly shape how people interpret and respond to\ncorrective information.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6570\u636e\u53ef\u89c6\u5316\u53ef\u4ee5\u7ea0\u6b63\u5bf9\u653f\u6cbb\u5bf9\u624b\u7684\u8bef\u89e3\uff0c\u4f46\u53ef\u89c6\u5316\u65b9\u5f0f\uff08\u5982\u5e73\u5747\u503c\u3001\u533a\u95f4\u6216\u5b8c\u6574\u5206\u5e03\uff09\u4f1a\u5f71\u54cd\u7ea0\u6b63\u6548\u679c\u3002", "motivation": "\u63a2\u7a76\u5982\u4f55\u901a\u8fc7\u6570\u636e\u53ef\u89c6\u5316\u7ea0\u6b63\u5bf9\u653f\u6cbb\u5bf9\u624b\u7684\u6781\u7aef\u653f\u7b56\u652f\u6301\u5ea6\u7684\u8bef\u89e3\uff0c\u4ee5\u51cf\u5c11\u653f\u6cbb\u6781\u7aef\u4e3b\u4e49\u3002", "method": "\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u6bd4\u8f83\u4e0d\u540c\u6570\u636e\u53ef\u89c6\u5316\u65b9\u5f0f\uff08\u4ec5\u5e73\u5747\u503c\u3001\u5e73\u5747\u503c\u52a0\u533a\u95f4\u3001\u5e73\u5747\u503c\u52a0\u5b8c\u6574\u5206\u5e03\uff09\u5bf9\u7ea0\u6b63\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u4ec5\u5e73\u5747\u503c\u548c\u5b8c\u6574\u5206\u5e03\u7684\u53ef\u89c6\u5316\u7ea0\u6b63\u6548\u679c\u6700\u5f3a\uff0c\u800c\u533a\u95f4\u53ef\u89c6\u5316\u6548\u679c\u8f83\u5f31\u3002\u5b8c\u6574\u5206\u5e03\u7ec4\u5bf9\u5bf9\u624b\u89c2\u70b9\u7684\u56de\u5fc6\u6700\u51c6\u786e\u3002", "conclusion": "\u6570\u636e\u53ef\u89c6\u5316\u662f\u7ea0\u6b63\u7fa4\u4f53\u8bef\u89e3\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u53ef\u89c6\u5316\u65b9\u5f0f\u4f1a\u5f71\u54cd\u4fe1\u606f\u89e3\u8bfb\u548c\u54cd\u5e94\u3002"}}
{"id": "2508.00131", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00131", "abs": "https://arxiv.org/abs/2508.00131", "authors": ["Christopher Harvey", "Sumaiya Shomaji", "Zijun Yao", "Amit Noheria"], "title": "ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks", "comment": "arXiv admin note: substantial text overlap with arXiv:2410.02937", "summary": "The electrocardiogram (ECG) is an inexpensive and widely available tool for\ncardiac assessment. Despite its standardized format and small file size, the\nhigh complexity and inter-individual variability of ECG signals (typically a\n60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep\nlearning models, especially when only small training datasets are available.\nThis study addresses these challenges by exploring feature generation methods\nfrom representative beat ECGs, focusing on Principal Component Analysis (PCA)\nand Autoencoders to reduce data complexity. We introduce three novel\nVariational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed\nbeta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their\neffectiveness in maintaining signal fidelity and enhancing downstream\nprediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE\nachieved superior signal reconstruction, reducing the mean absolute error (MAE)\nto 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE\nencodings, when combined with traditional ECG summary features, improved the\nprediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an\nholdout test set area under the receiver operating characteristic curve (AUROC)\nof 0.901 with a LGBM classifier. This performance nearly matches the 0.909\nAUROC of state-of-the-art CNN model but requires significantly less\ncomputational resources. Further, the ECG feature extraction-LGBM pipeline\navoids overfitting and retains predictive performance when trained with less\ndata. Our findings demonstrate that these VAE encodings are not only effective\nin simplifying ECG data but also provide a practical solution for applying deep\nlearning in contexts with limited-scale labeled training data.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528PCA\u548cAutoencoder\u7b80\u5316ECG\u4fe1\u53f7\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cdVAE\u53d8\u4f53\uff08SAE\u3001A beta-VAE\u3001C beta-VAE\uff09\uff0c\u5176\u4e2dA beta-VAE\u5728\u4fe1\u53f7\u91cd\u5efa\u4e0a\u8868\u73b0\u6700\u4f73\uff0cSAE\u7f16\u7801\u7ed3\u5408\u4f20\u7edf\u7279\u5f81\u63d0\u5347\u4e86LVEF\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3ECG\u4fe1\u53f7\u9ad8\u590d\u6742\u6027\u548c\u4e2a\u4f53\u5dee\u5f02\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5c0f\u6570\u636e\u96c6\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528PCA\u548cAutoencoder\uff08\u5305\u62ec\u4e09\u79cdVAE\u53d8\u4f53\uff09\u7b80\u5316ECG\u6570\u636e\uff0c\u5e76\u7ed3\u5408LGBM\u8fdb\u884c\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\u3002", "result": "A beta-VAE\u5c06MAE\u964d\u81f315.7\u00b13.2 \u03bcV\uff0cSAE\u7f16\u7801\u7ed3\u5408\u4f20\u7edf\u7279\u5f81\u5728LVEF\u9884\u6d4b\u4e2dAUROC\u8fbe0.901\uff0c\u63a5\u8fd1CNN\u6a21\u578b\u4f46\u8ba1\u7b97\u8d44\u6e90\u66f4\u5c11\u3002", "conclusion": "VAE\u7f16\u7801\u80fd\u6709\u6548\u7b80\u5316ECG\u6570\u636e\uff0c\u4e3a\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00238", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.00238", "abs": "https://arxiv.org/abs/2508.00238", "authors": ["Bryce Anderson", "Riley Galpin", "Tom S. Juzek"], "title": "Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English", "comment": "Accepted at AIES 2025. To appear in the AIES Proceedings. 14 pages, 2\n  figures, 2 tables. Licensed under CC BY-SA 4.0", "summary": "In recent years, written language, particularly in science and education, has\nundergone remarkable shifts in word usage. These changes are widely attributed\nto the growing influence of Large Language Models (LLMs), which frequently rely\non a distinct lexical style. Divergences between model output and target\naudience norms can be viewed as a form of misalignment. While these shifts are\noften linked to using Artificial Intelligence (AI) directly as a tool to\ngenerate text, it remains unclear whether the changes reflect broader changes\nin the human language system itself. To explore this question, we constructed a\ndataset of 22.1 million words from unscripted spoken language drawn from\nconversational science and technology podcasts. We analyzed lexical trends\nbefore and after ChatGPT's release in 2022, focusing on commonly LLM-associated\nwords. Our results show a moderate yet significant increase in the usage of\nthese words post-2022, suggesting a convergence between human word choices and\nLLM-associated patterns. In contrast, baseline synonym words exhibit no\nsignificant directional shift. Given the short time frame and the number of\nwords affected, this may indicate the onset of a remarkable shift in language\nuse. Whether this represents natural language change or a novel shift driven by\nAI exposure remains an open question. Similarly, although the shifts may stem\nfrom broader adoption patterns, it may also be that upstream training\nmisalignments ultimately contribute to changes in human language use. These\nfindings parallel ethical concerns that misaligned models may shape social and\nmoral beliefs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u4eba\u7c7b\u8bed\u8a00\u4f7f\u7528\u7684\u5f71\u54cd\uff0c\u53d1\u73b02022\u5e74\u540e\u4eba\u7c7b\u53e3\u8bed\u4e2d\u4e0eLLM\u76f8\u5173\u7684\u8bcd\u6c47\u4f7f\u7528\u663e\u8457\u589e\u52a0\u3002", "motivation": "\u63a2\u7a76LLM\u662f\u5426\u6b63\u5728\u6539\u53d8\u4eba\u7c7b\u8bed\u8a00\u7cfb\u7edf\u672c\u8eab\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4f5c\u4e3a\u6587\u672c\u751f\u6210\u5de5\u5177\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2210\u4e07\u5355\u8bcd\u7684\u6570\u636e\u96c6\uff0c\u5206\u6790ChatGPT\u53d1\u5e03\u524d\u540e\u8bcd\u6c47\u4f7f\u7528\u8d8b\u52bf\u3002", "result": "2022\u5e74\u540e\uff0c\u4e0eLLM\u76f8\u5173\u7684\u8bcd\u6c47\u4f7f\u7528\u663e\u8457\u589e\u52a0\uff0c\u800c\u57fa\u7ebf\u540c\u4e49\u8bcd\u65e0\u660e\u663e\u53d8\u5316\u3002", "conclusion": "LLM\u53ef\u80fd\u6b63\u5728\u5f15\u53d1\u4eba\u7c7b\u8bed\u8a00\u4f7f\u7528\u7684\u663e\u8457\u53d8\u5316\uff0c\u4f46\u8fd9\u662f\u81ea\u7136\u8bed\u8a00\u6f14\u53d8\u8fd8\u662fAI\u9a71\u52a8\u7684\u53d8\u5316\u5c1a\u4e0d\u660e\u786e\u3002"}}
{"id": "2508.00222", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00222", "abs": "https://arxiv.org/abs/2508.00222", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "AI": {"tldr": "RL-PLUS\u901a\u8fc7\u7ed3\u5408\u5185\u90e8\u63a2\u7d22\u548c\u5916\u90e8\u6570\u636e\uff0c\u89e3\u51b3\u4e86RLVR\u7684\u80fd\u529b\u8fb9\u754c\u95ee\u9898\uff0c\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "RLVR\u56e0\u7b56\u7565\u9650\u5236\u548c\u7a00\u758f\u5956\u52b1\u96be\u4ee5\u7a81\u7834\u57fa\u7840LLM\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u751a\u81f3\u5bfc\u81f4\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u3002", "method": "RL-PLUS\u7ed3\u5408\u591a\u91cd\u91cd\u8981\u6027\u91c7\u6837\u548c\u63a2\u7d22\u4f18\u52bf\u51fd\u6570\uff0c\u4f18\u5316\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u63d0\u534721.1%\u81f369.2%\u3002", "conclusion": "RL-PLUS\u6709\u6548\u89e3\u51b3\u4e86\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.00239", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00239", "abs": "https://arxiv.org/abs/2508.00239", "authors": ["Jacqueline Elise Bruen", "Myounghoon Jeon"], "title": "What's Behind the Magic? Audiences Seek Artistic Value in Generative AI's Contributions to a Live Dance Performance", "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "summary": "With the development of generative artificial intelligence (GenAI) tools to\ncreate art, stakeholders cannot come to an agreement on the value of these\nworks. In this study we uncovered the mixed opinions surrounding art made by\nAI. We developed two versions of a dance performance augmented by technology\neither with or without GenAI. For each version we informed audiences of the\nperformance's development either before or after a survey on their perceptions\nof the performance. There were thirty-nine participants (13 males, 26 female)\ndivided between the four performances. Results demonstrated that individuals\nwere more inclined to attribute artistic merit to works made by GenAI when they\nwere unaware of its use. We present this case study as a call to address the\nimportance of utilizing the social context and the users' interpretations of\nGenAI in shaping a technical explanation, leading to a greater discussion that\ncan bridge gaps in understanding.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u89c2\u4f17\u4e0d\u77e5\u9053\u4f5c\u54c1\u4f7f\u7528\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u65f6\uff0c\u66f4\u5bb9\u6613\u8ba4\u53ef\u5176\u827a\u672f\u4ef7\u503c\u3002", "motivation": "\u63a2\u8ba8\u4eba\u4eec\u5bf9AI\u521b\u4f5c\u827a\u672f\u7684\u770b\u6cd5\u5206\u6b67\uff0c\u4ee5\u53ca\u6280\u672f\u80cc\u666f\u4fe1\u606f\u5982\u4f55\u5f71\u54cd\u827a\u672f\u8bc4\u4ef7\u3002", "method": "\u5f00\u53d1\u4e24\u79cd\u821e\u8e48\u8868\u6f14\u7248\u672c\uff08\u4f7f\u7528\u6216\u4e0d\u4f7f\u7528GenAI\uff09\uff0c\u5e76\u5728\u89c2\u4f17\u89c2\u770b\u524d\u540e\u544a\u77e5\u6280\u672f\u80cc\u666f\uff0c\u8c03\u67e539\u540d\u53c2\u4e0e\u8005\u7684\u770b\u6cd5\u3002", "result": "\u89c2\u4f17\u5728\u4e0d\u77e5\u60c5\u65f6\u66f4\u503e\u5411\u4e8e\u8ba4\u53efGenAI\u4f5c\u54c1\u7684\u827a\u672f\u4ef7\u503c\u3002", "conclusion": "\u5f3a\u8c03\u793e\u4f1a\u80cc\u666f\u548c\u7528\u6237\u5bf9GenAI\u7684\u7406\u89e3\u5728\u6280\u672f\u89e3\u91ca\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u5f25\u5408\u8ba4\u77e5\u5dee\u8ddd\u3002"}}
{"id": "2508.00141", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00141", "abs": "https://arxiv.org/abs/2508.00141", "authors": ["Mohit Gupta", "Debjit Bhowmick", "Rhys Newbury", "Meead Saberi", "Shirui Pan", "Ben Beck"], "title": "INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks", "comment": null, "summary": "Accurate link-level bicycling volume estimation is essential for sustainable\nurban transportation planning. However, many cities face significant challenges\nof high data sparsity due to limited bicycling count sensor coverage. To\naddress this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning\n(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize\nsensor placement and improve link-level bicycling volume estimation in\ndata-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks\n(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL\nagent, enabling a data-driven strategic selection of sensor locations to\nmaximize estimation performance. Applied to Melbourne's bicycling network,\ncomprising 15,933 road segments with sensor coverage on only 141 road segments\n(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume\nestimation by strategically selecting additional sensor locations in\ndeployments of 50, 100, 200 and 500 sensors. Our framework outperforms\ntraditional heuristic methods for sensor placement such as betweenness\ncentrality, closeness centrality, observed bicycling activity and random\nplacement, across key metrics such as Mean Squared Error (MSE), Root Mean\nSquared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our\nexperiments benchmark INSPIRE-GNN against standard machine learning and deep\nlearning models in the bicycle volume estimation performance, underscoring its\neffectiveness. Our proposed framework provides transport planners actionable\ninsights to effectively expand sensor networks, optimize sensor placement and\nmaximize volume estimation accuracy and reliability of bicycling data for\ninformed transportation planning decisions.", "AI": {"tldr": "INSPIRE-GNN\u662f\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u7a00\u758f\u6570\u636e\u73af\u5883\u4e2d\u7684\u81ea\u884c\u8f66\u6d41\u91cf\u4f30\u8ba1\u548c\u4f20\u611f\u5668\u5e03\u5c40\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u81ea\u884c\u8f66\u6d41\u91cf\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4ea4\u901a\u89c4\u5212\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u3001\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u548c\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u4f18\u5316\u4f20\u611f\u5668\u5e03\u5c40\u3002", "result": "\u5728\u58a8\u5c14\u672c\u81ea\u884c\u8f66\u7f51\u7edc\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d41\u91cf\u4f30\u8ba1\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "INSPIRE-GNN\u4e3a\u4ea4\u901a\u89c4\u5212\u8005\u63d0\u4f9b\u4e86\u6709\u6548\u6269\u5c55\u4f20\u611f\u5668\u7f51\u7edc\u548c\u4f18\u5316\u5e03\u5c40\u7684\u5de5\u5177\uff0c\u63d0\u5347\u4e86\u6570\u636e\u51c6\u786e\u6027\u548c\u51b3\u7b56\u53ef\u9760\u6027\u3002"}}
{"id": "2508.00285", "categories": ["cs.CL", "I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2508.00285", "abs": "https://arxiv.org/abs/2508.00285", "authors": ["Peixian Li", "Yu Tian", "Ruiqi Tu", "Chengkai Wu", "Jingjing Ren", "Jingsong Li"], "title": "Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering", "comment": "23 pages, 8 figures", "summary": "Objective: Large Language Models (LLMs) demonstrate significant capabilities\nin medical text understanding and generation. However, their diagnostic\nreliability in complex clinical scenarios remains limited. This study aims to\nenhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We\npropose an Etiology-Aware Attention Steering Framework to integrate structured\nclinical reasoning into LLM-based diagnosis. Specifically, we first construct\nClinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines\nfor three representative acute abdominal emergencies: acute appendicitis, acute\npancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head\nIdentification algorithm to pinpoint attention heads crucial for the model's\netiology reasoning. To ensure reliable clinical reasoning alignment, we\nintroduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds\netiological reasoning cues into input representations and steers the selected\nEtiology-Aware Heads toward critical information through a Reasoning-Guided\nLoss function. Result: On the Consistent Diagnosis Cohort, our framework\nimproves average diagnostic accuracy by 15.65% and boosts the average Reasoning\nFocus Score by 31.6% over baselines. External validation on the Discrepant\nDiagnosis Cohort further confirms its effectiveness in enhancing diagnostic\naccuracy. Further assessments via Reasoning Attention Frequency indicate that\nour models exhibit enhanced reliability when faced with real-world complex\nscenarios. Conclusion: This study presents a practical and effective approach\nto enhance clinical reasoning in LLM-based diagnosis. By aligning model\nattention with structured CRS, the proposed framework offers a promising\nparadigm for building more interpretable and reliable AI diagnostic systems in\ncomplex clinical settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u75c5\u56e0\u611f\u77e5\u6ce8\u610f\u529b\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4e34\u5e8a\u63a8\u7406\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u590d\u6742\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u8bca\u65ad\u53ef\u9760\u6027\u4ecd\u6709\u9650\u3002\u7814\u7a76\u65e8\u5728\u63d0\u5347\u5176\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u6743\u5a01\u4e34\u5e8a\u6307\u5357\u7684\u4e34\u5e8a\u63a8\u7406\u652f\u67b6\uff08CRS\uff09\uff0c\u5f00\u53d1\u75c5\u56e0\u611f\u77e5\u5934\u8bc6\u522b\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u63a8\u7406\u5f15\u5bfc\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5c06\u75c5\u56e0\u63a8\u7406\u7ebf\u7d22\u5d4c\u5165\u8f93\u5165\u8868\u793a\u3002", "result": "\u5728\u4e00\u81f4\u6027\u8bca\u65ad\u961f\u5217\u4e2d\uff0c\u6846\u67b6\u5c06\u5e73\u5747\u8bca\u65ad\u51c6\u786e\u7387\u63d0\u534715.65%\uff0c\u63a8\u7406\u805a\u7126\u5206\u6570\u63d0\u534731.6%\u3002\u5916\u90e8\u9a8c\u8bc1\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u6a21\u578b\u6ce8\u610f\u529b\u4e0e\u7ed3\u6784\u5316CRS\u5bf9\u9f50\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u9760\u7684AI\u8bca\u65ad\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2508.00271", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.00271", "abs": "https://arxiv.org/abs/2508.00271", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning", "comment": "Technical Report, 14 pages", "summary": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.", "AI": {"tldr": "MetaAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u5b9e\u8df5\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u901a\u8fc7\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u548c\u5de5\u5177\u5b66\u4e60\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u901a\u8fc7\u5b9e\u8df5\u548c\u81ea\u6211\u53cd\u601d\u4e0d\u65ad\u6539\u8fdb\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u4ee5\u89e3\u51b3\u77e5\u8bc6\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002", "method": "MetaAgent\u4ece\u57fa\u7840\u80fd\u529b\u51fa\u53d1\uff0c\u901a\u8fc7\u751f\u6210\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\u3001\u5de5\u5177\u8def\u7531\u3001\u81ea\u6211\u53cd\u601d\u548c\u77e5\u8bc6\u5e93\u6784\u5efa\u9010\u6b65\u63d0\u5347\u80fd\u529b\u3002", "result": "\u5728GAIA\u3001WebWalkerQA\u548cBrowseCamp\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMetaAgent\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5339\u914d\u6216\u8d85\u8d8a\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u4ee3\u7406\u3002", "conclusion": "MetaAgent\u5c55\u793a\u4e86\u81ea\u8fdb\u5316\u4ee3\u7406\u7cfb\u7edf\u5728\u901a\u7528\u77e5\u8bc6\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u65e0\u9700\u8c03\u6574\u6a21\u578b\u53c2\u6570\u6216\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2508.00252", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00252", "abs": "https://arxiv.org/abs/2508.00252", "authors": ["Wataru Kawabe", "Hiroto Fukuda", "Akihisa Shitara", "Yuri Nakao", "Yusuke Sugano"], "title": "TofuML: A Spatio-Physical Interactive Machine Learning Device for Interactive Exploration of Machine Learning for Novices", "comment": "31 pages", "summary": "We introduce TofuML, an interactive system designed to make machine learning\n(ML) concepts more accessible and engaging for non-expert users. Unlike\nconventional GUI-based systems, TofuML employs a physical and spatial interface\nconsisting of a small device and a paper mat, allowing users to train and\nevaluate sound classification models through intuitive, toy-like interactions.\nThrough two user studies -- a comparative study against a GUI-based version and\na public event deployment -- we investigated how TofuML impacts users'\nengagement in the ML model creation process, their ability to provide\nappropriate training data, and their conception of potential applications. Our\nresults indicated that TofuML enhanced user engagement compared to a GUI while\nlowering barriers for non-experts to engage with ML. Users demonstrated\ncreativity in conceiving diverse ML applications, revealing opportunities to\noptimize between conceptual understanding and user engagement. These findings\ncontribute to developing interactive ML systems/frameworks designed for a wide\nrange of users.", "AI": {"tldr": "TofuML\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u901a\u8fc7\u7269\u7406\u548c\u7a7a\u95f4\u754c\u9762\u5e2e\u52a9\u975e\u4e13\u5bb6\u7528\u6237\u66f4\u76f4\u89c2\u5730\u5b66\u4e60\u548c\u53c2\u4e0e\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u3002", "motivation": "\u4f20\u7edfGUI\u7cfb\u7edf\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u4e0d\u591f\u53cb\u597d\uff0cTofuML\u65e8\u5728\u901a\u8fc7\u76f4\u89c2\u7684\u4ea4\u4e92\u65b9\u5f0f\u964d\u4f4eML\u7684\u5b66\u4e60\u95e8\u69db\u3002", "method": "TofuML\u91c7\u7528\u5c0f\u578b\u8bbe\u5907\u548c\u7eb8\u8d28\u57ab\u7684\u7269\u7406\u754c\u9762\uff0c\u7528\u6237\u901a\u8fc7\u73a9\u5177\u5f0f\u4ea4\u4e92\u8bad\u7ec3\u548c\u8bc4\u4f30\u58f0\u97f3\u5206\u7c7b\u6a21\u578b\u3002\u901a\u8fc7\u4e24\u9879\u7528\u6237\u7814\u7a76\uff08\u4e0eGUI\u7248\u672c\u7684\u5bf9\u6bd4\u7814\u7a76\u548c\u516c\u5171\u6d3b\u52a8\u90e8\u7f72\uff09\u8bc4\u4f30\u6548\u679c\u3002", "result": "TofuML\u6bd4GUI\u66f4\u80fd\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u964d\u4f4e\u975e\u4e13\u5bb6\u7684\u5b66\u4e60\u969c\u788d\uff0c\u5e76\u6fc0\u53d1\u7528\u6237\u5bf9ML\u5e94\u7528\u7684\u521b\u610f\u3002", "conclusion": "TofuML\u4e3a\u5f00\u53d1\u9762\u5411\u5e7f\u6cdb\u7528\u6237\u7684\u4ea4\u4e92\u5f0fML\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e73\u8861\u4e86\u6982\u5ff5\u7406\u89e3\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2508.00161", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00161", "abs": "https://arxiv.org/abs/2508.00161", "authors": ["Ziqian Zhong", "Aditi Raghunathan"], "title": "Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs", "comment": null, "summary": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6743\u91cd\u800c\u975e\u6fc0\u6d3b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7406\u89e3\u548c\u76d1\u63a7\u5fae\u8c03\u540e\u7684LLM\uff0c\u65e0\u9700\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u7684\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6fc0\u6d3b\u7684\u89e3\u91ca\u65b9\u6cd5\u9700\u8981\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u7684\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5728\u68c0\u6d4b\u65b0\u5a01\u80c1\uff08\u5982\u540e\u95e8\uff09\u65f6\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5fae\u8c03\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u6743\u91cd\u5dee\u5f02\u7684\u9876\u90e8\u5947\u5f02\u5411\u91cf\uff0c\u76d1\u63a7\u6fc0\u6d3b\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4ee5\u68c0\u6d4b\u65b0\u884c\u4e3a\u3002", "result": "\u5728\u540e\u95e8\u6a21\u578b\u4e2d\u963b\u6b62\u4e86100%\u653b\u51fb\uff08\u5047\u9633\u6027\u7387<1.2%\uff09\uff0c\u5728\u9057\u5fd8\u6a21\u578b\u4e2d\u68c0\u6d4b\u523095.42%\u7684\u64e6\u9664\u4e3b\u9898\u63a8\u7406\uff0c\u5e76\u80fd\u6062\u590d\u201c\u9057\u5fd8\u201d\u4fe1\u606f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u76d1\u63a7\u548c\u9884\u90e8\u7f72\u5ba1\u8ba1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u53ef\u63ed\u793a\u5546\u4e1a\u6a21\u578b\u7684\u5fae\u8c03\u91cd\u70b9\u3002"}}
{"id": "2508.00305", "categories": ["cs.CL", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.00305", "abs": "https://arxiv.org/abs/2508.00305", "authors": ["Ammar Ahmed", "Sheng Di", "Franck Cappello", "Zirui Liu", "Jingoo Han", "Ali Anwar"], "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models", "comment": null, "summary": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86LLMs\u7684\u4f18\u5316\u6280\u672f\uff08\u5982\u526a\u679d\u3001\u91cf\u5316\u548c\u6807\u8bb0\u4e22\u5f03\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u6548\u679c\uff0c\u63ed\u793a\u4e86\u7ec4\u5408\u4f18\u5316\u5bf9\u5927\u578b\u6a21\u578b\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u5f3a\u8c03\u4e86\u7cfb\u7edf\u7ea7\u5206\u6790\u4e0e\u4efb\u52a1\u7279\u5b9a\u6307\u6807\u7ed3\u5408\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8d44\u6e90\u9700\u6c42\u548c\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\uff0c\u5c24\u5176\u662f\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u5185\u5b58\u4f7f\u7528\u3001\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\uff0c\u5e76\u8bc4\u4f30\u4f18\u5316\u65b9\u6cd5\u5bf9\u6587\u672c\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u5355\u72ec\u548c\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u7684\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u53ef\u80fd\u56e0\u7d2f\u79ef\u8fd1\u4f3c\u8bef\u5dee\u5bf9\u5927\u578b\u6a21\u578b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u4e14\u4ec5\u4f9d\u8d56F1\u5206\u6570\u4f1a\u63a9\u76d6\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u7cbe\u5ea6-\u53ec\u56de\u6743\u8861\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7cfb\u7edf\u7ea7\u5206\u6790\u548c\u4efb\u52a1\u7279\u5b9a\u6307\u6807\uff0c\u7814\u7a76\u4e3aLLM\u5b9e\u8df5\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5e73\u8861\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u6307\u5bfc\u3002"}}
{"id": "2508.00282", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00282", "abs": "https://arxiv.org/abs/2508.00282", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "comment": null, "summary": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals.We conclude\nthat there is a core gap between the value-driven, embodied nature of human\ncognition and the statistical patterns of LLMs, highlighting the necessity of\nincorporating intrinsic motivation and physical grounding into the design of\nmore human-aligned agents.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4eba\u7c7b\u548cGPT-4o\u5728\u4efb\u52a1\u751f\u6210\u4e2d\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u4eba\u7c7b\u53d7\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u5f71\u54cd\uff0c\u800cLLM\u751f\u6210\u7684\u4efb\u52a1\u66f4\u62bd\u8c61\u4e14\u7f3a\u4e4f\u793e\u4f1a\u6027\u548c\u7269\u7406\u6027\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u4ee3\u7406\uff08\u5982LLM\uff09\u662f\u5426\u80fd\u6a21\u62df\u4eba\u7c7b\u57fa\u4e8e\u5185\u5728\u52a8\u673a\u7684\u4efb\u52a1\u751f\u6210\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u751f\u6210\u5b9e\u9a8c\u6bd4\u8f83\u4eba\u7c7b\u548cGPT-4o\u7684\u884c\u4e3a\uff0c\u5206\u6790\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u5bf9\u4efb\u52a1\u751f\u6210\u7684\u5f71\u54cd\u3002", "result": "\u4eba\u7c7b\u4efb\u52a1\u751f\u6210\u53d7\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u5f71\u54cd\uff0c\u800cLLM\u751f\u6210\u7684\u4efb\u52a1\u66f4\u62bd\u8c61\u3001\u7f3a\u4e4f\u793e\u4f1a\u6027\u548c\u7269\u7406\u6027\uff0c\u5c3d\u7ba1\u88ab\u8ba4\u4e3a\u66f4\u6709\u8da3\u548c\u65b0\u9896\u3002", "conclusion": "LLM\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b58\u5728\u6838\u5fc3\u5dee\u8ddd\uff0c\u9700\u5728\u8bbe\u8ba1\u66f4\u4eba\u6027\u5316\u7684\u4ee3\u7406\u65f6\u878d\u5165\u5185\u5728\u52a8\u673a\u548c\u7269\u7406\u57fa\u7840\u3002"}}
{"id": "2508.00300", "categories": ["cs.HC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00300", "abs": "https://arxiv.org/abs/2508.00300", "authors": ["Shruthi Chari", "Oshani Seneviratne", "Prithwish Chakraborty", "Pablo Meyer", "Deborah L. McGuinness"], "title": "MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems", "comment": null, "summary": "Explanations are crucial for building trustworthy AI systems, but a gap often\nexists between the explanations provided by models and those needed by users.\nTo address this gap, we introduce MetaExplainer, a neuro-symbolic framework\ndesigned to generate user-centered explanations. Our approach employs a\nthree-stage process: first, we decompose user questions into machine-readable\nformats using state-of-the-art large language models (LLM); second, we delegate\nthe task of generating system recommendations to model explainer methods; and\nfinally, we synthesize natural language explanations that summarize the\nexplainer outputs. Throughout this process, we utilize an Explanation Ontology\nto guide the language models and explainer methods. By leveraging LLMs and a\nstructured approach to explanation generation, MetaExplainer aims to enhance\nthe interpretability and trustworthiness of AI systems across various\napplications, providing users with tailored, question-driven explanations that\nbetter meet their needs. Comprehensive evaluations of MetaExplainer demonstrate\na step towards evaluating and utilizing current state-of-the-art explanation\nframeworks. Our results show high performance across all stages, with a 59.06%\nF1-score in question reframing, 70% faithfulness in model explanations, and 67%\ncontext-utilization in natural language synthesis. User studies corroborate\nthese findings, highlighting the creativity and comprehensiveness of generated\nexplanations. Tested on the Diabetes (PIMA Indian) tabular dataset,\nMetaExplainer supports diverse explanation types, including Contrastive,\nCounterfactual, Rationale, Case-Based, and Data explanations. The framework's\nversatility and traceability from using ontology to guide LLMs suggest broad\napplicability beyond the tested scenarios, positioning MetaExplainer as a\npromising tool for enhancing AI explainability across various domains.", "AI": {"tldr": "MetaExplainer\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8fc7\u7a0b\u751f\u6210\u7528\u6237\u4e2d\u5fc3\u7684\u89e3\u91ca\uff0c\u5229\u7528LLM\u548c\u89e3\u91ca\u672c\u4f53\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u89e3\u51b3\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u91ca\u4e0e\u7528\u6237\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347AI\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u4e09\u9636\u6bb5\u8fc7\u7a0b\uff1a1. \u4f7f\u7528LLM\u5206\u89e3\u7528\u6237\u95ee\u9898\uff1b2. \u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u751f\u6210\u7cfb\u7edf\u5efa\u8bae\uff1b3. \u5408\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002\u5229\u7528\u89e3\u91ca\u672c\u4f53\u6307\u5bfcLLM\u548c\u89e3\u91ca\u65b9\u6cd5\u3002", "result": "\u5728\u95ee\u9898\u91cd\u6784\u3001\u6a21\u578b\u89e3\u91ca\u5fe0\u5b9e\u5ea6\u548c\u81ea\u7136\u8bed\u8a00\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u751f\u6210\u89e3\u91ca\u7684\u521b\u9020\u6027\u548c\u5168\u9762\u6027\u3002", "conclusion": "MetaExplainer\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89e3\u91ca\u7c7b\u578b\u548c\u9886\u57df\uff0c\u6709\u671b\u63d0\u5347AI\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.00172", "categories": ["cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00172", "abs": "https://arxiv.org/abs/2508.00172", "authors": ["Fupei Guo", "Hao Zheng", "Xiang Zhang", "Li Chen", "Yue Wang", "Songyang Zhang"], "title": "DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission", "comment": "To appear in 2025 IEEE Global Communications Conference (Globecom)", "summary": "The rapid development of artificial intelligence has driven smart health with\nnext-generation wireless communication technologies, stimulating exciting\napplications in remote diagnosis and intervention. To enable a timely and\neffective response for remote healthcare, efficient transmission of medical\ndata through noisy channels with limited bandwidth emerges as a critical\nchallenge. In this work, we propose a novel diffusion-based semantic\ncommunication framework, namely DiSC-Med, for the medical image transmission,\nwhere medical-enhanced compression and denoising blocks are developed for\nbandwidth efficiency and robustness, respectively. Unlike conventional\npixel-wise communication framework, our proposed DiSC-Med is able to capture\nthe key semantic information and achieve superior reconstruction performance\nwith ultra-high bandwidth efficiency against noisy channels. Extensive\nexperiments on real-world medical datasets validate the effectiveness of our\nframework, demonstrating its potential for robust and efficient telehealth\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6DiSC-Med\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u533b\u5b66\u56fe\u50cf\u4f20\u8f93\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u548c\u65e0\u7ebf\u901a\u4fe1\u6280\u672f\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u8fdc\u7a0b\u533b\u7597\u7684\u9700\u6c42\uff0c\u4f46\u5982\u4f55\u5728\u6709\u9650\u5e26\u5bbd\u548c\u566a\u58f0\u4fe1\u9053\u4e2d\u9ad8\u6548\u4f20\u8f93\u533b\u5b66\u6570\u636e\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u533b\u5b66\u589e\u5f3a\u7684\u538b\u7f29\u548c\u53bb\u566a\u6a21\u5757\uff0c\u901a\u8fc7\u8bed\u4e49\u901a\u4fe1\u6846\u67b6DiSC-Med\u5b9e\u73b0\u9ad8\u6548\u5e26\u5bbd\u5229\u7528\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u771f\u5b9e\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDiSC-Med\u80fd\u9ad8\u6548\u6355\u83b7\u5173\u952e\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u5728\u566a\u58f0\u4fe1\u9053\u4e2d\u5b9e\u73b0\u5353\u8d8a\u7684\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "DiSC-Med\u4e3a\u8fdc\u7a0b\u533b\u7597\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00332", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00332", "abs": "https://arxiv.org/abs/2508.00332", "authors": ["Kaiyan Zhao", "Zhongtao Miao", "Yoshimasa Tsuruoka"], "title": "Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment", "comment": "Work in progress", "summary": "Multimodal sentence embedding models typically leverage image-caption pairs\nin addition to textual data during training. However, such pairs often contain\nnoise, including redundant or irrelevant information on either the image or\ncaption side. To mitigate this issue, we propose MCSEO, a method that enhances\nmultimodal sentence embeddings by incorporating fine-grained object-phrase\nalignment alongside traditional image-caption alignment. Specifically, MCSEO\nutilizes existing segmentation and object detection models to extract accurate\nobject-phrase pairs, which are then used to optimize a contrastive learning\nobjective tailored to object-phrase correspondence. Experimental results on\nsemantic textual similarity (STS) tasks across different backbone models\ndemonstrate that MCSEO consistently outperforms strong baselines, highlighting\nthe significance of precise object-phrase alignment in multimodal\nrepresentation learning.", "AI": {"tldr": "MCSEO\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u8c61-\u77ed\u8bed\u5bf9\u9f50\u589e\u5f3a\u591a\u6a21\u6001\u53e5\u5b50\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u8bad\u7ec3\u4e2d\u4f7f\u7528\u7684\u56fe\u50cf-\u6807\u9898\u5bf9\u5e38\u5305\u542b\u566a\u58f0\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "MCSEO\u5229\u7528\u5206\u5272\u548c\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u63d0\u53d6\u5bf9\u8c61-\u77ed\u8bed\u5bf9\uff0c\u4f18\u5316\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u3002", "result": "\u5728STS\u4efb\u52a1\u4e2d\uff0cMCSEO\u5728\u4e0d\u540c\u9aa8\u5e72\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7cbe\u786e\u7684\u5bf9\u8c61-\u77ed\u8bed\u5bf9\u9f50\u5bf9\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.00323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00323", "abs": "https://arxiv.org/abs/2508.00323", "authors": ["Jianyi Zhang", "Xu Ji", "Ziyin Zhou", "Yuchen Zhou", "Shubo Shi", "Haoyu Wu", "Zhen Li", "Shizhao Liu"], "title": "Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning", "comment": null, "summary": "Evaluating the performance of visual language models (VLMs) in graphic\nreasoning tasks has become an important research topic. However, VLMs still\nshow obvious deficiencies in simulating human-level graphic reasoning\ncapabilities, especially in complex graphic reasoning and abstract problem\nsolving, which are less studied and existing studies only focus on simple\ngraphics. To evaluate the performance of VLMs in complex graphic reasoning, we\npropose ReasonBench, the first evaluation benchmark focused on structured\ngraphic reasoning tasks, which includes 1,613 questions from real-world\nintelligence tests. ReasonBench covers reasoning dimensions related to\nlocation, attribute, quantity, and multi-element tasks, providing a\ncomprehensive evaluation of the performance of VLMs in spatial, relational, and\nabstract reasoning capabilities. We benchmark 11 mainstream VLMs (including\nclosed-source and open-source models) and reveal significant limitations of\ncurrent models. Based on these findings, we propose a dual optimization\nstrategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability\nof reasoning by decomposing layers, and ReasonTune enhances the task\nadaptability of model reasoning through training, all of which improves VLM\nperformance by 33.5\\%. All experimental data and code are in the repository:\nhttps://huggingface.co/datasets/cistine/ReasonBench.", "AI": {"tldr": "ReasonBench\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u7ed3\u6784\u5316\u56fe\u5f62\u63a8\u7406\u4efb\u52a1\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u53cc\u91cd\u4f18\u5316\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30VLMs\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u590d\u6742\u56fe\u5f62\u548c\u62bd\u8c61\u95ee\u9898\u89e3\u51b3\u4e0a\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faReasonBench\u57fa\u51c6\uff0c\u5305\u542b1,613\u4e2a\u771f\u5b9e\u4e16\u754c\u667a\u529b\u6d4b\u8bd5\u95ee\u9898\uff0c\u8986\u76d6\u4f4d\u7f6e\u3001\u5c5e\u6027\u3001\u6570\u91cf\u548c\u591a\u5143\u7d20\u4efb\u52a1\uff1b\u63d0\u51faDiaCoT\u548cReasonTune\u53cc\u91cd\u4f18\u5316\u7b56\u7565\u3002", "result": "\u8bc4\u4f3011\u79cd\u4e3b\u6d41VLMs\uff0c\u63ed\u793a\u5176\u663e\u8457\u5c40\u9650\u6027\uff1b\u53cc\u91cd\u4f18\u5316\u7b56\u7565\u4f7fVLM\u6027\u80fd\u63d0\u534733.5%\u3002", "conclusion": "ReasonBench\u4e3aVLMs\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u4e2d\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u5de5\u5177\uff0c\u53cc\u91cd\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.00321", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00321", "abs": "https://arxiv.org/abs/2508.00321", "authors": ["Shuning Zhang", "Ying Ma", "Xin Yi", "Hewu Li"], "title": "Evaluating the Efficacy of Large Language Models for Generating Fine-Grained Visual Privacy Policies in Homes", "comment": null, "summary": "The proliferation of visual sensors in smart home environments, particularly\nthrough wearable devices like smart glasses, introduces profound privacy\nchallenges. Existing privacy controls are often static and coarse-grained,\nfailing to accommodate the dynamic and socially nuanced nature of home\nenvironments. This paper investigates the viability of using Large Language\nModels (LLMs) as the core of a dynamic and adaptive privacy policy engine. We\npropose a conceptual framework where visual data is classified using a\nmulti-dimensional schema that considers data sensitivity, spatial context, and\nsocial presence. An LLM then reasons over this contextual information to\nenforce fine-grained privacy rules, such as selective object obfuscation, in\nreal-time. Through a comparative evaluation of state-of-the-art Vision Language\nModels (including GPT-4o and the Qwen-VL series) in simulated home settings ,\nour findings show the feasibility of this approach. The LLM-based engine\nachieved a top machine-evaluated appropriateness score of 3.99 out of 5, and\nthe policies generated by the models received a top human-evaluated score of\n4.00 out of 5.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u667a\u80fd\u5bb6\u5c45\u73af\u5883\u4e2d\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u52a8\u6001\u9690\u79c1\u7b56\u7565\u5f15\u64ce\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u591a\u7ef4\u6570\u636e\u5206\u7c7b\u548c\u5b9e\u65f6\u63a8\u7406\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u9690\u79c1\u63a7\u5236\u3002", "motivation": "\u667a\u80fd\u5bb6\u5c45\u4e2d\u89c6\u89c9\u4f20\u611f\u5668\u7684\u666e\u53ca\u5e26\u6765\u4e86\u9690\u79c1\u6311\u6218\uff0c\u73b0\u6709\u9690\u79c1\u63a7\u5236\u65b9\u6cd5\u9759\u6001\u4e14\u7c97\u7c92\u5ea6\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u548c\u793e\u4ea4\u590d\u6742\u7684\u5bb6\u5ead\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6982\u5ff5\u6846\u67b6\uff0c\u5229\u7528\u591a\u7ef4\u6570\u636e\u5206\u7c7b\uff08\u6570\u636e\u654f\u611f\u6027\u3001\u7a7a\u95f4\u80cc\u666f\u3001\u793e\u4ea4\u5b58\u5728\uff09\u548cLLM\u5b9e\u65f6\u63a8\u7406\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u9690\u79c1\u89c4\u5219\uff08\u5982\u9009\u62e9\u6027\u5bf9\u8c61\u6a21\u7cca\u5316\uff09\u3002", "result": "\u5728\u6a21\u62df\u5bb6\u5ead\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\u548cQwen-VL\u7cfb\u5217\uff09\uff0cLLM\u5f15\u64ce\u7684\u673a\u5668\u8bc4\u4f30\u5f97\u5206\u4e3a3.99/5\uff0c\u4eba\u7c7b\u8bc4\u4f30\u5f97\u5206\u4e3a4.00/5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eLLM\u7684\u52a8\u6001\u9690\u79c1\u7b56\u7565\u5f15\u64ce\u5728\u667a\u80fd\u5bb6\u5c45\u73af\u5883\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u548c\u9002\u5e94\u6027\u9690\u79c1\u63a7\u5236\u3002"}}
{"id": "2508.00174", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00174", "abs": "https://arxiv.org/abs/2508.00174", "authors": ["Yongchao Huang"], "title": "RL as Regressor: A Reinforcement Learning Approach for Function Approximation", "comment": "7 pages", "summary": "Standard regression techniques, while powerful, are often constrained by\npredefined, differentiable loss functions such as mean squared error. These\nfunctions may not fully capture the desired behavior of a system, especially\nwhen dealing with asymmetric costs or complex, non-differentiable objectives.\nIn this paper, we explore an alternative paradigm: framing regression as a\nReinforcement Learning (RL) problem. We demonstrate this by treating a model's\nprediction as an action and defining a custom reward signal based on the\nprediction error, and we can leverage powerful RL algorithms to perform\nfunction approximation. Through a progressive case study of learning a noisy\nsine wave, we illustrate the development of an Actor-Critic agent, iteratively\nenhancing it with Prioritized Experience Replay, increased network capacity,\nand positional encoding to enable a capable RL agent for this regression task.\nOur results show that the RL framework not only successfully solves the\nregression problem but also offers enhanced flexibility in defining objectives\nand guiding the learning process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06\u56de\u5f52\u95ee\u9898\u8f6c\u5316\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u5956\u52b1\u4fe1\u53f7\u548cRL\u7b97\u6cd5\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u76ee\u6807\u5b9a\u4e49\u548c\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7684\u53ef\u5fae\u635f\u5931\u51fd\u6570\uff0c\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u7cfb\u7edf\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5728\u975e\u5bf9\u79f0\u6210\u672c\u6216\u590d\u6742\u76ee\u6807\u65f6\u3002", "method": "\u5c06\u6a21\u578b\u9884\u6d4b\u89c6\u4e3a\u52a8\u4f5c\uff0c\u57fa\u4e8e\u9884\u6d4b\u8bef\u5dee\u5b9a\u4e49\u5956\u52b1\u4fe1\u53f7\uff0c\u5229\u7528Actor-Critic\u7b97\u6cd5\u5e76\u7ed3\u5408\u4f18\u5148\u7ea7\u7ecf\u9a8c\u56de\u653e\u3001\u7f51\u7edc\u5bb9\u91cf\u6269\u5c55\u548c\u4f4d\u7f6e\u7f16\u7801\u3002", "result": "RL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u56de\u5f52\u95ee\u9898\uff0c\u5e76\u5728\u76ee\u6807\u5b9a\u4e49\u548c\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u4e3a\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002"}}
{"id": "2508.00344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00344", "abs": "https://arxiv.org/abs/2508.00344", "authors": ["Keer Lu", "Chong Chen", "Bin Cui", "Huang Leng", "Wentao Zhang"], "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaPlan\u7684\u81ea\u9002\u5e94\u5168\u5c40\u89c4\u5212\u4ee3\u7406\u8303\u5f0f\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86PilotRL\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u63d0\u5347LLM\u4ee3\u7406\u5728\u957f\u671f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u8303\u5f0f\uff08\u5982ReAct\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u4e14\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faAdaPlan\u8303\u5f0f\uff0c\u7ed3\u5408\u5168\u5c40\u89c4\u5212\u4e0e\u6267\u884c\uff1b\u5f00\u53d1PilotRL\u6846\u67b6\uff0c\u5206\u9636\u6bb5\u4f18\u5316\u89c4\u5212\u4e0e\u6267\u884c\u534f\u8c03\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cPilotRL\u5728LLaMA3.1-8B-Instruct\u4e0a\u8868\u73b0\u4f18\u4e8eGPT-4o\u548cGPT-4o-mini\u3002", "conclusion": "AdaPlan\u548cPilotRL\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7406\u5728\u957f\u671f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00324", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00324", "abs": "https://arxiv.org/abs/2508.00324", "authors": ["Yeonjun In", "Wonjoong Kim", "Sangwu Park", "Chanyoung Park"], "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge", "comment": "under review", "summary": "Although large reasoning models (LRMs) have demonstrated impressive\ncapabilities on complex tasks, recent studies reveal that these models\nfrequently fulfill harmful user instructions, raising significant safety\nconcerns. In this paper, we investigate the underlying cause of LRM safety\nrisks and find that models already possess sufficient safety knowledge but fail\nto activate it during reasoning. Based on this insight, we propose R1-Act, a\nsimple and efficient post-training method that explicitly triggers safety\nknowledge through a structured reasoning process. R1-Act achieves strong safety\nimprovements while preserving reasoning performance, outperforming prior\nalignment methods. Notably, it requires only 1,000 training examples and 90\nminutes of training on a single RTX A6000 GPU. Extensive experiments across\nmultiple LRM backbones and sizes demonstrate the robustness, scalability, and\npractical efficiency of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faR1-Act\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u89e6\u53d1\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u4e14\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u6267\u884c\u6709\u5bb3\u6307\u4ee4\u65f6\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5df2\u5177\u5907\u5b89\u5168\u77e5\u8bc6\u4f46\u672a\u5728\u63a8\u7406\u4e2d\u6fc0\u6d3b\u3002", "method": "\u63d0\u51faR1-Act\uff0c\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u663e\u5f0f\u89e6\u53d1\u5b89\u5168\u77e5\u8bc6\u3002", "result": "R1-Act\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "R1-Act\u5177\u6709\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u548c\u67b6\u6784\u7684LRMs\u3002"}}
{"id": "2508.00328", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00328", "abs": "https://arxiv.org/abs/2508.00328", "authors": ["Shuning Zhang", "Ying Ma", "Yongquan `Owen' Hu", "Ting Dang", "Hong Jia", "Xin Yi", "Hewu Li"], "title": "From Patient Burdens to User Agency: Designing for Real-Time Protection Support in Online Health Consultations", "comment": null, "summary": "Online medical consultation platforms, while convenient, are undermined by\nsignificant privacy risks that erode user trust. We first conducted in-depth\nsemi-structured interviews with 12 users to understand their perceptions of\nsecurity and privacy landscapes on online medical consultation platforms, as\nwell as their practices, challenges and expectation. Our analysis reveals a\ncritical disconnect between users' desires for anonymity and control, and\nplatform realities that offload the responsibility of ``privacy labor''. To\nbridge this gap, we present SafeShare, an interaction technique that leverages\nlocalized LLM to redact consultations in real-time. SafeShare balances utility\nand privacy through selectively anonymize private information. A technical\nevaluation of SafeShare's core PII detection module on 3 dataset demonstrates\nhigh efficacy, achieving 89.64\\% accuracy with Qwen3-4B on IMCS21 dataset.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u7ebf\u533b\u7597\u54a8\u8be2\u5e73\u53f0\u7684\u9690\u79c1\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSafeShare\u7684\u5b9e\u65f6\u533f\u540d\u5316\u6280\u672f\uff0c\u4ee5\u5e73\u8861\u9690\u79c1\u4e0e\u5b9e\u7528\u6027\u3002", "motivation": "\u5728\u7ebf\u533b\u7597\u54a8\u8be2\u5e73\u53f0\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u7528\u6237\u5bf9\u533f\u540d\u6027\u548c\u63a7\u5236\u6743\u7684\u9700\u6c42\u4e0e\u5e73\u53f0\u73b0\u5b9e\u4e4b\u95f4\u5b58\u5728\u8131\u8282\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u4e86\u89e3\u7528\u6237\u9700\u6c42\uff0c\u5f00\u53d1SafeShare\u6280\u672f\uff0c\u5229\u7528\u672c\u5730\u5316LLM\u5b9e\u65f6\u533f\u540d\u5316\u54a8\u8be2\u5185\u5bb9\u3002", "result": "SafeShare\u5728PII\u68c0\u6d4b\u6a21\u5757\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728IMCS21\u6570\u636e\u96c6\u4e0a\u8fbe\u523089.64%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "SafeShare\u6709\u6548\u89e3\u51b3\u4e86\u7528\u6237\u9690\u79c1\u9700\u6c42\u4e0e\u5e73\u53f0\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5728\u7ebf\u533b\u7597\u54a8\u8be2\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002"}}
{"id": "2508.00180", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00180", "abs": "https://arxiv.org/abs/2508.00180", "authors": ["Adam Block", "Cyril Zhang"], "title": "EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes", "comment": null, "summary": "Stochasticity in language model fine-tuning, often caused by the small batch\nsizes typically used in this regime, can destabilize training by introducing\nlarge oscillations in generation quality. A popular approach to mitigating this\ninstability is to take an Exponential moving average (EMA) of weights\nthroughout training. While EMA reduces stochasticity, thereby smoothing\ntraining, the introduction of bias from old iterates often creates a lag in\noptimization relative to vanilla training. In this work, we propose the\nBias-Corrected Exponential Moving Average (BEMA), a simple and practical\naugmentation of EMA that retains variance-reduction benefits while eliminating\nbias. BEMA is motivated by a simple theoretical model wherein we demonstrate\nprovable acceleration of BEMA over both a standard EMA and vanilla training.\nThrough an extensive suite of experiments on Language Models, we show that BEMA\nleads to significantly improved convergence rates and final performance over\nboth EMA and vanilla training in a variety of standard LM benchmarks, making\nBEMA a practical and theoretically motivated intervention for more stable and\nefficient fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBEMA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6b63EMA\u7684\u504f\u5dee\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u56e0\u5c0f\u6279\u91cf\u8bad\u7ec3\u5bfc\u81f4\u7684\u968f\u673a\u6027\u53caEMA\u65b9\u6cd5\u5f15\u5165\u7684\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51faBias-Corrected Exponential Moving Average (BEMA)\uff0c\u4fdd\u7559EMA\u7684\u65b9\u5dee\u51cf\u5c11\u4f18\u52bf\uff0c\u540c\u65f6\u6d88\u9664\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBEMA\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u4f18\u4e8eEMA\u548c\u666e\u901a\u8bad\u7ec3\u3002", "conclusion": "BEMA\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u7406\u8bba\u652f\u6301\u7684\u65b9\u6cd5\uff0c\u53ef\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.00360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00360", "abs": "https://arxiv.org/abs/2508.00360", "authors": ["Alan Dao", "Dinh Bach Vu", "Alex Nguyen", "Norapat Buppodom"], "title": "Lucy: edgerunning agentic web search on mobile with machine generated task vectors", "comment": null, "summary": "Small language models (SLMs) are inherently limited in knowledge-intensive\ntasks due to their constrained capacity. While test-time computation offers a\npath to enhanced performance, most approaches treat reasoning as a fixed or\nheuristic process. In this work, we propose a new paradigm: viewing the model's\ninternal reasoning, delimited by <think> and </think> tags, as a dynamic task\nvector machine. Rather than treating the content inside these tags as a mere\ntrace of thought, we interpret the generation process itself as a mechanism\nthrough which the model \\textbf{constructs and refines its own task vectors} on\nthe fly. We developed a method to optimize this dynamic task vector machine\nthrough RLVR and successfully trained an agentic web-search model. We present\nLucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with\nMCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing\non par with much larger models such as DeepSeek-V3. This demonstrates that\nsmall models can rival large ones when equipped with structured,\nself-constructed task reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u5c06\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u89c6\u4e3a\u52a8\u6001\u4efb\u52a1\u5411\u91cf\u673a\uff0c\u901a\u8fc7\u4f18\u5316\u8fd9\u4e00\u673a\u5236\uff0c\u4f7f\u5c0f\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u5ab2\u7f8e\u5927\u6a21\u578b\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u63a8\u7406\u89c6\u4e3a\u56fa\u5b9a\u6216\u542f\u53d1\u5f0f\u8fc7\u7a0b\uff0c\u7f3a\u4e4f\u52a8\u6001\u6027\u3002", "method": "\u63d0\u51fa\u5c06\u6a21\u578b\u5185\u90e8\u63a8\u7406\uff08\u7531<think>\u548c</think>\u6807\u8bb0\u754c\u5b9a\uff09\u89c6\u4e3a\u52a8\u6001\u4efb\u52a1\u5411\u91cf\u673a\uff0c\u901a\u8fc7RLVR\u4f18\u5316\u8fd9\u4e00\u673a\u5236\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u4ee3\u7406\u5f0f\u7f51\u7edc\u641c\u7d22\u6a21\u578bLucy\u3002", "result": "Lucy\uff081.7B\u53c2\u6570\uff09\u5728SimpleQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523078.3%\u51c6\u786e\u7387\uff0c\u8868\u73b0\u4e0e\u66f4\u5927\u6a21\u578b\uff08\u5982DeepSeek-V3\uff09\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u81ea\u6784\u5efa\u7684\u4efb\u52a1\u63a8\u7406\u673a\u5236\uff0c\u5c0f\u6a21\u578b\u53ef\u4ee5\u5ab2\u7f8e\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00378", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00378", "abs": "https://arxiv.org/abs/2508.00378", "authors": ["Shixin Yi", "Lin Shang"], "title": "CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding", "comment": "Preparing for AAAI 2026, Multimodal Reasoning", "summary": "Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in\nvision-language models (VLMs), but it often produces explanations that are\nlinguistically fluent yet lack grounding in visual content. We observe that\nsuch hallucinations arise in part from the absence of an explicit verification\nmechanism during multi-step reasoning. To address this, we propose\n\\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with\n\\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces\nvisual verification into the reasoning process. CoRGI follows a three-stage\npipeline: it first generates a textual reasoning chain, then extracts\nsupporting visual evidence for each reasoning step via a dedicated module\n(VEVM), and finally synthesizes the textual rationale with visual evidence to\ngenerate a grounded, verified answer. The framework can be integrated with\nexisting VLMs without end-to-end retraining. We evaluate CoRGI on the VCR\nbenchmark and find that it improves reasoning performance on two representative\nopen-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm\nthe contribution of each step in the verification module, and human evaluations\nsuggest that CoRGI leads to more factual and helpful explanations. We also\nexamine alternative designs for the visual verification step and discuss\npotential limitations of post-hoc verification frameworks. These findings\nhighlight the importance of grounding intermediate reasoning steps in visual\nevidence to enhance the robustness of multimodal reasoning.", "AI": {"tldr": "CoRGI\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u9a8c\u8bc1\u6539\u8fdb\u591a\u6a21\u6001\u63a8\u7406\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3CoT\u63d0\u793a\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u4ea7\u751f\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u89e3\u91ca\u7f3a\u4e4f\u89c6\u89c9\u5185\u5bb9\u652f\u6301\u3002", "method": "CoRGI\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u751f\u6210\u6587\u672c\u63a8\u7406\u94fe\u3001\u63d0\u53d6\u89c6\u89c9\u8bc1\u636e\u3001\u5408\u6210\u9a8c\u8bc1\u7b54\u6848\u3002", "result": "\u5728VCR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoRGI\u63d0\u5347\u4e86Qwen-2.5VL\u548cLLaVA-1.6\u7684\u6027\u80fd\uff0c\u5e76\u751f\u6210\u66f4\u4e8b\u5b9e\u6027\u7684\u89e3\u91ca\u3002", "conclusion": "\u89c6\u89c9\u9a8c\u8bc1\u80fd\u589e\u5f3a\u591a\u6a21\u6001\u63a8\u7406\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u540e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u5b58\u5728\u6f5c\u5728\u9650\u5236\u3002"}}
{"id": "2508.00439", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00439", "abs": "https://arxiv.org/abs/2508.00439", "authors": ["Subin Park", "Jeonghyun Kim", "Jeanne Choi", "Joseph Seering", "Uichin Lee", "Sung-Ju Lee"], "title": "HateBuffer: Safeguarding Content Moderators' Mental Well-Being through Hate Speech Content Modification", "comment": "Accepted by ACM CSCW 2025; 39 pages (including 6 pages of Appendix)", "summary": "Hate speech remains a persistent and unresolved challenge in online\nplatforms. Content moderators, working on the front lines to review\nuser-generated content and shield viewers from hate speech, often find\nthemselves unprotected from the mental burden as they continuously engage with\noffensive language. To safeguard moderators' mental well-being, we designed\nHateBuffer, which anonymizes targets of hate speech, paraphrases offensive\nexpressions into less offensive forms, and shows the original expressions when\nmoderators opt to see them. Our user study with 80 participants consisted of a\nsimulated hate speech moderation task set on a fictional news platform,\nfollowed by semi-structured interviews. Although participants rated the hate\nseverity of comments lower while using HateBuffer, contrary to our\nexpectations, they did not experience improved emotion or reduced fatigue\ncompared with the control group. In interviews, however, participants described\nHateBuffer as an effective buffer against emotional contagion and the\nnormalization of biased opinions in hate speech. Notably, HateBuffer did not\ncompromise moderation accuracy and even contributed to a slight increase in\nrecall. We explore possible explanations for the discrepancy between the\nperceived benefits of HateBuffer and its measured impact on mental well-being.\nWe also underscore the promise of text-based content modification techniques as\ntools for a healthier content moderation environment.", "AI": {"tldr": "HateBuffer\u662f\u4e00\u79cd\u5de5\u5177\uff0c\u65e8\u5728\u901a\u8fc7\u533f\u540d\u5316\u4ec7\u6068\u8a00\u8bba\u76ee\u6807\u3001\u6539\u5199\u653b\u51fb\u6027\u8868\u8fbe\u6765\u4fdd\u62a4\u5185\u5bb9\u5ba1\u6838\u5458\u7684\u5fc3\u7406\u5065\u5eb7\uff0c\u4f46\u5b9e\u9a8c\u663e\u793a\u5176\u5bf9\u60c5\u7eea\u548c\u75b2\u52b3\u7684\u6539\u5584\u6548\u679c\u4e0d\u660e\u663e\uff0c\u4f46\u5bf9\u60c5\u611f\u4f20\u67d3\u548c\u504f\u89c1\u610f\u89c1\u7684\u7f13\u51b2\u6709\u6548\u3002", "motivation": "\u5728\u7ebf\u5e73\u53f0\u4e2d\u4ec7\u6068\u8a00\u8bba\u5bf9\u5185\u5bb9\u5ba1\u6838\u5458\u5fc3\u7406\u5065\u5eb7\u9020\u6210\u8d1f\u62c5\uff0c\u9700\u8981\u4e00\u79cd\u5de5\u5177\u6765\u51cf\u8f7b\u8fd9\u79cd\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1HateBuffer\u5de5\u5177\uff0c\u901a\u8fc7\u533f\u540d\u5316\u548c\u6539\u5199\u4ec7\u6068\u8a00\u8bba\uff0c\u5e76\u5728\u6a21\u62df\u4efb\u52a1\u548c\u7528\u6237\u7814\u7a76\u4e2d\u6d4b\u8bd5\u5176\u6548\u679c\u3002", "result": "HateBuffer\u964d\u4f4e\u4e86\u4ec7\u6068\u8a00\u8bba\u7684\u4e25\u91cd\u6027\u8bc4\u5206\uff0c\u4f46\u5bf9\u60c5\u7eea\u548c\u75b2\u52b3\u65e0\u663e\u8457\u6539\u5584\uff1b\u5ba1\u6838\u51c6\u786e\u6027\u672a\u53d7\u5f71\u54cd\uff0c\u53ec\u56de\u7387\u7565\u6709\u63d0\u5347\u3002", "conclusion": "HateBuffer\u5728\u60c5\u611f\u7f13\u51b2\u65b9\u9762\u6709\u6548\uff0c\u4f46\u5bf9\u5fc3\u7406\u5065\u5eb7\u7684\u76f4\u63a5\u6539\u5584\u6709\u9650\uff0c\u6587\u672c\u4fee\u6539\u6280\u672f\u5bf9\u5065\u5eb7\u5ba1\u6838\u73af\u5883\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.00201", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00201", "abs": "https://arxiv.org/abs/2508.00201", "authors": ["Mehdi Ben Ayed", "Fei Feng", "Jay Adams", "Vishwakarma Singh", "Kritarth Anand", "Jiajing Xu"], "title": "RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems", "comment": null, "summary": "Existing web-scale recommendation systems commonly use supervised learning\nmethods that prioritize immediate user feedback. Although reinforcement\nlearning (RL) offers a solution to optimize longer-term goals, such as\nin-session engagement, applying it at web scale is challenging due to the\nextremely large action space and engineering complexity. In this paper, we\nintroduce RecoMind, a simulator-based RL framework designed for the effective\noptimization of session-based goals at web-scale. RecoMind leverages existing\nrecommendation models to establish a simulation environment and to bootstrap\nthe RL policy to optimize immediate user interactions from the outset. This\nmethod integrates well with existing industry pipelines, simplifying the\ntraining and deployment of RL policies. Additionally, RecoMind introduces a\ncustom exploration strategy to efficiently explore web-scale action spaces with\nhundreds of millions of items. We evaluated RecoMind through extensive offline\nsimulations and online A/B testing on a video streaming platform. Both methods\nshowed that the RL policy trained using RecoMind significantly outperforms\ntraditional supervised learning recommendation approaches in in-session user\nsatisfaction. In online A/B tests, the RL policy increased videos watched for\nmore than 10 seconds by 15.81\\% and improved session depth by 4.71\\% for\nsessions with at least 10 interactions. As a result, RecoMind presents a\nsystematic and scalable approach for embedding RL into web-scale recommendation\nsystems, showing great promise for optimizing session-based user satisfaction.", "AI": {"tldr": "RecoMind\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u62df\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u4f1a\u8bdd\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\uff0c\u96be\u4ee5\u4f18\u5316\u957f\u671f\u76ee\u6807\uff08\u5982\u4f1a\u8bdd\u53c2\u4e0e\u5ea6\uff09\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u9762\u4e34\u5de5\u7a0b\u590d\u6742\u6027\u6311\u6218\u3002", "method": "RecoMind\u5229\u7528\u73b0\u6709\u63a8\u8350\u6a21\u578b\u6784\u5efa\u6a21\u62df\u73af\u5883\uff0c\u5e76\u901a\u8fc7\u81ea\u5b9a\u4e49\u63a2\u7d22\u7b56\u7565\u9ad8\u6548\u63a2\u7d22\u5927\u89c4\u6a21\u52a8\u4f5c\u7a7a\u95f4\uff0c\u7b80\u5316RL\u7b56\u7565\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\u3002", "result": "\u79bb\u7ebf\u6a21\u62df\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\uff0cRecoMind\u8bad\u7ec3\u7684RL\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f1a\u8bdd\u6df1\u5ea6\u548c\u89c2\u770b\u65f6\u957f\u5747\u6709\u63d0\u5347\u3002", "conclusion": "RecoMind\u4e3a\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684RL\u96c6\u6210\u65b9\u6cd5\uff0c\u4f18\u5316\u4f1a\u8bdd\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2508.00370", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00370", "abs": "https://arxiv.org/abs/2508.00370", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "comment": "9 pages", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.", "AI": {"tldr": "EdgeInfinite-Instruct\u901a\u8fc7\u5206\u6bb5\u76d1\u7763\u5fae\u8c03\uff08S-SFT\uff09\u548c\u7ec6\u7c92\u5ea6\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u4f18\u5316\u4e86Transformer\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\uff0c\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u4efb\u52a1\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3Transformer\u5927\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5185\u5b58\u9700\u6c42\u95ee\u9898\uff0c\u7279\u522b\u662f\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684KV\u7f13\u5b58\u548c\u9996\u4ee4\u724c\u65f6\u95f4\uff08TTFT\uff09\u6311\u6218\u3002", "method": "\u63d0\u51faEdgeInfinite-Instruct\uff0c\u91c7\u7528\u5206\u6bb5\u76d1\u7763\u5fae\u8c03\uff08S-SFT\uff09\u7b56\u7565\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u548c\u56fa\u5b9a\u5f62\u72b6\u8ba1\u7b97\u56fe\u4f18\u5316\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u79fb\u52a8\u4efb\u52a1\u4e2d\uff0c\u63d0\u5347\u4e86\u9886\u57df\u7279\u5b9a\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8fb9\u7f18NPU\u8bbe\u5907\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "EdgeInfinite-Instruct\u901a\u8fc7\u5b9a\u5236\u5316\u4f18\u5316\uff0c\u6709\u6548\u5e73\u8861\u4e86\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u957f\u5e8f\u5217\u4efb\u52a1\u3002"}}
{"id": "2508.00401", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00401", "abs": "https://arxiv.org/abs/2508.00401", "authors": ["Riddhi J. Pitliya", "Ozan Catal", "Toon Van de Maele", "Corrado Pezzato", "Tim Verbelen"], "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation", "comment": null, "summary": "We present a novel approach to multi-agent cooperation by implementing theory\nof mind (ToM) within active inference. ToM - the ability to understand that\nothers can have differing knowledge and goals - enables agents to reason about\nothers' beliefs while planning their own actions. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication,\nwhile being generalisable. In our framework, the ToM-equipped agent maintains\ndistinct representations of its own and others' beliefs and goals. We extend\nthe sophisticated inference tree-based planning algorithm to systematically\nexplore joint policy spaces through recursive reasoning. Our approach is\nevaluated through collision avoidance and foraging task simulations. Results\ndemonstrate that ToM-equipped agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour. This work advances practical applications in artificial\nintelligence while providing computational insights into ToM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u5b9e\u73b0\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684\u5171\u4eab\u751f\u6210\u6a21\u578b\u6216\u663e\u5f0f\u901a\u4fe1\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u56e0\u7f3a\u4e4f\u5bf9\u4ed6\u4eba\u4fe1\u5ff5\u548c\u76ee\u6807\u7684\u63a8\u7406\u80fd\u529b\u800c\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u6269\u5c55\u63a8\u7406\u6811\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u9012\u5f52\u63a8\u7406\u63a2\u7d22\u8054\u5408\u7b56\u7565\u7a7a\u95f4\uff0c\u667a\u80fd\u4f53\u7ef4\u62a4\u81ea\u8eab\u548c\u4ed6\u4eba\u4fe1\u5ff5\u53ca\u76ee\u6807\u7684\u72ec\u7acb\u8868\u793a\u3002", "result": "\u5728\u907f\u78b0\u548c\u89c5\u98df\u4efb\u52a1\u4e2d\uff0cToM\u667a\u80fd\u4f53\u8868\u73b0\u4f18\u4e8e\u975eToM\u667a\u80fd\u4f53\uff0c\u80fd\u907f\u514d\u78b0\u649e\u5e76\u51cf\u5c11\u5197\u4f59\u52aa\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u5de5\u667a\u80fd\u7684\u5b9e\u8df5\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u4e3a\u5fc3\u667a\u7406\u8bba\u7684\u8ba1\u7b97\u673a\u5236\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2508.00646", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00646", "abs": "https://arxiv.org/abs/2508.00646", "authors": ["Dennis Zyska", "Ilia Kuznetsov", "Florian M\u00fcller", "Iryna Gurevych"], "title": "Pull Requests From The Classroom: Co-Developing Curriculum And Code", "comment": null, "summary": "Educational technologies often misalign with instructors' pedagogical goals,\nforcing adaptations that compromise teaching efficacy. In this paper, we\npresent a case study on the co-development of curriculum and technology in the\ncontext of a university course on scientific writing. Specifically, we examine\nhow a custom-built peer feedback system was iteratively developed alongside the\ncourse to support annotation, feedback exchange, and revision. Results show\nthat while co-development fostered stronger alignment between software features\nand course goals, it also exposed usability limitations and\ninfrastructure-related frustrations, emphasizing the need for closer\ncoordination between teaching and technical teams.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6559\u80b2\u6280\u672f\u4e0e\u6559\u5e08\u6559\u5b66\u76ee\u6807\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4e00\u4e2a\u79d1\u5b66\u5199\u4f5c\u8bfe\u7a0b\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u8bfe\u7a0b\u4e0e\u6280\u672f\u5171\u540c\u5f00\u53d1\u7684\u8fc7\u7a0b\u53ca\u5176\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u6559\u80b2\u6280\u672f\u4e0e\u6559\u5e08\u6559\u5b66\u76ee\u6807\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6559\u5b66\u6548\u679c\u3002", "method": "\u91c7\u7528\u6848\u4f8b\u7814\u7a76\u65b9\u6cd5\uff0c\u5171\u540c\u5f00\u53d1\u8bfe\u7a0b\u4e0e\u5b9a\u5236\u5316\u7684\u540c\u884c\u53cd\u9988\u7cfb\u7edf\uff0c\u652f\u6301\u6ce8\u91ca\u3001\u53cd\u9988\u4ea4\u6362\u548c\u4fee\u8ba2\u3002", "result": "\u5171\u540c\u5f00\u53d1\u589e\u5f3a\u4e86\u8f6f\u4ef6\u529f\u80fd\u4e0e\u8bfe\u7a0b\u76ee\u6807\u7684\u5339\u914d\uff0c\u4f46\u4e5f\u66b4\u9732\u4e86\u53ef\u7528\u6027\u9650\u5236\u548c\u57fa\u7840\u8bbe\u65bd\u95ee\u9898\u3002", "conclusion": "\u9700\u8981\u52a0\u5f3a\u6559\u5b66\u56e2\u961f\u4e0e\u6280\u672f\u56e2\u961f\u7684\u534f\u4f5c\uff0c\u4ee5\u66f4\u597d\u5730\u5b9e\u73b0\u6280\u672f\u4e0e\u6559\u5b66\u76ee\u6807\u7684\u5339\u914d\u3002"}}
{"id": "2508.00202", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00202", "abs": "https://arxiv.org/abs/2508.00202", "authors": ["Ecem Bozkurt", "Antonio Ortega"], "title": "Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models", "comment": "5 pages, 2 figures, under review at CAMSAP 2025", "summary": "Foundation models (FMs) pretrained on large datasets have become fundamental\nfor various downstream machine learning tasks, in particular in scenarios where\nobtaining perfectly labeled data is prohibitively expensive. In this paper, we\nassume an FM has to be fine-tuned with noisy data and present a two-stage\nframework to ensure robust classification in the presence of label noise\nwithout model retraining. Recent work has shown that simple k-nearest neighbor\n(kNN) approaches using an embedding derived from an FM can achieve good\nperformance even in the presence of severe label noise. Our work is motivated\nby the fact that these methods make use of local geometry. In this paper,\nfollowing a similar two-stage procedure, reliability estimation followed by\nreliability-weighted inference, we show that improved performance can be\nachieved by introducing geometry information. For a given instance, our\nproposed inference uses a local neighborhood of training data, obtained using\nthe non-negative kernel (NNK) neighborhood construction. We propose several\nmethods for reliability estimation that can rely less on distance and local\nneighborhood as the label noise increases. Our evaluation on CIFAR-10 and\nDermaMNIST shows that our methods improve robustness across various noise\nconditions, surpassing standard K-NN approaches and recent\nadaptive-neighborhood baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528\u51e0\u4f55\u4fe1\u606f\u6539\u8fdb\u57fa\u7840\u6a21\u578b\u5728\u566a\u58f0\u6807\u7b7e\u6570\u636e\u4e0b\u7684\u9c81\u68d2\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u566a\u58f0\u6807\u7b7e\u6570\u636e\u4e0b\u5fae\u8c03\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5c40\u90e8\u51e0\u4f55\u4fe1\u606f\uff0c\u4f46\u6027\u80fd\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u53ef\u9760\u6027\u4f30\u8ba1\u548c\u52a0\u6743\u63a8\u7406\uff0c\u7ed3\u5408\u975e\u8d1f\u6838\u90bb\u57df\u6784\u9020\u548c\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5728CIFAR-10\u548cDermaMNIST\u4e0a\uff0c\u65b9\u6cd5\u5728\u591a\u79cd\u566a\u58f0\u6761\u4ef6\u4e0b\u4f18\u4e8e\u6807\u51c6K-NN\u548c\u81ea\u9002\u5e94\u90bb\u57df\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u4fe1\u606f\u548c\u6539\u8fdb\u53ef\u9760\u6027\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u566a\u58f0\u6807\u7b7e\u6570\u636e\u4e0b\u7684\u5206\u7c7b\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.00385", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00385", "abs": "https://arxiv.org/abs/2508.00385", "authors": ["Dingzirui Wang", "Xuangliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness", "comment": null, "summary": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u6f14\u793a\u65e0\u6548\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u6d41\u7684\u6f14\u793a\u9009\u62e9\u65b9\u6cd5GradS\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5047\u8bbeICL\u4e2d\u7684\u6f14\u793a\u603b\u662f\u6709\u6548\uff0c\u4f46\u5b9e\u9645\u4e0a\u8bb8\u591a\u6f14\u793a\u65e0\u6548\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u5176\u539f\u56e0\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u6d41\u548c\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u6a21\u578b\u5206\u6790\u6f14\u793a\u65e0\u6548\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u6d41\u5927\u5c0f\u7684\u6f14\u793a\u9009\u62e9\u65b9\u6cd5GradS\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u968f\u7740\u6a21\u578b\u5c42\u6570\u589e\u52a0\uff0c\u6f14\u793a\u6709\u6548\u6027\u5dee\u5f02\u653e\u5927\uff0cGradS\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53476.8%\u3002", "conclusion": "GradS\u901a\u8fc7\u8003\u8651\u6a21\u578b\u5df2\u5b66\u4e60\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6f14\u793a\u9009\u62e9\u7684\u6709\u6548\u6027\uff0c\u4e3aICL\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.00414", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00414", "abs": "https://arxiv.org/abs/2508.00414", "authors": ["Tianqing Fang", "Zhisong Zhang", "Xiaoyang Wang", "Rui Wang", "Can Qin", "Yuxuan Wan", "Jun-Yu Ma", "Ce Zhang", "Jiaqi Chen", "Xiyun Li", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training", "comment": "16 pages", "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro", "AI": {"tldr": "Cognitive Kernel-Pro \u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u4e14\u514d\u8d39\u7684 AI \u4ee3\u7406\u6846\u67b6\uff0c\u65e8\u5728\u63a8\u52a8\u9ad8\u7ea7 AI \u4ee3\u7406\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u521b\u65b0\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d AI \u4ee3\u7406\u7cfb\u7edf\u591a\u4e3a\u95ed\u6e90\u6216\u4f9d\u8d56\u4ed8\u8d39 API\uff0c\u9650\u5236\u4e86\u7814\u7a76\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "method": "\u5f00\u53d1\u5f00\u6e90\u6846\u67b6 Cognitive Kernel-Pro\uff0c\u7814\u7a76\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u6784\u5efa\uff08\u67e5\u8be2\u3001\u8f68\u8ff9\u3001\u53ef\u9a8c\u8bc1\u7b54\u6848\uff09\uff0c\u5e76\u63a2\u7d22\u4ee3\u7406\u6d4b\u8bd5\u65f6\u53cd\u601d\u548c\u6295\u7968\u7b56\u7565\u3002", "result": "\u5728 GAIA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u5f00\u6e90\u4ee3\u7406\u7684\u6700\u4f18\u6027\u80fd\uff0c8B \u53c2\u6570\u6a21\u578b\u8d85\u8d8a WebDancer \u548c WebSailor\u3002", "conclusion": "Cognitive Kernel-Pro \u4e3a\u9ad8\u80fd\u529b AI \u4ee3\u7406\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u63a8\u52a8\u4e86\u5f00\u6e90\u793e\u533a\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.00652", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00652", "abs": "https://arxiv.org/abs/2508.00652", "authors": ["Shuning Zhang", "Han Chen", "Yabo Wang", "Yiqun Xu", "Jiaqi Bai", "Yuanyuan Wu", "Shixuan Li", "Xin Yi", "Chunhui Wang", "Hewu Li"], "title": "The Manipulative Power of Voice Characteristics: Investigating Deceptive Patterns in Mandarin Chinese Female Synthetic Speech", "comment": null, "summary": "Pervasive voice interaction enables deceptive patterns through subtle voice\ncharacteristics, yet empirical investigation into this manipulation lags\nbehind, especially within major non-English language contexts. Addressing this\ngap, our study presents the first systematic investigation into voice\ncharacteristic-based dark patterns employing female synthetic voices in\nMandarin Chinese. This focus is crucial given the prevalence of female personas\nin commercial assistants and the prosodic significance in the Chinese language.\nGuided by the conceptual framework identifying key influencing factors, we\nsystematically evaluate effectiveness variations by manipulating voice\ncharacteristics (five characteristics, three intensities) across different\nscenarios (shopping vs. question-answering) with different commercial aims. A\npreliminary study (N=24) validated the experimental materials and the main\nstudy (N=36) revealed significant behavioral manipulation (up to +2027.6%).\nCrucially, the analysis showed that effectiveness varied significantly with\nvoice characteristics and scenario, mediated by user perception (of tone,\nintonation, timbre) and user demographics (individual preferences, though\nlimited demographic impact). These interconnected findings offer evidence-based\ninsights for ethical design.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8c03\u67e5\u4e86\u666e\u901a\u8bdd\u5408\u6210\u5973\u58f0\u4e2d\u57fa\u4e8e\u8bed\u97f3\u7279\u5f81\u7684\u9ed1\u6697\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u8bed\u97f3\u7279\u6027\u548c\u573a\u666f\u5bf9\u884c\u4e3a\u64cd\u7eb5\u7684\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u586b\u8865\u4e86\u975e\u82f1\u8bed\u8bed\u8a00\u73af\u5883\u4e2d\u8bed\u97f3\u7279\u5f81\u64cd\u7eb5\u7684\u5b9e\u8bc1\u7814\u7a76\u7a7a\u767d\uff0c\u5c24\u5176\u662f\u666e\u901a\u8bdd\u73af\u5883\u4e0b\u5973\u6027\u5408\u6210\u8bed\u97f3\u7684\u5546\u4e1a\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u64cd\u7eb5\u4e94\u79cd\u8bed\u97f3\u7279\u5f81\u548c\u4e09\u79cd\u5f3a\u5ea6\uff0c\u5728\u4e0d\u540c\u573a\u666f\uff08\u8d2d\u7269\u4e0e\u95ee\u7b54\uff09\u4e2d\u8bc4\u4f30\u6709\u6548\u6027\uff0c\u7ed3\u5408\u7528\u6237\u611f\u77e5\u548c\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bed\u97f3\u7279\u5f81\u548c\u573a\u666f\u663e\u8457\u5f71\u54cd\u884c\u4e3a\u64cd\u7eb5\uff08\u6700\u9ad8\u8fbe+2027.6%\uff09\uff0c\u7528\u6237\u611f\u77e5\u548c\u4e2a\u4f53\u504f\u597d\u8d77\u4e2d\u4ecb\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bed\u97f3\u4ea4\u4e92\u7684\u4f26\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.00230", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00230", "abs": "https://arxiv.org/abs/2508.00230", "authors": ["Paul Albert", "Frederic Z. Zhang", "Hemanth Saratchandran", "Anton van den Hengel", "Ehsan Abbasnejad"], "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product", "comment": "To appear in ICCV 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for\nadapting large pre-trained models. Amongst PEFT methods, low-rank adaptation\n(LoRA) has achieved notable success. However, recent studies have highlighted\nits limitations compared against full-rank alternatives, particularly when\napplied to multimodal and large language models. In this work, we present a\nquantitative comparison amongst full-rank and low-rank PEFT methods using a\nsynthetic matrix approximation benchmark with controlled spectral properties.\nOur results confirm that LoRA struggles to approximate matrices with relatively\nflat spectrums or high frequency components -- signs of high effective ranks.\nTo this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the\nKhatri-Rao product to produce weight updates, which, by construction, tends to\nproduce matrix product with a high effective rank. We demonstrate performance\ngains with KRAdapter on vision-language models up to 1B parameters and on large\nlanguage models up to 8B parameters, particularly on unseen common-sense\nreasoning tasks. In addition, KRAdapter maintains the memory and compute\nefficiency of LoRA, making it a practical and robust alternative to fine-tune\nbillion-scale parameter models.", "AI": {"tldr": "KRAdapter\u662f\u4e00\u79cd\u65b0\u578b\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7Khatri-Rao\u4e58\u79ef\u751f\u6210\u6743\u91cd\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86LoRA\u5728\u8fd1\u4f3c\u9ad8\u6709\u6548\u79e9\u77e9\u9635\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "LoRA\u5728\u9002\u5e94\u591a\u6a21\u6001\u548c\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9ad8\u6709\u6548\u79e9\u77e9\u9635\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faKRAdapter\uff0c\u5229\u7528Khatri-Rao\u4e58\u79ef\u751f\u6210\u6743\u91cd\u66f4\u65b0\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u9ad8\u6709\u6548\u79e9\u77e9\u9635\u3002", "result": "\u57281B\u53c2\u6570\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c8B\u53c2\u6570\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\uff0cKRAdapter\u5728\u672a\u89c1\u8fc7\u7684\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eLoRA\u3002", "conclusion": "KRAdapter\u5728\u4fdd\u6301LoRA\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u662f\u9002\u5e94\u5927\u89c4\u6a21\u6a21\u578b\u7684\u5b9e\u7528\u9009\u62e9\u3002"}}
{"id": "2508.00390", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00390", "abs": "https://arxiv.org/abs/2508.00390", "authors": ["Hengxing Cai", "Jinhan Dong", "Yijie Rao", "Jingcheng Deng", "Jingjun Tan", "Qien Chen", "Haidong Wang", "Zhen Wang", "Shiyu Huang", "Agachai Sumalee", "Renxin Zhong"], "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation", "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable\nagents to accurately localize targets and plan flight paths in complex\nenvironments based on natural language instructions, with broad applications in\nintelligent inspection, disaster rescue, and urban monitoring. Recent progress\nin Vision-Language Models (VLMs) has provided strong semantic understanding for\nthis task, while reinforcement learning (RL) has emerged as a promising\npost-training strategy to further improve generalization. However, existing RL\nmethods often suffer from inefficient use of training data, slow convergence,\nand insufficient consideration of the difficulty variation among training\nsamples, which limits further performance improvement. To address these\nchallenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling\n(SA-GCS)}, a novel training framework that systematically integrates Curriculum\nLearning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator\n(SA-DE) to quantify the complexity of training samples and a Gaussian\nCurriculum Scheduler (GCS) to dynamically adjust the sampling distribution,\nenabling a smooth progression from easy to challenging tasks. This design\nsignificantly improves training efficiency, accelerates convergence, and\nenhances overall model performance. Extensive experiments on the CityNav\nbenchmark demonstrate that SA-GCS consistently outperforms strong baselines\nacross all metrics, achieves faster and more stable convergence, and\ngeneralizes well across models of different scales, highlighting its robustness\nand scalability. The implementation of our approach is publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSA-GCS\u7684\u65b0\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u4f18\u5316\u65e0\u4eba\u673a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u5229\u7528\u6548\u7387\u4f4e\u3001\u6536\u655b\u6162\u4ee5\u53ca\u5bf9\u8bad\u7ec3\u6837\u672c\u96be\u5ea6\u5dee\u5f02\u8003\u8651\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSA-GCS\u6846\u67b6\uff0c\u5305\u542b\u8bed\u4e49\u611f\u77e5\u96be\u5ea6\u8bc4\u4f30\u5668\uff08SA-DE\uff09\u548c\u9ad8\u65af\u8bfe\u7a0b\u8c03\u5ea6\u5668\uff08GCS\uff09\uff0c\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6837\u672c\u5206\u5e03\u3002", "result": "\u5728CityNav\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSA-GCS\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u66f4\u7a33\u5b9a\uff0c\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "SA-GCS\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.00459", "categories": ["cs.AI", "68T07, 68T20", "I.2.6; I.2.7; I.2.3"], "pdf": "https://arxiv.org/pdf/2508.00459", "abs": "https://arxiv.org/abs/2508.00459", "authors": ["Andrea Asperti", "Alberto Naibo", "Claudio Sacerdoti Coen"], "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u9886\u57df\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5f62\u5f0f\u5316\u6570\u5b66\u8bc1\u660e\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5206\u6790\u4e86\u5176\u4e0e\u4ee3\u7801\u751f\u6210\u7684\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u6570\u5b66\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5f62\u5f0f\u5316\u8bc1\u660e\u4e2d\u7684\u56f0\u96be\uff0c\u4ee5\u63ed\u793a\u5176\u63a8\u7406\u673a\u5236\u548c\u76d1\u7763\u65b9\u5f0f\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6700\u65b0\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a2\u8ba8LLMs\u5728\u6570\u5b66\u8ba4\u77e5\u4e2d\u7684\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u5f62\u5f0f\u5316\u6570\u5b66\u8bc1\u660e\u6bd4\u4ee3\u7801\u5408\u6210\u66f4\u8106\u5f31\uff0c\u5e76\u8ba8\u8bba\u4e86LLMs\u662f\u5426\u771f\u6b63\u5177\u5907\u903b\u8f91\u72b6\u6001\u8868\u793a\u80fd\u529b\u3002", "conclusion": "\u65e8\u5728\u660e\u786e\u5f53\u524d\u6280\u672f\u7684\u8fb9\u754c\uff0c\u5e76\u63d0\u51fa\u53ef\u80fd\u7684\u6269\u5c55\u65b9\u5411\u3002"}}
{"id": "2508.00723", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00723", "abs": "https://arxiv.org/abs/2508.00723", "authors": ["Rebecca Yu", "Valerie Chen", "Ameet Talwalkar", "Hoda Heidari"], "title": "Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors Impacting AI Adoption", "comment": "To be published in Proceedings of the Eighth AAAI/ACM Conference on\n  AI, Ethics, and Society (AIES-25). 10 pages, 4 figures, 1 table", "summary": "Growing excitement around deploying AI across various domains calls for a\ncareful assessment of how human decision-makers interact with AI-powered\nsystems. In particular, it is essential to understand when decision-makers\nvoluntarily choose to consult AI tools, which we term decision-maker adoption.\nWe interviewed experts across four domains -- medicine, law, journalism, and\nthe public sector -- to explore current AI use cases and perceptions of\nadoption. From these interviews, we identify key factors that shape\ndecision-maker adoption of AI tools: the decision-maker's background,\nperceptions of the AI, consequences for the decision-maker, and perceived\nimplications for other stakeholders. We translate these factors into an AI\nadoption sheet to analyze how decision-makers approach adoption choices through\ncomparative, cross-domain case studies, highlighting how our factors help\nexplain inter-domain differences in adoption. Our findings offer practical\nguidance for supporting the responsible and context-aware deployment of AI by\nbetter accounting for the decision-maker's perspective.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u7c7b\u51b3\u7b56\u8005\u4e0eAI\u5de5\u5177\u7684\u4e92\u52a8\uff0c\u63d0\u51fa\u4e86\u5f71\u54cdAI\u5de5\u5177\u91c7\u7528\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u91c7\u7528\u8868\u6765\u6307\u5bfcAI\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002", "motivation": "\u968f\u7740AI\u5728\u5404\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u7406\u89e3\u51b3\u7b56\u8005\u4f55\u65f6\u81ea\u613f\u91c7\u7528AI\u5de5\u5177\uff0c\u4ee5\u652f\u6301\u5176\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002", "method": "\u901a\u8fc7\u8de8\u9886\u57df\uff08\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u65b0\u95fb\u3001\u516c\u5171\u90e8\u95e8\uff09\u4e13\u5bb6\u8bbf\u8c08\uff0c\u8bc6\u522b\u5f71\u54cdAI\u5de5\u5177\u91c7\u7528\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u5f00\u53d1AI\u91c7\u7528\u8868\u8fdb\u884c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u51b3\u7b56\u8005\u80cc\u666f\u3001\u5bf9AI\u7684\u8ba4\u77e5\u3001\u51b3\u7b56\u540e\u679c\u53ca\u5bf9\u5176\u4ed6\u5229\u76ca\u76f8\u5173\u8005\u7684\u5f71\u54cd\u662fAI\u91c7\u7528\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u4e3a\u652f\u6301AI\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u51b3\u7b56\u8005\u89c6\u89d2\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00264", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00264", "abs": "https://arxiv.org/abs/2508.00264", "authors": ["Jerry Huang", "Peng Lu", "Qiuhao Zeng"], "title": "Calibrated Language Models and How to Find Them with Label Smoothing", "comment": "Accepted to the Forty-second International Conference on Machine\n  Learning (ICML) 2025. First two authors contributed equally", "summary": "Recent advances in natural language processing (NLP) have opened up greater\nopportunities to enable fine-tuned large language models (LLMs) to behave as\nmore powerful interactive agents through improved instruction-following\nability. However, understanding how this impacts confidence calibration for\nreliable model output has not been researched in full. In this work, we examine\nvarious open-sourced LLMs, identifying significant calibration degradation\nafter instruction tuning in each. Seeking a practical solution, we look towards\nlabel smoothing, which has been shown as an effective method to regularize for\noverconfident predictions but has yet to be widely adopted in the supervised\nfine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing\nis sufficient to maintain calibration throughout the SFT process. However,\nsettings remain where the effectiveness of smoothing is severely diminished, in\nparticular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to\nstem from the ability to become over-confident, which has a direct relationship\nwith the hidden size and vocabulary size, and justify this theoretically and\nexperimentally. Finally, we address an outstanding issue regarding the memory\nfootprint of the cross-entropy loss computation in the label smoothed loss\nsetting, designing a customized kernel to dramatically reduce memory\nconsumption without sacrificing speed or performance in comparison to existing\nsolutions for non-smoothed losses.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6307\u4ee4\u8c03\u4f18\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u6807\u7b7e\u5e73\u6ed1\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u5185\u5b58\u6d88\u8017\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u6307\u4ee4\u8c03\u4f18\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\uff0c\u4f46\u5176\u5bf9\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5206\u6790\u4e86\u591a\u79cd\u5f00\u6e90LLM\uff0c\u53d1\u73b0\u6307\u4ee4\u8c03\u4f18\u540e\u6821\u51c6\u9000\u5316\uff0c\u63d0\u51fa\u6807\u7b7e\u5e73\u6ed1\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5b9a\u5236\u5185\u6838\u4ee5\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "result": "\u6807\u7b7e\u5e73\u6ed1\u80fd\u6709\u6548\u7ef4\u6301\u6821\u51c6\uff0c\u4f46\u5728\u5927\u8bcd\u6c47\u91cfLLM\u4e2d\u6548\u679c\u53d7\u9650\uff0c\u7406\u8bba\u53ca\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4e0e\u9690\u85cf\u5c42\u548c\u8bcd\u6c47\u91cf\u7684\u5173\u7cfb\u3002", "conclusion": "\u6807\u7b7e\u5e73\u6ed1\u662f\u89e3\u51b3\u6821\u51c6\u95ee\u9898\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u5b9a\u5236\u5185\u6838\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u5185\u5b58\u6548\u7387\u3002"}}
{"id": "2508.00420", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00420", "abs": "https://arxiv.org/abs/2508.00420", "authors": ["Rana Salama", "Abdou Youssef", "Mona Diab"], "title": "Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding", "comment": null, "summary": "Wavelets have emerged as a cutting edge technology in a number of fields.\nConcrete results of their application in Image and Signal processing suggest\nthat wavelets can be effectively applied to Natural Language Processing (NLP)\ntasks that capture a variety of linguistic properties. In this paper, we\nleverage the power of applying Discrete Wavelet Transforms (DWT) to word and\nsentence embeddings. We first evaluate, intrinsically and extrinsically, how\nwavelets can effectively be used to consolidate important information in a word\nvector while reducing its dimensionality. We further combine DWT with Discrete\nCosine Transform (DCT) to propose a non-parameterized model that compresses a\nsentence with a dense amount of information in a fixed size vector based on\nlocally varying word features. We show the efficacy of the proposed paradigm on\ndownstream applications models yielding comparable and even superior (in some\ntasks) results to original embeddings.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\uff08DWT\uff09\u5728\u8bcd\u548c\u53e5\u5b50\u5d4c\u5165\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DWT\u548c\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u7684\u975e\u53c2\u6570\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u538b\u7f29\u53e5\u5b50\u4fe1\u606f\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u539f\u59cb\u5d4c\u5165\u7684\u6548\u679c\u3002", "motivation": "\u5c0f\u6ce2\u6280\u672f\u5df2\u5728\u56fe\u50cf\u548c\u4fe1\u53f7\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5176\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5982\u4f55\u901a\u8fc7DWT\u4f18\u5316\u8bcd\u548c\u53e5\u5b50\u5d4c\u5165\u3002", "method": "\u4f7f\u7528DWT\u5bf9\u8bcd\u5411\u91cf\u8fdb\u884c\u964d\u7ef4\u548c\u4fe1\u606f\u6574\u5408\uff0c\u5e76\u7ed3\u5408DCT\u63d0\u51fa\u4e00\u79cd\u975e\u53c2\u6570\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u538b\u7f29\u53e5\u5b50\u4fe1\u606f\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5411\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u90e8\u5206\u4efb\u52a1\u751a\u81f3\u4f18\u4e8e\u539f\u59cb\u5d4c\u5165\u3002", "conclusion": "\u5c0f\u6ce2\u53d8\u6362\u5728NLP\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u901a\u8fc7DWT\u548cDCT\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u538b\u7f29\u4fe1\u606f\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.00500", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00500", "abs": "https://arxiv.org/abs/2508.00500", "authors": ["Haoyu Wang", "Chris M. Poskitt", "Jun Sun", "Jiali Wei"], "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "comment": null, "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.", "AI": {"tldr": "Pro2Guard\u662f\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u53ef\u8fbe\u6027\u5206\u6790\u7684\u4e3b\u52a8\u8fd0\u884c\u65f6\u5b89\u5168\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u548c\u9884\u9632LLM\u4ee3\u7406\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u7684\u54cd\u5e94\u5f0f\u5b89\u5168\u7cfb\u7edf\u7f3a\u4e4f\u9884\u89c1\u6027\uff0c\u96be\u4ee5\u5904\u7406\u957f\u671f\u4f9d\u8d56\u548c\u5206\u5e03\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u4e3b\u52a8\u7684\u65b9\u6cd5\u6765\u9884\u9632\u98ce\u9669\u3002", "method": "Pro2Guard\u5c06\u4ee3\u7406\u884c\u4e3a\u62bd\u8c61\u4e3a\u7b26\u53f7\u72b6\u6001\uff0c\u4ece\u6267\u884c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff08DTMC\uff09\uff0c\u5e76\u5728\u8fd0\u884c\u65f6\u9884\u6d4b\u4e0d\u5b89\u5168\u72b6\u6001\u7684\u6982\u7387\uff0c\u89e6\u53d1\u5e72\u9884\u3002", "result": "\u5728\u5bb6\u5ead\u4ee3\u7406\u548c\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0cPro2Guard\u80fd\u63d0\u524d\u9884\u6d4b\u98ce\u9669\uff0c\u5206\u522b\u963b\u6b6293.6%\u7684\u4e0d\u5b89\u5168\u4efb\u52a1\u548c100%\u7684\u4ea4\u901a\u8fdd\u89c4\u4e0e\u78b0\u649e\u3002", "conclusion": "Pro2Guard\u901a\u8fc7\u4e3b\u52a8\u9884\u6d4b\u548c\u5e72\u9884\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u5e73\u8861\u4efb\u52a1\u5b8c\u6210\u7387\u3002"}}
{"id": "2508.00737", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00737", "abs": "https://arxiv.org/abs/2508.00737", "authors": ["S\u00fceda \u00d6zkaya", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "title": "How LLMs are Shaping the Future of Virtual Reality", "comment": "Pre-print", "summary": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u6e38\u620f\u7684\u878d\u5408\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u53d9\u4e8b\u751f\u6210\u3001NPC\u4e92\u52a8\u3001\u53ef\u8bbf\u95ee\u6027\u7b49\u65b9\u9762\u7684\u5e94\u7528\u4e0e\u6311\u6218\u3002", "motivation": "\u7814\u7a76LLMs\u5982\u4f55\u63d0\u5347VR\u6e38\u620f\u7684\u6c89\u6d78\u611f\u3001\u9002\u5e94\u6027\u548c\u667a\u80fd\u6027\uff0c\u63a8\u52a8\u66f4\u771f\u5b9e\u7684\u6570\u5b57\u4f53\u9a8c\u3002", "method": "\u5206\u6790\u4e862018\u81f32025\u5e74\u95f462\u9879\u540c\u884c\u8bc4\u8bae\u7814\u7a76\uff0c\u603b\u7ed3\u4e86LLMs\u5728VR\u4e2d\u7684\u5173\u952e\u5e94\u7528\u9886\u57df\u548c\u6311\u6218\u3002", "result": "LLMs\u663e\u8457\u63d0\u5347\u4e86VR\u73af\u5883\u7684\u771f\u5b9e\u611f\u3001\u521b\u9020\u529b\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u4f46\u9700\u89e3\u51b3\u5b9e\u65f6\u6027\u80fd\u3001\u4f26\u7406\u7b49\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u591a\u6a21\u6001AI\u3001\u60c5\u611f\u8ba1\u7b97\u7b49\uff0c\u4ee5\u63a8\u52a8\u667a\u80fd\u4e14\u5305\u5bb9\u7684VR\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2508.00270", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00270", "abs": "https://arxiv.org/abs/2508.00270", "authors": ["Robin Schmucker", "Nimish Pachapurkar", "Shanmuga Bala", "Miral Shah", "Tom Mitchell"], "title": "Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring", "comment": null, "summary": "We present an online tutoring system that learns to provide effective\nfeedback to students after they answer questions incorrectly. Using data from\none million students, the system learns which assistance action (e.g., one of\nmultiple hints) to provide for each question to optimize student learning.\nEmploying the multi-armed bandit (MAB) framework and offline policy evaluation,\nwe assess 43,000 assistance actions, and identify trade-offs between assistance\npolicies optimized for different student outcomes (e.g., response correctness,\nsession completion). We design an algorithm that for each question decides on a\nsuitable policy training objective to enhance students' immediate second\nattempt success and overall practice session performance. We evaluate the\nresulting MAB policies in 166,000 practice sessions, verifying significant\nimprovements in student outcomes. While MAB policies optimize feedback for the\noverall student population, we further investigate whether contextual bandit\n(CB) policies can enhance outcomes by personalizing feedback based on\nindividual student features (e.g., ability estimates, response times). Using\ncausal inference, we examine (i) how effects of assistance actions vary across\nstudents and (ii) whether CB policies, which leverage such effect\nheterogeneity, outperform MAB policies. While our analysis reveals that some\nactions for some questions exhibit effect heterogeneity, effect sizes may often\nbe too small for CB policies to provide significant improvements beyond what\nwell-optimized MAB policies that deliver the same action to all students\nalready achieve. We discuss insights gained from deploying data-driven systems\nat scale and implications for future refinements. Today, the teaching policies\noptimized by our system support thousands of students daily.", "AI": {"tldr": "\u5728\u7ebf\u8f85\u5bfc\u7cfb\u7edf\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u4f18\u5316\u5b66\u751f\u53cd\u9988\uff0c\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff0c\u5e76\u63a2\u7d22\u4e2a\u6027\u5316\u53cd\u9988\u7684\u6f5c\u529b\u3002", "motivation": "\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u4f18\u5316\u5b66\u751f\u53cd\u9988\uff0c\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u679c\uff0c\u5e76\u63a2\u7d22\u4e2a\u6027\u5316\u53cd\u9988\u662f\u5426\u80fd\u8fdb\u4e00\u6b65\u6539\u5584\u7ed3\u679c\u3002", "method": "\u4f7f\u7528\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u6846\u67b6\u548c\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\uff0c\u5206\u679043,000\u4e2a\u8f85\u52a9\u52a8\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u7b97\u6cd5\u9009\u62e9\u9002\u5408\u7684\u8bad\u7ec3\u76ee\u6807\u3002\u8fdb\u4e00\u6b65\u7814\u7a76\u4e0a\u4e0b\u6587\u8001\u864e\u673a\uff08CB\uff09\u7b56\u7565\u7684\u4e2a\u6027\u5316\u6548\u679c\u3002", "result": "MAB\u7b56\u7565\u663e\u8457\u63d0\u5347\u5b66\u751f\u8868\u73b0\uff0c\u4f46CB\u7b56\u7565\u7684\u4e2a\u6027\u5316\u6548\u679c\u6709\u9650\uff0c\u6539\u8fdb\u7a7a\u95f4\u8f83\u5c0f\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7cfb\u7edf\u80fd\u6709\u6548\u4f18\u5316\u53cd\u9988\u7b56\u7565\uff0c\u4f46\u4e2a\u6027\u5316\u53cd\u9988\u7684\u6f5c\u529b\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.00429", "categories": ["cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00429", "abs": "https://arxiv.org/abs/2508.00429", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "comment": "17 pages, work in progress", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.", "AI": {"tldr": "ReaGAN\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8282\u70b9\u7ea7\u81ea\u4e3b\u51b3\u7b56\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u89e3\u51b3GNN\u4e2d\u8282\u70b9\u4fe1\u606f\u4e0d\u5e73\u8861\u548c\u5168\u5c40\u8bed\u4e49\u5173\u7cfb\u5ffd\u7565\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfGNN\u7684\u56fa\u5b9a\u805a\u5408\u673a\u5236\u65e0\u6cd5\u5904\u7406\u8282\u70b9\u4fe1\u606f\u4e0d\u5e73\u8861\u548c\u5ffd\u7565\u5168\u5c40\u8bed\u4e49\u5173\u7cfb\u7684\u95ee\u9898\u3002", "method": "ReaGAN\u4e3a\u6bcf\u4e2a\u8282\u70b9\u8d4b\u4e88\u4ee3\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u57fa\u4e8e\u5185\u90e8\u8bb0\u5fc6\u81ea\u4e3b\u89c4\u5212\u884c\u52a8\uff0c\u5e76\u7ed3\u5408RAG\u6280\u672f\u8bbf\u95ee\u5168\u5c40\u8bed\u4e49\u76f8\u5173\u5185\u5bb9\u3002", "result": "ReaGAN\u5728\u5c11\u6837\u672c\u60c5\u5883\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5229\u7528\u51bb\u7ed3\u7684LLM\u9aa8\u5e72\u7f51\u7edc\u5b9e\u73b0\u7ade\u4e89\u6027\u80fd\u3002", "conclusion": "ReaGAN\u5c55\u793a\u4e86\u4ee3\u7406\u89c4\u5212\u548c\u5c40\u90e8-\u5168\u5c40\u68c0\u7d22\u5728\u56fe\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00576", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00576", "abs": "https://arxiv.org/abs/2508.00576", "authors": ["Zhanliang Wang", "Kai Wang"], "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models", "comment": null, "summary": "Multimodal AI models have achieved impressive performance in tasks that\nrequire integrating information from multiple modalities, such as vision and\nlanguage. However, their \"black-box\" nature poses a major barrier to deployment\nin high-stakes applications where interpretability and trustworthiness are\nessential. How to explain cross-modal interactions in multimodal AI models\nremains a major challenge. While existing model explanation methods, such as\nattention map and Grad-CAM, offer coarse insights into cross-modal\nrelationships, they cannot precisely quantify the synergistic effects between\nmodalities, and are limited to open-source models with accessible internal\nweights. Here we introduce MultiSHAP, a model-agnostic interpretability\nframework that leverages the Shapley Interaction Index to attribute multimodal\npredictions to pairwise interactions between fine-grained visual and textual\nelements (such as image patches and text tokens), while being applicable to\nboth open- and closed-source models. Our approach provides: (1) instance-level\nexplanations that reveal synergistic and suppressive cross-modal effects for\nindividual samples - \"why the model makes a specific prediction on this input\",\nand (2) dataset-level explanation that uncovers generalizable interaction\npatterns across samples - \"how the model integrates information across\nmodalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP\nfaithfully captures cross-modal reasoning mechanisms, while real-world case\nstudies demonstrate its practical utility. Our framework is extensible beyond\ntwo modalities, offering a general solution for interpreting complex multimodal\nAI models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMultiSHAP\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u591a\u6a21\u6001AI\u6a21\u578b\u4e2d\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u9002\u7528\u4e8e\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u63d0\u4f9b\u5b9e\u4f8b\u7ea7\u548c\u6570\u636e\u96c6\u7ea7\u89e3\u91ca\u3002", "motivation": "\u591a\u6a21\u6001AI\u6a21\u578b\u7684\u2018\u9ed1\u76d2\u2019\u7279\u6027\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u786e\u91cf\u5316\u6a21\u6001\u95f4\u7684\u534f\u540c\u6548\u5e94\u3002", "method": "\u5f15\u5165MultiSHAP\u6846\u67b6\uff0c\u5229\u7528Shapley Interaction Index\u91cf\u5316\u89c6\u89c9\u548c\u6587\u672c\u5143\u7d20\u95f4\u7684\u4ea4\u4e92\u6548\u5e94\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1MultiSHAP\u80fd\u51c6\u786e\u6355\u6349\u8de8\u6a21\u6001\u63a8\u7406\u673a\u5236\uff0c\u5b9e\u9645\u6848\u4f8b\u5c55\u793a\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "MultiSHAP\u4e3a\u590d\u6742\u591a\u6a21\u6001AI\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u89e3\u91ca\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u8d85\u8fc7\u4e24\u79cd\u6a21\u6001\u3002"}}
{"id": "2508.00665", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00665", "abs": "https://arxiv.org/abs/2508.00665", "authors": ["Maryam Mosleh", "Marie Devlin", "Ellis Solaiman"], "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI", "comment": null, "summary": "Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u7edfXAI\u6280\u672f\u4e0e\u751f\u6210\u5f0fAI\u6a21\u578b\u548c\u7528\u6237\u4e2a\u6027\u5316\uff0c\u4ee5\u751f\u6210\u591a\u6a21\u6001\u3001\u4e2a\u6027\u5316\u7684\u89e3\u91ca\u3002", "motivation": "\u5f53\u524dAI\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u7cfb\u7edf\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u4e14\u5927\u591a\u6570XAI\u6280\u672f\u5ffd\u89c6\u7528\u6237\u89d2\u8272\u548c\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff0c\u6574\u5408\u4f20\u7edfXAI\u6280\u672f\u3001\u751f\u6210\u5f0fAI\u6a21\u578b\u548c\u7528\u6237\u4e2a\u6027\u5316\u3002", "result": "\u6846\u67b6\u65e8\u5728\u63d0\u4f9b\u52a8\u6001\u3001\u4e2a\u6027\u5316\u7684\u89e3\u91ca\uff0c\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "\u76ee\u6807\u662f\u63a8\u52a8\u53ef\u89e3\u91caAI\uff0c\u65e2\u63d0\u5347\u900f\u660e\u5ea6\u53c8\u652f\u6301\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4f53\u9a8c\u3002"}}
{"id": "2508.00286", "categories": ["cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00286", "abs": "https://arxiv.org/abs/2508.00286", "authors": ["Mohsen Zaker Esteghamati"], "title": "Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem", "comment": null, "summary": "This study presents a methodology to treat performance-based seismic design\nas an inverse engineering problem, where design parameters are directly derived\nto achieve specific performance objectives. By implementing explainable machine\nlearning models, this methodology directly maps design variables and\nperformance metrics, tackling computational inefficiencies of performance-based\ndesign. The resultant machine learning model is integrated as an evaluation\nfunction into a genetic optimization algorithm to solve the inverse problem.\nThe developed methodology is then applied to two different inventories of steel\nand concrete moment frames in Los Angeles and Charleston to obtain sectional\nproperties of frame members that minimize expected annualized seismic loss in\nterms of repair costs. The results show high accuracy of the surrogate models\n(e.g., R2> 90%) across a diverse set of building types, geometries, seismic\ndesign, and site hazard, where the optimization algorithm could identify the\noptimum values of members' properties for a fixed set of geometric variables,\nconsistent with engineering principles.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u7684\u6027\u80fd\u5316\u6297\u9707\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u76f4\u63a5\u63a8\u5bfc\u8bbe\u8ba1\u53c2\u6570\u4ee5\u6ee1\u8db3\u7279\u5b9a\u6027\u80fd\u76ee\u6807\uff0c\u5e76\u7ed3\u5408\u9057\u4f20\u4f18\u5316\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3\u6027\u80fd\u5316\u6297\u9707\u8bbe\u8ba1\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u76f4\u63a5\u6620\u5c04\u8bbe\u8ba1\u53d8\u91cf\u4e0e\u6027\u80fd\u6307\u6807\u3002", "method": "\u4f7f\u7528\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6620\u5c04\u8bbe\u8ba1\u53d8\u91cf\u4e0e\u6027\u80fd\u6307\u6807\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u8bc4\u4f30\u51fd\u6570\u96c6\u6210\u5230\u9057\u4f20\u4f18\u5316\u7b97\u6cd5\u4e2d\u3002", "result": "\u5728\u6d1b\u6749\u77f6\u548c\u67e5\u5c14\u65af\u987f\u7684\u94a2\u548c\u6df7\u51dd\u571f\u6846\u67b6\u7ed3\u6784\u4e2d\u5e94\u7528\uff0c\u6a21\u578b\u7cbe\u5ea6\u9ad8\uff08R2>90%\uff09\uff0c\u4f18\u5316\u7b97\u6cd5\u80fd\u8bc6\u522b\u7b26\u5408\u5de5\u7a0b\u539f\u7406\u7684\u6784\u4ef6\u6700\u4f18\u5c5e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u5efa\u7b51\u7c7b\u578b\u548c\u5730\u9707\u573a\u666f\uff0c\u4e3a\u6027\u80fd\u5316\u6297\u9707\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.00454", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00454", "abs": "https://arxiv.org/abs/2508.00454", "authors": ["Yuqi Tang", "Kehua Feng", "Yunfeng Wang", "Zhiwen Chen", "Chengfei Lv", "Gang Yu", "Qiang Zhang", "Keyan Ding"], "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges", "comment": "15 pages, 2 pages, under review at AAAI 2026", "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u591a\u4e2aLLM\u8bc4\u59d4\u7684\u504f\u597d\u77e5\u8bc6\u805a\u5408\u5230\u4e00\u4e2a\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bc4\u4f30\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u591a\u6837\u6027\u53cd\u9988\u7684\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u4f9d\u8d56\u5355\u4e00LLM\u4f5c\u4e3a\u8bc4\u59d4\u7684\u65b9\u6cd5\u5b58\u5728\u504f\u89c1\uff0c\u5f71\u54cd\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002\u591a\u8bc4\u59d4\u65b9\u6cd5\u867d\u6709\u6548\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u5668\uff0c\u5c06\u591a\u4e2aLLM\u8bc4\u59d4\u7684\u504f\u597d\u77e5\u8bc6\u805a\u5408\u5230\u5355\u4e00\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u4e03\u4e2a\u5bf9\u8bdd\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u8bc4\u4f30\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u591a\u8bc4\u59d4\u53cd\u9988\u7684\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u5feb\u901f\u7075\u6d3b\u7684\u5bf9\u8bdd\u8d28\u91cf\u8bc4\u4f30\u3002"}}
{"id": "2508.00581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00581", "abs": "https://arxiv.org/abs/2508.00581", "authors": ["Ruiqing Ding", "Qianfang Sun", "Yongkang Leng", "Hui Yin", "Xiaojian Li"], "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation", "comment": "16 pages, 10 figures", "summary": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5LLM\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u590d\u6742\u7535\u5b50\u75c5\u5386\u751f\u6210\u5168\u9762\u7684\u9884\u54a8\u8be2\u95ee\u5377\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5LLM\u65b9\u6cd5\u5728\u4fe1\u606f\u5b8c\u6574\u6027\u3001\u903b\u8f91\u987a\u5e8f\u548c\u75be\u75c5\u7ea7\u5408\u6210\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u9884\u54a8\u8be2\u662f\u533b\u7597\u4fdd\u5065\u7684\u5173\u952e\u73af\u8282\uff0c\u4f46\u76f4\u63a5\u4ece\u590d\u6742\u7535\u5b50\u75c5\u5386\u751f\u6210\u95ee\u5377\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u4fe1\u606f\u5b8c\u6574\u6027\u548c\u903b\u8f91\u6027\u95ee\u9898\u3002", "method": "\u5206\u4e09\u9636\u6bb5\uff1a1) \u63d0\u53d6\u539f\u5b50\u65ad\u8a00\uff1b2) \u6784\u5efa\u4e2a\u4eba\u56e0\u679c\u7f51\u7edc\u5e76\u5408\u6210\u75be\u75c5\u77e5\u8bc6\uff1b3) \u751f\u6210\u4e2a\u6027\u5316\u95ee\u5377\u3002", "result": "\u5728\u771f\u5b9e\u7535\u5b50\u75c5\u5386\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8868\u73b0\u4f18\u4e8e\u76f4\u63a5\u65b9\u6cd5\uff0c\u4fe1\u606f\u8986\u76d6\u3001\u8bca\u65ad\u76f8\u5173\u6027\u3001\u53ef\u7406\u89e3\u6027\u548c\u751f\u6210\u65f6\u95f4\u5747\u66f4\u4f18\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u663e\u5f0f\u4e34\u5e8a\u77e5\u8bc6\uff0c\u63d0\u5347\u4e86\u9884\u54a8\u8be2\u95ee\u5377\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.00674", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00674", "abs": "https://arxiv.org/abs/2508.00674", "authors": ["Banan Alkhateeb", "Ellis Solaiman"], "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations", "comment": null, "summary": "Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u5206\u6bb5\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u91ca\u7cfb\u7edf\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u65b9\u6cd5\u89e3\u51b3\u793e\u4ea4\u5a92\u4f53\u63a8\u8350\u7f3a\u4e4f\u4e2a\u6027\u5316\u89e3\u91ca\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u793e\u4ea4\u5a92\u4f53\u63a8\u8350\u7cfb\u7edf\u7684\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u7684\u9700\u6c42\uff0c\u5bfc\u81f4\u63a8\u8350\u4ef7\u503c\u964d\u4f4e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u89e3\u91ca\u7cfb\u7edf\uff0c\u6839\u636e\u7528\u6237\u9700\u6c42\u548c\u4e0a\u4e0b\u6587\u63d0\u4f9b\u4e0d\u540c\u5f62\u5f0f\u7684\u89e3\u91ca\uff0c\u5305\u62ec\u6280\u672f\u8be6\u7ec6\u7248\u548c\u7b80\u5316\u7248\u3002", "result": "\u7cfb\u7edf\u9996\u6b21\u5728\u4e00\u4e2a\u6d41\u7a0b\u4e2d\u540c\u65f6\u8c03\u6574\u89e3\u91ca\u98ce\u683c\uff08\u89c6\u89c9vs\u6570\u5b57\uff09\u548c\u7c92\u5ea6\uff08\u4e13\u5bb6vs\u666e\u901a\u7528\u6237\uff09\u3002", "conclusion": "\u901a\u8fc730\u540dX\u7528\u6237\u7684\u516c\u5f00\u8bd5\u70b9\u9a8c\u8bc1\u5176\u5bf9\u51b3\u7b56\u548c\u4fe1\u4efb\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.00304", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00304", "abs": "https://arxiv.org/abs/2508.00304", "authors": ["Tianyin Liao", "Ziwei Zhang", "Yufei Sun", "Chunyu Hu", "Jianxin Li"], "title": "Invariant Graph Transformer for Out-of-Distribution Generalization", "comment": null, "summary": "Graph Transformers (GTs) have demonstrated great effectiveness across various\ngraph analytical tasks. However, the existing GTs focus on training and testing\ngraph data originated from the same distribution, but fail to generalize under\ndistribution shifts. Graph invariant learning, aiming to capture generalizable\ngraph structural patterns with labels under distribution shifts, is potentially\na promising solution, but how to design attention mechanisms and positional and\nstructural encodings (PSEs) based on graph invariant learning principles\nremains challenging. To solve these challenges, we introduce Graph\nOut-Of-Distribution generalized Transformer (GOODFormer), aiming to learn\ngeneralized graph representations by capturing invariant relationships between\npredictive graph structures and labels through jointly optimizing three\nmodules. Specifically, we first develop a GT-based entropy-guided invariant\nsubgraph disentangler to separate invariant and variant subgraphs while\npreserving the sharpness of the attention function. Next, we design an evolving\nsubgraph positional and structural encoder to effectively and efficiently\ncapture the encoding information of dynamically changing subgraphs during\ntraining. Finally, we propose an invariant learning module utilizing subgraph\nnode representations and encodings to derive generalizable graph\nrepresentations that can to unseen graphs. We also provide theoretical\njustifications for our method. Extensive experiments on benchmark datasets\ndemonstrate the superiority of our method over state-of-the-art baselines under\ndistribution shifts.", "AI": {"tldr": "GOODFormer\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u4e0d\u53d8\u5b66\u4e60\u7684Transformer\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u56fe\u6570\u636e\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56feTransformer\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u56fe\u4e0d\u53d8\u5b66\u4e60\u53ef\u80fd\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\u8054\u5408\u4f18\u5316\uff1a\u71b5\u5f15\u5bfc\u7684\u4e0d\u53d8\u5b50\u56fe\u89e3\u8026\u5668\u3001\u52a8\u6001\u5b50\u56fe\u7f16\u7801\u5668\u3001\u4e0d\u53d8\u5b66\u4e60\u6a21\u5757\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GOODFormer\u80fd\u6709\u6548\u5b66\u4e60\u6cdb\u5316\u56fe\u8868\u793a\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u504f\u79fb\u573a\u666f\u3002"}}
{"id": "2508.00476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00476", "abs": "https://arxiv.org/abs/2508.00476", "authors": ["Jeongwoo Kang", "Markarit Vartampetian", "Felix Herron", "Yongxin Zhou", "Diandra Fabre", "Gabriela Gonzalez-Saez"], "title": "GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts", "comment": null, "summary": "This paper documents GETALP's submission to the Third Run of the Automatic\nMinuting Shared Task at SIGDial 2025. We participated in Task B:\nquestion-answering based on meeting transcripts. Our method is based on a\nretrieval augmented generation (RAG) system and Abstract Meaning\nRepresentations (AMR). We propose three systems combining these two approaches.\nOur results show that incorporating AMR leads to high-quality responses for\napproximately 35% of the questions and provides notable improvements in\nanswering questions that involve distinguishing between different participants\n(e.g., who questions).", "AI": {"tldr": "GETALP\u56e2\u961f\u5728SIGDial 2025\u7684\u81ea\u52a8\u4f1a\u8bae\u7eaa\u8981\u4efb\u52a1\u4e2d\u63d0\u4ea4\u4e86\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u62bd\u8c61\u610f\u4e49\u8868\u793a\uff08AMR\uff09\u7684\u7cfb\u7edf\uff0c\u7ed3\u679c\u663e\u793aAMR\u663e\u8457\u63d0\u5347\u4e86\u90e8\u5206\u95ee\u9898\u7684\u56de\u7b54\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u7ed3\u5408RAG\u548cAMR\u6280\u672f\u63d0\u5347\u4f1a\u8bae\u7eaa\u8981\u4e2d\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6d89\u53ca\u533a\u5206\u4e0d\u540c\u53c2\u4e0e\u8005\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u7ed3\u5408RAG\u548cAMR\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u57fa\u4e8e\u4f1a\u8bae\u8f6c\u5f55\u7684\u95ee\u7b54\u4efb\u52a1\u3002", "result": "AMR\u663e\u8457\u63d0\u5347\u4e86\u7ea635%\u95ee\u9898\u7684\u56de\u7b54\u8d28\u91cf\uff0c\u5e76\u5728\u533a\u5206\u53c2\u4e0e\u8005\u7684\u95ee\u9898\uff08\u5982\u201c\u8c01\u201d\u95ee\u9898\uff09\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u7ed3\u5408RAG\u548cAMR\u7684\u65b9\u6cd5\u5728\u4f1a\u8bae\u7eaa\u8981\u95ee\u7b54\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u95ee\u9898\u65f6\u3002"}}
{"id": "2508.00632", "categories": ["cs.AI", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00632", "abs": "https://arxiv.org/abs/2508.00632", "authors": ["Alexia Jolicoeur-Martineau"], "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings", "comment": null, "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AVR-Eval\u8bc4\u4f30\u6307\u6807\u548cAVR-Agent\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u548c\u8bc4\u4f30\u4ea4\u4e92\u5f0f\u97f3\u89c6\u9891\u5185\u5bb9\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u5229\u7528\u9ad8\u8d28\u91cf\u8d44\u6e90\u548c\u53cd\u9988\u65b9\u9762\u4e0e\u4eba\u7c7b\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u89e3\u51b3AI\u751f\u6210\u590d\u6742\u4ea4\u4e92\u5f0f\u97f3\u89c6\u9891\u5185\u5bb9\uff08\u5982\u89c6\u9891\u6e38\u620f\uff09\u65f6\u7f3a\u4e4f\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u548c\u96be\u4ee5\u5904\u7406\u590d\u6742\u5185\u5bb9\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faAVR-Eval\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6a21\u578b\u6bd4\u8f83\u5185\u5bb9\u8d28\u91cf\uff1b\u5f00\u53d1AVR-Agent\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u751f\u6210\u5e76\u8fed\u4ee3\u4f18\u5316JavaScript\u4ee3\u7801\u3002", "result": "AVR-Agent\u751f\u6210\u7684\u5185\u5bb9\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u6b21\u751f\u6210\u7684\u5185\u5bb9\uff0c\u4f46\u6a21\u578b\u672a\u80fd\u6709\u6548\u5229\u7528\u5b9a\u5236\u8d44\u6e90\u548c\u53cd\u9988\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u5229\u7528\u9ad8\u8d28\u91cf\u8d44\u6e90\u548c\u53cd\u9988\u65b9\u9762\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\u673a\u5668\u4e0e\u4eba\u7c7b\u5185\u5bb9\u521b\u4f5c\u7684\u6839\u672c\u5dee\u5f02\u3002"}}
{"id": "2508.00325", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.00325", "abs": "https://arxiv.org/abs/2508.00325", "authors": ["Yongquan Qu", "Matthieu Blanke", "Sara Shamekh", "Pierre Gentine"], "title": "PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models", "comment": null, "summary": "Earth system modeling presents a fundamental challenge in scientific\ncomputing: capturing complex, multiscale nonlinear dynamics in computationally\nefficient models while minimizing forecast errors caused by necessary\nsimplifications. Even the most powerful AI- or physics-based forecast system\nsuffer from gradual error accumulation. Data assimilation (DA) aims to mitigate\nthese errors by optimally blending (noisy) observations with prior model\nforecasts, but conventional variational methods often assume Gaussian error\nstatistics that fail to capture the true, non-Gaussian behavior of chaotic\ndynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates\n(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance\nmisfit on new observations) with (2) a single forward pass through a pretrained\ngenerative prior conditioned on the background forecast via a conditional\nWasserstein coupling. This strategy relaxes restrictive statistical assumptions\nand leverages rich historical data without requiring an explicit regularization\nfunctional, and it also avoids the need to backpropagate gradients through the\ncomplex neural network that encodes the prior during assimilation cycles.\nExperiments on standard chaotic testbeds demonstrate that this strategy\nconsistently reduces forecast errors across a range of observation sparsities\nand noise levels, outperforming classical variational methods.", "AI": {"tldr": "PnP-DA\u662f\u4e00\u79cd\u63d2\u62d4\u5f0f\u6570\u636e\u540c\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7\u68af\u5ea6\u66f4\u65b0\u548c\u9884\u8bad\u7ec3\u751f\u6210\u5148\u9a8c\uff0c\u51cf\u5c11\u5730\u7403\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\uff0c\u4f18\u4e8e\u4f20\u7edf\u53d8\u5206\u65b9\u6cd5\u3002", "motivation": "\u5730\u7403\u7cfb\u7edf\u5efa\u6a21\u4e2d\uff0c\u590d\u6742\u591a\u5c3a\u5ea6\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u8bef\u5dee\u7d2f\u79ef\u662f\u4e3b\u8981\u6311\u6218\uff0c\u4f20\u7edf\u53d8\u5206\u65b9\u6cd5\u7684\u9ad8\u65af\u8bef\u5dee\u5047\u8bbe\u4e0d\u9002\u7528\u4e8e\u6df7\u6c8c\u7cfb\u7edf\u3002", "method": "PnP-DA\u4ea4\u66ff\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u5206\u6790\u66f4\u65b0\u548c\u9884\u8bad\u7ec3\u751f\u6210\u5148\u9a8c\uff0c\u907f\u514d\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u7684\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5728\u6807\u51c6\u6df7\u6c8c\u6d4b\u8bd5\u4e2d\uff0cPnP-DA\u663e\u8457\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "PnP-DA\u901a\u8fc7\u653e\u677e\u7edf\u8ba1\u5047\u8bbe\u548c\u5229\u7528\u5386\u53f2\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u540c\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00489", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00489", "abs": "https://arxiv.org/abs/2508.00489", "authors": ["Yixuan Tang", "Jincheng Wang", "Anthony K. H. Tung"], "title": "The Missing Parts: Augmenting Fact Verification with Half-Truth Detection", "comment": null, "summary": "Fact verification systems typically assess whether a claim is supported by\nretrieved evidence, assuming that truthfulness depends solely on what is\nstated. However, many real-world claims are half-truths, factually correct yet\nmisleading due to the omission of critical context. Existing models struggle\nwith such cases, as they are not designed to reason about what is left unsaid.\nWe introduce the task of half-truth detection, and propose PolitiFact-Hidden, a\nnew benchmark with 15k political claims annotated with sentence-level evidence\nalignment and inferred claim intent. To address this challenge, we present\nTRACER, a modular re-assessment framework that identifies omission-based\nmisinformation by aligning evidence, inferring implied intent, and estimating\nthe causal impact of hidden content. TRACER can be integrated into existing\nfact-checking pipelines and consistently improves performance across multiple\nstrong baselines. Notably, it boosts Half-True classification F1 by up to 16\npoints, highlighting the importance of modeling omissions for trustworthy fact\nverification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u534a\u771f\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86PolitiFact-Hidden\u57fa\u51c6\u548cTRACER\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u56e0\u9057\u6f0f\u5173\u952e\u4fe1\u606f\u800c\u5bfc\u81f4\u7684\u8bef\u5bfc\u6027\u58f0\u660e\u3002", "motivation": "\u73b0\u6709\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u65e0\u6cd5\u5904\u7406\u534a\u771f\u58f0\u660e\uff08\u90e8\u5206\u6b63\u786e\u4f46\u56e0\u9057\u6f0f\u5173\u952e\u4fe1\u606f\u800c\u8bef\u5bfc\uff09\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u68c0\u6d4b\u6b64\u7c7b\u60c5\u51b5\u3002", "method": "\u63d0\u51faTRACER\u6846\u67b6\uff0c\u901a\u8fc7\u8bc1\u636e\u5bf9\u9f50\u3001\u63a8\u65ad\u9690\u542b\u610f\u56fe\u548c\u8bc4\u4f30\u9690\u85cf\u5185\u5bb9\u7684\u5f71\u54cd\u6765\u8bc6\u522b\u9057\u6f0f\u578b\u8bef\u5bfc\u4fe1\u606f\u3002", "result": "TRACER\u663e\u8457\u63d0\u5347\u4e86\u534a\u771f\u5206\u7c7b\u7684F1\u5206\u6570\uff08\u6700\u9ad816\u5206\uff09\uff0c\u5e76\u53ef\u4e0e\u73b0\u6709\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u96c6\u6210\u3002", "conclusion": "\u5efa\u6a21\u9057\u6f0f\u4fe1\u606f\u5bf9\u53ef\u4fe1\u4e8b\u5b9e\u6838\u67e5\u81f3\u5173\u91cd\u8981\uff0cTRACER\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00658", "categories": ["cs.AI", "cs.LG", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.00658", "abs": "https://arxiv.org/abs/2508.00658", "authors": ["Chakattrai Sookkongwaree", "Tattep Lakmuang", "Chainarong Amornbunchornvej"], "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies", "comment": "First draft", "summary": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9891\u5e26\u53d8\u6ede\u540e\u683c\u5170\u6770\u56e0\u679c\u6027\uff08MB-VLGC\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u9891\u7387\u4f9d\u8d56\u6027\u56e0\u679c\u5ef6\u8fdf\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\uff0c\u56e0\u679c\u5173\u7cfb\u7684\u6ede\u540e\u65f6\u95f4\u53ef\u80fd\u56e0\u9891\u7387\u4e0d\u540c\u800c\u53d8\u5316\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u8fd9\u4e00\u70b9\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5f62\u5f0f\u5316MB-VLGC\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5efa\u6a21\u9891\u7387\u4f9d\u8d56\u6027\u56e0\u679c\u5ef6\u8fdf\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9ad8\u6548\u63a8\u7406\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMB-VLGC\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MB-VLGC\u6846\u67b6\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u7c7b\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002"}}
{"id": "2508.00331", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00331", "abs": "https://arxiv.org/abs/2508.00331", "authors": ["George Wang", "Garrett Baker", "Andrew Gordon", "Daniel Murfet"], "title": "Embryology of a Language Model", "comment": null, "summary": "Understanding how language models develop their internal computational\nstructure is a central problem in the science of deep learning. While\nsusceptibilities, drawn from statistical physics, offer a promising analytical\ntool, their full potential for visualizing network organization remains\nuntapped. In this work, we introduce an embryological approach, applying UMAP\nto the susceptibility matrix to visualize the model's structural development\nover training. Our visualizations reveal the emergence of a clear ``body\nplan,'' charting the formation of known features like the induction circuit and\ndiscovering previously unknown structures, such as a ``spacing fin'' dedicated\nto counting space tokens. This work demonstrates that susceptibility analysis\ncan move beyond validation to uncover novel mechanisms, providing a powerful,\nholistic lens for studying the developmental principles of complex neural\nnetworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eUMAP\u548c\u654f\u611f\u6027\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u53ef\u89c6\u5316\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u53d1\u5c55\uff0c\u63ed\u793a\u4e86\u65b0\u7684\u7f51\u7edc\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\u7ed3\u6784\u7684\u5f62\u6210\u673a\u5236\uff0c\u63a2\u7d22\u654f\u611f\u6027\u5206\u6790\u5728\u53ef\u89c6\u5316\u7f51\u7edc\u7ec4\u7ec7\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5e94\u7528UMAP\u5bf9\u654f\u611f\u6027\u77e9\u9635\u8fdb\u884c\u5206\u6790\uff0c\u53ef\u89c6\u5316\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u53d1\u5c55\u3002", "result": "\u53d1\u73b0\u4e86\u6e05\u6670\u7684\u201c\u8eab\u4f53\u8ba1\u5212\u201d\uff0c\u5305\u62ec\u5df2\u77e5\u7684\u611f\u5e94\u7535\u8def\u548c\u65b0\u7684\u7ed3\u6784\uff08\u5982\u201c\u95f4\u8ddd\u9ccd\u201d\uff09\u3002", "conclusion": "\u654f\u611f\u6027\u5206\u6790\u4e0d\u4ec5\u80fd\u9a8c\u8bc1\u6a21\u578b\uff0c\u8fd8\u80fd\u63ed\u793a\u65b0\u673a\u5236\uff0c\u4e3a\u7814\u7a76\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u80b2\u539f\u7406\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2508.00522", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00522", "abs": "https://arxiv.org/abs/2508.00522", "authors": ["Jiaxin Deng", "Qingcheng Zhu", "Junbiao Pang", "Linlin Yang", "Zhongqian Fu", "Baochang Zhang"], "title": "EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond", "comment": null, "summary": "Little research explores the correlation between the expressive ability and\ngeneralization ability of the low-rank adaptation (LoRA). Sharpness-Aware\nMinimization (SAM) improves model generalization for both Convolutional Neural\nNetworks (CNNs) and Transformers by encouraging convergence to locally flat\nminima. However, the connection between sharpness and generalization has not\nbeen fully explored for LoRA due to the lack of tools to either empirically\nseek flat minima or develop theoretical methods. In this work, we propose\nFlat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for\nLoRA. Concretely, we theoretically demonstrate that perturbations in the full\nparameter space can be transferred to the low-rank subspace. This approach\neliminates the potential interference introduced by perturbations across\nmultiple matrices in the low-rank subspace. Our extensive experiments on large\nlanguage models and vision-language models demonstrate that EFlat-LoRA achieves\noptimize efficiency comparable to that of LoRA while simultaneously attaining\ncomparable or even better performance. For example, on the GLUE dataset with\nRoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and\n0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat\nshows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,\nrespectively. These empirical results also verify that the generalization of\nLoRA is closely related to sharpness, which is omitted by previous methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFlat-LoRA\u548cEFlat-LoRA\uff0c\u901a\u8fc7\u5bfb\u627e\u5e73\u5766\u6700\u5c0f\u503c\u63d0\u5347LoRA\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8eLoRA\u548c\u5168\u5fae\u8c03\u3002", "motivation": "\u63a2\u7d22LoRA\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u586b\u8865\u73b0\u6709\u65b9\u6cd5\u5728\u5e73\u5766\u6700\u5c0f\u503c\u4e0e\u6cdb\u5316\u80fd\u529b\u8054\u7cfb\u4e0a\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51faFlat-LoRA\u548cEFlat-LoRA\uff0c\u7406\u8bba\u8bc1\u660e\u5168\u53c2\u6570\u7a7a\u95f4\u6270\u52a8\u53ef\u8f6c\u79fb\u5230\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u907f\u514d\u591a\u77e9\u9635\u5e72\u6270\u3002", "result": "EFlat-LoRA\u5728GLUE\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8eLoRA\u548c\u5168\u5fae\u8c03\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "LoRA\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u5e73\u5766\u6027\u5bc6\u5207\u76f8\u5173\uff0cEFlat-LoRA\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.00350", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00350", "abs": "https://arxiv.org/abs/2508.00350", "authors": ["Qilin Liao", "Shuo Yang", "Bo Zhao", "Ping Luo", "Hengshuang Zhao"], "title": "BOOD: Boundary-based Out-Of-Distribution Data Generation", "comment": "14 pages, 8 figures, To be published in the Proceedings of the\n  International Conference on Machine Learning (ICML) 2025", "summary": "Harnessing the power of diffusion models to synthesize auxiliary training\ndata based on latent space features has proven effective in enhancing\nout-of-distribution (OOD) detection performance. However, extracting effective\nfeatures outside the in-distribution (ID) boundary in latent space remains\nchallenging due to the difficulty of identifying decision boundaries between\nclasses. This paper proposes a novel framework called Boundary-based\nOut-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD\nfeatures and generates human-compatible outlier images using diffusion models.\nBOOD first learns a text-conditioned latent feature space from the ID dataset,\nselects ID features closest to the decision boundary, and perturbs them to\ncross the decision boundary to form OOD features. These synthetic OOD features\nare then decoded into images in pixel space by a diffusion model. Compared to\nprevious works, BOOD provides a more training efficient strategy for\nsynthesizing informative OOD features, facilitating clearer distinctions\nbetween ID and OOD data. Extensive experimental results on common benchmarks\ndemonstrate that BOOD surpasses the state-of-the-art method significantly,\nachieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%\nimprovement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.", "AI": {"tldr": "BOOD\u6846\u67b6\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684OOD\u7279\u5f81\u548c\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347OOD\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u63d0\u53d6ID\u8fb9\u754c\u5916\u7684\u6709\u6548\u7279\u5f81\uff0cBOOD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "BOOD\u901a\u8fc7\u5b66\u4e60\u6587\u672c\u6761\u4ef6\u6f5c\u5728\u7a7a\u95f4\uff0c\u9009\u62e9\u63a5\u8fd1\u51b3\u7b56\u8fb9\u754c\u7684ID\u7279\u5f81\u5e76\u6270\u52a8\u751f\u6210OOD\u7279\u5f81\uff0c\u518d\u7528\u6269\u6563\u6a21\u578b\u89e3\u7801\u4e3a\u56fe\u50cf\u3002", "result": "\u5728CIFAR-100\u4e0a\uff0cBOOD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cFPR95\u964d\u4f4e29.64%\uff0cAUROC\u63d0\u53477.27%\u3002", "conclusion": "BOOD\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u751f\u6210OOD\u7279\u5f81\u7684\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.00537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00537", "abs": "https://arxiv.org/abs/2508.00537", "authors": ["Giulio Zhou", "Tsz Kin Lam", "Alexandra Birch", "Barry Haddow"], "title": "The Prosody of Emojis", "comment": null, "summary": "Prosodic features such as pitch, timing, and intonation are central to spoken\ncommunication, conveying emotion, intent, and discourse structure. In\ntext-based settings, where these cues are absent, emojis act as visual\nsurrogates that add affective and pragmatic nuance. This study examines how\nemojis influence prosodic realisation in speech and how listeners interpret\nprosodic cues to recover emoji meanings. Unlike previous work, we directly link\nprosody and emoji by analysing actual human speech data, collected through\nstructured but open-ended production and perception tasks. This provides\nempirical evidence of how emoji semantics shape spoken delivery and perception.\nResults show that speakers adapt their prosody based on emoji cues, listeners\ncan often identify the intended emoji from prosodic variation alone, and\ngreater semantic differences between emojis correspond to increased prosodic\ndivergence. These findings suggest that emojis can act as meaningful carriers\nof prosodic intent, offering insight into their communicative role in digitally\nmediated contexts.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8868\u60c5\u7b26\u53f7\u5982\u4f55\u5f71\u54cd\u8bed\u97f3\u4e2d\u7684\u97f5\u5f8b\u5b9e\u73b0\uff0c\u4ee5\u53ca\u542c\u8005\u5982\u4f55\u901a\u8fc7\u97f5\u5f8b\u7ebf\u7d22\u7406\u89e3\u8868\u60c5\u7b26\u53f7\u7684\u542b\u4e49\u3002", "motivation": "\u63a2\u7d22\u5728\u6587\u672c\u4ea4\u6d41\u4e2d\u7f3a\u5931\u7684\u97f5\u5f8b\u7279\u5f81\uff08\u5982\u97f3\u9ad8\u3001\u8282\u594f\u3001\u8bed\u8c03\uff09\u5982\u4f55\u901a\u8fc7\u8868\u60c5\u7b26\u53f7\u5728\u8bed\u97f3\u4e2d\u4f53\u73b0\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u4f46\u5f00\u653e\u5f0f\u7684\u751f\u4ea7\u548c\u611f\u77e5\u4efb\u52a1\u6536\u96c6\u4eba\u7c7b\u8bed\u97f3\u6570\u636e\uff0c\u76f4\u63a5\u5206\u6790\u97f5\u5f8b\u4e0e\u8868\u60c5\u7b26\u53f7\u7684\u8054\u7cfb\u3002", "result": "\u8bf4\u8bdd\u8005\u4f1a\u6839\u636e\u8868\u60c5\u7b26\u53f7\u8c03\u6574\u97f5\u5f8b\uff0c\u542c\u8005\u80fd\u901a\u8fc7\u97f5\u5f8b\u53d8\u5316\u8bc6\u522b\u8868\u60c5\u7b26\u53f7\uff0c\u8bed\u4e49\u5dee\u5f02\u8d8a\u5927\uff0c\u97f5\u5f8b\u5dee\u5f02\u8d8a\u660e\u663e\u3002", "conclusion": "\u8868\u60c5\u7b26\u53f7\u53ef\u4ee5\u4f5c\u4e3a\u97f5\u5f8b\u610f\u56fe\u7684\u6709\u610f\u4e49\u8f7d\u4f53\uff0c\u63ed\u793a\u4e86\u5176\u5728\u6570\u5b57\u5a92\u4ecb\u4e2d\u7684\u4ea4\u9645\u4f5c\u7528\u3002"}}
{"id": "2508.00357", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00357", "abs": "https://arxiv.org/abs/2508.00357", "authors": ["Yoonhyuk Choi", "Jiho Choi", "Chong-Kwon Kim"], "title": "Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization", "comment": null, "summary": "Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct\nnode features, particularly on heterophilic graphs where adjacent nodes often\nhave dissimilar labels. Although sheaf neural networks partially mitigate this\nproblem, they typically rely on static or heavily parameterized sheaf\nstructures that hinder generalization and scalability. Existing sheaf-based\nmodels either predefine restriction maps or introduce excessive complexity, yet\nfail to provide rigorous stability guarantees. In this paper, we introduce a\nnovel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified\narchitecture that combines cellular-sheaf message passing with several\nmechanisms, including optimal transport-based lifting, variance-reduced\ndiffusion, and PAC-Bayes spectral regularization for robust semi-supervised\nnode classification. We establish performance bounds theoretically and\ndemonstrate that the resulting bound-aware objective can be achieved via\nend-to-end training in linear computational complexity. Experiments on nine\nhomophilic and heterophilic benchmarks show that SGPC outperforms\nstate-of-the-art spectral and sheaf-based GNNs while providing certified\nconfidence intervals on unseen nodes.", "AI": {"tldr": "SGPC\u662f\u4e00\u79cd\u7ed3\u5408\u7ec6\u80de\u9798\u6d88\u606f\u4f20\u9012\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7PAC-Bayes\u6821\u51c6\u63d0\u5347GNN\u5728\u5f02\u8d28\u56fe\u4e0a\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u7a33\u5b9a\u6027\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3GNN\u5728\u5f02\u8d28\u56fe\u4e0a\u56e0\u8fc7\u5ea6\u5e73\u6ed1\u5bfc\u81f4\u7684\u8282\u70b9\u7279\u5f81\u5d29\u6e83\u95ee\u9898\uff0c\u73b0\u6709\u9798\u7f51\u7edc\u65b9\u6cd5\u6cdb\u5316\u6027\u548c\u6269\u5c55\u6027\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u7ec6\u80de\u9798\u6d88\u606f\u4f20\u9012\u3001\u6700\u4f18\u4f20\u8f93\u63d0\u5347\u3001\u65b9\u5dee\u51cf\u5c11\u6269\u6563\u548cPAC-Bayes\u8c31\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7ebf\u6027\u590d\u6742\u5ea6\u8bad\u7ec3\u3002", "result": "\u5728\u4e5d\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u672a\u89c1\u8282\u70b9\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "conclusion": "SGPC\u5728\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5f02\u8d28\u56fe\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2508.00544", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00544", "abs": "https://arxiv.org/abs/2508.00544", "authors": ["Joonas Tapaninaho", "Mourad Oussala"], "title": "PaPaformer: Language Model from Pre-trained Paraller Paths", "comment": null, "summary": "The training of modern large-language models requires an increasingly amount\nof computation power and time. Even smaller variants, such as small-language\nmodels (SLMs), take several days to train in the best-case scenarios, often\nrequiring multiple GPUs. This paper explores methods to train and evaluate\ndecoder-only transformer-based language models in hours instead of days/weeks.\nWe introduces \\textit{PaPaformer}, a decoder-only transformer architecture\nvariant, whose lower-dimensional parallel paths are combined into larger model.\nThe paper shows that these lower-dimensional paths can be trained individually\nwith different types of training data and then combined into one larger model.\nThis method gives the option to reduce the total number of model parameters and\nthe training time with increasing performance. Moreover, the use of parallel\npath structure opens interesting possibilities to customize paths to\naccommodate specific task requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPaPaformer\u7684\u5e76\u884c\u8def\u5f84\u89e3\u7801\u5668-\u4ec5\u53d8\u538b\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u8bad\u7ec3\u4f4e\u7ef4\u8def\u5f84\u5e76\u7ec4\u5408\u6210\u66f4\u5927\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u5373\u4f7f\u662f\u5c0f\u578b\u6a21\u578b\u4e5f\u9700\u8981\u591aGPU\u548c\u591a\u5929\u8bad\u7ec3\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5f15\u5165PaPaformer\u67b6\u6784\uff0c\u901a\u8fc7\u5e76\u884c\u8bad\u7ec3\u4f4e\u7ef4\u8def\u5f84\u5e76\u7ec4\u5408\u6210\u66f4\u5927\u6a21\u578b\uff0c\u51cf\u5c11\u53c2\u6570\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u652f\u6301\u8def\u5f84\u5b9a\u5236\u4ee5\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u3002", "conclusion": "PaPaformer\u4e3a\u9ad8\u6548\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u548c\u63d0\u5347\u7075\u6d3b\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00784", "abs": "https://arxiv.org/abs/2508.00784", "authors": ["Tom Or", "Omri Azencot"], "title": "Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics", "comment": null, "summary": "Generative models achieve remarkable results in multiple data domains,\nincluding images and texts, among other examples. Unfortunately, malicious\nusers exploit synthetic media for spreading misinformation and disseminating\ndeepfakes. Consequently, the need for robust and stable fake detectors is\npressing, especially when new generative models appear everyday. While the\nmajority of existing work train classifiers that discriminate between real and\nfake information, such tools typically generalize only within the same family\nof generators and data modalities, yielding poor results on other generative\nclasses and data domains. Towards a universal classifier, we propose the use of\nlarge pre-trained multi-modal models for the detection of generative content.\nEffectively, we show that the latent code of these models naturally captures\ninformation discriminating real from fake. Building on this observation, we\ndemonstrate that linear classifiers trained on these features can achieve\nstate-of-the-art results across various modalities, while remaining\ncomputationally efficient, fast to train, and effective even in few-shot\nsettings. Our work primarily focuses on fake detection in audio and images,\nachieving performance that surpasses or matches that of strong baseline\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u7684\u901a\u7528\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u68c0\u6d4b\u751f\u6210\u5185\u5bb9\uff0c\u5728\u97f3\u9891\u548c\u56fe\u50cf\u9886\u57df\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u88ab\u6076\u610f\u7528\u4e8e\u4f20\u64ad\u865a\u5047\u4fe1\u606f\uff0c\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u5f00\u53d1\u901a\u7528\u4e14\u9ad8\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u5728\u4ee3\u7801\u7279\u5f81\uff0c\u8bad\u7ec3\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u9ad8\u6548\u68c0\u6d4b\u3002", "result": "\u5728\u97f3\u9891\u548c\u56fe\u50cf\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u5c11\u6837\u672c\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u7528\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00364", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00364", "abs": "https://arxiv.org/abs/2508.00364", "authors": ["Chanyoung Yoon", "Sangbong Yoo", "Soobin Yim", "Chansoo Kim", "Yun Jang"], "title": "OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions", "comment": null, "summary": "Designing residential interiors strongly impacts occupant satisfaction but\nremains challenging due to unstructured spatial layouts, high computational\ndemands, and reliance on expert knowledge. Existing methods based on\noptimization or deep learning are either computationally expensive or\nconstrained by data scarcity. Reinforcement learning (RL) approaches often\nlimit furniture placement to discrete positions and fail to incorporate design\nprinciples adequately. We propose OID-PPO, a novel RL framework for Optimal\nInterior Design using Proximal Policy Optimization, which integrates\nexpert-defined functional and visual guidelines into a structured reward\nfunction. OID-PPO utilizes a diagonal Gaussian policy for continuous and\nflexible furniture placement, effectively exploring latent environmental\ndynamics under partial observability. Experiments conducted across diverse room\nshapes and furniture configurations demonstrate that OID-PPO significantly\noutperforms state-of-the-art methods in terms of layout quality and\ncomputational efficiency. Ablation studies further demonstrate the impact of\nstructured guideline integration and reveal the distinct contributions of\nindividual design constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5ba4\u5185\u8bbe\u8ba1\u4f18\u5316\u6846\u67b6OID-PPO\uff0c\u901a\u8fc7\u6574\u5408\u4e13\u5bb6\u5b9a\u4e49\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e03\u5c40\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f4f\u5b85\u5ba4\u5185\u8bbe\u8ba1\u5bf9\u5c45\u4f4f\u6ee1\u610f\u5ea6\u5f71\u54cd\u91cd\u5927\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6570\u636e\u7a00\u7f3a\u6216\u8bbe\u8ba1\u539f\u5219\u878d\u5165\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528Proximal Policy Optimization\uff08PPO\uff09\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u89d2\u9ad8\u65af\u7b56\u7565\u5b9e\u73b0\u8fde\u7eed\u7075\u6d3b\u7684\u5bb6\u5177\u5e03\u5c40\uff0c\u5e76\u878d\u5165\u529f\u80fd\u4e0e\u89c6\u89c9\u8bbe\u8ba1\u51c6\u5219\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOID-PPO\u5728\u591a\u6837\u623f\u95f4\u5f62\u72b6\u548c\u5bb6\u5177\u914d\u7f6e\u4e0b\uff0c\u5e03\u5c40\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OID-PPO\u901a\u8fc7\u7ed3\u6784\u5316\u5956\u52b1\u51fd\u6570\u548c\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5ba4\u5185\u8bbe\u8ba1\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u51c6\u5219\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00574", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00574", "abs": "https://arxiv.org/abs/2508.00574", "authors": ["Jianwei Wang", "Ziming Wu", "Fuming Lai", "Shaobing Lian", "Ziqian Zeng"], "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought", "comment": null, "summary": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.", "AI": {"tldr": "SynAdapt \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u7684\u8fde\u7eed CoT\uff08CCoT\uff09\u4f5c\u4e3a\u5bf9\u9f50\u76ee\u6807\uff0c\u5e76\u96c6\u6210\u96be\u5ea6\u5206\u7c7b\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u8fde\u7eed CoT\uff08CCoT\uff09\u65b9\u6cd5\u5b58\u5728\u95f4\u63a5\u5fae\u8c03\u3001\u5bf9\u9f50\u4e0d\u8db3\u6216\u76ee\u6807\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51fa SynAdapt \u6846\u67b6\uff0c\u751f\u6210\u5408\u6210\u7684 CCoT \u4f5c\u4e3a\u5bf9\u9f50\u76ee\u6807\uff0c\u5e76\u5f15\u5165\u96be\u5ea6\u5206\u7c7b\u5668\u81ea\u9002\u5e94\u5904\u7406\u96be\u9898\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSynAdapt \u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u5e73\u8861\u3002", "conclusion": "SynAdapt \u901a\u8fc7\u521b\u65b0\u7684 CCoT \u751f\u6210\u548c\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2502.18148", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2502.18148", "abs": "https://arxiv.org/abs/2502.18148", "authors": ["Muhammad Farid Adilazuarda", "Musa Izzanardi Wijanarko", "Lucky Susanto", "Khumaisa Nur'aini", "Derry Wijaya", "Alham Fikri Aji"], "title": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts", "comment": null, "summary": "Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance.", "AI": {"tldr": "NusaAksara\u662f\u4e00\u4e2a\u9488\u5bf9\u5370\u5c3c\u8bed\u8a00\u53ca\u5176\u539f\u59cb\u811a\u672c\u7684\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\uff0c\u4efb\u52a1\u591a\u6837\uff0c\u4f46\u73b0\u6709NLP\u6280\u672f\u5bf9\u5176\u652f\u6301\u4e0d\u8db3\u3002", "motivation": "\u5370\u5c3c\u8bed\u8a00\u548c\u811a\u672c\u4e30\u5bcc\uff0c\u4f46NLP\u7814\u7a76\u591a\u57fa\u4e8e\u7f57\u9a6c\u5316\u6587\u672c\uff0c\u7f3a\u4e4f\u5bf9\u539f\u59cb\u811a\u672c\u7684\u652f\u6301\u3002", "method": "\u6784\u5efa\u5305\u542b8\u79cd\u811a\u672c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u591a\u79cd\u4efb\u52a1\uff0c\u6570\u636e\u7531\u4e13\u5bb6\u4e25\u683c\u6784\u5efa\u3002", "result": "\u5927\u591a\u6570NLP\u6280\u672f\u5bf9\u5370\u5c3c\u672c\u5730\u811a\u672c\u652f\u6301\u4e0d\u8db3\uff0c\u6027\u80fd\u63a5\u8fd1\u96f6\u3002", "conclusion": "NusaAksara\u586b\u8865\u4e86\u5370\u5c3c\u8bed\u8a00\u539f\u59cb\u811a\u672c\u7684NLP\u7814\u7a76\u7a7a\u767d\uff0c\u51f8\u663e\u6280\u672f\u6539\u8fdb\u9700\u6c42\u3002"}}
{"id": "2508.00392", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00392", "abs": "https://arxiv.org/abs/2508.00392", "authors": ["Lijun Zhang", "Wenhao Yang", "Guanghui Wang", "Wei Jiang", "Zhi-Hua Zhou"], "title": "Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions", "comment": "arXiv admin note: text overlap with arXiv:1906.10851", "summary": "To deal with changing environments, a new performance measure -- adaptive\nregret, defined as the maximum static regret over any interval, was proposed in\nonline learning. Under the setting of online convex optimization, several\nalgorithms have been successfully developed to minimize the adaptive regret.\nHowever, existing algorithms lack universality in the sense that they can only\nhandle one type of convex functions and need apriori knowledge of parameters,\nwhich hinders their application in real-world scenarios. To address this\nlimitation, this paper investigates universal algorithms with dual adaptivity,\nwhich automatically adapt to the property of functions (convex, exponentially\nconcave, or strongly convex), as well as the nature of environments (stationary\nor changing). Specifically, we propose a meta-expert framework for dual\nadaptive algorithms, where multiple experts are created dynamically and\naggregated by a meta-algorithm. The meta-algorithm is required to yield a\nsecond-order bound, which can accommodate unknown function types. We further\nincorporate the technique of sleeping experts to capture the changing\nenvironments. For the construction of experts, we introduce two strategies\n(increasing the number of experts or enhancing the capabilities of experts) to\nachieve universality. Theoretical analysis shows that our algorithms are able\nto minimize the adaptive regret for multiple types of convex functions\nsimultaneously, and also allow the type of functions to switch between rounds.\nMoreover, we extend our meta-expert framework to online composite optimization,\nand develop a universal algorithm for minimizing the adaptive regret of\ncomposite functions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u53cc\u91cd\u9002\u5e94\u6027\u7684\u901a\u7528\u7b97\u6cd5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u51f8\u4f18\u5316\u4e2d\u6700\u5c0f\u5316\u81ea\u9002\u5e94\u9057\u61be\uff0c\u80fd\u591f\u81ea\u52a8\u9002\u5e94\u51fd\u6570\u7c7b\u578b\u548c\u73af\u5883\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u53ea\u80fd\u5904\u7406\u5355\u4e00\u7c7b\u578b\u7684\u51f8\u51fd\u6570\u4e14\u9700\u8981\u5148\u9a8c\u53c2\u6570\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5143\u4e13\u5bb6\u6846\u67b6\u7684\u53cc\u91cd\u81ea\u9002\u5e94\u7b97\u6cd5\uff0c\u52a8\u6001\u521b\u5efa\u591a\u4e2a\u4e13\u5bb6\u5e76\u901a\u8fc7\u5143\u7b97\u6cd5\u805a\u5408\uff0c\u7ed3\u5408\u7761\u7720\u4e13\u5bb6\u6280\u672f\u6355\u6349\u73af\u5883\u53d8\u5316\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u7b97\u6cd5\u80fd\u540c\u65f6\u6700\u5c0f\u5316\u591a\u79cd\u51f8\u51fd\u6570\u7684\u81ea\u9002\u5e94\u9057\u61be\uff0c\u5e76\u5141\u8bb8\u51fd\u6570\u7c7b\u578b\u5728\u8f6e\u6b21\u95f4\u5207\u6362\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u5230\u5728\u7ebf\u590d\u5408\u4f18\u5316\uff0c\u4e3a\u590d\u5408\u51fd\u6570\u7684\u81ea\u9002\u5e94\u9057\u61be\u6700\u5c0f\u5316\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00600", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00600", "abs": "https://arxiv.org/abs/2508.00600", "authors": ["Mingruo Yuan", "Shuyi Zhang", "Ben Kao"], "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models", "comment": null, "summary": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.", "AI": {"tldr": "CRUX\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u4e24\u79cd\u65b0\u6307\u6807\u6539\u8fdbLLM\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5ffd\u7565\u4e86\u54cd\u5e94\u4e0e\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u76f8\u5173\u6027\uff0c\u800cCRUX\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "CRUX\u901a\u8fc7\u4e0a\u4e0b\u6587\u71b5\u51cf\u5c11\u548c\u7edf\u4e00\u4e00\u81f4\u6027\u68c0\u67e5\u4e24\u79cd\u65b0\u6307\u6807\uff0c\u6574\u5408\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u6027\u548c\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cCRUX\u7684AUROC\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "CRUX\u4e3aLLM\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2508.00394", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00394", "abs": "https://arxiv.org/abs/2508.00394", "authors": ["Antonis Klironomos", "Baifan Zhou", "Zhipeng Tan", "Zhuoxun Zheng", "Mohamed H. Gad-Elrab", "Heiko Paulheim", "Evgeny Kharlamov"], "title": "ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs", "comment": null, "summary": "Nowadays machine learning (ML) practitioners have access to numerous ML\nlibraries available online. Such libraries can be used to create ML pipelines\nthat consist of a series of steps where each step may invoke up to several ML\nlibraries that are used for various data-driven analytical tasks. Development\nof high-quality ML pipelines is non-trivial; it requires training, ML\nexpertise, and careful development of each step. At the same time, domain\nexperts in science and engineering may not possess such ML expertise and\ntraining while they are in pressing need of ML-based analytics. In this paper,\nwe present our ExeKGLib, a Python library enhanced with a graphical interface\nlayer that allows users with minimal ML knowledge to build ML pipelines. This\nis achieved by relying on knowledge graphs that encode ML knowledge in simple\nterms accessible to non-ML experts. ExeKGLib also allows improving the\ntransparency and reusability of the built ML workflows and ensures that they\nare executable. We show the usability and usefulness of ExeKGLib by presenting\nreal use cases.", "AI": {"tldr": "ExeKGLib\u662f\u4e00\u4e2aPython\u5e93\uff0c\u901a\u8fc7\u56fe\u5f62\u754c\u9762\u548c\u77e5\u8bc6\u56fe\u8c31\u5e2e\u52a9\u975eML\u4e13\u5bb6\u6784\u5efaML\u7ba1\u9053\u3002", "motivation": "\u89e3\u51b3\u975eML\u4e13\u5bb6\u5728\u6784\u5efa\u9ad8\u8d28\u91cfML\u7ba1\u9053\u65f6\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7f16\u7801ML\u77e5\u8bc6\uff0c\u63d0\u4f9b\u56fe\u5f62\u754c\u9762\u7b80\u5316\u64cd\u4f5c\u3002", "result": "ExeKGLib\u63d0\u9ad8\u4e86ML\u7ba1\u9053\u7684\u900f\u660e\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u7528\u4f8b\u9a8c\u8bc1\u4e86\u5176\u53ef\u7528\u6027\u3002", "conclusion": "ExeKGLib\u4e3a\u975eML\u4e13\u5bb6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684ML\u7ba1\u9053\u6784\u5efa\u5de5\u5177\u3002"}}
{"id": "2508.00605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00605", "abs": "https://arxiv.org/abs/2508.00605", "authors": ["Farhana Haque", "Md. Abdur Rahman", "Sumon Ahmed"], "title": "GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language", "comment": null, "summary": "Topic modeling is a Natural Language Processing (NLP) technique that is used\nto identify latent themes and extract topics from text corpora by grouping\nsimilar documents based on their most significant keywords. Although widely\nresearched in English, topic modeling remains understudied in Bengali due to\nits morphological complexity, lack of adequate resources and initiatives. In\nthis contribution, a novel Graph Convolutional Network (GCN) based model called\nGHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input\nvectors of documents as nodes in the graph, which GCN uses to produce\nsemantically rich embeddings. The embeddings are then decomposed using\nNon-negative Matrix Factorization (NMF) to get the topical representations of\nthe underlying themes of the text corpus. This study compares the proposed\nmodel against a wide range of Bengali topic modeling techniques, from\ntraditional methods such as LDA, LSA, and NMF to contemporary frameworks such\nas BERTopic and Top2Vec on three Bengali datasets. The experimental results\ndemonstrate the effectiveness of the proposed model by outperforming other\nmodels in topic coherence and diversity. In addition, we introduce a novel\nBengali dataset called \"NCTBText\" sourced from Bengali textbook materials to\nenrich and diversify the predominantly newspaper-centric Bengali corpora.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u7684\u6df7\u5408\u4e3b\u9898\u6a21\u578bGHTM\uff0c\u7528\u4e8e\u5b5f\u52a0\u62c9\u8bed\u6587\u672c\u7684\u4e3b\u9898\u5efa\u6a21\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u56e0\u5176\u5f62\u6001\u590d\u6742\u6027\u548c\u8d44\u6e90\u532e\u4e4f\uff0c\u4e3b\u9898\u5efa\u6a21\u7814\u7a76\u8f83\u5c11\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528GCN\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u6587\u6863\u5d4c\u5165\uff0c\u518d\u901a\u8fc7\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u63d0\u53d6\u4e3b\u9898\u3002", "result": "GHTM\u5728\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u548c\u73b0\u4ee3\u65b9\u6cd5\u3002", "conclusion": "GHTM\u4e3a\u5b5f\u52a0\u62c9\u8bed\u4e3b\u9898\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u6570\u636e\u96c6NCTBText\u3002"}}
{"id": "2508.00410", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00410", "abs": "https://arxiv.org/abs/2508.00410", "authors": ["Zizhuo Zhang", "Jianing Zhu", "Xinmu Ge", "Zihua Zhao", "Zhanke Zhou", "Xuan Li", "Xiao Feng", "Jiangchao Yao", "Bo Han"], "title": "Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement", "comment": null, "summary": "Although reinforcement learning with verifiable rewards (RLVR) shows promise\nin improving the reasoning ability of large language models (LLMs), the scaling\nup dilemma remains due to the reliance on human annotated labels especially for\ncomplex tasks. Recent alternatives that explore various self-reward signals\nexhibit the eliciting potential of LLM reasoning, but suffer from the\nnon-negligible collapse issue. Inspired by the success of self-supervised\nlearning, we propose \\textit{Co-Reward}, a novel RL framework that leverages\ncontrastive agreement across semantically analogical questions as a reward\nbasis. Specifically, we construct a similar question for each training sample\n(without labels) and synthesize their individual surrogate labels through a\nsimple rollout voting, and then the reward is constructed by cross-referring\nthe labels of each question pair to enforce the internal reasoning consistency\nacross analogical inputs. Intuitively, such a self-supervised reward-shaping\nmechanism increases the difficulty of learning collapse into a trivial\nsolution, and promotes stable reasoning elicitation and improvement through\nexpanding the input sample variants. Empirically, Co-Reward achieves superior\nperformance compared to other self-reward baselines on multiple reasoning\nbenchmarks and LLM series, and reaches or even surpasses ground-truth (GT)\nlabeled reward, with improvements of up to $+6.8\\%$ on MATH500 over GT reward\non Llama-3.2-3B-Instruct. Our code is publicly available at\nhttps://github.com/tmlr-group/Co-Reward.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCo-Reward\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8bed\u4e49\u76f8\u4f3c\u95ee\u9898\u7684\u7b54\u6848\u4e00\u81f4\u6027\u6765\u6784\u5efa\u5956\u52b1\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u5956\u52b1\u65b9\u6cd5\u4e2d\u7684\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4eba\u5de5\u6807\u6ce8\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u3002\u81ea\u5956\u52b1\u65b9\u6cd5\u867d\u7136\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u5d29\u6e83\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u5956\u52b1\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u63a8\u7406\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86Co-Reward\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u6784\u9020\u76f8\u4f3c\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6295\u7968\u5408\u6210\u4ee3\u7406\u6807\u7b7e\uff0c\u7136\u540e\u901a\u8fc7\u4ea4\u53c9\u5f15\u7528\u95ee\u9898\u5bf9\u7684\u6807\u7b7e\u6765\u6784\u5efa\u5956\u52b1\u4fe1\u53f7\uff0c\u4ee5\u589e\u5f3a\u63a8\u7406\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCo-Reward\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548cLLM\u7cfb\u5217\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u81ea\u5956\u52b1\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u8d85\u8fc7\u4e86\u57fa\u4e8e\u771f\u5b9e\u6807\u7b7e\u7684\u5956\u52b1\uff0c\u5982\u5728MATH500\u4e0a\u5bf9Llama-3.2-3B-Instruct\u7684\u6539\u8fdb\u8fbe\u5230+6.8%\u3002", "conclusion": "Co-Reward\u901a\u8fc7\u81ea\u76d1\u7763\u7684\u5956\u52b1\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u5956\u52b1\u65b9\u6cd5\u4e2d\u7684\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.00614", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00614", "abs": "https://arxiv.org/abs/2508.00614", "authors": ["Lennart Meincke", "Ethan Mollick", "Lilach Mollick", "Dan Shapiro"], "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?", "comment": null, "summary": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5a01\u80c1\u6216\u5956\u52b1AI\u6a21\u578b\u5bf9\u6574\u4f53\u6027\u80fd\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u63d0\u793a\u53d8\u5316\u53ef\u80fd\u5bf9\u4e2a\u522b\u95ee\u9898\u4ea7\u751f\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u9a8c\u8bc1\u4e24\u79cd\u5e38\u89c1\u63d0\u793a\u7b56\u7565\uff08\u5a01\u80c1\u548c\u5956\u52b1\uff09\u5bf9AI\u6a21\u578b\u6027\u80fd\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u4ee5\u5e2e\u52a9\u5546\u4e1a\u3001\u6559\u80b2\u548c\u653f\u7b56\u9886\u5bfc\u8005\u7406\u89e3AI\u5de5\u4f5c\u7684\u6280\u672f\u7ec6\u8282\u3002", "method": "\u5728GPQA\u548cMMLU-Pro\u57fa\u51c6\u4e0a\u6d4b\u8bd5\u5a01\u80c1\u548c\u5956\u52b1\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5a01\u80c1\u6216\u5956\u52b1\u5bf9\u6574\u4f53\u6027\u80fd\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u63d0\u793a\u53d8\u5316\u53ef\u80fd\u663e\u8457\u5f71\u54cd\u4e2a\u522b\u95ee\u9898\u7684\u8868\u73b0\u3002", "conclusion": "\u7b80\u5355\u63d0\u793a\u7b56\u7565\u53ef\u80fd\u4e0d\u5982\u9884\u671f\u6709\u6548\uff0c\u5c24\u5176\u662f\u5bf9\u590d\u6742\u95ee\u9898\uff0c\u4f46\u4e2a\u522b\u95ee\u9898\u53ef\u80fd\u56e0\u63d0\u793a\u53d8\u5316\u800c\u6709\u663e\u8457\u5dee\u5f02\u3002"}}
{"id": "2508.00415", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00415", "abs": "https://arxiv.org/abs/2508.00415", "authors": ["Yue Yang", "Yuxiang Lin", "Ying Zhang", "Zihan Su", "Chang Chuan Goh", "Tangtangfang Fang", "Anthony Graham Bellotti", "Boon Giin Lee"], "title": "Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection", "comment": null, "summary": "Prediction of post-loan default is an important task in credit risk\nmanagement, and can be addressed by detection of financial anomalies using\nmachine learning. This study introduces a ResE-BiLSTM model, using a sliding\nwindow technique, and is evaluated on 44 independent cohorts from the extensive\nFreddie Mac US mortgage dataset, to improve prediction performance. The\nResE-BiLSTM is compared with five baseline models: Long Short-Term Memory\n(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks\n(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including\nAccuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to\nevaluate the contribution of individual components in the ResE-BiLSTM\narchitecture. Additionally, SHAP analysis was employed to interpret the\nunderlying features the model relied upon for its predictions. Experimental\nresults demonstrate that ResE-BiLSTM achieves superior predictive performance\ncompared to baseline models, underscoring its practical value and applicability\nin real-world scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aResE-BiLSTM\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u8d37\u6b3e\u8fdd\u7ea6\uff0c\u5e76\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\u548cSHAP\u5206\u6790\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u5bf9\u4fe1\u7528\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u4f7f\u7528ResE-BiLSTM\u6a21\u578b\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\uff0c\u5e76\u5728Freddie Mac\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4e0e\u4e94\u79cd\u57fa\u7ebf\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eResE-BiLSTM\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "ResE-BiLSTM\u6a21\u578b\u5728\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2508.00619", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00619", "abs": "https://arxiv.org/abs/2508.00619", "authors": ["Shantanu Thorat", "Andrew Caines"], "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models", "comment": "MPhil in Advanced Computer Science thesis for University of Cambridge", "summary": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u73b0\u6709AIG\u6587\u672c\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51faDACTYL\u6570\u636e\u96c6\u548c\u4e24\u79cd\u4f18\u5316\u65b9\u6cd5\uff08BCE\u548cDXO\uff09\uff0c\u53d1\u73b0DXO\u5728\u6cdb\u5316\u6027\u4e0a\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709AIG\u6587\u672c\u68c0\u6d4b\u5668\u5728\u5185\u90e8\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4e0d\u591f\u9c81\u68d2\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5f15\u5165DACTYL\u6570\u636e\u96c6\uff08\u4e13\u6ce8\u4e8e\u5355\u6b21/\u5c11\u91cf\u6837\u672c\u751f\u6210\u548cCPT\u6a21\u578b\u6587\u672c\uff09\uff0c\u5e76\u6bd4\u8f83BCE\u548cDXO\u4e24\u79cd\u4f18\u5316\u65b9\u6cd5\u3002", "result": "DXO\u4f18\u5316\u65b9\u6cd5\u5728OOD\u6587\u672c\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u6cdb\u5316\u6027\u66f4\u5f3a\u3002", "conclusion": "AIG\u6587\u672c\u68c0\u6d4b\u5668\u9700\u6539\u8fdb\uff0cDXO\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8eBCE\u3002"}}
{"id": "2508.00472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00472", "abs": "https://arxiv.org/abs/2508.00472", "authors": ["Leonidas Akritidis", "Panayiotis Bozanis"], "title": "A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces", "comment": null, "summary": "The tabular form constitutes the standard way of representing data in\nrelational database systems and spreadsheets. But, similarly to other forms,\ntabular data suffers from class imbalance, a problem that causes serious\nperformance degradation in a wide variety of machine learning tasks. One of the\nmost effective solutions dictates the usage of Generative Adversarial Networks\n(GANs) in order to synthesize artificial data instances for the\nunder-represented classes. Despite their good performance, none of the proposed\nGAN models takes into account the vector subspaces of the input samples in the\nreal data space, leading to data generation in arbitrary locations. Moreover,\nthe class labels are treated in the same manner as the other categorical\nvariables during training, so conditional sampling by class is rendered less\neffective. To overcome these problems, this study presents ctdGAN, a\nconditional GAN for alleviating class imbalance in tabular datasets. Initially,\nctdGAN executes a space partitioning step to assign cluster labels to the input\nsamples. Subsequently, it utilizes these labels to synthesize samples via a\nnovel probabilistic sampling strategy and a new loss function that penalizes\nboth cluster and class mis-predictions. In this way, ctdGAN is trained to\ngenerate samples in subspaces that resemble those of the original data\ndistribution. We also introduce several other improvements, including a simple,\nyet effective cluster-wise scaling technique that captures multiple feature\nmodes without affecting data dimensionality. The exhaustive evaluation of\nctdGAN with 14 imbalanced datasets demonstrated its superiority in generating\nhigh fidelity samples and improving classification accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51factdGAN\uff0c\u4e00\u79cd\u7528\u4e8e\u89e3\u51b3\u8868\u683c\u6570\u636e\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u6761\u4ef6GAN\uff0c\u901a\u8fc7\u7a7a\u95f4\u5206\u533a\u548c\u6982\u7387\u91c7\u6837\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u3002", "motivation": "\u8868\u683c\u6570\u636e\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u673a\u5668\u5b66\u4e60\u6027\u80fd\uff0c\u73b0\u6709GAN\u65b9\u6cd5\u672a\u8003\u8651\u8f93\u5165\u6837\u672c\u7684\u5411\u91cf\u5b50\u7a7a\u95f4\uff0c\u5bfc\u81f4\u751f\u6210\u6570\u636e\u4f4d\u7f6e\u4e0d\u51c6\u786e\u3002", "method": "ctdGAN\u901a\u8fc7\u7a7a\u95f4\u5206\u533a\u4e3a\u8f93\u5165\u6837\u672c\u5206\u914d\u805a\u7c7b\u6807\u7b7e\uff0c\u5229\u7528\u65b0\u635f\u5931\u51fd\u6570\u548c\u6982\u7387\u91c7\u6837\u7b56\u7565\u751f\u6210\u6837\u672c\uff0c\u540c\u65f6\u5f15\u5165\u805a\u7c7b\u7f29\u653e\u6280\u672f\u3002", "result": "\u572814\u4e2a\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cctdGAN\u80fd\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\u5e76\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "ctdGAN\u901a\u8fc7\u6539\u8fdb\u751f\u6210\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8868\u683c\u6570\u636e\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2508.00669", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00669", "abs": "https://arxiv.org/abs/2508.00669", "authors": ["Wenxuan Wang", "Zizhan Ma", "Meidan Ding", "Shiyi Zheng", "Shengyuan Liu", "Jie Liu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Linlin Shen", "Yixuan Yuan"], "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u5b66\u63a8\u7406\u9886\u57df\u7684\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86\u8bad\u7ec3\u65f6\u548c\u6d4b\u8bd5\u65f6\u7684\u589e\u5f3a\u6280\u672f\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u591a\u6a21\u6001\u6570\u636e\u548c\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "LLMs\u5728\u533b\u5b66\u4e2d\u7684\u5e94\u7528\u7f3a\u4e4f\u7cfb\u7edf\u6027\u3001\u900f\u660e\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u662f\u4e34\u5e8a\u5b9e\u8df5\u7684\u6838\u5fc3\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u6cd5\uff0c\u5305\u62ec\u8bad\u7ec3\u65f6\u7b56\u7565\uff08\u5982\u76d1\u7763\u5fae\u8c03\uff09\u548c\u6d4b\u8bd5\u65f6\u673a\u5236\uff08\u5982\u63d0\u793a\u5de5\u7a0b\uff09\uff0c\u5e76\u5206\u6790\u4e8660\u9879\u5173\u952e\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9700\u8981\u89e3\u51b3\u5fe0\u5b9e\u6027\u4e0e\u5408\u7406\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63a8\u52a8\u591a\u6a21\u6001\u63a8\u7406\u7684\u53d1\u5c55\u3002", "conclusion": "\u672a\u6765\u65b9\u5411\u5305\u62ec\u6784\u5efa\u9ad8\u6548\u3001\u7a33\u5065\u4e14\u793e\u4f1a\u6280\u672f\u8d23\u4efb\u5f3a\u7684\u533b\u5b66AI\u3002"}}
{"id": "2508.00507", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00507", "abs": "https://arxiv.org/abs/2508.00507", "authors": ["Yiming Xu", "Jiarun Chen", "Zhen Peng", "Zihan Chen", "Qika Lin", "Lan Ma", "Bin Shi", "Bo Dong"], "title": "Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection", "comment": "Accepted by ACM Multimedia 2025 (MM '25)", "summary": "The natural combination of intricate topological structures and rich textual\ninformation in text-attributed graphs (TAGs) opens up a novel perspective for\ngraph anomaly detection (GAD). However, existing GAD methods primarily focus on\ndesigning complex optimization objectives within the graph domain, overlooking\nthe complementary value of the textual modality, whose features are often\nencoded by shallow embedding techniques, such as bag-of-words or skip-gram, so\nthat semantic context related to anomalies may be missed. To unleash the\nenormous potential of textual modality, large language models (LLMs) have\nemerged as promising alternatives due to their strong semantic understanding\nand reasoning capabilities. Nevertheless, their application to TAG anomaly\ndetection remains nascent, and they struggle to encode high-order structural\ninformation inherent in graphs due to input length constraints. For\nhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework that\ncombines LLMs and graph neural networks (GNNs) to leverage their complementary\nstrengths. CoLL employs multi-LLM collaboration for evidence-augmented\ngeneration to capture anomaly-relevant contexts while delivering human-readable\nrationales for detected anomalies. Moreover, CoLL integrates a GNN equipped\nwith a gating mechanism to adaptively fuse textual features with evidence while\npreserving high-order topological information. Extensive experiments\ndemonstrate the superiority of CoLL, achieving an average improvement of 13.37%\nin AP. This study opens a new avenue for incorporating LLMs in advancing GAD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u65b0\u6846\u67b6CoLL\uff0c\u7528\u4e8e\u6587\u672c\u5c5e\u6027\u56fe\uff08TAGs\uff09\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u89c6\u6587\u672c\u6a21\u6001\u7684\u6f5c\u529b\uff0c\u800cLLMs\u867d\u80fd\u7406\u89e3\u8bed\u4e49\u4f46\u96be\u4ee5\u7f16\u7801\u56fe\u7ed3\u6784\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "CoLL\u6846\u67b6\u901a\u8fc7\u591aLLM\u534f\u4f5c\u751f\u6210\u5f02\u5e38\u76f8\u5173\u8bc1\u636e\uff0c\u5e76\u7ed3\u5408GNN\u7684\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u878d\u5408\u6587\u672c\u7279\u5f81\u4e0e\u56fe\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCoLL\u5728AP\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u534713.37%\u3002", "conclusion": "CoLL\u4e3a\u5229\u7528LLMs\u63a8\u8fdb\u56fe\u5f02\u5e38\u68c0\u6d4b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.00673", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00673", "abs": "https://arxiv.org/abs/2508.00673", "authors": ["Farhan Farsi", "Farnaz Aghababaloo", "Shahriar Shariati Motlagh", "Parsa Ghofrani", "MohammadAli SadraeiJavaheri", "Shayan Bali", "Amirhossein Shabani", "Farbod Bijary", "Ghazal Zamaninejad", "AmirMohammad Salehoof", "Saeedeh Momtazi"], "title": "MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language", "comment": "Preprint. Under review", "summary": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa19\u4e2a\u6ce2\u65af\u8bed\u548c\u4f0a\u6717\u6587\u5316\u76f8\u5173\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u586b\u8865\u5927\u8bed\u8a00\u6a21\u578b\u5728\u975e\u897f\u65b9\u6587\u5316\u548c\u8bed\u8a00\u4e2d\u7684\u8bc4\u4f30\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u8d44\u6e90\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8bed\u548c\u897f\u65b9\u6587\u5316\uff0c\u7f3a\u4e4f\u5bf9\u5176\u4ed6\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u7684\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1\u4e8619\u4e2a\u6ce2\u65af\u8bed\u548c\u4f0a\u6717\u6587\u5316\u76f8\u5173\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u6cd5\u5f8b\u3001\u8bed\u6cd5\u3001\u4e60\u8bed\u548c\u8003\u8bd5\u7b49\u5185\u5bb9\uff0c\u5e76\u5bf941\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u5bf941\u4e2a\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u6587\u5316\u548c\u8bed\u8a00\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6ce2\u65af\u8bed\u548c\u4f0a\u6717\u6587\u5316\u7684\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\uff0c\u5f3a\u8c03\u4e86\u591a\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00513", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00513", "abs": "https://arxiv.org/abs/2508.00513", "authors": ["Yiming Xu", "Xu Hua", "Zhen Peng", "Bin Shi", "Jiarun Chen", "Xingbo Fu", "Song Wang", "Bo Dong"], "title": "Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning", "comment": "Accepted by ECAI 2025", "summary": "The widespread application of graph data in various high-risk scenarios has\nincreased attention to graph anomaly detection (GAD). Faced with real-world\ngraphs that often carry node descriptions in the form of raw text sequences,\ntermed text-attributed graphs (TAGs), existing graph anomaly detection\npipelines typically involve shallow embedding techniques to encode such textual\ninformation into features, and then rely on complex self-supervised tasks\nwithin the graph domain to detect anomalies. However, this text encoding\nprocess is separated from the anomaly detection training objective in the graph\ndomain, making it difficult to ensure that the extracted textual features focus\non GAD-relevant information, seriously constraining the detection capability.\nHow to seamlessly integrate raw text and graph topology to unleash the vast\npotential of cross-modal data in TAGs for anomaly detection poses a challenging\nissue. This paper presents a novel end-to-end paradigm for text-attributed\ngraph anomaly detection, named CMUCL. We simultaneously model data from both\ntext and graph structures, and jointly train text and graph encoders by\nleveraging cross-modal and uni-modal multi-scale consistency to uncover\npotential anomaly-related information. Accordingly, we design an anomaly score\nestimator based on inconsistency mining to derive node-specific anomaly scores.\nConsidering the lack of benchmark datasets tailored for anomaly detection on\nTAGs, we release 8 datasets to facilitate future research. Extensive\nevaluations show that CMUCL significantly advances in text-attributed graph\nanomaly detection, delivering an 11.13% increase in average accuracy (AP) over\nthe suboptimal.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCMUCL\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u672c\u5c5e\u6027\u56fe\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u6587\u672c\u548c\u56fe\u5f62\u7f16\u7801\u5668\uff0c\u5229\u7528\u8de8\u6a21\u6001\u548c\u591a\u5c3a\u5ea6\u4e00\u81f4\u6027\u63d0\u9ad8\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u5c5e\u6027\u56fe\u7684\u5f02\u5e38\u68c0\u6d4b\u4e2d\uff0c\u6587\u672c\u7f16\u7801\u4e0e\u5f02\u5e38\u68c0\u6d4b\u76ee\u6807\u5206\u79bb\uff0c\u5bfc\u81f4\u68c0\u6d4b\u80fd\u529b\u53d7\u9650\u3002", "method": "\u63d0\u51faCMUCL\u65b9\u6cd5\uff0c\u8054\u5408\u8bad\u7ec3\u6587\u672c\u548c\u56fe\u5f62\u7f16\u7801\u5668\uff0c\u5229\u7528\u8de8\u6a21\u6001\u548c\u591a\u5c3a\u5ea6\u4e00\u81f4\u6027\u6316\u6398\u5f02\u5e38\u4fe1\u606f\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u4e0d\u4e00\u81f4\u6027\u6316\u6398\u7684\u5f02\u5e38\u8bc4\u5206\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCMUCL\u5728\u6587\u672c\u5c5e\u6027\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u51c6\u786e\u7387\uff08AP\uff09\u63d0\u5347\u4e8611.13%\u3002", "conclusion": "CMUCL\u65b9\u6cd5\u6709\u6548\u6574\u5408\u6587\u672c\u548c\u56fe\u7ed3\u6784\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e868\u4e2a\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.00675", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00675", "abs": "https://arxiv.org/abs/2508.00675", "authors": ["Gleb Schmidt", "Johannes R\u00f6misch", "Mariia Halchynska", "Svetlana Gorovaia", "Ivan P. Yamshchikov"], "title": "Team \"better_call_claude\": Style Change Detection using a Sequential Sentence Pair Classifier", "comment": null, "summary": "Style change detection - identifying the points in a document where writing\nstyle shifts - remains one of the most important and challenging problems in\ncomputational authorship analysis. At PAN 2025, the shared task challenges\nparticipants to detect style switches at the most fine-grained level:\nindividual sentences. The task spans three datasets, each designed with\ncontrolled and increasing thematic variety within documents. We propose to\naddress this problem by modeling the content of each problem instance - that\nis, a series of sentences - as a whole, using a Sequential Sentence Pair\nClassifier (SSPC). The architecture leverages a pre-trained language model\n(PLM) to obtain representations of individual sentences, which are then fed\ninto a bidirectional LSTM (BiLSTM) to contextualize them within the document.\nThe BiLSTM-produced vectors of adjacent sentences are concatenated and passed\nto a multi-layer perceptron for prediction per adjacency. Building on the work\nof previous PAN participants classical text segmentation, the approach is\nrelatively conservative and lightweight. Nevertheless, it proves effective in\nleveraging contextual information and addressing what is arguably the most\nchallenging aspect of this year's shared task: the notorious problem of\n\"stylistically shallow\", short sentences that are prevalent in the proposed\nbenchmark data. Evaluated on the official PAN-2025 test datasets, the model\nachieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,\nand HARD data, respectively, outperforming not only the official random\nbaselines but also a much more challenging one: claude-3.7-sonnet's zero-shot\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u5411LSTM\u7684\u5e8f\u5217\u53e5\u5b50\u5bf9\u5206\u7c7b\u5668\uff08SSPC\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u6587\u6863\u4e2d\u7684\u98ce\u683c\u53d8\u5316\uff0c\u5e76\u5728PAN 2025\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u7684\u8868\u73b0\u3002", "motivation": "\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u662f\u8ba1\u7b97\u4f5c\u8005\u5206\u6790\u4e2d\u6700\u91cd\u8981\u4e14\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\u4e4b\u4e00\uff0c\u5c24\u5176\u662f\u5728\u53e5\u5b50\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u751f\u6210\u53e5\u5b50\u8868\u793a\uff0c\u518d\u901a\u8fc7\u53cc\u5411LSTM\uff08BiLSTM\uff09\u8fdb\u884c\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u6700\u540e\u7528\u591a\u5c42\u611f\u77e5\u673a\u9884\u6d4b\u76f8\u90bb\u53e5\u5b50\u7684\u98ce\u683c\u53d8\u5316\u3002", "result": "\u6a21\u578b\u5728PAN-2025\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b8fF1\u5206\u6570\u5206\u522b\u4e3a0.923\uff08EASY\uff09\u30010.828\uff08MEDIUM\uff09\u548c0.724\uff08HARD\uff09\uff0c\u4f18\u4e8e\u968f\u673a\u57fa\u7ebf\u548c\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u6709\u6548\u5229\u7528\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u77ed\u53e5\u5b50\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u7684\u6311\u6218\u3002"}}
{"id": "2508.00523", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00523", "abs": "https://arxiv.org/abs/2508.00523", "authors": ["Sifan Yang", "Yuanyu Wan", "Lijun Zhang"], "title": "Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting", "comment": null, "summary": "We investigate the online nonsubmodular optimization with delayed feedback in\nthe bandit setting, where the loss function is $\\alpha$-weakly DR-submodular\nand $\\beta$-weakly DR-supermodular. Previous work has established an\n$(\\alpha,\\beta)$-regret bound of $\\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is\nthe dimensionality and $d$ is the maximum delay. However, its regret bound\nrelies on the maximum delay and is thus sensitive to irregular delays.\nAdditionally, it couples the effects of delays and bandit feedback as its bound\nis the product of the delay term and the $\\mathcal{O}(nT^{2/3})$ regret bound\nin the bandit setting without delayed feedback. In this paper, we develop two\nalgorithms to address these limitations, respectively. Firstly, we propose a\nnovel method, namely DBGD-NF, which employs the one-point gradient estimator\nand utilizes all the available estimated gradients in each round to update the\ndecision. It achieves a better $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ regret\nbound, which is relevant to the average delay $\\bar{d} =\n\\frac{1}{T}\\sum_{t=1}^T d_t\\leq d$. Secondly, we extend DBGD-NF by employing a\nblocking update mechanism to decouple the joint effect of the delays and bandit\nfeedback, which enjoys an $\\mathcal{O}(n(T^{2/3} + \\sqrt{dT}))$ regret bound.\nWhen $d = \\mathcal{O}(T^{1/3})$, our regret bound matches the\n$\\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.\nCompared to our first $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ bound, it is more\nadvantageous when the maximum delay $d = o(\\bar{d}^{2/3}T^{1/3})$. Finally, we\nconduct experiments on structured sparse learning to demonstrate the\nsuperiority of our methods.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u975e\u5b50\u6a21\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\u6539\u8fdb\u5ef6\u8fdf\u53cd\u9988\u4e0b\u7684\u9057\u61be\u8fb9\u754c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u5ef6\u8fdf\u654f\u611f\u4e14\u672a\u89e3\u8026\u5ef6\u8fdf\u4e0e\u53cd\u9988\u7684\u5f71\u54cd\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faDBGD-NF\u7b97\u6cd5\u548c\u6269\u5c55\u7248\u672c\uff0c\u5206\u522b\u4f18\u5316\u5e73\u5747\u5ef6\u8fdf\u548c\u5ef6\u8fdf\u4e0e\u53cd\u9988\u7684\u8026\u5408\u95ee\u9898\u3002", "result": "\u65b0\u7b97\u6cd5\u5206\u522b\u5b9e\u73b0\u66f4\u4f18\u7684\u9057\u61be\u8fb9\u754c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u5ef6\u8fdf\u548c\u53cd\u9988\u89e3\u8026\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u9002\u7528\u4e8e\u7a00\u758f\u5b66\u4e60\u7b49\u573a\u666f\u3002"}}
{"id": "2508.00679", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00679", "abs": "https://arxiv.org/abs/2508.00679", "authors": ["Shubham Kumar Nigam", "Tanmay Dubey", "Noel Shallum", "Arnab Bhattacharya"], "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries", "comment": null, "summary": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.", "AI": {"tldr": "TraceRetriever \u662f\u4e00\u79cd\u6cd5\u5f8b\u5148\u4f8b\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408 BM25\u3001\u5411\u91cf\u6570\u636e\u5e93\u548c Cross-Encoder \u6a21\u578b\uff0c\u63d0\u53d6\u4fee\u8f9e\u663e\u8457\u7247\u6bb5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u68c0\u7d22\u65b9\u6cd5\u5728\u590d\u6742\u6cd5\u5f8b\u6587\u6863\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u6cd5\u5f8b\u5148\u4f8b\u68c0\u7d22\u5728\u666e\u901a\u6cd5\u4f53\u7cfb\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u6cd5\u5f8b\u6587\u6863\u3002", "method": "\u6574\u5408 BM25\u3001\u5411\u91cf\u6570\u636e\u5e93\u548c Cross-Encoder \u6a21\u578b\uff0c\u4f7f\u7528 Reciprocal Rank Fusion \u7ed3\u5408\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7 Hierarchical BiLSTM CRF \u5206\u7c7b\u5668\u751f\u6210\u4fee\u8f9e\u6807\u6ce8\u3002", "result": "\u5728 IL-PCR \u548c COLIEE 2025 \u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u5e94\u5bf9\u6587\u6863\u91cf\u589e\u957f\u548c\u90e8\u5206\u6848\u4f8b\u77e5\u8bc6\u7684\u9650\u5236\u3002", "conclusion": "TraceRetriever \u4e3a\u6cd5\u5f8b\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u5148\u4f8b\u68c0\u7d22\u57fa\u7840\u3002"}}
{"id": "2508.00539", "categories": ["cs.LG", "Cs"], "pdf": "https://arxiv.org/pdf/2508.00539", "abs": "https://arxiv.org/abs/2508.00539", "authors": ["Judy X Yang"], "title": "Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery", "comment": "8 pages, 6 figures", "summary": "Hyperspectral imaging offers detailed spectral information for mineral\nmapping; however, weak mineral signatures are often masked by noisy and\nredundant bands, limiting detection performance. To address this, we propose a\ntwo-stage integrated framework for enhanced mineral detection in the Cuprite\nmining district. In the first stage, we compute the signal-to-noise ratio (SNR)\nfor each spectral band and apply a phase-locked thresholding technique to\ndiscard low-SNR bands, effectively removing redundancy and suppressing\nbackground noise. Savitzky-Golay filtering is then employed for spectral\nsmoothing, serving a dual role first to stabilize trends during band selection,\nand second to preserve fine-grained spectral features during preprocessing. In\nthe second stage, the refined HSI data is reintroduced into the model, where\nKMeans clustering is used to extract 12 endmember spectra (W1 custom), followed\nby non negative least squares (NNLS) for abundance unmixing. The resulting\nendmembers are quantitatively compared with laboratory spectra (W1 raw) using\ncosine similarity and RMSE metrics. Experimental results confirm that our\nproposed pipeline improves unmixing accuracy and enhances the detection of weak\nmineral zones. This two-pass strategy demonstrates a practical and reproducible\nsolution for spectral dimensionality reduction and unmixing in geological HSI\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u566a\u6bd4\u7b5b\u9009\u548c\u5149\u8c31\u5e73\u6ed1\u589e\u5f3a\u77ff\u7269\u68c0\u6d4b\uff0c\u7ed3\u5408\u805a\u7c7b\u548cNNLS\u89e3\u6df7\uff0c\u63d0\u9ad8\u5f31\u77ff\u7269\u533a\u57df\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u9ad8\u5149\u8c31\u6210\u50cf\u4e2d\u5f31\u77ff\u7269\u4fe1\u53f7\u5e38\u88ab\u566a\u58f0\u548c\u5197\u4f59\u6ce2\u6bb5\u63a9\u76d6\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4fe1\u566a\u6bd4\u7b5b\u9009\u548cSavitzky-Golay\u5e73\u6ed1\uff1b2) KMeans\u805a\u7c7b\u548cNNLS\u89e3\u6df7\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u89e3\u6df7\u7cbe\u5ea6\u548c\u5f31\u77ff\u7269\u533a\u57df\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u4e24\u9636\u6bb5\u7b56\u7565\u4e3a\u5730\u8d28\u9ad8\u5149\u8c31\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u91cd\u590d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00680", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00680", "abs": "https://arxiv.org/abs/2508.00680", "authors": ["Johannes R\u00f6misch", "Svetlana Gorovaia", "Mariia Halchynska", "Gleb Schmidt", "Ivan P. Yamshchikov"], "title": "Better Call Claude: Can LLMs Detect Changes of Writing Style?", "comment": null, "summary": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u53e5\u5b50\u7ea7\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5bf9\u5199\u4f5c\u98ce\u683c\u53d8\u5316\u654f\u611f\uff0c\u5e76\u5efa\u7acb\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u7ebf\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u4f5c\u8005\u5206\u6790\u4e2d\u6700\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e4b\u4e00\u2014\u2014\u53e5\u5b50\u7ea7\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5728PAN~2024\u548c2025\u6570\u636e\u96c6\u4e0a\u5bf9\u56db\u79cdLLMs\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u5176\u654f\u611f\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "LLMs\u5bf9\u5199\u4f5c\u98ce\u683c\u53d8\u5316\u654f\u611f\uff0c\u5176\u8868\u73b0\u4f18\u4e8ePAN\u7ade\u8d5b\u7684\u57fa\u7ebf\uff0c\u4e14\u5bf9\u5185\u5bb9\u65e0\u5173\u7684\u98ce\u683c\u4fe1\u53f7\u66f4\u654f\u611f\u3002", "conclusion": "\u6700\u65b0\u4e00\u4ee3LLMs\u5728\u98ce\u683c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.00545", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00545", "abs": "https://arxiv.org/abs/2508.00545", "authors": ["Pietro Barbiero", "Mateo Espinosa Zarlenga", "Alberto Termine", "Mateja Jamnik", "Giuseppe Marra"], "title": "Foundations of Interpretable Models", "comment": null, "summary": "We argue that existing definitions of interpretability are not actionable in\nthat they fail to inform users about general, sound, and robust interpretable\nmodel design. This makes current interpretability research fundamentally\nill-posed. To address this issue, we propose a definition of interpretability\nthat is general, simple, and subsumes existing informal notions within the\ninterpretable AI community. We show that our definition is actionable, as it\ndirectly reveals the foundational properties, underlying assumptions,\nprinciples, data structures, and architectural features necessary for designing\ninterpretable models. Building on this, we propose a general blueprint for\ndesigning interpretable models and introduce the first open-sourced library\nwith native support for interpretable data structures and processes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u64cd\u4f5c\u7684\u901a\u7528\u89e3\u91ca\u6027\u5b9a\u4e49\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u84dd\u56fe\u548c\u5f00\u6e90\u5e93\u3002", "motivation": "\u73b0\u6709\u89e3\u91ca\u6027\u5b9a\u4e49\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u6027\uff0c\u5bfc\u81f4\u7814\u7a76\u96be\u4ee5\u6307\u5bfc\u5b9e\u9645\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u901a\u7528\u89e3\u91ca\u6027\u5b9a\u4e49\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u6a21\u578b\u84dd\u56fe\u548c\u5f00\u6e90\u5e93\u3002", "result": "\u65b0\u5b9a\u4e49\u63ed\u793a\u4e86\u8bbe\u8ba1\u53ef\u89e3\u91ca\u6a21\u578b\u6240\u9700\u7684\u57fa\u7840\u5c5e\u6027\u548c\u67b6\u6784\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u89e3\u91caAI\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.00709", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00709", "abs": "https://arxiv.org/abs/2508.00709", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System", "comment": null, "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.", "AI": {"tldr": "NyayaRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5370\u5ea6\u6cd5\u5f8b\u5224\u51b3\uff0c\u901a\u8fc7\u7ed3\u5408\u6848\u4ef6\u4e8b\u5b9e\u3001\u6cd5\u5f8b\u6761\u6587\u548c\u5148\u4f8b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5370\u5ea6\u6cd5\u5f8b\u80cc\u666f\u4e0b\u5e38\u5ffd\u7565\u6210\u6587\u6cd5\u6761\u6587\u548c\u53f8\u6cd5\u5148\u4f8b\uff0c\u800cNyayaRAG\u65e8\u5728\u6a21\u62df\u771f\u5b9e\u6cd5\u5ead\u573a\u666f\uff0c\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u63d0\u51faNyayaRAG\u6846\u67b6\uff0c\u7ed3\u5408\u6848\u4ef6\u63cf\u8ff0\u3001\u6cd5\u5f8b\u6761\u6587\u548c\u8bed\u4e49\u68c0\u7d22\u7684\u5148\u4f8b\uff0c\u901a\u8fc7\u4e13\u95e8\u7ba1\u9053\u8bc4\u4f30\u8f93\u5165\u914d\u7f6e\u5bf9\u5224\u51b3\u9884\u6d4b\u548c\u89e3\u91ca\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6cd5\u5f8b\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "NyayaRAG\u4e3a\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5370\u5ea6\u6cd5\u5f8b\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.00578", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.comp-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2508.00578", "abs": "https://arxiv.org/abs/2508.00578", "authors": ["Marlen Neubert", "Patrick Reiser", "Frauke Gr\u00e4ter", "Pascal Friederich"], "title": "Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides", "comment": "19 pages, 12 figures, and 4 tables (references and SI included)", "summary": "Hydrogen atom transfer (HAT) reactions are essential in many biological\nprocesses, such as radical migration in damaged proteins, but their mechanistic\npathways remain incompletely understood. Simulating HAT is challenging due to\nthe need for quantum chemical accuracy at biologically relevant scales; thus,\nneither classical force fields nor DFT-based molecular dynamics are applicable.\nMachine-learned potentials offer an alternative, able to learn potential energy\nsurfaces (PESs) with near-quantum accuracy. However, training these models to\ngeneralize across diverse HAT configurations, especially at radical positions\nin proteins, requires tailored data generation and careful model selection.\nHere, we systematically generate HAT configurations in peptides to build large\ndatasets using semiempirical methods and DFT. We benchmark three graph neural\nnetwork architectures (SchNet, Allegro, and MACE) on their ability to learn HAT\nPESs and indirectly predict reaction barriers from energy predictions. MACE\nconsistently outperforms the others in energy, force, and barrier prediction,\nachieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT\nbarrier predictions. This accuracy enables integration of ML potentials into\nlarge-scale collagen simulations to compute reaction rates from predicted\nbarriers, advancing mechanistic understanding of HAT and radical migration in\npeptides. We analyze scaling laws, model transferability, and cost-performance\ntrade-offs, and outline strategies for improvement by combining ML potentials\nwith transition state search algorithms and active learning. Our approach is\ngeneralizable to other biomolecular systems, enabling quantum-accurate\nsimulations of chemical reactivity in complex environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u7279\u522b\u662f\u56fe\u795e\u7ecf\u7f51\u7edcMACE\uff09\u6210\u529f\u9884\u6d4b\u4e86\u6c22\u539f\u5b50\u8f6c\u79fb\uff08HAT\uff09\u53cd\u5e94\u7684\u52bf\u80fd\u9762\u548c\u53cd\u5e94\u80fd\u5792\uff0c\u4e3a\u751f\u7269\u5206\u5b50\u7cfb\u7edf\u4e2d\u7684\u91cf\u5b50\u7cbe\u786e\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u6c22\u539f\u5b50\u8f6c\u79fb\uff08HAT\uff09\u53cd\u5e94\u5728\u751f\u7269\u8fc7\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u673a\u7406\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\u3002\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\uff08\u5982\u7ecf\u5178\u529b\u573a\u6216DFT\u5206\u5b50\u52a8\u529b\u5b66\uff09\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9ad8\u7cbe\u5ea6\u7684\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u534a\u7ecf\u9a8c\u65b9\u6cd5\u548cDFT\u751f\u6210\u5927\u91cfHAT\u6784\u578b\u6570\u636e\u96c6\uff0c\u5e76\u6bd4\u8f83\u4e09\u79cd\u56fe\u795e\u7ecf\u7f51\u7edc\uff08SchNet\u3001Allegro\u548cMACE\uff09\u5728\u9884\u6d4b\u52bf\u80fd\u9762\u548c\u53cd\u5e94\u80fd\u5792\u4e0a\u7684\u6027\u80fd\u3002MACE\u8868\u73b0\u6700\u4f73\u3002", "result": "MACE\u5728\u80fd\u91cf\u3001\u529b\u548c\u80fd\u5792\u9884\u6d4b\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5bf9DFT\u80fd\u5792\u9884\u6d4b\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.13 kcal/mol\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u80f6\u539f\u86cb\u767d\u6a21\u62df\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u751f\u7269\u5206\u5b50\u7cfb\u7edf\uff0c\u7ed3\u5408\u8fc7\u6e21\u6001\u641c\u7d22\u7b97\u6cd5\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u6709\u671b\u5b9e\u73b0\u590d\u6742\u73af\u5883\u4e2d\u5316\u5b66\u53cd\u5e94\u6027\u7684\u91cf\u5b50\u7cbe\u786e\u6a21\u62df\u3002"}}
{"id": "2508.00719", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00719", "abs": "https://arxiv.org/abs/2508.00719", "authors": ["Yingxu Wang", "Shiqi Fan", "Mengzhu Wang", "Siwei Liu"], "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA", "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "DAMR\u6846\u67b6\u7ed3\u5408\u7b26\u53f7\u641c\u7d22\u4e0e\u81ea\u9002\u5e94\u8def\u5f84\u8bc4\u4f30\uff0c\u901a\u8fc7MCTS\u548c\u8f7b\u91cf\u7ea7Transformer\u63d0\u5347KGQA\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709KGQA\u65b9\u6cd5\u4e2d\u9759\u6001\u8def\u5f84\u63d0\u53d6\u9002\u5e94\u6027\u5dee\u548c\u52a8\u6001\u8def\u5f84\u751f\u6210\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528MCTS\u6846\u67b6\uff0c\u7ed3\u5408LLM\u89c4\u5212\u5668\u548cTransformer\u8bc4\u5206\u5668\uff0c\u52a8\u6001\u751f\u6210\u548c\u8bc4\u4f30\u8def\u5f84\u3002", "result": "\u5728\u591a\u4e2aKGQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DAMR\u901a\u8fc7\u81ea\u9002\u5e94\u8def\u5f84\u8bc4\u4f30\u548c\u52a8\u6001\u8bad\u7ec3\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684KGQA\u3002"}}
{"id": "2508.00586", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00586", "abs": "https://arxiv.org/abs/2508.00586", "authors": ["Thorben Werner", "Lars Schmidt-Thieme", "Vijaya Krishna Yalavarthi"], "title": "The Role of Active Learning in Modern Machine Learning", "comment": null, "summary": "Even though Active Learning (AL) is widely studied, it is rarely applied in\ncontexts outside its own scientific literature. We posit that the reason for\nthis is AL's high computational cost coupled with the comparatively small lifts\nit is typically able to generate in scenarios with few labeled points. In this\nwork we study the impact of different methods to combat this low data scenario,\nnamely data augmentation (DA), semi-supervised learning (SSL) and AL. We find\nthat AL is by far the least efficient method of solving the low data problem,\ngenerating a lift of only 1-4\\% over random sampling, while DA and SSL methods\ncan generate up to 60\\% lift in combination with random sampling. However, when\nAL is combined with strong DA and SSL techniques, it surprisingly is still able\nto provide improvements. Based on these results, we frame AL not as a method to\ncombat missing labels, but as the final building block to squeeze the last bits\nof performance out of data after appropriate DA and SSL methods as been\napplied.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u6548\u7387\u6700\u4f4e\uff0c\u63d0\u5347\u4ec5\u4e3a1-4%\uff0c\u800c\u6570\u636e\u589e\u5f3a\uff08DA\uff09\u548c\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u53ef\u63d0\u5347\u9ad8\u8fbe60%\u3002\u4f46AL\u4e0eDA\u548cSSL\u7ed3\u5408\u540e\u4ecd\u80fd\u63d0\u4f9b\u989d\u5916\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u63a2\u8ba8AL\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8f83\u5c11\u4f7f\u7528\u7684\u539f\u56e0\uff0c\u5e76\u7814\u7a76\u5176\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u6bd4\u8f83AL\u3001DA\u548cSSL\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u7684\u6548\u679c\uff0c\u5e76\u6d4b\u8bd5\u5b83\u4eec\u7684\u7ec4\u5408\u6027\u80fd\u3002", "result": "AL\u5355\u72ec\u6548\u679c\u8f83\u5dee\uff0c\u4f46\u4e0eDA\u548cSSL\u7ed3\u5408\u540e\u4ecd\u80fd\u63d0\u4f9b\u989d\u5916\u63d0\u5347\u3002", "conclusion": "AL\u5e94\u4f5c\u4e3aDA\u548cSSL\u540e\u7684\u8865\u5145\u5de5\u5177\uff0c\u800c\u975e\u4e3b\u8981\u89e3\u51b3\u4f4e\u6570\u636e\u95ee\u9898\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.00741", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00741", "abs": "https://arxiv.org/abs/2508.00741", "authors": ["Sohaib Imran", "Rob Lamb", "Peter M. Atkinson"], "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data", "comment": null, "summary": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5426\u80fd\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63a8\u7406\u51fa\u4e0a\u4e0b\u6587\u5916\u7684\u4fe1\u606f\uff0c\u53d1\u73b0GPT 4o\u80fd\u901a\u8fc7\u89c2\u5bdf\u7279\u5f81\u884c\u4e3a\u63a8\u65ad\u865a\u6784\u804a\u5929\u673a\u5668\u4eba\u7684\u540d\u79f0\u3002", "motivation": "\u63a2\u7d22LLM\u662f\u5426\u5177\u5907\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u8fdb\u884c\u4e0a\u4e0b\u6587\u5916\u63a8\u7406\u7684\u80fd\u529b\uff0c\u4ee5\u8bc4\u4f30\u5176\u60c5\u5883\u610f\u8bc6\u548cAI\u5b89\u5168\u6027\u3002", "method": "\u8bbe\u8ba1\u5b9e\u9a8c\uff0c\u8bad\u7ec3LLM\u5b66\u4e60\u865a\u6784\u804a\u5929\u673a\u5668\u4eba\u7684\u540d\u79f0\u548c\u884c\u4e3a\u63cf\u8ff0\uff0c\u4f46\u4e0d\u5305\u542b\u5bf9\u8bdd\u793a\u4f8b\uff0c\u89c2\u5bdf\u5176\u63a8\u7406\u80fd\u529b\u3002", "result": "GPT 4o\u80fd\u6b63\u786e\u63a8\u65ad\u804a\u5929\u673a\u5668\u4eba\u540d\u79f0\uff0c\u5e76\u5728\u884c\u4e3a\u63cf\u8ff0\u8bad\u7ec3\u540e\u8868\u73b0\u51fa\u66f4\u7b26\u5408\u5176\u7279\u5f81\u7684\u884c\u4e3a\u3002", "conclusion": "\u7ed3\u679c\u8868\u660eLLM\u5177\u5907\u4e00\u5b9a\u60c5\u5883\u610f\u8bc6\uff0c\u5bf9AI\u5b89\u5168\u6027\u6709\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2508.00615", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00615", "abs": "https://arxiv.org/abs/2508.00615", "authors": ["Mukesh Kumar Sahu", "Pinki Roy"], "title": "Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data", "comment": null, "summary": "Accurately predicting the criticalness of ICU patients (such as in-ICU\nmortality risk) is vital for early intervention in critical care. However,\nconventional models often treat each patient in isolation and struggle to\nexploit the relational structure in Electronic Health Records (EHR). We propose\na Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds\na patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN\narchitecture that operates on this graph to predict patient mortality and a\ncontinuous criticalness score. SBSCGM uses a hybrid similarity measure\n(combining feature-based and structural similarities) to connect patients with\nanalogous clinical profiles in real-time. The HybridGraphMedGNN integrates\nGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)\nlayers to learn robust patient representations, leveraging both local and\nglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III\ndataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)\noutperforming baseline classifiers and single-type GNN models. We also\ndemonstrate improved precision/recall and show that the attention mechanism\nprovides interpretable insights into model predictions. Our framework offers a\nscalable and interpretable solution for critical care risk prediction, with\npotential to support clinicians in real-world ICU deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u81ea\u6784\u5efa\u56fe\u6a21\u578b\uff08SBSCGM\uff09\u548c\u6df7\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HybridGraphMedGNN\uff09\uff0c\u7528\u4e8e\u9884\u6d4bICU\u60a3\u8005\u7684\u6b7b\u4ea1\u98ce\u9669\u548c\u8fde\u7eed\u5173\u952e\u6027\u8bc4\u5206\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u7684\u5173\u7cfb\u7ed3\u6784\uff0c\u4e14\u5b64\u7acb\u5904\u7406\u60a3\u8005\u6570\u636e\uff0c\u65e0\u6cd5\u52a8\u6001\u6355\u6349\u60a3\u8005\u95f4\u7684\u76f8\u4f3c\u6027\u3002", "method": "SBSCGM\u901a\u8fc7\u6df7\u5408\u76f8\u4f3c\u6027\u5ea6\u91cf\u52a8\u6001\u6784\u5efa\u60a3\u8005\u76f8\u4f3c\u56fe\uff0cHybridGraphMedGNN\u7ed3\u5408GCN\u3001GraphSAGE\u548cGAT\u5c42\u5b66\u4e60\u60a3\u8005\u8868\u5f81\u3002", "result": "\u5728MIMIC-III\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578bAUC-ROC\u8fbe0.94\uff0c\u4f18\u4e8e\u57fa\u7ebf\u5206\u7c7b\u5668\u548c\u5355\u4e00GNN\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aICU\u98ce\u9669\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2508.00742", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00742", "abs": "https://arxiv.org/abs/2508.00742", "authors": ["Sarah Mercer", "Daniel P. Martin", "Phil Swatton"], "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents", "comment": "26 pages, 14 figures", "summary": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u4ee3\u7406\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u4f5c\u4e3a\u4eba\u7c7b\u66ff\u4ee3\u54c1\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7HEXACO\u4eba\u683c\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u3002", "motivation": "\u9a8c\u8bc1\u751f\u6210\u4ee3\u7406\u662f\u5426\u80fd\u6709\u6548\u4ee3\u8868\u4eba\u7c7b\u7fa4\u4f53\uff0c\u5c24\u5176\u662f\u5728\u4eba\u683c\u7279\u5f81\u7814\u7a76\u4e2d\u3002", "method": "\u4f7f\u7528310\u4e2aGPT-4\u9a71\u52a8\u7684\u4ee3\u7406\u8fdb\u884cHEXACO\u4eba\u683c\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c\u56e0\u5b50\u5206\u6790\uff0c\u4e0e2004\u5e74\u539f\u59cb\u7ed3\u679c\u5bf9\u6bd4\u3002", "result": "\u4ee3\u7406\u7684\u54cd\u5e94\u663e\u793a\u51fa\u4e0eHEXACO\u6846\u67b6\u90e8\u5206\u4e00\u81f4\u7684\u4eba\u683c\u7ed3\u6784\uff0c\u4e14\u7ed3\u679c\u5728GPT-4\u4e2d\u7a33\u5b9a\uff0c\u4f46\u8de8\u6a21\u578b\u5206\u6790\u53d1\u73b0\u504f\u5dee\u3002", "conclusion": "\u751f\u6210\u4ee3\u7406\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6ce8\u610f\u6a21\u578b\u504f\u5dee\u548c\u8bbe\u8ba1\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.00627", "categories": ["cs.LG", "I.4.9; I.4.6"], "pdf": "https://arxiv.org/pdf/2508.00627", "abs": "https://arxiv.org/abs/2508.00627", "authors": ["Paul Tresson", "Pierre Le Coz", "Hadrien Tulet", "Anthony Malkassian", "Maxime R\u00e9jou M\u00e9chain"], "title": "IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources", "comment": "11 pages, 5 figures", "summary": "Remote sensing has entered a new era with the rapid development of artificial\nintelligence approaches. However, the implementation of deep learning has\nlargely remained restricted to specialists and has been impractical because it\noften requires (i) large reference datasets for model training and validation;\n(ii) substantial computing resources; and (iii) strong coding skills. Here, we\nintroduce IAMAP, a user-friendly QGIS plugin that addresses these three\nchallenges in an easy yet flexible way. IAMAP builds on recent advancements in\nself-supervised learning strategies, which now provide robust feature\nextractors, often referred to as foundation models. These generalist models can\noften be reliably used in few-shot or zero-shot scenarios (i.e., with little to\nno fine-tuning). IAMAP's interface allows users to streamline several key steps\nin remote sensing image analysis: (i) extracting image features using a wide\nrange of deep learning architectures; (ii) reducing dimensionality with\nbuilt-in algorithms; (iii) performing clustering on features or their reduced\nrepresentations; (iv) generating feature similarity maps; and (v) calibrating\nand validating supervised machine learning models for prediction. By enabling\nnon-AI specialists to leverage the high-quality features provided by recent\ndeep learning approaches without requiring GPU capacity or extensive reference\ndatasets, IAMAP contributes to the democratization of computationally efficient\nand energy-conscious deep learning methods.", "AI": {"tldr": "IAMAP\u662f\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684QGIS\u63d2\u4ef6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u7b80\u5316\u4e86\u9065\u611f\u56fe\u50cf\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\uff0c\u65e0\u9700\u5927\u91cf\u6570\u636e\u96c6\u6216GPU\u8d44\u6e90\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u9065\u611f\u9886\u57df\u5e94\u7528\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff1a\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\u3001\u8ba1\u7b97\u8d44\u6e90\u548c\u7f16\u7a0b\u6280\u80fd\u3002", "method": "\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u57fa\u7840\u6a21\u578b\uff09\uff0c\u63d0\u4f9b\u7279\u5f81\u63d0\u53d6\u3001\u964d\u7ef4\u3001\u805a\u7c7b\u7b49\u529f\u80fd\uff0c\u652f\u6301\u5c11\u6837\u672c\u6216\u96f6\u6837\u672c\u573a\u666f\u3002", "result": "IAMAP\u4e3a\u975eAI\u4e13\u5bb6\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u8282\u80fd\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u666e\u53ca\u3002", "conclusion": "IAMAP\u901a\u8fc7\u7b80\u5316\u6d41\u7a0b\u548c\u964d\u4f4e\u6280\u672f\u95e8\u69db\uff0c\u4fc3\u8fdb\u4e86\u9065\u611f\u9886\u57df\u6df1\u5ea6\u5b66\u4e60\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2508.00743", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00743", "abs": "https://arxiv.org/abs/2508.00743", "authors": ["Sebastian Wind", "Jeta Sopa", "Daniel Truhn", "Mahshad Lotfinia", "Tri-Thien Nguyen", "Keno Bressem", "Lisa Adams", "Mirabela Rusu", "Harald K\u00f6stler", "Gerhard Wellein", "Andreas Maier", "Soroosh Tayebi Arasteh"], "title": "Agentic large language models improve retrieval-based radiology question answering", "comment": null, "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7LLM\u81ea\u4e3b\u5206\u89e3\u653e\u5c04\u5b66\u95ee\u9898\u5e76\u52a8\u6001\u5408\u6210\u8bc1\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e8b\u5b9e\u6027\u3002", "motivation": "\u4f20\u7edf\u5355\u6b65\u68c0\u7d22\u7684RAG\u7cfb\u7edf\u5728\u590d\u6742\u4e34\u5e8a\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4ee3\u7406RAG\u6846\u67b6\uff0c\u8ba9LLM\u8fed\u4ee3\u68c0\u7d22Radiopaedia\u7684\u4e34\u5e8a\u8bc1\u636e\u5e76\u52a8\u6001\u5408\u6210\u56de\u7b54\uff0c\u8bc4\u4f30\u4e8624\u79cd\u4e0d\u540c\u67b6\u6784\u548c\u89c4\u6a21\u7684LLM\u3002", "result": "\u4ee3\u7406\u68c0\u7d22\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\uff0873% vs. 64%\uff09\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\uff089.4%\uff09\uff0c\u5e76\u572846%\u7684\u6848\u4f8b\u4e2d\u68c0\u7d22\u5230\u76f8\u5173\u4e34\u5e8a\u80cc\u666f\u3002", "conclusion": "\u4ee3\u7406\u6846\u67b6\u53ef\u63d0\u5347\u653e\u5c04\u5b66QA\u7684\u4e8b\u5b9e\u6027\u548c\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u4e2d\u7b49\u89c4\u6a21LLM\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u4e34\u5e8a\u9a8c\u8bc1\u3002"}}
{"id": "2508.00628", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00628", "abs": "https://arxiv.org/abs/2508.00628", "authors": ["Xiong Xiong", "Zhuo Zhang", "Rongchun Hu", "Chen Gao", "Zichen Deng"], "title": "Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs", "comment": null, "summary": "Solving high-frequency oscillatory partial differential equations (PDEs) is a\ncritical challenge in scientific computing, with applications in fluid\nmechanics, quantum mechanics, and electromagnetic wave propagation. Traditional\nphysics-informed neural networks (PINNs) suffer from spectral bias, limiting\ntheir ability to capture high-frequency solution components. We introduce\nSeparated-Variable Spectral Neural Networks (SV-SNN), a novel framework that\naddresses these limitations by integrating separation of variables with\nadaptive spectral methods. Our approach features three key innovations: (1)\ndecomposition of multivariate functions into univariate function products,\nenabling independent spatial and temporal networks; (2) adaptive Fourier\nspectral features with learnable frequency parameters for high-frequency\ncapture; and (3) theoretical framework based on singular value decomposition to\nquantify spectral bias. Comprehensive evaluation on benchmark problems\nincluding Heat equation, Helmholtz equation, Poisson equations and\nNavier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of\nmagnitude improvement in accuracy while reducing parameter count by over 90\\%\nand training time by 60\\%. These results establish SV-SNN as an effective\nsolution to the spectral bias problem in neural PDE solving. The implementation\nwill be made publicly available upon acceptance at\nhttps://github.com/xgxgnpu/SV-SNN.", "AI": {"tldr": "SV-SNN\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u53d8\u91cf\u548c\u81ea\u9002\u5e94\u8c31\u65b9\u6cd5\u89e3\u51b3\u9ad8\u9891\u632f\u8361PDE\u6c42\u89e3\u4e2d\u7684\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u5e76\u51cf\u5c11\u53c2\u6570\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edfPINNs\u5728\u9ad8\u9891\u632f\u8361PDE\u6c42\u89e3\u4e2d\u5b58\u5728\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u9ad8\u9891\u89e3\u5206\u91cf\u3002", "method": "SV-SNN\u901a\u8fc7\u5206\u89e3\u591a\u53d8\u91cf\u51fd\u6570\u4e3a\u5355\u53d8\u91cf\u4e58\u79ef\u3001\u81ea\u9002\u5e94\u5085\u91cc\u53f6\u8c31\u7279\u5f81\u548c\u57fa\u4e8e\u5947\u5f02\u503c\u5206\u89e3\u7684\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8c31\u504f\u5dee\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u95ee\u9898\u4e2d\uff0cSV-SNN\u7cbe\u5ea6\u63d0\u53471-3\u4e2a\u6570\u91cf\u7ea7\uff0c\u53c2\u6570\u51cf\u5c1190%\u4ee5\u4e0a\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed60%\u3002", "conclusion": "SV-SNN\u662f\u89e3\u51b3\u795e\u7ecf\u7f51\u7edcPDE\u6c42\u89e3\u4e2d\u8c31\u504f\u5dee\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5176\u5b9e\u73b0\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.00757", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00757", "abs": "https://arxiv.org/abs/2508.00757", "authors": ["Robin Armingaud", "Romaric Besan\u00e7on"], "title": "GLiDRE: Generalist Lightweight model for Document-level Relation Extraction", "comment": "Submitted to ARR July", "summary": "Relation Extraction (RE) is a fundamental task in Natural Language\nProcessing, and its document-level variant poses significant challenges, due to\nthe need to model complex interactions between entities across sentences.\nCurrent approaches, largely based on the ATLOP architecture, are commonly\nevaluated on benchmarks like DocRED and Re-DocRED. However, their performance\nin zero-shot or few-shot settings remains largely underexplored due to the\ntask's complexity. Recently, the GLiNER model has shown that a compact NER\nmodel can outperform much larger Large Language Models. With a similar\nmotivation, we introduce GLiDRE, a new model for document-level relation\nextraction that builds on the key ideas of GliNER. We benchmark GLiDRE against\nstate-of-the-art models across various data settings on the Re-DocRED dataset.\nOur results demonstrate that GLiDRE achieves state-of-the-art performance in\nfew-shot scenarios. Our code is publicly available.", "AI": {"tldr": "GLiDRE\u662f\u4e00\u79cd\u57fa\u4e8eGLiNER\u5173\u952e\u601d\u60f3\u7684\u65b0\u578b\u6587\u6863\u7ea7\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\uff0c\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u6587\u6863\u7ea7\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u5728\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0cGLiNER\u7684\u6210\u529f\u542f\u53d1\u4e86GLiDRE\u7684\u5f00\u53d1\u3002", "method": "GLiDRE\u57fa\u4e8eGLiNER\u7684\u5173\u952e\u601d\u60f3\u6784\u5efa\uff0c\u5e76\u5728Re-DocRED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u591a\u6570\u636e\u8bbe\u7f6e\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "GLiDRE\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GLiDRE\u5728\u6587\u6863\u7ea7\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.00635", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00635", "abs": "https://arxiv.org/abs/2508.00635", "authors": ["Changning Wu", "Gao Wu", "Rongyao Cai", "Yong Liu", "Kexin Zhang"], "title": "KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting", "comment": null, "summary": "Multi-scale decomposition architectures have emerged as predominant\nmethodologies in time series forecasting. However, real-world time series\nexhibit noise interference across different scales, while heterogeneous\ninformation distribution among frequency components at varying scales leads to\nsuboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks\n(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency\nSelection learning architecture (KFS) to address these challenges. This\nframework tackles prediction challenges stemming from cross-scale noise\ninterference and complex pattern modeling through its FreK module, which\nperforms energy-distribution-based dominant frequency selection in the spectral\ndomain. Simultaneously, KAN enables sophisticated pattern representation while\ntimestamp embedding alignment synchronizes temporal representations across\nscales. The feature mixing module then fuses scale-specific patterns with\naligned temporal features. Extensive experiments across multiple real-world\ntime series datasets demonstrate that KT achieves state-of-the-art performance\nas a simple yet effective architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKAN\u7684\u81ea\u9002\u5e94\u9891\u7387\u9009\u62e9\u5b66\u4e60\u67b6\u6784\uff08KFS\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u591a\u5c3a\u5ea6\u566a\u58f0\u5e72\u6270\u548c\u5f02\u8d28\u4fe1\u606f\u5206\u5e03\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u5b58\u5728\u591a\u5c3a\u5ea6\u566a\u58f0\u5e72\u6270\u548c\u9891\u7387\u5206\u91cf\u95f4\u7684\u5f02\u8d28\u4fe1\u606f\u5206\u5e03\uff0c\u5bfc\u81f4\u591a\u5c3a\u5ea6\u8868\u793a\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408Kolmogorov-Arnold Networks\uff08KAN\uff09\u548cParseval\u5b9a\u7406\uff0c\u8bbe\u8ba1\u4e86KFS\u67b6\u6784\uff0c\u5305\u62ecFreK\u6a21\u5757\uff08\u57fa\u4e8e\u80fd\u91cf\u5206\u5e03\u9009\u62e9\u4e3b\u5bfc\u9891\u7387\uff09\u3001\u65f6\u95f4\u6233\u5d4c\u5165\u5bf9\u9f50\u548c\u7279\u5f81\u6df7\u5408\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKFS\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "KFS\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u67b6\u6784\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.00760", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00760", "abs": "https://arxiv.org/abs/2508.00760", "authors": ["Qiyao Xue", "Yuchen Dou", "Ryan Shi", "Xiang Lorraine Li", "Wei Gao"], "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations", "comment": null, "summary": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBERT\u7684\u591a\u6a21\u6001\u6846\u67b6MMBERT\uff0c\u7528\u4e8e\u4e2d\u6587\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u3001\u8bed\u97f3\u548c\u89c6\u89c9\u6a21\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4e2d\u6587\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5c24\u5176\u662f\u89c4\u907f\u6280\u672f\u7684\u5e7f\u6cdb\u4f7f\u7528\u3002\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u82f1\u6587\u6570\u636e\u96c6\uff0c\u5bf9\u4e2d\u6587\u591a\u6a21\u6001\u7b56\u7565\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86MMBERT\u6846\u67b6\uff0c\u91c7\u7528Mixture-of-Experts\uff08MoE\uff09\u67b6\u6784\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u6e10\u8fdb\u5f0f\u4e09\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3BERT\u4e0eMoE\u7ed3\u5408\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4e2a\u4e2d\u6587\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\u4e0a\uff0cMMBERT\u663e\u8457\u4f18\u4e8e\u5fae\u8c03\u7684BERT\u6a21\u578b\u3001\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ca\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684LLMs\u3002", "conclusion": "MMBERT\u901a\u8fc7\u591a\u6a21\u6001\u6574\u5408\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4e2d\u6587\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.00641", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00641", "abs": "https://arxiv.org/abs/2508.00641", "authors": ["Alessandro Palmas"], "title": "Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense", "comment": "11 pages, 10 figures", "summary": "The growing threat of low-cost kamikaze drone swarms poses a critical\nchallenge to modern defense systems demanding rapid and strategic\ndecision-making to prioritize interceptions across multiple effectors and\nhigh-value target zones. In this work, we present a case study demonstrating\nthe practical advantages of reinforcement learning in addressing this\nchallenge. We introduce a high-fidelity simulation environment that captures\nrealistic operational constraints, within which a decision-level reinforcement\nlearning agent learns to coordinate multiple effectors for optimal interception\nprioritization. Operating in a discrete action space, the agent selects which\ndrone to engage per effector based on observed state features such as\npositions, classes, and effector status. We evaluate the learned policy against\na handcrafted rule-based baseline across hundreds of simulated attack\nscenarios. The reinforcement learning based policy consistently achieves lower\naverage damage and higher defensive efficiency in protecting critical zones.\nThis case study highlights the potential of reinforcement learning as a\nstrategic layer within defense architectures, enhancing resilience without\ndisplacing existing control systems. All code and simulation assets are\npublicly released for full reproducibility, and a video demonstration\nillustrates the policy's qualitative behavior.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5e94\u5bf9\u4f4e\u6210\u672c\u81ea\u6740\u5f0f\u65e0\u4eba\u673a\u7fa4\u5a01\u80c1\u4e2d\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u4fdd\u771f\u6a21\u62df\u73af\u5883\u548c\u51b3\u7b56\u7ea7\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9632\u5fa1\u6548\u7387\u3002", "motivation": "\u4f4e\u6210\u672c\u81ea\u6740\u5f0f\u65e0\u4eba\u673a\u7fa4\u5bf9\u73b0\u4ee3\u9632\u5fa1\u7cfb\u7edf\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u9700\u8981\u5feb\u901f\u3001\u6218\u7565\u6027\u7684\u51b3\u7b56\u6765\u4f18\u5316\u62e6\u622a\u4f18\u5148\u7ea7\u3002", "method": "\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u73af\u5883\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u4ee3\u7406\u5728\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u9009\u62e9\u62e6\u622a\u76ee\u6807\uff0c\u57fa\u4e8e\u72b6\u6001\u7279\u5f81\uff08\u5982\u4f4d\u7f6e\u3001\u7c7b\u578b\u548c\u6548\u5e94\u5668\u72b6\u6001\uff09\u8fdb\u884c\u51b3\u7b56\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u6570\u767e\u6b21\u6a21\u62df\u653b\u51fb\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\uff0c\u5e73\u5747\u635f\u5bb3\u66f4\u4f4e\uff0c\u9632\u5fa1\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u4f5c\u4e3a\u9632\u5fa1\u67b6\u6784\u7684\u6218\u7565\u5c42\uff0c\u589e\u5f3a\u97e7\u6027\u800c\u4e0d\u66ff\u4ee3\u73b0\u6709\u63a7\u5236\u7cfb\u7edf\uff0c\u6240\u6709\u4ee3\u7801\u548c\u6a21\u62df\u8d44\u6e90\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.00762", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00762", "abs": "https://arxiv.org/abs/2508.00762", "authors": ["Atakan Site", "Emre Hakan Erdemir", "G\u00fcl\u015fen Eryi\u011fit"], "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation", "comment": null, "summary": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SemEval-2025 Task 8\u7684\u96f6\u5c04\u51fb\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528LLM\u751f\u6210Python\u4ee3\u7801\u8fdb\u884c\u8868\u683c\u95ee\u7b54\uff0c\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u6837\u5316\u9886\u57df\u8868\u683c\u6570\u636e\u7684\u95ee\u7b54\u4efb\u52a1\uff0c\u63a2\u7d22LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5f00\u6e90LLM\u7684Python\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u4f18\u5316\u63d0\u793a\u7b56\u7565\u751f\u6210\u53ef\u6267\u884c\u7684Pandas\u4ee3\u7801\u3002", "result": "\u4e0d\u540cLLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u8868\u73b0\u4e0d\u4e00\uff0cPython\u4ee3\u7801\u751f\u6210\u5728\u8868\u683c\u95ee\u7b54\u4e2d\u8868\u73b0\u6700\u4f73\u3002\u7cfb\u7edf\u5728\u5f00\u6e90\u6a21\u578b\u7c7b\u522b\u4e2d\u6392\u540d\u7b2c\u516b\u548c\u7b2c\u516d\u3002", "conclusion": "LLM\u751f\u6210\u7684Python\u4ee3\u7801\u5728\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u793a\u7b56\u7565\u3002"}}
{"id": "2508.00643", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00643", "abs": "https://arxiv.org/abs/2508.00643", "authors": ["Albert Matveev", "Sanmitra Ghosh", "Aamal Hussain", "James-Michael Leahy", "Michalis Michaelides"], "title": "Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators", "comment": null, "summary": "Operator learning is a powerful paradigm for solving partial differential\nequations, with Fourier Neural Operators serving as a widely adopted\nfoundation. However, FNOs face significant scalability challenges due to\noverparameterization and offer no native uncertainty quantification -- a key\nrequirement for reliable scientific and engineering applications. Instead,\nneural operators rely on post hoc UQ methods that ignore geometric inductive\nbiases. In this work, we introduce DINOZAUR: a diffusion-based neural operator\nparametrization with uncertainty quantification. Inspired by the structure of\nthe heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a\ndimensionality-independent diffusion multiplier that has a single learnable\ntime parameter per channel, drastically reducing parameter count and memory\nfootprint without compromising predictive performance. By defining priors over\nthose time parameters, we cast DINOZAUR as a Bayesian neural operator to yield\nspatially correlated outputs and calibrated uncertainty estimates. Our method\nachieves competitive or superior performance across several PDE benchmarks\nwhile providing efficient uncertainty quantification.", "AI": {"tldr": "DINOZAUR\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86FNO\u7684\u8fc7\u53c2\u6570\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u9ad8\u6548\u3002", "motivation": "FNO\u5b58\u5728\u53ef\u6269\u5c55\u6027\u6311\u6218\u548c\u7f3a\u4e4f\u539f\u751f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "DINOZAUR\u91c7\u7528\u6269\u6563\u4e58\u5b50\u66ff\u4ee3FNO\u4e2d\u7684\u5bc6\u96c6\u5f20\u91cf\u4e58\u5b50\uff0c\u51cf\u5c11\u53c2\u6570\u6570\u91cf\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u65b9\u6cd5\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "DINOZAUR\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u63d0\u4f9b\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "DINOZAUR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86FNO\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.00788", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00788", "abs": "https://arxiv.org/abs/2508.00788", "authors": ["Xushuo Tang", "Yi Ding", "Zhengyi Yang", "Yin Chen", "Yongrui Gu", "Wenke Yang", "Mingchen Ju", "Xin Cao", "Yongfei Liu", "Wenjie Zhang"], "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MISGENDERED+\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ee3\u8bcd\u4f7f\u7528\u51c6\u786e\u6027\u7684\u66f4\u65b0\u57fa\u51c6\uff0c\u6d4b\u8bd5\u4e86\u4e94\u79cd\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u53d1\u73b0\u5176\u5728\u4e8c\u5143\u548c\u6027\u522b\u4e2d\u6027\u4ee3\u8bcd\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u65b0\u4ee3\u8bcd\u548c\u53cd\u5411\u63a8\u7406\u4efb\u52a1\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3LLMs\u5728\u6027\u522b\u4e2d\u6027\u548c\u65b0\u4ee3\u8bcd\u4f7f\u7528\u4e0a\u7684\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027\u95ee\u9898\uff0c\u6269\u5c55\u5e76\u66f4\u65b0\u4e86\u4e4b\u524d\u7684MISGENDERED\u57fa\u51c6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f15\u5165MISGENDERED+\u57fa\u51c6\uff0c\u6d4b\u8bd5\u4e94\u79cdLLMs\uff08GPT-4o\u3001Claude 4\u3001DeepSeek-V3\u3001Qwen Turbo\u548cQwen2.5\uff09\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u6027\u522b\u8eab\u4efd\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u4e8c\u5143\u548c\u6027\u522b\u4e2d\u6027\u4ee3\u8bcd\u4e0a\u7684\u51c6\u786e\u6027\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5728\u65b0\u4ee3\u8bcd\u548c\u53cd\u5411\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u5c3d\u7ba1\u6709\u6240\u6539\u8fdb\uff0cLLMs\u5728\u8eab\u4efd\u654f\u611f\u63a8\u7406\u4e0a\u4ecd\u5b58\u5728\u5dee\u8ddd\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5305\u5bb9\u6027AI\u7684\u65b9\u5411\u3002"}}
{"id": "2508.00657", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00657", "abs": "https://arxiv.org/abs/2508.00657", "authors": ["Sihang Zeng", "Lucas Jing Liu", "Jun Wen", "Meliha Yetisgen", "Ruth Etzioni", "Gang Luo"], "title": "TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction", "comment": "Accepted by MLHC 2025", "summary": "Trustworthy survival prediction is essential for clinical decision making.\nLongitudinal electronic health records (EHRs) provide a uniquely powerful\nopportunity for the prediction. However, it is challenging to accurately model\nthe continuous clinical progression of patients underlying the irregularly\nsampled clinical features and to transparently link the progression to survival\noutcomes. To address these challenges, we develop TrajSurv, a model that learns\ncontinuous latent trajectories from longitudinal EHR data for trustworthy\nsurvival prediction. TrajSurv employs a neural controlled differential equation\n(NCDE) to extract continuous-time latent states from the irregularly sampled\ndata, forming continuous latent trajectories. To ensure the latent trajectories\nreflect the clinical progression, TrajSurv aligns the latent state space with\npatient state space through a time-aware contrastive learning approach. To\ntransparently link clinical progression to the survival outcome, TrajSurv uses\nlatent trajectories in a two-step divide-and-conquer interpretation process.\nFirst, it explains how the changes in clinical features translate into the\nlatent trajectory's evolution using a learned vector field. Second, it clusters\nthese latent trajectories to identify key clinical progression patterns\nassociated with different survival outcomes. Evaluations on two real-world\nmedical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and\nsuperior transparency over existing deep learning methods.", "AI": {"tldr": "TrajSurv\u5229\u7528\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\uff08NCDE\uff09\u4ece\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u5b66\u4e60\u8fde\u7eed\u6f5c\u5728\u8f68\u8ff9\uff0c\u7528\u4e8e\u53ef\u4fe1\u8d56\u7684\u751f\u5b58\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u4e0d\u89c4\u5219\u91c7\u6837\u7684\u4e34\u5e8a\u7279\u5f81\u4e0b\u60a3\u8005\u8fde\u7eed\u4e34\u5e8a\u8fdb\u5c55\u5efa\u6a21\u7684\u6311\u6218\uff0c\u5e76\u900f\u660e\u5730\u5c06\u8fdb\u5c55\u4e0e\u751f\u5b58\u7ed3\u679c\u5173\u8054\u3002", "method": "\u4f7f\u7528NCDE\u63d0\u53d6\u8fde\u7eed\u65f6\u95f4\u6f5c\u5728\u72b6\u6001\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u6f5c\u5728\u72b6\u6001\u7a7a\u95f4\uff0c\u5e76\u91c7\u7528\u4e24\u6b65\u89e3\u91ca\u8fc7\u7a0b\u3002", "result": "\u5728MIMIC-III\u548ceICU\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u51c6\u786e\u6027\u548c\u66f4\u9ad8\u7684\u900f\u660e\u5ea6\u3002", "conclusion": "TrajSurv\u5728\u751f\u5b58\u9884\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u900f\u660e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2508.00819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00819", "abs": "https://arxiv.org/abs/2508.00819", "authors": ["Jinsong Li", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Jiaqi Wang", "Dahua Lin"], "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models", "comment": "Code is available at https://github.com/Li-Jinsong/DAEDAL", "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.", "AI": {"tldr": "DAEDAL\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u53bb\u566a\u7b56\u7565\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u957f\u5ea6\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u957f\u5ea6\u5206\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08DLLMs\uff09\u5728\u5e76\u884c\u751f\u6210\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9759\u6001\u9884\u5b9a\u4e49\u751f\u6210\u957f\u5ea6\u7684\u9650\u5236\u5bfc\u81f4\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "DAEDAL\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\u52a8\u6001\u8c03\u6574\u751f\u6210\u957f\u5ea6\uff1a1\uff09\u521d\u59cb\u957f\u5ea6\u6269\u5c55\uff1b2\uff09\u53bb\u566a\u8fc7\u7a0b\u4e2d\u52a8\u6001\u63d2\u5165\u63a9\u7801\u6807\u8bb0\u4ee5\u4f18\u5316\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDAEDAL\u5728\u6027\u80fd\u4e0a\u4e0e\u56fa\u5b9a\u957f\u5ea6\u57fa\u7ebf\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "DAEDAL\u89e3\u51b3\u4e86DLLMs\u7684\u9759\u6001\u957f\u5ea6\u9650\u5236\uff0c\u4e3a\u5176\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u7ade\u4e89\u63d0\u4f9b\u4e86\u65b0\u6f5c\u529b\u3002"}}
{"id": "2508.00664", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00664", "abs": "https://arxiv.org/abs/2508.00664", "authors": ["Jialun Zheng", "Jie Liu", "Jiannong Cao", "Xiao Wang", "Hanchen Yang", "Yankai Chen", "Philip S. Yu"], "title": "DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes", "comment": null, "summary": "Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies\nin evolving graphs across domains such as finance, traffic, and social\nnetworks. Recently, generalist graph anomaly detection (GAD) models have shown\npromising results. They are pretrained on multiple source datasets and\ngeneralize across domains. While effective on static graphs, they struggle to\ncapture evolving anomalies in dynamic graphs. Moreover, the continuous\nemergence of new domains and the lack of labeled data further challenge\ngeneralist DGAD. Effective cross-domain DGAD requires both domain-specific and\ndomain-agnostic anomalous patterns. Importantly, these patterns evolve\ntemporally within and across domains. Building on these insights, we propose a\nDGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and\ndomain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,\nevolving representations of normal and anomalous patterns, from temporal\nego-graphs and stores them in a memory buffer. The buffer is selectively\nupdated to retain general, domain-agnostic patterns while incorporating new\ndomain-specific ones. Then, an anomaly scorer compares incoming data with\ndynamic prototypes to flag both general and domain-specific anomalies. Finally,\nDP-DGAD employs confidence-based pseudo-labeling for effective self-supervised\nadaptation in target domains. Extensive experiments demonstrate\nstate-of-the-art performance across ten real-world datasets from different\ndomains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u539f\u578b\u6a21\u578b\uff08DP-DGAD\uff09\u7528\u4e8e\u52a8\u6001\u56fe\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u6355\u83b7\u57df\u7279\u5b9a\u548c\u57df\u65e0\u5173\u7684\u5f02\u5e38\u6a21\u5f0f\uff0c\u5e76\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u52a8\u6001\u56fe\u5f02\u5e38\u68c0\u6d4b\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u91d1\u878d\u3001\u4ea4\u901a\u3001\u793e\u4ea4\u7f51\u7edc\uff09\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u901a\u7528\u6a21\u578b\u96be\u4ee5\u6355\u6349\u52a8\u6001\u56fe\u4e2d\u7684\u5f02\u5e38\u6a21\u5f0f\uff0c\u4e14\u65b0\u9886\u57df\u6570\u636e\u7f3a\u4e4f\u6807\u7b7e\u3002", "method": "DP-DGAD\u901a\u8fc7\u63d0\u53d6\u52a8\u6001\u539f\u578b\uff08\u6b63\u5e38\u548c\u5f02\u5e38\u6a21\u5f0f\u7684\u6f14\u5316\u8868\u793a\uff09\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u5185\u5b58\u7f13\u51b2\u533a\uff0c\u5e76\u4f7f\u7528\u5f02\u5e38\u8bc4\u5206\u5668\u548c\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u8fdb\u884c\u81ea\u76d1\u7763\u9002\u5e94\u3002", "result": "\u5728\u5341\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DP-DGAD\u80fd\u591f\u6709\u6548\u6355\u6349\u52a8\u6001\u56fe\u4e2d\u7684\u5f02\u5e38\u6a21\u5f0f\uff0c\u9002\u7528\u4e8e\u8de8\u9886\u57df\u5e94\u7528\u3002"}}
{"id": "2508.00692", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00692", "abs": "https://arxiv.org/abs/2508.00692", "authors": ["Young-ho Cho", "Hao Zhu", "Duehee Lee", "Ross Baldick"], "title": "Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network", "comment": null, "summary": "For conducting resource adequacy studies, we synthesize multiple long-term\nwind power scenarios of distributed wind farms simultaneously by using the\nspatio-temporal features: spatial and temporal correlation, waveforms, marginal\nand ramp rates distributions of waveform, power spectral densities, and\nstatistical characteristics. Generating the spatial correlation in scenarios\nrequires the design of common factors for neighboring wind farms and\nantithetical factors for distant wind farms. The generalized dynamic factor\nmodel (GDFM) can extract the common factors through cross spectral density\nanalysis, but it cannot closely imitate waveforms. The GAN can synthesize\nplausible samples representing the temporal correlation by verifying samples\nthrough a fake sample discriminator. To combine the advantages of GDFM and GAN,\nwe use the GAN to provide a filter that extracts dynamic factors with temporal\ninformation from the observation data, and we then apply this filter in the\nGDFM to represent both spatial and frequency correlations of plausible\nwaveforms. Numerical tests on the combination of GDFM and GAN have demonstrated\nperformance improvements over competing alternatives in synthesizing wind power\nscenarios from Australia, better realizing plausible statistical\ncharacteristics of actual wind power compared to alternatives such as the GDFM\nwith a filter synthesized from distributions of actual dynamic filters and the\nGAN with direct synthesis without dynamic factors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5e7f\u4e49\u52a8\u6001\u56e0\u5b50\u6a21\u578b\uff08GDFM\uff09\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5408\u6210\u5206\u5e03\u5f0f\u98ce\u7535\u573a\u7684\u957f\u671f\u98ce\u7535\u529f\u7387\u573a\u666f\uff0c\u4ee5\u66f4\u597d\u5730\u6a21\u62df\u65f6\u7a7a\u7279\u5f81\u548c\u7edf\u8ba1\u7279\u6027\u3002", "motivation": "\u8d44\u6e90\u5145\u8db3\u6027\u7814\u7a76\u9700\u8981\u51c6\u786e\u6a21\u62df\u98ce\u7535\u573a\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982GDFM\u6216GAN\u5355\u72ec\u4f7f\u7528\uff09\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u7a7a\u95f4\u548c\u65f6\u95f4\u76f8\u5173\u6027\u9700\u6c42\u3002", "method": "\u7ed3\u5408GDFM\u548cGAN\uff0c\u5229\u7528GAN\u63d0\u53d6\u52a8\u6001\u56e0\u5b50\u5e76\u4f5c\u4e3aGDFM\u7684\u6ee4\u6ce2\u5668\uff0c\u4ee5\u540c\u65f6\u6355\u6349\u7a7a\u95f4\u548c\u9891\u7387\u76f8\u5173\u6027\u3002", "result": "\u5728\u6fb3\u5927\u5229\u4e9a\u98ce\u7535\u6570\u636e\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u98ce\u7535\u573a\u666f\u65f6\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528GDFM\u6216GAN\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u6a21\u62df\u5b9e\u9645\u98ce\u7535\u7684\u7edf\u8ba1\u7279\u6027\u3002", "conclusion": "GDFM\u4e0eGAN\u7684\u7ed3\u5408\u65b9\u6cd5\u5728\u5408\u6210\u98ce\u7535\u529f\u7387\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u8d44\u6e90\u5145\u8db3\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6a21\u62df\u5de5\u5177\u3002"}}
{"id": "2508.00695", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00695", "abs": "https://arxiv.org/abs/2508.00695", "authors": ["Sergio Rubio-Mart\u00edn", "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s", "Antonio Serrano-Garc\u00eda", "Clara Margarita Franch-Pato", "Arturo Crespo-\u00c1lvaro", "Jos\u00e9 Alberto Ben\u00edtez-Andrades"], "title": "Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach", "comment": null, "summary": "The classification of clinical notes into specific diagnostic categories is\ncritical in healthcare, especially for mental health conditions like Anxiety\nand Adjustment Disorder. In this study, we compare the performance of various\nArtificial Intelligence models, including both traditional Machine Learning\napproaches (Random Forest, Support Vector Machine, K-nearest neighbors,\nDecision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT\nand SciBERT), to classify clinical notes into these two diagnoses.\nAdditionally, we implemented three oversampling strategies: No Oversampling,\nRandom Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to\nassess their impact on model performance. Hyperparameter tuning was also\napplied to optimize model accuracy. Our results indicate that oversampling\ntechniques had minimal impact on model performance overall. The only exception\nwas SMOTE, which showed a positive effect specifically with BERT-based models.\nHowever, hyperparameter optimization significantly improved accuracy across the\nmodels, enhancing their ability to generalize and perform on the dataset. The\nDecision Tree and eXtreme Gradient Boost models achieved the highest accuracy\namong machine learning approaches, both reaching 96%, while the DistilBERT and\nSciBERT models also attained 96% accuracy in the deep learning category. These\nfindings underscore the importance of hyperparameter tuning in maximizing model\nperformance. This study contributes to the ongoing research on AI-assisted\ndiagnostic tools in mental health by providing insights into the efficacy of\ndifferent model architectures and data balancing methods.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cdAI\u6a21\u578b\u5bf9\u4e34\u5e8a\u7b14\u8bb0\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u53d1\u73b0\u8d85\u53c2\u6570\u8c03\u4f18\u663e\u8457\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\uff0c\u800c\u8fc7\u91c7\u6837\u6280\u672f\u5f71\u54cd\u6709\u9650\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u5206\u7c7b\u5bf9\u5fc3\u7406\u5065\u5eb7\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8bc4\u4f30\u4e0d\u540cAI\u6a21\u578b\u548c\u6570\u636e\u5e73\u8861\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff08\u5982\u968f\u673a\u68ee\u6797\u3001SVM\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982DistilBERT\u3001SciBERT\uff09\uff0c\u5e76\u6d4b\u8bd5\u4e09\u79cd\u8fc7\u91c7\u6837\u7b56\u7565\u53ca\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u8d85\u53c2\u6570\u8c03\u4f18\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u8fc7\u91c7\u6837\u6280\u672f\u4ec5\u5bf9BERT\u6a21\u578b\u6709\u79ef\u6781\u5f71\u54cd\u3002\u51b3\u7b56\u6811\u548cXGBoost\u51c6\u786e\u7387\u8fbe96%\uff0cBERT\u6a21\u578b\u4ea6\u7136\u3002", "conclusion": "\u8d85\u53c2\u6570\u8c03\u4f18\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4e3aAI\u8f85\u52a9\u5fc3\u7406\u5065\u5eb7\u8bca\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2508.00706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00706", "abs": "https://arxiv.org/abs/2508.00706", "authors": ["Haozhe Tian", "Pietro Ferraro", "Robert Shorten", "Mahdi Jalili", "Homayoun Hamedmoghadam"], "title": "Learning Network Dismantling without Handcrafted Inputs", "comment": null, "summary": "The application of message-passing Graph Neural Networks has been a\nbreakthrough for important network science problems. However, the competitive\nperformance often relies on using handcrafted structural features as inputs,\nwhich increases computational cost and introduces bias into the otherwise\npurely data-driven network representations. Here, we eliminate the need for\nhandcrafted features by introducing an attention mechanism and utilizing\nmessage-iteration profiles, in addition to an effective algorithmic approach to\ngenerate a structurally diverse training set of small synthetic networks.\nThereby, we build an expressive message-passing framework and use it to\nefficiently solve the NP-hard problem of Network Dismantling, virtually\nequivalent to vital node identification, with significant real-world\napplications. Trained solely on diversified synthetic networks, our proposed\nmodel -- MIND: Message Iteration Network Dismantler -- generalizes to large,\nunseen real networks with millions of nodes, outperforming state-of-the-art\nnetwork dismantling methods. Increased efficiency and generalizability of the\nproposed model can be leveraged beyond dismantling in a range of complex\nnetwork problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u624b\u5de5\u7279\u5f81\u7684\u6d88\u606f\u4f20\u9012\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6MIND\uff0c\u7528\u4e8e\u89e3\u51b3\u7f51\u7edc\u62c6\u89e3\u95ee\u9898\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u7f51\u7edc\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u5f15\u5165\u504f\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u548c\u6d88\u606f\u8fed\u4ee3\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u751f\u6210\u7ed3\u6784\u591a\u6837\u7684\u5c0f\u578b\u5408\u6210\u7f51\u7edc\u4f5c\u4e3a\u8bad\u7ec3\u96c6\u3002", "result": "MIND\u6a21\u578b\u5728\u5927\u578b\u771f\u5b9e\u7f51\u7edc\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MIND\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u7f51\u7edc\u62c6\u89e3\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u590d\u6742\u7f51\u7edc\u95ee\u9898\u3002"}}
{"id": "2508.00707", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00707", "abs": "https://arxiv.org/abs/2508.00707", "authors": ["Yannik Schnitzer", "Alessandro Abate", "David Parker"], "title": "Efficient Solution and Learning of Robust Factored MDPs", "comment": null, "summary": "Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling\nepistemic uncertainty about transition dynamics. Learning r-MDPs from\ninteractions with an unknown environment enables the synthesis of robust\npolicies with provable (PAC) guarantees on performance, but this can require a\nlarge number of sample interactions. We propose novel methods for solving and\nlearning r-MDPs based on factored state-space representations that leverage the\nindependence between model uncertainty across system components. Although\npolicy synthesis for factored r-MDPs leads to hard, non-convex optimisation\nproblems, we show how to reformulate these into tractable linear programs.\nBuilding on these, we also propose methods to learn factored model\nrepresentations directly. Our experimental results show that exploiting\nfactored structure can yield dimensional gains in sample efficiency, producing\nmore effective robust policies with tighter performance guarantees than\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u89e3\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u548c\u5b66\u4e60\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08r-MDPs\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "motivation": "r-MDPs\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8f6c\u79fb\u52a8\u6001\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u80fd\u591f\u5408\u6210\u5177\u6709\u6027\u80fd\u4fdd\u8bc1\u7684\u9c81\u68d2\u7b56\u7565\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6837\u672c\u4ea4\u4e92\u3002", "method": "\u5229\u7528\u7cfb\u7edf\u7ec4\u4ef6\u95f4\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u72ec\u7acb\u6027\uff0c\u5c06\u975e\u51f8\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u7684\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u76f4\u63a5\u5b66\u4e60\u5206\u89e3\u6a21\u578b\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5229\u7528\u5206\u89e3\u7ed3\u6784\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u751f\u6210\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\u7684\u9c81\u68d2\u7b56\u7565\u548c\u66f4\u4e25\u683c\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "\u5206\u89e3\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u662f\u89e3\u51b3\u548c\u5b66\u4e60r-MDPs\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u6027\u80fd\u3002"}}
{"id": "2508.00712", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00712", "abs": "https://arxiv.org/abs/2508.00712", "authors": ["Dien Nguyen", "Diego Perez-Liebana", "Simon Lucas"], "title": "JSON-Bag: A generic game trajectory representation", "comment": "8 pages, 3 figures, 6 tables, to be published in IEEE Conference on\n  Games 2025", "summary": "We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically\nrepresent game trajectories by tokenizing their JSON descriptions and apply\nJensen-Shannon distance (JSD) as distance metric for them. Using a\nprototype-based nearest-neighbor search (P-NNS), we evaluate the validity of\nJSON-Bag with JSD on six tabletop games -- \\textit{7 Wonders},\n\\textit{Dominion}, \\textit{Sea Salt and Paper}, \\textit{Can't Stop},\n\\textit{Connect4}, \\textit{Dots and boxes} -- each over three game trajectory\nclassification tasks: classifying the playing agents, game parameters, or game\nseeds that were used to generate the trajectories.\n  Our approach outperforms a baseline using hand-crafted features in the\nmajority of tasks. Evaluating on N-shot classification suggests using JSON-Bag\nprototype to represent game trajectory classes is also sample efficient.\nAdditionally, we demonstrate JSON-Bag ability for automatic feature extraction\nby treating tokens as individual features to be used in Random Forest to solve\nthe tasks above, which significantly improves accuracy on underperforming\ntasks. Finally, we show that, across all six games, the JSD between JSON-Bag\nprototypes of agent classes highly correlates with the distances between\nagents' policies.", "AI": {"tldr": "JSON-Bag\u6a21\u578b\u901a\u8fc7JSON\u63cf\u8ff0\u7684\u6e38\u620f\u8f68\u8ff9\u8fdb\u884c\u6807\u8bb0\u5316\uff0c\u5e76\u4f7f\u7528Jensen-Shannon\u8ddd\u79bb\uff08JSD\uff09\u4f5c\u4e3a\u8ddd\u79bb\u5ea6\u91cf\u3002\u5728\u516d\u4e2a\u684c\u6e38\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u57fa\u4e8e\u624b\u5de5\u7279\u5f81\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u6837\u672c\u6548\u7387\u548c\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u6e38\u620f\u8f68\u8ff9\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u624b\u5de5\u7279\u5f81\u63d0\u53d6\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528JSON-Bag\u6a21\u578b\u6807\u8bb0\u5316\u6e38\u620f\u8f68\u8ff9\u7684JSON\u63cf\u8ff0\uff0c\u5e94\u7528JSD\u4f5c\u4e3a\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u6700\u8fd1\u90bb\u641c\u7d22\uff08P-NNS\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u6570\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6837\u672c\u6548\u7387\u9ad8\uff0c\u4e14\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u6027\u80fd\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "JSON-Bag\u6a21\u578b\u662f\u4e00\u79cd\u6709\u6548\u7684\u901a\u7528\u6e38\u620f\u8f68\u8ff9\u8868\u793a\u65b9\u6cd5\uff0c\u4e14JSD\u4e0e\u7b56\u7565\u8ddd\u79bb\u9ad8\u5ea6\u76f8\u5173\u3002"}}
{"id": "2508.00716", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00716", "abs": "https://arxiv.org/abs/2508.00716", "authors": ["Yingxu Wang", "Mengzhu Wang", "Zhichao Huang", "Suyu Liu"], "title": "Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning", "comment": null, "summary": "Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled\nsource graphs to unlabeled target graphs by learning domain-invariant\nrepresentations, which is essential in applications such as molecular property\nprediction and social network analysis. However, most existing GDA methods rely\non the assumption of clean source labels, which rarely holds in real-world\nscenarios where annotation noise is pervasive. This label noise severely\nimpairs feature alignment and degrades adaptation performance under domain\nshifts. To address this challenge, we propose Nested Graph Pseudo-Label\nRefinement (NeGPR), a novel framework tailored for graph-level domain\nadaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,\nsemantic and topology branches, by enforcing neighborhood consistency in the\nfeature space, thereby reducing the influence of noisy supervision. To bridge\ndomain gaps, NeGPR employs a nested refinement mechanism in which one branch\nselects high-confidence target samples to guide the adaptation of the other,\nenabling progressive cross-domain learning. Furthermore, since pseudo-labels\nmay still contain noise and the pre-trained branches are already overfitted to\nthe noisy labels in the source domain, NeGPR incorporates a noise-aware\nregularization strategy. This regularization is theoretically proven to\nmitigate the adverse effects of pseudo-label noise, even under the presence of\nsource overfitting, thus enhancing the robustness of the adaptation process.\nExtensive experiments on benchmark datasets demonstrate that NeGPR consistently\noutperforms state-of-the-art methods under severe label noise, achieving gains\nof up to 12.7% in accuracy.", "AI": {"tldr": "NeGPR\u662f\u4e00\u79cd\u9488\u5bf9\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u56fe\u7ea7\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u9884\u8bad\u7ec3\u548c\u5d4c\u5957\u4f2a\u6807\u7b7e\u7ec6\u5316\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u8de8\u57df\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u6e90\u6807\u7b7e\u5e38\u542b\u566a\u58f0\uff0c\u73b0\u6709\u56fe\u57df\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u5e72\u51c0\u6807\u7b7e\u5047\u8bbe\uff0c\u6027\u80fd\u53d7\u9650\u3002", "method": "NeGPR\u91c7\u7528\u53cc\u5206\u652f\u9884\u8bad\u7ec3\uff08\u8bed\u4e49\u548c\u62d3\u6251\u5206\u652f\uff09\u548c\u5d4c\u5957\u7ec6\u5316\u673a\u5236\uff0c\u7ed3\u5408\u566a\u58f0\u611f\u77e5\u6b63\u5219\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cNeGPR\u5728\u4e25\u91cd\u6807\u7b7e\u566a\u58f0\u4e0b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u5347\u8fbe12.7%\u3002", "conclusion": "NeGPR\u901a\u8fc7\u566a\u58f0\u9c81\u68d2\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u56fe\u57df\u9002\u5e94\u6027\u80fd\u3002"}}
{"id": "2508.00718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00718", "abs": "https://arxiv.org/abs/2508.00718", "authors": ["Ivona Krchova", "Mariana Vargas Vieyra", "Mario Scriminaci", "Andrey Sidorenko"], "title": "Democratizing Tabular Data Access with an Open$\\unicode{x2013}$Source Synthetic$\\unicode{x2013}$Data SDK", "comment": null, "summary": "Machine learning development critically depends on access to high-quality\ndata. However, increasing restrictions due to privacy, proprietary interests,\nand ethical concerns have created significant barriers to data accessibility.\nSynthetic data offers a viable solution by enabling safe, broad data usage\nwithout compromising sensitive information. This paper presents the MOSTLY AI\nSynthetic Data Software Development Kit (SDK), an open-source toolkit designed\nspecifically for synthesizing high-quality tabular data. The SDK integrates\nrobust features such as differential privacy guarantees, fairness-aware data\ngeneration, and automated quality assurance into a flexible and accessible\nPython interface. Leveraging the TabularARGN autoregressive framework, the SDK\nsupports diverse data types and complex multi-table and sequential datasets,\ndelivering competitive performance with notable improvements in speed and\nusability. Currently deployed both as a cloud service and locally installable\nsoftware, the SDK has seen rapid adoption, highlighting its practicality in\naddressing real-world data bottlenecks and promoting widespread data\ndemocratization.", "AI": {"tldr": "MOSTLY AI SDK\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u8868\u683c\u6570\u636e\uff0c\u89e3\u51b3\u6570\u636e\u8bbf\u95ee\u53d7\u9650\u95ee\u9898\u3002", "motivation": "\u9690\u79c1\u3001\u4e13\u6709\u5229\u76ca\u548c\u4f26\u7406\u95ee\u9898\u5bfc\u81f4\u6570\u636e\u8bbf\u95ee\u53d7\u9650\uff0c\u5408\u6210\u6570\u636e\u6210\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eTabularARGN\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u96c6\u6210\u5dee\u5206\u9690\u79c1\u3001\u516c\u5e73\u6027\u751f\u6210\u548c\u81ea\u52a8\u5316\u8d28\u91cf\u4fdd\u8bc1\u3002", "result": "SDK\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\u548c\u590d\u6742\u6570\u636e\u96c6\uff0c\u6027\u80fd\u4f18\u8d8a\uff0c\u901f\u5ea6\u5feb\u4e14\u6613\u7528\u3002", "conclusion": "SDK\u5df2\u5e7f\u6cdb\u90e8\u7f72\uff0c\u6709\u6548\u89e3\u51b3\u6570\u636e\u74f6\u9888\uff0c\u63a8\u52a8\u6570\u636e\u6c11\u4e3b\u5316\u3002"}}
{"id": "2508.00734", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00734", "abs": "https://arxiv.org/abs/2508.00734", "authors": ["Liuyun Xu", "Seymour M. J. Spence"], "title": "Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems", "comment": null, "summary": "Existing variance reduction techniques used in stochastic simulations for\nrare event analysis still require a substantial number of model evaluations to\nestimate small failure probabilities. In the context of complex, nonlinear\nfinite element modeling environments, this can become computationally\nchallenging-particularly for systems subjected to stochastic excitation. To\naddress this challenge, a multi-fidelity stratified sampling scheme with\nadaptive machine learning metamodels is introduced for efficiently propagating\nuncertainties and estimating small failure probabilities. In this approach, a\nhigh-fidelity dataset generated through stratified sampling is used to train a\ndeep learning-based metamodel, which then serves as a cost-effective and highly\ncorrelated low-fidelity model. An adaptive training scheme is proposed to\nbalance the trade-off between approximation quality and computational demand\nassociated with the development of the low-fidelity model. By integrating the\nlow-fidelity outputs with additional high-fidelity results, an unbiased\nestimate of the strata-wise failure probabilities is obtained using a\nmulti-fidelity Monte Carlo framework. The overall probability of failure is\nthen computed using the total probability theorem. Application to a full-scale\nhigh-rise steel building subjected to stochastic wind excitation demonstrates\nthat the proposed scheme can accurately estimate exceedance probability curves\nfor nonlinear responses of interest, while achieving significant computational\nsavings compared to single-fidelity variance reduction approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4fdd\u771f\u5ea6\u5206\u5c42\u91c7\u6837\u548c\u81ea\u9002\u5e94\u673a\u5668\u5b66\u4e60\u5143\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u4f30\u8ba1\u5c0f\u5931\u6548\u6982\u7387\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u975e\u7ebf\u6027\u6709\u9650\u5143\u6a21\u578b\u4e2d\u8ba1\u7b97\u5c0f\u5931\u6548\u6982\u7387\u65f6\u4ecd\u9700\u8981\u5927\u91cf\u6a21\u578b\u8bc4\u4f30\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u7ed3\u5408\u5206\u5c42\u91c7\u6837\u751f\u6210\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u5143\u6a21\u578b\u4f5c\u4e3a\u4f4e\u4fdd\u771f\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u8bad\u7ec3\u5e73\u8861\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u5e94\u7528\u4e8e\u9ad8\u5c42\u94a2\u7ed3\u6784\u5efa\u7b51\u7684\u98ce\u6fc0\u52b1\u5206\u6790\uff0c\u51c6\u786e\u4f30\u8ba1\u975e\u7ebf\u6027\u54cd\u5e94\u8d85\u8d8a\u6982\u7387\u66f2\u7ebf\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7cfb\u7edf\u7684\u5931\u6548\u6982\u7387\u5206\u6790\u3002"}}
{"id": "2508.00754", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00754", "abs": "https://arxiv.org/abs/2508.00754", "authors": ["Yaxin Ma", "Benjamin Colburn", "Jose C. Principe"], "title": "A Simple and Effective Method for Uncertainty Quantification and OOD Detection", "comment": null, "summary": "Bayesian neural networks and deep ensemble methods have been proposed for\nuncertainty quantification; however, they are computationally intensive and\nrequire large storage. By utilizing a single deterministic model, we can solve\nthe above issue. We propose an effective method based on feature space density\nto quantify uncertainty for distributional shifts and out-of-distribution (OOD)\ndetection. Specifically, we leverage the information potential field derived\nfrom kernel density estimation to approximate the feature space density of the\ntraining set. By comparing this density with the feature space representation\nof test samples, we can effectively determine whether a distributional shift\nhas occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons\nand Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The\nresults demonstrate that our method outperforms baseline models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7a7a\u95f4\u5bc6\u5ea6\u7684\u5355\u786e\u5b9a\u6027\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u5206\u5e03\u504f\u79fb\u548cOOD\u68c0\u6d4b\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u96c6\u6210\u65b9\u6cd5\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u9ad8\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u4fe1\u606f\u52bf\u573a\u8fd1\u4f3c\u8bad\u7ec3\u96c6\u7279\u5f81\u7a7a\u95f4\u5bc6\u5ea6\uff0c\u901a\u8fc7\u6bd4\u8f83\u6d4b\u8bd5\u6837\u672c\u7279\u5f81\u7a7a\u95f4\u8868\u793a\u68c0\u6d4b\u5206\u5e03\u504f\u79fb\u3002", "result": "\u57282D\u5408\u6210\u6570\u636e\u96c6\u548cOOD\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5355\u786e\u5b9a\u6027\u6a21\u578b\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548cOOD\u68c0\u6d4b\u4e2d\u9ad8\u6548\u4e14\u6709\u6548\u3002"}}
{"id": "2508.00758", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00758", "abs": "https://arxiv.org/abs/2508.00758", "authors": ["Timur Sattarov", "Marco Schreyer", "Damian Borth"], "title": "Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data", "comment": "22 pages, 16 figures, 7 tables, preprint version", "summary": "Anomaly detection in tabular data remains challenging due to complex feature\ninteractions and the scarcity of anomalous examples. Denoising autoencoders\nrely on fixed-magnitude noise, limiting adaptability to diverse data\ndistributions. Diffusion models introduce scheduled noise and iterative\ndenoising, but lack explicit reconstruction mappings. We propose the\nDiffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates\ndiffusion-based noise scheduling and contrastive learning into the encoding\nprocess to improve anomaly detection. We evaluated DDAE on 57 datasets from\nADBench. Our method outperforms in semi-supervised settings and achieves\ncompetitive results in unsupervised settings, improving PR-AUC by up to 65%\n(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)\nmodel baselines. We observed that higher noise levels benefit unsupervised\ntraining, while lower noise with linear scheduling is optimal in\nsemi-supervised settings. These findings underscore the importance of\nprincipled noise strategies in tabular anomaly detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u53bb\u566a\u81ea\u7f16\u7801\u5668\u7684\u65b0\u6846\u67b6DDAE\uff0c\u7528\u4e8e\u8868\u683c\u6570\u636e\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u8868\u683c\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u56e0\u590d\u6742\u7684\u7279\u5f81\u4ea4\u4e92\u548c\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u53bb\u566a\u81ea\u7f16\u7801\u5668\u548c\u6269\u6563\u6a21\u578b\uff09\u5404\u6709\u5c40\u9650\u6027\u3002", "method": "DDAE\u6846\u67b6\u6574\u5408\u4e86\u6269\u6563\u6a21\u578b\u7684\u566a\u58f0\u8c03\u5ea6\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6539\u8fdb\u4e86\u7f16\u7801\u8fc7\u7a0b\u3002", "result": "\u5728ADBench\u768457\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cDDAE\u5728\u534a\u76d1\u7763\u548c\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cPR-AUC\u548cROC-AUC\u5206\u522b\u63d0\u5347\u6700\u9ad865%\u548c16%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u566a\u58f0\u7b56\u7565\u5728\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86DDAE\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00768", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00768", "abs": "https://arxiv.org/abs/2508.00768", "authors": ["Antonio Tudisco", "Andrea Marchesin", "Maurizio Zamboni", "Mariagrazia Graziano", "Giovanna Turvani"], "title": "Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy", "comment": null, "summary": "Recent advancements in Quantum Computing and Machine Learning have increased\nattention to Quantum Machine Learning (QML), which aims to develop machine\nlearning models by exploiting the quantum computing paradigm. One of the widely\nused models in this area is the Variational Quantum Circuit (VQC), a hybrid\nmodel where the quantum circuit handles data inference while classical\noptimization adjusts the parameters of the circuit. The quantum circuit\nconsists of an encoding layer, which loads data into the circuit, and a\ntemplate circuit, known as the ansatz, responsible for processing the data.\nThis work involves performing an analysis by considering both Amplitude- and\nAngle-encoding models, and examining how the type of rotational gate applied\naffects the classification performance of the model. This comparison is carried\nout by training the different models on two datasets, Wine and Diabetes, and\nevaluating their performance. The study demonstrates that, under identical\nmodel topologies, the difference in accuracy between the best and worst models\nranges from 10% to 30%, with differences reaching up to 41%. Moreover, the\nresults highlight how the choice of rotational gates used in encoding can\nsignificantly impact the model's classification performance. The findings\nconfirm that the embedding represents a hyperparameter for VQC models.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u4e86\u632f\u5e45\u548c\u89d2\u5ea6\u7f16\u7801\u6a21\u578b\u5728\u4e0d\u540c\u65cb\u8f6c\u95e8\u4e0b\u7684\u5206\u7c7b\u8868\u73b0\uff0c\u53d1\u73b0\u7f16\u7801\u65b9\u5f0f\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u7684\u7ed3\u5408\uff08QML\uff09\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u5c24\u5176\u662f\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u7684\u6027\u80fd\u4f18\u5316\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7528\u632f\u5e45\u548c\u89d2\u5ea6\u7f16\u7801\u6a21\u578b\uff0c\u901a\u8fc7\u4e0d\u540c\u65cb\u8f6c\u95e8\u8bad\u7ec3Wine\u548cDiabetes\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u5728\u76f8\u540c\u62d3\u6251\u7ed3\u6784\u4e0b\uff0c\u6700\u4f73\u548c\u6700\u5dee\u6a21\u578b\u7684\u51c6\u786e\u7387\u5dee\u5f02\u4e3a10%\u81f330%\uff0c\u6700\u9ad8\u8fbe41%\u3002\u65cb\u8f6c\u95e8\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u7f16\u7801\u65b9\u5f0f\u662fVQC\u6a21\u578b\u7684\u91cd\u8981\u8d85\u53c2\u6570\uff0c\u9009\u62e9\u5408\u9002\u65cb\u8f6c\u95e8\u80fd\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.00785", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00785", "abs": "https://arxiv.org/abs/2508.00785", "authors": ["Bushra Akter", "Md Biplob Hosen", "Sabbir Ahmed", "Mehrin Anannya", "Md. Farhad Hossain"], "title": "Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors", "comment": null, "summary": "Academic performance depends on a multivariable nexus of socio-academic and\nfinancial factors. This study investigates these influences to develop\neffective strategies for optimizing students' CGPA. To achieve this, we\nreviewed various literature to identify key influencing factors and constructed\nan initial hypothetical causal graph based on the findings. Additionally, an\nonline survey was conducted, where 1,050 students participated, providing\ncomprehensive data for analysis. Rigorous data preprocessing techniques,\nincluding cleaning and visualization, ensured data quality before analysis.\nCausal analysis validated the relationships among variables, offering deeper\ninsights into their direct and indirect effects on CGPA. Regression models were\nimplemented for CGPA prediction, while classification models categorized\nstudents based on performance levels. Ridge Regression demonstrated strong\npredictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared\nError of 0.023. Random Forest outperformed in classification, attaining an\nF1-score near perfection and an accuracy of 98.68%. Explainable AI techniques\nsuch as SHAP, LIME, and Interpret enhanced model interpretability, highlighting\ncritical factors such as study hours, scholarships, parental education, and\nprior academic performance. The study culminated in the development of a\nweb-based application that provides students with personalized insights,\nallowing them to predict academic performance, identify areas for improvement,\nand make informed decisions to enhance their outcomes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u8c03\u67e5\u6570\u636e\u5206\u6790\u4e86\u5f71\u54cd\u5b66\u751fCGPA\u7684\u591a\u53d8\u91cf\u56e0\u7d20\uff0c\u4f7f\u7528\u56e0\u679c\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u5cad\u56de\u5f52\u548c\u968f\u673a\u68ee\u6797\uff09\u8fdb\u884c\u9884\u6d4b\u548c\u5206\u7c7b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u4e2a\u6027\u5316\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5f71\u54cd\u5b66\u751f\u5b66\u672f\u8868\u73b0\u7684\u591a\u53d8\u91cf\u56e0\u7d20\uff0c\u4ee5\u5236\u5b9a\u4f18\u5316CGPA\u7684\u6709\u6548\u7b56\u7565\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6587\u732e\u7efc\u8ff0\u3001\u5728\u7ebf\u8c03\u67e5\uff081050\u540d\u5b66\u751f\uff09\u3001\u6570\u636e\u9884\u5904\u7406\u3001\u56e0\u679c\u5206\u6790\u3001\u56de\u5f52\u548c\u5206\u7c7b\u6a21\u578b\uff08\u5982\u5cad\u56de\u5f52\u548c\u968f\u673a\u68ee\u6797\uff09\uff0c\u4ee5\u53ca\u53ef\u89e3\u91caAI\u6280\u672f\uff08\u5982SHAP\u548cLIME\uff09\u3002", "result": "\u5cad\u56de\u5f52\u5728\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff08MAE=0.12\uff0cMSE=0.023\uff09\uff0c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u51c6\u786e\u7387\u8fbe98.68%\u3002\u5173\u952e\u56e0\u7d20\u5305\u62ec\u5b66\u4e60\u65f6\u95f4\u3001\u5956\u5b66\u91d1\u3001\u7236\u6bcd\u6559\u80b2\u6c34\u5e73\u548c\u5148\u524d\u5b66\u672f\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u4e2a\u6027\u5316\u5e94\u7528\uff0c\u5e2e\u52a9\u5b66\u751f\u9884\u6d4b\u5b66\u672f\u8868\u73b0\u5e76\u4f18\u5316\u51b3\u7b56\uff0c\u4e3a\u63d0\u5347CGPA\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.00806", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00806", "abs": "https://arxiv.org/abs/2508.00806", "authors": ["Ping Chen", "Zhuohong Deng", "Ping Li", "Shuibing He", "Hongzi Zhu", "Yi Zheng", "Zhefeng Wang", "Baoxing Huai", "Minyi Guo"], "title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management", "comment": "8 pages", "summary": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.", "AI": {"tldr": "Adacc\u662f\u4e00\u4e2a\u7ed3\u5408\u81ea\u9002\u5e94\u538b\u7f29\u548c\u6fc0\u6d3b\u68c0\u67e5\u70b9\u7684\u5185\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11GPU\u5185\u5b58\u5360\u7528\u5e76\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u91cd\u8ba1\u7b97\u4f1a\u5f15\u5165\u9ad8\u8fbe30%\u7684\u5f00\u9500\uff0cAdacc\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u7ba1\u7406\u6765\u51cf\u5c11\u8fd9\u79cd\u5f00\u9500\u3002", "method": "Adacc\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u5c42\u7279\u5b9a\u538b\u7f29\u7b97\u6cd5\u3001\u57fa\u4e8eMILP\u7684\u6700\u4f18\u8c03\u5ea6\u7b56\u7565\u548c\u81ea\u9002\u5e94\u7b56\u7565\u6f14\u5316\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAdacc\u80fd\u5c06LLM\u8bad\u7ec3\u901f\u5ea6\u63d0\u53471.01x\u81f31.37x\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "Adacc\u901a\u8fc7\u81ea\u9002\u5e94\u538b\u7f29\u548c\u52a8\u6001\u7b56\u7565\u8c03\u6574\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
