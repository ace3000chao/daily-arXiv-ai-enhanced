{"id": "2507.11582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11582", "abs": "https://arxiv.org/abs/2507.11582", "authors": ["Kazuyoshi Otsuka"], "title": "Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance", "comment": "38 pages. Manuscript submitted for review to the Journal of\n  Computational Literary Studies (JCLS)", "summary": "This study positions large language models (LLMs) as \"subjective literary\ncritics\" to explore aesthetic preferences and evaluation patterns in literary\nassessment. Ten Japanese science fiction short stories were translated into\nEnglish and evaluated by six state-of-the-art LLMs across seven independent\nsessions. Principal component analysis and clustering techniques revealed\nsignificant variations in evaluation consistency ({\\alpha} ranging from 1.00 to\n0.35) and five distinct evaluation patterns. Additionally, evaluation variance\nacross stories differed by up to 4.5-fold, with TF-IDF analysis confirming\ndistinctive evaluation vocabularies for each model. Our seven-session\nwithin-day protocol using an original Science Fiction corpus strategically\nminimizes external biases, allowing us to observe implicit value systems shaped\nby RLHF and their influence on literary judgment. These findings suggest that\nLLMs may possess individual evaluation characteristics similar to human\ncritical schools, rather than functioning as neutral benchmarkers.", "AI": {"tldr": "\u7814\u7a76\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u201c\u4e3b\u89c2\u6587\u5b66\u8bc4\u8bba\u5bb6\u201d\uff0c\u63a2\u7d22\u5176\u5728\u6587\u5b66\u8bc4\u4f30\u4e2d\u7684\u5ba1\u7f8e\u504f\u597d\u548c\u8bc4\u4ef7\u6a21\u5f0f\u3002\u901a\u8fc7\u5206\u6790\u5341\u7bc7\u65e5\u672c\u79d1\u5e7b\u77ed\u7bc7\u5c0f\u8bf4\u7684\u7ffb\u8bd1\u6587\u672c\uff0c\u53d1\u73b0LLMs\u5728\u8bc4\u4ef7\u4e00\u81f4\u6027\u548c\u8bcd\u6c47\u4f7f\u7528\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u6587\u5b66\u8bc4\u4ef7\u4e2d\u662f\u5426\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u8bc4\u8bba\u5bb6\u7684\u4e2a\u4f53\u5316\u7279\u5f81\uff0c\u800c\u975e\u4e2d\u7acb\u57fa\u51c6\u3002", "method": "\u5c06\u5341\u7bc7\u65e5\u672c\u79d1\u5e7b\u5c0f\u8bf4\u7ffb\u8bd1\u6210\u82f1\u6587\uff0c\u7531\u516d\u79cd\u5148\u8fdbLLMs\u8fdb\u884c\u4e03\u6b21\u72ec\u7acb\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790\u548c\u805a\u7c7b\u6280\u672f\u5206\u6790\u6570\u636e\u3002", "result": "\u53d1\u73b0\u8bc4\u4ef7\u4e00\u81f4\u6027\u5dee\u5f02\u663e\u8457\uff08\u03b1\u8303\u56f41.00\u81f30.35\uff09\uff0c\u8bc6\u522b\u51fa\u4e94\u79cd\u8bc4\u4ef7\u6a21\u5f0f\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u4f7f\u7528\u72ec\u7279\u7684\u8bc4\u4ef7\u8bcd\u6c47\u3002", "conclusion": "LLMs\u53ef\u80fd\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u8bc4\u8bba\u5bb6\u7684\u4e2a\u4f53\u5316\u8bc4\u4ef7\u7279\u5f81\uff0c\u800c\u975e\u5355\u7eaf\u7684\u4e2d\u7acb\u5de5\u5177\u3002"}}
{"id": "2507.11625", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11625", "abs": "https://arxiv.org/abs/2507.11625", "authors": ["Varun Srivastava", "Fan Lei", "Srija Mukhopadhyay", "Vivek Gupta", "Ross Maciejewski"], "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering", "comment": "Published as a conference paper at COLM 2025", "summary": "Recent advancements in multimodal large language models (MLLMs) have driven\nresearchers to explore how well these models read data visualizations, e.g.,\nbar charts, scatter plots. More recently, attention has shifted to visual\nquestion answering with maps (Map-VQA). However, Map-VQA research has primarily\nfocused on choropleth maps, which cover only a limited range of thematic\ncategories and visual analytical tasks. To address these gaps, we introduce\nMapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three\nmap types: choropleth maps, cartograms, and proportional symbol maps spanning\ntopics from six distinct themes (e.g., housing, crime). We evaluate multiple\nMLLMs using six visual analytical tasks, comparing their performance against\none another and a human baseline. An additional experiment examining the impact\nof map design changes (e.g., altered color schemes, modified legend designs,\nand removal of map elements) provides insights into the robustness and\nsensitivity of MLLMs, their reliance on internal geographic knowledge, and\npotential avenues for improving Map-VQA performance.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MapIQ\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5730\u56fe\u89c6\u89c9\u95ee\u7b54\uff08Map-VQA\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u591a\u79cd\u5730\u56fe\u7c7b\u578b\u548c\u4e3b\u9898\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u5bf9\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\u7684\u654f\u611f\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709Map-VQA\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u7b49\u503c\u533a\u57df\u56fe\uff0c\u8986\u76d6\u7684\u4e3b\u9898\u548c\u4efb\u52a1\u6709\u9650\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efaMapIQ\u6570\u636e\u96c6\uff0814,706\u4e2a\u95ee\u7b54\u5bf9\uff09\uff0c\u6db5\u76d6\u4e09\u79cd\u5730\u56fe\u7c7b\u578b\u548c\u516d\u4e2a\u4e3b\u9898\uff0c\u8bc4\u4f30\u591a\u4e2aMLLMs\u5728\u516d\u9879\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6d4b\u8bd5\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u3002", "result": "MLLMs\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u57fa\u7ebf\u76f8\u6bd4\u5b58\u5728\u5dee\u8ddd\uff0c\u6a21\u578b\u5bf9\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\u654f\u611f\uff0c\u4f9d\u8d56\u5185\u90e8\u5730\u7406\u77e5\u8bc6\u3002", "conclusion": "MapIQ\u4e3aMap-VQA\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u63ed\u793a\u4e86MLLMs\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2507.11634", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11634", "abs": "https://arxiv.org/abs/2507.11634", "authors": ["Farideh Majidi", "Ziaeddin Beheshtifard"], "title": "Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation", "comment": "Proceedings of the First National Conference on Artificial\n  Intelligence and Emerging Research: Convergence of Humans and Intelligent\n  Systems", "summary": "This research examines cross-lingual sentiment analysis using few-shot\nlearning and incremental learning methods in Persian. The main objective is to\ndevelop a model capable of performing sentiment analysis in Persian using\nlimited data, while getting prior knowledge from high-resource languages. To\nachieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and\nDistilBERT) were employed, which were fine-tuned using few-shot and incremental\nlearning approaches on small samples of Persian data from diverse sources,\nincluding X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled\nthe models to learn from a broad range of contexts. Experimental results show\nthat the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%\naccuracy on Persian sentiment analysis. These findings highlight the\neffectiveness of combining few-shot learning and incremental learning with\nmultilingual pre-trained models.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6ce2\u65af\u8bed\u4e2d\u4f7f\u7528\u5c11\u6837\u672c\u5b66\u4e60\u548c\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8de8\u8bed\u8a00\u60c5\u611f\u5206\u6790\uff0c\u76ee\u6807\u662f\u5229\u7528\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u77e5\u8bc6\u5f00\u53d1\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u4e2d\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528XLM-RoBERTa\u3001mDeBERTa\u548cDistilBERT\u4e09\u79cd\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u548c\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u6ce2\u65af\u8bed\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "mDeBERTa\u548cXLM-RoBERTa\u5728\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u4e2d\u8fbe\u523096%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u3001\u589e\u91cf\u5b66\u4e60\u548c\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2507.11661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11661", "abs": "https://arxiv.org/abs/2507.11661", "authors": ["Guimin Hu", "Yi Xin", "Lijie Hu", "Zhihong Zhu", "Hasti Seifi"], "title": "Partitioner Guided Modal Learning Framework", "comment": "acm multimedia 2025", "summary": "Multimodal learning benefits from multiple modal information, and each\nlearned modal representations can be divided into uni-modal that can be learned\nfrom uni-modal training and paired-modal features that can be learned from\ncross-modal interaction. Building on this perspective, we propose a\npartitioner-guided modal learning framework, PgM, which consists of the modal\npartitioner, uni-modal learner, paired-modal learner, and uni-paired modal\ndecoder. Modal partitioner segments the learned modal representation into\nuni-modal and paired-modal features. Modal learner incorporates two dedicated\ncomponents for uni-modal and paired-modal learning. Uni-paired modal decoder\nreconstructs modal representation based on uni-modal and paired-modal features.\nPgM offers three key benefits: 1) thorough learning of uni-modal and\npaired-modal features, 2) flexible distribution adjustment for uni-modal and\npaired-modal representations to suit diverse downstream tasks, and 3) different\nlearning rates across modalities and partitions. Extensive experiments\ndemonstrate the effectiveness of PgM across four multimodal tasks and further\nhighlight its transferability to existing models. Additionally, we visualize\nthe distribution of uni-modal and paired-modal features across modalities and\ntasks, offering insights into their respective contributions.", "AI": {"tldr": "PgM\u6846\u67b6\u901a\u8fc7\u6a21\u6001\u5206\u5272\u5668\u5c06\u591a\u6a21\u6001\u8868\u793a\u5206\u4e3a\u5355\u6a21\u6001\u548c\u914d\u5bf9\u6a21\u6001\u7279\u5f81\uff0c\u7ed3\u5408\u4e13\u7528\u5b66\u4e60\u7ec4\u4ef6\u548c\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u7279\u5f81\u5b66\u4e60\u548c\u8c03\u6574\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\uff0c\u5355\u6a21\u6001\u548c\u914d\u5bf9\u6a21\u6001\u7279\u5f81\u7684\u533a\u5206\u4e0e\u5b66\u4e60\u5bf9\u4efb\u52a1\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faPgM\u6846\u67b6\uff0c\u5305\u542b\u6a21\u6001\u5206\u5272\u5668\u3001\u5355\u6a21\u6001\u5b66\u4e60\u5668\u3001\u914d\u5bf9\u6a21\u6001\u5b66\u4e60\u5668\u548c\u89e3\u7801\u5668\uff0c\u652f\u6301\u7279\u5f81\u5206\u5272\u4e0e\u7075\u6d3b\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePgM\u5728\u56db\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u5e76\u5177\u6709\u8fc1\u79fb\u6027\u3002\u7279\u5f81\u5206\u5e03\u53ef\u89c6\u5316\u63ed\u793a\u4e86\u5176\u8d21\u732e\u3002", "conclusion": "PgM\u901a\u8fc7\u660e\u786e\u533a\u5206\u548c\u5b66\u4e60\u5355\u6a21\u6001\u4e0e\u914d\u5bf9\u6a21\u6001\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4efb\u52a1\u7684\u6027\u80fd\u4e0e\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.11599", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11599", "abs": "https://arxiv.org/abs/2507.11599", "authors": ["Harish Vijayakumar"], "title": "Neuroaesthetics and the Science of Visual Experience", "comment": "7 pages", "summary": "Neuroaesthetics is an interdisciplinary field that brings together\nneuroscience, psychology, and the arts to explore how the human brain perceives\nand responds to visual beauty. This paper examines the neural mechanisms behind\naesthetic experiences, aiming to explain why certain designs or artworks feel\nemotionally or cognitively \"right.\" By analyzing the interaction between\nperception, emotion, and cognition, neuroaesthetics reveals how beauty is\nconstructed in the brain and how this understanding can inform fields such as\ngraphic and interface design. This paper offers a clear and accessible overview\nof core neuroaesthetic principles, making the subject approachable to a wide\naudience. The findings suggest that impactful design is more than surface-level\nappeal: well-crafted visual experiences can engage, support, and connect people\nin meaningful ways.", "AI": {"tldr": "\u795e\u7ecf\u7f8e\u5b66\u7814\u7a76\u5927\u8111\u5982\u4f55\u611f\u77e5\u7f8e\uff0c\u63ed\u793a\u8bbe\u8ba1\u5982\u4f55\u901a\u8fc7\u795e\u7ecf\u673a\u5236\u5f71\u54cd\u60c5\u611f\u4e0e\u8ba4\u77e5\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u7f8e\u611f\u7684\u795e\u7ecf\u673a\u5236\uff0c\u89e3\u91ca\u4e3a\u4f55\u67d0\u4e9b\u8bbe\u8ba1\u6216\u827a\u672f\u54c1\u80fd\u5f15\u53d1\u60c5\u611f\u5171\u9e23\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u3001\u5fc3\u7406\u5b66\u548c\u827a\u672f\uff0c\u5206\u6790\u611f\u77e5\u3001\u60c5\u611f\u4e0e\u8ba4\u77e5\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u8bbe\u8ba1\u4e0d\u4ec5\u662f\u8868\u9762\u5438\u5f15\u529b\uff0c\u8fd8\u80fd\u901a\u8fc7\u795e\u7ecf\u673a\u5236\u5b9e\u73b0\u6df1\u5c42\u6b21\u7684\u60c5\u611f\u8fde\u63a5\u3002", "conclusion": "\u795e\u7ecf\u7f8e\u5b66\u4e3a\u8bbe\u8ba1\u9886\u57df\u63d0\u4f9b\u4e86\u79d1\u5b66\u4f9d\u636e\uff0c\u5f3a\u8c03\u89c6\u89c9\u4f53\u9a8c\u7684\u6df1\u5c42\u4ef7\u503c\u3002"}}
{"id": "2507.11595", "categories": ["cs.AI", "cs.CY", "I.4.8; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.11595", "abs": "https://arxiv.org/abs/2507.11595", "authors": ["Hengyue Zhao"], "title": "A Study on the Application of Artificial Intelligence in Ecological Design", "comment": null, "summary": "This paper asks whether our relationship with nature can move from human\ndominance to genuine interdependence, and whether artificial intelligence (AI)\ncan mediate that shift. We examine a new ecological-design paradigm in which AI\ninteracts with non-human life forms. Through case studies we show how artists\nand designers apply AI for data analysis, image recognition, and ecological\nrestoration, producing results that differ from conventional media. We argue\nthat AI not only expands creative methods but also reframes the theory and\npractice of ecological design. Building on the author's prototype for\nAI-assisted water remediation, the study proposes design pathways that couple\nreinforcement learning with plant-based phytoremediation. The findings\nhighlight AI's potential to link scientific insight, artistic practice, and\nenvironmental stewardship, offering a roadmap for future research on\nsustainable, technology-enabled ecosystems.", "AI": {"tldr": "\u63a2\u8ba8AI\u5982\u4f55\u4fc3\u8fdb\u4eba\u4e0e\u81ea\u7136\u4ece\u652f\u914d\u5173\u7cfb\u8f6c\u5411\u76f8\u4e92\u4f9d\u5b58\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793aAI\u5728\u751f\u6001\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u4e0e\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76AI\u662f\u5426\u80fd\u4f5c\u4e3a\u5a92\u4ecb\uff0c\u63a8\u52a8\u4eba\u4e0e\u81ea\u7136\u5173\u7cfb\u7684\u8f6c\u53d8\uff0c\u5b9e\u73b0\u751f\u6001\u8bbe\u8ba1\u7684\u521b\u65b0\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5206\u6790AI\u5728\u6570\u636e\u3001\u56fe\u50cf\u8bc6\u522b\u548c\u751f\u6001\u4fee\u590d\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u7ed3\u5408\u539f\u578b\u8bbe\u8ba1\u63d0\u51faAI\u4e0e\u690d\u7269\u4fee\u590d\u7ed3\u5408\u7684\u65b9\u6cd5\u3002", "result": "AI\u4e0d\u4ec5\u6269\u5c55\u4e86\u521b\u610f\u65b9\u6cd5\uff0c\u8fd8\u91cd\u6784\u4e86\u751f\u6001\u8bbe\u8ba1\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u79d1\u5b66\u3001\u827a\u672f\u548c\u73af\u4fdd\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "AI\u4e3a\u53ef\u6301\u7eed\u6280\u672f\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7814\u7a76\u8def\u5f84\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u751f\u6001\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.11547", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11547", "abs": "https://arxiv.org/abs/2507.11547", "authors": ["Yingxue Zhao", "Qianyi Chen", "Haoran Li", "Haosu Zhou", "Hamid Reza Attar", "Tobias Pfaff", "Tailin Wu", "Nan Li"], "title": "Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming", "comment": null, "summary": "In recent years, various artificial intelligence-based surrogate models have\nbeen proposed to provide rapid manufacturability predictions of material\nforming processes. However, traditional AI-based surrogate models, typically\nbuilt with scalar or image-based neural networks, are limited in their ability\nto capture complex 3D spatial relationships and to operate in a\npermutation-invariant manner. To overcome these issues, emerging graph-based\nsurrogate models are developed using graph neural networks. This study\ndeveloped a new graph neural network surrogate model named Recurrent U\nNet-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate\npredictions of sheet material deformation fields across multiple forming\ntimesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model\ntemporal dynamics and a U-Net inspired graph-based downsample/upsample\nmechanism to handle spatial long-range dependencies. A novel 'node-to-surface'\ncontact representation method was proposed, offering significant improvements\nin computational efficiency for large-scale contact interactions. The RUGNN\nmodel was validated using a cold forming case study and a more complex hot\nforming case study using aluminium alloys. Results demonstrate that the RUGNN\nmodel provides accurate deformation predictions closely matching ground truth\nFE simulations and outperforming several baseline GNN architectures. Model\ntuning was also performed to identify suitable hyperparameters, training\nstrategies, and input feature representations. These results demonstrate that\nRUGNN is a reliable approach to support sheet material forming design by\nenabling accurate manufacturability predictions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRUGNN\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u6750\u6599\u6210\u5f62\u8fc7\u7a0b\u4e2d\u7684\u53d8\u5f62\u573a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfAI\u6a21\u578b\u5728\u6355\u63493D\u7a7a\u95f4\u5173\u7cfb\u548c\u7f6e\u6362\u4e0d\u53d8\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eAI\u7684\u66ff\u4ee3\u6a21\u578b\u5728\u6355\u6349\u590d\u67423D\u7a7a\u95f4\u5173\u7cfb\u548c\u7f6e\u6362\u4e0d\u53d8\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u6765\u652f\u6301\u6750\u6599\u6210\u5f62\u8bbe\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86RUGNN\u6a21\u578b\uff0c\u7ed3\u5408\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRUs\uff09\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u91c7\u7528U-Net\u542f\u53d1\u7684\u56fe\u4e0a\u4e0b\u91c7\u6837\u673a\u5236\u5904\u7406\u7a7a\u95f4\u957f\u7a0b\u4f9d\u8d56\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684'\u8282\u70b9\u5230\u8868\u9762'\u63a5\u89e6\u8868\u793a\u65b9\u6cd5\u3002", "result": "RUGNN\u5728\u51b7\u6210\u5f62\u548c\u70ed\u6210\u5f62\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0e\u6709\u9650\u5143\u6a21\u62df\u63a5\u8fd1\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebfGNN\u67b6\u6784\u3002", "conclusion": "RUGNN\u662f\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u51c6\u786e\u7684\u6210\u5f62\u6027\u9884\u6d4b\u652f\u6301\u6750\u6599\u6210\u5f62\u8bbe\u8ba1\u3002"}}
{"id": "2507.11694", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11694", "abs": "https://arxiv.org/abs/2507.11694", "authors": ["Maximiliano Hormaz\u00e1bal Lagos", "\u00c1lvaro Bueno S\u00e1ez", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro", "H\u00e9ctor Cerezo-Costas"], "title": "ExpliCIT-QA: Explainable Code-Based Image Table Question Answering", "comment": "This work has been accepted for presentation at the 24nd Portuguese\n  Conference on Artificial Intelligence (EPIA 2025) and will be published in\n  the proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "summary": "We present ExpliCIT-QA, a system that extends our previous MRT approach for\ntabular question answering into a multimodal pipeline capable of handling\ncomplex table images and providing explainable answers. ExpliCIT-QA follows a\nmodular design, consisting of: (1) Multimodal Table Understanding, which uses a\nChain-of-Thought approach to extract and transform content from table images;\n(2) Language-based Reasoning, where a step-by-step explanation in natural\nlanguage is generated to solve the problem; (3) Automatic Code Generation,\nwhere Python/Pandas scripts are created based on the reasoning steps, with\nfeedback for handling errors; (4) Code Execution to compute the final answer;\nand (5) Natural Language Explanation that describes how the answer was\ncomputed. The system is built for transparency and auditability: all\nintermediate outputs, parsed tables, reasoning steps, generated code, and final\nanswers are available for inspection. This strategy works towards closing the\nexplainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on\nthe TableVQA-Bench benchmark, comparing it with existing baselines. We\ndemonstrated improvements in interpretability and transparency, which open the\ndoor for applications in sensitive domains like finance and healthcare where\nauditing results are critical.", "AI": {"tldr": "ExpliCIT-QA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u8868\u683c\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7b54\u6848\uff0c\u5305\u62ec\u8868\u683c\u7406\u89e3\u3001\u8bed\u8a00\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u4e0e\u6267\u884c\uff0c\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u8868\u683c\u95ee\u7b54\u7cfb\u7edf\u4e2d\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u91d1\u878d\u548c\u533b\u7597\u7b49\u654f\u611f\u9886\u57df\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5305\u62ec\u591a\u6a21\u6001\u8868\u683c\u7406\u89e3\u3001\u8bed\u8a00\u63a8\u7406\u3001\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u3001\u4ee3\u7801\u6267\u884c\u548c\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u5728TableVQA-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "ExpliCIT-QA\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u586b\u8865\u4e86\u8868\u683c\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u7a7a\u767d\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5ba1\u8ba1\u7684\u654f\u611f\u9886\u57df\u3002"}}
{"id": "2507.11628", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11628", "abs": "https://arxiv.org/abs/2507.11628", "authors": ["Jiangnan Xu", "Haeseul Cha", "Gosu Choi", "Gyu-cheol Lee", "Yeo-Jin Yoon", "Zucheul Lee", "Konstantinos Papangelis", "Dae Hyun Kim", "Juho Kim"], "title": "DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday Storytelling", "comment": null, "summary": "An interactive vignette is a popular and immersive visual storytelling\napproach that invites viewers to role-play a character and influences the\nnarrative in an interactive environment. However, it has not been widely used\nby everyday storytellers yet due to authoring complexity, which conflicts with\nthe immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted\nauthoring system for interactive vignette creation in everyday storytelling. It\ntakes a natural language story as input and extracts the three core elements of\nan interactive vignette (environment, characters, and events), enabling authors\nto focus on refining these elements instead of constructing them from scratch.\nThen, it automatically transforms the single-branch story input into a\nbranch-and-bottleneck structure using an LLM-powered narrative planner, which\nenables flexible viewer interactions while freeing the author from\nmulti-branching. A technical evaluation (N=16) shows that DiaryPlay-generated\ncharacter activities are on par with human-authored ones regarding\nbelievability. A user study (N=16) shows that DiaryPlay effectively supports\nauthors in creating interactive vignette elements, maintains authorial intent\nwhile reacting to viewer interactions, and provides engaging viewing\nexperiences.", "AI": {"tldr": "DiaryPlay\u662f\u4e00\u4e2aAI\u8f85\u52a9\u7684\u4ea4\u4e92\u5f0f\u5c0f\u6545\u4e8b\u521b\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u7b80\u5316\u521b\u4f5c\u6d41\u7a0b\uff0c\u5e76\u5229\u7528LLM\u751f\u6210\u5206\u652f\u53d9\u4e8b\u7ed3\u6784\u3002", "motivation": "\u89e3\u51b3\u4ea4\u4e92\u5f0f\u5c0f\u6545\u4e8b\u521b\u4f5c\u590d\u6742\u5ea6\u9ad8\u3001\u4e0d\u9002\u5408\u65e5\u5e38\u8bb2\u6545\u4e8b\u7684\u95ee\u9898\u3002", "method": "\u8f93\u5165\u81ea\u7136\u8bed\u8a00\u6545\u4e8b\uff0c\u63d0\u53d6\u6838\u5fc3\u5143\u7d20\uff08\u73af\u5883\u3001\u89d2\u8272\u3001\u4e8b\u4ef6\uff09\uff0c\u5229\u7528LLM\u751f\u6210\u5206\u652f\u53d9\u4e8b\u7ed3\u6784\u3002", "result": "\u6280\u672f\u8bc4\u4f30\u663e\u793a\u751f\u6210\u7684\u89d2\u8272\u6d3b\u52a8\u53ef\u4fe1\u5ea6\u4e0e\u4eba\u5de5\u521b\u4f5c\u76f8\u5f53\uff1b\u7528\u6237\u7814\u7a76\u8868\u660e\u7cfb\u7edf\u80fd\u6709\u6548\u652f\u6301\u521b\u4f5c\u5e76\u4fdd\u6301\u4f5c\u8005\u610f\u56fe\u3002", "conclusion": "DiaryPlay\u4e3a\u65e5\u5e38\u8bb2\u6545\u4e8b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u4ea4\u4e92\u5f0f\u5c0f\u6545\u4e8b\u521b\u4f5c\u5de5\u5177\u3002"}}
{"id": "2507.11633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11633", "abs": "https://arxiv.org/abs/2507.11633", "authors": ["Yuxuan Zhang", "Haoyang Yu", "Lanxiang Hu", "Haojian Jin", "Hao Zhang"], "title": "General Modular Harness for LLM Agents in Multi-Turn Gaming Environments", "comment": "8 pages, ICML MAS workshop", "summary": "We introduce a modular harness design for LLM agents that composes of\nperception, memory, and reasoning components, enabling a single LLM or VLM\nbackbone to tackle a wide spectrum of multi turn gaming environments without\ndomain-specific engineering. Using classic and modern game suites as\nlow-barrier, high-diversity testbeds, our framework provides a unified workflow\nfor analyzing how each module affects performance across dynamic interactive\nsettings. Extensive experiments demonstrate that the harness lifts gameplay\nperformance consistently over un-harnessed baselines and reveals distinct\ncontribution patterns, for example, memory dominates in long-horizon puzzles\nwhile perception is critical in vision noisy arcades. These findings highlight\nthe effectiveness of our modular harness design in advancing general-purpose\nagent, given the familiarity and ubiquity of games in everyday human\nexperience.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u7ed3\u5408\u611f\u77e5\u3001\u8bb0\u5fc6\u548c\u63a8\u7406\u7ec4\u4ef6\uff0c\u4f7fLLM/VLM\u80fd\u9002\u5e94\u591a\u8f6e\u6e38\u620f\u73af\u5883\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u5de5\u7a0b\u3002", "motivation": "\u901a\u8fc7\u6e38\u620f\u73af\u5883\u6d4b\u8bd5\u6a21\u5757\u5316\u8bbe\u8ba1\u5bf9\u901a\u7528\u667a\u80fd\u4f53\u7684\u6027\u80fd\u63d0\u5347\u6548\u679c\u3002", "method": "\u4f7f\u7528\u7ecf\u5178\u548c\u73b0\u4ee3\u6e38\u620f\u5957\u4ef6\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5206\u6790\u5404\u6a21\u5757\u5728\u52a8\u6001\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u6e38\u620f\u8868\u73b0\uff0c\u4e0d\u540c\u6a21\u5757\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u8d21\u732e\u5404\u5f02\u3002", "conclusion": "\u6a21\u5757\u5316\u8bbe\u8ba1\u6709\u6548\u63a8\u8fdb\u901a\u7528\u667a\u80fd\u4f53\u53d1\u5c55\uff0c\u6e38\u620f\u73af\u5883\u9a8c\u8bc1\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2507.11570", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11570", "abs": "https://arxiv.org/abs/2507.11570", "authors": ["Ha Na Cho", "Sairam Sutari", "Alexander Lopez", "Hansen Bow", "Kai Zheng"], "title": "SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery", "comment": null, "summary": "Objective: To develop and evaluate machine learning (ML) models for\npredicting length of stay (LOS) in elective spine surgery, with a focus on the\nbenefits of temporal modeling and model interpretability. Materials and\nMethods: We compared traditional ML models (e.g., linear regression, random\nforest, support vector machine (SVM), and XGBoost) with our developed model,\nSurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an\nattention, using structured perioperative electronic health records (EHR) data.\nPerformance was evaluated using the coefficient of determination (R2), and key\npredictors were identified using explainable AI. Results: SurgeryLSTM achieved\nthe highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)\nand baseline models. The attention mechanism improved interpretability by\ndynamically identifying influential temporal segments within preoperative\nclinical sequences, allowing clinicians to trace which events or features most\ncontributed to each LOS prediction. Key predictors of LOS included bone\ndisorder, chronic kidney disease, and lumbar fusion identified as the most\nimpactful predictors of LOS. Discussion: Temporal modeling with attention\nmechanisms significantly improves LOS prediction by capturing the sequential\nnature of patient data. Unlike static models, SurgeryLSTM provides both higher\naccuracy and greater interpretability, which are critical for clinical\nadoption. These results highlight the potential of integrating attention-based\ntemporal models into hospital planning workflows. Conclusion: SurgeryLSTM\npresents an effective and interpretable AI solution for LOS prediction in\nelective spine surgery. Our findings support the integration of temporal,\nexplainable ML approaches into clinical decision support systems to enhance\ndischarge readiness and individualized patient care.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSurgeryLSTM\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u810a\u67f1\u624b\u672f\u4f4f\u9662\u65f6\u957f\uff08LOS\uff09\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u5efa\u6a21\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u810a\u67f1\u624b\u672f\u4f4f\u9662\u65f6\u957f\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u540c\u65f6\u6ce8\u91cd\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6bd4\u8f83\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u7ebf\u6027\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001SVM\u548cXGBoost\uff09\u4e0e\u63d0\u51fa\u7684SurgeryLSTM\u6a21\u578b\uff08\u57fa\u4e8e\u63a9\u7801\u53cc\u5411LSTM\u548c\u6ce8\u610f\u529b\u673a\u5236\uff09\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u3002", "result": "SurgeryLSTM\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u6700\u4f73\uff08R2=0.86\uff09\uff0c\u4f18\u4e8eXGBoost\uff08R2=0.85\uff09\u548c\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002\u6ce8\u610f\u529b\u673a\u5236\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8bc6\u522b\u51fa\u5173\u952e\u9884\u6d4b\u56e0\u7d20\u5982\u9aa8\u75c5\u3001\u6162\u6027\u80be\u75c5\u548c\u8170\u690e\u878d\u5408\u672f\u3002", "conclusion": "\u7ed3\u8bba\u662fSurgeryLSTM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5c06\u65f6\u95f4\u5efa\u6a21\u548c\u53ef\u89e3\u91ca\u6027\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6574\u5408\u5230\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\uff0c\u4ee5\u4f18\u5316\u51fa\u9662\u51c6\u5907\u548c\u4e2a\u6027\u5316\u60a3\u8005\u62a4\u7406\u3002"}}
{"id": "2507.11742", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11742", "abs": "https://arxiv.org/abs/2507.11742", "authors": ["Meng Li", "Timothy M. McPhillips", "Dingmin Wang", "Shin-Rong Tsai", "Bertram Lud\u00e4scher"], "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks", "comment": "Preprint. Accepted to COLM 2025", "summary": "Recognizing the information flows and operations comprising data science and\nmachine learning Python notebooks is critical for evaluating, reusing, and\nadapting notebooks for new tasks. Investigating a notebook via re-execution\noften is impractical due to the challenges of resolving data and software\ndependencies. While Large Language Models (LLMs) pre-trained on large codebases\nhave demonstrated effectiveness in understanding code without running it, we\nobserve that they fail to understand some realistic notebooks due to\nhallucinations and long-context challenges. To address these issues, we propose\na notebook understanding task yielding an information flow graph and\ncorresponding cell execution dependency graph for a notebook, and demonstrate\nthe effectiveness of a pincer strategy that uses limited syntactic analysis to\nassist full comprehension of the notebook using an LLM. Our Capture and Resolve\nAssisted Bounding Strategy (CRABS) employs shallow syntactic parsing and\nanalysis of the abstract syntax tree (AST) to capture the correct\ninterpretation of a notebook between lower and upper estimates of the\ninter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via\ncell-by-cell zero-shot learning, thereby identifying the true data inputs and\noutputs of each cell. We evaluate and demonstrate the effectiveness of our\napproach using an annotated dataset of 50 representative, highly up-voted\nKaggle notebooks that together represent 3454 actual cell inputs and outputs.\nThe LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the\nsyntactic structure of these notebooks. Across 50 notebooks, CRABS achieves\naverage F1 scores of 98% identifying cell-to-cell information flows and 99%\nidentifying transitive cell execution dependencies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6709\u9650\u8bed\u6cd5\u5206\u6790\u548cLLM\u7684\u65b9\u6cd5\uff08CRABS\uff09\uff0c\u7528\u4e8e\u7406\u89e3Python\u7b14\u8bb0\u672c\u7684\u4fe1\u606f\u6d41\u548c\u6267\u884c\u4f9d\u8d56\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86LLM\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u5e7b\u89c9\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u7531\u4e8e\u91cd\u65b0\u6267\u884c\u7b14\u8bb0\u672c\u4ee5\u7406\u89e3\u5176\u4fe1\u606f\u6d41\u548c\u64cd\u4f5c\u4e0d\u5207\u5b9e\u9645\uff0c\u4e14\u73b0\u6709LLM\u5728\u5904\u7406\u590d\u6742\u7b14\u8bb0\u672c\u65f6\u5b58\u5728\u5e7b\u89c9\u548c\u957f\u4e0a\u4e0b\u6587\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCRABS\u7b56\u7565\uff0c\u7ed3\u5408\u6d45\u5c42\u8bed\u6cd5\u5206\u6790\uff08AST\uff09\u548cLLM\u7684\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u6355\u83b7\u7b14\u8bb0\u672c\u7684\u8f93\u5165\u8f93\u51fa\u96c6\u5e76\u89e3\u51b3\u5269\u4f59\u6b67\u4e49\u3002", "result": "\u572850\u4e2a\u4ee3\u8868\u6027Kaggle\u7b14\u8bb0\u672c\u4e0a\u6d4b\u8bd5\uff0cLLM\u89e3\u51b3\u4e8698%\u7684\u6b67\u4e49\uff0cCRABS\u5728\u4fe1\u606f\u6d41\u548c\u6267\u884c\u4f9d\u8d56\u8bc6\u522b\u4e0a\u5206\u522b\u8fbe\u523098%\u548c99%\u7684F1\u5206\u6570\u3002", "conclusion": "CRABS\u901a\u8fc7\u7ed3\u5408\u8bed\u6cd5\u5206\u6790\u548cLLM\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7b14\u8bb0\u672c\u7406\u89e3\u95ee\u9898\uff0c\u4e3a\u8bc4\u4f30\u548c\u91cd\u7528\u7b14\u8bb0\u672c\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2507.11677", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11677", "abs": "https://arxiv.org/abs/2507.11677", "authors": ["Mashrur Rashik", "Jean-Daniel Fekete", "Narges Mahyar"], "title": "CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations", "comment": "To appear in the IEEE Visualization and Visual Analytics (VIS)\n  Conference, Short Paper, 2025", "summary": "Communicating climate change remains challenging, as climate reports, though\nrich in data and visualizations, often feel too abstract or technical for the\npublic. Although personalization can enhance communication, most tools still\nlack the narrative and visualization tailoring needed to connect with\nindividual experiences. We present CLAImate, an AI-enabled prototype that\npersonalizes conversation narratives and localizes visualizations based on\nusers' climate knowledge and geographic location. We evaluated CLAImate through\ninternal verification of factual correctness, a formative study with experts,\nand a pilot with UK residents. CLAImate achieved 66% SNLI accuracy and 70%\nFACTSCORE. Visualization experts appreciated its clarity and personalization,\nand seven out of ten UK participants reported better understanding and local\nrelevance of climate risks with CLAImate. We also discuss design challenges in\npersonalization, accuracy, and scalability, and outline future directions for\nintegrating visualizations in personalized conversational interfaces.", "AI": {"tldr": "CLAImate\u662f\u4e00\u4e2aAI\u539f\u578b\u5de5\u5177\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u5bf9\u8bdd\u548c\u672c\u5730\u5316\u53ef\u89c6\u5316\u6539\u5584\u6c14\u5019\u53d8\u5316\u7684\u6c9f\u901a\u6548\u679c\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u62a5\u544a\u901a\u5e38\u8fc7\u4e8e\u62bd\u8c61\u6216\u6280\u672f\u5316\uff0c\u96be\u4ee5\u5f15\u8d77\u516c\u4f17\u5171\u9e23\uff0c\u800c\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u4e2a\u6027\u5316\u53d9\u4e8b\u548c\u53ef\u89c6\u5316\u3002", "method": "CLAImate\u6839\u636e\u7528\u6237\u7684\u6c14\u5019\u77e5\u8bc6\u548c\u5730\u7406\u4f4d\u7f6e\u4e2a\u6027\u5316\u5bf9\u8bdd\u53d9\u4e8b\u548c\u53ef\u89c6\u5316\uff0c\u5e76\u901a\u8fc7\u5185\u90e8\u9a8c\u8bc1\u3001\u4e13\u5bb6\u7814\u7a76\u53ca\u82f1\u56fd\u5c45\u6c11\u8bd5\u70b9\u8bc4\u4f30\u6548\u679c\u3002", "result": "CLAImate\u5728SNLI\u51c6\u786e\u7387\u8fbe\u523066%\uff0cFACTSCORE\u4e3a70%\uff0c\u4e13\u5bb6\u8ba4\u53ef\u5176\u6e05\u6670\u5ea6\u548c\u4e2a\u6027\u5316\uff0c70%\u7684\u82f1\u56fd\u53c2\u4e0e\u8005\u8868\u793a\u5bf9\u6c14\u5019\u98ce\u9669\u7684\u7406\u89e3\u548c\u672c\u5730\u76f8\u5173\u6027\u6709\u6240\u63d0\u5347\u3002", "conclusion": "CLAImate\u5c55\u793a\u4e86\u5728\u4e2a\u6027\u5316\u6c9f\u901a\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u4e5f\u9762\u4e34\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u7b49\u8bbe\u8ba1\u6311\u6218\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6574\u5408\u53ef\u89c6\u5316\u4e0e\u5bf9\u8bdd\u754c\u9762\u3002"}}
{"id": "2507.11662", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11662", "abs": "https://arxiv.org/abs/2507.11662", "authors": ["Moises Andrade", "Joonhyuk Cha", "Brandon Ho", "Vriksha Srihari", "Karmesh Yadav", "Zsolt Kira"], "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "comment": "Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv", "summary": "Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%.", "AI": {"tldr": "MLLMs\u4f5c\u4e3a\u9a8c\u8bc1\u5668\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5b58\u5728\u4e00\u81f4\u6027\u504f\u5dee\uff0c\u63d0\u51fa\u81ea\u6211\u57fa\u7840\u9a8c\u8bc1\uff08SGV\uff09\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5728\u7f3a\u4e4f\u660e\u786e\u6210\u529f\u6807\u51c6\u7684\u9886\u57df\uff08\u5982\u8ba1\u7b97\u673a\u4f7f\u7528\uff09\u6269\u5c55AI\u9a8c\u8bc1\u5668\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u81ea\u6211\u57fa\u7840\u9a8c\u8bc1\uff08SGV\uff09\uff0c\u5229\u7528MLLMs\u7684\u91c7\u6837\u673a\u5236\uff0c\u5206\u4e24\u6b65\uff1a\u5148\u83b7\u53d6\u4efb\u52a1\u5b8c\u6210\u7684\u5e7f\u6cdb\u5148\u9a8c\uff0c\u518d\u8bc4\u4f30\u5019\u9009\u8f68\u8ff9\u3002", "result": "SGV\u4f7fMLLMs\u9a8c\u8bc1\u5668\u51c6\u786e\u7387\u63d0\u534720\u70b9\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u534748%\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u3002", "conclusion": "SGV\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u4f5c\u4e3a\u9a8c\u8bc1\u5668\u7684\u4e00\u81f4\u6027\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.11574", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11574", "abs": "https://arxiv.org/abs/2507.11574", "authors": ["Kazuma Kobayashi", "Shailesh Garg", "Farid Ahmed", "Souvik Chakraborty", "Syed Bahauddin Alam"], "title": "Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators", "comment": null, "summary": "Robust uncertainty quantification (UQ) remains a critical barrier to the safe\ndeployment of deep learning in real-time virtual sensing, particularly in\nhigh-stakes domains where sparse, noisy, or non-collocated sensor data are the\nnorm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework\nthat transforms neural operator-based virtual sensing with calibrated,\ndistribution-free prediction intervals. By unifying Monte Carlo dropout with\nsplit conformal prediction in a single DeepONet architecture, CMCO achieves\nspatially resolved uncertainty estimates without retraining, ensembling, or\ncustom loss design. Our method addresses a longstanding challenge: how to endow\noperator learning with efficient and reliable UQ across heterogeneous domains.\nThrough rigorous evaluation on three distinct applications: turbulent flow,\nelastoplastic deformation, and global cosmic radiation dose estimation-CMCO\nconsistently attains near-nominal empirical coverage, even in settings with\nstrong spatial gradients and proxy-based sensing. This breakthrough offers a\ngeneral-purpose, plug-and-play UQ solution for neural operators, unlocking\nreal-time, trustworthy inference in digital twins, sensor fusion, and\nsafety-critical monitoring. By bridging theory and deployment with minimal\ncomputational overhead, CMCO establishes a new foundation for scalable,\ngeneralizable, and uncertainty-aware scientific machine learning.", "AI": {"tldr": "CMCO\u6846\u67b6\u7ed3\u5408\u8499\u7279\u5361\u6d1bdropout\u548c\u5206\u5f62\u9884\u6d4b\uff0c\u4e3a\u795e\u7ecf\u7b97\u5b50\u63d0\u4f9b\u9ad8\u6548\u3001\u5206\u5e03\u81ea\u7531\u7684\u9884\u6d4b\u533a\u95f4\uff0c\u89e3\u51b3\u4e86\u5b9e\u65f6\u865a\u62df\u4f20\u611f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u5b9e\u65f6\u865a\u62df\u4f20\u611f\u4e2d\u56e0\u7a00\u758f\u3001\u566a\u58f0\u6216\u975e\u5171\u4f4d\u4f20\u611f\u5668\u6570\u636e\u5bfc\u81f4\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u96be\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u8499\u7279\u5361\u6d1bdropout\u4e0e\u5206\u5f62\u9884\u6d4b\u7ed3\u5408\u5728DeepONet\u67b6\u6784\u4e2d\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5b9a\u5236\u635f\u5931\u8bbe\u8ba1\u7684\u7a7a\u95f4\u89e3\u6790\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5728\u6e4d\u6d41\u3001\u5f39\u5851\u6027\u53d8\u5f62\u548c\u5168\u7403\u5b87\u5b99\u8f90\u5c04\u5242\u91cf\u4f30\u8ba1\u4e09\u4e2a\u5e94\u7528\u4e2d\uff0cCMCO\u5b9e\u73b0\u4e86\u63a5\u8fd1\u540d\u4e49\u8986\u76d6\u7387\u7684\u6027\u80fd\u3002", "conclusion": "CMCO\u4e3a\u795e\u7ecf\u7b97\u5b50\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u5373\u63d2\u5373\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2507.11764", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.11764", "abs": "https://arxiv.org/abs/2507.11764", "authors": ["Matteo Fasulo", "Luca Babboni", "Luca Tedeschini"], "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles", "comment": "14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab", "summary": "This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).", "AI": {"tldr": "AI Wizards\u53c2\u4e0eCLEF 2025 CheckThat! Lab\u4efb\u52a11\uff0c\u901a\u8fc7\u7ed3\u5408\u60c5\u611f\u5206\u6570\u589e\u5f3aTransformer\u6a21\u578b\uff0c\u5728\u591a\u8bed\u8a00\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u63d0\u5347\u4e3b\u89c2\u6027\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u65b0\u95fb\u6587\u7ae0\u4e2d\u53e5\u5b50\u4e3b\u89c2\u6027\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u548c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528mDeBERTaV3-base\u3001ModernBERT-base\u548cLlama3.2-1B\u7b49\u6a21\u578b\uff0c\u7ed3\u5408\u60c5\u611f\u5206\u6570\u589e\u5f3a\u53e5\u5b50\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u51b3\u7b56\u9608\u503c\u6821\u51c6\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "\u60c5\u611f\u7279\u5f81\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4e3b\u89c2F1\u5206\u6570\u4e0a\uff0c\u6700\u7ec8\u5728\u5e0c\u814a\u8bed\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c\u4e00\uff08Macro F1 = 0.51\uff09\u3002", "conclusion": "\u7ed3\u5408\u60c5\u611f\u7279\u5f81\u7684Transformer\u6a21\u578b\u5728\u591a\u8bed\u8a00\u4e3b\u89c2\u6027\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.11797", "categories": ["cs.HC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.11797", "abs": "https://arxiv.org/abs/2507.11797", "authors": ["Diana Romero", "Yasra Chandio", "Fatima Anwar", "Salma Elmalaki"], "title": "GIST: Group Interaction Sensing Toolkit for Mixed Reality", "comment": "11 pages, 6 figures", "summary": "Understanding how teams coordinate, share work, and negotiate roles in\nimmersive environments is critical for designing effective mixed-reality (MR)\napplications that support real-time collaboration. However, existing methods\neither rely on external cameras and offline annotation or focus narrowly on\nsingle modalities, limiting their validity and applicability. To address this,\nwe present a novel group interaction sensing toolkit (GIST), a deployable\nsystem that passively captures multi-modal interaction data, such as speech,\ngaze, and spatial proximity from commodity MR headset's sensors and\nautomatically derives both overall static interaction networks and dynamic\nmoment-by-moment behavior patterns. We evaluate GIST with a human subject study\nwith 48 participants across 12 four-person groups performing an open-ended\nimage-sorting task in MR. Our analysis shows strong alignment between the\nidentified behavior modes and shifts in interaction network structure,\nconfirming that momentary changes in speech, gaze, and proximity data are\nobservable through the sensor data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGIST\u7684\u7fa4\u7ec4\u4ea4\u4e92\u611f\u77e5\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u5728\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\u73af\u5883\u4e2d\u88ab\u52a8\u6355\u83b7\u591a\u6a21\u6001\u4ea4\u4e92\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u4f20\u611f\u5668\u6570\u636e\u8bc6\u522b\u52a8\u6001\u884c\u4e3a\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6444\u50cf\u5934\u6216\u4ec5\u5173\u6ce8\u5355\u4e00\u6a21\u6001\uff0c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u5f00\u53d1\u4e86GIST\u5de5\u5177\u5305\uff0c\u5229\u7528MR\u5934\u663e\u4f20\u611f\u5668\u6355\u83b7\u8bed\u97f3\u3001\u6ce8\u89c6\u548c\u7a7a\u95f4\u63a5\u8fd1\u7b49\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u9759\u6001\u4ea4\u4e92\u7f51\u7edc\u548c\u52a8\u6001\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u901a\u8fc748\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cGIST\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u884c\u4e3a\u6a21\u5f0f\u4e0e\u4ea4\u4e92\u7f51\u7edc\u7ed3\u6784\u7684\u53d8\u5316\u3002", "conclusion": "GIST\u901a\u8fc7\u4f20\u611f\u5668\u6570\u636e\u6210\u529f\u6355\u6349\u4e86\u4ea4\u4e92\u4e2d\u7684\u77ac\u65f6\u53d8\u5316\uff0c\u4e3aMR\u534f\u4f5c\u5e94\u7528\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.11733", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11733", "abs": "https://arxiv.org/abs/2507.11733", "authors": ["Srikanth Vemula"], "title": "ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making", "comment": null, "summary": "This Study introduces Clarity and Reasoning Interface for Artificial\nIntelligence(ClarifAI), a novel approach designed to augment the transparency\nand interpretability of artificial intelligence (AI) in the realm of improved\ndecision making. Leveraging the Case-Based Reasoning (CBR) methodology and\nintegrating an ontology-driven approach, ClarifAI aims to meet the intricate\nexplanatory demands of various stakeholders involved in AI-powered\napplications. The paper elaborates on ClarifAI's theoretical foundations,\ncombining CBR and ontologies to furnish exhaustive explanation mechanisms. It\nfurther elaborates on the design principles and architectural blueprint,\nhighlighting ClarifAI's potential to enhance AI interpretability across\ndifferent sectors and its applicability in high-stake environments. This\nresearch delineates the significant role of ClariAI in advancing the\ninterpretability of AI systems, paving the way for its deployment in critical\ndecision-making processes.", "AI": {"tldr": "ClarifAI\u7ed3\u5408\u6848\u4f8b\u63a8\u7406\u548c\u672c\u4f53\u9a71\u52a8\u65b9\u6cd5\uff0c\u63d0\u5347AI\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u7684\u51b3\u7b56\u573a\u666f\u3002", "motivation": "\u89e3\u51b3AI\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\uff0c\u6ee1\u8db3\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u6848\u4f8b\u63a8\u7406\uff08CBR\uff09\u548c\u672c\u4f53\u9a71\u52a8\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u7406\u8bba\u6846\u67b6\u548c\u67b6\u6784\u84dd\u56fe\u3002", "result": "ClarifAI\u80fd\u663e\u8457\u63d0\u5347AI\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u548c\u9ad8\u98ce\u9669\u73af\u5883\u3002", "conclusion": "ClarifAI\u4e3aAI\u7cfb\u7edf\u5728\u5173\u952e\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11589", "categories": ["cs.LG", "gr-qc"], "pdf": "https://arxiv.org/pdf/2507.11589", "abs": "https://arxiv.org/abs/2507.11589", "authors": ["Sandeep Suresh Cranganore", "Andrei Bodnar", "Arturs Berzins", "Johannes Brandstetter"], "title": "Einstein Fields: A Neural Perspective To Computational General Relativity", "comment": "63 pages, 22 figures, 10 Tables, Github:\n  https://github.com/AndreiB137/EinFields", "summary": "We introduce Einstein Fields, a neural representation that is designed to\ncompress computationally intensive four-dimensional numerical relativity\nsimulations into compact implicit neural network weights. By modeling the\n\\emph{metric}, which is the core tensor field of general relativity, Einstein\nFields enable the derivation of physical quantities via automatic\ndifferentiation. However, unlike conventional neural fields (e.g., signed\ndistance, occupancy, or radiance fields), Einstein Fields are \\emph{Neural\nTensor Fields} with the key difference that when encoding the spacetime\ngeometry of general relativity into neural field representations, dynamics\nemerge naturally as a byproduct. Einstein Fields show remarkable potential,\nincluding continuum modeling of 4D spacetime, mesh-agnosticity, storage\nefficiency, derivative accuracy, and ease of use. We address these challenges\nacross several canonical test beds of general relativity and release an open\nsource JAX-based library, paving the way for more scalable and expressive\napproaches to numerical relativity. Code is made available at\nhttps://github.com/AndreiB137/EinFields", "AI": {"tldr": "Einstein Fields\u662f\u4e00\u79cd\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u538b\u7f29\u8ba1\u7b97\u5bc6\u96c6\u578b\u56db\u7ef4\u6570\u503c\u76f8\u5bf9\u8bba\u6a21\u62df\u4e3a\u7d27\u51d1\u7684\u9690\u5f0f\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u76f8\u5bf9\u8bba\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cEinstein Fields\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u8868\u793a\u63d0\u9ad8\u6548\u7387\u548c\u6613\u7528\u6027\u3002", "method": "\u91c7\u7528Neural Tensor Fields\u5efa\u6a21\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684\u6838\u5fc3\u5f20\u91cf\u573a\uff08metric\uff09\uff0c\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u63a8\u5bfc\u7269\u7406\u91cf\u3002", "result": "Einstein Fields\u57284D\u65f6\u7a7a\u8fde\u7eed\u5efa\u6a21\u3001\u5b58\u50a8\u6548\u7387\u3001\u5bfc\u6570\u51c6\u786e\u6027\u7b49\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\u3002", "conclusion": "Einstein Fields\u4e3a\u6570\u503c\u76f8\u5bf9\u8bba\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u4ee3\u7801\u3002"}}
{"id": "2507.11809", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11809", "abs": "https://arxiv.org/abs/2507.11809", "authors": ["Dante Campregher", "Yanxu Chen", "Sander Hoffman", "Maria Heuss"], "title": "Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models", "comment": "18 Pages, 13 figures", "summary": "This paper presents a reproducibility study examining how Large Language\nModels (LLMs) manage competing factual and counterfactual information, focusing\non the role of attention heads in this process. We attempt to reproduce and\nreconcile findings from three recent studies by Ortu et al., Yu, Merullo, and\nPavlick and McDougall et al. that investigate the competition between\nmodel-learned facts and contradictory context information through Mechanistic\nInterpretability tools. Our study specifically examines the relationship\nbetween attention head strength and factual output ratios, evaluates competing\nhypotheses about attention heads' suppression mechanisms, and investigates the\ndomain specificity of these attention patterns. Our findings suggest that\nattention heads promoting factual output do so via general copy suppression\nrather than selective counterfactual suppression, as strengthening them can\nalso inhibit correct facts. Additionally, we show that attention head behavior\nis domain-dependent, with larger models exhibiting more specialized and\ncategory-sensitive patterns.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u5904\u7406\u4e8b\u5b9e\u4e0e\u53cd\u4e8b\u5b9e\u4fe1\u606f\u7684\u7ade\u4e89\uff0c\u91cd\u70b9\u5173\u6ce8\u6ce8\u610f\u529b\u5934\u7684\u4f5c\u7528\u3002", "motivation": "\u65e8\u5728\u590d\u73b0\u5e76\u6574\u5408\u8fd1\u671f\u4e09\u9879\u7814\u7a76\uff08Ortu\u7b49\u4eba\u3001Yu\u7b49\u4eba\u3001Pavlick\u548cMcDougall\u7b49\u4eba\uff09\u7684\u53d1\u73b0\uff0c\u63a2\u8ba8\u6a21\u578b\u5b66\u4e60\u7684\u4e8b\u5b9e\u4e0e\u77db\u76fe\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e4b\u95f4\u7684\u7ade\u4e89\u673a\u5236\u3002", "method": "\u901a\u8fc7\u673a\u5236\u89e3\u91ca\u5de5\u5177\u5206\u6790\u6ce8\u610f\u529b\u5934\u5f3a\u5ea6\u4e0e\u4e8b\u5b9e\u8f93\u51fa\u6bd4\u4f8b\u7684\u5173\u7cfb\uff0c\u8bc4\u4f30\u5173\u4e8e\u6ce8\u610f\u529b\u5934\u6291\u5236\u673a\u5236\u7684\u7ade\u4e89\u5047\u8bbe\uff0c\u5e76\u7814\u7a76\u8fd9\u4e9b\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u9886\u57df\u7279\u5f02\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4fc3\u8fdb\u4e8b\u5b9e\u8f93\u51fa\u7684\u6ce8\u610f\u529b\u5934\u901a\u8fc7\u901a\u7528\u7684\u590d\u5236\u6291\u5236\u800c\u975e\u9009\u62e9\u6027\u53cd\u4e8b\u5b9e\u6291\u5236\u5b9e\u73b0\uff0c\u4e14\u5176\u884c\u4e3a\u5177\u6709\u9886\u57df\u4f9d\u8d56\u6027\uff0c\u66f4\u5927\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u4e13\u4e1a\u548c\u7c7b\u522b\u654f\u611f\u7684\u6a21\u5f0f\u3002", "conclusion": "\u6ce8\u610f\u529b\u5934\u7684\u4f5c\u7528\u673a\u5236\u662f\u901a\u7528\u7684\uff0c\u4f46\u5176\u884c\u4e3a\u53d7\u9886\u57df\u548c\u6a21\u578b\u89c4\u6a21\u5f71\u54cd\uff0c\u4e3a\u7406\u89e3LLMs\u7684\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.11841", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.11841", "abs": "https://arxiv.org/abs/2507.11841", "authors": ["Xingyu Lan", "Yutong Yang", "Yifan Wang"], "title": "\"Mapping What I Feel\": Understanding Affective Geovisualization Design Through the Lens of People-Place Relationships", "comment": null, "summary": "Affective visualization design is an emerging research direction focused on\ncommunicating and influencing emotion through visualization. However, as\nrevealed by previous research, this area is highly interdisciplinary and\ninvolves theories and practices from diverse fields and disciplines, thus\nawaiting analysis from more fine-grained angles. To address this need, this\nwork focuses on a pioneering and relatively mature sub-area, affective\ngeovisualization design, to further the research in this direction and provide\nmore domain-specific insights. Through an analysis of a curated corpus of\naffective geovisualization designs using the Person-Process-Place (PPP) model\nfrom geographic theory, we derived a design taxonomy that characterizes a\nvariety of methods for eliciting and enhancing emotions through geographic\nvisualization. We also identified four underlying high-level design paradigms\nof affective geovisualization design (e.g., computational, anthropomorphic)\nthat guide distinct approaches to linking geographic information with human\nexperience. By extending existing affective visualization design frameworks\nwith geographic specificity, we provide additional design examples,\ndomain-specific analyses, and insights to guide future research and practices\nin this underexplored yet highly innovative domain.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u60c5\u611f\u5730\u7406\u53ef\u89c6\u5316\u8bbe\u8ba1\uff0c\u901a\u8fc7PPP\u6a21\u578b\u5206\u7c7b\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u56db\u79cd\u8bbe\u8ba1\u8303\u5f0f\uff0c\u4e3a\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u60c5\u611f\u53ef\u89c6\u5316\u8bbe\u8ba1\u662f\u4e00\u4e2a\u8de8\u5b66\u79d1\u7684\u65b0\u5174\u9886\u57df\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u6790\u3002\u672c\u6587\u805a\u7126\u4e8e\u60c5\u611f\u5730\u7406\u53ef\u89c6\u5316\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u4f9b\u66f4\u5177\u4f53\u7684\u9886\u57df\u89c1\u89e3\u3002", "method": "\u4f7f\u7528\u5730\u7406\u7406\u8bba\u4e2d\u7684PPP\u6a21\u578b\u5206\u6790\u60c5\u611f\u5730\u7406\u53ef\u89c6\u5316\u8bbe\u8ba1\uff0c\u5e76\u5206\u7c7b\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u56db\u79cd\u9ad8\u5c42\u6b21\u8bbe\u8ba1\u8303\u5f0f\uff08\u5982\u8ba1\u7b97\u578b\u3001\u62df\u4eba\u5316\u578b\uff09\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u60c5\u611f\u53ef\u89c6\u5316\u8bbe\u8ba1\u6846\u67b6\u3002", "conclusion": "\u901a\u8fc7\u5730\u7406\u7279\u5f02\u6027\u6269\u5c55\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u548c\u521b\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.11737", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11737", "abs": "https://arxiv.org/abs/2507.11737", "authors": ["Chenyu Zhou", "Jingyuan Yang", "Linwei Xin", "Yitian Chen", "Ziyan He", "Dongdong Ge"], "title": "Auto-Formulating Dynamic Programming Problems with Large Language Models", "comment": null, "summary": "Dynamic programming (DP) is a fundamental method in operations research, but\nformulating DP models has traditionally required expert knowledge of both the\nproblem context and DP techniques. Large Language Models (LLMs) offer the\npotential to automate this process. However, DP problems pose unique challenges\ndue to their inherently stochastic transitions and the limited availability of\ntraining data. These factors make it difficult to directly apply existing\nLLM-based models or frameworks developed for other optimization problems, such\nas linear or integer programming. We introduce DP-Bench, the first benchmark\ncovering a wide range of textbook-level DP problems to enable systematic\nevaluation. We present Dynamic Programming Language Model (DPLM), a\n7B-parameter specialized model that achieves performance comparable to\nstate-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on\nhard problems. Central to DPLM's effectiveness is DualReflect, our novel\nsynthetic data generation pipeline, designed to scale up training data from a\nlimited set of initial examples. DualReflect combines forward generation for\ndiversity and backward generation for reliability. Our results reveal a key\ninsight: backward generation is favored in low-data regimes for its strong\ncorrectness guarantees, while forward generation, though lacking such\nguarantees, becomes increasingly valuable at scale for introducing diverse\nformulations. This trade-off highlights the complementary strengths of both\napproaches and the importance of combining them.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDP-Bench\u57fa\u51c6\u548cDPLM\u6a21\u578b\uff0c\u901a\u8fc7DualReflect\u6570\u636e\u751f\u6210\u65b9\u6cd5\u89e3\u51b3\u52a8\u6001\u89c4\u5212\u95ee\u9898\u4e2dLLM\u5e94\u7528\u7684\u6311\u6218\u3002", "motivation": "\u52a8\u6001\u89c4\u5212\u6a21\u578b\u6784\u5efa\u4f20\u7edf\u4e0a\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0cLLM\u6709\u6f5c\u529b\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u4f46\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u968f\u673a\u6027\u6311\u6218\u3002", "method": "\u5f15\u5165DP-Bench\u57fa\u51c6\u548cDPLM\u6a21\u578b\uff0c\u91c7\u7528DualReflect\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u524d\u5411\u548c\u540e\u5411\u751f\u6210\u7b56\u7565\u3002", "result": "DPLM\u5728\u6027\u80fd\u4e0a\u5ab2\u7f8e\u4e3b\u6d41LLM\uff0c\u5e76\u5728\u96be\u9898\u4e0a\u8868\u73b0\u66f4\u4f18\uff1bDualReflect\u5728\u4f4e\u6570\u636e\u548c\u9ad8\u6570\u636e\u573a\u666f\u4e0b\u5404\u6709\u4f18\u52bf\u3002", "conclusion": "\u7ed3\u5408\u524d\u5411\u548c\u540e\u5411\u751f\u6210\u7b56\u7565\u7684DualReflect\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u89c4\u5212\u95ee\u9898\u4e2dLLM\u7684\u6570\u636e\u6311\u6218\u3002"}}
{"id": "2507.11590", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11590", "abs": "https://arxiv.org/abs/2507.11590", "authors": ["Raju Challagundla", "Mohsen Dorodchi", "Pu Wang", "Minwoo Lee"], "title": "Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques", "comment": null, "summary": "As privacy regulations become more stringent and access to real-world data\nbecomes increasingly constrained, synthetic data generation has emerged as a\nvital solution, especially for tabular datasets, which are central to domains\nlike finance, healthcare and the social sciences. This survey presents a\ncomprehensive and focused review of recent advances in synthetic tabular data\ngeneration, emphasizing methods that preserve complex feature relationships,\nmaintain statistical fidelity, and satisfy privacy requirements. A key\ncontribution of this work is the introduction of a novel taxonomy based on\npractical generation objectives, including intended downstream applications,\nprivacy guarantees, and data utility, directly informing methodological design\nand evaluation strategies. Therefore, this review prioritizes the actionable\ngoals that drive synthetic data creation, including conditional generation and\nrisk-sensitive modeling. Additionally, the survey proposes a benchmark\nframework to align technical innovation with real-world demands. By bridging\ntheoretical foundations with practical deployment, this work serves as both a\nroadmap for future research and a guide for implementing synthetic tabular data\nin privacy-critical environments.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5408\u6210\u8868\u683c\u6570\u636e\u751f\u6210\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u4fdd\u7559\u590d\u6742\u7279\u5f81\u5173\u7cfb\u3001\u7edf\u8ba1\u4fdd\u771f\u5ea6\u548c\u9690\u79c1\u8981\u6c42\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u9645\u751f\u6210\u76ee\u6807\u7684\u65b0\u5206\u7c7b\u6cd5\u3002", "motivation": "\u968f\u7740\u9690\u79c1\u6cd5\u89c4\u7684\u4e25\u683c\u5316\u548c\u771f\u5b9e\u6570\u636e\u83b7\u53d6\u53d7\u9650\uff0c\u5408\u6210\u6570\u636e\u751f\u6210\u6210\u4e3a\u89e3\u51b3\u9690\u79c1\u95ee\u9898\u7684\u5173\u952e\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u91d1\u878d\u3001\u533b\u7597\u548c\u793e\u4f1a\u79d1\u5b66\u7b49\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0b\u6e38\u5e94\u7528\u3001\u9690\u79c1\u4fdd\u8bc1\u548c\u6570\u636e\u6548\u7528\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u51c6\u6846\u67b6\u4ee5\u8bc4\u4f30\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "result": "\u7efc\u8ff0\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\uff0c\u5e76\u4e3a\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\u5b9e\u65bd\u5408\u6210\u8868\u683c\u6570\u636e\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u8fde\u63a5\u7406\u8bba\u57fa\u7840\u4e0e\u5b9e\u9645\u9700\u6c42\uff0c\u63a8\u52a8\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5e76\u4e3a\u5176\u5728\u9690\u79c1\u5173\u952e\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2507.11832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11832", "abs": "https://arxiv.org/abs/2507.11832", "authors": ["Yash Ingle", "Pruthwik Mishra"], "title": "ILID: Native Script Language Identification for Indian Languages", "comment": "8 pages, 1 figure, 7 tables, Paper accepted in RANLP 2025", "summary": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script making the task even more challenging.\nIn this paper, we release a dataset of 230K sentences consisting of English and\nall 22 official Indian languages labeled with their language identifiers where\ndata in most languages are newly created. We also develop and release robust\nbaseline models using state-of-the-art approaches in machine learning and deep\nlearning that can aid the research in this field. Our baseline models are\ncomparable to the state-of-the-art models for the language identification task.", "AI": {"tldr": "\u8bba\u6587\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b23\u4e07\u53e5\u82f1\u8bed\u548c22\u79cd\u5370\u5ea6\u5b98\u65b9\u8bed\u8a00\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u7528\u4e8e\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\uff0c\u5c24\u5176\u5728\u5608\u6742\u3001\u7b80\u77ed\u548c\u6df7\u5408\u4ee3\u7801\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8bed\u8a00\u8bc6\u522b\u662fNLP\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u5c24\u5176\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\u3002\u5370\u5ea6\u8bed\u8a00\u56e0\u5171\u4eab\u811a\u672c\u548c\u76f8\u4f3c\u6027\u800c\u589e\u52a0\u4e86\u8bc6\u522b\u96be\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u82f1\u8bed\u548c22\u79cd\u5370\u5ea6\u5b98\u65b9\u8bed\u8a00\u768423\u4e07\u53e5\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u65b9\u6cd5\u5f00\u53d1\u4e86\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u57fa\u7ebf\u6a21\u578b\u5728\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u53d1\u5e03\u65b0\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u4e3a\u5370\u5ea6\u8bed\u8a00\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u5c55\u793a\u4e86\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.11848", "categories": ["cs.HC", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11848", "abs": "https://arxiv.org/abs/2507.11848", "authors": ["Changjian Chen", "Pengcheng Wang", "Fei Lyu", "Zhuo Tang", "Li Yang", "Long Wang", "Yong Cai", "Feng Yu", "Kenli Li"], "title": "Interactive Hybrid Rice Breeding with Parametric Dual Projection", "comment": null, "summary": "Hybrid rice breeding crossbreeds different rice lines and cultivates the\nresulting hybrids in fields to select those with desirable agronomic traits,\nsuch as higher yields. Recently, genomic selection has emerged as an efficient\nway for hybrid rice breeding. It predicts the traits of hybrids based on their\ngenes, which helps exclude many undesired hybrids, largely reducing the\nworkload of field cultivation. However, due to the limited accuracy of genomic\nprediction models, breeders still need to combine their experience with the\nmodels to identify regulatory genes that control traits and select hybrids,\nwhich remains a time-consuming process. To ease this process, in this paper, we\nproposed a visual analysis method to facilitate interactive hybrid rice\nbreeding. Regulatory gene identification and hybrid selection naturally\nensemble a dual-analysis task. Therefore, we developed a parametric dual\nprojection method with theoretical guarantees to facilitate interactive dual\nanalysis. Based on this dual projection method, we further developed a gene\nvisualization and a hybrid visualization to verify the identified regulatory\ngenes and hybrids. The effectiveness of our method is demonstrated through the\nquantitative evaluation of the parametric dual projection method, identified\nregulatory genes and desired hybrids in the case study, and positive feedback\nfrom breeders.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89c6\u5316\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ea4\u4e92\u5f0f\u6742\u4ea4\u6c34\u7a3b\u80b2\u79cd\uff0c\u7ed3\u5408\u57fa\u56e0\u548c\u6742\u4ea4\u53ef\u89c6\u5316\uff0c\u63d0\u9ad8\u80b2\u79cd\u6548\u7387\u3002", "motivation": "\u57fa\u56e0\u7ec4\u9009\u62e9\u867d\u80fd\u9884\u6d4b\u6742\u4ea4\u6c34\u7a3b\u6027\u72b6\uff0c\u4f46\u6a21\u578b\u51c6\u786e\u6027\u6709\u9650\uff0c\u4ecd\u9700\u7ed3\u5408\u7ecf\u9a8c\u7b5b\u9009\u57fa\u56e0\u548c\u6742\u4ea4\u79cd\uff0c\u8fc7\u7a0b\u8017\u65f6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u53cc\u6295\u5f71\u65b9\u6cd5\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u53cc\u5206\u6790\uff0c\u5e76\u57fa\u4e8e\u6b64\u65b9\u6cd5\u8bbe\u8ba1\u4e86\u57fa\u56e0\u548c\u6742\u4ea4\u53ef\u89c6\u5316\u5de5\u5177\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u8bc4\u4f30\u3001\u6848\u4f8b\u7814\u7a76\u548c\u80b2\u79cd\u8005\u53cd\u9988\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6742\u4ea4\u6c34\u7a3b\u80b2\u79cd\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.11787", "categories": ["cs.AI", "68-68W50"], "pdf": "https://arxiv.org/pdf/2507.11787", "abs": "https://arxiv.org/abs/2507.11787", "authors": ["Chandrashekar Muniyappa", "Eunjin Kim"], "title": "Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "Swarm Intelligence (SI) is gaining a lot of popularity in artificial\nintelligence, where the natural behavior of animals and insects is observed and\ntranslated into computer algorithms called swarm computing to solve real-world\nproblems. Due to their effectiveness, they are applied in solving various\ncomputer optimization problems. This survey will review all the latest\ndevelopments in Searching for documents based on semantic similarity using\nSwarm Intelligence algorithms and recommend future research directions.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u7fa4\u4f53\u667a\u80fd\u7b97\u6cd5\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u6587\u6863\u641c\u7d22\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7fa4\u4f53\u667a\u80fd\uff08SI\uff09\u56e0\u5176\u9ad8\u6548\u6027\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u5e7f\u53d7\u6b22\u8fce\uff0c\u901a\u8fc7\u6a21\u62df\u81ea\u7136\u754c\u52a8\u7269\u548c\u6606\u866b\u884c\u4e3a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u603b\u7ed3\u5176\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u6587\u6863\u641c\u7d22\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u7fa4\u4f53\u667a\u80fd\u7b97\u6cd5\u5728\u6587\u6863\u641c\u7d22\u4e2d\u7684\u6700\u65b0\u5e94\u7528\u548c\u53d1\u5c55\u3002", "result": "\u603b\u7ed3\u4e86\u7fa4\u4f53\u667a\u80fd\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u6587\u6863\u641c\u7d22\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7fa4\u4f53\u667a\u80fd\u5728\u6587\u6863\u641c\u7d22\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u5e94\u8fdb\u4e00\u6b65\u4f18\u5316\u7b97\u6cd5\u5e76\u62d3\u5c55\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.11620", "categories": ["cs.LG", "astro-ph.HE", "astro-ph.IM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11620", "abs": "https://arxiv.org/abs/2507.11620", "authors": ["Steven Dillmann", "Juan Rafael Mart\u00ednez-Galarza"], "title": "Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification", "comment": "Accepted at the 2025 ICML Workshop on Machine Learning for\n  Astrophysics, Code available at:\n  https://github.com/StevenDillmann/ml-xraytransients-mnras", "summary": "Event time series are sequences of discrete events occurring at irregular\ntime intervals, each associated with a domain-specific observational modality.\nThey are common in domains such as high-energy astrophysics, computational\nsocial science, cybersecurity, finance, healthcare, neuroscience, and\nseismology. Their unstructured and irregular structure poses significant\nchallenges for extracting meaningful patterns and identifying salient phenomena\nusing conventional techniques. We propose novel two- and three-dimensional\ntensor representations for event time series, coupled with sparse autoencoders\nthat learn physically meaningful latent representations. These embeddings\nsupport a variety of downstream tasks, including anomaly detection,\nsimilarity-based retrieval, semantic clustering, and unsupervised\nclassification. We demonstrate our approach on a real-world dataset from X-ray\nastronomy, showing that these representations successfully capture temporal and\nspectral signatures and isolate diverse classes of X-ray transients. Our\nframework offers a flexible, scalable, and generalizable solution for analyzing\ncomplex, irregular event time series across scientific and industrial domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u4e0d\u89c4\u5219\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\u7684\u4e8c\u7ef4\u548c\u4e09\u7ef4\u5f20\u91cf\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7269\u7406\u610f\u4e49\u660e\u786e\u7684\u6f5c\u5728\u8868\u793a\uff0c\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\u7684\u4e0d\u89c4\u5219\u6027\u548c\u975e\u7ed3\u6784\u5316\u7279\u6027\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u63d0\u53d6\u6709\u610f\u4e49\u6a21\u5f0f\uff0c\u9700\u8981\u65b0\u7684\u8868\u793a\u548c\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e8c\u7ef4\u548c\u4e09\u7ef4\u5f20\u91cf\u8868\u793a\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\uff0c\u7ed3\u5408\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5728X\u5c04\u7ebf\u5929\u6587\u5b66\u6570\u636e\u96c6\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u6355\u6349\u4e86\u65f6\u95f4\u548c\u5149\u8c31\u7279\u5f81\uff0c\u5e76\u5206\u79bb\u4e86\u591a\u79cdX\u5c04\u7ebf\u77ac\u53d8\u7c7b\u522b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8de8\u79d1\u5b66\u548c\u5de5\u4e1a\u9886\u57df\u7684\u590d\u6742\u4e0d\u89c4\u5219\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11851", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11851", "abs": "https://arxiv.org/abs/2507.11851", "authors": ["Mohammad Samragh", "Arnav Kundu", "David Harrison", "Kumari Nishu", "Devang Naik", "Minsik Cho", "Mehrdad Farajtabar"], "title": "Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential", "comment": null, "summary": "Autoregressive language models are constrained by their inherently sequential\nnature, generating one token at a time. This paradigm limits inference speed\nand parallelism, especially during later stages of generation when the\ndirection and semantics of text are relatively certain. In this work, we\npropose a novel framework that leverages the inherent knowledge of vanilla\nautoregressive language models about future tokens, combining techniques to\nrealize this potential and enable simultaneous prediction of multiple\nsubsequent tokens. Our approach introduces several key innovations: (1) a\nmasked-input formulation where multiple future tokens are jointly predicted\nfrom a common prefix; (2) a gated LoRA formulation that preserves the original\nLLM's functionality, while equipping it for multi-token prediction; (3) a\nlightweight, learnable sampler module that generates coherent sequences from\nthe predicted future tokens; (4) a set of auxiliary training losses, including\na consistency loss, to enhance the coherence and accuracy of jointly generated\ntokens; and (5) a speculative generation strategy that expands tokens\nquadratically in the future while maintaining high fidelity. Our method\nachieves significant speedups through supervised fine-tuning on pretrained\nmodels. For example, it generates code and math nearly 5x faster, and improves\ngeneral chat and knowledge tasks by almost 2.5x. These gains come without any\nloss in quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ee4\u724c\u9884\u6d4b\u63d0\u5347\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u548c\u5e76\u884c\u6027\u3002", "motivation": "\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u56e0\u9010\u4ee4\u724c\u751f\u6210\u7684\u987a\u5e8f\u6027\u9650\u5236\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u5e76\u884c\u6027\uff0c\u5c24\u5176\u5728\u751f\u6210\u540e\u671f\u65b9\u5411\u660e\u786e\u65f6\u3002", "method": "\u7ed3\u5408\u63a9\u7801\u8f93\u5165\u3001\u95e8\u63a7LoRA\u3001\u8f7b\u91cf\u91c7\u6837\u5668\u3001\u8f85\u52a9\u8bad\u7ec3\u635f\u5931\u548c\u63a8\u6d4b\u751f\u6210\u7b56\u7565\uff0c\u5b9e\u73b0\u591a\u4ee4\u724c\u9884\u6d4b\u3002", "result": "\u4ee3\u7801\u548c\u6570\u5b66\u751f\u6210\u901f\u5ea6\u63d0\u5347\u8fd15\u500d\uff0c\u901a\u7528\u804a\u5929\u548c\u77e5\u8bc6\u4efb\u52a1\u63d0\u5347\u7ea62.5\u500d\uff0c\u4e14\u8d28\u91cf\u65e0\u635f\u5931\u3002", "conclusion": "\u65b0\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6548\u7387\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.11903", "categories": ["cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.11903", "abs": "https://arxiv.org/abs/2507.11903", "authors": ["Daocheng Lin", "Yifan Wang", "Yutong Yang", "Xingyu Lan"], "title": "Unveiling the Visual Rhetoric of Persuasive Cartography: A Case Study of the Design of Octopus Maps", "comment": null, "summary": "When designed deliberately, data visualizations can become powerful\npersuasive tools, influencing viewers' opinions, values, and actions. While\nresearchers have begun studying this issue (e.g., to evaluate the effects of\npersuasive visualization), we argue that a fundamental mechanism of persuasion\nresides in rhetorical construction, a perspective inadequately addressed in\ncurrent visualization research. To fill this gap, we present a focused analysis\nof octopus maps, a visual genre that has maintained persuasive power across\ncenturies and achieved significant social impact. Employing rhetorical schema\ntheory, we collected and analyzed 90 octopus maps spanning from the 19th\ncentury to contemporary times. We closely examined how octopus maps implement\ntheir persuasive intents and constructed a design space that reveals how visual\nmetaphors are strategically constructed and what common rhetorical strategies\nare applied to components such as maps, octopus imagery, and text. Through the\nabove analysis, we also uncover a set of interesting findings. For instance,\ncontrary to the common perception that octopus maps are primarily a historical\nphenomenon, our research shows that they remain a lively design convention in\ntoday's digital age. Additionally, while most octopus maps stem from Western\ndiscourse that views the octopus as an evil symbol, some designs offer\nalternative interpretations, highlighting the dynamic nature of rhetoric across\ndifferent sociocultural settings. Lastly, drawing from the lessons provided by\noctopus maps, we discuss the associated ethical concerns of persuasive\nvisualization.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6570\u636e\u53ef\u89c6\u5316\u5982\u4f55\u901a\u8fc7\u4fee\u8f9e\u6784\u9020\u6210\u4e3a\u5f3a\u5927\u7684\u8bf4\u670d\u5de5\u5177\uff0c\u5e76\u4ee5\u7ae0\u9c7c\u5730\u56fe\u4e3a\u4f8b\u5206\u6790\u4e86\u5176\u8de8\u4e16\u7eaa\u7684\u4fee\u8f9e\u7b56\u7565\u548c\u793e\u4f1a\u5f71\u54cd\u3002", "motivation": "\u586b\u8865\u5f53\u524d\u53ef\u89c6\u5316\u7814\u7a76\u4e2d\u4fee\u8f9e\u6784\u9020\u89c6\u89d2\u7684\u4e0d\u8db3\uff0c\u63ed\u793a\u7ae0\u9c7c\u5730\u56fe\u4f5c\u4e3a\u8bf4\u670d\u6027\u53ef\u89c6\u5316\u7684\u6301\u4e45\u5f71\u54cd\u529b\u3002", "method": "\u91c7\u7528\u4fee\u8f9e\u56fe\u5f0f\u7406\u8bba\uff0c\u6536\u96c6\u5e76\u5206\u6790\u4e8690\u4e2a19\u4e16\u7eaa\u81f3\u4eca\u7684\u7ae0\u9c7c\u5730\u56fe\uff0c\u6784\u5efa\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u5e76\u8bc6\u522b\u4e86\u5e38\u89c1\u7684\u4fee\u8f9e\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u7ae0\u9c7c\u5730\u56fe\u5728\u73b0\u4ee3\u4ecd\u6d3b\u8dc3\uff0c\u4e14\u5176\u4fee\u8f9e\u7b56\u7565\u56e0\u6587\u5316\u80cc\u666f\u800c\u5f02\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8bf4\u670d\u6027\u53ef\u89c6\u5316\u7684\u4f26\u7406\u95ee\u9898\u3002", "conclusion": "\u7ae0\u9c7c\u5730\u56fe\u5c55\u793a\u4e86\u4fee\u8f9e\u6784\u9020\u5728\u8bf4\u670d\u6027\u53ef\u89c6\u5316\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u547c\u5401\u5173\u6ce8\u5176\u4f26\u7406\u5f71\u54cd\u3002"}}
{"id": "2507.11916", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11916", "abs": "https://arxiv.org/abs/2507.11916", "authors": ["Ehsan Futuhi", "Nathan R. Sturtevant"], "title": "A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS", "comment": null, "summary": "The rapid advancement of GPU technology has unlocked powerful parallel\nprocessing capabilities, creating new opportunities to enhance classic search\nalgorithms. A recent successful application of GPUs is in compressing large\npattern database (PDB) heuristics using neural networks while preserving\nheuristic admissibility. However, very few algorithms have been designed to\nexploit GPUs during search. Several variants of A* exist that batch GPU\ncomputations. In this paper we introduce a method for batching GPU computations\nin depth first search. In particular, we describe a new cost-bounded\ndepth-first search (CB-DFS) method that leverages the combined parallelism of\nmodern CPUs and GPUs. This is used to create algorithms like \\emph{Batch IDA*},\nan extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an\nextensions of Budgeted Tree Search. Our approach builds on the general approach\nused by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality\nguarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding\ntile puzzle (STP), showing that GPU operations can be efficiently batched in\nDFS. Additionally, we conduct extensive experiments to analyze the effects of\nhyperparameters, neural network heuristic size, and hardware resources on\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528GPU\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\u4f18\u5316\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff08DFS\uff09\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u6210\u672c\u53d7\u9650\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff08CB-DFS\uff09\uff0c\u5e76\u6269\u5c55\u4e86Batch IDA*\u548cBatch BTS\u7b97\u6cd5\u3002", "motivation": "GPU\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u7ecf\u5178\u641c\u7d22\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u673a\u4f1a\uff0c\u4f46\u76ee\u524d\u5f88\u5c11\u6709\u7b97\u6cd5\u5728\u641c\u7d22\u8fc7\u7a0b\u4e2d\u5145\u5206\u5229\u7528GPU\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6279\u5904\u7406GPU\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u73b0\u4ee3CPU\u548cGPU\u7684\u5e76\u884c\u80fd\u529b\uff0c\u8bbe\u8ba1\u4e86CB-DFS\u7b97\u6cd5\uff0c\u5e76\u6269\u5c55\u4e86Batch IDA*\u548cBatch BTS\u3002", "result": "\u57283x3\u9b54\u65b9\u548c4x4\u6ed1\u52a8\u62fc\u56fe\uff08STP\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e86\u8d85\u53c2\u6570\u3001\u795e\u7ecf\u7f51\u7edc\u542f\u53d1\u5f0f\u5927\u5c0f\u548c\u786c\u4ef6\u8d44\u6e90\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cGPU\u64cd\u4f5c\u53ef\u4ee5\u5728DFS\u4e2d\u9ad8\u6548\u6279\u5904\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u4f18\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2507.11639", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11639", "abs": "https://arxiv.org/abs/2507.11639", "authors": ["Fouad Oubari", "Raphael Meunier", "Rodrigue D\u00e9catoire", "Mathilde Mougeot"], "title": "Deep Generative Methods and Tire Architecture Design", "comment": null, "summary": "As deep generative models proliferate across the AI landscape, industrial\npractitioners still face critical yet unanswered questions about which deep\ngenerative models best suit complex manufacturing design tasks. This work\naddresses this question through a complete study of five representative models\n(Variational Autoencoder, Generative Adversarial Network, multimodal\nVariational Autoencoder, Denoising Diffusion Probabilistic Model, and\nMultinomial Diffusion Model) on industrial tire architecture generation. Our\nevaluation spans three key industrial scenarios: (i) unconditional generation\nof complete multi-component designs, (ii) component-conditioned generation\n(reconstructing architectures from partial observations), and (iii)\ndimension-constrained generation (creating designs that satisfy specific\ndimensional requirements). To enable discrete diffusion models to handle\nconditional scenarios, we introduce categorical inpainting, a mask-aware\nreverse diffusion process that preserves known labels without requiring\nadditional training. Our evaluation employs geometry-aware metrics specifically\ncalibrated for industrial requirements, quantifying spatial coherence,\ncomponent interaction, structural connectivity, and perceptual fidelity. Our\nfindings reveal that diffusion models achieve the strongest overall\nperformance; a masking-trained VAE nonetheless outperforms the multimodal\nvariant MMVAE\\textsuperscript{+} on nearly all component-conditioned metrics,\nand within the diffusion family MDM leads in-distribution whereas DDPM\ngeneralises better to out-of-distribution dimensional constraints.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e94\u79cd\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u5de5\u4e1a\u8f6e\u80ce\u67b6\u6784\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6269\u6563\u6a21\u578b\u6574\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u7c7b\u522b\u4fee\u590d\u65b9\u6cd5\u3002", "motivation": "\u5de5\u4e1a\u5b9e\u8df5\u4e2d\u7f3a\u4e4f\u5173\u4e8e\u54ea\u79cd\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u6700\u9002\u5408\u590d\u6742\u5236\u9020\u8bbe\u8ba1\u4efb\u52a1\u7684\u660e\u786e\u6307\u5bfc\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u4e94\u79cd\u6a21\u578b\uff08VAE\u3001GAN\u3001MMVAE\u3001DDPM\u3001MDM\uff09\u5728\u4e09\u79cd\u5de5\u4e1a\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u7c7b\u522b\u4fee\u590d\u65b9\u6cd5\u5904\u7406\u6761\u4ef6\u751f\u6210\u3002", "result": "\u6269\u6563\u6a21\u578b\u6574\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u5176\u4e2dMDM\u5728\u5206\u5e03\u5185\u8868\u73b0\u6700\u597d\uff0cDDPM\u5728\u5206\u5e03\u5916\u7ef4\u5ea6\u7ea6\u675f\u4e0b\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u662f\u5de5\u4e1a\u8f6e\u80ce\u67b6\u6784\u751f\u6210\u4efb\u52a1\u7684\u6700\u4f73\u9009\u62e9\uff0c\u540c\u65f6\u7c7b\u522b\u4fee\u590d\u65b9\u6cd5\u4e3a\u6761\u4ef6\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11862", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11862", "abs": "https://arxiv.org/abs/2507.11862", "authors": ["Junhong Ye", "Xu Yuan", "Xinying Qiu"], "title": "Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition", "comment": "Accepted to CLNLP 2025", "summary": "Accurate recognition of personally identifiable information (PII) is central\nto automated text anonymization. This paper investigates the effectiveness of\ncross-domain model transfer, multi-domain data fusion, and sample-efficient\nlearning for PII recognition. Using annotated corpora from healthcare (I2B2),\nlegal (TAB), and biography (Wikipedia), we evaluate models across four\ndimensions: in-domain performance, cross-domain transferability, fusion, and\nfew-shot learning. Results show legal-domain data transfers well to\nbiographical texts, while medical domains resist incoming transfer. Fusion\nbenefits are domain-specific, and high-quality recognition is achievable with\nonly 10% of training data in low-specialization domains.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8de8\u9886\u57df\u6a21\u578b\u8fc1\u79fb\u3001\u591a\u9886\u57df\u6570\u636e\u878d\u5408\u548c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u5bf9PII\u8bc6\u522b\u7684\u6548\u679c\uff0c\u53d1\u73b0\u6cd5\u5f8b\u9886\u57df\u6570\u636e\u5bf9\u4f20\u8bb0\u6587\u672c\u8fc1\u79fb\u6548\u679c\u597d\uff0c\u533b\u7597\u9886\u57df\u8fc1\u79fb\u6548\u679c\u5dee\uff0c\u878d\u5408\u6548\u679c\u56e0\u9886\u57df\u800c\u5f02\uff0c\u4f4e\u4e13\u4e1a\u5316\u9886\u57df\u4ec5\u970010%\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8bc6\u522b\u3002", "motivation": "\u63a2\u7d22PII\u8bc6\u522b\u7684\u8de8\u9886\u57df\u6a21\u578b\u8fc1\u79fb\u3001\u6570\u636e\u878d\u5408\u548c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6587\u672c\u533f\u540d\u5316\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u533b\u7597\uff08I2B2\uff09\u3001\u6cd5\u5f8b\uff08TAB\uff09\u548c\u4f20\u8bb0\uff08Wikipedia\uff09\u9886\u57df\u7684\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u9886\u57df\u5185\u6027\u80fd\u3001\u8de8\u9886\u57df\u8fc1\u79fb\u6027\u3001\u6570\u636e\u878d\u5408\u548c\u5c11\u6837\u672c\u5b66\u4e60\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6cd5\u5f8b\u9886\u57df\u6570\u636e\u5bf9\u4f20\u8bb0\u6587\u672c\u8fc1\u79fb\u6548\u679c\u597d\uff0c\u533b\u7597\u9886\u57df\u8fc1\u79fb\u6548\u679c\u5dee\uff1b\u6570\u636e\u878d\u5408\u6548\u679c\u56e0\u9886\u57df\u800c\u5f02\uff1b\u4f4e\u4e13\u4e1a\u5316\u9886\u57df\u4ec5\u970010%\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8bc6\u522b\u3002", "conclusion": "\u8de8\u9886\u57df\u8fc1\u79fb\u3001\u6570\u636e\u878d\u5408\u548c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u5bf9PII\u8bc6\u522b\u6548\u679c\u663e\u8457\uff0c\u4f46\u6548\u679c\u56e0\u9886\u57df\u4e0d\u540c\u800c\u5f02\uff0c\u9700\u6839\u636e\u9886\u57df\u7279\u70b9\u9009\u62e9\u5408\u9002\u65b9\u6cd5\u3002"}}
{"id": "2507.11911", "categories": ["cs.HC", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11911", "abs": "https://arxiv.org/abs/2507.11911", "authors": ["Xiaoqing Chen", "Siyang Li", "Dongrui Wu"], "title": "AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding", "comment": null, "summary": "Electroencephalogram (EEG) decoding models for brain-computer interfaces\n(BCIs) struggle with cross-dataset learning and generalization due to channel\nlayout inconsistencies, non-stationary signal distributions, and limited\nneurophysiological prior integration. To address these issues, we propose a\nplug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has\ntwo main components: 1) Spatial Alignment, which selects task-relevant channels\nbased on brain-region priors, aligns EEG distributions across domains, and\nremaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,\nwhich models multi-dataset signals into unified spatiotemporal patches for EEG\ndecoding. Compared to 17 state-of-the-art approaches that need dataset-specific\ntuning, the proposed calibration-free AFPM achieves performance gains of up to\n4.40% on motor imagery and 3.58% on event-related potential tasks. To our\nknowledge, this is the first calibration-free cross-dataset EEG decoding\nframework, substantially enhancing the practicalness of BCIs in real-world\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6821\u51c6\u7684\u8de8\u6570\u636e\u96c6EEG\u89e3\u7801\u6846\u67b6AFPM\uff0c\u901a\u8fc7\u7a7a\u95f4\u5bf9\u9f50\u548c\u5e27-\u5757\u7f16\u7801\u63d0\u5347BCI\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3EEG\u89e3\u7801\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u5b66\u4e60\u548c\u6cdb\u5316\u4e2d\u7684\u95ee\u9898\uff0c\u5982\u901a\u9053\u5e03\u5c40\u4e0d\u4e00\u81f4\u3001\u4fe1\u53f7\u5206\u5e03\u975e\u5e73\u7a33\u548c\u795e\u7ecf\u751f\u7406\u5b66\u5148\u9a8c\u6574\u5408\u4e0d\u8db3\u3002", "method": "AFPM\u6846\u67b6\u5305\u62ec\u7a7a\u95f4\u5bf9\u9f50\uff08\u9009\u62e9\u4efb\u52a1\u76f8\u5173\u901a\u9053\u5e76\u7edf\u4e00\u5e03\u5c40\uff09\u548c\u5e27-\u5757\u7f16\u7801\uff08\u5c06\u4fe1\u53f7\u5efa\u6a21\u4e3a\u7edf\u4e00\u65f6\u7a7a\u5757\uff09\u3002", "result": "\u5728\u8fd0\u52a8\u60f3\u8c61\u548c\u4e8b\u4ef6\u76f8\u5173\u7535\u4f4d\u4efb\u52a1\u4e0a\u5206\u522b\u63d0\u53474.40%\u548c3.58%\uff0c\u4f18\u4e8e17\u79cd\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AFPM\u662f\u9996\u4e2a\u65e0\u9700\u6821\u51c6\u7684\u8de8\u6570\u636e\u96c6EEG\u89e3\u7801\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86BCI\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.11988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11988", "abs": "https://arxiv.org/abs/2507.11988", "authors": ["Yexuan Shi", "Mingyu Wang", "Yunxiang Cao", "Hongjie Lai", "Junjian Lan", "Xin Han", "Yu Wang", "Jie Geng", "Zhenan Li", "Zihao Xia", "Xiang Chen", "Chen Li", "Jian Xu", "Wenbo Duan", "Yuanshuo Zhu"], "title": "Aime: Towards Fully-Autonomous Multi-Agent Framework", "comment": "14 pages, 1 figures,", "summary": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration.", "AI": {"tldr": "Aime\u662f\u4e00\u4e2a\u65b0\u578b\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u53cd\u5e94\u5f0f\u89c4\u5212\u548c\u6267\u884c\u89e3\u51b3\u4e86\u4f20\u7edf\u9759\u6001\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u91c7\u7528\u9759\u6001\u89c4\u5212\u4e0e\u6267\u884c\u6846\u67b6\uff0c\u5b58\u5728\u6267\u884c\u50f5\u5316\u3001\u80fd\u529b\u9759\u6001\u548c\u901a\u4fe1\u4f4e\u6548\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "Aime\u6846\u67b6\u5305\u542b\u52a8\u6001\u89c4\u5212\u5668\u3001\u52a8\u6001\u89d2\u8272\u5de5\u5382\u548c\u8fdb\u5ea6\u7ba1\u7406\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u7b56\u7565\u8c03\u6574\u3001\u6309\u9700\u89d2\u8272\u521b\u5efa\u548c\u5168\u5c40\u72b6\u6001\u7ba1\u7406\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08GAIA\u3001SWE-bench Verified\u3001WebVoyager\uff09\u4e2d\uff0cAime\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u4e13\u7528\u667a\u80fd\u4f53\u3002", "conclusion": "Aime\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5177\u5f39\u6027\u548c\u9ad8\u6548\u6027\u7684\u57fa\u7840\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2507.11645", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11645", "abs": "https://arxiv.org/abs/2507.11645", "authors": ["Ahmed Salah", "David Yevick"], "title": "Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation", "comment": "15 pages, 11 figures", "summary": "Grokking refers to delayed generalization in which the increase in test\naccuracy of a neural network occurs appreciably after the improvement in\ntraining accuracy This paper introduces several practical metrics including\nvariance under dropout, robustness, embedding similarity, and sparsity\nmeasures, that can forecast grokking behavior. Specifically, the resilience of\nneural networks to noise during inference is estimated from a Dropout\nRobustness Curve (DRC) obtained from the variation of the accuracy with the\ndropout rate as the model transitions from memorization to generalization. The\nvariance of the test accuracy under stochastic dropout across training\ncheckpoints further exhibits a local maximum during the grokking. Additionally,\nthe percentage of inactive neurons decreases during generalization, while the\nembeddings tend to a bimodal distribution independent of initialization that\ncorrelates with the observed cosine similarity patterns and dataset symmetries.\nThese metrics additionally provide valuable insight into the origin and\nbehaviour of grokking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u51e0\u79cd\u5b9e\u7528\u6307\u6807\uff08\u5982Dropout\u9c81\u68d2\u6027\u66f2\u7ebf\u3001\u7a00\u758f\u6027\u7b49\uff09\u6765\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u201cgrokking\u201d\u884c\u4e3a\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u8d77\u6e90\u548c\u52a8\u6001\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u4e2d\u201cgrokking\u201d\uff08\u5ef6\u8fdf\u6cdb\u5316\uff09\u73b0\u8c61\u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u7406\u89e3\u5176\u884c\u4e3a\u673a\u5236\u3002", "method": "\u901a\u8fc7Dropout\u9c81\u68d2\u6027\u66f2\u7ebf\uff08DRC\uff09\u3001\u6d4b\u8bd5\u7cbe\u5ea6\u65b9\u5dee\u3001\u795e\u7ecf\u5143\u7a00\u758f\u6027\u548c\u5d4c\u5165\u76f8\u4f3c\u6027\u7b49\u6307\u6807\u5206\u6790\u6a21\u578b\u4ece\u8bb0\u5fc6\u5230\u6cdb\u5316\u7684\u8f6c\u53d8\u3002", "result": "\u53d1\u73b0DRC\u3001\u7cbe\u5ea6\u65b9\u5dee\u548c\u795e\u7ecf\u5143\u6d3b\u52a8\u6027\u7b49\u6307\u6807\u80fd\u6709\u6548\u9884\u6d4bgrokking\uff0c\u4e14\u5d4c\u5165\u5206\u5e03\u4e0e\u6570\u636e\u96c6\u5bf9\u79f0\u6027\u76f8\u5173\u3002", "conclusion": "\u63d0\u51fa\u7684\u6307\u6807\u4e0d\u4ec5\u9884\u6d4b\u4e86grokking\u884c\u4e3a\uff0c\u8fd8\u4e3a\u5176\u8d77\u6e90\u548c\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.11867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11867", "abs": "https://arxiv.org/abs/2507.11867", "authors": ["Xiangyu Yang", "Xinying Qiu"], "title": "COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction", "comment": "Accepted to CLNLP 2025", "summary": "Grammatical Error Correction (GEC) and grammatical acceptability judgment\n(COLA) are core tasks in natural language processing, sharing foundational\ngrammatical knowledge yet typically evolving independently. This paper\nintroduces COLA-GEC, a novel bidirectional framework that enhances both tasks\nthrough mutual knowledge transfer. First, we augment grammatical acceptability\nmodels using GEC datasets, significantly improving their performance across\nmultiple languages. Second, we integrate grammatical acceptability signals into\nGEC model training via a dynamic loss function, effectively guiding corrections\ntoward grammatically acceptable outputs. Our approach achieves state-of-the-art\nresults on several multilingual benchmarks. Comprehensive error analysis\nhighlights remaining challenges, particularly in punctuation error correction,\nproviding insights for future improvements in grammatical modeling.", "AI": {"tldr": "COLA-GEC\u6846\u67b6\u901a\u8fc7\u53cc\u5411\u77e5\u8bc6\u8f6c\u79fb\u63d0\u5347\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\uff08GEC\uff09\u548c\u8bed\u6cd5\u53ef\u63a5\u53d7\u6027\u5224\u65ad\uff08COLA\uff09\u4efb\u52a1\uff0c\u5b9e\u73b0\u591a\u8bed\u8a00\u57fa\u51c6\u4e0a\u7684\u6700\u4f18\u8868\u73b0\u3002", "motivation": "GEC\u548cCOLA\u4efb\u52a1\u5171\u4eab\u8bed\u6cd5\u77e5\u8bc6\u4f46\u72ec\u7acb\u53d1\u5c55\uff0c\u901a\u8fc7\u53cc\u5411\u6846\u67b6\u5b9e\u73b0\u77e5\u8bc6\u4e92\u8865\u3002", "method": "1. \u5229\u7528GEC\u6570\u636e\u96c6\u589e\u5f3a\u8bed\u6cd5\u53ef\u63a5\u53d7\u6027\u6a21\u578b\uff1b2. \u901a\u8fc7\u52a8\u6001\u635f\u5931\u51fd\u6570\u5c06\u8bed\u6cd5\u53ef\u63a5\u53d7\u6027\u4fe1\u53f7\u6574\u5408\u5230GEC\u8bad\u7ec3\u4e2d\u3002", "result": "\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\uff0c\u4f46\u6807\u70b9\u9519\u8bef\u7ea0\u6b63\u4ecd\u6709\u6311\u6218\u3002", "conclusion": "COLA-GEC\u6846\u67b6\u6709\u6548\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u8bed\u6cd5\u5efa\u6a21\u3002"}}
{"id": "2507.11960", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11960", "abs": "https://arxiv.org/abs/2507.11960", "authors": ["Hyein Hong", "Sangbong Yoo", "SeokHwan Choi", "Jisue Kim", "Seongbum Seo", "Haneol Cho", "Chansoo Kim", "Yun Jang"], "title": "d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement", "comment": null, "summary": "Approaches to enhancing data quality (DQ) are classified into two main\ncategories: data- and process-driven. However, prior research has predominantly\nutilized batch data preprocessing within the data-driven framework, which often\nproves insufficient for optimizing machine learning (ML) model performance and\nfrequently leads to distortions in data characteristics. Existing studies have\nprimarily focused on data preprocessing rather than genuine data quality\nimprovement (DQI). In this paper, we introduce d-DQIVAR, a novel visual\nanalytics system designed to facilitate DQI strategies aimed at improving ML\nmodel performance. Our system integrates visual analytics techniques that\nleverage both data-driven and process-driven approaches. Data-driven techniques\ntackle DQ issues such as imputation, outlier detection, deletion, format\nstandardization, removal of duplicate records, and feature selection.\nProcess-driven strategies encompass evaluating DQ and DQI procedures by\nconsidering DQ dimensions and ML model performance and applying the\nKolmogorov-Smirnov test. We illustrate how our system empowers users to harness\nexpert and domain knowledge effectively within a practical workflow through\ncase studies, evaluations, and user studies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fad-DQIVAR\u7cfb\u7edf\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u6d41\u7a0b\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u5206\u6790\u63d0\u5347\u6570\u636e\u8d28\u91cf\uff08DQ\uff09\u4ee5\u4f18\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6570\u636e\u9884\u5904\u7406\u800c\u975e\u771f\u6b63\u7684\u6570\u636e\u8d28\u91cf\u6539\u8fdb\uff08DQI\uff09\uff0c\u4e14\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u6279\u91cf\u9884\u5904\u7406\u4e2d\u5e38\u5bfc\u81f4\u6570\u636e\u7279\u6027\u5931\u771f\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "d-DQIVAR\u7cfb\u7edf\u6574\u5408\u6570\u636e\u9a71\u52a8\uff08\u5982\u586b\u8865\u3001\u5f02\u5e38\u68c0\u6d4b\uff09\u548c\u6d41\u7a0b\u9a71\u52a8\uff08\u5982DQ\u8bc4\u4f30\u3001K-S\u68c0\u9a8c\uff09\u6280\u672f\uff0c\u7ed3\u5408\u53ef\u89c6\u5316\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u7528\u6237\u8bc4\u4f30\uff0c\u7cfb\u7edf\u8bc1\u660e\u80fd\u6709\u6548\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\u63d0\u5347DQ\u548c\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "d-DQIVAR\u4e3a\u6570\u636e\u8d28\u91cf\u6539\u8fdb\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u7ed3\u5408\u4e24\u79cd\u65b9\u6cd5\u4f18\u5316\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.11992", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11992", "abs": "https://arxiv.org/abs/2507.11992", "authors": ["Pranav Rajbhandari", "Abhi Veda", "Matthew Garratt", "Mandayam Srinivasan", "Sridhar Ravi"], "title": "Understanding visual attention beehind bee-inspired UAV navigation", "comment": null, "summary": "Bio-inspired design is often used in autonomous UAV navigation due to the\ncapacity of biological systems for flight and obstacle avoidance despite\nlimited sensory and computational capabilities. In particular, honeybees mainly\nuse the sensory input of optic flow, the apparent motion of objects in their\nvisual field, to navigate cluttered environments. In our work, we train a\nReinforcement Learning agent to navigate a tunnel with obstacles using only\noptic flow as sensory input. We inspect the attention patterns of trained\nagents to determine the regions of optic flow on which they primarily base\ntheir motor decisions. We find that agents trained in this way pay most\nattention to regions of discontinuity in optic flow, as well as regions with\nlarge optic flow magnitude. The trained agents appear to navigate a cluttered\ntunnel by avoiding the obstacles that produce large optic flow, while\nmaintaining a centered position in their environment, which resembles the\nbehavior seen in flying insects. This pattern persists across independently\ntrained agents, which suggests that this could be a good strategy for\ndeveloping a simple explicit control law for physical UAVs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5149\u6d41\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u4ee3\u7406\u4e3b\u8981\u5173\u6ce8\u5149\u6d41\u7684\u4e0d\u8fde\u7eed\u533a\u57df\u548c\u5927\u5149\u6d41\u533a\u57df\uff0c\u884c\u4e3a\u7c7b\u4f3c\u6606\u866b\u98de\u884c\u3002", "motivation": "\u751f\u7269\u7cfb\u7edf\uff08\u5982\u871c\u8702\uff09\u5229\u7528\u5149\u6d41\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u80fd\u529b\u6fc0\u53d1\u4e86\u7814\u7a76\uff0c\u5e0c\u671b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6a21\u62df\u8fd9\u79cd\u884c\u4e3a\u3002", "method": "\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4ec5\u4f7f\u7528\u5149\u6d41\u4f5c\u4e3a\u8f93\u5165\u5728\u969c\u788d\u7269\u96a7\u9053\u4e2d\u5bfc\u822a\uff0c\u5e76\u5206\u6790\u5176\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "result": "\u4ee3\u7406\u4e3b\u8981\u5173\u6ce8\u5149\u6d41\u7684\u4e0d\u8fde\u7eed\u533a\u57df\u548c\u5927\u5149\u6d41\u533a\u57df\uff0c\u884c\u4e3a\u7c7b\u4f3c\u6606\u866b\u98de\u884c\uff0c\u4e14\u5728\u4e0d\u540c\u4ee3\u7406\u4e2d\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u8fd9\u79cd\u7b56\u7565\u53ef\u80fd\u9002\u7528\u4e8e\u5f00\u53d1\u7b80\u5355\u7684\u65e0\u4eba\u673a\u663e\u5f0f\u63a7\u5236\u6cd5\u5219\u3002"}}
{"id": "2507.11649", "categories": ["cs.LG", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.11649", "abs": "https://arxiv.org/abs/2507.11649", "authors": ["Daniel Commey", "Benjamin Appiah", "Griffith S. Klogo", "Garth V. Crosby"], "title": "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training on decentralized\ndata without exposing raw data. However, the evaluation phase in FL may leak\nsensitive information through shared performance metrics. In this paper, we\npropose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to\nenable privacy-preserving and verifiable evaluation for FL. Instead of\nrevealing raw loss values, clients generate a succinct proof asserting that\ntheir local loss is below a predefined threshold. Our approach is implemented\nwithout reliance on external APIs, using self-contained modules for federated\nlearning simulation, ZKP circuit design, and experimental evaluation on both\nthe MNIST and Human Activity Recognition (HAR) datasets. We focus on a\nthreshold-based proof for a simple Convolutional Neural Network (CNN) model\n(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate\nthe approach in terms of computational overhead, communication cost, and\nverifiability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u77e5\u8bc6\u8bc1\u660e\uff08ZKP\uff09\u7684\u8054\u90a6\u5b66\u4e60\u9690\u79c1\u4fdd\u62a4\u8bc4\u4f30\u534f\u8bae\uff0c\u907f\u514d\u901a\u8fc7\u6027\u80fd\u6307\u6807\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u8bc4\u4f30\u9636\u6bb5\u53ef\u80fd\u901a\u8fc7\u5171\u4eab\u6027\u80fd\u6307\u6807\u6cc4\u9732\u654f\u611f\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u4e14\u53ef\u9a8c\u8bc1\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u96f6\u77e5\u8bc6\u8bc1\u660e\u751f\u6210\u7b80\u6d01\u8bc1\u660e\uff0c\u65ad\u8a00\u672c\u5730\u635f\u5931\u4f4e\u4e8e\u9884\u5b9a\u4e49\u9608\u503c\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8API\uff0c\u5b9e\u73b0\u81ea\u5305\u542b\u6a21\u5757\u3002", "result": "\u5728MNIST\u548cHAR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u3001\u901a\u4fe1\u6210\u672c\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u8bae\u6709\u6548\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u53ef\u9a8c\u8bc1\u7684\u8054\u90a6\u5b66\u4e60\u8bc4\u4f30\u3002"}}
{"id": "2507.11875", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11875", "abs": "https://arxiv.org/abs/2507.11875", "authors": ["Tianyou Huang", "Xinglu Chen", "Jingshen Zhang", "Xinying Qiu", "Ruiying Niu"], "title": "DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation", "comment": "Accepted to CCL 2025", "summary": "This paper introduces DualReward, a novel reinforcement learning framework\nfor automatic distractor generation in cloze tests. Unlike conventional\napproaches that rely primarily on supervised learning or static generative\nmodels, our method employs a dual reward structure with adaptive scaling that\ndifferentiates between human-created gold standard distractors and\nmodel-generated candidates. The framework dynamically adjusts reward signal\nintensity based on model performance and confidence. We evaluate our approach\non both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,\ndemonstrating consistent improvements over state-of-the-art baselines.\nExperimental results show that our adaptive reward scaling mechanism provides\nmodest but consistent benefits on homogeneous datasets (CLOTH-F) and more\nsubstantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data\n(MCQ), suggesting its particular effectiveness for handling varied question\ntypes and domains. Our work offers a flexible framework that effectively\nbalances learning from reliable human examples while exploring novel,\nhigh-quality distractors for automated test generation.", "AI": {"tldr": "DualReward\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5b8c\u5f62\u586b\u7a7a\u6d4b\u8bd5\u7684\u5e72\u6270\u9879\uff0c\u901a\u8fc7\u53cc\u5956\u52b1\u7ed3\u6784\u548c\u81ea\u9002\u5e94\u7f29\u653e\u673a\u5236\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\u6216\u9759\u6001\u751f\u6210\u6a21\u578b\uff0c\u65e0\u6cd5\u52a8\u6001\u533a\u5206\u4eba\u7c7b\u521b\u5efa\u7684\u5e72\u6270\u9879\u548c\u6a21\u578b\u751f\u6210\u7684\u5019\u9009\u5e72\u6270\u9879\u3002", "method": "\u91c7\u7528\u53cc\u5956\u52b1\u7ed3\u6784\u548c\u81ea\u9002\u5e94\u7f29\u653e\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u5956\u52b1\u4fe1\u53f7\u5f3a\u5ea6\u3002", "result": "\u5728CLOTH-F\u548cMCQ\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8de8\u57df\u6570\u636e\u4e0a\u63d0\u5347\u663e\u8457\uff08P@1\u63d0\u9ad83.48-3.86%\uff09\u3002", "conclusion": "DualReward\u6846\u67b6\u80fd\u6709\u6548\u5e73\u8861\u4ece\u53ef\u9760\u4eba\u7c7b\u793a\u4f8b\u4e2d\u5b66\u4e60\u548c\u63a2\u7d22\u9ad8\u8d28\u91cf\u5e72\u6270\u9879\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u3002"}}
{"id": "2507.11984", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11984", "abs": "https://arxiv.org/abs/2507.11984", "authors": ["Hyeon Jeon", "Jeongin Park", "Soohyun Lee", "Dae Hyun Kim", "Sungbok Shin", "Jinwook Seo"], "title": "Dataset-Adaptive Dimensionality Reduction", "comment": "IEEE VIS 2025 & IEEE Transactions on Visualization and Computer\n  Graphics (TVCG)", "summary": "Selecting the appropriate dimensionality reduction (DR) technique and\ndetermining its optimal hyperparameter settings that maximize the accuracy of\nthe output projections typically involves extensive trial and error, often\nresulting in unnecessary computational overhead. To address this challenge, we\npropose a dataset-adaptive approach to DR optimization guided by structural\ncomplexity metrics. These metrics quantify the intrinsic complexity of a\ndataset, predicting whether higher-dimensional spaces are necessary to\nrepresent it accurately. Since complex datasets are often inaccurately\nrepresented in two-dimensional projections, leveraging these metrics enables us\nto predict the maximum achievable accuracy of DR techniques for a given\ndataset, eliminating redundant trials in optimizing DR. We introduce the design\nand theoretical foundations of these structural complexity metrics. We\nquantitatively verify that our metrics effectively approximate the ground truth\ncomplexity of datasets and confirm their suitability for guiding\ndataset-adaptive DR workflow. Finally, we empirically show that our\ndataset-adaptive workflow significantly enhances the efficiency of DR\noptimization without compromising accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u590d\u6742\u6027\u5ea6\u91cf\u7684\u6570\u636e\u96c6\u81ea\u9002\u5e94\u964d\u7ef4\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u964d\u7ef4\u6548\u7387\u4e14\u4e0d\u635f\u5931\u7cbe\u5ea6\u3002", "motivation": "\u964d\u7ef4\u6280\u672f\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bd5\u9519\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u7ed3\u6784\u590d\u6742\u6027\u5ea6\u91cf\u91cf\u5316\u6570\u636e\u96c6\u5185\u5728\u590d\u6742\u6027\uff0c\u9884\u6d4b\u964d\u7ef4\u6280\u672f\u7684\u6700\u5927\u53ef\u8fbe\u5230\u7cbe\u5ea6\uff0c\u907f\u514d\u5197\u4f59\u8bd5\u9519\u3002", "result": "\u9a8c\u8bc1\u4e86\u5ea6\u91cf\u80fd\u6709\u6548\u8fd1\u4f3c\u6570\u636e\u96c6\u771f\u5b9e\u590d\u6742\u6027\uff0c\u5e76\u8bc1\u5b9e\u5176\u9002\u7528\u4e8e\u6307\u5bfc\u81ea\u9002\u5e94\u964d\u7ef4\u6d41\u7a0b\u3002", "conclusion": "\u6570\u636e\u96c6\u81ea\u9002\u5e94\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u964d\u7ef4\u4f18\u5316\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7cbe\u5ea6\u3002"}}
{"id": "2507.12110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12110", "abs": "https://arxiv.org/abs/2507.12110", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Zhuang Zhang"], "title": "Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs", "comment": "16 pages, 16 figures", "summary": "The exploration-exploitation trade-off constitutes one of the fundamental\nchallenges in reinforcement learning (RL), which is exacerbated in multi-agent\nreinforcement learning (MARL) due to the exponential growth of joint\nstate-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)\nmethod for optimizing cooperative decision-making of connected and autonomous\nvehicles (CAVs) in mixed traffic. This work presents two primary contributions:\nFirst, we construct a game topology tensor for dynamic traffic flow,\neffectively compressing high-dimensional traffic state information and decrease\nthe search space for MARL algorithms. Second, building upon the designed game\ntopology tensor and using QMIX as the backbone RL algorithm, we establish a\ntopology-enhanced MARL framework incorporating visit counts and agent mutual\ninformation. Extensive simulations across varying traffic densities and CAV\npenetration rates demonstrate the effectiveness of TPE-MARL. Evaluations\nencompassing training dynamics, exploration patterns, macroscopic traffic\nperformance metrics, and microscopic vehicle behaviors reveal that TPE-MARL\nsuccessfully balances exploration and exploitation. Consequently, it exhibits\nsuperior performance in terms of traffic efficiency, safety, decision\nsmoothness, and task completion. Furthermore, the algorithm demonstrates\ndecision-making rationality comparable to or exceeding that of human drivers in\nboth mixed-autonomy and fully autonomous traffic scenarios. Code of our work is\navailable at\n\\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u62d3\u6251\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08TPE-MARL\uff09\uff0c\u7528\u4e8e\u4f18\u5316\u6df7\u5408\u4ea4\u901a\u4e2d\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAVs\uff09\u7684\u534f\u540c\u51b3\u7b56\u3002\u901a\u8fc7\u6784\u5efa\u6e38\u620f\u62d3\u6251\u5f20\u91cf\u548c\u7ed3\u5408QMIX\u7b97\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u51b3\u7b56\u5408\u7406\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u6df7\u5408\u4ea4\u901a\u4e2d\u9762\u4e34\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u6307\u6570\u589e\u957f\u7684\u6311\u6218\uff0c\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u5c24\u4e3a\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u62d3\u6251\u7ed3\u6784\u4f18\u5316MARL\uff0c\u63d0\u5347CAVs\u7684\u534f\u540c\u51b3\u7b56\u80fd\u529b\u3002", "method": "1. \u6784\u5efa\u52a8\u6001\u4ea4\u901a\u6d41\u7684\u6e38\u620f\u62d3\u6251\u5f20\u91cf\uff0c\u538b\u7f29\u9ad8\u7ef4\u72b6\u6001\u4fe1\u606f\uff1b2. \u57fa\u4e8eQMIX\u7b97\u6cd5\uff0c\u7ed3\u5408\u8bbf\u95ee\u8ba1\u6570\u548c\u667a\u80fd\u4f53\u4e92\u4fe1\u606f\uff0c\u5efa\u7acb\u62d3\u6251\u589e\u5f3a\u7684MARL\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTPE-MARL\u5728\u591a\u79cd\u4ea4\u901a\u5bc6\u5ea6\u548cCAV\u6e17\u900f\u7387\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u8861\u4e86\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u51b3\u7b56\u5e73\u6ed1\u6027\uff0c\u4e14\u51b3\u7b56\u5408\u7406\u6027\u63a5\u8fd1\u6216\u8d85\u8fc7\u4eba\u7c7b\u9a7e\u9a76\u5458\u3002", "conclusion": "TPE-MARL\u901a\u8fc7\u62d3\u6251\u7ed3\u6784\u4f18\u5316MARL\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u6df7\u5408\u4ea4\u901a\u4e2d\u7684CAV\u534f\u540c\u51b3\u7b56\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11660", "categories": ["cs.LG", "cs.MA", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11660", "abs": "https://arxiv.org/abs/2507.11660", "authors": ["Joao F. Rocha", "Ke Xu", "Xingzhi Sun", "Ananya Krishna", "Dhananjay Bhaskar", "Blanche Mongeon", "Morgan Craig", "Mark Gerstein", "Smita Krishnaswamy"], "title": "STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics", "comment": null, "summary": "The advent of single-cell technology has significantly improved our\nunderstanding of cellular states and subpopulations in various tissues under\nnormal and diseased conditions by employing data-driven approaches such as\nclustering and trajectory inference. However, these methods consider cells as\nindependent data points of population distributions. With spatial\ntranscriptomics, we can represent cellular organization, along with dynamic\ncell-cell interactions that lead to changes in cell state. Still, key\ncomputational advances are necessary to enable the data-driven learning of such\ncomplex interactive cellular dynamics. While agent-based modeling (ABM)\nprovides a powerful framework, traditional approaches rely on handcrafted rules\nderived from domain knowledge rather than data-driven approaches. To address\nthis, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)\nintegrating ABM with deep learning to model intercellular communication, and\nits effect on the intracellular gene regulatory network. Using graph ODE\nnetworks (GDEs) with shared weights per cell type, our approach represents\ngenes as vertices and interactions as directed edges, dynamically learning\ntheir strengths through a designed attention mechanism. Trained to match\ncontinuous trajectories of simulated as well as inferred trajectories from\nspatial transcriptomics data, the model captures both intercellular and\nintracellular interactions, enabling a more adaptive and accurate\nrepresentation of cellular dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u5efa\u6a21\uff08ABM\uff09\u7684\u65b9\u6cd5STAGED\uff0c\u7528\u4e8e\u6a21\u62df\u7ec6\u80de\u95f4\u901a\u4fe1\u53ca\u5176\u5bf9\u7ec6\u80de\u5185\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u7684\u5f71\u54cd\u3002", "motivation": "\u5355\u7ec6\u80de\u6280\u672f\u867d\u7136\u80fd\u63ed\u793a\u7ec6\u80de\u72b6\u6001\u548c\u4e9a\u7fa4\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5c06\u7ec6\u80de\u89c6\u4e3a\u72ec\u7acb\u6570\u636e\u70b9\uff0c\u7f3a\u4e4f\u5bf9\u7ec6\u80de\u95f4\u52a8\u6001\u4ea4\u4e92\u7684\u5efa\u6a21\u3002", "method": "\u7ed3\u5408ABM\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff0c\u4f7f\u7528\u56feODE\u7f51\u7edc\uff08GDEs\uff09\u548c\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u5b66\u4e60\u57fa\u56e0\u95f4\u4ea4\u4e92\u5f3a\u5ea6\u3002", "result": "\u6a21\u578b\u80fd\u591f\u6355\u6349\u7ec6\u80de\u95f4\u548c\u7ec6\u80de\u5185\u4ea4\u4e92\uff0c\u66f4\u51c6\u786e\u5730\u8868\u793a\u7ec6\u80de\u52a8\u6001\u3002", "conclusion": "STAGED\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u590d\u6742\u7ec6\u80de\u52a8\u6001\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.11878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11878", "abs": "https://arxiv.org/abs/2507.11878", "authors": ["Jiachen Zhao", "Jing Huang", "Zhengxuan Wu", "David Bau", "Weiyan Shi"], "title": "LLMs Encode Harmfulness and Refusal Separately", "comment": null, "summary": "LLMs are trained to refuse harmful instructions, but do they truly understand\nharmfulness beyond just refusing? Prior work has shown that LLMs' refusal\nbehaviors can be mediated by a one-dimensional subspace, i.e., a refusal\ndirection. In this work, we identify a new dimension to analyze safety\nmechanisms in LLMs, i.e., harmfulness, which is encoded internally as a\nseparate concept from refusal. There exists a harmfulness direction that is\ndistinct from the refusal direction. As causal evidence, steering along the\nharmfulness direction can lead LLMs to interpret harmless instructions as\nharmful, but steering along the refusal direction tends to elicit refusal\nresponses directly without reversing the model's judgment on harmfulness.\nFurthermore, using our identified harmfulness concept, we find that certain\njailbreak methods work by reducing the refusal signals without reversing the\nmodel's internal belief of harmfulness. We also find that adversarially\nfinetuning models to accept harmful instructions has minimal impact on the\nmodel's internal belief of harmfulness. These insights lead to a practical\nsafety application: The model's latent harmfulness representation can serve as\nan intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing\nover-refusals that is robust to finetuning attacks. For instance, our Latent\nGuard achieves performance comparable to or better than Llama Guard 3 8B, a\ndedicated finetuned safeguard model, across different jailbreak methods. Our\nfindings suggest that LLMs' internal understanding of harmfulness is more\nrobust than their refusal decision to diverse input instructions, offering a\nnew perspective to study AI safety", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5185\u90e8\u5bf9\u201c\u6709\u5bb3\u6027\u201d\u548c\u201c\u62d2\u7edd\u201d\u662f\u4e24\u4e2a\u72ec\u7acb\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u64cd\u7eb5\u201c\u6709\u5bb3\u6027\u65b9\u5411\u201d\u53ef\u4ee5\u6539\u53d8\u6a21\u578b\u5bf9\u6307\u4ee4\u7684\u5224\u65ad\uff0c\u800c\u201c\u62d2\u7edd\u65b9\u5411\u201d\u4ec5\u5f71\u54cd\u62d2\u7edd\u884c\u4e3a\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u673a\u5236\uff08Latent Guard\uff09\u3002", "motivation": "\u63a2\u7a76LLMs\u662f\u5426\u771f\u6b63\u7406\u89e3\u201c\u6709\u5bb3\u6027\u201d\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u673a\u68b0\u5730\u62d2\u7edd\u6709\u5bb3\u6307\u4ee4\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u548c\u5206\u6790LLMs\u5185\u90e8\u7684\u201c\u6709\u5bb3\u6027\u65b9\u5411\u201d\u548c\u201c\u62d2\u7edd\u65b9\u5411\u201d\uff0c\u9a8c\u8bc1\u5176\u72ec\u7acb\u6027\u548c\u5f71\u54cd\u3002\u8fdb\u4e00\u6b65\u63d0\u51faLatent Guard\u4f5c\u4e3a\u5b89\u5168\u5e94\u7528\u3002", "result": "\u53d1\u73b0\u201c\u6709\u5bb3\u6027\u201d\u548c\u201c\u62d2\u7edd\u201d\u662f\u4e24\u4e2a\u72ec\u7acb\u7684\u6982\u5ff5\uff0cLatent Guard\u5728\u68c0\u6d4b\u4e0d\u5b89\u5168\u8f93\u5165\u548c\u51cf\u5c11\u8fc7\u5ea6\u62d2\u7edd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LLMs\u5bf9\u201c\u6709\u5bb3\u6027\u201d\u7684\u7406\u89e3\u6bd4\u62d2\u7edd\u884c\u4e3a\u66f4\u7a33\u5065\uff0c\u4e3aAI\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.11999", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11999", "abs": "https://arxiv.org/abs/2507.11999", "authors": ["Xiaolin Wen", "Qishuang Fu", "Shuangyue Han", "Yichen Guo", "Joseph K. Liu", "Yong Wang"], "title": "Envisage: Towards Expressive Visual Graph Querying", "comment": null, "summary": "Graph querying is the process of retrieving information from graph data using\nspecialized languages (e.g., Cypher), often requiring programming expertise.\nVisual Graph Querying (VGQ) streamlines this process by enabling users to\nconstruct and execute queries via an interactive interface without resorting to\ncomplex coding. However, current VGQ tools only allow users to construct simple\nand specific query graphs, limiting users' ability to interactively express\ntheir query intent, especially for underspecified query intent. To address\nthese limitations, we propose Envisage, an interactive visual graph querying\nsystem to enhance the expressiveness of VGQ in complex query scenarios by\nsupporting intuitive graph structure construction and flexible parameterized\nrule specification. Specifically, Envisage comprises four stages: Query\nExpression allows users to interactively construct graph queries through\nintuitive operations; Query Verification enables the validation of constructed\nqueries via rule verification and query instantiation; Progressive Query\nExecution can progressively execute queries to ensure meaningful querying\nresults; and Result Analysis facilitates result exploration and interpretation.\nTo evaluate Envisage, we conducted two case studies and in-depth user\ninterviews with 14 graph analysts. The results demonstrate its effectiveness\nand usability in constructing, verifying, and executing complex graph queries.", "AI": {"tldr": "Envisage\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u89c6\u89c9\u56fe\u67e5\u8be2\u7cfb\u7edf\uff0c\u901a\u8fc7\u652f\u6301\u76f4\u89c2\u7684\u56fe\u7ed3\u6784\u6784\u5efa\u548c\u7075\u6d3b\u7684\u89c4\u5219\u89c4\u8303\uff0c\u63d0\u5347\u590d\u6742\u67e5\u8be2\u573a\u666f\u4e0b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u56fe\u67e5\u8be2\u5de5\u5177\u4ec5\u652f\u6301\u7b80\u5355\u67e5\u8be2\uff0c\u9650\u5236\u4e86\u7528\u6237\u8868\u8fbe\u590d\u6742\u67e5\u8be2\u610f\u56fe\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u6a21\u7cca\u67e5\u8be2\u610f\u56fe\u3002", "method": "Envisage\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a\u67e5\u8be2\u8868\u8fbe\u3001\u67e5\u8be2\u9a8c\u8bc1\u3001\u6e10\u8fdb\u67e5\u8be2\u6267\u884c\u548c\u7ed3\u679c\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u7528\u6237\u8bbf\u8c08\uff0c\u8bc1\u660eEnvisage\u5728\u6784\u5efa\u3001\u9a8c\u8bc1\u548c\u6267\u884c\u590d\u6742\u56fe\u67e5\u8be2\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u53ef\u7528\u6027\u3002", "conclusion": "Envisage\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u56fe\u67e5\u8be2\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u67e5\u8be2\u573a\u666f\u3002"}}
{"id": "2507.12186", "categories": ["cs.AI", "I.2.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2507.12186", "abs": "https://arxiv.org/abs/2507.12186", "authors": ["Edward Kim", "Hanna Kurniawati"], "title": "Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation", "comment": "8 pages, 2 tables, 3 figures. To be presented at International Joint\n  Conference on Artificial Intelligence 2025", "summary": "This paper proposes Partially Observable Reference Policy Programming, a\nnovel anytime online approximate POMDP solver which samples meaningful future\nhistories very deeply while simultaneously forcing a gradual policy update. We\nprovide theoretical guarantees for the algorithm's underlying scheme which say\nthat the performance loss is bounded by the average of the sampling\napproximation errors rather than the usual maximum, a crucial requirement given\nthe sampling sparsity of online planning. Empirical evaluations on two\nlarge-scale problems with dynamically evolving environments -- including a\nhelicopter emergency scenario in the Corsica region requiring approximately 150\nplanning steps -- corroborate the theoretical results and indicate that our\nsolver considerably outperforms current online benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u8fd1\u4f3cPOMDP\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u6df1\u5ea6\u91c7\u6837\u672a\u6765\u5386\u53f2\u5e76\u9010\u6b65\u66f4\u65b0\u7b56\u7565\uff0c\u6027\u80fd\u635f\u5931\u7531\u91c7\u6837\u8bef\u5dee\u7684\u5e73\u5747\u503c\u800c\u975e\u6700\u5927\u503c\u51b3\u5b9a\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u89c4\u5212\u4e2d\u91c7\u6837\u7a00\u758f\u6027\u95ee\u9898\uff0c\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u90e8\u5206\u53ef\u89c2\u6d4b\u53c2\u8003\u7b56\u7565\u7f16\u7a0b\uff08PORPP\uff09\uff0c\u7ed3\u5408\u6df1\u5ea6\u91c7\u6837\u548c\u9010\u6b65\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u6027\u80fd\u635f\u5931\u53d7\u91c7\u6837\u8bef\u5dee\u5e73\u5747\u503c\u9650\u5236\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5728\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PORPP\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2507.11688", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2507.11688", "abs": "https://arxiv.org/abs/2507.11688", "authors": ["Travis Pence", "Daisuke Yamada", "Vikas Singh"], "title": "Composing Linear Layers from Irreducibles", "comment": "27 Pages, 13 Tables, 8 Figures", "summary": "Contemporary large models often exhibit behaviors suggesting the presence of\nlow-level primitives that compose into modules with richer functionality, but\nthese fundamental building blocks remain poorly understood. We investigate this\ncompositional structure in linear layers by asking: can we identify/synthesize\nlinear transformations from a minimal set of geometric primitives? Using\nClifford algebra, we show that linear layers can be expressed as compositions\nof bivectors -- geometric objects encoding oriented planes -- and introduce a\ndifferentiable algorithm that decomposes them into products of rotors. This\nconstruction uses only O(log^2 d) parameters, versus O(d^2) required by dense\nmatrices. Applied to the key, query, and value projections in LLM attention\nlayers, our rotor-based layers match the performance of strong baselines such\nas block-Hadamard and low-rank approximations. Our findings provide an\nalgebraic perspective on how these geometric primitives can compose into\nhigher-level functions within deep models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7ebf\u6027\u5c42\u4e2d\u7684\u51e0\u4f55\u57fa\u5143\uff08\u5982\u53cc\u5411\u91cf\uff09\u5982\u4f55\u7ec4\u5408\u6210\u66f4\u9ad8\u5c42\u6b21\u7684\u529f\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eClifford\u4ee3\u6570\u7684\u53ef\u5fae\u5206\u7b97\u6cd5\uff0c\u7528O(log\u00b2d)\u53c2\u6570\u5b9e\u73b0\u4e0e\u5bc6\u96c6\u77e9\u9635\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u6a21\u578b\u4e2d\u4f4e\u5c42\u6b21\u57fa\u5143\u5982\u4f55\u7ec4\u5408\u6210\u529f\u80fd\u66f4\u4e30\u5bcc\u7684\u6a21\u5757\uff0c\u5c24\u5176\u662f\u7ebf\u6027\u5c42\u4e2d\u7684\u51e0\u4f55\u57fa\u5143\u3002", "method": "\u4f7f\u7528Clifford\u4ee3\u6570\uff0c\u5c06\u7ebf\u6027\u5c42\u8868\u793a\u4e3a\u53cc\u5411\u91cf\u7684\u7ec4\u5408\uff0c\u5e76\u8bbe\u8ba1\u53ef\u5fae\u5206\u7b97\u6cd5\u5c06\u5176\u5206\u89e3\u4e3a\u8f6c\u5b50\u4e58\u79ef\u3002", "result": "\u63d0\u51fa\u7684\u8f6c\u5b50\u57fa\u7ebf\u6027\u5c42\u5728LLM\u6ce8\u610f\u529b\u5c42\u4e2d\u8868\u73b0\u4e0e\u5757Hadamard\u548c\u4f4e\u79e9\u8fd1\u4f3c\u7b49\u57fa\u7ebf\u76f8\u5f53\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u51e0\u4f55\u57fa\u5143\u5728\u6df1\u5ea6\u6a21\u578b\u4e2d\u7684\u7ec4\u5408\u63d0\u4f9b\u4e86\u4ee3\u6570\u89c6\u89d2\u3002"}}
{"id": "2507.11882", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11882", "abs": "https://arxiv.org/abs/2507.11882", "authors": ["Bo Zeng", "Chenyang Lyu", "Sinuo Liu", "Mingyan Zeng", "Minghao Wu", "Xuanfan Ni", "Tianqi Shi", "Yu Zhao", "Yefeng Liu", "Chenyu Zhu", "Ruizhe Li", "Jiahui Geng", "Qing Li", "Yu Tong", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models", "comment": "ACL 2025 Main Conference paper", "summary": "Instruction-following capability has become a major ability to be evaluated\nfor Large Language Models (LLMs). However, existing datasets, such as IFEval,\nare either predominantly monolingual and centered on English or simply machine\ntranslated to other languages, limiting their applicability in multilingual\ncontexts. In this paper, we present an carefully-curated extension of IFEval to\na localized multilingual version named Marco-Bench-MIF, covering 30 languages\nwith varying levels of localization. Our benchmark addresses linguistic\nconstraints (e.g., modifying capitalization requirements for Chinese) and\ncultural references (e.g., substituting region-specific company names in\nprompts) via a hybrid pipeline combining translation with verification. Through\ncomprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)\n25-35% accuracy gap between high/low-resource languages, (2) model scales\nlargely impact performance by 45-60% yet persists script-specific challenges,\nand (3) machine-translated data underestimates accuracy by7-22% versus\nlocalized data. Our analysis identifies challenges in multilingual instruction\nfollowing, including keyword consistency preservation and compositional\nconstraint adherence across languages. Our Marco-Bench-MIF is available at\nhttps://github.com/AIDC-AI/Marco-Bench-MIF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u7248\u672c\u7684\u6307\u4ee4\u8ddf\u968f\u8bc4\u6d4b\u57fa\u51c6Marco-Bench-MIF\uff0c\u8986\u76d630\u79cd\u8bed\u8a00\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u8bed\u8a00\u548c\u6587\u5316\u9002\u5e94\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6307\u4ee4\u8ddf\u968f\u8bc4\u6d4b\u6570\u636e\u96c6\u591a\u4e3a\u82f1\u8bed\u6216\u673a\u5668\u7ffb\u8bd1\uff0c\u9650\u5236\u4e86\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u7ffb\u8bd1\u4e0e\u9a8c\u8bc1\u7684\u6df7\u5408\u6d41\u7a0b\uff0c\u521b\u5efa\u4e86\u672c\u5730\u5316\u7684\u591a\u8bed\u8a00\u8bc4\u6d4b\u57fa\u51c6Marco-Bench-MIF\u3002", "result": "\u8bc4\u6d4b\u53d1\u73b0\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u95f4\u5b58\u572825-35%\u7684\u51c6\u786e\u7387\u5dee\u8ddd\uff0c\u6a21\u578b\u89c4\u6a21\u5f71\u54cd\u6027\u80fd45-60%\uff0c\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u4f4e\u4f30\u51c6\u786e\u73877-22%\u3002", "conclusion": "Marco-Bench-MIF\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u7684\u6311\u6218\uff0c\u5982\u5173\u952e\u8bcd\u4e00\u81f4\u6027\u548c\u8de8\u8bed\u8a00\u7ea6\u675f\u9075\u5faa\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u57fa\u51c6\u3002"}}
{"id": "2507.12204", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12204", "abs": "https://arxiv.org/abs/2507.12204", "authors": ["Pengyu Zhu", "Janghee Cho"], "title": "Tao-Technology for Teen Mobile Use: Harmonizing Adaptation, Autonomy, and Reflection", "comment": null, "summary": "Adolescents' mobile technology use is often regulated through rigid control\nmechanisms that fail to account for their autonomy and natural usage patterns.\nDrawing on Taoist philosophy, particularly Wu Wei, Yin-Yang, and Zi Ran, this\nposition paper proposes Tao-Technology, a self-organizing, adaptive regulatory\nframework. Integrating insights from Reflective Informatics and Information\nEcologies, we explore how mobile technology can dynamically adjust to context\nwhile fostering self-reflection and meaning-making. This approach shifts from\nexternal restrictions to dynamic co-adaptative regulation, ensuring technology\ngovernance remains flexible yet structured, supporting adolescents in\ncultivating a balanced and intentional relationship with digital technology.", "AI": {"tldr": "\u63d0\u51faTao-Technology\u6846\u67b6\uff0c\u57fa\u4e8e\u9053\u5bb6\u54f2\u5b66\uff0c\u52a8\u6001\u8c03\u8282\u9752\u5c11\u5e74\u79fb\u52a8\u6280\u672f\u4f7f\u7528\uff0c\u53d6\u4ee3\u50f5\u5316\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u9752\u5c11\u5e74\u79fb\u52a8\u6280\u672f\u76d1\u7ba1\u673a\u5236\u8fc7\u4e8e\u50f5\u5316\uff0c\u5ffd\u89c6\u81ea\u4e3b\u6027\u548c\u81ea\u7136\u4f7f\u7528\u6a21\u5f0f\u3002", "method": "\u7ed3\u5408\u9053\u5bb6\u54f2\u5b66\uff08\u65e0\u4e3a\u3001\u9634\u9633\u3001\u81ea\u7136\uff09\u548c\u53cd\u601d\u4fe1\u606f\u5b66\u3001\u4fe1\u606f\u751f\u6001\u5b66\uff0c\u63d0\u51fa\u52a8\u6001\u81ea\u9002\u5e94\u6846\u67b6\u3002", "result": "Tao-Technology\u6846\u67b6\u652f\u6301\u52a8\u6001\u8c03\u6574\uff0c\u4fc3\u8fdb\u81ea\u6211\u53cd\u601d\u548c\u610f\u4e49\u6784\u5efa\u3002", "conclusion": "\u4ece\u5916\u90e8\u9650\u5236\u8f6c\u5411\u52a8\u6001\u534f\u540c\u8c03\u8282\uff0c\u5e2e\u52a9\u9752\u5c11\u5e74\u4e0e\u6280\u672f\u5efa\u7acb\u5e73\u8861\u5173\u7cfb\u3002"}}
{"id": "2507.12207", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.12207", "abs": "https://arxiv.org/abs/2507.12207", "authors": ["Subin Lin", "Chuanbo Hua"], "title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution", "comment": "ICML 2025 CO-Build Workshop Poster", "summary": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems.", "AI": {"tldr": "BuildEvo\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bbe\u8ba1\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u5efa\u7b51\u80fd\u8017\u9884\u6d4b\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7ed3\u5408\u7269\u7406\u539f\u7406\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800c\u9ad8\u7ea7\u6a21\u578b\u7f3a\u4e4f\u900f\u660e\u6027\u4e14\u5ffd\u89c6\u7269\u7406\u539f\u7406\uff0c\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u901a\u8fc7\u8fdb\u5316\u8fc7\u7a0b\u5f15\u5bfcLLMs\u6784\u5efa\u548c\u4f18\u5316\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7ed3\u5408\u5efa\u7b51\u7279\u6027\u548c\u64cd\u4f5c\u6570\u636e\u7684\u7269\u7406\u6d1e\u5bdf\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u4e14\u9884\u6d4b\u903b\u8f91\u900f\u660e\u3002", "conclusion": "BuildEvo\u63a8\u52a8\u4e86\u81ea\u52a8\u5316\u8bbe\u8ba1\u7a33\u5065\u3001\u57fa\u4e8e\u7269\u7406\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4e3a\u590d\u6742\u80fd\u6e90\u7cfb\u7edf\u63d0\u4f9b\u53ef\u4fe1\u6a21\u578b\u3002"}}
{"id": "2507.11690", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11690", "abs": "https://arxiv.org/abs/2507.11690", "authors": ["Amaya Dharmasiri", "William Yang", "Polina Kirichenko", "Lydia Liu", "Olga Russakovsky"], "title": "The Impact of Coreset Selection on Spurious Correlations and Group Robustness", "comment": "10 pages, 9 additional pages for Appendix", "summary": "Coreset selection methods have shown promise in reducing the training data\nsize while maintaining model performance for data-efficient machine learning.\nHowever, as many datasets suffer from biases that cause models to learn\nspurious correlations instead of causal features, it is important to understand\nwhether and how dataset reduction methods may perpetuate, amplify, or mitigate\nthese biases. In this work, we conduct the first comprehensive analysis of the\nimplications of data selection on the spurious bias levels of the selected\ncoresets and the robustness of downstream models trained on them. We use an\nextensive experimental setting spanning ten different spurious correlations\nbenchmarks, five score metrics to characterize sample importance/ difficulty,\nand five data selection policies across a broad range of coreset sizes.\nThereby, we unravel a series of nontrivial nuances in interactions between\nsample difficulty and bias alignment, as well as dataset bias and resultant\nmodel robustness. For example, we find that selecting coresets using\nembedding-based sample characterization scores runs a comparatively lower risk\nof inadvertently exacerbating bias than selecting using characterizations based\non learning dynamics. Most importantly, our analysis reveals that although some\ncoreset selection methods could achieve lower bias levels by prioritizing\ndifficult samples, they do not reliably guarantee downstream robustness.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u5bf9\u6838\u5fc3\u96c6\u504f\u5dee\u6c34\u5e73\u53ca\u4e0b\u6e38\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5d4c\u5165\u8868\u5f81\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u5b66\u4e60\u52a8\u6001\u7684\u65b9\u6cd5\u66f4\u80fd\u51cf\u5c11\u504f\u5dee\uff0c\u4f46\u96be\u6837\u672c\u4f18\u5148\u7b56\u7565\u672a\u5fc5\u80fd\u4fdd\u8bc1\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u662f\u5426\u53ca\u5982\u4f55\u5f71\u54cd\u6838\u5fc3\u96c6\u7684\u504f\u5dee\u6c34\u5e73\uff0c\u4ee5\u53ca\u4e0b\u6e38\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5728\u5341\u4e2a\u4e0d\u540c\u7684\u865a\u5047\u76f8\u5173\u6027\u57fa\u51c6\u4e0a\uff0c\u7ed3\u5408\u4e94\u79cd\u6837\u672c\u91cd\u8981\u6027/\u96be\u5ea6\u8bc4\u5206\u548c\u4e94\u79cd\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u5d4c\u5165\u8868\u5f81\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u5b66\u4e60\u52a8\u6001\u7684\u65b9\u6cd5\u66f4\u4e0d\u6613\u52a0\u5267\u504f\u5dee\uff0c\u4f46\u96be\u6837\u672c\u4f18\u5148\u7b56\u7565\u672a\u5fc5\u80fd\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u9700\u8c28\u614e\u8bbe\u8ba1\uff0c\u4ee5\u907f\u514d\u504f\u5dee\u653e\u5927\uff0c\u4e14\u96be\u6837\u672c\u4f18\u5148\u7b56\u7565\u9700\u7ed3\u5408\u5176\u4ed6\u56e0\u7d20\u4ee5\u786e\u4fdd\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.11936", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11936", "abs": "https://arxiv.org/abs/2507.11936", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "title": "A Survey of Deep Learning for Geometry Problem Solving", "comment": "Work in progress", "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u4efb\u52a1\u603b\u7ed3\u3001\u65b9\u6cd5\u56de\u987e\u3001\u8bc4\u4f30\u6307\u6807\u5206\u6790\u53ca\u672a\u6765\u6311\u6218\u8ba8\u8bba\u3002", "motivation": "\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u662f\u6570\u5b66\u63a8\u7406\u7684\u5173\u952e\u9886\u57df\uff0c\u6d89\u53ca\u6559\u80b2\u548c\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\u7b49\u591a\u4e2a\u91cd\u8981\u9886\u57df\uff0c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u76f8\u5173\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u7684\u76f8\u5173\u4efb\u52a1\u3001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3001\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\uff0c\u4ee5\u53ca\u8ba8\u8bba\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "\u63d0\u4f9b\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u5168\u9762\u53c2\u8003\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "conclusion": "\u672c\u6587\u4e3a\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u7684\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.12212", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12212", "abs": "https://arxiv.org/abs/2507.12212", "authors": ["Garyoung Kim", "Huisung Kwon", "Seoju Yun", "Yu-Won Youn"], "title": "Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness", "comment": "7 pages, 3 figures", "summary": "Generative AI does not only replicate human creativity but also reproduces\ndeep-seated cultural biases, making it crucial to critically examine how\nconcepts like ugliness are understood and expressed by these tools. This study\ninvestigates how four different generative AI models understand and express\nugliness through text and image and explores the biases embedded within these\nrepresentations. We extracted 13 adjectives associated with ugliness through\niterative prompting of a large language model and generated 624 images across\nfour AI models and three prompts. Demographic and socioeconomic attributes\nwithin the images were independently coded and thematically analyzed. Our\nfindings show that AI models disproportionately associate ugliness with old\nwhite male figures, reflecting entrenched social biases as well as paradoxical\nbiases, where efforts to avoid stereotypical depictions of marginalized groups\ninadvertently result in the disproportionate projection of negative attributes\nonto majority groups. Qualitative analysis further reveals that, despite\nsupposed attempts to frame ugliness within social contexts, conventional\nphysical markers such as asymmetry and aging persist as central visual motifs.\nThese findings demonstrate that despite attempts to create more equal\nrepresentations, generative AI continues to perpetuate inherited and\nparadoxical biases, underscoring the critical work being done to create ethical\nAI training paradigms and advance methodologies for more inclusive AI\ndevelopment.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5982\u4f55\u7406\u89e3\u548c\u8868\u8fbe\u201c\u4e11\u964b\u201d\uff0c\u53d1\u73b0\u5176\u518d\u73b0\u4e86\u6587\u5316\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5c06\u4e11\u964b\u4e0e\u8001\u5e74\u767d\u4eba\u7537\u6027\u76f8\u5173\u8054\u3002", "motivation": "\u6279\u5224\u6027\u5206\u6790\u751f\u6210\u5f0fAI\u5982\u4f55\u7406\u89e3\u548c\u8868\u8fbe\u201c\u4e11\u964b\u201d\uff0c\u63ed\u793a\u5176\u5185\u5728\u7684\u6587\u5316\u504f\u89c1\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u63d0\u53d613\u4e2a\u5f62\u5bb9\u8bcd\uff0c\u751f\u6210624\u5f20\u56fe\u50cf\uff0c\u5e76\u8fdb\u884c\u7f16\u7801\u548c\u4e3b\u9898\u5206\u6790\u3002", "result": "AI\u6a21\u578b\u5c06\u4e11\u964b\u4e0e\u8001\u5e74\u767d\u4eba\u7537\u6027\u5173\u8054\uff0c\u5e76\u8868\u73b0\u51fa\u77db\u76fe\u504f\u89c1\uff0c\u907f\u514d\u8fb9\u7f18\u5316\u7fa4\u4f53\u5374\u8d1f\u9762\u6295\u5c04\u591a\u6570\u7fa4\u4f53\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u4ecd\u5ef6\u7eed\u504f\u89c1\uff0c\u9700\u6539\u8fdb\u4f26\u7406\u8bad\u7ec3\u8303\u5f0f\u4ee5\u63a8\u52a8\u5305\u5bb9\u6027\u53d1\u5c55\u3002"}}
{"id": "2507.12215", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12215", "abs": "https://arxiv.org/abs/2507.12215", "authors": ["Yuhao Chen", "Shuochen Liu", "Yuanjie Lyu", "Chao Zhang", "Jiayao Shi", "Tong Xu"], "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning", "comment": "10 pages, 7 figures", "summary": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e2d\u56fd\u8c61\u68cb\uff08Xiangqi\uff09\u7684LLM\u8bad\u7ec3\u6846\u67b6Xiangqi-R1\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7a7a\u95f4\u6218\u7565\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7a7a\u95f4\u6218\u7565\u63a8\u7406\u4e2d\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u590d\u6742\u4e14\u5b8c\u5168\u53ef\u89c2\u5bdf\u7684\u68cb\u76d8\u6e38\u620f\uff08\u5982\u4e2d\u56fd\u8c61\u68cb\uff09\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u5fae\u8c03\u5408\u6cd5\u79fb\u52a8\u9884\u6d4b\uff1b2\uff09\u5f15\u5165\u6218\u7565\u6ce8\u91ca\uff1b3\uff09\u901a\u8fc7GRPO\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u63a8\u7406\u7a33\u5b9a\u6027\u3002", "result": "Xiangqi-R1\u5728\u79fb\u52a8\u5408\u6cd5\u6027\u548c\u5206\u6790\u51c6\u786e\u6027\u4e0a\u5206\u522b\u63d0\u5347\u4e8618%\u548c22%\uff0c\u4f18\u4e8e\u901a\u7528LLMs\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u7a7a\u95f4\u590d\u6742\u9886\u57df\u6784\u5efa\u901a\u7528\u6218\u7565\u667a\u80fd\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.11702", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11702", "abs": "https://arxiv.org/abs/2507.11702", "authors": ["Hein de Wilde", "Ali Mohammed Mansoor Alsahag", "Pierre Blanchet"], "title": "Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption", "comment": null, "summary": "Railroad traffic disruption as a result of leaf-fall cost the UK rail\nindustry over 300 million per year and measures to mitigate such disruptions\nare employed on a large scale, with 1.67 million kilometers of track being\ntreated in the UK in 2021 alone. Therefore, the ability to anticipate the\ntiming of leaf-fall would offer substantial benefits for rail network\noperators, enabling the efficient scheduling of such mitigation measures.\nHowever, current methodologies for predicting leaf-fall exhibit considerable\nlimitations in terms of scalability and reliability. This study endeavors to\ndevise a prediction system that leverages specialized prediction methods and\nthe latest satellite data sources to generate both scalable and reliable\ninsights into leaf-fall timings. An LSTM network trained on ground-truth\nleaf-falling data combined with multispectral and meteorological satellite data\ndemonstrated a root-mean-square error of 6.32 days for predicting the start of\nleaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which\nimproves upon previous work on the topic, offers promising opportunities for\nthe optimization of leaf mitigation measures in the railway industry and the\nimprovement of our understanding of complex ecological systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eLSTM\u7f51\u7edc\u548c\u536b\u661f\u6570\u636e\u7684\u53f6\u843d\u9884\u6d4b\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u94c1\u8def\u884c\u4e1a\u7684\u53f6\u843d\u7f13\u89e3\u63aa\u65bd\u3002", "motivation": "\u82f1\u56fd\u94c1\u8def\u56e0\u53f6\u843d\u6bcf\u5e74\u635f\u5931\u8d853\u4ebf\u82f1\u9551\uff0c\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u7ed3\u5408\u5730\u9762\u771f\u5b9e\u53f6\u843d\u6570\u636e\u3001\u591a\u5149\u8c31\u548c\u6c14\u8c61\u536b\u661f\u6570\u636e\uff0c\u8bad\u7ec3LSTM\u7f51\u7edc\u9884\u6d4b\u53f6\u843d\u65f6\u95f4\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u53f6\u843d\u5f00\u59cb\u548c\u7ed3\u675f\u7684\u5747\u65b9\u6839\u8bef\u5dee\u5206\u522b\u4e3a6.32\u5929\u548c9.31\u5929\uff0c\u4f18\u4e8e\u5148\u524d\u7814\u7a76\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u94c1\u8def\u884c\u4e1a\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u53f6\u843d\u9884\u6d4b\u5de5\u5177\uff0c\u5e76\u6709\u52a9\u4e8e\u7406\u89e3\u590d\u6742\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2507.11939", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.11939", "abs": "https://arxiv.org/abs/2507.11939", "authors": ["Yichen Xu", "Liangyu Chen", "Liang Zhang", "Wenxuan Wang", "Qin Jin"], "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering", "comment": "Work in Progress", "summary": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.", "AI": {"tldr": "PolyChartQA\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u56fe\u8868\u95ee\u7b54\u57fa\u51c6\uff0c\u8986\u76d610\u79cd\u8bed\u8a00\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u56fe\u8868\u7406\u89e3\u57fa\u51c6\u7684\u82f1\u8bed\u4e2d\u5fc3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u8868\u7406\u89e3\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\uff0c\u9650\u5236\u4e86\u5168\u7403\u7528\u6237\u7684\u8bbf\u95ee\u548c\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u89e3\u8026\u7ba1\u9053\uff0c\u5206\u79bb\u56fe\u8868\u6570\u636e\u548c\u6e32\u67d3\u4ee3\u7801\uff0c\u5229\u7528LLM\u7ffb\u8bd1\u548c\u4e25\u683c\u8d28\u91cf\u63a7\u5236\u751f\u6210\u591a\u8bed\u8a00\u56fe\u8868\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u82f1\u8bed\u4e0e\u5176\u4ed6\u8bed\u8a00\uff08\u5c24\u5176\u662f\u975e\u62c9\u4e01\u6587\u5b57\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "PolyChartQA\u4e3a\u63a8\u8fdb\u5168\u7403\u5305\u5bb9\u6027\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.12296", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12296", "abs": "https://arxiv.org/abs/2507.12296", "authors": ["Bevan Koopman", "Guido Zuccon"], "title": "Humans are more gullible than LLMs in believing common psychological myths", "comment": null, "summary": "Despite widespread debunking, many psychological myths remain deeply\nentrenched. This paper investigates whether Large Language Models (LLMs) mimic\nhuman behaviour of myth belief and explores methods to mitigate such\ntendencies. Using 50 popular psychological myths, we evaluate myth belief\nacross multiple LLMs under different prompting strategies, including\nretrieval-augmented generation and swaying prompts. Results show that LLMs\nexhibit significantly lower myth belief rates than humans, though user\nprompting can influence responses. RAG proves effective in reducing myth belief\nand reveals latent debiasing potential within LLMs. Our findings contribute to\nthe emerging field of Machine Psychology and highlight how cognitive science\nmethods can inform the evaluation and development of LLM-based systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u6a21\u4eff\u4eba\u7c7b\u5bf9\u5fc3\u7406\u5b66\u795e\u8bdd\u7684\u4fe1\u5ff5\uff0c\u5e76\u63a2\u7d22\u51cf\u5c11\u8fd9\u79cd\u503e\u5411\u7684\u65b9\u6cd5\u3002\u7ed3\u679c\u663e\u793aLLMs\u7684\u795e\u8bdd\u4fe1\u5ff5\u7387\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u4f46\u7528\u6237\u63d0\u793a\u4f1a\u5f71\u54cd\u5176\u56de\u7b54\u3002", "motivation": "\u5c3d\u7ba1\u5fc3\u7406\u5b66\u795e\u8bdd\u5df2\u88ab\u5e7f\u6cdb\u8f9f\u8c23\uff0c\u4f46\u4ecd\u6839\u6df1\u8482\u56fa\u3002\u7814\u7a76\u65e8\u5728\u4e86\u89e3LLMs\u662f\u5426\u6a21\u4eff\u4eba\u7c7b\u7684\u8fd9\u79cd\u4fe1\u5ff5\uff0c\u5e76\u5bfb\u627e\u51cf\u5c11\u5176\u503e\u5411\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u752850\u4e2a\u6d41\u884c\u7684\u5fc3\u7406\u5b66\u795e\u8bdd\uff0c\u8bc4\u4f30\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4e0bLLMs\u7684\u795e\u8bdd\u4fe1\u5ff5\u7387\uff0c\u5305\u62ec\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u5f15\u5bfc\u63d0\u793a\u3002", "result": "LLMs\u7684\u795e\u8bdd\u4fe1\u5ff5\u7387\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff0cRAG\u80fd\u6709\u6548\u51cf\u5c11\u5176\u795e\u8bdd\u4fe1\u5ff5\uff0c\u5e76\u63ed\u793aLLMs\u6f5c\u5728\u7684\u7ea0\u504f\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u673a\u5668\u5fc3\u7406\u5b66\u9886\u57df\u63d0\u4f9b\u8d21\u732e\uff0c\u5e76\u5c55\u793a\u8ba4\u77e5\u79d1\u5b66\u65b9\u6cd5\u5982\u4f55\u6307\u5bfcLLM\u7cfb\u7edf\u7684\u8bc4\u4f30\u4e0e\u5f00\u53d1\u3002"}}
{"id": "2507.11706", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11706", "abs": "https://arxiv.org/abs/2507.11706", "authors": ["Taira Tsuchiya", "Shinji Ito", "Haipeng Luo"], "title": "Reinforcement Learning from Adversarial Preferences in Tabular MDPs", "comment": "40 pages", "summary": "We introduce a new framework of episodic tabular Markov decision processes\n(MDPs) with adversarial preferences, which we refer to as preference-based MDPs\n(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the\nnumerical value of the loss is directly observed, in PbMDPs the learner instead\nobserves preferences between two candidate arms, which represent the choices\nbeing compared. In this work, we focus specifically on the setting where the\nreward functions are determined by Borda scores. We begin by establishing a\nregret lower bound for PbMDPs with Borda scores. As a preliminary step, we\npresent a simple instance to prove a lower bound of $\\Omega(\\sqrt{HSAT})$ for\nepisodic MDPs with adversarial losses, where $H$ is the number of steps per\nepisode, $S$ is the number of states, $A$ is the number of actions, and $T$ is\nthe number of episodes. Leveraging this construction, we then derive a regret\nlower bound of $\\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda\nscores, where $K$ is the number of arms. Next, we develop algorithms that\nachieve a regret bound of order $T^{2/3}$. We first propose a global\noptimization approach based on online linear optimization over the set of all\noccupancy measures, achieving a regret bound of $\\tilde{O}((H^2 S^2 K)^{1/3}\nT^{2/3} )$ under known transitions. However, this approach suffers from\nsuboptimal dependence on the potentially large number of states $S$ and\ncomputational inefficiency. To address this, we propose a policy optimization\nalgorithm whose regret is roughly bounded by $\\tilde{O}( (H^6 S K^5)^{1/3}\nT^{2/3} )$ under known transitions, and further extend the result to the\nunknown-transition setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u597d\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08PbMDPs\uff09\u6846\u67b6\uff0c\u7814\u7a76\u4e86Borda\u5206\u6570\u4e0b\u7684\u9057\u61be\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\u5b9e\u73b0T^{2/3}\u7684\u9057\u61be\u754c\u3002", "motivation": "\u7814\u7a76\u5728\u504f\u597d\u800c\u975e\u6570\u503c\u635f\u5931\u4e0b\u5b66\u4e60\u7684MDP\u95ee\u9898\uff0c\u586b\u8865\u4e86\u6807\u51c6MDP\u4e0e\u504f\u597d\u5b66\u4e60\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "method": "1. \u5efa\u7acbPbMDPs\u7684\u9057\u61be\u4e0b\u754c\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u5168\u5c40\u4f18\u5316\u7684\u7b97\u6cd5\u548c\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86PbMDPs\u7684\u9057\u61be\u4e0b\u754c\u4e3a\u03a9((H^2SK)^{1/3}T^{2/3})\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\u5206\u522b\u5b9e\u73b0O~((H^2S^2K)^{1/3}T^{2/3})\u548cO~((H^6SK^5)^{1/3}T^{2/3})\u7684\u9057\u61be\u754c\u3002", "conclusion": "\u8bba\u6587\u4e3a\u504f\u597d\u5b66\u4e60\u7684MDP\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u7b97\u6cd5\u652f\u6301\uff0c\u672a\u6765\u53ef\u4f18\u5316\u7b97\u6cd5\u6548\u7387\u3002"}}
{"id": "2507.11941", "categories": ["cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11941", "abs": "https://arxiv.org/abs/2507.11941", "authors": ["Amos You"], "title": "BlockBPE: Parallel BPE Tokenization", "comment": "ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models\n  (ICML 2025)", "summary": "Tokenization is a critical preprocessing step in large language model\npipelines, yet widely-used implementations remain CPU-bound and suboptimal for\nbatch inference workflows on GPU. We present BlockBPE, a parallel GPU\nimplementation of byte-pair encoding (BPE) that achieves near linear-time\ncomplexity under realistic assumptions and is optimized for high-throughput,\nbatch inference. Unlike existing Rust-based tokenizers such as HuggingFace\nTokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex\npre-tokenization and exhibit $O(n \\log n)$ runtime-BlockBPE eliminates the\nRegex pre-tokenization which leads to small loss in generation quality, but\nenables highly parallelized token merges within thread blocks, reducing overall\ncomplexity to $O(nd)$ where $d \\ll n$. On high-batch inference workloads,\nBlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over\nHuggingFace Tokenizers.", "AI": {"tldr": "BlockBPE\u662f\u4e00\u79cd\u5e76\u884cGPU\u5b9e\u73b0\u7684BPE\u7b97\u6cd5\uff0c\u4f18\u5316\u4e86\u6279\u5904\u7406\u63a8\u7406\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709CPU\u7ed1\u5b9a\u7684BPE\u5b9e\u73b0\u5728\u5927\u89c4\u6a21\u6279\u5904\u7406\u63a8\u7406\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u4f18\u5316GPU\u6027\u80fd\u3002", "method": "BlockBPE\u901a\u8fc7\u6d88\u9664Regex\u9884\u5206\u8bcd\uff0c\u5b9e\u73b0\u9ad8\u5ea6\u5e76\u884c\u5316\u7684\u6807\u8bb0\u5408\u5e76\uff0c\u590d\u6742\u5ea6\u964d\u81f3O(nd)\u3002", "result": "\u5728\u9ad8\u6279\u5904\u7406\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cBlockBPE\u541e\u5410\u91cf\u6bd4tiktoken\u9ad82\u500d\uff0c\u6bd4HuggingFace Tokenizers\u9ad82.5\u500d\u3002", "conclusion": "BlockBPE\u663e\u8457\u63d0\u5347\u4e86BPE\u5728GPU\u4e0a\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u541e\u5410\u91cf\u573a\u666f\u3002"}}
{"id": "2507.12298", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12298", "abs": "https://arxiv.org/abs/2507.12298", "authors": ["Rui Sheng", "Xingbo Wang", "Jiachen Wang", "Xiaofu Jin", "Zhonghua Sheng", "Zhenxing Xu", "Suraj Rajendran", "Huamin Qu", "Fei Wang"], "title": "TrialCompass: Visual Analytics for Enhancing the Eligibility Criteria Design of Clinical Trials", "comment": null, "summary": "Eligibility criteria play a critical role in clinical trials by determining\nthe target patient population, which significantly influences the outcomes of\nmedical interventions. However, current approaches for designing eligibility\ncriteria have limitations to support interactive exploration of the large space\nof eligibility criteria. They also ignore incorporating detailed\ncharacteristics from the original electronic health record (EHR) data for\ncriteria refinement. To address these limitations, we proposed TrialCompass, a\nvisual analytics system integrating a novel workflow, which can empower\nclinicians to iteratively explore the vast space of eligibility criteria\nthrough knowledge-driven and outcome-driven approaches. TrialCompass supports\nhistory-tracking to help clinicians trace the evolution of their adjustments\nand decisions when exploring various forms of data (i.e., eligibility criteria,\noutcome metrics, and detailed characteristics of original EHR data) through\nthese two approaches. This feature can help clinicians comprehend the impact of\neligibility criteria on outcome metrics and patient characteristics, which\nfacilitates systematic refinement of eligibility criteria. Using a real-world\ndataset, we demonstrated the effectiveness of TrialCompass in providing\ninsights into designing eligibility criteria for septic shock and\nsepsis-associated acute kidney injury. We also discussed the research prospects\nof applying visual analytics to clinical trials.", "AI": {"tldr": "TrialCompass\u662f\u4e00\u4e2a\u53ef\u89c6\u5316\u5206\u6790\u7cfb\u7edf\uff0c\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u901a\u8fc7\u77e5\u8bc6\u9a71\u52a8\u548c\u7ed3\u679c\u9a71\u52a8\u7684\u65b9\u6cd5\u8fed\u4ee3\u63a2\u7d22\u4e34\u5e8a\u8bd5\u9a8c\u7684\u8d44\u683c\u6807\u51c6\uff0c\u5e76\u7ed3\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u8fdb\u884c\u7ec6\u5316\u3002", "motivation": "\u73b0\u6709\u8d44\u683c\u6807\u51c6\u8bbe\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u652f\u6301\u4ea4\u4e92\u5f0f\u63a2\u7d22\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u7684\u8be6\u7ec6\u7279\u5f81\u3002", "method": "\u63d0\u51faTrialCompass\u7cfb\u7edf\uff0c\u6574\u5408\u77e5\u8bc6\u9a71\u52a8\u548c\u7ed3\u679c\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u652f\u6301\u5386\u53f2\u8ffd\u8e2a\u4ee5\u4f18\u5316\u8d44\u683c\u6807\u51c6\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\uff0cTrialCompass\u5728\u8113\u6bd2\u75c7\u4f11\u514b\u548c\u6025\u6027\u80be\u635f\u4f24\u7684\u8d44\u683c\u6807\u51c6\u8bbe\u8ba1\u4e2d\u63d0\u4f9b\u4e86\u6709\u6548\u89c1\u89e3\u3002", "conclusion": "TrialCompass\u5c55\u793a\u4e86\u53ef\u89c6\u5316\u5206\u6790\u5728\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.11710", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11710", "abs": "https://arxiv.org/abs/2507.11710", "authors": ["Jay Revolinsky", "Harry Shomer", "Jiliang Tang"], "title": "Subgraph Generation for Generalizing on Out-of-Distribution Links", "comment": "18 pages, 7 figures, preprint", "summary": "Graphs Neural Networks (GNNs) demonstrate high-performance on the link\nprediction (LP) task. However, these models often rely on all dataset samples\nbeing drawn from the same distribution. In addition, graph generative models\n(GGMs) show a pronounced ability to generate novel output graphs. Despite this,\nGGM applications remain largely limited to domain-specific tasks. To bridge\nthis gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)\nstructurally-conditioned graph generation, and (2) adversarial co-training\nbetween an auto-encoder and GNN. As such, FLEX ensures structural-alignment\nbetween sample distributions to enhance link-prediction performance in\nout-of-distribution (OOD) scenarios. Notably, FLEX does not require expert\nknowledge to function in different OOD scenarios. Numerous experiments are\nconducted in synthetic and real-world OOD settings to demonstrate FLEX's\nperformance-enhancing ability, with further analysis for understanding the\neffects of graph data augmentation on link structures. The source code is\navailable here: https://github.com/revolins/FlexOOD.", "AI": {"tldr": "FLEX\u662f\u4e00\u4e2a\u56fe\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u6761\u4ef6\u751f\u6210\u548c\u5bf9\u6297\u6027\u534f\u540c\u8bad\u7ec3\uff0c\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u540c\u65f6\u6269\u5c55\u56fe\u751f\u6210\u6a21\u578b\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u7ed3\u5408\u7ed3\u6784\u6761\u4ef6\u751f\u6210\u548c\u5bf9\u6297\u6027\u534f\u540c\u8bad\u7ec3\uff08\u81ea\u52a8\u7f16\u7801\u5668\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\uff0c\u786e\u4fdd\u6837\u672c\u5206\u5e03\u7684\u7ed3\u6784\u5bf9\u9f50\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5206\u5e03\u5916\u573a\u666f\u4e2d\uff0cFLEX\u663e\u8457\u63d0\u5347\u4e86\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "FLEX\u65e0\u9700\u4e13\u5bb6\u77e5\u8bc6\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u5206\u5e03\u5916\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u56fe\u6570\u636e\u589e\u5f3a\u4f18\u5316\u94fe\u63a5\u7ed3\u6784\u3002"}}
{"id": "2507.11942", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11942", "abs": "https://arxiv.org/abs/2507.11942", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao", "Baoyuan Qi", "Guoming Liu"], "title": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression", "comment": "ACL 2025", "summary": "Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6ce8\u610f\u529b\u611f\u77e5\u7684\u4efb\u52a1\u65e0\u5173\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\uff08DAC\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u71b5\u548c\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u52a8\u6001\u611f\u77e5\u538b\u7f29\u8fc7\u7a0b\u4e2d\u7684\u71b5\u53d8\u5316\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u538b\u7f29\u3002\u5b9e\u9a8c\u8bc1\u660eDAC\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4fe1\u606f\u71b5\u538b\u7f29\u8bcd\u6c47\u5355\u5143\uff0c\u4f46\u5ffd\u7565\u4e86\u6ce8\u610f\u529b\u5173\u952e\u4ee4\u724c\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u538b\u7f29\u8fc7\u7a0b\u4e2d\u71b5\u7684\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6ce8\u610f\u529b\u611f\u77e5\u65b9\u6cd5\uff08DAC\uff09\uff0c\u6574\u5408\u71b5\u548c\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u52a8\u6001\u611f\u77e5\u71b5\u53d8\u5316\u3002", "result": "\u5728LongBench\u3001GSM8K\u548cBBH\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cDAC\u8868\u73b0\u7a33\u5065\u4e14\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DAC\u901a\u8fc7\u52a8\u6001\u6574\u5408\u71b5\u548c\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u63d0\u793a\u538b\u7f29\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u3002"}}
{"id": "2507.12334", "categories": ["cs.HC", "H.5.0"], "pdf": "https://arxiv.org/pdf/2507.12334", "abs": "https://arxiv.org/abs/2507.12334", "authors": ["Chase Stokes", "Anjana Arunkumar", "Marti A. Hearst", "Lace Padilla"], "title": "An Analysis of Text Functions in Information Visualization", "comment": "11 pages, 3 figures, IEEE VIS Conference", "summary": "Text is an integral but understudied component of visualization design.\nAlthough recent studies have examined how text elements (e.g., titles and\nannotations) influence comprehension, preferences, and predictions, many\nquestions remain about textual design and use in practice. This paper\nintroduces a framework for understanding text functions in information\nvisualizations, building on and filling gaps in prior classifications and\ntaxonomies. Through an analysis of 120 real-world visualizations and 804 text\nelements, we identified ten distinct text functions, ranging from identifying\ndata mappings to presenting valenced subtext. We further identify patterns in\ntext usage and conduct a factor analysis, revealing four overarching\ntext-informed design strategies: Attribution and Variables, Annotation-Centric\nDesign, Visual Embellishments, and Narrative Framing. In addition to these\nfactors, we explore features of title rhetoric and text multifunctionality,\nwhile also uncovering previously unexamined text functions, such as text\nreplacing visual elements. Our findings highlight the flexibility of text,\ndemonstrating how different text elements in a given design can combine to\ncommunicate, synthesize, and frame visual information. This framework adds\nimportant nuance and detail to existing frameworks that analyze the diverse\nroles of text in visualization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u89e3\u4fe1\u606f\u53ef\u89c6\u5316\u4e2d\u6587\u672c\u529f\u80fd\u7684\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5206\u7c7b\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u5206\u6790120\u4e2a\u5b9e\u9645\u53ef\u89c6\u5316\u6848\u4f8b\u548c804\u4e2a\u6587\u672c\u5143\u7d20\uff0c\u8bc6\u522b\u4e86\u5341\u79cd\u6587\u672c\u529f\u80fd\u3002", "motivation": "\u7814\u7a76\u6587\u672c\u5728\u53ef\u89c6\u5316\u8bbe\u8ba1\u4e2d\u7684\u4f5c\u7528\uff0c\u586b\u8865\u73b0\u6709\u5206\u7c7b\u7684\u4e0d\u8db3\u3002", "method": "\u5206\u6790120\u4e2a\u5b9e\u9645\u53ef\u89c6\u5316\u6848\u4f8b\u548c804\u4e2a\u6587\u672c\u5143\u7d20\uff0c\u8bc6\u522b\u6587\u672c\u529f\u80fd\u5e76\u8fdb\u884c\u56e0\u5b50\u5206\u6790\u3002", "result": "\u53d1\u73b0\u5341\u79cd\u6587\u672c\u529f\u80fd\u548c\u56db\u79cd\u6587\u672c\u9a71\u52a8\u7684\u8bbe\u8ba1\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u6587\u672c\u7684\u591a\u529f\u80fd\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e30\u5bcc\u4e86\u73b0\u6709\u6587\u672c\u5728\u53ef\u89c6\u5316\u4e2d\u4f5c\u7528\u7684\u7406\u89e3\uff0c\u5f3a\u8c03\u4e86\u6587\u672c\u7684\u591a\u6837\u6027\u548c\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2507.11729", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11729", "abs": "https://arxiv.org/abs/2507.11729", "authors": ["Amirhossein Ahmadi", "Hamidreza Zareipour", "Henry Leung"], "title": "Globalization for Scalable Short-term Load Forecasting", "comment": "63 pages with 22 figures", "summary": "Forecasting load in power transmission networks is essential across various\nhierarchical levels, from the system level down to individual points of\ndelivery (PoD). While intuitive and locally accurate, traditional local\nforecasting models (LFMs) face significant limitations, particularly in\nhandling generalizability, overfitting, data drift, and the cold start problem.\nThese methods also struggle with scalability, becoming computationally\nexpensive and less efficient as the network's size and data volume grow. In\ncontrast, global forecasting models (GFMs) offer a new approach to enhance\nprediction generalizability, scalability, accuracy, and robustness through\nglobalization and cross-learning. This paper investigates global load\nforecasting in the presence of data drifts, highlighting the impact of\ndifferent modeling techniques and data heterogeneity. We explore\nfeature-transforming and target-transforming models, demonstrating how\nglobalization, data heterogeneity, and data drift affect each differently. In\naddition, we examine the role of globalization in peak load forecasting and its\npotential for hierarchical forecasting. To address data heterogeneity and the\nbalance between globality and locality, we propose separate time series\nclustering (TSC) methods, introducing model-based TSC for feature-transforming\nmodels and new weighted instance-based TSC for target-transforming models.\nThrough extensive experiments on a real-world dataset of Alberta's electricity\nload, we demonstrate that global target-transforming models consistently\noutperform their local counterparts, especially when enriched with global\nfeatures and clustering techniques. In contrast, global feature-transforming\nmodels face challenges in balancing local and global dynamics, often requiring\nTSC to manage data heterogeneity effectively.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7535\u529b\u4f20\u8f93\u7f51\u7edc\u4e2d\u7684\u5168\u5c40\u8d1f\u8f7d\u9884\u6d4b\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u6570\u636e\u6f02\u79fb\u3001\u5efa\u6a21\u6280\u672f\u548c\u6570\u636e\u5f02\u8d28\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5168\u5c40\u76ee\u6807\u8f6c\u6362\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u5c40\u90e8\u9884\u6d4b\u6a21\u578b\u5728\u901a\u7528\u6027\u3001\u8fc7\u62df\u5408\u3001\u6570\u636e\u6f02\u79fb\u548c\u51b7\u542f\u52a8\u95ee\u9898\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u5168\u5c40\u9884\u6d4b\u6a21\u578b\u901a\u8fc7\u5168\u5c40\u5316\u548c\u4ea4\u53c9\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u7814\u7a76\u4e86\u7279\u5f81\u8f6c\u6362\u548c\u76ee\u6807\u8f6c\u6362\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u7684\u65b9\u6cd5\uff08\u6a21\u578b\u57fa\u4e8e\u7279\u5f81\u8f6c\u6362\u548c\u52a0\u6743\u5b9e\u4f8b\u57fa\u4e8e\u76ee\u6807\u8f6c\u6362\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5168\u7403\u5316\u5bf9\u5cf0\u503c\u8d1f\u8f7d\u9884\u6d4b\u548c\u5206\u5c42\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "result": "\u5168\u5c40\u76ee\u6807\u8f6c\u6362\u6a21\u578b\u5728\u5168\u5c40\u7279\u5f81\u548c\u805a\u7c7b\u6280\u672f\u7684\u652f\u6301\u4e0b\u8868\u73b0\u4f18\u4e8e\u5c40\u90e8\u6a21\u578b\uff0c\u800c\u5168\u5c40\u7279\u5f81\u8f6c\u6362\u6a21\u578b\u9700\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u5e73\u8861\u5c40\u90e8\u548c\u5168\u5c40\u52a8\u6001\u3002", "conclusion": "\u5168\u5c40\u76ee\u6807\u8f6c\u6362\u6a21\u578b\u5728\u8d1f\u8f7d\u9884\u6d4b\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u800c\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u662f\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u7684\u6709\u6548\u624b\u6bb5\u3002"}}
{"id": "2507.11953", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11953", "abs": "https://arxiv.org/abs/2507.11953", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao"], "title": "IAM: Efficient Inference through Attention Mapping between Different-scale LLMs", "comment": "ACL 2025", "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.", "AI": {"tldr": "IAM\u6846\u67b6\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u89c4\u6a21LLM\u95f4\u6ce8\u610f\u529b\u77e9\u9635\u7684\u9ad8\u76f8\u4f3c\u6027\uff0c\u4f18\u5316\u6ce8\u610f\u529b\u8ba1\u7b97\u548cKV\u7f13\u5b58\u4f7f\u7528\uff0c\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u5f53\u524dLLM\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5229\u7528\u6a21\u578b\u5185\u90e8\u7a00\u758f\u6027\uff0c\u672a\u5229\u7528\u5916\u90e8\u4fe1\u606f\u4f18\u5316\u3002", "method": "\u63d0\u51faIAM\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u77e9\u9635\u76f8\u4f3c\u6027\u3001\u9009\u62e9\u6620\u5c04\u5c42\u53ca\u9a8c\u8bc1\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u5c0f\u89c4\u6a21\u4e0e\u5927\u89c4\u6a21LLM\u95f4\u7684\u6ce8\u610f\u529b\u6620\u5c04\u3002", "result": "IAM\u52a0\u901f\u9884\u586b\u514515%\uff0c\u51cf\u5c11KV\u7f13\u5b58\u4f7f\u752822.1%\uff0c\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u901a\u7528\u6027\u3002", "conclusion": "IAM\u662f\u4e00\u79cd\u901a\u7528\u4e14\u4e0e\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u6b63\u4ea4\u7684LLM\u6548\u7387\u63d0\u5347\u5de5\u5177\u3002"}}
{"id": "2507.12337", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12337", "abs": "https://arxiv.org/abs/2507.12337", "authors": ["Xiao Pang", "Yan Huang", "Chang Liu", "JiYuan Liu", "MingYou Liu"], "title": "MExplore: an entity-based visual analytics approach for medical expertise acquisition", "comment": null, "summary": "Acquiring medical expertise is a critical component of medical education and\nprofessional development. While existing studies focus primarily on\nconstructing medical knowledge bases or developing learning tools based on the\nstructured, private healthcare data, they often lack methods for extracting\nexpertise from unstructured medical texts. These texts constitute a significant\nportion of medical literature and offer greater flexibility and detail compared\nto structured data formats. Furthermore, many studies fail to provide explicit\nanalytical and learning pathways in this context.\n  This paper introduces MExplore, an interactive visual analytics system\ndesigned to support the acquisition of medical expertise. To address the\nchallenges of the inconsistencies and confidentiality concerns inherent in\nunstructured medical texts, we propose a workflow that employs a fine-tuned\nBERT-based model to extract medical entities (MEs) from them. We then present a\nnovel multilevel visual analysis framework that integrates multiple coordinated\nvisualizations, enabling a progressive and interactive exploration of medical\nknowledge.\n  To assess the effectiveness of MExplore, we conducted three case studies, a\nuser study, and interviews with domain experts. The results indicate that the\nsystem significantly enhances the medical expertise acquisition process,\nproviding an effective interactive approach for acquiring and retaining\nknowledge from medical texts.", "AI": {"tldr": "MExplore\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u533b\u5b66\u6587\u672c\u4e2d\u63d0\u53d6\u533b\u5b66\u5b9e\u4f53\uff0c\u5e76\u901a\u8fc7\u591a\u7ea7\u53ef\u89c6\u5316\u6846\u67b6\u652f\u6301\u533b\u5b66\u4e13\u4e1a\u77e5\u8bc6\u7684\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u4ece\u975e\u7ed3\u6784\u5316\u533b\u5b66\u6587\u672c\u4e2d\u63d0\u53d6\u4e13\u4e1a\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u800c\u8fd9\u4e9b\u6587\u672c\u5728\u533b\u5b66\u6587\u732e\u4e2d\u5360\u5f88\u5927\u6bd4\u4f8b\u4e14\u66f4\u5177\u7075\u6d3b\u6027\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684BERT\u6a21\u578b\u63d0\u53d6\u533b\u5b66\u5b9e\u4f53\uff0c\u5e76\u901a\u8fc7\u591a\u7ea7\u53ef\u89c6\u5316\u6846\u67b6\u8fdb\u884c\u4ea4\u4e92\u5f0f\u63a2\u7d22\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u3001\u7528\u6237\u7814\u7a76\u548c\u4e13\u5bb6\u8bbf\u8c08\u8868\u660e\uff0cMExplore\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u4e13\u4e1a\u77e5\u8bc6\u7684\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "MExplore\u4e3a\u4ece\u533b\u5b66\u6587\u672c\u4e2d\u83b7\u53d6\u548c\u4fdd\u7559\u77e5\u8bc6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4ea4\u4e92\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2507.11732", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11732", "abs": "https://arxiv.org/abs/2507.11732", "authors": ["Shiyu Chen", "Cencheng Shen", "Youngser Park", "Carey E. Priebe"], "title": "Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning", "comment": null, "summary": "Graph neural networks (GNNs) have emerged as a powerful framework for a wide\nrange of node-level graph learning tasks. However, their performance is often\nconstrained by reliance on random or minimally informed initial feature\nrepresentations, which can lead to slow convergence and suboptimal solutions.\nIn this paper, we leverage a statistically grounded method, one-hot graph\nencoder embedding (GEE), to generate high-quality initial node features that\nenhance the end-to-end training of GNNs. We refer to this integrated framework\nas the GEE-powered GNN (GG), and demonstrate its effectiveness through\nextensive simulations and real-world experiments across both unsupervised and\nsupervised settings. In node clustering, GG consistently achieves\nstate-of-the-art performance, ranking first across all evaluated real-world\ndatasets, while exhibiting faster convergence compared to the standard GNN. For\nnode classification, we further propose an enhanced variant, GG-C, which\nconcatenates the outputs of GG and GEE and outperforms competing baselines.\nThese results confirm the importance of principled, structure-aware feature\ninitialization in realizing the full potential of GNNs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u65b9\u6cd5\u7684\u56fe\u7f16\u7801\u5d4c\u5165\uff08GEE\uff09\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u8282\u70b9\u7279\u5f81\uff0c\u4ee5\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u6027\u80fd\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u8282\u70b9\u805a\u7c7b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edfGNN\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u968f\u673a\u6216\u4f4e\u4fe1\u606f\u91cf\u7684\u521d\u59cb\u7279\u5f81\u8868\u793a\uff0c\u5bfc\u81f4\u6536\u655b\u6162\u548c\u6b21\u4f18\u89e3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u7684\u7279\u5f81\u521d\u59cb\u5316\u63d0\u5347GNN\u6027\u80fd\u3002", "method": "\u63d0\u51faGEE-powered GNN\uff08GG\uff09\u6846\u67b6\uff0c\u5229\u7528GEE\u751f\u6210\u9ad8\u8d28\u91cf\u521d\u59cb\u7279\u5f81\u3002\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fdb\u4e00\u6b65\u63d0\u51faGG-C\u53d8\u4f53\uff0c\u7ed3\u5408GG\u548cGEE\u7684\u8f93\u51fa\u3002", "result": "\u5728\u8282\u70b9\u805a\u7c7b\u4efb\u52a1\u4e2d\uff0cGG\u5728\u6240\u6709\u8bc4\u4f30\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u6536\u655b\u66f4\u5feb\u3002GG-C\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u7684\u7279\u5f81\u521d\u59cb\u5316\u5bf9\u53d1\u6325GNN\u7684\u6f5c\u529b\u81f3\u5173\u91cd\u8981\uff0cGG\u548cGG-C\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.11954", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11954", "abs": "https://arxiv.org/abs/2507.11954", "authors": ["Artem Alekseev", "Mikhail Chaichuk", "Miron Butko", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "title": "The benefits of query-based KGQA systems for complex and temporal questions in LLM era", "comment": "15 pages, 3 figures, 7 tables", "summary": "Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u67e5\u8be2\u7684\u591a\u9636\u6bb5\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u8df3\u548c\u65f6\u95f4\u95ee\u9898\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u63a8\u7406\u548c\u65f6\u95f4\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u57fa\u4e8e\u67e5\u8be2\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u67e5\u8be2\u6846\u67b6\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u5b9e\u4f53\u94fe\u63a5\u548c\u8c13\u8bcd\u5339\u914d\u65b9\u6cd5\uff0c\u5229\u7528CoT\u63a8\u7406\u3002", "result": "\u5728\u591a\u8df3\u548c\u65f6\u95f4\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u67e5\u8be2\u7684\u591a\u9636\u6bb5\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u591a\u8df3\u548c\u65f6\u95f4\u95ee\u9898\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12377", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12377", "abs": "https://arxiv.org/abs/2507.12377", "authors": ["Ke Er Amy Zhang", "Jodie Jenkinson", "Laura Garrison"], "title": "Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight", "comment": "11 pages, 5 figures, accepted to IEEE VIS 2025 Conference", "summary": "We conduct a deconstructive reading of a qualitative interview study with 17\nvisual data journalists from newsrooms across the globe. We borrow a\ndeconstruction approach from literary critique to explore the instability of\nmeaning in language and reveal implicit beliefs in words and ideas. Through our\nanalysis we surface two sets of opposing implicit beliefs in visual data\njournalism: objectivity/subjectivity and humanism/mechanism. We contextualize\nthese beliefs through a genealogical analysis, which brings deconstruction\ntheory into practice by providing a historic backdrop for these opposing\nperspectives. Our analysis shows that these beliefs held within visual data\njournalism are not self-enclosed but rather a product of external societal\nforces and paradigm shifts over time. Through this work, we demonstrate how\nthinking with critical theories such as deconstruction and genealogy can\nreframe \"success\" in visual data storytelling and diversify visualization\nresearch outcomes. These efforts push the ways in which we as researchers\nproduce domain knowledge to examine the sociotechnical issues of today's values\ntowards datafication and data visualization.", "AI": {"tldr": "\u901a\u8fc7\u5bf917\u4f4d\u5168\u7403\u89c6\u89c9\u6570\u636e\u8bb0\u8005\u7684\u8bbf\u8c08\u8fdb\u884c\u89e3\u6784\u6027\u9605\u8bfb\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u6570\u636e\u65b0\u95fb\u4e2d\u9690\u542b\u7684\u5bf9\u7acb\u4fe1\u5ff5\uff08\u5ba2\u89c2\u6027/\u4e3b\u89c2\u6027\u3001\u4eba\u6587\u4e3b\u4e49/\u673a\u68b0\u4e3b\u4e49\uff09\uff0c\u5e76\u901a\u8fc7\u8c31\u7cfb\u5206\u6790\u5c06\u5176\u7f6e\u4e8e\u5386\u53f2\u80cc\u666f\u4e2d\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u610f\u4e49\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u63ed\u793a\u89c6\u89c9\u6570\u636e\u65b0\u95fb\u4e2d\u7684\u9690\u542b\u4fe1\u5ff5\u53ca\u5176\u793e\u4f1a\u6839\u6e90\u3002", "method": "\u91c7\u7528\u6587\u5b66\u6279\u8bc4\u4e2d\u7684\u89e3\u6784\u65b9\u6cd5\uff0c\u7ed3\u5408\u8c31\u7cfb\u5206\u6790\uff0c\u5bf9\u8bbf\u8c08\u8fdb\u884c\u89e3\u6784\u6027\u9605\u8bfb\u3002", "result": "\u53d1\u73b0\u89c6\u89c9\u6570\u636e\u65b0\u95fb\u4e2d\u7684\u5bf9\u7acb\u4fe1\u5ff5\u662f\u5916\u90e8\u793e\u4f1a\u529b\u91cf\u548c\u8303\u5f0f\u53d8\u8fc1\u7684\u4ea7\u7269\u3002", "conclusion": "\u901a\u8fc7\u89e3\u6784\u548c\u8c31\u7cfb\u7406\u8bba\uff0c\u91cd\u65b0\u5b9a\u4e49\u89c6\u89c9\u6570\u636e\u53d9\u4e8b\u7684\u201c\u6210\u529f\u201d\uff0c\u5e76\u63a8\u52a8\u53ef\u89c6\u5316\u7814\u7a76\u7684\u591a\u6837\u5316\u3002"}}
{"id": "2507.11739", "categories": ["cs.LG", "cs.CE", "math.DS"], "pdf": "https://arxiv.org/pdf/2507.11739", "abs": "https://arxiv.org/abs/2507.11739", "authors": ["Urban Fasel"], "title": "Sparse Identification of Nonlinear Dynamics with Conformal Prediction", "comment": null, "summary": "The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for\ndiscovering nonlinear dynamical system models from data. Quantifying\nuncertainty in SINDy models is essential for assessing their reliability,\nparticularly in safety-critical applications. While various uncertainty\nquantification methods exist for SINDy, including Bayesian and ensemble\napproaches, this work explores the integration of Conformal Prediction, a\nframework that can provide valid prediction intervals with coverage guarantees\nbased on minimal assumptions like data exchangeability. We introduce three\napplications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)\nquantifying uncertainty in time series prediction, (2) model selection based on\nlibrary feature importance, and (3) quantifying the uncertainty of identified\nmodel coefficients using feature conformal prediction. We demonstrate the three\napplications on stochastic predator-prey dynamics and several chaotic dynamical\nsystems. We show that conformal prediction methods integrated with E-SINDy can\nreliably achieve desired target coverage for time series forecasting,\neffectively quantify feature importance, and produce more robust uncertainty\nintervals for model coefficients, even under non-Gaussian noise, compared to\nstandard E-SINDy coefficient estimates.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06Conformal Prediction\u6846\u67b6\u4e0eEnsemble-SINDy\uff08E-SINDy\uff09\u7ed3\u5408\uff0c\u4ee5\u91cf\u5316SINDy\u6a21\u578b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3001\u6a21\u578b\u9009\u62e9\u548c\u7cfb\u6570\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u91cf\u5316SINDy\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u5bf9\u4e8e\u8bc4\u4f30\u5176\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u3002", "method": "\u901a\u8fc7Conformal Prediction\u6846\u67b6\u4e0eE-SINDy\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u5e94\u7528\uff1a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u57fa\u4e8e\u7279\u5f81\u91cd\u8981\u6027\u7684\u6a21\u578b\u9009\u62e9\uff0c\u4ee5\u53ca\u6a21\u578b\u7cfb\u6570\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u80fd\u53ef\u9760\u5730\u8fbe\u5230\u76ee\u6807\u8986\u76d6\u8303\u56f4\uff0c\u6709\u6548\u91cf\u5316\u7279\u5f81\u91cd\u8981\u6027\uff0c\u5e76\u5728\u975e\u9ad8\u65af\u566a\u58f0\u4e0b\u751f\u6210\u66f4\u7a33\u5065\u7684\u7cfb\u6570\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\u3002", "conclusion": "Conformal Prediction\u4e0eE-SINDy\u7684\u7ed3\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u3002"}}
{"id": "2507.11959", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11959", "abs": "https://arxiv.org/abs/2507.11959", "authors": ["Xinyu Wang", "Vahid Partovi Nia", "Peng Lu", "Jerry Huang", "Xiao-Wen Chang", "Boxing Chen", "Yufei Cui"], "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs", "comment": "Accepted at ECAI 2025 (European Conference on Artificial\n  Intelligence)", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684PoT\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8eLLM\u6743\u91cd\uff0c\u5728\u6781\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u53cd\u91cf\u5316\u3002", "motivation": "LLMs\u90e8\u7f72\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0cPoT\u91cf\u5316\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u5de5\u5177\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728GPU\u4e0a\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u540e\u8bad\u7ec3\u7b97\u6cd5\uff1a\u521d\u59cb\u5316\u91cf\u5316\u5c3a\u5ea6\u5e76\u4f7f\u7528\u6821\u51c6\u96c6\u4f18\u5316\u3002", "result": "\u57282-\u548c3-bit\u683c\u5f0f\u4e0b\u4f18\u4e8e\u73b0\u6709\u6574\u6570\u91cf\u5316\u65b9\u6cd5\uff0c\u53cd\u91cf\u5316\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u65b0PoT\u91cf\u5316\u6846\u67b6\u5728\u4f4e\u7cbe\u5ea6\u548c\u9ad8\u6548\u53cd\u91cf\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.11821", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11821", "abs": "https://arxiv.org/abs/2507.11821", "authors": ["Pouya Shaeri", "Arash Karimi", "Ariane Middel"], "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory", "comment": "Submitted to a computer science conference", "summary": "Neural networks are often benchmarked using standard datasets such as MNIST,\nFashionMNIST, or other variants of MNIST, which, while accessible, are limited\nto generic classes such as digits or clothing items. For researchers working on\ndomain-specific tasks, such as classifying trees, food items, or other\nreal-world objects, these data sets are insufficient and irrelevant.\nAdditionally, creating and publishing a custom dataset can be time consuming,\nlegally constrained, or beyond the scope of individual projects. We present\nMNIST-Gen, an automated, modular, and adaptive framework for generating\nMNIST-style image datasets tailored to user-specified categories using\nhierarchical semantic categorization. The system combines CLIP-based semantic\nunderstanding with reinforcement learning and human feedback to achieve\nintelligent categorization with minimal manual intervention. Our hierarchical\napproach supports complex category structures with semantic characteristics,\nenabling fine-grained subcategorization and multiple processing modes:\nindividual review for maximum control, smart batch processing for large\ndatasets, and fast batch processing for rapid creation. Inspired by category\ntheory, MNIST-Gen models each data transformation stage as a composable\nmorphism, enhancing clarity, modularity, and extensibility. As proof of\nconcept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and\n\\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing\ntask-specific evaluation data while achieving 85\\% automatic categorization\naccuracy and 80\\% time savings compared to manual approaches.", "AI": {"tldr": "MNIST-Gen\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7279\u5b9a\u9886\u57df\u7684MNIST\u98ce\u683c\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4e86\u8bed\u4e49\u7406\u89e3\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u8282\u7701\u65f6\u95f4\u548c\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u6807\u51c6\u6570\u636e\u96c6\uff08\u5982MNIST\uff09\u5bf9\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e0d\u9002\u7528\uff0c\u800c\u521b\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8017\u65f6\u4e14\u590d\u6742\u3002", "method": "\u7ed3\u5408CLIP\u8bed\u4e49\u7406\u89e3\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u4eba\u5de5\u53cd\u9988\uff0c\u91c7\u7528\u5206\u5c42\u8bed\u4e49\u5206\u7c7b\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u3002", "result": "\u751f\u6210Tree-MNIST\u548cFood-MNIST\u6570\u636e\u96c6\uff0c\u81ea\u52a8\u5206\u7c7b\u51c6\u786e\u7387\u8fbe85%\uff0c\u8282\u770180%\u65f6\u95f4\u3002", "conclusion": "MNIST-Gen\u4e3a\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u96c6\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11757", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11757", "abs": "https://arxiv.org/abs/2507.11757", "authors": ["Yuehua Song", "Yong Gao"], "title": "A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction", "comment": null, "summary": "Accurately predicting drug-target interactions (DTIs) is pivotal for\nadvancing drug discovery and target validation techniques. While machine\nlearning approaches including those that are based on Graph Neural Networks\n(GNN) have achieved notable success in DTI prediction, many of them have\ndifficulties in effectively integrating the diverse features of drugs, targets\nand their interactions. To address this limitation, we introduce a novel\nframework to take advantage of the power of both transductive learning and\ninductive learning so that features at molecular level and drug-target\ninteraction network level can be exploited. Within this framework is a\nGNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and\ntarget molecular structures as meta-nodes in a drug-target interaction graph,\nenabling a detailed exploration of their intricate relationships. To evaluate\nthe proposed model, we have compiled a special benchmark comprising drug\nSMILES, protein sequences, and their interaction data, which is interesting in\nits own right. Our experimental results demonstrate that the GiG model\nsignificantly outperforms existing approaches across all evaluation metrics,\nhighlighting the benefits of integrating different learning paradigms and\ninteraction data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGraph-in-Graph\uff08GiG\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u8f6c\u5bfc\u5b66\u4e60\u548c\u5f52\u7eb3\u5b66\u4e60\uff0c\u6709\u6548\u6574\u5408\u836f\u7269\u548c\u9776\u6807\u7684\u5206\u5b50\u7279\u5f81\u53ca\u5176\u76f8\u4e92\u4f5c\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u836f\u7269-\u9776\u6807\u76f8\u4e92\u4f5c\u7528\uff08DTI\uff09\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684DTI\u9884\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u836f\u7269\u3001\u9776\u6807\u53ca\u5176\u76f8\u4e92\u4f5c\u7528\u7684\u591a\u6837\u5316\u7279\u5f81\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faGiG\u6a21\u578b\uff0c\u5c06\u836f\u7269\u548c\u9776\u6807\u5206\u5b50\u7ed3\u6784\u56fe\u8868\u793a\u4e3a\u836f\u7269-\u9776\u6807\u76f8\u4e92\u4f5c\u7528\u56fe\u4e2d\u7684\u5143\u8282\u70b9\uff0c\u7ed3\u5408\u8f6c\u5bfc\u5b66\u4e60\u548c\u5f52\u7eb3\u5b66\u4e60\uff0c\u63a2\u7d22\u5176\u590d\u6742\u5173\u7cfb\u3002", "result": "GiG\u6a21\u578b\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6574\u5408\u4e0d\u540c\u5b66\u4e60\u8303\u5f0f\u548c\u4ea4\u4e92\u6570\u636e\u7684\u4f18\u52bf\u3002", "conclusion": "GiG\u6846\u67b6\u901a\u8fc7\u6574\u5408\u5206\u5b50\u6c34\u5e73\u548c\u7f51\u7edc\u6c34\u5e73\u7684\u7279\u5f81\uff0c\u4e3aDTI\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11966", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.11966", "abs": "https://arxiv.org/abs/2507.11966", "authors": ["Ziyu Ge", "Gabriel Chua", "Leanne Tan", "Roy Ka-Wei Lee"], "title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation", "comment": null, "summary": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e2d\u4fdd\u7559\u6bd2\u6027\u5185\u5bb9\u7684\u7ffb\u8bd1\uff0c\u4ee5\u65b0\u52a0\u5761\u82f1\u8bed\u4e3a\u4f8b\uff0c\u901a\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u7684\u5c11\u6837\u672c\u63d0\u793a\u5de5\u7a0b\u548c\u6a21\u578b\u4f18\u5316\uff0c\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u7ebf\u4ea4\u6d41\u4e2d\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u65b9\u8a00\u7684\u7ffb\u8bd1\u7cfb\u7edf\u5e38\u65e0\u6cd5\u4fdd\u7559\u672c\u5730\u4fda\u8bed\u3001\u6df7\u5408\u8bed\u548c\u6587\u5316\u5d4c\u5165\u7684\u6709\u5bb3\u8a00\u8bba\u6807\u8bb0\uff0c\u4e14\u7f3a\u4e4f\u5e73\u884c\u6570\u636e\u548c\u5b89\u5168\u8fc7\u6ee4\u5668\u3002", "method": "1. \u4eba\u5de5\u9a8c\u8bc1\u7684\u5c11\u6837\u672c\u63d0\u793a\u5de5\u7a0b\uff0c\u7b5b\u9009\u548c\u6392\u5e8f\u65b0\u52a0\u5761\u82f1\u8bed\u793a\u4f8b\uff1b2. \u901a\u8fc7\u76f4\u63a5\u548c\u56de\u8bd1\u4f18\u5316\u6a21\u578b-\u63d0\u793a\u5bf9\u3002", "result": "\u5b9a\u91cf\u4eba\u5de5\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u8fd8\u652f\u6301\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u6587\u5316\u654f\u611f\u5185\u5bb9\u5ba1\u6838\uff0c\u5f3a\u8c03\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u4fdd\u7559\u793e\u4f1a\u8bed\u8a00\u7ec6\u5fae\u5dee\u522b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.12356", "categories": ["cs.CL", "cs.HC", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.12356", "abs": "https://arxiv.org/abs/2507.12356", "authors": ["Liu He", "Yuanchao Li", "Rui Feng", "XinRan Han", "Yin-Long Liu", "Yuwei Yang", "Zude Zhu", "Jiahong Yuan"], "title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception", "comment": "12 pages, 5 figures, conference or other essential info", "summary": "Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6027\u522b\u504f\u89c1\u5f71\u54cd\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u8bed\u97f3\u611f\u77e5\uff0c\u7537\u6027\u8bed\u97f3\u66f4\u6613\u88ab\u8bc6\u522b\u4e3aAD\uff0c\u5c24\u5176\u5728\u4e2d\u6587\u8bed\u97f3\u4e2d\u3002\u58f0\u5b66\u5206\u6790\u663e\u793a\u7537\u6027\u8bed\u97f3\u7684shimmer\u503c\u4e0eAD\u611f\u77e5\u663e\u8457\u76f8\u5173\u3002", "motivation": "\u63a2\u8ba8\u6027\u522b\u504f\u89c1\u5728AD\u8bed\u97f3\u611f\u77e5\u4e2d\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u5176\u5bf9\u8bed\u97f3\u8bc6\u522b\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc716\u540d\u4e2d\u6587\u542c\u4f17\u5bf9\u4e2d\u6587\u548c\u5e0c\u814a\u8bed\u8bed\u97f3\u7684\u611f\u77e5\u5b9e\u9a8c\uff0c\u7ed3\u5408\u58f0\u5b66\u5206\u6790\uff08\u5982shimmer\u503c\u548c\u8bed\u97f3\u90e8\u5206\uff09\u3002", "result": "\u7537\u6027\u8bed\u97f3\u66f4\u9891\u7e41\u88ab\u8bc6\u522b\u4e3aAD\uff0c\u4e2d\u6587\u8bed\u97f3\u4e2d\u6027\u522b\u504f\u89c1\u66f4\u660e\u663e\uff1bshimmer\u503c\u4e0eAD\u611f\u77e5\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u6027\u522b\u504f\u89c1\u5728AD\u8bed\u97f3\u611f\u77e5\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u9700\u5728AD\u68c0\u6d4b\u6a21\u578b\u4e2d\u52a0\u4ee5\u89e3\u51b3\uff0c\u5e76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u8de8\u8bed\u8a00\u6027\u80fd\u3002"}}
{"id": "2507.11759", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11759", "abs": "https://arxiv.org/abs/2507.11759", "authors": ["Alexandra Volokhova", "L\u00e9na N\u00e9hale Ezzine", "Piotr Gai\u0144ski", "Luca Scimeca", "Emmanuel Bengio", "Prudencio Tossou", "Yoshua Bengio", "Alex Hernandez-Garcia"], "title": "Torsional-GFN: a conditional conformation generator for small molecules", "comment": "The two first authors are Alexandra Volokhova and L\\'ena N\\'ehale\n  Ezzine, with equal contribution", "summary": "Generating stable molecular conformations is crucial in several drug\ndiscovery applications, such as estimating the binding affinity of a molecule\nto a target. Recently, generative machine learning methods have emerged as a\npromising, more efficient method than molecular dynamics for sampling of\nconformations from the Boltzmann distribution. In this paper, we introduce\nTorsional-GFN, a conditional GFlowNet specifically designed to sample\nconformations of molecules proportionally to their Boltzmann distribution,\nusing only a reward function as training signal. Conditioned on a molecular\ngraph and its local structure (bond lengths and angles), Torsional-GFN samples\nrotations of its torsion angles. Our results demonstrate that Torsional-GFN is\nable to sample conformations approximately proportional to the Boltzmann\ndistribution for multiple molecules with a single model, and allows for\nzero-shot generalization to unseen bond lengths and angles coming from the MD\nsimulations for such molecules. Our work presents a promising avenue for\nscaling the proposed approach to larger molecular systems, achieving zero-shot\ngeneralization to unseen molecules, and including the generation of the local\nstructure into the GFlowNet model.", "AI": {"tldr": "Torsional-GFN\u662f\u4e00\u79cd\u57fa\u4e8eGFlowNet\u7684\u6761\u4ef6\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u4e2d\u91c7\u6837\u5206\u5b50\u6784\u8c61\uff0c\u4ec5\u9700\u5956\u52b1\u51fd\u6570\u4f5c\u4e3a\u8bad\u7ec3\u4fe1\u53f7\u3002", "motivation": "\u5728\u836f\u7269\u53d1\u73b0\u4e2d\uff0c\u751f\u6210\u7a33\u5b9a\u7684\u5206\u5b50\u6784\u8c61\u5bf9\u4f30\u8ba1\u5206\u5b50\u4e0e\u9776\u6807\u7684\u7ed3\u5408\u4eb2\u548c\u529b\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5982\u5206\u5b50\u52a8\u529b\u5b66\u6548\u7387\u8f83\u4f4e\uff0c\u800c\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "Torsional-GFN\u901a\u8fc7\u6761\u4ef6\u5316\u5206\u5b50\u56fe\u53ca\u5176\u5c40\u90e8\u7ed3\u6784\uff08\u952e\u957f\u548c\u952e\u89d2\uff09\uff0c\u91c7\u6837\u626d\u8f6c\u89d2\u7684\u65cb\u8f6c\uff0c\u4ece\u800c\u751f\u6210\u5206\u5b50\u6784\u8c61\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTorsional-GFN\u80fd\u591f\u8fd1\u4f3c\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u91c7\u6837\u5206\u5b50\u6784\u8c61\uff0c\u5e76\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u952e\u957f\u548c\u952e\u89d2\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6269\u5c55\u65b9\u6cd5\u81f3\u66f4\u5927\u5206\u5b50\u7cfb\u7edf\u3001\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u5206\u5b50\u4ee5\u53ca\u5c06\u5c40\u90e8\u7ed3\u6784\u751f\u6210\u7eb3\u5165\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2507.11972", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.11972", "abs": "https://arxiv.org/abs/2507.11972", "authors": ["Yuhong Zhang", "Jialu Li", "Shilai Yang", "Yuchen Xu", "Gert Cauwenberghs", "Tzyy-Ping Jung"], "title": "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker", "comment": null, "summary": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9605\u8bfb\u7406\u89e3\u4e2d\u7684\u5dee\u5f02\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u5206\u6790\u63ed\u793a\u4e86LLMs\u5728\u8bed\u8a00\u7406\u89e3\u4e0a\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u4e0eLLMs\u5728\u9605\u8bfb\u7406\u89e3\u4e2d\u7684\u5dee\u5f02\uff0c\u4ee5\u6539\u8fdb\u4eba\u7c7b\u4e0eAI\u7684\u534f\u4f5c\u5b66\u4e60\u7b56\u7565\u3002", "method": "\u4f7f\u7528LLM\u5c06\u9605\u8bfb\u6750\u6599\u4e2d\u7684\u5355\u8bcd\u5206\u7ec4\u4e3a\u8282\u70b9\u548c\u8fb9\uff0c\u6784\u5efa\u57fa\u4e8e\u8bed\u4e49\u548c\u95ee\u9898\u5bfc\u5411\u7684\u56fe\u7ed3\u6784\uff0c\u5e76\u4e0e\u4eba\u7c7b\u773c\u52a8\u6570\u636e\u5bf9\u6bd4\u3002", "result": "LLMs\u5728\u56fe\u62d3\u6251\u7ed3\u6784\u5c42\u9762\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4eba\u7c7b\u4e0eAI\u7684\u534f\u4f5c\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u6269\u5c55\u4e86\u5148\u524d\u7684\u7814\u7a76\u6210\u679c\u3002"}}
{"id": "2507.12370", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12370", "abs": "https://arxiv.org/abs/2507.12370", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate", "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u7528\u6237\u8bf7\u6c42\u4e2d\u6a21\u7cca\u6027\u7684\u68c0\u6d4b\u548c\u89e3\u51b3\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u7528\u6237\u8bf7\u6c42\u65f6\u5b58\u5728\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u589e\u5f3a\u5176\u68c0\u6d4b\u548c\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u7ed3\u5408\u4e09\u79cdLLM\u67b6\u6784\uff08Llama3-8B\u3001Gemma2-9B\u548cMistral-7B\u53d8\u4f53\uff09\u548c\u5305\u542b\u591a\u6837\u6a21\u7cca\u6027\u7684\u6570\u636e\u96c6\u3002", "result": "\u8fa9\u8bba\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86Llama3-8B\u548cMistral-7B\u53d8\u4f53\u7684\u6027\u80fd\uff0c\u5176\u4e2dMistral-7B\u4e3b\u5bfc\u7684\u8fa9\u8bba\u6210\u529f\u7387\u9ad8\u8fbe76.7%\uff0c\u5c24\u5176\u64c5\u957f\u590d\u6742\u6a21\u7cca\u6027\u548c\u9ad8\u6548\u5171\u8bc6\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\u662f\u589e\u5f3aLLM\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u8bed\u8a00\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2507.11771", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11771", "abs": "https://arxiv.org/abs/2507.11771", "authors": ["Sheikh Abdur Raheem Ali", "Justin Xu", "Ivory Yang", "Jasmine Xinze Li", "Ayse Arslan", "Clark Benham"], "title": "Scaling laws for activation steering with Llama 2 models and refusal mechanisms", "comment": null, "summary": "As large language models (LLMs) evolve in complexity and capability, the\nefficacy of less widely deployed alignment techniques are uncertain. Building\non previous work on activation steering and contrastive activation addition\n(CAA), this paper explores the effectiveness of CAA with model scale using the\nfamily of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable\n'directions' in the model's residual stream vector space using contrastive\npairs (for example, hate to love) and adding this direction to the residual\nstream during the forward pass. It directly manipulates the residual stream and\naims to extract features from language models to better control their outputs.\nUsing answer matching questions centered around the refusal behavior, we found\nthat 1) CAA is most effective when applied at early-mid layers. 2) The\neffectiveness of CAA diminishes with model size. 3) Negative steering has more\npronounced effects than positive steering across all model sizes.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5bf9\u6bd4\u6fc0\u6d3b\u52a0\u6cd5\uff08CAA\uff09\u5728\u4e0d\u540c\u89c4\u6a21\u7684Llama 2\u6a21\u578b\uff087B\u300113B\u300170B\uff09\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5176\u6548\u679c\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u51cf\u5f31\uff0c\u4e14\u5728\u65e9\u671f\u81f3\u4e2d\u5c42\u5e94\u7528\u65f6\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u590d\u6742\u6027\u548c\u80fd\u529b\u63d0\u5347\uff0c\u8f83\u5c11\u5e7f\u6cdb\u90e8\u7f72\u7684\u5bf9\u9f50\u6280\u672f\u6548\u679c\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1CAA\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5bf9\uff08\u5982\u4ec7\u6068\u5230\u7231\uff09\u5728\u6a21\u578b\u7684\u6b8b\u5dee\u6d41\u5411\u91cf\u7a7a\u95f4\u4e2d\u5bfb\u627e\u7406\u60f3\u65b9\u5411\uff0c\u5e76\u5728\u524d\u5411\u4f20\u64ad\u65f6\u5c06\u8be5\u65b9\u5411\u6dfb\u52a0\u5230\u6b8b\u5dee\u6d41\u4e2d\uff0c\u76f4\u63a5\u64cd\u63a7\u6a21\u578b\u8f93\u51fa\u3002", "result": "1) CAA\u5728\u65e9\u671f\u81f3\u4e2d\u5c42\u5e94\u7528\u65f6\u6548\u679c\u6700\u4f73\uff1b2) CAA\u6548\u679c\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u51cf\u5f31\uff1b3) \u8d1f\u9762\u5bfc\u5411\u5728\u6240\u6709\u6a21\u578b\u89c4\u6a21\u4e2d\u6548\u679c\u66f4\u663e\u8457\u3002", "conclusion": "CAA\u7684\u6548\u679c\u53d7\u6a21\u578b\u89c4\u6a21\u548c\u5e94\u7528\u5c42\u5f71\u54cd\uff0c\u8d1f\u9762\u5bfc\u5411\u66f4\u5177\u4f18\u52bf\uff0c\u4e3a\u6a21\u578b\u5bf9\u9f50\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.11979", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11979", "abs": "https://arxiv.org/abs/2507.11979", "authors": ["Yuki Sakamoto", "Takahisa Uchida", "Hiroshi Ishiguro"], "title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness", "comment": null, "summary": "Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4ef7\u503c\u76f8\u4f3c\u6027\u5bf9LLM\u4ee3\u7406\u4e4b\u95f4\u5173\u7cfb\u5efa\u7acb\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4ef7\u503c\u76f8\u4f3c\u6027\u9ad8\u7684\u4ee3\u7406\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u4e92\u4fe1\u548c\u4eba\u9645\u4eb2\u8fd1\u3002", "motivation": "\u63a2\u7d22\u4ef7\u503c\u76f8\u4f3c\u6027\u5728\u4eba\u5de5\u793e\u4f1a\u4e2d\u662f\u5426\u4e0e\u4eba\u7c7b\u793e\u4f1a\u4e2d\u4e00\u6837\u5bf9\u5173\u7cfb\u5efa\u7acb\u6709\u91cd\u8981\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\uff1a\u521d\u6b65\u5b9e\u9a8c\u8bc4\u4f30LLM\u4e2d\u4ef7\u503c\u7684\u53ef\u63a7\u6027\uff0c\u4e3b\u5b9e\u9a8c\u751f\u6210\u5177\u6709\u7279\u5b9a\u4ef7\u503c\u7684\u4ee3\u7406\u5bf9\u5e76\u5206\u6790\u5176\u4e92\u8bc4\u3002", "result": "\u4ef7\u503c\u76f8\u4f3c\u6027\u9ad8\u7684\u4ee3\u7406\u5bf9\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u4e92\u4fe1\u548c\u4eba\u9645\u4eb2\u8fd1\u3002", "conclusion": "LLM\u4ee3\u7406\u6a21\u62df\u53ef\u4f5c\u4e3a\u793e\u4f1a\u79d1\u5b66\u7406\u8bba\u7684\u6709\u6548\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u4e3a\u65b0\u7406\u8bba\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2507.11776", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11776", "abs": "https://arxiv.org/abs/2507.11776", "authors": ["Merel Kampere", "Ali Mohammed Mansoor Alsahag"], "title": "Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network", "comment": null, "summary": "The Dutch railway network is one of the busiest in the world, with delays\nbeing a prominent concern for the principal passenger railway operator NS. This\nresearch addresses a gap in delay prediction studies within the Dutch railway\nnetwork by employing an XGBoost Classifier with a focus on topological\nfeatures. Current research predominantly emphasizes short-term predictions and\nneglects the broader network-wide patterns essential for mitigating ripple\neffects. This research implements and improves an existing methodology,\noriginally designed to forecast the evolution of the fast-changing US air\nnetwork, to predict delays in the Dutch Railways. By integrating Node\nCentrality Measures and comparing multiple classifiers like RandomForest,\nDecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is\nto predict delayed trajectories. However, the results reveal limited\nperformance, especially in non-simultaneous testing scenarios, suggesting the\nnecessity for more context-specific adaptations. Regardless, this research\ncontributes to the understanding of transportation network evaluation and\nproposes future directions for developing more robust predictive models for\ndelays.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528XGBoost\u5206\u7c7b\u5668\u7ed3\u5408\u62d3\u6251\u7279\u5f81\u9884\u6d4b\u8377\u5170\u94c1\u8def\u7f51\u7edc\u5ef6\u8bef\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u540c\u6b65\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "motivation": "\u8377\u5170\u94c1\u8def\u7f51\u7edc\u7e41\u5fd9\u4e14\u5ef6\u8bef\u95ee\u9898\u7a81\u51fa\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u77ed\u671f\u9884\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u7f51\u7edc\u6574\u4f53\u6a21\u5f0f\u7684\u5206\u6790\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528XGBoost\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u8282\u70b9\u4e2d\u5fc3\u6027\u5ea6\u91cf\uff0c\u5e76\u4e0e\u591a\u79cd\u5206\u7c7b\u5668\uff08\u5982\u968f\u673a\u68ee\u6797\u3001\u51b3\u7b56\u6811\u7b49\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u9884\u6d4b\u5ef6\u8bef\u8f68\u8ff9\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6a21\u578b\u6027\u80fd\u6709\u9650\uff0c\u5c24\u5176\u5728\u975e\u540c\u6b65\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u9700\u8981\u66f4\u591a\u9488\u5bf9\u6027\u7684\u4f18\u5316\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4ea4\u901a\u7f51\u7edc\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u5f00\u53d1\u66f4\u9c81\u68d2\u9884\u6d4b\u6a21\u578b\u7684\u65b9\u5411\u3002"}}
{"id": "2507.11981", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11981", "abs": "https://arxiv.org/abs/2507.11981", "authors": ["Lukas Ellinger", "Miriam Ansch\u00fctz", "Georg Groh"], "title": "Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions", "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u7b80\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u591a\u4e49\u8bcd\u7684\u5b9a\u4e49\u4f1a\u964d\u4f4e\u5b8c\u6574\u6027\uff0c\u589e\u52a0\u8bef\u89e3\u98ce\u9669\u3002\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u53ef\u663e\u8457\u6539\u5584\u5b9a\u4e49\u8d28\u91cf\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u76ee\u6807\u7fa4\u4f53\uff08\u5982\u513f\u7ae5\u6216\u8bed\u8a00\u5b66\u4e60\u8005\uff09\u5bf9\u591a\u4e49\u8bcd\u5b9a\u4e49\u7684\u9700\u6c42\uff0c\u907f\u514d\u56e0\u7b80\u5316\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u6216\u8bef\u5bfc\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u591a\u8bed\u8a00\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u591a\u4e2aLLM\u6a21\u578b\uff08\u5982DeepSeek v3\u3001GPT-4o mini\u7b49\uff09\uff0c\u7ed3\u5408LLM-as-Judge\u548c\u4eba\u5de5\u6807\u6ce8\u3002", "result": "\u7b80\u5316\u663e\u8457\u964d\u4f4e\u5b9a\u4e49\u5b8c\u6574\u6027\uff0c\u5ffd\u7565\u591a\u4e49\u6027\uff1b\u5fae\u8c03Llama 3.1 8B\u53ef\u63d0\u5347\u5b9a\u4e49\u8d28\u91cf\u3002", "conclusion": "\u6559\u80b2NLP\u9700\u5e73\u8861\u7b80\u6d01\u6027\u4e0e\u5b8c\u6574\u6027\uff0c\u786e\u4fdd\u4e3a\u6240\u6709\u5b66\u4e60\u8005\u63d0\u4f9b\u53ef\u9760\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b9a\u4e49\u3002"}}
{"id": "2507.11789", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11789", "abs": "https://arxiv.org/abs/2507.11789", "authors": ["Alessandro Palma", "Sergei Rybakov", "Leon Hetzel", "Stephan G\u00fcnnemann", "Fabian J. Theis"], "title": "Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation", "comment": "31 pages, 14 figures", "summary": "Latent space interpolations are a powerful tool for navigating deep\ngenerative models in applied settings. An example is single-cell RNA\nsequencing, where existing methods model cellular state transitions as latent\nspace interpolations with variational autoencoders, often assuming linear\nshifts and Euclidean geometry. However, unless explicitly enforced, linear\ninterpolations in the latent space may not correspond to geodesic paths on the\ndata manifold, limiting methods that assume Euclidean geometry in the data\nrepresentations. We introduce FlatVI, a novel training framework that\nregularises the latent manifold of discrete-likelihood variational autoencoders\ntowards Euclidean geometry, specifically tailored for modelling single-cell\ncount data. By encouraging straight lines in the latent space to approximate\ngeodesic interpolations on the decoded single-cell manifold, FlatVI enhances\ncompatibility with downstream approaches that assume Euclidean latent geometry.\nExperiments on synthetic data support the theoretical soundness of our\napproach, while applications to time-resolved single-cell RNA sequencing data\ndemonstrate improved trajectory reconstruction and manifold interpolation.", "AI": {"tldr": "FlatVI\u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u79bb\u6563\u4f3c\u7136\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u6d41\u5f62\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u5355\u7ec6\u80de\u8ba1\u6570\u6570\u636e\u7684\u5efa\u6a21\uff0c\u63d0\u5347\u4e0b\u6e38\u65b9\u6cd5\u7684\u517c\u5bb9\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u4e2d\u5047\u8bbe\u7ebf\u6027\u8f6c\u79fb\u548c\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\uff0c\u4f46\u7ebf\u6027\u63d2\u503c\u53ef\u80fd\u4e0d\u7b26\u5408\u6570\u636e\u6d41\u5f62\u4e0a\u7684\u6d4b\u5730\u7ebf\u8def\u5f84\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "FlatVI\u901a\u8fc7\u6b63\u5219\u5316\u6f5c\u5728\u6d41\u5f62\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\uff0c\u4ece\u800c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f4\u7ebf\u63d2\u503c\u8fd1\u4f3c\u4e8e\u89e3\u7801\u540e\u7684\u5355\u7ec6\u80de\u6d41\u5f62\u4e0a\u7684\u6d4b\u5730\u7ebf\u63d2\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlatVI\u5728\u5408\u6210\u6570\u636e\u548c\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u4e2d\u5747\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8f68\u8ff9\u91cd\u5efa\u548c\u6d41\u5f62\u63d2\u503c\u6548\u679c\u3002", "conclusion": "FlatVI\u901a\u8fc7\u4f18\u5316\u6f5c\u5728\u6d41\u5f62\u7684\u51e0\u4f55\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u7ec6\u80de\u6570\u636e\u5efa\u6a21\u7684\u51c6\u786e\u6027\u548c\u4e0b\u6e38\u5e94\u7528\u7684\u517c\u5bb9\u6027\u3002"}}
{"id": "2507.12004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12004", "abs": "https://arxiv.org/abs/2507.12004", "authors": ["Josip Juki\u0107"], "title": "Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis", "comment": null, "summary": "This thesis addresses challenges related to data and parameter efficiency in\nneural language models, with a focus on representation analysis and the\nintroduction of new optimization techniques. The first part examines the\nproperties and dynamics of language representations within neural models,\nemphasizing their significance in enhancing robustness and generalization. It\nproposes innovative approaches based on representation smoothness, including\nregularization strategies that utilize Jacobian and Hessian matrices to\nstabilize training and mitigate sensitivity to input perturbations. The second\npart focuses on methods to significantly enhance data and parameter efficiency\nby integrating active learning strategies with parameter-efficient fine-tuning,\nguided by insights from representation smoothness analysis. It presents\nsmoothness-informed early-stopping techniques designed to eliminate the need\nfor labeled validation sets and proposes innovative combinations of active\nlearning and parameter-efficient fine-tuning to reduce labeling efforts and\ncomputational resources. Extensive experimental evaluations across various NLP\ntasks demonstrate that these combined approaches substantially outperform\ntraditional methods in terms of performance, stability, and efficiency. The\nthird part explores weak supervision techniques enhanced by in-context learning\nto effectively utilize unlabeled data, further reducing dependence on extensive\nlabeling. It shows that using in-context learning as a mechanism for weak\nsupervision enables models to better generalize from limited labeled data by\nleveraging unlabeled examples more effectively during training. Comprehensive\nempirical evaluations confirm significant gains in model accuracy,\nadaptability, and robustness, especially in low-resource settings and dynamic\ndata environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8868\u793a\u5e73\u6ed1\u5206\u6790\u548c\u4f18\u5316\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u548c\u53c2\u6570\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u7684\u6311\u6218\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u5206\u6790\u8bed\u8a00\u8868\u793a\u7684\u52a8\u6001\u7279\u6027\u5e76\u63d0\u51fa\u57fa\u4e8e\u5e73\u6ed1\u6027\u7684\u6b63\u5219\u5316\u7b56\u7565\uff1b2. \u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff1b3. \u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u589e\u5f3a\u5f31\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u548c\u52a8\u6001\u6570\u636e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.11807", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11807", "abs": "https://arxiv.org/abs/2507.11807", "authors": ["Ruofan Hu", "Dongyu Zhang", "Huayi Zhang", "Elke Rundensteiner"], "title": "CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels", "comment": "KDD 2025, 12 pages, 7 figures", "summary": "Learning with noisy labels (LNL) is essential for training deep neural\nnetworks with imperfect data. Meta-learning approaches have achieved success by\nusing a clean unbiased labeled set to train a robust model. However, this\napproach heavily depends on the availability of a clean labeled meta-dataset,\nwhich is difficult to obtain in practice. In this work, we thus tackle the\nchallenge of meta-learning for noisy label scenarios without relying on a clean\nlabeled dataset. Our approach leverages the data itself while bypassing the\nneed for labels. Building on the insight that clean samples effectively\npreserve the consistency of related data structures across the last hidden and\nthe final layer, whereas noisy samples disrupt this consistency, we design the\nCross-layer Information Divergence-based Meta Update Strategy (CLID-MU).\nCLID-MU leverages the alignment of data structures across these diverse feature\nspaces to evaluate model performance and use this alignment to guide training.\nExperiments on benchmark datasets with varying amounts of labels under both\nsynthetic and real-world noise demonstrate that CLID-MU outperforms\nstate-of-the-art methods. The code is released at\nhttps://github.com/ruofanhu/CLID-MU.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u5e72\u51c0\u6807\u7b7e\u6570\u636e\u7684\u5143\u5b66\u4e60\u65b9\u6cd5CLID-MU\uff0c\u901a\u8fc7\u8de8\u5c42\u4fe1\u606f\u5dee\u5f02\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5728\u566a\u58f0\u6807\u7b7e\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5143\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5e72\u51c0\u6807\u7b7e\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5e72\u51c0\u6807\u7b7e\u7684\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5e72\u51c0\u6837\u672c\u5728\u9690\u85cf\u5c42\u548c\u6700\u7ec8\u5c42\u7684\u6570\u636e\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u8bbe\u8ba1\u8de8\u5c42\u4fe1\u606f\u5dee\u5f02\u7b56\u7565\uff08CLID-MU\uff09\u6307\u5bfc\u8bad\u7ec3\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u566a\u58f0\u6807\u7b7e\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCLID-MU\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CLID-MU\u4e3a\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u4e0d\u4f9d\u8d56\u5e72\u51c0\u6807\u7b7e\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.12039", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12039", "abs": "https://arxiv.org/abs/2507.12039", "authors": ["Anca Dinu", "Andra-Maria Florescu", "Alina Resceanu"], "title": "A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans", "comment": "Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)", "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8bed\u8a00\u521b\u9020\u529b\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u65b0\u8bcd\u548c\u77ed\u8bed\u7684\u80fd\u529b\u3002\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u6240\u6709\u8bc4\u4f30\u6807\u51c6\u4e0a\u5747\u4f18\u4e8e\u4eba\u7c7b\uff0c\u5e76\u5728\u516d\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f73\u3002\u4eba\u7c7b\u66f4\u503e\u5411\u4e8e\u6269\u5c55\u521b\u9020\u529b\uff08E-creativity\uff09\uff0c\u800cLLMs\u5219\u504f\u5411\u56fa\u5b9a\u521b\u9020\u529b\uff08F-creativity\uff09\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4eba\u7c7b\u548cLLMs\u5728\u8bed\u8a00\u521b\u9020\u529b\u65b9\u9762\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u4e24\u8005\u5728\u751f\u6210\u65b0\u8bcd\u548c\u77ed\u8bed\u65f6\u7684\u5dee\u5f02\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u8bcd\u5f62\u53d8\u5316\uff08\u6d3e\u751f\u548c\u590d\u5408\uff09\u548c\u9690\u55bb\u8bed\u8a00\u4f7f\u7528\u7684\u4efb\u52a1\uff0c\u5bf924\u540d\u4eba\u7c7b\u548c24\u4e2aLLMs\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528OCSAI\u5de5\u5177\u81ea\u52a8\u8bc4\u4f30\u7b54\u6848\u7684\u539f\u521b\u6027\u3001\u7cbe\u7ec6\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "result": "LLMs\u5728\u6240\u6709\u8bc4\u4f30\u6807\u51c6\uff08\u539f\u521b\u6027\u3001\u7cbe\u7ec6\u5ea6\u548c\u7075\u6d3b\u6027\uff09\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u5e76\u5728\u516b\u9879\u4efb\u52a1\u4e2d\u7684\u516d\u9879\u4e2d\u8868\u73b0\u66f4\u4f73\u3002\u4eba\u7c7b\u66f4\u503e\u5411\u4e8eE-creativity\uff0c\u800cLLMs\u504f\u5411F-creativity\u3002", "conclusion": "\u7814\u7a76\u8868\u660eLLMs\u5728\u8bed\u8a00\u521b\u9020\u529b\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u4eba\u7c7b\u548cLLMs\u5728\u521b\u9020\u529b\u7c7b\u578b\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u4eba\u7c7b\u66f4\u503e\u5411\u4e8e\u6269\u5c55\u6027\u521b\u9020\u529b\uff0c\u800cLLMs\u66f4\u504f\u5411\u56fa\u5b9a\u6a21\u5f0f\u3002"}}
{"id": "2507.11818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11818", "abs": "https://arxiv.org/abs/2507.11818", "authors": ["Andrei Rekesh", "Miruna Cretu", "Dmytro Shevchuk", "Vignesh Ram Somnath", "Pietro Li\u00f2", "Robert A. Batey", "Mike Tyers", "Micha\u0142 Koziarski", "Cheng-Hao Liu"], "title": "SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling", "comment": null, "summary": "Ensuring synthesizability in generative small molecule design remains a major\nchallenge. While recent developments in synthesizable molecule generation have\ndemonstrated promising results, these efforts have been largely confined to 2D\nmolecular graph representations, limiting the ability to perform geometry-based\nconditional generation. In this work, we present SynCoGen (Synthesizable\nCo-Generation), a single framework that combines simultaneous masked graph\ndiffusion and flow matching for synthesizable 3D molecule generation. SynCoGen\nsamples from the joint distribution of molecular building blocks, chemical\nreactions, and atomic coordinates. To train the model, we curated SynSpace, a\ndataset containing over 600K synthesis-aware building block graphs and 3.3M\nconformers. SynCoGen achieves state-of-the-art performance in unconditional\nsmall molecule graph and conformer generation, and the model delivers\ncompetitive performance in zero-shot molecular linker design for protein ligand\ngeneration in drug discovery. Overall, this multimodal formulation represents a\nfoundation for future applications enabled by non-autoregressive molecular\ngeneration, including analog expansion, lead optimization, and direct structure\nconditioning.", "AI": {"tldr": "SynCoGen\u662f\u4e00\u4e2a\u7ed3\u5408\u63a9\u7801\u56fe\u6269\u6563\u548c\u6d41\u5339\u914d\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u5408\u6210\u76843D\u5206\u5b50\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e2D\u5206\u5b50\u56fe\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u751f\u6210\u53ef\u5408\u6210\u7684\u5c0f\u5206\u5b50\u662f\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5c40\u9650\u4e8e2D\u5206\u5b50\u56fe\uff0c\u65e0\u6cd5\u5b9e\u73b0\u57fa\u4e8e\u51e0\u4f55\u7684\u6761\u4ef6\u751f\u6210\u3002", "method": "SynCoGen\u901a\u8fc7\u8054\u5408\u91c7\u6837\u5206\u5b50\u6784\u5efa\u5757\u3001\u5316\u5b66\u53cd\u5e94\u548c\u539f\u5b50\u5750\u6807\u7684\u5206\u5e03\uff0c\u7ed3\u5408\u63a9\u7801\u56fe\u6269\u6563\u548c\u6d41\u5339\u914d\u6280\u672f\uff0c\u751f\u62103D\u5206\u5b50\u3002", "result": "SynCoGen\u5728\u65e0\u6761\u4ef6\u5c0f\u5206\u5b50\u56fe\u548c\u6784\u8c61\u751f\u6210\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728\u96f6\u5c04\u51fb\u5206\u5b50\u8fde\u63a5\u8bbe\u8ba1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SynCoGen\u4e3a\u672a\u6765\u975e\u81ea\u56de\u5f52\u5206\u5b50\u751f\u6210\u5e94\u7528\uff08\u5982\u7c7b\u4f3c\u7269\u6269\u5c55\u548c\u76f4\u63a5\u7ed3\u6784\u6761\u4ef6\u5316\uff09\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.12059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12059", "abs": "https://arxiv.org/abs/2507.12059", "authors": ["Anthony G Cohn", "Robert E Blackwell"], "title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited", "comment": "8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop\n  on Qualitative Reasoning at IJCAI", "summary": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8628\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65b9\u5411\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u65b0\u578b\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e5f\u65e0\u6cd5\u5b8c\u5168\u6b63\u786e\u5b8c\u6210\u4efb\u52a1\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u65b9\u5411\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u6a21\u677f\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\u5176\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u6a21\u677f\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd528\u79cdLLMs\uff0c\u6d4b\u8bd5\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u786e\u5b9a\u6b63\u786e\u65b9\u5411\u7684\u80fd\u529b\u3002", "result": "\u5373\u4f7f\u662f\u6700\u65b0\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e5f\u65e0\u6cd5\u5728\u6240\u6709\u95ee\u9898\u4e0a\u53ef\u9760\u5730\u786e\u5b9a\u6b63\u786e\u65b9\u5411\u3002", "conclusion": "LLMs\u5728\u65b9\u5411\u63a8\u7406\u4efb\u52a1\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u5176\u80fd\u529b\u3002"}}
{"id": "2507.12064", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12064", "abs": "https://arxiv.org/abs/2507.12064", "authors": ["Jeremi K. Ochab", "Mateusz Matias", "Tymoteusz Boba", "Tomasz Walkowiak"], "title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features", "comment": null, "summary": "This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u5757\u5316\u98ce\u683c\u6d4b\u91cf\u7ba1\u9053\u7684\u4e8c\u8fdb\u5236AI\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528spaCy\u6a21\u578b\u9884\u5904\u7406\u6587\u672c\u5e76\u63d0\u53d6\u7279\u5f81\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u68af\u5ea6\u63d0\u5347\u673a\u4f5c\u4e3a\u5206\u7c7b\u5668\uff0c\u8bad\u7ec3\u96c6\u5305\u542b50\u4e07\u6761\u673a\u5668\u751f\u6210\u6587\u672c\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u975e\u795e\u7ecf\u7f51\u7edc\u7684\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u4f46\u53ef\u89e3\u91ca\u7684AI\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\u3002", "method": "\u4f7f\u7528spaCy\u6a21\u578b\u8fdb\u884c\u6587\u672c\u9884\u5904\u7406\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u68af\u5ea6\u63d0\u5347\u673a\u4f5c\u4e3a\u5206\u7c7b\u5668\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u4f18\u5316\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "result": "\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5305\u542b50\u4e07\u6761\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u4f18\u5316\u4e86\u5206\u7c7b\u5668\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5ef6\u7eed\u4e86\u975e\u795e\u7ecf\u7f51\u7edc\u3001\u4f4e\u6210\u672c\u4e14\u53ef\u89e3\u91ca\u7684AI\u68c0\u6d4b\u7b56\u7565\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.11836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11836", "abs": "https://arxiv.org/abs/2507.11836", "authors": ["Jian Gao", "Jianshe Wu", "JingYi Ding"], "title": "HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction", "comment": null, "summary": "Dynamic link prediction in continuous-time dynamic graphs is a fundamental\ntask for modeling evolving complex systems. Existing node-centric and\nevent-centric methods focus on individual interactions or atomic states,\nfailing to capture the structural cohesion of composite hyper-events, groups of\ncausally related events. To address this, we propose HyperEvent, a framework\nreframing dynamic link prediction as hyper-event recognition. Central to\nHyperEvent is the dynamic construction of an association sequence using event\ncorrelation vectors. These vectors quantify pairwise dependencies between the\nquery event and relevant historical events, thereby characterizing the\nstructural cohesion of a potential hyper-event. The framework predicts the\noccurrence of the query event by evaluating whether it collectively forms a\nvalid hyper-event with these historical events. Notably, HyperEvent outperforms\nstate-of-the-art methods on 4 out of 5 datasets in the official leaderboard.\nFor scalability, we further introduce an efficient parallel training algorithm\nthat segments large event streams to enable concurrent training. Experiments\nvalidate HyperEvent's superior accuracy and efficiency on large-scale graphs.\nAmong which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank\nover state-of-the-art baseline on the large-scale Flight dataset while\nutilizing only 10.17% of the training time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHyperEvent\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u52a8\u6001\u94fe\u63a5\u9884\u6d4b\u91cd\u6784\u4e3a\u8d85\u4e8b\u4ef6\u8bc6\u522b\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u5173\u6027\u5411\u91cf\u52a8\u6001\u6784\u5efa\u5173\u8054\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6355\u6349\u590d\u5408\u8d85\u4e8b\u4ef6\u7684\u7ed3\u6784\u51dd\u805a\u529b\uff0c\u9650\u5236\u4e86\u52a8\u6001\u56fe\u5efa\u6a21\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faHyperEvent\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u5173\u6027\u5411\u91cf\u52a8\u6001\u6784\u5efa\u5173\u8054\u5e8f\u5217\uff0c\u8bc4\u4f30\u67e5\u8be2\u4e8b\u4ef6\u4e0e\u5386\u53f2\u4e8b\u4ef6\u662f\u5426\u5f62\u6210\u6709\u6548\u8d85\u4e8b\u4ef6\u3002", "result": "\u57285\u4e2a\u6570\u636e\u96c6\u4e2d4\u4e2a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5927\u89c4\u6a21Flight\u6570\u636e\u96c6\u4e0aMRR\u63d0\u53476.95%\uff0c\u8bad\u7ec3\u65f6\u95f4\u4ec5\u970010.17%\u3002", "conclusion": "HyperEvent\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u52a8\u6001\u56fe\u5efa\u6a21\u3002"}}
{"id": "2507.12075", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12075", "abs": "https://arxiv.org/abs/2507.12075", "authors": ["Giuliano Martinelli", "Tommaso Bonomo", "Pere-Llu\u00eds Huguet Cabot", "Roberto Navigli"], "title": "BOOKCOREF: Coreference Resolution at Book Scale", "comment": "Accepted to ACL 2025 Main Conference. 19 pages", "summary": "Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u6807\u6ce8\u7ba1\u9053\uff0c\u521b\u5efa\u4e86\u9996\u4e2a\u4e66\u7c4d\u89c4\u6a21\u7684\u5171\u6307\u6d88\u89e3\u57fa\u51c6BOOKCOREF\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u7684\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u5171\u6307\u6d88\u89e3\u57fa\u51c6\u5728\u957f\u6587\u672c\u8bc4\u4f30\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u7cfb\u7edf\u5728\u4e66\u7c4d\u89c4\u6a21\uff08\u6570\u5341\u4e07token\uff09\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u6807\u6ce8\u7ba1\u9053\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5168\u53d9\u4e8b\u6587\u672c\u5171\u6307\u6d88\u89e3\u6807\u6ce8\uff0c\u5e76\u521b\u5efa\u4e86BOOKCOREF\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\u7a33\u5065\uff0cBOOKCOREF\u4f7f\u5f53\u524d\u957f\u6587\u6863\u5171\u6307\u6d88\u89e3\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe20 CoNLL-F1\u5206\u3002", "conclusion": "\u4e66\u7c4d\u89c4\u6a21\u5171\u6307\u6d88\u89e3\u5e26\u6765\u4e86\u65b0\u6311\u6218\uff0c\u5f53\u524d\u6a21\u578b\u8868\u73b0\u4e0d\u5982\u77ed\u6587\u6863\uff0c\u7814\u7a76\u9f13\u52b1\u5f00\u53d1\u65b0\u7cfb\u7edf\u3002"}}
{"id": "2507.11839", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11839", "abs": "https://arxiv.org/abs/2507.11839", "authors": ["Chengyue Gong", "Xinshi Chen", "Yuxuan Zhang", "Yuxuan Song", "Hao Zhou", "Wenzhi Xiao"], "title": "Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM", "comment": null, "summary": "Lightweight inference is critical for biomolecular structure prediction and\nother downstream tasks, enabling efficient real-world deployment and\ninference-time scaling for large-scale applications. In this work, we address\nthe challenge of balancing model efficiency and prediction accuracy by making\nseveral key modifications, 1) Multi-step AF3 sampler is replaced by a few-step\nODE sampler, significantly reducing computational overhead for the diffusion\nmodule part during inference; 2) In the open-source Protenix framework, a\nsubset of pairformer or diffusion transformer blocks doesn't make contributions\nto the final structure prediction, presenting opportunities for architectural\npruning and lightweight redesign; 3) A model incorporating an ESM module is\ntrained to substitute the conventional MSA module, reducing MSA preprocessing\ntime. Building on these key insights, we present Protenix-Mini, a compact and\noptimized model designed for efficient protein structure prediction. This\nstreamlined version incorporates a more efficient architectural design with a\ntwo-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating\nredundant Transformer components and refining the sampling process,\nProtenix-Mini significantly reduces model complexity with slight accuracy drop.\nEvaluations on benchmark datasets demonstrate that it achieves high-fidelity\npredictions, with only a negligible 1 to 5 percent decrease in performance on\nbenchmark datasets compared to its full-scale counterpart. This makes\nProtenix-Mini an ideal choice for applications where computational resources\nare limited but accurate structure prediction remains crucial.", "AI": {"tldr": "Protenix-Mini\u901a\u8fc7\u4f18\u5316\u67b6\u6784\u548c\u91c7\u6837\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\uff0c\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u964d\u4f4e\uff0c\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u5206\u5b50\u7ed3\u6784\u9884\u6d4b\u4e2d\u6a21\u578b\u6548\u7387\u4e0e\u9884\u6d4b\u7cbe\u5ea6\u5e73\u8861\u7684\u6311\u6218\u3002", "method": "1) \u7528\u5c11\u6b65ODE\u91c7\u6837\u5668\u66ff\u4ee3\u591a\u6b65AF3\u91c7\u6837\u5668\uff1b2) \u526a\u679d\u5197\u4f59Transformer\u6a21\u5757\uff1b3) \u7528ESM\u6a21\u5757\u66ff\u4ee3\u4f20\u7edfMSA\u6a21\u5757\u3002", "result": "Protenix-Mini\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4ec5\u4e0b\u964d1-5%\uff0c\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "Protenix-Mini\u662f\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u4f46\u9700\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u7684\u7406\u60f3\u9009\u62e9\u3002"}}
{"id": "2507.12079", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12079", "abs": "https://arxiv.org/abs/2507.12079", "authors": ["Tosin Adewumi", "Foteini Simistira Liwicki", "Marcus Liwicki", "Viktor Gardelli", "Lama Alkhaled", "Hamam Mokayed"], "title": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning", "comment": "This paper was accepted for the special issue AI for Education by the\n  IEEE Signal Processing Magazine journal", "summary": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u7ed3\u5408\u82cf\u683c\u62c9\u5e95\u6cd5\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u7b80\u5316\u6e38\u620f\u5316\u548c\u5f62\u6210\u6027\u53cd\u9988\u7684MEGA\u65b9\u6cd5\u5bf9\u5927\u5b66\u751f\u6570\u5b66\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u4f18\u4e8e\u4f20\u7edf\u9010\u6b65\u6cd5\u3002", "motivation": "\u8bb8\u591a\u5b66\u751f\u5728\u6570\u5b66\u5b66\u4e60\u4e2d\u9047\u5230\u56f0\u96be\uff0c\u5bfc\u81f4\u4ed6\u4eec\u56de\u907f\u6570\u5b66\u76f8\u5173\u5b66\u79d1\uff0c\u800c\u4f20\u7edf\u6559\u5b66\u65b9\u6cd5\u53ef\u80fd\u4e0d\u591f\u6709\u6548\u3002", "method": "\u91c7\u7528\u7ec4\u5185\u8bbe\u8ba1\uff0c\u968f\u673a\u5206\u914d\u95ee\u9898\uff0c\u6bd4\u8f83MEGA\u65b9\u6cd5\u4e0e\u4f20\u7edf\u7684\u9010\u6b65\u6cd5\uff08CoT\uff09\uff0c\u4f7f\u7528GSM8K\u548cMATH\u6570\u636e\u96c6\u8bc4\u4f30\u4e24\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT4o\u548cClaude 3.5 Sonnet\uff09\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cMEGA\u65b9\u6cd5\u5728\u4e24\u79cd\u6570\u636e\u96c6\u4e0a\u5747\u88ab\u5b66\u751f\u8ba4\u4e3a\u66f4\u6709\u5229\u4e8e\u5b66\u4e60\uff0c\u5c24\u5176\u5728\u8f83\u96be\u7684MATH\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u66f4\u4f18\uff0847.5% vs 26.67%\uff09\u3002", "conclusion": "MEGA\u65b9\u6cd5\u5728\u89e3\u91ca\u590d\u6742\u6570\u5b66\u95ee\u9898\u65f6\u8868\u73b0\u66f4\u4f73\uff0c\u662f\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6570\u5b66\u6559\u5b66\u5de5\u5177\u3002"}}
{"id": "2507.11847", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11847", "abs": "https://arxiv.org/abs/2507.11847", "authors": ["Yu-Jie Zhang", "Sheng-An Xu", "Peng Zhao", "Masashi Sugiyama"], "title": "Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update", "comment": null, "summary": "We study the generalized linear bandit (GLB) problem, a contextual\nmulti-armed bandit framework that extends the classical linear model by\nincorporating a non-linear link function, thereby modeling a broad class of\nreward distributions such as Bernoulli and Poisson. While GLBs are widely\napplicable to real-world scenarios, their non-linear nature introduces\nsignificant challenges in achieving both computational and statistical\nefficiency. Existing methods typically trade off between two objectives, either\nincurring high per-round costs for optimal regret guarantees or compromising\nstatistical efficiency to enable constant-time updates. In this paper, we\npropose a jointly efficient algorithm that attains a nearly optimal regret\nbound with $\\mathcal{O}(1)$ time and space complexities per round. The core of\nour method is a tight confidence set for the online mirror descent (OMD)\nestimator, which is derived through a novel analysis that leverages the notion\nof mix loss from online prediction. The analysis shows that our OMD estimator,\neven with its one-pass updates, achieves statistical efficiency comparable to\nmaximum likelihood estimation, thereby leading to a jointly efficient\noptimistic method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7ebf\u6027\u8001\u864e\u673a\u95ee\u9898\u7684\u8054\u5408\u9ad8\u6548\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u9057\u61be\u8fb9\u754c\uff0c\u5e76\u5177\u6709\u6bcf\u8f6eO(1)\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u5e7f\u4e49\u7ebf\u6027\u8001\u864e\u673a\u95ee\u9898\u56e0\u5176\u975e\u7ebf\u6027\u7279\u6027\u5728\u8ba1\u7b97\u548c\u7edf\u8ba1\u6548\u7387\u4e0a\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5728\u4e24\u8005\u4e4b\u95f4\u6743\u8861\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u955c\u50cf\u4e0b\u964d\uff08OMD\uff09\u4f30\u8ba1\u5668\u6784\u5efa\u7d27\u5bc6\u7f6e\u4fe1\u96c6\uff0c\u5229\u7528\u6df7\u5408\u635f\u5931\u6982\u5ff5\u8fdb\u884c\u65b0\u9896\u5206\u6790\u3002", "result": "\u7b97\u6cd5\u5728\u7edf\u8ba1\u6548\u7387\u4e0a\u63a5\u8fd1\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u8054\u5408\u9ad8\u6548\u4f18\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u548c\u7edf\u8ba1\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2507.12126", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12126", "abs": "https://arxiv.org/abs/2507.12126", "authors": ["Payal Bhattad", "Sai Manoj Pudukotai Dinakarrao", "Anju Gupta"], "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis", "comment": null, "summary": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6587\u672c\u589e\u5f3a\u7684\u6846\u67b6\uff0c\u5305\u62ec\u53ef\u6269\u5c55\u6027\u5206\u6790\u548c\u8fed\u4ee3\u589e\u5f3a\u4e0e\u6458\u8981\u7ec6\u5316\uff08IASR\uff09\uff0c\u9a8c\u8bc1\u4e86GPT-3.5 Turbo\u5728\u8bed\u4e49\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u751f\u6210\u6548\u7387\u4e0a\u7684\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u589e\u5f3a\u6280\u672f\u5728\u8bed\u4e49\u4fdd\u5b58\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6216\u8fed\u4ee3\u751f\u6210\u65f6\u5bfc\u81f4\u7684\u5197\u4f59\u548c\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u8bc4\u4f30\u7ec4\u4ef6\uff1a\u53ef\u6269\u5c55\u6027\u5206\u6790\u548cIASR\uff0c\u7528\u4e8e\u6d4b\u91cf\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u9012\u5f52\u91ca\u4e49\u4e2d\u7684\u8bed\u4e49\u6f02\u79fb\u3002", "result": "GPT-3.5 Turbo\u8868\u73b0\u6700\u4f73\uff0c\u5e94\u7528\u4e8eBERTopic\u4efb\u52a1\u65f6\uff0c\u4e3b\u9898\u7c92\u5ea6\u589e\u52a0400%\u4e14\u5b8c\u5168\u6d88\u9664\u4e3b\u9898\u91cd\u53e0\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u5728\u5b9e\u9645NLP\u6d41\u7a0b\u4e2d\u8bc4\u4f30LLM\u589e\u5f3a\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.11855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11855", "abs": "https://arxiv.org/abs/2507.11855", "authors": ["Davin Hill", "Brian L. Hill", "Aria Masoomi", "Vijay S. Nori", "Robert E. Tillman", "Jennifer Dy"], "title": "OrdShap: Feature Position Importance for Sequential Black-Box Models", "comment": null, "summary": "Sequential deep learning models excel in domains with temporal or sequential\ndependencies, but their complexity necessitates post-hoc feature attribution\nmethods for understanding their predictions. While existing techniques quantify\nfeature importance, they inherently assume fixed feature ordering - conflating\nthe effects of (1) feature values and (2) their positions within input\nsequences. To address this gap, we introduce OrdShap, a novel attribution\nmethod that disentangles these effects by quantifying how a model's predictions\nchange in response to permuting feature position. We establish a game-theoretic\nconnection between OrdShap and Sanchez-Berganti\\~nos values, providing a\ntheoretically grounded approach to position-sensitive attribution. Empirical\nresults from health, natural language, and synthetic datasets highlight\nOrdShap's effectiveness in capturing feature value and feature position\nattributions, and provide deeper insight into model behavior.", "AI": {"tldr": "OrdShap\u662f\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u7279\u5f81\u4f4d\u7f6e\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u6df7\u6dc6\u7279\u5f81\u503c\u548c\u4f4d\u7f6e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u5047\u8bbe\u7279\u5f81\u987a\u5e8f\u56fa\u5b9a\uff0c\u65e0\u6cd5\u533a\u5206\u7279\u5f81\u503c\u548c\u4f4d\u7f6e\u5bf9\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faOrdShap\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f6e\u6362\u7279\u5f81\u4f4d\u7f6e\u91cf\u5316\u5176\u5bf9\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u4e0eSanchez-Berganti\u00f1os\u503c\u5efa\u7acb\u535a\u5f08\u8bba\u8054\u7cfb\u3002", "result": "\u5728\u5065\u5eb7\u3001\u81ea\u7136\u8bed\u8a00\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOrdShap\u80fd\u6709\u6548\u6355\u6349\u7279\u5f81\u503c\u548c\u4f4d\u7f6e\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u6a21\u578b\u884c\u4e3a\u6d1e\u5bdf\u3002", "conclusion": "OrdShap\u4e3a\u4f4d\u7f6e\u654f\u611f\u7684\u7279\u5f81\u5f52\u56e0\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u5de5\u5177\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.12143", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12143", "abs": "https://arxiv.org/abs/2507.12143", "authors": ["Pavel \u0160indel\u00e1\u0159", "Ond\u0159ej Bojar"], "title": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators", "comment": "30 pages, 7 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum", "summary": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.", "AI": {"tldr": "ELOQUENT\u7684Sensemaking\u4efb\u52a1\u65e8\u5728\u901a\u8fc7\u6559\u5e08\u3001\u5b66\u751f\u548c\u8bc4\u4f30\u8005\u7cfb\u7edf\u7684\u4e09\u6b65\u6d4b\u8bd5\u8bc4\u4f30\u751f\u6210\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u95ee\u9898\u751f\u6210\u3001\u7b54\u6848\u9650\u5236\u548c\u8bc4\u4f30\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002", "motivation": "\u4e3a\u751f\u6210\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u53ef\u6d4b\u8bd5\u7684\u9ad8\u6807\u51c6\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u8bfe\u5802\u8003\u8bd5\u7684\u4e09\u6b65\u6d41\u7a0b\uff08\u95ee\u9898\u751f\u6210\u3001\u56de\u7b54\u548c\u8bc4\u5206\uff09\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u6587\u672c\u7406\u89e3\u80fd\u529b\u3002", "method": "\u4f7f\u75287\u79cd\u6765\u6e90\u7684\u6d4b\u8bd5\u6750\u6599\uff08\u5982\u4e8b\u5b9e\u6838\u67e5\u3001\u6559\u79d1\u4e66\u7b49\uff09\uff0c4\u4e2a\u56e2\u961f\u63d0\u4ea4\u4e86\u6559\u5e08\u3001\u5b66\u751f\u548c\u8bc4\u4f30\u8005\u7cfb\u7edf\uff0c\u5e76\u5bf9\u6bd4\u4e86\u81ea\u52a8\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u95ee\u9898\u751f\u6210\u8d28\u91cf\u8bc4\u4f30\u3001\u7b54\u6848\u9650\u5236\u548cLLM\u8bc4\u4f30\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u5c24\u5176\u662fLLM-as-a-Judge\u8303\u5f0f\u6613\u8bef\u5224\u3002", "conclusion": "\u9700\u8981\u6539\u8fdb\u95ee\u9898\u751f\u6210\u548c\u7b54\u6848\u9650\u5236\u7684\u8bc4\u4f30\u7b56\u7565\uff0c\u5e76\u4f18\u5316LLM\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.11865", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11865", "abs": "https://arxiv.org/abs/2507.11865", "authors": ["Hanwen Dai", "Chang Gao", "Fang He", "Congyuan Ji", "Yanni Yang"], "title": "A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers", "comment": null, "summary": "The rapid expansion of platform integration has emerged as an effective\nsolution to mitigate market fragmentation by consolidating multiple\nride-hailing platforms into a single application. To address heterogeneous\npassenger preferences, third-party integrators provide Discount Express service\ndelivered by express drivers at lower trip fares. For the individual platform,\nencouraging broader participation of drivers in Discount Express services has\nthe potential to expand the accessible demand pool and improve matching\nefficiency, but often at the cost of reduced profit margins. This study aims to\ndynamically manage drivers' acceptance of Discount Express from the perspective\nof individual platforms. The lack of historical data under the new business\nmodel necessitates online learning. However, early-stage exploration through\ntrial and error can be costly in practice, highlighting the need for reliable\nearly-stage performance in real-world deployment. To address these challenges,\nthis study formulates the decision regarding the proportion of drivers'\nacceptance behavior as a continuous control task. In response to the high\nstochasticity, the opaque matching mechanisms employed by third-party\nintegrator, and the limited availability of historical data, we propose a\npolicy-improved deep deterministic policy gradient (pi-DDPG) framework. The\nproposed framework incorporates a refiner module to boost policy performance\nduring the early training phase, leverages a convolutional long short-term\nmemory network to effectively capture complex spatiotemporal patterns, and\nadopts a prioritized experience replay mechanism to enhance learning\nefficiency. A simulator based on a real-world dataset is developed to validate\nthe effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate\nthat pi-DDPG achieves superior learning efficiency and significantly reduces\nearly-stage training losses.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u7ba1\u7406\u53f8\u673a\u63a5\u53d7\u6298\u6263\u5feb\u8f66\u670d\u52a1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7pi-DDPG\u6846\u67b6\u4f18\u5316\u51b3\u7b56\uff0c\u63d0\u5347\u65e9\u671f\u5b66\u4e60\u6548\u7387\u548c\u5339\u914d\u6027\u80fd\u3002", "motivation": "\u5e73\u53f0\u6574\u5408\u4e2d\uff0c\u53f8\u673a\u53c2\u4e0e\u6298\u6263\u5feb\u8f66\u670d\u52a1\u867d\u80fd\u6269\u5927\u9700\u6c42\u6c60\u548c\u63d0\u9ad8\u5339\u914d\u6548\u7387\uff0c\u4f46\u4f1a\u964d\u4f4e\u5229\u6da6\u3002\u7f3a\u4e4f\u5386\u53f2\u6570\u636e\u548c\u9ad8\u968f\u673a\u6027\u589e\u52a0\u4e86\u7ba1\u7406\u96be\u5ea6\u3002", "method": "\u91c7\u7528pi-DDPG\u6846\u67b6\uff0c\u5305\u542b\u6539\u8fdb\u6a21\u5757\u3001\u5377\u79efLSTM\u7f51\u7edc\u548c\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u673a\u5236\uff0c\u4ee5\u52a8\u6001\u7ba1\u7406\u53f8\u673a\u63a5\u53d7\u884c\u4e3a\u3002", "result": "pi-DDPG\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5b66\u4e60\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u65e9\u671f\u8bad\u7ec3\u635f\u5931\u3002", "conclusion": "pi-DDPG\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5e73\u53f0\u5728\u7f3a\u4e4f\u5386\u53f2\u6570\u636e\u548c\u9ad8\u968f\u673a\u6027\u4e0b\u7684\u52a8\u6001\u7ba1\u7406\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.12208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12208", "abs": "https://arxiv.org/abs/2507.12208", "authors": ["Michael Carl", "Takanori Mizowaki", "Aishvarya Ray", "Masaru Yamada", "Devi Sri Bandaru", "Xinyue Ren"], "title": "Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production", "comment": null, "summary": "The paper introduces a Behavioural Translation Style Space (BTSS) that\ndescribes possible behavioural translation patterns. The suggested BTSS is\norganized as a hierarchical structure that entails various embedded processing\nlayers. We posit that observable translation behaviour - i.e., eye and finger\nmovements - is fundamental when executing the physical act of translation but\nit is caused and shaped by higher-order cognitive processes and affective\ntranslation states. We analyse records of keystrokes and gaze data as\nindicators of the hidden mental processing structure and organize the\nbehavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the\nbasis for a computational translation agent to simulate the temporal dynamics\nof affect, automatized behaviour and cognition during human translation\nproduction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u884c\u4e3a\u7ffb\u8bd1\u98ce\u683c\u7a7a\u95f4\uff08BTSS\uff09\uff0c\u7528\u4e8e\u63cf\u8ff0\u53ef\u80fd\u7684\u7ffb\u8bd1\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u51fb\u952e\u548c\u6ce8\u89c6\u6570\u636e\u63ed\u793a\u9690\u85cf\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u89c2\u5bdf\u7ffb\u8bd1\u884c\u4e3a\uff08\u5982\u773c\u52a8\u548c\u624b\u6307\u52a8\u4f5c\uff09\u6765\u7406\u89e3\u80cc\u540e\u7684\u9ad8\u9636\u8ba4\u77e5\u548c\u60c5\u611f\u72b6\u6001\u3002", "method": "\u901a\u8fc7\u5206\u6790\u51fb\u952e\u548c\u6ce8\u89c6\u6570\u636e\uff0c\u6784\u5efa\u591a\u5c42\u5d4c\u5165\u7684BTSS\uff0c\u4f5c\u4e3a\u8ba1\u7b97\u7ffb\u8bd1\u4ee3\u7406\u7684\u57fa\u7840\u3002", "result": "BTSS\u80fd\u591f\u6a21\u62df\u7ffb\u8bd1\u8fc7\u7a0b\u4e2d\u60c5\u611f\u3001\u81ea\u52a8\u884c\u4e3a\u548c\u8ba4\u77e5\u7684\u65f6\u95f4\u52a8\u6001\u3002", "conclusion": "BTSS\u4e3a\u7406\u89e3\u7ffb\u8bd1\u884c\u4e3a\u53ca\u5176\u80cc\u540e\u7684\u8ba4\u77e5\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u4e3a\u8ba1\u7b97\u7ffb\u8bd1\u4ee3\u7406\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.11901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11901", "abs": "https://arxiv.org/abs/2507.11901", "authors": ["Juscimara G. Avelino", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "Imbalanced Regression Pipeline Recommendation", "comment": null, "summary": "Imbalanced problems are prevalent in various real-world scenarios and are\nextensively explored in classification tasks. However, they also present\nchallenges for regression tasks due to the rarity of certain target values. A\ncommon alternative is to employ balancing algorithms in preprocessing to\naddress dataset imbalance. However, due to the variety of resampling methods\nand learning models, determining the optimal solution requires testing many\ncombinations. Furthermore, the learning model, dataset, and evaluation metric\naffect the best strategies. This work proposes the Meta-learning for Imbalanced\nRegression (Meta-IR) framework, which diverges from existing literature by\ntraining meta-classifiers to recommend the best pipeline composed of the\nresampling strategy and learning model per task in a zero-shot fashion. The\nmeta-classifiers are trained using a set of meta-features to learn how to map\nthe meta-features to the classes indicating the best pipeline. We propose two\nformulations: Independent and Chained. Independent trains the meta-classifiers\nto separately indicate the best learning algorithm and resampling strategy.\nChained involves a sequential procedure where the output of one meta-classifier\nis used as input for another to model intrinsic relationship factors. The\nChained scenario showed superior performance, suggesting a relationship between\nthe learning algorithm and the resampling strategy per task. Compared with\nAutoML frameworks, Meta-IR obtained better results. Moreover, compared with\nbaselines of six learning algorithms and six resampling algorithms plus no\nresampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of\nthem. The code, data, and further information of the experiments can be found\non GitHub: https://github.com/JusciAvelino/Meta-IR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMeta-IR\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u8bad\u7ec3\u5143\u5206\u7c7b\u5668\u63a8\u8350\u6700\u4f73\u7684\u6570\u636e\u9884\u5904\u7406\u548c\u6a21\u578b\u7ec4\u5408\u3002", "motivation": "\u56de\u5f52\u4efb\u52a1\u4e2d\u76ee\u6807\u503c\u7684\u4e0d\u5e73\u8861\u95ee\u9898\u7f3a\u4e4f\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7ec4\u5408\u6d4b\u8bd5\u3002", "method": "\u63d0\u51fa\u72ec\u7acb\u548c\u94fe\u5f0f\u4e24\u79cd\u5143\u5206\u7c7b\u5668\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5206\u522b\u72ec\u7acb\u6216\u987a\u5e8f\u63a8\u8350\u6700\u4f73\u5b66\u4e60\u7b97\u6cd5\u548c\u91cd\u91c7\u6837\u7b56\u7565\u3002", "result": "\u94fe\u5f0f\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0cMeta-IR\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e42\u79cd\u57fa\u7ebf\u914d\u7f6e\u548cAutoML\u6846\u67b6\u3002", "conclusion": "Meta-IR\u901a\u8fc7\u5143\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.12217", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12217", "abs": "https://arxiv.org/abs/2507.12217", "authors": ["Reuben Smit", "Retief Louw", "Herman Kamper"], "title": "Towards few-shot isolated word reading assessment", "comment": "Accepted to SLaTE 2025", "summary": "We explore an ASR-free method for isolated word reading assessment in\nlow-resource settings. Our few-shot approach compares input child speech to a\nsmall set of adult-provided reference templates. Inputs and templates are\nencoded using intermediate layers from large self-supervised learned (SSL)\nmodels. Using an Afrikaans child speech benchmark, we investigate design\noptions such as discretising SSL features and barycentre averaging of the\ntemplates. Idealised experiments show reasonable performance for adults, but a\nsubstantial drop for child speech input, even with child templates. Despite the\nsuccess of employing SSL representations in low-resource speech tasks, our work\nhighlights the limitations of SSL representations for processing child data\nwhen used in a few-shot classification system.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u65e0\u9700\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u5b64\u7acb\u8bcd\u9605\u8bfb\u8bc4\u4f30\u65b9\u6cd5\uff0c\u91c7\u7528\u5c11\u91cf\u6837\u672c\u5bf9\u6bd4\u513f\u7ae5\u8bed\u97f3\u4e0e\u6210\u4eba\u53c2\u8003\u6a21\u677f\uff0c\u53d1\u73b0\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u8868\u793a\u5728\u5904\u7406\u513f\u7ae5\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\uff0c\u4f20\u7edfASR\u65b9\u6cd5\u53ef\u80fd\u4e0d\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u513f\u7ae5\u8bed\u97f3\u7684\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u5c11\u91cf\u6837\u672c\u5bf9\u6bd4\u513f\u7ae5\u8bed\u97f3\u4e0e\u6210\u4eba\u53c2\u8003\u6a21\u677f\uff0c\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u7f16\u7801\uff0c\u5e76\u7814\u7a76\u79bb\u6563\u5316SSL\u7279\u5f81\u548c\u6a21\u677f\u91cd\u5fc3\u5e73\u5747\u7b49\u8bbe\u8ba1\u9009\u9879\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u6210\u4eba\u8bed\u97f3\u8868\u73b0\u826f\u597d\uff0c\u4f46\u513f\u7ae5\u8bed\u97f3\u8f93\u5165\u5373\u4f7f\u4f7f\u7528\u513f\u7ae5\u6a21\u677f\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5c3d\u7ba1SSL\u8868\u793a\u5728\u4f4e\u8d44\u6e90\u8bed\u97f3\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5c11\u91cf\u6837\u672c\u5206\u7c7b\u7cfb\u7edf\u4e2d\u5904\u7406\u513f\u7ae5\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2507.11948", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11948", "abs": "https://arxiv.org/abs/2507.11948", "authors": ["Carlo Baronio", "Pietro Marsella", "Ben Pan", "Simon Guo", "Silas Alberti"], "title": "Kevin: Multi-Turn RL for Generating CUDA Kernels", "comment": null, "summary": "Writing GPU kernels is a challenging task and critical for AI systems'\nefficiency. It is also highly iterative: domain experts write code and improve\nperformance through execution feedback. Moreover, it presents verifiable\nrewards like correctness and speedup, making it a natural environment to apply\nReinforcement Learning (RL). To explicitly incorporate the iterative nature of\nthis process into training, we develop a flexible multi-turn RL recipe that\naddresses unique challenges encountered in real-world settings, such as\nlearning from long trajectories and effective reward attribution across turns.\nWe present Kevin - K(ernel D)evin, the first model trained with multi-turn RL\nfor CUDA kernel generation and optimization. In our evaluation setup, Kevin\nshows significant gains over its base model (QwQ-32B), improving correctness of\ngenerated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to\n1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini\n(0.78x). Finally, we study its behavior across test-time scaling axes: we found\nscaling serial refinement more beneficial than parallel sampling. In\nparticular, when given more refinement turns, Kevin shows a higher rate of\nimprovement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5Kevin\uff0c\u7528\u4e8e\u751f\u6210\u548c\u4f18\u5316CUDA\u5185\u6838\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b63\u786e\u6027\u548c\u6027\u80fd\u3002", "motivation": "GPU\u5185\u6838\u7f16\u5199\u5bf9AI\u7cfb\u7edf\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5177\u6709\u6311\u6218\u6027\u4e14\u9700\u8981\u8fed\u4ee3\u4f18\u5316\uff0c\u56e0\u6b64\u9002\u5408\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u8f6eRL\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u957f\u8f68\u8ff9\u5b66\u4e60\u548c\u8de8\u8f6e\u5956\u52b1\u5206\u914d\u7b49\u73b0\u5b9e\u6311\u6218\u3002", "result": "Kevin\u5728\u6b63\u786e\u6027\uff0856%\u523082%\uff09\u548c\u6027\u80fd\uff080.53x\u52301.10x\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u524d\u6cbf\u6a21\u578b\u3002", "conclusion": "\u591a\u8f6eRL\u5728CUDA\u5185\u6838\u4f18\u5316\u4e2d\u6709\u6548\uff0c\u4e14\u4e32\u884c\u7ec6\u5316\u6bd4\u5e76\u884c\u91c7\u6837\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2507.11902", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11902", "abs": "https://arxiv.org/abs/2507.11902", "authors": ["Juscimara G. Avelino", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "Resampling strategies for imbalanced regression: a survey and empirical analysis", "comment": null, "summary": "Imbalanced problems can arise in different real-world situations, and to\naddress this, certain strategies in the form of resampling or balancing\nalgorithms are proposed. This issue has largely been studied in the context of\nclassification, and yet, the same problem features in regression tasks, where\ntarget values are continuous. This work presents an extensive experimental\nstudy comprising various balancing and predictive models, and wich uses metrics\nto capture important elements for the user and to evaluate the predictive model\nin an imbalanced regression data context. It also proposes a taxonomy for\nimbalanced regression approaches based on three crucial criteria: regression\nmodel, learning process, and evaluation metrics. The study offers new insights\ninto the use of such strategies, highlighting the advantages they bring to each\nmodel's learning process, and indicating directions for further studies. The\ncode, data and further information related to the experiments performed herein\ncan be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u56de\u5f52\u6a21\u578b\u3001\u5b66\u4e60\u8fc7\u7a0b\u548c\u8bc4\u4f30\u6307\u6807\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u5e73\u8861\u7b56\u7565\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u5e73\u8861\u56de\u5f52\u4efb\u52a1\u7684\u6311\u6218\uff0c\u586b\u8865\u5206\u7c7b\u4efb\u52a1\u4e2d\u4e0d\u5e73\u8861\u95ee\u9898\u7814\u7a76\u4e4b\u5916\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7814\u7a76\uff0c\u7ed3\u5408\u591a\u79cd\u5e73\u8861\u548c\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u7279\u5b9a\u6307\u6807\u8bc4\u4f30\u4e0d\u5e73\u8861\u56de\u5f52\u6570\u636e\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e0d\u5e73\u8861\u56de\u5f52\u7b56\u7565\u7684\u65b0\u89c1\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5bf9\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\u7684\u4f18\u52bf\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e0d\u5e73\u8861\u56de\u5f52\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2507.12252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12252", "abs": "https://arxiv.org/abs/2507.12252", "authors": ["Shilin Zhou", "Zhenghua Li"], "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language Models", "comment": null, "summary": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7c92\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8bcd\u7ea7\u548c\u77ed\u8bed\u7ea7\u878d\u5408\u7684\u4f18\u52bf\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u5347\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u4e2d\u5173\u952e\u8bcd\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u7aef\u5230\u7aefASR\u6a21\u578b\u5728\u901a\u7528\u8bed\u97f3\u8f6c\u5f55\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8bc6\u522b\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5173\u952e\u8bcd\uff08\u5982\u4e13\u6709\u540d\u8bcd\u6216\u7528\u6237\u7279\u5b9a\u5b9e\u4f53\uff09\u65f6\u4ecd\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7c92\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8bcd\u7ea7\u548c\u77ed\u8bed\u7ea7\u878d\u5408\uff0c\u5e76\u91c7\u7528\u540e\u878d\u5408\u7b56\u7565\u5c06ASR\u7684\u58f0\u5b66\u4fe1\u606f\u4e0eLLM\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u4e2d\u6587\u548c\u82f1\u6587\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5173\u952e\u8bcd\u76f8\u5173\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u975e\u5173\u952e\u8bcd\u6587\u672c\u7684\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\uff0c\u8bcd\u7ea7\u548c\u77ed\u8bed\u7ea7\u7ec4\u4ef6\u5747\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u663e\u8457\u8d21\u732e\uff0c\u4e8c\u8005\u5728\u591a\u7c92\u5ea6\u6846\u67b6\u4e2d\u4e92\u8865\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.11926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11926", "abs": "https://arxiv.org/abs/2507.11926", "authors": ["Max Hopkins", "Sihan Liu", "Christopher Ye", "Yuichi Yoshida"], "title": "From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning", "comment": "67 pages", "summary": "The epidemic failure of replicability across empirical science and machine\nlearning has recently motivated the formal study of replicable learning\nalgorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from\na fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the\ndesign of data-efficient replicable algorithms is now more or less understood.\nIn contrast, there remain significant gaps in our knowledge for control\nsettings like reinforcement learning where an agent must interact directly with\na shifting environment. Karbasi et. al show that with access to a generative\nmodel of an environment with $S$ states and $A$ actions (the RL 'batch\nsetting'), replicably learning a near-optimal policy costs only\n$\\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a\ngenerative model jumps to $\\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the\nsubstantial difficulty of environment exploration. This gap raises a key\nquestion in the broader theory of replicability: Is replicable exploration\ninherently more expensive than batch learning? Is sample-efficient replicable\nRL even possible?\n  In this work, we (nearly) resolve this problem (for low-horizon tabular\nMDPs): exploration is not a significant barrier to replicable learning! Our\nmain result is a replicable RL algorithm on $\\tilde{O}(S^2A)$ samples, bridging\nthe gap between the generative and episodic settings. We complement this with a\nmatching $\\tilde{\\Omega}(S^2A)$ lower bound in the generative setting (under\nthe common parallel sampling assumption) and an unconditional lower bound in\nthe episodic setting of $\\tilde{\\Omega}(S^2)$ showcasing the near-optimality of\nour algorithm with respect to the state space $S$.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u53ef\u590d\u5236\u5b66\u4e60\u7b97\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4f4e\u8303\u56f4\u8868\u683cMDP\u4e2d\u6837\u672c\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u586b\u8865\u4e86\u751f\u6210\u6a21\u578b\u4e0e\u4ea4\u4e92\u73af\u5883\u4e4b\u95f4\u7684\u6837\u672c\u6210\u672c\u5dee\u8ddd\u3002", "motivation": "\u89e3\u51b3\u53ef\u590d\u5236\u5b66\u4e60\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u6210\u672c\u95ee\u9898\uff0c\u586b\u8865\u73b0\u6709\u7406\u8bba\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u590d\u5236\u7684RL\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u4f4e\u8303\u56f4\u8868\u683cMDP\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e3a$\tilde{O}(S^2A)$\u3002", "result": "\u7b97\u6cd5\u5728\u751f\u6210\u6a21\u578b\u548c\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u6837\u672c\u6210\u672c\u63a5\u8fd1\u6700\u4f18\uff0c\u5e76\u63d0\u4f9b\u4e86\u5339\u914d\u7684\u4e0b\u754c\u3002", "conclusion": "\u63a2\u7d22\u5e76\u975e\u53ef\u590d\u5236\u5b66\u4e60\u7684\u663e\u8457\u969c\u788d\uff0c\u6837\u672c\u9ad8\u6548\u7684RL\u7b97\u6cd5\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2507.12260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12260", "abs": "https://arxiv.org/abs/2507.12260", "authors": ["Yikang Liu", "Wanyang Zhang", "Yiming Wang", "Jialong Tang", "Pei Zhang", "Baosong Yang", "Fei Huang", "Rui Wang", "Hai Hu"], "title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese", "comment": null, "summary": "In this paper, we propose the first quantitative measure for translationese\n-- the translationese-index (T-index) for graded and generalizable measurement\nof translationese, computed from the likelihood ratios of two contrastively\nfine-tuned language models (LMs). We use a synthesized dataset and a dataset\nwith translations in the wild to evaluate T-index's generalizability in\ncross-domain settings and its validity against human judgments. Our results\nshow that T-index is both robust and efficient. T-index scored by two 0.5B LMs\nfine-tuned on only 1-5k pairs of synthetic data can well capture translationese\nin the wild. We find that the relative differences in T-indices between\ntranslations can well predict pairwise translationese annotations obtained from\nhuman annotators; and the absolute values of T-indices correlate well with\nhuman ratings of degrees of translationese (Pearson's $r = 0.568$).\nAdditionally, the correlation between T-index and existing machine translation\n(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting\nthat T-index is not covered by these metrics and can serve as a complementary\nmetric in MT QE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5316\u6307\u6807T-index\uff0c\u7528\u4e8e\u6d4b\u91cf\u7ffb\u8bd1\u6587\u672c\u7684\u7279\u6027\uff08translationese\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u7ffb\u8bd1\u6587\u672c\u7279\u6027\u7684\u91cf\u5316\u6d4b\u91cf\uff0cT-index\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97T-index\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u7ffb\u8bd1\u6570\u636e\u4e0a\u8bc4\u4f30\u5176\u6cdb\u5316\u6027\u548c\u6709\u6548\u6027\u3002", "result": "T-index\u80fd\u591f\u6709\u6548\u6355\u6349\u7ffb\u8bd1\u7279\u6027\uff0c\u4e0e\u4eba\u5de5\u6807\u6ce8\u76f8\u5173\u6027\u9ad8\uff08Pearson's r = 0.568\uff09\uff0c\u4e14\u4e0e\u73b0\u6709MT\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u76f8\u5173\u6027\u4f4e\u3002", "conclusion": "T-index\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u7ffb\u8bd1\u7279\u6027\u6d4b\u91cf\u6307\u6807\uff0c\u53ef\u4f5c\u4e3aMT\u8d28\u91cf\u8bc4\u4f30\u7684\u8865\u5145\u5de5\u5177\u3002"}}
{"id": "2507.11928", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11928", "abs": "https://arxiv.org/abs/2507.11928", "authors": ["Abhishek Sriram", "Neal Tuffy"], "title": "Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning", "comment": "This paper is a pre-print version and has been submitted to the IEEE\n  International Conference on Future Machine Learning and Data Science (FMLDS\n  2025)", "summary": "This paper presents a machine learning-accelerated optimization framework for\nRF power amplifier design that reduces simulation requirements by 65% while\nmaintaining $\\pm0.3$ to $\\pm0.4$ dBm accuracy. The proposed method combines\nMaxMin Latin Hypercube Sampling with CatBoost gradient boosting to\nintelligently explore multidimensional parameter spaces. Instead of\nexhaustively simulating all parameter combinations to achieve target P2dB\ncompression specifications, our approach strategically selects approximately\n35% of critical simulation points. The framework processes ADS netlists,\nexecutes harmonic balance simulations on the reduced dataset, and trains a\nCatBoost model to predict P2dB performance across the entire design space.\nValidation across 15 PA operating modes yields an average $R^2$ of 0.901, with\nthe system ranking parameter combinations by their likelihood of meeting target\nspecifications. The integrated solution delivers 58.24% to 77.78% reduction in\nsimulation time through automated GUI-based workflows, enabling rapid design\niterations without compromising accuracy standards required for production RF\ncircuits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u52a0\u901f\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8eRF\u529f\u7387\u653e\u5927\u5668\u8bbe\u8ba1\uff0c\u51cf\u5c1165%\u7684\u4eff\u771f\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u00b10.3\u81f3\u00b10.4 dBm\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u7a77\u4e3e\u6240\u6709\u53c2\u6570\u7ec4\u5408\u4ee5\u5b9e\u73b0\u76ee\u6807P2dB\u538b\u7f29\u89c4\u683c\uff0c\u4eff\u771f\u6210\u672c\u9ad8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u667a\u80fd\u91c7\u6837\u548c\u673a\u5668\u5b66\u4e60\u51cf\u5c11\u4eff\u771f\u9700\u6c42\uff0c\u63d0\u9ad8\u8bbe\u8ba1\u6548\u7387\u3002", "method": "\u7ed3\u5408MaxMin\u62c9\u4e01\u8d85\u7acb\u65b9\u91c7\u6837\u548cCatBoost\u68af\u5ea6\u63d0\u5347\uff0c\u667a\u80fd\u63a2\u7d22\u591a\u7ef4\u53c2\u6570\u7a7a\u95f4\uff0c\u4ec5\u9009\u62e935%\u7684\u5173\u952e\u4eff\u771f\u70b9\u3002\u6846\u67b6\u5904\u7406ADS\u7f51\u8868\uff0c\u6267\u884c\u8c10\u6ce2\u5e73\u8861\u4eff\u771f\uff0c\u5e76\u8bad\u7ec3CatBoost\u6a21\u578b\u9884\u6d4bP2dB\u6027\u80fd\u3002", "result": "\u572815\u79cdPA\u5de5\u4f5c\u6a21\u5f0f\u4e0b\u9a8c\u8bc1\uff0c\u5e73\u5747R\u00b2\u4e3a0.901\uff0c\u4eff\u771f\u65f6\u95f4\u51cf\u5c1158.24%\u81f377.78%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5316GUI\u5de5\u4f5c\u6d41\u5b9e\u73b0\u5feb\u901f\u8bbe\u8ba1\u8fed\u4ee3\uff0c\u540c\u65f6\u6ee1\u8db3\u751f\u4ea7RF\u7535\u8def\u7684\u7cbe\u5ea6\u8981\u6c42\u3002"}}
{"id": "2507.12261", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12261", "abs": "https://arxiv.org/abs/2507.12261", "authors": ["Johann Frei", "Nils Feldhus", "Lisa Raithel", "Roland Roller", "Alexander Meyer", "Frank Kramer"], "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes", "comment": "Submitted to EMNLP 2025 System Demonstrations | Code:\n  https://github.com/j-frei/Infherno | Video:\n  https://www.youtube.com/watch?v=kyj5C2ivbMw | Demo:\n  https://infherno.misit-augsburg.de | HuggingFace Spaces:\n  https://huggingface.co/spaces/nfel/infherno", "summary": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u4ee3\u7406\u3001\u4ee3\u7801\u6267\u884c\u548c\u533b\u5b66\u672f\u8bed\u6570\u636e\u5e93\u7684\u7aef\u5230\u7aef\u6846\u67b6Infherno\uff0c\u7528\u4e8e\u5c06\u81ea\u7531\u5f62\u5f0f\u7684\u4e34\u5e8a\u7b14\u8bb0\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316FHIR\u8d44\u6e90\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u6a21\u5757\u5316\u89c4\u5219\u7cfb\u7edf\u6216\u6307\u4ee4\u8c03\u4f18\u7684LLM\uff09\u5728\u5c06\u4e34\u5e8a\u7b14\u8bb0\u8f6c\u6362\u4e3aFHIR\u8d44\u6e90\u65f6\u5b58\u5728\u6cdb\u5316\u6027\u4e0d\u8db3\u548c\u7ed3\u6784\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528LLM\u4ee3\u7406\u3001\u4ee3\u7801\u6267\u884c\u548c\u533b\u5b66\u672f\u8bed\u6570\u636e\u5e93\u5de5\u5177\u6784\u5efa\u7aef\u5230\u7aef\u6846\u67b6Infherno\uff0c\u786e\u4fdd\u7b26\u5408FHIR\u6587\u6863\u6a21\u5f0f\u3002", "result": "Infherno\u5728\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u9884\u6d4bFHIR\u8d44\u6e90\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u63a5\u8fd1\u4eba\u7c7b\u57fa\u7ebf\u6c34\u5e73\u3002", "conclusion": "Infherno\u652f\u6301\u4e34\u5e8a\u6570\u636e\u96c6\u6210\u548c\u8de8\u673a\u6784\u4e92\u64cd\u4f5c\u6027\uff0c\u63d0\u4f9b\u81ea\u5b9a\u4e49\u548c\u5408\u6210\u6570\u636e\u7684\u524d\u7aef\uff0c\u9002\u7528\u4e8e\u672c\u5730\u548c\u4e13\u6709\u6a21\u578b\u3002"}}
{"id": "2507.11975", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11975", "abs": "https://arxiv.org/abs/2507.11975", "authors": ["Valentin Frank Ingmar Guenter", "Athanasios Sideris"], "title": "Online Training and Pruning of Deep Reinforcement Learning Networks", "comment": "25 pages, 5 figures, 4 tables", "summary": "Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms\nhas been shown to enhance performance when feature extraction networks are used\nbut the gained performance comes at the significant expense of increased\ncomputational and memory complexity. Neural network pruning methods have\nsuccessfully addressed this challenge in supervised learning. However, their\napplication to RL is underexplored. We propose an approach to integrate\nsimultaneous training and pruning within advanced RL methods, in particular to\nRL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our\nnetworks (XiNet) are trained to solve stochastic optimization problems over the\nRL networks' weights and the parameters of variational Bernoulli distributions\nfor 0/1 Random Variables $\\xi$ scaling each unit in the networks. The\nstochastic problem formulation induces regularization terms that promote\nconvergence of the variational parameters to 0 when a unit contributes little\nto the performance. In this case, the corresponding structure is rendered\npermanently inactive and pruned from its network. We propose a cost-aware,\nsparsity-promoting regularization scheme, tailored to the DenseNet architecture\nof OFENets expressing the parameter complexity of involved networks in terms of\nthe parameters of the RVs in these networks. Then, when matching this cost with\nthe regularization terms, the many hyperparameters associated with them are\nautomatically selected, effectively combining the RL objectives and network\ncompression. We evaluate our method on continuous control benchmarks (MuJoCo)\nand the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned\nconsiderably with minimal loss in performance. Furthermore, our results confirm\nthat pruning large networks during training produces more efficient and higher\nperforming RL agents rather than training smaller networks from scratch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u540c\u65f6\u8bad\u7ec3\u548c\u526a\u679d\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86Online Feature Extractor Network\uff08OFENet\uff09\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u590d\u6742\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5185\u5b58\u590d\u6742\u5ea6\u9ad8\u3002\u526a\u679d\u65b9\u6cd5\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\u5df2\u6210\u529f\u5e94\u7528\uff0c\u4f46\u5728RL\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faXiNet\uff0c\u901a\u8fc7\u53d8\u5206\u4f2f\u52aa\u5229\u5206\u5e03\u548c\u968f\u673a\u53d8\u91cf\u03be\u5bf9\u7f51\u7edc\u5355\u5143\u8fdb\u884c\u526a\u679d\uff0c\u7ed3\u5408\u6210\u672c\u611f\u77e5\u7684\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6848\uff0c\u81ea\u52a8\u9009\u62e9\u8d85\u53c2\u6570\u3002", "result": "\u5728MuJoCo\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOFENet\u526a\u679d\u540e\u6027\u80fd\u635f\u5931\u6781\u5c0f\uff0c\u4e14\u8bad\u7ec3\u65f6\u526a\u679d\u6bd4\u4ece\u5934\u8bad\u7ec3\u5c0f\u7f51\u7edc\u66f4\u9ad8\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86RL\u76ee\u6807\u548c\u7f51\u7edc\u538b\u7f29\uff0c\u63d0\u5347\u4e86RL\u4ee3\u7406\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.12295", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12295", "abs": "https://arxiv.org/abs/2507.12295", "authors": ["Feng Xiao", "Jicong Fan"], "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding", "comment": null, "summary": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u591a\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u548c\u591a\u9886\u57df\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5d4c\u5165\u8d28\u91cf\u5bf9\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u6e90\u4e86\u5de5\u5177\u5305\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u8bc4\u4f30\u57fa\u51c6\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u6bd4\u8f83\u548c\u521b\u65b0\u3002", "method": "\u5229\u7528\u591a\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08\u5982BERT\u3001LLaMa-2\u7b49\uff09\u7684\u5d4c\u5165\u548c\u591a\u9886\u57df\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5d4c\u5165\u8d28\u91cf\u5bf9\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u5728LLM\u5d4c\u5165\u4e0b\u672a\u8868\u73b0\u51fa\u4f18\u52bf\uff1b\u8de8\u6a21\u578b\u6027\u80fd\u77e9\u9635\u5177\u6709\u4f4e\u79e9\u7279\u6027\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u672a\u6765\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u5f00\u6e90\u4e86\u5de5\u5177\u5305\u4ee5\u4fc3\u8fdb\u53d1\u5c55\u3002"}}
{"id": "2507.11997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11997", "abs": "https://arxiv.org/abs/2507.11997", "authors": ["Tairan Huang", "Yili Wang"], "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection", "comment": null, "summary": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.", "AI": {"tldr": "MLED\u6846\u67b6\u901a\u8fc7\u591a\u7ea7LLM\u589e\u5f3a\uff0c\u7ed3\u5408\u6587\u672c\u4fe1\u606f\u548c\u56fe\u7ed3\u6784\uff0c\u63d0\u5347\u56fe\u6b3a\u8bc8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u539f\u59cb\u6587\u672c\u4fe1\u606f\u7684\u8bed\u4e49\u7ebf\u7d22\uff0c\u4e14\u96be\u4ee5\u878d\u5408LLM\u5904\u7406\u7684\u6587\u672c\u5d4c\u5165\u4e0e\u56fe\u7ed3\u6784\u3002", "method": "\u8bbe\u8ba1\u591a\u7ea7LLM\u589e\u5f3a\u6846\u67b6\uff08\u7c7b\u578b\u7ea7\u548c\u5173\u7cfb\u7ea7\u589e\u5f3a\u5668\uff09\uff0c\u878d\u5408LLM\u63d0\u53d6\u7684\u5916\u90e8\u77e5\u8bc6\u4e0e\u56fe\u7ed3\u6784\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cMLED\u4f5c\u4e3a\u901a\u7528\u6846\u67b6\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MLED\u6709\u6548\u6574\u5408LLM\u4e0e\u56fe\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u6b3a\u8bc8\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2507.12308", "categories": ["cs.CL", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.12308", "abs": "https://arxiv.org/abs/2507.12308", "authors": ["Prashanth Vijayaraghavan", "Apoorva Nitsure", "Charles Mackin", "Luyao Shi", "Stefano Ambrogio", "Arvind Haran", "Viresh Paruthi", "Ali Elzein", "Dan Coops", "David Beymer", "Tyler Baldwin", "Ehsan Degan"], "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization", "comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)", "summary": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u73b0\u6709\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728VHDL\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Descriptions\uff08CoDes\uff09\u7684\u65b0\u65b9\u6cd5\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u901a\u7528\u4ee3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\uff08\u5982VHDL\uff09\u9886\u57df\u7684\u7814\u7a76\u548c\u4f18\u5316\u4e0d\u8db3\uff0c\u4e9f\u9700\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faCoDes\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e00\u7cfb\u5217\u4e2d\u95f4\u63cf\u8ff0\u6b65\u9aa4\uff08\u57fa\u4e8e\u95ee\u9898\u9648\u8ff0\u6216VHDL\u4ee3\u7801\uff09\u5e76\u7ed3\u5408\u539f\u59cb\u8f93\u5165\u63d0\u793a\uff0c\u4ee5\u63d0\u5347LLMs\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoDes\u65b9\u6cd5\u5728VHDL-Eval\u548cVHDL-Xform\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u7b56\u7565\u3002", "conclusion": "CoDes\u4e0d\u4ec5\u63d0\u5347\u4e86VHDL\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u7684\u8d28\u91cf\uff0c\u8fd8\u4e3a\u672a\u6765\u4f18\u5316\u4ee3\u7801LLMs\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2507.12011", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12011", "abs": "https://arxiv.org/abs/2507.12011", "authors": ["Yao Lu", "Hongyu Gao", "Zhuangzhi Chen", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning", "comment": null, "summary": "Although deep neural networks have made remarkable achievements in the field\nof automatic modulation recognition (AMR), these models often require a large\namount of labeled data for training. However, in many practical scenarios, the\navailable target domain data is scarce and difficult to meet the needs of model\ntraining. The most direct way is to collect data manually and perform expert\nannotation, but the high time and labor costs are unbearable. Another common\nmethod is data augmentation. Although it can enrich training samples to a\ncertain extent, it does not introduce new data and therefore cannot\nfundamentally solve the problem of data scarcity. To address these challenges,\nwe introduce a data expansion framework called Dynamic Uncertainty-driven\nSample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring\nfunction to filter out useful samples from relevant AMR datasets and employs an\nactive learning strategy to continuously refine the scorer. Extensive\nexperiments demonstrate that DUSE consistently outperforms 8 coreset selection\nbaselines in both class-balance and class-imbalance settings. Besides, DUSE\nexhibits strong cross-architecture generalization for unseen models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDUSE\u7684\u6570\u636e\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\u51fd\u6570\u4ece\u76f8\u5173\u6570\u636e\u96c6\u4e2d\u7b5b\u9009\u6709\u7528\u6837\u672c\uff0c\u5e76\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u8bc4\u5206\u5668\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u8c03\u5236\u8bc6\u522b\uff08AMR\uff09\u4e2d\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728AMR\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002\u5b9e\u9645\u573a\u666f\u4e2d\u76ee\u6807\u57df\u6570\u636e\u7a00\u7f3a\uff0c\u624b\u52a8\u6536\u96c6\u6216\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u65e0\u6cd5\u6839\u672c\u89e3\u51b3\u95ee\u9898\u3002", "method": "\u63d0\u51faDUSE\u6846\u67b6\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\u51fd\u6570\u7b5b\u9009\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u6301\u7eed\u4f18\u5316\u8bc4\u5206\u5668\u3002", "result": "DUSE\u5728\u7c7b\u5e73\u8861\u548c\u7c7b\u4e0d\u5e73\u8861\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e8\u79cd\u6838\u5fc3\u96c6\u9009\u62e9\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u672a\u89c1\u6a21\u578b\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DUSE\u6709\u6548\u89e3\u51b3\u4e86AMR\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.12002", "categories": ["cs.LG", "I.2.0; J.4"], "pdf": "https://arxiv.org/pdf/2507.12002", "abs": "https://arxiv.org/abs/2507.12002", "authors": ["Alice Zhang", "Callihan Bertley", "Dawei Liang", "Edison Thomaz"], "title": "Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing", "comment": null, "summary": "Social interactions play a crucial role in shaping human behavior,\nrelationships, and societies. It encompasses various forms of communication,\nsuch as verbal conversation, non-verbal gestures, facial expressions, and body\nlanguage. In this work, we develop a novel computational approach to detect a\nfoundational aspect of human social interactions, in-person verbal\nconversations, by leveraging audio and inertial data captured with a commodity\nsmartwatch in acoustically-challenging scenarios. To evaluate our approach, we\nconducted a lab study with 11 participants and a semi-naturalistic study with\n24 participants. We analyzed machine learning and deep learning models with 3\ndifferent fusion methods, showing the advantages of fusing audio and inertial\ndata to consider not only verbal cues but also non-verbal gestures in\nconversations. Furthermore, we perform a comprehensive set of evaluations\nacross activities and sampling rates to demonstrate the benefits of multimodal\nsensing in specific contexts. Overall, our framework achieved 82.0$\\pm$3.0%\nmacro F1-score when detecting conversations in the lab and 77.2$\\pm$1.8% in the\nsemi-naturalistic setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u667a\u80fd\u624b\u8868\u7684\u591a\u6a21\u6001\u6570\u636e\uff08\u97f3\u9891\u548c\u60ef\u6027\u6570\u636e\uff09\u68c0\u6d4b\u9762\u5bf9\u9762\u5bf9\u8bdd\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9a8c\u5ba4\u548c\u534a\u81ea\u7136\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u793e\u4ea4\u4e92\u52a8\u5bf9\u4eba\u7c7b\u884c\u4e3a\u548c\u793e\u4f1a\u5173\u7cfb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u58f0\u5b66\u73af\u5883\u4e2d\u68c0\u6d4b\u5bf9\u8bdd\u7684\u80fd\u529b\u6709\u9650\u3002", "method": "\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u79cd\u878d\u5408\u65b9\u5f0f\u5206\u6790\u97f3\u9891\u548c\u60ef\u6027\u6570\u636e\uff0c\u540c\u65f6\u8003\u8651\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u7ebf\u7d22\u3002", "result": "\u5728\u5b9e\u9a8c\u5ba4\u548c\u534a\u81ea\u7136\u73af\u5883\u4e2d\uff0c\u6a21\u578b\u7684\u5b8fF1\u5206\u6570\u5206\u522b\u8fbe\u523082.0\u00b13.0%\u548c77.2\u00b11.8%\u3002", "conclusion": "\u591a\u6a21\u6001\u4f20\u611f\u5728\u7279\u5b9a\u60c5\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2507.12372", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12372", "abs": "https://arxiv.org/abs/2507.12372", "authors": ["Meysam Alizadeh", "Fabrizio Gilardi", "Zeynab Samei", "Mohsen Mosleh"], "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics", "comment": null, "summary": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5177\u5907\u7f51\u9875\u6d4f\u89c8\u80fd\u529b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u80fd\u901a\u8fc7\u7528\u6237\u540d\u63a8\u65ad\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u7684 demographics\uff0c\u53d1\u73b0\u5176\u5177\u5907\u4e00\u5b9a\u51c6\u786e\u6027\uff0c\u4f46\u4e5f\u5b58\u5728\u504f\u89c1\u548c\u6ee5\u7528\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22LLMs\u5728\u5b9e\u65f6\u4fe1\u606f\u68c0\u7d22\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u4e86\u6b64\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\uff0848\u4e2aX/Twitter\u8d26\u6237\uff09\u548c\u8c03\u67e5\u6570\u636e\u96c6\uff081,384\u540d\u56fd\u9645\u53c2\u4e0e\u8005\uff09\uff0c\u8bc4\u4f30LLMs\u8bbf\u95ee\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5e76\u9884\u6d4b\u7528\u6237 demographics \u7684\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u663e\u793aLLMs\u80fd\u591f\u4ee5\u5408\u7406\u51c6\u786e\u6027\u9884\u6d4b\u7528\u6237 demographics\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u6a21\u578b\u5728\u89e3\u6790\u793e\u4ea4\u5a92\u4f53\u8d44\u6599\u65f6\u53ef\u80fd\u5f15\u5165\u6027\u522b\u548c\u653f\u6cbb\u504f\u89c1\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u8fd9\u79cd\u80fd\u529b\u5bf9\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u6709\u6f5c\u5728\u4ef7\u503c\uff0c\u4f46\u4e5f\u5b58\u5728\u6ee5\u7528\u98ce\u9669\uff0c\u5efa\u8bae\u9650\u5236\u516c\u5171\u5e94\u7528\u4e2d\u7684\u529f\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u7814\u7a76\u7528\u9014\u7684\u53d7\u63a7\u8bbf\u95ee\u3002"}}
{"id": "2507.12145", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12145", "abs": "https://arxiv.org/abs/2507.12145", "authors": ["Muhammad Azlan Qazi", "Alexandros Iosifidis", "Qi Zhang"], "title": "PRISM: Distributed Inference for Foundation Models at Edge", "comment": null, "summary": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u4fe1\u4f18\u5316\u7684\u5206\u5e03\u5f0fTransformer\u63a8\u7406\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u8fb9\u7f18\u90e8\u7f72\u9762\u4e34\u901a\u4fe1\u548c\u8ba1\u7b97\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7b56\u7565\u3002", "method": "\u91c7\u7528Segment Means\u8868\u793a\u8fd1\u4f3c\u4e2d\u95f4\u7279\u5f81\uff0c\u91cd\u6784\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u5e76\u8bbe\u8ba1\u5206\u533a\u611f\u77e5\u7684\u56e0\u679c\u63a9\u7801\u3002", "result": "\u5728ViT\u3001BERT\u548cGPT-2\u4e0a\u6d4b\u8bd5\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c1199.2%\uff0c\u8ba1\u7b97\u51cf\u5c1151.24%\uff0c\u7cbe\u5ea6\u635f\u5931\u5c0f\u3002", "conclusion": "PRISM\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12041", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12041", "abs": "https://arxiv.org/abs/2507.12041", "authors": ["Anmol Kagrecha", "Henrik Marklund", "Potsawee Manakul", "Richard Zeckhauser", "Benjamin Van Roy"], "title": "Granular feedback merits sophisticated aggregation", "comment": "31 pages, 8 figures", "summary": "Human feedback is increasingly used across diverse applications like training\nAI models, developing recommender systems, and measuring public opinion -- with\ngranular feedback often being preferred over binary feedback for its greater\ninformativeness. While it is easy to accurately estimate a population's\ndistribution of feedback given feedback from a large number of individuals,\ncost constraints typically necessitate using smaller groups. A simple method to\napproximate the population distribution is regularized averaging: compute the\nempirical distribution and regularize it toward a prior. Can we do better? As\nwe will discuss, the answer to this question depends on feedback granularity.\n  Suppose one wants to predict a population's distribution of feedback using\nfeedback from a limited number of individuals. We show that, as feedback\ngranularity increases, one can substantially improve upon predictions of\nregularized averaging by combining individuals' feedback in ways more\nsophisticated than regularized averaging.\n  Our empirical analysis using questions on social attitudes confirms this\npattern. In particular, with binary feedback, sophistication barely reduces the\nnumber of individuals required to attain a fixed level of performance. By\ncontrast, with five-point feedback, sophisticated methods match the performance\nof regularized averaging with about half as many individuals.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u6709\u9650\u4e2a\u4f53\u53cd\u9988\u4e0b\u9884\u6d4b\u7fa4\u4f53\u53cd\u9988\u5206\u5e03\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u53cd\u9988\u7c92\u5ea6\u8d8a\u9ad8\uff0c\u590d\u6742\u65b9\u6cd5\u6bd4\u6b63\u5219\u5316\u5e73\u5747\u66f4\u6709\u6548\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u6210\u672c\u9650\u5236\u4e0b\uff0c\u5229\u7528\u6709\u9650\u4e2a\u4f53\u53cd\u9988\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7fa4\u4f53\u53cd\u9988\u5206\u5e03\uff0c\u5c24\u5176\u662f\u9ad8\u7c92\u5ea6\u53cd\u9988\u7684\u60c5\u51b5\u3002", "method": "\u6bd4\u8f83\u6b63\u5219\u5316\u5e73\u5747\u4e0e\u66f4\u590d\u6742\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u53cd\u9988\u7c92\u5ea6\u4e0b\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u793e\u4f1a\u6001\u5ea6\u95ee\u9898\u7684\u5b9e\u8bc1\u5206\u6790\u9a8c\u8bc1\u3002", "result": "\u9ad8\u7c92\u5ea6\u53cd\u9988\uff08\u5982\u4e94\u70b9\uff09\u4e0b\uff0c\u590d\u6742\u65b9\u6cd5\u6240\u9700\u4e2a\u4f53\u6570\u91cf\u7ea6\u4e3a\u6b63\u5219\u5316\u5e73\u5747\u7684\u4e00\u534a\u5373\u53ef\u8fbe\u5230\u76f8\u540c\u6027\u80fd\u3002", "conclusion": "\u53cd\u9988\u7c92\u5ea6\u5f71\u54cd\u9884\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u9ad8\u7c92\u5ea6\u53cd\u9988\u66f4\u9002\u5408\u590d\u6742\u65b9\u6cd5\u4ee5\u8282\u7701\u6210\u672c\u3002"}}
{"id": "2507.12379", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12379", "abs": "https://arxiv.org/abs/2507.12379", "authors": ["Yucheng Sun", "Alessandro Stolfo", "Mrinmaya Sachan"], "title": "Probing for Arithmetic Errors in Language Models", "comment": null, "summary": "We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u53ef\u7528\u4e8e\u68c0\u6d4b\u7b97\u672f\u9519\u8bef\uff0c\u7b80\u5355\u63a2\u9488\u80fd\u9ad8\u7cbe\u5ea6\u89e3\u7801\u9884\u6d4b\u8f93\u51fa\u548c\u6b63\u786e\u7b54\u6848\uff0c\u8f7b\u91cf\u7ea7\u9519\u8bef\u68c0\u6d4b\u5668\u51c6\u786e\u7387\u8d8590%\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u590d\u6742\u4efb\u52a1\u4e2d\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u662f\u5426\u53ef\u7528\u4e8e\u68c0\u6d4b\u7b97\u672f\u9519\u8bef\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u3002", "method": "\u4ece3\u4f4d\u6570\u52a0\u6cd5\u4efb\u52a1\u5f00\u59cb\uff0c\u8bad\u7ec3\u7b80\u5355\u63a2\u9488\u89e3\u7801\u9690\u85cf\u72b6\u6001\uff1b\u6269\u5c55\u5230\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u4efb\u52a1\uff0c\u9a8c\u8bc1\u63a2\u9488\u6cdb\u5316\u80fd\u529b\uff1b\u5229\u7528\u63a2\u9488\u6307\u5bfc\u9009\u62e9\u6027\u91cd\u65b0\u63d0\u793a\u3002", "result": "\u63a2\u9488\u80fd\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u9519\u8bef\uff0c\u5e76\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff1b\u9009\u62e9\u6027\u91cd\u65b0\u63d0\u793a\u53ef\u63d0\u5347\u4efb\u52a1\u51c6\u786e\u7387\u3002", "conclusion": "\u7b97\u672f\u9519\u8bef\u53ef\u901a\u8fc7\u5185\u90e8\u6fc0\u6d3b\u9884\u6d4b\uff0c\u7b80\u5355\u63a2\u9488\u4e3a\u8f7b\u91cf\u7ea7\u6a21\u578b\u81ea\u6211\u4fee\u6b63\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.12196", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12196", "abs": "https://arxiv.org/abs/2507.12196", "authors": ["Nikolaos Louloudakis", "Ajitha Rajan"], "title": "Selective Quantization Tuning for ONNX Models", "comment": "5 pages, 3 figures, 2 tables", "summary": "Quantization is a process that reduces the precision of deep neural network\nmodels to lower model size and computational demands, often at the cost of\naccuracy. However, fully quantized models may exhibit sub-optimal performance\nbelow acceptable levels and face deployment challenges on low-end hardware\naccelerators due to practical constraints. To address these issues,\nquantization can be selectively applied to only a subset of layers, but\nselecting which layers to exclude is non-trivial. To this direction, we propose\nTuneQn, a suite enabling selective quantization, deployment and execution of\nONNX models across various CPU and GPU devices, combined with profiling and\nmulti-objective optimization. TuneQn generates selectively quantized ONNX\nmodels, deploys them on different hardware, measures performance on metrics\nlike accuracy and size, performs Pareto Front minimization to identify the best\nmodel candidate and visualizes the results. To demonstrate the effectiveness of\nTuneQn, we evaluated TuneQn on four ONNX models with two quantization settings\nacross CPU and GPU devices. As a result, we demonstrated that our utility\neffectively performs selective quantization and tuning, selecting ONNX model\ncandidates with up to a $54.14$% reduction in accuracy loss compared to the\nfully quantized model, and up to a $72.9$% model size reduction compared to the\noriginal model.", "AI": {"tldr": "TuneQn\u662f\u4e00\u4e2a\u5de5\u5177\u5957\u4ef6\uff0c\u7528\u4e8e\u9009\u62e9\u6027\u91cf\u5316\u3001\u90e8\u7f72\u548c\u6267\u884cONNX\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u548c\u6027\u80fd\u5206\u6790\uff0c\u663e\u8457\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\u548c\u6a21\u578b\u5927\u5c0f\u3002", "motivation": "\u89e3\u51b3\u5168\u91cf\u5316\u6a21\u578b\u7cbe\u5ea6\u4e0b\u964d\u548c\u4f4e\u7aef\u786c\u4ef6\u90e8\u7f72\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u91cf\u5316\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faTuneQn\u5957\u4ef6\uff0c\u7ed3\u5408\u9009\u62e9\u6027\u91cf\u5316\u3001\u90e8\u7f72\u3001\u6027\u80fd\u5206\u6790\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u751f\u6210\u4f18\u5316\u7684ONNX\u6a21\u578b\u3002", "result": "\u5728\u56db\u79cdONNX\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0cTuneQn\u663e\u8457\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\uff08\u6700\u9ad854.14%\uff09\u548c\u6a21\u578b\u5927\u5c0f\uff08\u6700\u9ad872.9%\uff09\u3002", "conclusion": "TuneQn\u6709\u6548\u5b9e\u73b0\u9009\u62e9\u6027\u91cf\u5316\u548c\u4f18\u5316\uff0c\u4e3a\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12043", "abs": "https://arxiv.org/abs/2507.12043", "authors": ["Wen Wen", "Tieliang Gong", "Yunjiao Zhang", "Zeyu Gao", "Weizhan Zhang", "Yong-Jin Liu"], "title": "Information-Theoretic Generalization Bounds of Replay-based Continual Learning", "comment": null, "summary": "Continual learning (CL) has emerged as a dominant paradigm for acquiring\nknowledge from sequential tasks while avoiding catastrophic forgetting.\nAlthough many CL methods have been proposed to show impressive empirical\nperformance, the theoretical understanding of their generalization behavior\nremains limited, particularly for replay-based approaches. In this paper, we\nestablish a unified theoretical framework for replay-based CL, deriving a\nseries of information-theoretic bounds that explicitly characterize how the\nmemory buffer interacts with the current task to affect generalization.\nSpecifically, our hypothesis-based bounds reveal that utilizing the limited\nexemplars of previous tasks alongside the current task data, rather than\nexhaustive replay, facilitates improved generalization while effectively\nmitigating catastrophic forgetting. Furthermore, our prediction-based bounds\nyield tighter and computationally tractable upper bounds of the generalization\ngap through the use of low-dimensional variables. Our analysis is general and\nbroadly applicable to a wide range of learning algorithms, exemplified by\nstochastic gradient Langevin dynamics (SGLD) as a representative method.\nComprehensive experimental evaluations demonstrate the effectiveness of our\nderived bounds in capturing the generalization dynamics in replay-based CL\nsettings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u57fa\u4e8e\u56de\u653e\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5185\u5b58\u7f13\u51b2\u533a\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u5728\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u7684\u540c\u65f6\u4ece\u987a\u5e8f\u4efb\u52a1\u4e2d\u83b7\u53d6\u77e5\u8bc6\uff0c\u4f46\u5176\u6cdb\u5316\u884c\u4e3a\u7684\u7406\u8bba\u7406\u89e3\u6709\u9650\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u56de\u653e\u7684\u65b9\u6cd5\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63a8\u5bfc\u4e86\u4e00\u7cfb\u5217\u4fe1\u606f\u8bba\u754c\u9650\uff0c\u5206\u6790\u4e86\u5185\u5b58\u7f13\u51b2\u533a\u4e0e\u5f53\u524d\u4efb\u52a1\u7684\u4ea4\u4e92\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5229\u7528\u6709\u9650\u7684\u5386\u53f2\u4efb\u52a1\u6837\u672c\u4e0e\u5f53\u524d\u4efb\u52a1\u6570\u636e\u7ed3\u5408\uff0c\u53ef\u4ee5\u6539\u5584\u6cdb\u5316\u5e76\u51cf\u5c11\u9057\u5fd8\u3002", "conclusion": "\u672c\u6587\u7684\u7406\u8bba\u6846\u67b6\u548c\u754c\u9650\u4e3a\u57fa\u4e8e\u56de\u653e\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u652f\u6301\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.12425", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.12425", "abs": "https://arxiv.org/abs/2507.12425", "authors": ["Chandana Cheerla"], "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data", "comment": null, "summary": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684RAG\u6846\u67b6\uff0c\u7ed3\u5408\u6df7\u5408\u68c0\u7d22\u7b56\u7565\u548c\u5143\u6570\u636e\u611f\u77e5\u8fc7\u6ee4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f01\u4e1a\u6570\u636e\u5904\u7406\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f01\u4e1a\u4f9d\u8d56\u4e13\u6709\u6570\u636e\u51b3\u7b56\uff0c\u4f46\u73b0\u6709LLMs\u548cRAG\u6846\u67b6\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u3002", "method": "\u91c7\u7528\u6df7\u5408\u68c0\u7d22\uff08\u5bc6\u96c6\u5d4c\u5165\u548cBM25\uff09\u3001\u5143\u6570\u636e\u8fc7\u6ee4\u3001\u8bed\u4e49\u5206\u5757\u548c\u91cf\u5316\u7d22\u5f15\uff0c\u7ed3\u5408\u4eba\u5de5\u53cd\u9988\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPrecision@5\u63d0\u534715%\uff0cRecall@5\u63d0\u534713%\uff0c\u5b9a\u6027\u8bc4\u4f30\u4e2dFaithfulness\u548cRelevance\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u6846\u67b6\u6709\u6548\u63d0\u5347\u4f01\u4e1a\u4efb\u52a1\u54cd\u5e94\u8d28\u91cf\uff0c\u672a\u6765\u5c06\u6269\u5c55\u81f3\u591a\u6a21\u6001\u6570\u636e\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u68c0\u7d22\u3002"}}
{"id": "2507.12053", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12053", "abs": "https://arxiv.org/abs/2507.12053", "authors": ["Seanglidet Yean", "Jiazu Zhou", "Bu-Sung Lee", "Markus Schl\u00e4pfer"], "title": "FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling", "comment": "International Conference on Intelligent Digitization of Systems and\n  Services, Valencia, Spain, 2025 (IDSS 2025)", "summary": "The mobility patterns of people in cities evolve alongside changes in land\nuse and population. This makes it crucial for urban planners to simulate and\nanalyze human mobility patterns for purposes such as transportation\noptimization and sustainable urban development. Existing generative models\nborrowed from machine learning rely heavily on historical trajectories and\noften overlook evolving factors like changes in population density and land\nuse. Mechanistic approaches incorporate population density and facility\ndistribution but assume static scenarios, limiting their utility for future\nprojections where historical data for calibration is unavailable. This study\nintroduces a novel, data-driven approach for generating origin-destination\nmobility flows tailored to simulated urban scenarios. Our method leverages\nadaptive factors such as dynamic region sizes and land use archetypes, and it\nutilizes conditional generative adversarial networks (cGANs) to blend\nhistorical data with these adaptive parameters. The approach facilitates rapid\nmobility flow generation with adjustable spatial granularity based on regions\nof interest, without requiring extensive calibration data or complex behavior\nmodeling. The promising performance of our approach is demonstrated by its\napplication to mobile phone data from Singapore, and by its comparison with\nexisting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08cGANs\uff09\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9002\u5e94\u52a8\u6001\u57ce\u5e02\u60c5\u666f\u7684\u51fa\u884c\u6d41\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u5047\u8bbe\u6216\u5386\u53f2\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u57ce\u5e02\u89c4\u5212\u548c\u4ea4\u901a\u4f18\u5316\u9700\u8981\u6a21\u62df\u52a8\u6001\u7684\u4eba\u7c7b\u51fa\u884c\u6a21\u5f0f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6216\u4f9d\u8d56\u5386\u53f2\u6570\u636e\uff0c\u6216\u5047\u8bbe\u9759\u6001\u60c5\u666f\uff0c\u65e0\u6cd5\u9002\u5e94\u672a\u6765\u53d8\u5316\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u533a\u57df\u5927\u5c0f\u548c\u571f\u5730\u5229\u7528\u7c7b\u578b\uff0c\u5229\u7528cGANs\u878d\u5408\u5386\u53f2\u6570\u636e\u4e0e\u81ea\u9002\u5e94\u53c2\u6570\uff0c\u5feb\u901f\u751f\u6210\u53ef\u8c03\u7a7a\u95f4\u7c92\u5ea6\u7684\u51fa\u884c\u6d41\u3002", "result": "\u5728\u65b0\u52a0\u5761\u624b\u673a\u6570\u636e\u4e0a\u7684\u5e94\u7528\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u5927\u91cf\u6821\u51c6\u6570\u636e\u6216\u590d\u6742\u884c\u4e3a\u5efa\u6a21\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u57ce\u5e02\u60c5\u666f\u4e0b\u7684\u51fa\u884c\u6d41\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12428", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12428", "abs": "https://arxiv.org/abs/2507.12428", "authors": ["Yik Siu Chan", "Zheng-Xin Yong", "Stephen H. Bach"], "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models", "comment": null, "summary": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u94fe\uff08CoTs\uff09\u9884\u6d4b\u6700\u7ec8\u8f93\u51fa\u7684\u5bf9\u9f50\u98ce\u9669\uff0c\u53d1\u73b0\u57fa\u4e8eCoT\u6fc0\u6d3b\u7684\u7ebf\u6027\u63a2\u9488\u4f18\u4e8e\u6587\u672c\u65b9\u6cd5\uff0c\u4e14\u80fd\u63d0\u524d\u9884\u6d4b\u3002", "motivation": "Open-weights\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u53ef\u80fd\u5305\u542b\u6709\u5bb3\u5185\u5bb9\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7CoTs\u9884\u6d4b\u6700\u7ec8\u8f93\u51fa\u7684\u5bf9\u9f50\u98ce\u9669\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u79cd\u76d1\u6d4b\u65b9\u6cd5\uff08\u4eba\u7c7b\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6587\u672c\u5206\u7c7b\u5668\uff09\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8eCoT\u6587\u672c\u548c\u6fc0\u6d3b\u7684\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8eCoT\u6fc0\u6d3b\u7684\u7ebf\u6027\u63a2\u9488\u663e\u8457\u4f18\u4e8e\u6587\u672c\u65b9\u6cd5\uff0c\u4e14\u80fd\u5728\u63a8\u7406\u5b8c\u6210\u524d\u51c6\u786e\u9884\u6d4b\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u63a2\u9488\u53ef\u5b9e\u73b0\u5b9e\u65f6\u5b89\u5168\u76d1\u6d4b\u548c\u65e9\u671f\u5e72\u9884\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u3002"}}
{"id": "2507.12070", "categories": ["cs.LG", "I.5.1; F.1.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.12070", "abs": "https://arxiv.org/abs/2507.12070", "authors": ["George Bird"], "title": "Emergence of Quantised Representations Isolated to Anisotropic Functions", "comment": "36 pages, 31 figures", "summary": "This paper describes a novel methodology for determining representational\nalignment, developed upon the existing Spotlight Resonance method. Using this,\nit is found that algebraic symmetries of network primitives are a strong\npredictor for task-agnostic structure in representations. Particularly, this\nnew tool is used to gain insight into how discrete representations can form and\narrange in autoencoder models, through an ablation study where only the\nactivation function is altered. Representations are found to tend to discretise\nwhen the activation functions are defined through a discrete algebraic\npermutation-equivariant symmetry. In contrast, they remain continuous under a\ncontinuous algebraic orthogonal-equivariant definition. These findings\ncorroborate the hypothesis that functional form choices can carry unintended\ninductive biases which produce task-independent artefactual structures in\nrepresentations, particularly that contemporary forms induce discretisation of\notherwise continuous structure -- a quantisation effect. Moreover, this\nsupports a general causal model for one mode in which discrete representations\nmay form, and could constitute a prerequisite for downstream interpretability\nphenomena, including grandmother neurons, discrete coding schemes, general\nlinear features and possibly Superposition. Hence, this tool and proposed\nmechanism for the influence of functional form on representations may provide\nseveral insights into emergent interpretability research. Finally, preliminary\nresults indicate that quantisation of representations appears to correlate with\na measurable increase in reconstruction error, reinforcing previous conjectures\nthat this collapse can be detrimental.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSpotlight Resonance\u65b9\u6cd5\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u786e\u5b9a\u8868\u793a\u5bf9\u9f50\uff0c\u53d1\u73b0\u7f51\u7edc\u539f\u8bed\u7684\u4ee3\u6570\u5bf9\u79f0\u6027\u662f\u4efb\u52a1\u65e0\u5173\u8868\u793a\u7ed3\u6784\u7684\u5f3a\u9884\u6d4b\u56e0\u5b50\u3002\u901a\u8fc7\u6539\u53d8\u6fc0\u6d3b\u51fd\u6570\u7684\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u79bb\u6563\u8868\u793a\u7684\u5f62\u6210\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u529f\u80fd\u5f62\u5f0f\u9009\u62e9\u5982\u4f55\u65e0\u610f\u4e2d\u5f15\u5165\u5f52\u7eb3\u504f\u5dee\uff0c\u5bfc\u81f4\u8868\u793a\u4e2d\u51fa\u73b0\u4efb\u52a1\u65e0\u5173\u7684\u7ed3\u6784\uff0c\u7279\u522b\u662f\u5f53\u4ee3\u5f62\u5f0f\u5982\u4f55\u5bfc\u81f4\u8fde\u7eed\u7ed3\u6784\u7684\u79bb\u6563\u5316\uff08\u91cf\u5316\u6548\u5e94\uff09\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684Spotlight Resonance\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u53d8\u6fc0\u6d3b\u51fd\u6570\u8fdb\u884c\u6d88\u878d\u7814\u7a76\uff0c\u5206\u6790\u79bb\u6563\u8868\u793a\u7684\u5f62\u6210\u548c\u6392\u5217\u3002", "result": "\u53d1\u73b0\u79bb\u6563\u4ee3\u6570\u7f6e\u6362\u7b49\u53d8\u5bf9\u79f0\u6027\u4e0b\u8868\u793a\u503e\u5411\u4e8e\u79bb\u6563\u5316\uff0c\u800c\u8fde\u7eed\u4ee3\u6570\u6b63\u4ea4\u7b49\u53d8\u5b9a\u4e49\u4e0b\u8868\u793a\u4fdd\u6301\u8fde\u7eed\u3002\u91cf\u5316\u8868\u793a\u4e0e\u91cd\u5efa\u8bef\u5dee\u589e\u52a0\u76f8\u5173\u3002", "conclusion": "\u529f\u80fd\u5f62\u5f0f\u5bf9\u8868\u793a\u7684\u5f71\u54cd\u673a\u5236\u4e3a\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u652f\u6301\u79bb\u6563\u8868\u793a\u5f62\u6210\u7684\u56e0\u679c\u6a21\u578b\uff0c\u5e76\u53ef\u80fd\u89e3\u91ca\u4e0b\u6e38\u89e3\u91ca\u6027\u73b0\u8c61\u3002"}}
{"id": "2507.12451", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12451", "abs": "https://arxiv.org/abs/2507.12451", "authors": ["Suman Adhya", "Debarshi Kumar Sanyal"], "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling", "comment": "Accepted as a long paper for ACL 2025 main conference", "summary": "Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.", "AI": {"tldr": "S2WTM\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u7403\u9762\u5207\u7247Wasserstein\u8ddd\u79bb\u89e3\u51b3VAE-NTMs\u4e2d\u7684\u540e\u9a8c\u574d\u584c\u95ee\u9898\uff0c\u751f\u6210\u66f4\u4e00\u81f4\u548c\u591a\u6837\u7684\u4e3b\u9898\u3002", "motivation": "VAE-NTMs\u5728\u5efa\u6a21\u9ad8\u7ef4\u6587\u672c\u6570\u636e\u7684\u8d85\u7403\u9762\u6f5c\u5728\u8868\u793a\u65f6\u5bb9\u6613\u51fa\u73b0\u540e\u9a8c\u574d\u584c\uff0c\u5bfc\u81f4\u6f5c\u5728\u8868\u793a\u65e0\u6548\u3002", "method": "S2WTM\u91c7\u7528\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u5e76\u5229\u7528\u7403\u9762\u5207\u7247Wasserstein\u8ddd\u79bb\u5bf9\u9f50\u540e\u9a8c\u5206\u5e03\u4e0e\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eS2WTM\u4f18\u4e8e\u73b0\u6709\u4e3b\u9898\u6a21\u578b\uff0c\u751f\u6210\u66f4\u4e00\u81f4\u548c\u591a\u6837\u7684\u4e3b\u9898\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "S2WTM\u6709\u6548\u89e3\u51b3\u4e86\u540e\u9a8c\u574d\u584c\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4e3b\u9898\u5efa\u6a21\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12094", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.12094", "abs": "https://arxiv.org/abs/2507.12094", "authors": ["Yiding Feng", "Wei Tang"], "title": "Measuring Informativeness Gap of (Mis)Calibrated Predictors", "comment": null, "summary": "In many applications, decision-makers must choose between multiple predictive\nmodels that may all be miscalibrated. Which model (i.e., predictor) is more\n\"useful\" in downstream decision tasks? To answer this, our first contribution\nintroduces the notion of the informativeness gap between any two predictors,\ndefined as the maximum normalized payoff advantage one predictor offers over\nthe other across all decision-making tasks. Our framework strictly generalizes\nseveral existing notions: it subsumes U-Calibration [KLST-23] and Calibration\nDecision Loss [HW-24], which compare a miscalibrated predictor to its\ncalibrated counterpart, and it recovers Blackwell informativeness [Bla-51,\nBla-53] as a special case when both predictors are perfectly calibrated. Our\nsecond contribution is a dual characterization of the informativeness gap,\nwhich gives rise to a natural informativeness measure that can be viewed as a\nrelaxed variant of the earth mover's distance (EMD) between two prediction\ndistributions. We show that this measure satisfies natural desiderata: it is\ncomplete and sound, and it can be estimated sample-efficiently in the\nprediction-only access setting. Along the way, we also obtain novel\ncombinatorial structural results when applying this measure to perfectly\ncalibrated predictors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cf\u9884\u6d4b\u6a21\u578b\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u2018\u6709\u7528\u6027\u2019\u7684\u65b0\u6846\u67b6\uff0c\u79f0\u4e3a\u2018\u4fe1\u606f\u5dee\u8ddd\u2019\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u53ef\u8ba1\u7b97\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u591a\u9884\u6d4b\u6a21\u578b\u4e2d\u9009\u62e9\u6700\u4f18\u6a21\u578b\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u53ef\u80fd\u90fd\u5b58\u5728\u6821\u51c6\u8bef\u5dee\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5f15\u5165\u2018\u4fe1\u606f\u5dee\u8ddd\u2019\u6982\u5ff5\uff0c\u901a\u8fc7\u6700\u5927\u6807\u51c6\u5316\u6536\u76ca\u4f18\u52bf\u6bd4\u8f83\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u7ed9\u51fa\u5176\u5bf9\u5076\u8868\u5f81\u548c\u4e00\u79cd\u7c7b\u4f3cEMD\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u65b0\u6846\u67b6\u7edf\u4e00\u4e86\u73b0\u6709\u6982\u5ff5\uff0c\u5982U-Calibration\u548cBlackwell\u4fe1\u606f\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6837\u672c\u9ad8\u6548\u53ef\u4f30\u8ba1\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4fe1\u606f\u5dee\u8ddd\u6846\u67b6\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.12466", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12466", "abs": "https://arxiv.org/abs/2507.12466", "authors": ["David Mizrahi", "Anders Boesen Lindbo Larsen", "Jesse Allardice", "Suzie Petryk", "Yuri Gorokhov", "Jeffrey Li", "Alex Fang", "Josh Gardner", "Tom Gunter", "Afshin Dehghan"], "title": "Language Models Improve When Pretraining Data Matches Target Tasks", "comment": "44 pages, 25 figures, 13 tables", "summary": "Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBETR\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u4f18\u5316\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u8bc4\u4f30\u57fa\u51c6\u7684\u76f8\u4f3c\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u663e\u5f0f\u4f18\u5316\u9884\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u800c\u975e\u4f9d\u8d56\u9690\u5f0f\u7684\u57fa\u51c6\u9a71\u52a8\u8fed\u4ee3\u3002", "method": "BETR\u65b9\u6cd5\u901a\u8fc7\u5d4c\u5165\u57fa\u51c6\u793a\u4f8b\u548c\u9884\u8bad\u7ec3\u6587\u6863\u5230\u5171\u4eab\u7a7a\u95f4\uff0c\u57fa\u4e8e\u76f8\u4f3c\u6027\u8bc4\u5206\u9009\u62e9\u6570\u636e\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u9884\u6d4b\u5168\u91cf\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBETR\u572810\u9879\u4efb\u52a1\u4e2d\u76849\u9879\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u53472.1\u500d\uff0c\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u8868\u660e\uff0c\u76f4\u63a5\u5339\u914d\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u76ee\u6807\u4efb\u52a1\u80fd\u7cbe\u786e\u5851\u9020\u6a21\u578b\u80fd\u529b\uff0c\u4e14\u6570\u636e\u9009\u62e9\u7b56\u7565\u9700\u9002\u5e94\u6a21\u578b\u89c4\u6a21\u3002"}}
{"id": "2507.12262", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.12262", "abs": "https://arxiv.org/abs/2507.12262", "authors": ["Zachary James", "Joseph Guinness"], "title": "A Framework for Nonstationary Gaussian Processes with Neural Network Parameters", "comment": null, "summary": "Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u975e\u5e73\u7a33\u6838\u7684\u9ad8\u65af\u8fc7\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u52a8\u6001\u8c03\u6574\u6838\u53c2\u6570\uff0c\u63d0\u5347\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\u7684\u5e73\u7a33\u6838\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u6570\u636e\u96c6\u3002", "method": "\u5c06\u975e\u5e73\u7a33\u6838\u53c2\u6570\u5efa\u6a21\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\uff0c\u8054\u5408\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u548c\u9ad8\u65af\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5e73\u7a33\u6a21\u578b\u548c\u53d8\u5206\u63a8\u65ad\u7684\u5c42\u6b21\u6a21\u578b\uff0c\u4e14\u80fd\u6062\u590d\u7a7a\u95f4\u6570\u636e\u7684\u975e\u5e73\u7a33\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7075\u6d3b\u3001\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u975e\u5e73\u7a33\u6838\uff0c\u65e0\u9700\u91cd\u65b0\u8bbe\u8ba1\u4f18\u5316\u8fc7\u7a0b\u3002"}}
{"id": "2507.12127", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12127", "abs": "https://arxiv.org/abs/2507.12127", "authors": ["Ngoc Duy Pham", "Thusitha Dayaratne", "Viet Vo", "Shangqi Lai", "Sharif Abuadbba", "Hajime Suzuki", "Xingliang Yuan", "Carsten Rudolph"], "title": "Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks", "comment": null, "summary": "Advancements in wireless and mobile technologies, including 5G advanced and\nthe envisioned 6G, are driving exponential growth in wireless devices. However,\nthis rapid expansion exacerbates spectrum scarcity, posing a critical\nchallenge. Dynamic spectrum allocation (DSA)--which relies on sensing and\ndynamically sharing spectrum--has emerged as an essential solution to address\nthis issue. While machine learning (ML) models hold significant potential for\nimproving spectrum sensing, their adoption in centralized ML-based DSA systems\nis limited by privacy concerns, bandwidth constraints, and regulatory\nchallenges. To overcome these limitations, distributed ML-based approaches such\nas Federated Learning (FL) offer promising alternatives. This work addresses\ntwo key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of\nlabeled data for training FL models in practical spectrum sensing scenarios is\ntackled with a semi-supervised FL approach, combined with energy detection,\nenabling model training on unlabeled datasets. Second, we examine the security\nvulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our\nanalysis highlights the shortcomings of existing majority-based defenses in\ncountering such attacks. To address these vulnerabilities, we propose a novel\ndefense mechanism inspired by vaccination, which effectively mitigates data\npoisoning attacks without relying on majority-based assumptions. Extensive\nexperiments on both synthetic and real-world datasets validate our solutions,\ndemonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets\nand maintain Byzantine robustness against both targeted and untargeted data\npoisoning attacks, even when a significant proportion of participants are\nmalicious.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u534a\u76d1\u7763\u9891\u8c31\u611f\u77e5\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u548c\u6570\u636e\u6295\u6bd2\u653b\u51fb\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u65e0\u7ebf\u8bbe\u5907\u6570\u91cf\u6fc0\u589e\u5bfc\u81f4\u9891\u8c31\u7a00\u7f3a\uff0c\u52a8\u6001\u9891\u8c31\u5206\u914d\uff08DSA\uff09\u6210\u4e3a\u5173\u952e\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u9690\u79c1\u548c\u5e26\u5bbd\u9650\u5236\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u7ed3\u5408\u80fd\u91cf\u68c0\u6d4b\uff0c\u89e3\u51b3\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff1b\u63d0\u51fa\u57fa\u4e8e\u75ab\u82d7\u63a5\u79cd\u7684\u9632\u5fa1\u673a\u5236\u5e94\u5bf9\u6570\u636e\u6295\u6bd2\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u672a\u6807\u8bb0\u6570\u636e\u96c6\u4e0a\u7684\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u62b5\u5fa1\u6076\u610f\u53c2\u4e0e\u8005\u7684\u6570\u636e\u6295\u6bd2\u653b\u51fb\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u9891\u8c31\u611f\u77e5\uff08FLSS\uff09\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12133", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12133", "abs": "https://arxiv.org/abs/2507.12133", "authors": ["Hanwen Liu", "Yuhe Huang", "Yifeng Gong", "Yanjie Zhai", "Jiaxuan Lu"], "title": "HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD", "comment": null, "summary": "Device recognition is vital for security in wireless communication systems,\nparticularly for applications like access control. Radio Frequency Fingerprint\nIdentification (RFFI) offers a non-cryptographic solution by exploiting\nhardware-induced signal distortions. This paper proposes HyDRA, a Hybrid\nDual-mode RF Architecture that integrates an optimized Variational Mode\nDecomposition (VMD) with a novel architecture based on the fusion of\nConvolutional Neural Networks (CNNs), Transformers, and Mamba components,\ndesigned to support both closed-set and open-set classification tasks. The\noptimized VMD enhances preprocessing efficiency and classification accuracy by\nfixing center frequencies and using closed-form solutions. HyDRA employs the\nTransformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and\nthe Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting\nto varying conditions. Evaluation on public datasets demonstrates\nstate-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance\nin our proposed open-set classification method, effectively identifying\nunauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves\nmillisecond-level inference speed with low power consumption, providing a\npractical solution for real-time wireless authentication in real-world\nenvironments.", "AI": {"tldr": "HyDRA\u662f\u4e00\u79cd\u6df7\u5408\u53cc\u6a21\u5c04\u9891\u67b6\u6784\uff0c\u7ed3\u5408\u4f18\u5316\u7684VMD\u548c\u65b0\u578bCNN\u3001Transformer\u4e0eMamba\u878d\u5408\u67b6\u6784\uff0c\u652f\u6301\u95ed\u96c6\u548c\u5f00\u96c6\u5206\u7c7b\u4efb\u52a1\uff0c\u5728\u65e0\u7ebf\u8bbe\u5907\u8bc6\u522b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u8ba4\u8bc1\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u8bbe\u5907\u8bc6\u522b\u5bf9\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u8bbf\u95ee\u63a7\u5236\u7b49\u5e94\u7528\u3002RFFI\u901a\u8fc7\u786c\u4ef6\u5f15\u8d77\u7684\u4fe1\u53f7\u5931\u771f\u63d0\u4f9b\u975e\u52a0\u5bc6\u89e3\u51b3\u65b9\u6848\u3002", "method": "HyDRA\u7ed3\u5408\u4f18\u5316\u7684VMD\u9884\u5904\u7406\u548cCNN\u3001Transformer\u3001Mamba\u878d\u5408\u67b6\u6784\uff0c\u4f7f\u7528TDSE\u5efa\u6a21\u5168\u5c40\u4f9d\u8d56\uff0cMLFE\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\u5904\u7406\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cHyDRA\u5728\u95ed\u96c6\u573a\u666f\u4e2d\u8fbe\u5230SOTA\u51c6\u786e\u7387\uff0c\u5f00\u96c6\u5206\u7c7b\u65b9\u6cd5\u8868\u73b0\u7a33\u5065\uff0c\u53ef\u6709\u6548\u8bc6\u522b\u672a\u6388\u6743\u8bbe\u5907\u3002", "conclusion": "HyDRA\u5728NVIDIA Jetson Xavier NX\u4e0a\u5b9e\u73b0\u6beb\u79d2\u7ea7\u63a8\u7406\u901f\u5ea6\u548c\u4f4e\u529f\u8017\uff0c\u4e3a\u5b9e\u65f6\u65e0\u7ebf\u8ba4\u8bc1\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12142", "categories": ["cs.LG", "cs.CL", "cs.NA", "math.DG", "math.NA", "68T07, 65F55, 53Z50"], "pdf": "https://arxiv.org/pdf/2507.12142", "abs": "https://arxiv.org/abs/2507.12142", "authors": ["Vladimir Bogachev", "Vladimir Aletov", "Alexander Molozhavenko", "Denis Bobkov", "Vera Soboleva", "Aibek Alanov", "Maxim Rakhuba"], "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRiemannLoRA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06LoRA\u77e9\u9635\u89c6\u4e3a\u5149\u6ed1\u6d41\u5f62\u4e0a\u7684\u5143\u7d20\uff0c\u89e3\u51b3\u4e86\u521d\u59cb\u5316\u7b56\u7565\u548c\u4f4e\u79e9\u77e9\u9635\u5206\u89e3\u4e2d\u7684\u8fc7\u53c2\u6570\u5316\u95ee\u9898\u3002", "motivation": "LoRA\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4ecd\u9762\u4e34\u521d\u59cb\u5316\u7b56\u7565\u548c\u8fc7\u53c2\u6570\u5316\u7684\u6311\u6218\u3002", "method": "\u5c06\u56fa\u5b9a\u79e9\u7684LoRA\u77e9\u9635\u89c6\u4e3a\u5149\u6ed1\u6d41\u5f62\u4e0a\u7684\u5143\u7d20\uff0c\u901a\u8fc7\u6d41\u5f62\u4e0a\u7684\u6700\u5feb\u635f\u5931\u4e0b\u964d\u65b9\u5411\u63d0\u4f9b\u521d\u59cb\u5316\uff0c\u5e76\u91c7\u7528\u6570\u503c\u7a33\u5b9a\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u5b9e\u73b0\u3002", "result": "\u5728LLM\u548c\u6269\u6563\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRiemannLoRA\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6LoRA\u53ca\u5176\u5148\u8fdb\u6539\u8fdb\u3002", "conclusion": "RiemannLoRA\u4e3aLoRA\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.12305", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12305", "abs": "https://arxiv.org/abs/2507.12305", "authors": ["M. Anwar Ma'sum", "Mahardhika Pratama", "Savitha Ramasamy", "Lin Liu", "Habibullah Habibullah", "Ryszard Kowalczyk"], "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning", "comment": "ICCV 2025", "summary": "The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u53c2\u6570\u589e\u957f\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u4e2d\u6570\u636e\u9690\u79c1\u7ea6\u675f\u52a0\u5267\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u5f00\u653e\u653f\u7b56\u6216\u53c2\u6570\u589e\u957f\u95ee\u9898\u3002", "method": "\u5305\u542b\u8f7b\u91cf\u7ea7\u63d0\u793a\u751f\u6210\u5668\u3001\u53ef\u8bad\u7ec3\u7f29\u653e\u79fb\u4f4d\u5668\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u6cdb\u5316\u4fdd\u6301\u548c\u786c\u8f6f\u66f4\u65b0\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53c2\u6570\u8f83\u5c11\u4e14\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u9002\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.12341", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12341", "abs": "https://arxiv.org/abs/2507.12341", "authors": ["Antoine Saillenfest", "Pirmin Lemberger"], "title": "Nonlinear Concept Erasure: a Density Matching Approach", "comment": "17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)", "summary": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLEOPARD\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u4ece\u6587\u672c\u8868\u793a\u4e2d\u79fb\u9664\u654f\u611f\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4ed6\u8bed\u4e49\u4fe1\u606f\uff0c\u4ee5\u4fc3\u8fdb\u516c\u5e73\u6027\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u786e\u4fdd\u795e\u7ecf\u6a21\u578b\u65e0\u6cd5\u4ece\u6587\u672c\u8868\u793a\u4e2d\u63a8\u65ad\u654f\u611f\u4fe1\u606f\uff08\u5982\u6027\u522b\u6216\u79cd\u65cf\uff09\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5173\u6ce8\u516c\u5e73\u6027\u65f6\u3002", "method": "\u91c7\u7528\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u5b66\u4e60\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u6b63\u4ea4\u6295\u5f71\uff0c\u4f7f\u79bb\u6563\u6982\u5ff5\u7684\u6761\u4ef6\u7279\u5f81\u5206\u5e03\u5728\u6295\u5f71\u540e\u65e0\u6cd5\u533a\u5206\uff0c\u540c\u65f6\u901a\u8fc7\u8c03\u6574\u6295\u5f71\u79e9\u63a7\u5236\u4fe1\u606f\u79fb\u9664\u7a0b\u5ea6\u3002", "result": "LEOPARD\u5728\u7ecf\u5178\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u79bb\u6563\u5c5e\u6027\u7684\u975e\u7ebf\u6027\u64e6\u9664\uff0c\u5e76\u5728\u6df1\u5ea6\u975e\u7ebf\u6027\u5206\u7c7b\u5668\u4e2d\u6709\u6548\u51cf\u5c11\u4e86\u504f\u89c1\u3002", "conclusion": "LEOPARD\u65b9\u6cd5\u5728\u79fb\u9664\u654f\u611f\u4fe1\u606f\u548c\u4fc3\u8fdb\u516c\u5e73\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12144", "categories": ["cs.LG", "physics.ao-ph", "86-10, 68T07", "I.2.1; I.6.5; G.3"], "pdf": "https://arxiv.org/pdf/2507.12144", "abs": "https://arxiv.org/abs/2507.12144", "authors": ["Boris Bonev", "Thorsten Kurth", "Ankur Mahesh", "Mauro Bisson", "Jean Kossaifi", "Karthik Kashinath", "Anima Anandkumar", "William D. Collins", "Michael S. Pritchard", "Alexander Keller"], "title": "FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale", "comment": null, "summary": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 90-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 20 seconds. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions.", "AI": {"tldr": "FourCastNet 3\u901a\u8fc7\u51e0\u4f55\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u5168\u7403\u5929\u6c14\u5efa\u6a21\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u51c6\u786e\u7684\u6982\u7387\u96c6\u5408\u9884\u62a5\uff0c\u8d85\u8d8a\u4f20\u7edf\u6a21\u578b\u548c\u6269\u6563\u65b9\u6cd5\u3002", "motivation": "\u6539\u8fdb\u5168\u7403\u5929\u6c14\u5efa\u6a21\uff0c\u63d0\u4f9b\u66f4\u5feb\u901f\u3001\u51c6\u786e\u7684\u6982\u7387\u9884\u62a5\uff0c\u540c\u65f6\u4fdd\u6301\u5149\u8c31\u771f\u5b9e\u6027\u548c\u52a8\u6001\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u7eaf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u6a21\u578b\u548c\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u8303\u5f0f\uff0c\u9002\u5e94\u7403\u9762\u51e0\u4f55\u7279\u6027\u3002", "result": "\u9884\u62a5\u901f\u5ea6\u63d0\u53478\u81f360\u500d\uff0c\u7cbe\u5ea6\u8d85\u8d8a\u4f20\u7edf\u6a21\u578b\uff0c60\u5929\u5185\u4fdd\u6301\u826f\u597d\u6982\u7387\u6821\u51c6\u548c\u5149\u8c31\u771f\u5b9e\u6027\u3002", "conclusion": "FourCastNet 3\u5728\u8ba1\u7b97\u6548\u7387\u3001\u6982\u7387\u6280\u80fd\u548c\u5149\u8c31\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u671b\u6539\u8fdb\u6c14\u8c61\u9884\u62a5\u548c\u9884\u8b66\u7cfb\u7edf\u3002"}}
{"id": "2507.12314", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12314", "abs": "https://arxiv.org/abs/2507.12314", "authors": ["Zihao Xue", "Zhen Bi", "Long Ma", "Zhenlin Hu", "Yan Wang", "Zhenfang Liu", "Qing Sheng", "Jie Xiao", "Jungang Lou"], "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack", "comment": null, "summary": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u673a\u5236Thought Purity (TP)\uff0c\u7528\u4e8e\u4fdd\u62a4\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u514d\u53d7Chain-of-Thought Attack (CoTA)\u7684\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728Chain-of-Thought\u751f\u6210\u8fc7\u7a0b\u4e2d\u6613\u53d7\u5b89\u5168\u5a01\u80c1\uff08\u5982\u540e\u95e8\u63d0\u793a\u653b\u51fb\uff09\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u63a8\u7406\u673a\u5236\u88ab\u7834\u574f\u3002", "method": "\u63d0\u51faThought Purity (TP)\u9632\u5fa1\u8303\u5f0f\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a\u5b89\u5168\u4f18\u5316\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3001\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7684\u89c4\u5219\u7ea6\u675f\u548c\u81ea\u9002\u5e94\u76d1\u63a7\u6307\u6807\u3002", "result": "TP\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9CoTA\u653b\u51fb\u7684\u62b5\u6297\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u64cd\u4f5c\u6548\u80fd\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u67b6\u6784\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "TP\u662f\u9996\u4e2a\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u63a8\u7406\u7cfb\u7edf\u4e2dCoTA\u6f0f\u6d1e\u7684\u5168\u9762\u9632\u5fa1\u673a\u5236\uff0c\u5e73\u8861\u4e86\u5b89\u5168\u6027\u4e0e\u529f\u80fd\u6027\u3002"}}
{"id": "2507.12165", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12165", "abs": "https://arxiv.org/abs/2507.12165", "authors": ["Fouad Oubari", "Mohamed El-Baha", "Raphael Meunier", "Rodrigue D\u00e9catoire", "Mathilde Mougeot"], "title": "Multi-Component VAE with Gaussian Markov Random Field", "comment": null, "summary": "Multi-component datasets with intricate dependencies, like industrial\nassemblies or multi-modal imaging, challenge current generative modeling\ntechniques. Existing Multi-component Variational AutoEncoders typically rely on\nsimplified aggregation strategies, neglecting critical nuances and consequently\ncompromising structural coherence across generated components. To explicitly\naddress this gap, we introduce the Gaussian Markov Random Field Multi-Component\nVariational AutoEncoder , a novel generative framework embedding Gaussian\nMarkov Random Fields into both prior and posterior distributions. This design\nchoice explicitly models cross-component relationships, enabling richer\nrepresentation and faithful reproduction of complex interactions. Empirically,\nour GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula\ndataset specifically constructed to evaluate intricate component relationships,\ndemonstrates competitive results on the PolyMNIST benchmark, and significantly\nenhances structural coherence on the real-world BIKED dataset. Our results\nindicate that the GMRF MCVAE is especially suited for practical applications\ndemanding robust and realistic modeling of multi-component coherence", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578bGMRF MCVAE\uff0c\u901a\u8fc7\u5d4c\u5165\u9ad8\u65af\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\u6765\u663e\u5f0f\u5efa\u6a21\u591a\u7ec4\u4ef6\u5173\u7cfb\uff0c\u63d0\u5347\u4e86\u751f\u6210\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u7ec4\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4f9d\u8d56\u7b80\u5316\u805a\u5408\u7b56\u7565\uff0c\u5ffd\u7565\u4e86\u5173\u952e\u7ec6\u8282\uff0c\u5bfc\u81f4\u751f\u6210\u7ec4\u4ef6\u95f4\u7ed3\u6784\u4e00\u81f4\u6027\u4e0d\u8db3\u3002", "method": "\u5728\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03\u4e2d\u5d4c\u5165\u9ad8\u65af\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\uff0c\u663e\u5f0f\u5efa\u6a21\u8de8\u7ec4\u4ef6\u5173\u7cfb\u3002", "result": "\u5728\u5408\u6210Copula\u6570\u636e\u96c6\u3001PolyMNIST\u57fa\u51c6\u548c\u771f\u5b9eBIKED\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "conclusion": "GMRF MCVAE\u9002\u7528\u4e8e\u9700\u8981\u591a\u7ec4\u4ef6\u4e00\u81f4\u6027\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.12412", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12412", "abs": "https://arxiv.org/abs/2507.12412", "authors": ["Dzung Dinh", "Boqi Chen", "Marc Niethammer", "Junier Oliva"], "title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data", "comment": null, "summary": "In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines.", "AI": {"tldr": "NOCTA\u662f\u4e00\u79cd\u975e\u8d2a\u5a6a\u7684\u76ee\u6807\u6210\u672c\u6743\u8861\u83b7\u53d6\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u9884\u6d4b\u4efb\u52a1\u4e2d\u52a8\u6001\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u7279\u5f81\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u4e2d\uff0c\u52a8\u6001\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u7279\u5f81\u4ee5\u5e73\u8861\u6210\u672c\u548c\u65f6\u95f4\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86NOCTA\u65b9\u6cd5\uff0c\u5305\u62ec\u975e\u53c2\u6570\uff08NOCTA-NP\uff09\u548c\u53c2\u6570\uff08NOCTA-P\uff09\u4e24\u79cd\u4e92\u8865\u7684\u4f30\u8ba1\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNOCTA\u5728\u5408\u6210\u548c\u771f\u5b9e\u533b\u7597\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "NOCTA\u80fd\u6709\u6548\u89e3\u51b3\u52a8\u6001\u7279\u5f81\u83b7\u53d6\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u9884\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2507.12166", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12166", "abs": "https://arxiv.org/abs/2507.12166", "authors": ["Xiucheng Wang", "Qiming Zhang", "Nan Cheng", "Junting Chen", "Zezhong Zhang", "Zan Li", "Shuguang Cui", "Xuemin Shen"], "title": "RadioDiff-3D: A 3D$\\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication", "comment": null, "summary": "Radio maps (RMs) serve as a critical foundation for enabling\nenvironment-aware wireless communication, as they provide the spatial\ndistribution of wireless channel characteristics. Despite recent progress in RM\nconstruction using data-driven approaches, most existing methods focus solely\non pathloss prediction in a fixed 2D plane, neglecting key parameters such as\ndirection of arrival (DoA), time of arrival (ToA), and vertical spatial\nvariations. Such a limitation is primarily due to the reliance on static\nlearning paradigms, which hinder generalization beyond the training data\ndistribution. To address these challenges, we propose UrbanRadio3D, a\nlarge-scale, high-resolution 3D RM dataset constructed via ray tracing in\nrealistic urban environments. UrbanRadio3D is over 37$\\times$3 larger than\nprevious datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,\nforming a novel 3D$\\times$33D dataset with 7$\\times$3 more height layers than\nprior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet\nwith 3D convolutional operators is proposed. Moreover, we further introduce\nRadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D\nconvolutional architecture. RadioDiff-3D supports both radiation-aware\nscenarios with known transmitter locations and radiation-unaware settings based\non sparse spatial observations. Extensive evaluations on UrbanRadio3D validate\nthat RadioDiff-3D achieves superior performance in constructing rich,\nhigh-dimensional radio maps under diverse environmental dynamics. This work\nprovides a foundational dataset and benchmark for future research in 3D\nenvironment-aware communication. The dataset is available at\nhttps://github.com/UNIC-Lab/UrbanRadio3D.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86UrbanRadio3D\u6570\u636e\u96c6\u548cRadioDiff-3D\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u9ad8\u5206\u8fa8\u73873D\u65e0\u7ebf\u7535\u5730\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65b9\u5411\u3001\u65f6\u95f4\u548c\u5782\u76f4\u7a7a\u95f4\u53d8\u5316\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65e0\u7ebf\u7535\u5730\u56fe\u6784\u5efa\u65b9\u6cd5\u4ec5\u5173\u6ce8\u56fa\u5b9a2D\u5e73\u9762\u7684\u8def\u5f84\u635f\u8017\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u65b9\u5411\u3001\u65f6\u95f4\u548c\u5782\u76f4\u7a7a\u95f4\u53d8\u5316\u7b49\u5173\u952e\u53c2\u6570\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5c04\u7ebf\u8ffd\u8e2a\u6784\u5efaUrbanRadio3D\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e3D\u5377\u79ef\u7684UNet\u548c\u6269\u6563\u6a21\u578bRadioDiff-3D\uff0c\u652f\u6301\u8f90\u5c04\u611f\u77e5\u548c\u975e\u611f\u77e5\u573a\u666f\u3002", "result": "RadioDiff-3D\u5728UrbanRadio3D\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u6784\u5efa\u4e30\u5bcc\u7684\u9ad8\u7ef4\u65e0\u7ebf\u7535\u5730\u56fe\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u672a\u67653D\u73af\u5883\u611f\u77e5\u901a\u4fe1\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u6570\u636e\u96c6\u548c\u57fa\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2507.12419", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12419", "abs": "https://arxiv.org/abs/2507.12419", "authors": ["Andrea Perin", "Giacomo Lagomarsini", "Claudio Gallicchio", "Giuseppe Nuti"], "title": "Mixture of Raytraced Experts", "comment": "Preliminary version (pre-submission)", "summary": "We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts\n(MoE) architecture which can dynamically select sequences of experts, producing\ncomputational graphs of variable width and depth. Existing MoE architectures\ngenerally require a fixed amount of computation for a given sample. Our\napproach, in contrast, yields predictions with increasing accuracy as the\ncomputation cycles through the experts' sequence. We train our model by\niteratively sampling from a set of candidate experts, unfolding the sequence\nakin to how Recurrent Neural Networks are trained. Our method does not require\nload-balancing mechanisms, and preliminary experiments show a reduction in\ntraining epochs of 10\\% to 40\\% with a comparable/higher accuracy. These\nresults point to new research directions in the field of MoEs, allowing the\ndesign of potentially faster and more expressive models. The code is available\nat https://github.com/nutig/RayTracing", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5e8f\u5217\u7684\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff08MoE\uff09\uff0c\u901a\u8fc7\u53ef\u53d8\u8ba1\u7b97\u56fe\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u65e0\u9700\u8d1f\u8f7d\u5747\u8861\u673a\u5236\uff0c\u5b9e\u9a8c\u663e\u793a\u8bad\u7ec3\u5468\u671f\u51cf\u5c1110%-40%\u3002", "motivation": "\u73b0\u6709MoE\u67b6\u6784\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u8ba1\u7b97\u91cf\u56fa\u5b9a\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684MoE\u67b6\u6784\u3002", "method": "\u91c7\u7528\u7c7b\u4f3cRNN\u7684\u5e8f\u5217\u5c55\u5f00\u65b9\u6cd5\u8bad\u7ec3\u6a21\u578b\uff0c\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5e8f\u5217\uff0c\u751f\u6210\u53ef\u53d8\u8ba1\u7b97\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u8bad\u7ec3\u5468\u671f\u51cf\u5c1110%-40%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aMoE\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u53ef\u80fd\u8bbe\u8ba1\u51fa\u66f4\u5feb\u3001\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u3002"}}
{"id": "2507.12192", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12192", "abs": "https://arxiv.org/abs/2507.12192", "authors": ["Victor F. Lopes de Souza", "Karima Bakhti", "Sofiane Ramdani", "Denis Mottet", "Abdelhak Imoussaten"], "title": "Explainable Evidential Clustering", "comment": null, "summary": "Unsupervised classification is a fundamental machine learning problem.\nReal-world data often contain imperfections, characterized by uncertainty and\nimprecision, which are not well handled by traditional methods. Evidential\nclustering, based on Dempster-Shafer theory, addresses these challenges. This\npaper explores the underexplored problem of explaining evidential clustering\nresults, which is crucial for high-stakes domains such as healthcare. Our\nanalysis shows that, in the general case, representativity is a necessary and\nsufficient condition for decision trees to serve as abductive explainers.\nBuilding on the concept of representativity, we generalize this idea to\naccommodate partial labeling through utility functions. These functions enable\nthe representation of \"tolerable\" mistakes, leading to the definition of\nevidential mistakeness as explanation cost and the construction of explainers\ntailored to evidential classifiers. Finally, we propose the Iterative\nEvidential Mistake Minimization (IEMM) algorithm, which provides interpretable\nand cautious decision tree explanations for evidential clustering functions. We\nvalidate the proposed algorithm on synthetic and real-world data. Taking into\naccount the decision-maker's preferences, we were able to provide an\nexplanation that was satisfactory up to 93% of the time.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8eDempster-Shafer\u7406\u8bba\u7684\u8bc1\u636e\u805a\u7c7b\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u91ca\u5176\u7ed3\u679c\u7684\u6846\u67b6\u3002\u901a\u8fc7\u5f15\u5165\u4ee3\u8868\u6027\u548c\u6548\u7528\u51fd\u6570\uff0c\u5b9a\u4e49\u4e86\u8bc1\u636e\u9519\u8bef\u6027\uff0c\u5e76\u63d0\u51fa\u4e86IEMM\u7b97\u6cd5\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u6811\u89e3\u91ca\u3002", "motivation": "\u73b0\u5b9e\u6570\u636e\u5e38\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u4e0d\u7cbe\u786e\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u3002\u8bc1\u636e\u805a\u7c7b\u80fd\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5176\u7ed3\u679c\u89e3\u91ca\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u3002", "method": "\u57fa\u4e8e\u4ee3\u8868\u6027\u6761\u4ef6\uff0c\u901a\u8fc7\u6548\u7528\u51fd\u6570\u6269\u5c55\u90e8\u5206\u6807\u7b7e\uff0c\u5b9a\u4e49\u8bc1\u636e\u9519\u8bef\u6027\u4f5c\u4e3a\u89e3\u91ca\u6210\u672c\uff0c\u5e76\u63d0\u51faIEMM\u7b97\u6cd5\u6784\u5efa\u89e3\u91ca\u5668\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0cIEMM\u7b97\u6cd5\u80fd\u63d0\u4f9b\u9ad8\u8fbe93%\u6ee1\u610f\u5ea6\u7684\u89e3\u91ca\u3002", "conclusion": "IEMM\u7b97\u6cd5\u4e3a\u8bc1\u636e\u805a\u7c7b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u8c28\u614e\u7684\u51b3\u7b56\u6811\u89e3\u91ca\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u3002"}}
{"id": "2507.12218", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.12218", "abs": "https://arxiv.org/abs/2507.12218", "authors": ["Tomohisa Okazaki"], "title": "Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation", "comment": null, "summary": "Many physical systems are described by partial differential equations (PDEs),\nand solving these equations and estimating their coefficients or boundary\nconditions (BCs) from observational data play a crucial role in understanding\nthe associated phenomena. Recently, a machine learning approach known as\nphysics-informed neural network, which solves PDEs using neural networks by\nminimizing the sum of residuals from the PDEs, BCs, and data, has gained\nsignificant attention in the scientific community. In this study, we\ninvestigate a physics-informed linear model (PILM) that uses linear\ncombinations of basis functions to represent solutions, thereby enabling an\nanalytical representation of optimal solutions. The PILM was formulated and\nverified for illustrative forward and inverse problems including cases with\nuncertain BCs. Furthermore, the PILM was applied to estimate crustal strain\nrates using geodetic data. Specifically, physical regularization that enforces\nelastic equilibrium on the velocity fields was compared with mathematical\nregularization that imposes smoothness constraints. From a Bayesian\nperspective, mathematical regularization exhibited superior performance. The\nPILM provides an analytically solvable framework applicable to linear forward\nand inverse problems, underdetermined systems, and physical regularization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u57fa\u51fd\u6570\u7ec4\u5408\u7684\u7269\u7406\u4fe1\u606f\u7ebf\u6027\u6a21\u578b\uff08PILM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u7684\u6b63\u53cd\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5730\u58f3\u5e94\u53d8\u7387\u4f30\u8ba1\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u89e3\u51b3PDEs\u548c\u4f30\u8ba1\u53c2\u6570\u6216\u8fb9\u754c\u6761\u4ef6\u65f6\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u6027\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0cPILM\u901a\u8fc7\u7ebf\u6027\u6a21\u578b\u63d0\u4f9b\u89e3\u6790\u89e3\u6846\u67b6\uff0c\u7b80\u5316\u4e86\u6c42\u89e3\u8fc7\u7a0b\u3002", "method": "PILM\u4f7f\u7528\u7ebf\u6027\u57fa\u51fd\u6570\u7ec4\u5408\u8868\u793a\u89e3\uff0c\u901a\u8fc7\u6700\u5c0f\u5316PDEs\u3001\u8fb9\u754c\u6761\u4ef6\u548c\u6570\u636e\u7684\u6b8b\u5dee\u548c\u6765\u6c42\u89e3\u95ee\u9898\uff0c\u5e76\u6bd4\u8f83\u4e86\u7269\u7406\u6b63\u5219\u5316\u548c\u6570\u5b66\u6b63\u5219\u5316\u7684\u6548\u679c\u3002", "result": "PILM\u5728\u6b63\u53cd\u95ee\u9898\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u6570\u5b66\u6b63\u5219\u5316\u5728\u8d1d\u53f6\u65af\u89c6\u89d2\u4e0b\u4f18\u4e8e\u7269\u7406\u6b63\u5219\u5316\u3002", "conclusion": "PILM\u4e3a\u7ebf\u6027\u6b63\u53cd\u95ee\u9898\u3001\u6b20\u5b9a\u7cfb\u7edf\u548c\u7269\u7406\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u53ef\u89e3\u6790\u6c42\u89e3\u7684\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.12224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12224", "abs": "https://arxiv.org/abs/2507.12224", "authors": ["Razvan Pascanu", "Clare Lyle", "Ionut-Vlad Modoranu", "Naima Elosegui Borras", "Dan Alistarh", "Petar Velickovic", "Sarath Chandar", "Soham De", "James Martens"], "title": "Optimizers Qualitatively Alter Solutions And We Should Leverage This", "comment": null, "summary": "Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not\nguarantee convergence to a unique global minimum of the loss when using\noptimizers relying only on local information, such as SGD. Indeed, this was a\nprimary source of skepticism regarding the feasibility of DNNs in the early\ndays of the field. The past decades of progress in deep learning have revealed\nthis skepticism to be misplaced, and a large body of empirical evidence shows\nthat sufficiently large DNNs following standard training protocols exhibit\nwell-behaved optimization dynamics that converge to performant solutions. This\nsuccess has biased the community to use convex optimization as a mental model\nfor learning, leading to a focus on training efficiency, either in terms of\nrequired iteration, FLOPs or wall-clock time, when improving optimizers. We\nargue that, while this perspective has proven extremely fruitful, another\nperspective specific to DNNs has received considerably less attention: the\noptimizer not only influences the rate of convergence, but also the qualitative\nproperties of the learned solutions. Restated, the optimizer can and will\nencode inductive biases and change the effective expressivity of a given class\nof models. Furthermore, we believe the optimizer can be an effective way of\nencoding desiderata in the learning process. We contend that the community\nshould aim at understanding the biases of already existing methods, as well as\naim to build new optimizers with the explicit intent of inducing certain\nproperties of the solution, rather than solely judging them based on their\nconvergence rates. We hope our arguments will inspire research to improve our\nunderstanding of how the learning process can impact the type of solution we\nconverge to, and lead to a greater recognition of optimizers design as a\ncritical lever that complements the roles of architecture and data in shaping\nmodel outcomes.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u4f18\u5316\u5668\u7684\u89d2\u8272\uff0c\u6307\u51fa\u5176\u4e0d\u4ec5\u5f71\u54cd\u6536\u655b\u901f\u5ea6\uff0c\u8fd8\u5f71\u54cd\u5b66\u4e60\u89e3\u7684\u6027\u8d28\uff0c\u547c\u5401\u793e\u533a\u5173\u6ce8\u4f18\u5316\u5668\u7684\u8bbe\u8ba1\u4ee5\u5f15\u5bfc\u6a21\u578b\u7279\u6027\u3002", "motivation": "\u65e9\u671f\u5bf9DNNs\u53ef\u884c\u6027\u7684\u8d28\u7591\u6e90\u4e8e\u5176\u975e\u7ebf\u6027\u7279\u6027\u5bfc\u81f4\u65e0\u6cd5\u4fdd\u8bc1\u6536\u655b\u5230\u552f\u4e00\u5168\u5c40\u6700\u5c0f\u3002\u5c3d\u7ba1\u5b9e\u8bc1\u8868\u660eDNNs\u4f18\u5316\u8868\u73b0\u826f\u597d\uff0c\u4f46\u793e\u533a\u8fc7\u4e8e\u5173\u6ce8\u6548\u7387\u800c\u5ffd\u89c6\u4e86\u4f18\u5316\u5668\u5bf9\u89e3\u6027\u8d28\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u4f18\u5316\u5668\u7684\u884c\u4e3a\uff0c\u63d0\u51fa\u4f18\u5316\u5668\u4e0d\u4ec5\u5f71\u54cd\u6536\u655b\u901f\u5ea6\uff0c\u8fd8\u80fd\u7f16\u7801\u5f52\u7eb3\u504f\u7f6e\u5e76\u6539\u53d8\u6a21\u578b\u7684\u6709\u6548\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u4f18\u5316\u5668\u53ef\u4ee5\u6210\u4e3a\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7f16\u7801\u671f\u671b\u7279\u6027\u7684\u6709\u6548\u5de5\u5177\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63d0\u9ad8\u6548\u7387\u7684\u624b\u6bb5\u3002", "conclusion": "\u547c\u5401\u793e\u533a\u66f4\u6df1\u5165\u5730\u7406\u89e3\u4f18\u5316\u5668\u7684\u504f\u7f6e\uff0c\u5e76\u8bbe\u8ba1\u65b0\u7684\u4f18\u5316\u5668\u4ee5\u5f15\u5bfc\u7279\u5b9a\u89e3\u6027\u8d28\uff0c\u800c\u4e0d\u4ec5\u5173\u6ce8\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2507.12257", "categories": ["cs.LG", "physics.data-an", "stat.ML", "stat.OT"], "pdf": "https://arxiv.org/pdf/2507.12257", "abs": "https://arxiv.org/abs/2507.12257", "authors": ["Matteo Tusoni", "Giuseppe Masi", "Andrea Coletta", "Aldo Glielmo", "Viviana Arrigoni", "Novella Bartolini"], "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws", "comment": null, "summary": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u529f\u7387\u8c31\u7279\u5f81\u7684\u9c81\u68d2\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u968f\u673a\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u566a\u58f0\u654f\u611f\uff0c\u5bb9\u6613\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\u3002", "method": "\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u7684\u529f\u7387\u8c31\u5206\u5e03\u7279\u5f81\uff0c\u63d0\u53d6\u56e0\u679c\u4fe1\u53f7\uff0c\u6784\u5efa\u9c81\u68d2\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u529f\u7387\u8c31\u7279\u5f81\u6709\u6548\u63d0\u5347\u4e86\u56e0\u679c\u53d1\u73b0\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.12297", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12297", "abs": "https://arxiv.org/abs/2507.12297", "authors": ["Yuan-Chen Shu", "Zhiwei Lin", "Yongtao Wang"], "title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging", "comment": null, "summary": "To address the performance limitations of the Segment Anything Model (SAM) in\nspecific domains, existing works primarily adopt adapter-based one-step\nadaptation paradigms. However, some of these methods are specific developed for\nspecific domains. If used on other domains may lead to performance degradation.\nThis issue of catastrophic forgetting severely limits the model's scalability.\nTo address this issue, this paper proposes RegCL, a novel non-replay continual\nlearning (CL) framework designed for efficient multi-domain knowledge\nintegration through model merging. Specifically, RegCL incorporates the model\nmerging algorithm into the continual learning paradigm by merging the\nparameters of SAM's adaptation modules (e.g., LoRA modules) trained on\ndifferent domains. The merging process is guided by weight optimization, which\nminimizes prediction discrepancies between the merged model and each of the\ndomain-specific models. RegCL effectively consolidates multi-domain knowledge\nwhile maintaining parameter efficiency, i.e., the model size remains constant\nregardless of the number of tasks, and no historical data storage is required.\nExperimental results demonstrate that RegCL achieves favorable continual\nlearning performance across multiple downstream datasets, validating its\neffectiveness in dynamic scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRegCL\u7684\u65b0\u578b\u975e\u56de\u653e\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u5b9e\u73b0\u591a\u9886\u57df\u77e5\u8bc6\u7684\u9ad8\u6548\u6574\u5408\uff0c\u89e3\u51b3\u4e86Segment Anything Model (SAM)\u5728\u7279\u5b9a\u9886\u57df\u4e2d\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u57fa\u4e8e\u9002\u914d\u5668\u7684\u4e00\u6b65\u9002\u5e94\u8303\u5f0f\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u80fd\u56e0\u9886\u57df\u7279\u5b9a\u6027\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "RegCL\u5c06\u6a21\u578b\u5408\u5e76\u7b97\u6cd5\u5f15\u5165\u6301\u7eed\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u5408\u5e76\u4e0d\u540c\u9886\u57df\u8bad\u7ec3\u7684SAM\u9002\u914d\u6a21\u5757\uff08\u5982LoRA\u6a21\u5757\uff09\u53c2\u6570\uff0c\u5e76\u4ee5\u6743\u91cd\u4f18\u5316\u4e3a\u6307\u5bfc\uff0c\u6700\u5c0f\u5316\u9884\u6d4b\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRegCL\u5728\u591a\u4e2a\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "RegCL\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u6709\u6548\u6574\u5408\u591a\u9886\u57df\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u6548\u7387\uff0c\u65e0\u9700\u5386\u53f2\u6570\u636e\u5b58\u50a8\uff0c\u6a21\u578b\u5927\u5c0f\u6052\u5b9a\u3002"}}
{"id": "2507.12380", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12380", "abs": "https://arxiv.org/abs/2507.12380", "authors": ["Maximilian Krahn", "Vikas Garg"], "title": "Heat Kernel Goes Topological", "comment": null, "summary": "Topological neural networks have emerged as powerful successors of graph\nneural networks. However, they typically involve higher-order message passing,\nwhich incurs significant computational expense. We circumvent this issue with a\nnovel topological framework that introduces a Laplacian operator on\ncombinatorial complexes (CCs), enabling efficient computation of heat kernels\nthat serve as node descriptors. Our approach captures multiscale information\nand enables permutation-equivariant representations, allowing easy integration\ninto modern transformer-based architectures.\n  Theoretically, the proposed method is maximally expressive because it can\ndistinguish arbitrary non-isomorphic CCs. Empirically, it significantly\noutperforms existing topological methods in terms of computational efficiency.\nBesides demonstrating competitive performance with the state-of-the-art\ndescriptors on standard molecular datasets, it exhibits superior capability in\ndistinguishing complex topological structures and avoiding blind spots on\ntopological benchmarks. Overall, this work advances topological deep learning\nby providing expressive yet scalable representations, thereby opening up\nexciting avenues for molecular classification and property prediction tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u5408\u590d\u5f62\uff08CCs\uff09\u7684\u65b0\u578b\u62d3\u6251\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u8ba1\u7b97\u70ed\u6838\uff0c\u89e3\u51b3\u4e86\u9ad8\u9636\u6d88\u606f\u4f20\u9012\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u62d3\u6251\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u9ad8\u9636\u6d88\u606f\u4f20\u9012\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6355\u6349\u591a\u5c3a\u5ea6\u4fe1\u606f\u5e76\u5b9e\u73b0\u7f6e\u6362\u7b49\u53d8\u8868\u793a\u3002", "method": "\u5f15\u5165\u7ec4\u5408\u590d\u5f62\u4e0a\u7684\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u8ba1\u7b97\u70ed\u6838\u4f5c\u4e3a\u8282\u70b9\u63cf\u8ff0\u7b26\uff0c\u652f\u6301\u591a\u5c3a\u5ea6\u4fe1\u606f\u6355\u6349\u548c\u7f6e\u6362\u7b49\u53d8\u8868\u793a\uff0c\u6613\u4e8e\u4e0e\u73b0\u4ee3Transformer\u67b6\u6784\u96c6\u6210\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u533a\u5206\u4efb\u610f\u975e\u540c\u6784\u7ec4\u5408\u590d\u5f62\uff1b\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u8ba1\u7b97\u6548\u7387\u548c\u62d3\u6251\u7ed3\u6784\u533a\u5206\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u63d0\u4f9b\u8868\u8fbe\u529b\u5f3a\u4e14\u53ef\u6269\u5c55\u7684\u8868\u793a\uff0c\u63a8\u52a8\u4e86\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u4e3a\u5206\u5b50\u5206\u7c7b\u548c\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.12383", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12383", "abs": "https://arxiv.org/abs/2507.12383", "authors": ["Mohit Prashant", "Arvind Easwaran"], "title": "Improving Reinforcement Learning Sample-Efficiency using Local Approximation", "comment": "Preprint", "summary": "In this study, we derive Probably Approximately Correct (PAC) bounds on the\nasymptotic sample-complexity for RL within the infinite-horizon Markov Decision\nProcess (MDP) setting that are sharper than those in existing literature. The\npremise of our study is twofold: firstly, the further two states are from each\nother, transition-wise, the less relevant the value of the first state is when\nlearning the $\\epsilon$-optimal value of the second; secondly, the amount of\n'effort', sample-complexity-wise, expended in learning the $\\epsilon$-optimal\nvalue of a state is independent of the number of samples required to learn the\n$\\epsilon$-optimal value of a second state that is a sufficient number of\ntransitions away from the first. Inversely, states within each other's vicinity\nhave values that are dependent on each other and will require a similar number\nof samples to learn. By approximating the original MDP using smaller MDPs\nconstructed using subsets of the original's state-space, we are able to reduce\nthe sample-complexity by a logarithmic factor to $O(SA \\log A)$ timesteps,\nwhere $S$ and $A$ are the state and action space sizes. We are able to extend\nthese results to an infinite-horizon, model-free setting by constructing a\nPAC-MDP algorithm with the aforementioned sample-complexity. We conclude with\nshowing how significant the improvement is by comparing our algorithm against\nprior work in an experimental setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u65e0\u9650\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u4e2d\u66f4\u4e25\u683c\u7684\u6837\u672c\u590d\u6742\u5ea6PAC\u754c\u9650\uff0c\u901a\u8fc7\u8fd1\u4f3c\u539f\u59cbMDP\u51cf\u5c11\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u6269\u5c55\u5230\u65e0\u6a21\u578b\u8bbe\u7f6e\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u4e2d\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\u4e0d\u591f\u7cbe\u786e\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u6790\u72b6\u6001\u95f4\u7684\u76f8\u5173\u6027\uff0c\u63d0\u51fa\u66f4\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u539f\u59cbMDP\u7684\u5b50\u96c6\u8fd1\u4f3c\uff0c\u51cf\u5c11\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u8bbe\u8ba1PAC-MDP\u7b97\u6cd5\u6269\u5c55\u5230\u65e0\u6a21\u578b\u8bbe\u7f6e\u3002", "result": "\u6837\u672c\u590d\u6742\u5ea6\u964d\u4f4e\u5230O(SA log A)\u65f6\u95f4\u6b65\uff0c\u5b9e\u9a8c\u8868\u660e\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\uff0c\u9002\u7528\u4e8e\u65e0\u9650\u65f6\u95f4MDP\u548c\u65e0\u6a21\u578b\u8bbe\u7f6e\u3002"}}
{"id": "2507.12384", "categories": ["cs.LG", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.12384", "abs": "https://arxiv.org/abs/2507.12384", "authors": ["Bo Wen", "Guoyun Gao", "Zhicheng Xu", "Ruibin Mao", "Xiaojuan Qi", "X. Sharon Hu", "Xunzhao Yin", "Can Li"], "title": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries", "comment": null, "summary": "The rapid advancement of artificial intelligence has raised concerns\nregarding its trustworthiness, especially in terms of interpretability and\nrobustness. Tree-based models like Random Forest and XGBoost excel in\ninterpretability and accuracy for tabular data, but scaling them remains\ncomputationally expensive due to poor data locality and high data dependence.\nPrevious efforts to accelerate these models with analog content addressable\nmemory (CAM) have struggled, due to the fact that the difficult-to-implement\nsharp decision boundaries are highly susceptible to device variations, which\nleads to poor hardware performance and vulnerability to adversarial attacks.\nThis work presents a novel hardware-software co-design approach using $MoS_2$\nFlash-based analog CAM with inherent soft boundaries, enabling efficient\ninference with soft tree-based models. Our soft tree model inference\nexperiments on $MoS_2$ analog CAM arrays show this method achieves exceptional\nrobustness against device variation and adversarial attacks while achieving\nstate-of-the-art accuracy. Specifically, our fabricated analog CAM arrays\nachieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,\nwhile maintaining decision explainability. Our experimentally calibrated model\nvalidated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device\nthreshold variation, compared to a $45.3\\%$ drop for traditional decision\ntrees. This work paves the way for specialized hardware that enhances AI's\ntrustworthiness and efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e$MoS_2$\u95ea\u5b58\u6a21\u62dfCAM\u7684\u8f6f\u6811\u6a21\u578b\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6811\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u5bf9\u6a21\u578b\u53ef\u4fe1\u5ea6\u7684\u62c5\u5fe7\uff0c\u5c24\u5176\u662f\u5728\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u3002\u6811\u6a21\u578b\u867d\u5728\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f20\u7edf\u786c\u4ef6\u5b9e\u73b0\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6613\u53d7\u8bbe\u5907\u53d8\u5316\u548c\u5bf9\u6297\u653b\u51fb\u5f71\u54cd\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528$MoS_2$\u95ea\u5b58\u6a21\u62dfCAM\u786c\u4ef6\uff0c\u7ed3\u5408\u8f6f\u6811\u6a21\u578b\uff0c\u5229\u7528\u5176\u56fa\u6709\u7684\u8f6f\u8fb9\u754c\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728WDBC\u6570\u636e\u96c6\u4e0a\u8fbe\u523096%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u5728\u8bbe\u5907\u9608\u503c\u53d8\u531610%\u65f6\uff0cMNIST\u6570\u636e\u96c6\u7684\u51c6\u786e\u7387\u4ec5\u4e0b\u964d0.6%\uff0c\u8fdc\u4f18\u4e8e\u4f20\u7edf\u51b3\u7b56\u6811\u768445.3%\u4e0b\u964d\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63d0\u5347AI\u7684\u53ef\u4fe1\u5ea6\u548c\u6548\u7387\u63d0\u4f9b\u4e86\u4e13\u7528\u786c\u4ef6\u8bbe\u8ba1\u7684\u65b0\u601d\u8def\u3002"}}
{"id": "2507.12399", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.12399", "abs": "https://arxiv.org/abs/2507.12399", "authors": ["Florian E. Dorner", "Yatong Chen", "Andr\u00e9 F. Cruz", "Fanny Yang"], "title": "ROC-n-reroll: How verifier imperfection affects test-time scaling", "comment": "35 pages, 9 Figures", "summary": "Test-time scaling aims to improve language model performance by leveraging\nadditional compute during inference. While many works have empirically studied\ntechniques like Best-of-N (BoN) and rejection sampling that make use of a\nverifier to enable test-time scaling, there is little theoretical understanding\nof how verifier imperfection affects performance. In this work, we address this\ngap. Specifically, we prove how instance-level accuracy of these methods is\nprecisely characterized by the geometry of the verifier's ROC curve.\nInterestingly, while scaling is determined by the local geometry of the ROC\ncurve for rejection sampling, it depends on global properties of the ROC curve\nfor BoN. As a consequence when the ROC curve is unknown, it is impossible to\nextrapolate the performance of rejection sampling based on the low-compute\nregime. Furthermore, while rejection sampling outperforms BoN for fixed\ncompute, in the infinite-compute limit both methods converge to the same level\nof accuracy, determined by the slope of the ROC curve near the origin. Our\ntheoretical results are confirmed by experiments on GSM8K using different\nversions of Llama and Qwen to generate and verify solutions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08test-time scaling\uff09\u4e2d\u9a8c\u8bc1\u5668\u4e0d\u5b8c\u7f8e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7ROC\u66f2\u7ebf\u7684\u51e0\u4f55\u7279\u6027\u5206\u6790\u4e86Best-of-N\u548c\u62d2\u7edd\u91c7\u6837\u7684\u5b9e\u4f8b\u7ea7\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u9a8c\u8bc1\u5668\u4e0d\u5b8c\u7f8e\u5982\u4f55\u5f71\u54cd\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\u80fd\u7684\u7406\u8bba\u7406\u89e3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9a8c\u8bc1\u5668ROC\u66f2\u7ebf\u7684\u51e0\u4f55\u7279\u6027\uff0c\u6bd4\u8f83Best-of-N\u548c\u62d2\u7edd\u91c7\u6837\u5728\u4e0d\u540c\u8ba1\u7b97\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u62d2\u7edd\u91c7\u6837\u5728\u56fa\u5b9a\u8ba1\u7b97\u4e0b\u4f18\u4e8eBest-of-N\uff0c\u4f46\u5728\u65e0\u9650\u8ba1\u7b97\u4e0b\u4e24\u8005\u6027\u80fd\u76f8\u540c\uff0c\u7531ROC\u66f2\u7ebf\u539f\u70b9\u9644\u8fd1\u7684\u659c\u7387\u51b3\u5b9a\u3002", "conclusion": "\u9a8c\u8bc1\u5668ROC\u66f2\u7ebf\u7684\u51e0\u4f55\u7279\u6027\u51b3\u5b9a\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u7ed3\u679c\u652f\u6301\u4e86\u7406\u8bba\u5206\u6790\u3002"}}
{"id": "2507.12435", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12435", "abs": "https://arxiv.org/abs/2507.12435", "authors": ["Yi Li", "David Mccoy", "Nolan Gunter", "Kaitlyn Lee", "Alejandro Schuler", "Mark van der Laan"], "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks", "comment": null, "summary": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTDA\u7684\u65b0\u6846\u67b6\uff0c\u5c06TMLE\u76f4\u63a5\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u652f\u6301\u591a\u53c2\u6570\u76ee\u6807\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u9884\u6d4b\u65b9\u9762\u5f3a\u5927\uff0c\u4f46\u5728\u56e0\u679c\u53c2\u6570\u63a8\u65ad\uff08\u5982\u6cbb\u7597\u6548\u679c\u6216\u751f\u5b58\u66f2\u7ebf\uff09\u4e0a\u7f3a\u4e4f\u6709\u6548\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u76ee\u6807\u635f\u5931\u65e0\u6cd5\u4fdd\u8bc1\u89e3\u51b3\u9ad8\u6548\u5f71\u54cd\u51fd\u6570\u65b9\u7a0b\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "TDA\u901a\u8fc7\u5206\u533a\u6a21\u578b\u53c2\u6570\uff0c\u51bb\u7ed3\u5927\u90e8\u5206\u53c2\u6570\uff0c\u4ec5\u66f4\u65b0\u4e00\u5c0f\u90e8\u5206\u201c\u76ee\u6807\u201d\u5b50\u96c6\uff0c\u5e76\u6cbf\u76ee\u6807\u68af\u5ea6\u8fed\u4ee3\u66f4\u65b0\uff0c\u4ece\u800c\u6d88\u9664\u4e00\u9636\u504f\u5dee\u5e76\u751f\u6210\u6e10\u8fd1\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u5728IHDP\u6570\u636e\u96c6\u548c\u6a21\u62df\u751f\u5b58\u6570\u636e\u4e0a\uff0cTDA\u76f8\u6bd4\u6807\u51c6\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u5668\u548c\u73b0\u6709\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u504f\u5dee\u5e76\u63d0\u9ad8\u4e86\u8986\u76d6\u7387\u3002", "conclusion": "TDA\u4e3a\u590d\u6742\u591a\u53c2\u6570\u76ee\u6807\u7684\u6df1\u5ea6\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u63a5\u3001\u53ef\u6269\u5c55\u7684\u4e25\u683c\u56e0\u679c\u63a8\u65ad\u9014\u5f84\u3002"}}
{"id": "2507.12439", "categories": ["cs.LG", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.12439", "abs": "https://arxiv.org/abs/2507.12439", "authors": ["Daniel Commey", "Rebecca A. Sarpong", "Griffith S. Klogo", "Winful Bagyl-Bac", "Garth V. Crosby"], "title": "A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning", "comment": null, "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients while preserving data privacy. However, its\nopen-participation nature exposes it to data-poisoning attacks, in which\nmalicious actors submit corrupted model updates to degrade the global model.\nExisting defenses are often reactive, relying on statistical aggregation rules\nthat can be computationally expensive and that typically assume an honest\nmajority. This paper introduces a proactive, economic defense: a lightweight\nBayesian incentive mechanism that makes malicious behavior economically\nirrational. Each training round is modeled as a Bayesian game of incomplete\ninformation in which the server, acting as the principal, uses a small, private\nvalidation dataset to verify update quality before issuing payments. The design\nsatisfies Individual Rationality (IR) for benevolent clients, ensuring their\nparticipation is profitable, and Incentive Compatibility (IC), making poisoning\nan economically dominated strategy. Extensive experiments on non-IID partitions\nof MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping\nadversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3\npercentage points lower than in a scenario with 30% label-flipping adversaries.\nThis outcome is 51.7 percentage points better than standard FedAvg, which\ncollapses under the same 50% attack. The mechanism is computationally light,\nbudget-bounded, and readily integrates into existing FL frameworks, offering a\npractical route to economically robust and sustainable FL ecosystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8d1d\u53f6\u65af\u6fc0\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u7ecf\u6d4e\u624b\u6bb5\u4e3b\u52a8\u9632\u5fa1\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u786e\u4fdd\u6076\u610f\u884c\u4e3a\u5728\u7ecf\u6d4e\u4e0a\u4e0d\u5212\u7b97\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u5f00\u653e\u53c2\u4e0e\u6027\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u591a\u4e3a\u88ab\u52a8\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5047\u8bbe\u8bda\u5b9e\u53c2\u4e0e\u8005\u5360\u591a\u6570\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8d1d\u53f6\u65af\u535a\u5f08\u6a21\u578b\uff0c\u670d\u52a1\u5668\u4f7f\u7528\u5c0f\u578b\u79c1\u6709\u9a8c\u8bc1\u6570\u636e\u96c6\u9a8c\u8bc1\u66f4\u65b0\u8d28\u91cf\u540e\u53d1\u653e\u5956\u52b1\uff0c\u6ee1\u8db3\u4e2a\u4f53\u7406\u6027\uff08IR\uff09\u548c\u6fc0\u52b1\u76f8\u5bb9\u6027\uff08IC\uff09\u3002", "result": "\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684MNIST\u548cFashionMNIST\u6570\u636e\u96c6\u4e0a\uff0c\u5373\u4f7f\u9762\u5bf950%\u7684\u6807\u7b7e\u7ffb\u8f6c\u653b\u51fb\uff0c\u6a21\u578b\u51c6\u786e\u7387\u4ecd\u4fdd\u630196.7%\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6FedAvg\u3002", "conclusion": "\u8be5\u673a\u5236\u8ba1\u7b97\u8f7b\u91cf\u3001\u9884\u7b97\u53ef\u63a7\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u4e3a\u6784\u5efa\u7ecf\u6d4e\u4e0a\u7a33\u5065\u4e14\u53ef\u6301\u7eed\u7684\u8054\u90a6\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2507.12453", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12453", "abs": "https://arxiv.org/abs/2507.12453", "authors": ["Qian Xie", "Linda Cai", "Alexander Terenin", "Peter I. Frazier", "Ziv Scully"], "title": "Cost-aware Stopping for Bayesian Optimization", "comment": null, "summary": "In automated machine learning, scientific discovery, and other applications\nof Bayesian optimization, deciding when to stop evaluating expensive black-box\nfunctions is an important practical consideration. While several adaptive\nstopping rules have been proposed, in the cost-aware setting they lack\nguarantees ensuring they stop before incurring excessive function evaluation\ncosts. We propose a cost-aware stopping rule for Bayesian optimization that\nadapts to varying evaluation costs and is free of heuristic tuning. Our rule is\ngrounded in a theoretical connection to state-of-the-art cost-aware acquisition\nfunctions, namely the Pandora's Box Gittins Index (PBGI) and log expected\nimprovement per cost. We prove a theoretical guarantee bounding the expected\ncumulative evaluation cost incurred by our stopping rule when paired with these\ntwo acquisition functions. In experiments on synthetic and empirical tasks,\nincluding hyperparameter optimization and neural architecture size search, we\nshow that combining our stopping rule with the PBGI acquisition function\nconsistently matches or outperforms other acquisition-function--stopping-rule\npairs in terms of cost-adjusted simple regret, a metric capturing trade-offs\nbetween solution quality and cumulative evaluation cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u611f\u77e5\u7684\u505c\u6b62\u89c4\u5219\uff0c\u7528\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u65e0\u9700\u542f\u53d1\u5f0f\u8c03\u6574\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\uff0c\u5982\u4f55\u5728\u9ad8\u6210\u672c\u7684\u9ed1\u76d2\u51fd\u6570\u8bc4\u4f30\u4e2d\u9002\u65f6\u505c\u6b62\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6210\u672c\u4fdd\u8bc1\u3002", "method": "\u57fa\u4e8ePandora's Box Gittins Index (PBGI)\u548clog expected improvement per cost\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u505c\u6b62\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u89c4\u5219\u4e0ePBGI\u7ed3\u5408\u65f6\uff0c\u5728\u6210\u672c\u8c03\u6574\u7684\u7b80\u5355\u9057\u61be\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u505c\u6b62\u89c4\u5219\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u9ad8\u6210\u672c\u4f18\u5316\u4efb\u52a1\u3002"}}
