<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 81]
- [cs.LG](#cs.LG) [Total: 138]
- [cs.AI](#cs.AI) [Total: 51]
- [cs.HC](#cs.HC) [Total: 35]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)
*Khalid Hasan,Jamil Saquer,Mukulika Ghosh*

Main category: cs.CL

TL;DR: 论文评估了多种Transformer模型（如BERT、RoBERTa等）与LSTM在心理健康分类任务中的表现，发现Transformer模型表现更优，尤其是RoBERTa，同时LSTM结合BERT嵌入也表现良好。


<details>
  <summary>Details</summary>
Motivation: 心理健康障碍日益普遍，需要开发自动化工具进行早期检测和监控。

Method: 使用多种Transformer模型和LSTM方法，结合不同文本嵌入技术，对Reddit数据进行心理健康分类。

Result: Transformer模型表现优于传统方法，RoBERTa在测试集上F1分数达99.54%，LSTM结合BERT嵌入也表现优异（F1>94%）。

Conclusion: Transformer模型在实时、可扩展的心理健康监测中效果显著，为临床和数字心理健康干预提供了新思路。

Abstract: The rising prevalence of mental health disorders necessitates the development
of robust, automated tools for early detection and monitoring. Recent advances
in Natural Language Processing (NLP), particularly transformer-based
architectures, have demonstrated significant potential in text analysis. This
study provides a comprehensive evaluation of state-of-the-art transformer
models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term
Memory (LSTM) based approaches using different text embedding techniques for
mental health disorder classification on Reddit. We construct a large annotated
dataset, validating its reliability through statistical judgmental analysis and
topic modeling. Experimental results demonstrate the superior performance of
transformer models over traditional deep-learning approaches. RoBERTa achieved
the highest classification performance, with a 99.54% F1 score on the hold-out
test set and a 96.05% F1 score on the external test set. Notably, LSTM models
augmented with BERT embeddings proved highly competitive, achieving F1 scores
exceeding 94% on the external dataset while requiring significantly fewer
computational resources. These findings highlight the effectiveness of
transformer-based models for real-time, scalable mental health monitoring. We
discuss the implications for clinical applications and digital mental health
interventions, offering insights into the capabilities and limitations of
state-of-the-art NLP methodologies in mental disorder detection.

</details>


### [2] [Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables](https://arxiv.org/abs/2507.19521)
*Vishakh Padmakumar,Joseph Chee Chang,Kyle Lo,Doug Downey,Aakanksha Naik*

Main category: cs.CL

TL;DR: 论文提出了一种利用大语言模型（LLMs）生成文档比较框架的方法，解决了现有评估模糊性和缺乏编辑方法的问题，并通过数据集和编辑技术提升了性能。


<details>
  <summary>Details</summary>
Motivation: 学术文献数量激增，需要有效组织和比较文档，但现有方法在评估和编辑方面存在不足。

Method: 通过合成意图增强未标注表格数据，创建数据集以减少模糊性；提出多种LLM框架编辑技术，并对比单次生成方法。

Result: 结合表格意图显著提升基线性能；小规模开源模型通过微调可媲美先进提示LLM；编辑技术进一步优化生成框架。

Conclusion: 论文首次同时解决了评估模糊性和编辑问题，展示了数据集和编辑技术对提升框架生成性能的有效性。

Abstract: The increasing volume of academic literature makes it essential for
researchers to organize, compare, and contrast collections of documents. Large
language models (LLMs) can support this process by generating schemas defining
shared aspects along which to compare papers. However, progress on schema
generation has been slow due to: (i) ambiguity in reference-based evaluations,
and (ii) lack of editing/refinement methods. Our work is the first to address
both issues. First, we present an approach for augmenting unannotated table
corpora with synthesized intents and apply it to create a dataset for studying
schema generation conditioned on a given information need, thus reducing
ambiguity. With this dataset, we show how incorporating table intents
significantly improves baseline performance in reconstructing reference
schemas. Next, we propose several LLM-based schema editing techniques. We start
by comprehensively benchmarking several single-shot schema generation methods,
including prompted LLM workflows and fine-tuned models, showing that smaller,
open-weight models can be fine-tuned to be competitive with state-of-the-art
prompted LLMs. Then we demonstrate that our editing techniques can further
improve schemas generated by these methods.

</details>


### [3] [Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri](https://arxiv.org/abs/2507.19537)
*Felix Kraus,Nicolas Blumenröhr,Danah Tonne,Achim Streit*

Main category: cs.CL

TL;DR: WOKIE是一个开源、模块化的自动化SKOS词表翻译工具，结合外部翻译服务和LLM优化，提升翻译质量和语义互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决数字人文学科中语言多样性导致的资源访问和语义互操作性限制问题。

Method: 结合外部翻译服务和LLM进行针对性优化，支持多语言翻译和本体匹配改进。

Result: 在15种语言的词表测试中，WOKIE显著提升了翻译质量、性能和本体匹配效果。

Conclusion: WOKIE通过无障碍自动化翻译和优化的本体匹配，支持更包容和多语言的研究基础设施。

Abstract: We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for
the automated translation of SKOS thesauri. This work addresses a critical need
in the Digital Humanities (DH), where language diversity can limit access,
reuse, and semantic interoperability of knowledge resources. WOKIE combines
external translation services with targeted refinement using Large Language
Models (LLMs), balancing translation quality, scalability, and cost. Designed
to run on everyday hardware and be easily extended, the application requires no
prior expertise in machine translation or LLMs. We evaluate WOKIE across
several DH thesauri in 15 languages with different parameters, translation
services and LLMs, systematically analysing translation quality, performance,
and ontology matching improvements. Our results show that WOKIE is suitable to
enhance the accessibility, reuse, and cross-lingual interoperability of
thesauri by hurdle-free automated translation and improved ontology matching
performance, supporting more inclusive and multilingual research
infrastructures.

</details>


### [4] [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586)
*Shengyuan Wang,Jie Feng,Tianhui Liu,Dan Pei,Yong Li*

Main category: cs.CL

TL;DR: 该论文提出了一个评估和缓解大语言模型（LLMs）中地理空间幻觉的综合框架，通过知识图谱评估和动态事实对齐方法（KTO）显著提升了模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs在地理空间任务中表现出不准确的地理空间知识（地理空间幻觉），这一问题尚未被系统研究，影响了模型的可靠性。

Method: 提出了一个基于结构化地理空间知识图谱的评估框架，并通过Kahneman-Tversky优化（KTO）方法动态对齐事实以减少幻觉。

Result: 在20个先进LLMs上的评估揭示了地理空间幻觉问题，KTO方法将性能提升了29.6%。

Conclusion: 提出的框架和方法有效提升了LLMs在地理空间知识和推理任务中的可信度。

Abstract: Large language models (LLMs) possess extensive world knowledge, including
geospatial knowledge, which has been successfully applied to various geospatial
tasks such as mobility prediction and social indicator prediction. However,
LLMs often generate inaccurate geospatial knowledge, leading to geospatial
hallucinations (incorrect or inconsistent representations of geospatial
information) that compromise their reliability. While the phenomenon of general
knowledge hallucination in LLMs has been widely studied, the systematic
evaluation and mitigation of geospatial hallucinations remain largely
unexplored. To address this gap, we propose a comprehensive evaluation
framework for geospatial hallucinations, leveraging structured geospatial
knowledge graphs for controlled assessment. Through extensive evaluation across
20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.
Building on these insights, we introduce a dynamic factuality aligning method
based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial
hallucinations in LLMs, leading to a performance improvement of over 29.6% on
the proposed benchmark. Extensive experimental results demonstrate the
effectiveness of our benchmark and learning algorithm in enhancing the
trustworthiness of LLMs in geospatial knowledge and reasoning tasks.

</details>


### [5] [Efficient Attention Mechanisms for Large Language Models: A Survey](https://arxiv.org/abs/2507.19595)
*Yutao Sun,Zhenyu Li,Yike Zhang,Tengyu Pan,Bowen Dong,Yuyi Guo,Jianyong Wang*

Main category: cs.CL

TL;DR: 论文综述了Transformer架构中高效注意力机制的两类主要方法：线性注意力和稀疏注意力，旨在解决自注意力二次复杂度的问题，并探讨了其在大规模预训练语言模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 自注意力的二次时间和内存复杂度是长上下文建模的主要障碍，需要高效注意力机制来解决。

Method: 线性注意力通过核近似、循环公式或快速权重动态实现线性复杂度；稀疏注意力通过固定模式、块路由或聚类策略选择部分token进行计算。

Result: 综述整合了算法创新和硬件层面的考虑，分析了高效注意力在大规模预训练语言模型中的应用。

Conclusion: 该工作为设计可扩展且高效的语言模型提供了理论基础和实践参考。

Abstract: Transformer-based architectures have become the prevailing backbone of large
language models. However, the quadratic time and memory complexity of
self-attention remains a fundamental obstacle to efficient long-context
modeling. To address this limitation, recent research has introduced two
principal categories of efficient attention mechanisms. Linear attention
methods achieve linear complexity through kernel approximations, recurrent
formulations, or fastweight dynamics, thereby enabling scalable inference with
reduced computational overhead. Sparse attention techniques, in contrast, limit
attention computation to selected subsets of tokens based on fixed patterns,
block-wise routing, or clustering strategies, enhancing efficiency while
preserving contextual coverage. This survey provides a systematic and
comprehensive overview of these developments, integrating both algorithmic
innovations and hardware-level considerations. In addition, we analyze the
incorporation of efficient attention into largescale pre-trained language
models, including both architectures built entirely on efficient attention and
hybrid designs that combine local and global components. By aligning
theoretical foundations with practical deployment strategies, this work aims to
serve as a foundational reference for advancing the design of scalable and
efficient language models.

</details>


### [6] [MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?](https://arxiv.org/abs/2507.19598)
*Muntasir Wahed,Xiaona Zhou,Kiet A. Nguyen,Tianjiao Yu,Nirav Diwan,Gang Wang,Dilek Hakkani-Tür,Ismini Lourentzou*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在代码生成中的鲁棒性问题，提出了一种多轮恶意代码提示攻击方法，并引入了一个新基准MOCHA进行评估。实验表明，微调模型能显著提升防御能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在代码生成中的安全性漏洞，尤其是多轮恶意提示攻击的潜在威胁。

Method: 提出代码分解攻击方法，并开发MOCHA基准评估模型鲁棒性。通过微调模型提升防御能力。

Result: 实验显示多轮攻击下模型存在漏洞，微调后拒绝率提升32.4%。

Conclusion: 微调能有效增强模型对恶意代码提示的防御能力，MOCHA基准为未来研究提供了工具。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
enhanced their code generation capabilities. However, their robustness against
adversarial misuse, particularly through multi-turn malicious coding prompts,
remains underexplored. In this work, we introduce code decomposition attacks,
where a malicious coding task is broken down into a series of seemingly benign
subtasks across multiple conversational turns to evade safety filters. To
facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale
benchmark designed to evaluate the robustness of code LLMs against both
single-turn and multi-turn malicious prompts. Empirical results across open-
and closed-source models reveal persistent vulnerabilities, especially under
multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while
preserving coding ability, and importantly, enhances robustness on external
adversarial datasets with up to 32.4% increase in rejection rates without any
additional supervision.

</details>


### [7] [HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track](https://arxiv.org/abs/2507.19616)
*Xuchen Wei,Yangxin Wu,Yaoyin Zhang,Henglyu Liu,Kehai Chen,Xuefeng Bai,Min Zhang*

Main category: cs.CL

TL;DR: HITSZ团队在IWSLT 2025 Indic赛道中提出了一种端到端语音转文本翻译系统，结合Whisper ASR模型和Krutrim LLM，显著提升了低资源语言对的翻译质量，并探索了Chain-of-Thought方法的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言对（英语与印度语言）的语音转文本翻译任务中，提升翻译质量是一个重要挑战。

Method: 提出端到端系统，整合预训练的Whisper ASR模型和印度语言专用的Krutrim LLM，并探索Chain-of-Thought方法。

Result: 系统在英语到印度语言和反向翻译中分别获得平均BLEU分数28.88和27.86；Chain-of-Thought方法在某些语言对中显著提升翻译质量（如泰米尔语到英语BLEU提升13.84）。

Conclusion: 端到端系统有效提升了低资源语言对的翻译质量，但Chain-of-Thought方法的输出格式一致性仍需改进。

Abstract: This paper presents HITSZ's submission for the IWSLT 2025 Indic track,
focusing on speech-to-text translation (ST) for English-to-Indic and
Indic-to-English language pairs. To enhance translation quality in this
low-resource scenario, we propose an end-to-end system integrating the
pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an
Indic-specialized large language model (LLM). Experimental results demonstrate
that our end-to-end system achieved average BLEU scores of $28.88$ for
English-to-Indic directions and $27.86$ for Indic-to-English directions.
Furthermore, we investigated the Chain-of-Thought (CoT) method. While this
method showed potential for significant translation quality improvements on
successfully parsed outputs (e.g. a $13.84$ BLEU increase for
Tamil-to-English), we observed challenges in ensuring the model consistently
adheres to the required CoT output format.

</details>


### [8] [MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks](https://arxiv.org/abs/2507.19634)
*Sara Papi,Maike Züfle,Marco Gaido,Beatrice Savoldi,Danni Liu,Ioannis Douros,Luisa Bentivogli,Jan Niehues*

Main category: cs.CL

TL;DR: MCIF是一个多语言、多模态的基准测试，用于评估大语言模型在多语言和多模态任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在多语言、多模态和长上下文评估方面存在不足，需要更全面的评估工具。

Method: 引入MCIF基准测试，涵盖语音、视觉和文本三种模态及四种语言，基于科学讲座进行人工标注。

Result: MCIF为评估模型在多语言、多模态任务中的指令遵循能力提供了全面工具。

Conclusion: MCIF填补了现有基准测试的空白，促进了多模态大语言模型的发展。

Abstract: Recent advances in large language models have catalyzed the development of
multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified
frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to
general-purpose instruction-following models, a key frontier lies in evaluating
their multilingual and multimodal capabilities over both long and short
contexts. However, existing benchmarks fall short in evaluating these
dimensions jointly: they are often limited to English, mostly focus on one
single modality at a time, rely on short-form contexts, or lack human
annotations -- hindering comprehensive assessment of model performance across
languages, modalities, and task complexity. To address these gaps, we introduce
MCIF (Multimodal Crosslingual Instruction Following), the first multilingual
human-annotated benchmark based on scientific talks that is designed to
evaluate instruction-following in crosslingual, multimodal settings over both
short- and long-form inputs. MCIF spans three core modalities -- speech,
vision, and text -- and four diverse languages (English, German, Italian, and
Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret
instructions across languages and combine them with multimodal contextual
information. MCIF is released under a CC-BY 4.0 license to encourage open
research and progress in MLLMs development.

</details>


### [9] [RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams](https://arxiv.org/abs/2507.19666)
*Andrei Vlad Man,Răzvan-Alexandru Smădu,Cristian-George Craciun,Dumitru-Clementin Cercel,Florin Pop,Mihaela-Claudia Cercel*

Main category: cs.CL

TL;DR: 评估LLMs和VLMs在理解和推理罗马尼亚驾驶法律方面的能力，通过文本和视觉问答任务，引入RoD-TAL数据集，并展示领域特定微调和推理模型对性能的提升。


<details>
  <summary>Details</summary>
Motivation: 解决AI与法律系统交叉领域中对法律教育工具的需求，特别是在资源匮乏的语言如罗马尼亚语中。

Method: 使用RoD-TAL数据集，评估检索增强生成（RAG）管道、密集检索器和推理优化模型在信息检索（IR）、问答（QA）、视觉IR和视觉QA任务中的表现。

Result: 领域特定微调显著提升检索性能，推理模型提高QA准确性，但视觉推理仍有挑战。

Conclusion: LLMs和VLMs在法律教育中有潜力，但视觉推理仍需改进。

Abstract: The intersection of AI and legal systems presents a growing need for tools
that support legal education, particularly in under-resourced languages such as
Romanian. In this work, we aim to evaluate the capabilities of Large Language
Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning
about Romanian driving law through textual and visual question-answering tasks.
To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising
Romanian driving test questions, text-based and image-based, alongside
annotated legal references and human explanations. We implement and assess
retrieval-augmented generation (RAG) pipelines, dense retrievers, and
reasoning-optimized models across tasks including Information Retrieval (IR),
Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate
that domain-specific fine-tuning significantly enhances retrieval performance.
At the same time, chain-of-thought prompting and specialized reasoning models
improve QA accuracy, surpassing the minimum grades required to pass driving
exams. However, visual reasoning remains challenging, highlighting the
potential and the limitations of applying LLMs and VLMs to legal education.

</details>


### [10] [Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks](https://arxiv.org/abs/2507.19699)
*Maitha Alshehhi,Ahmed Sharshar,Mohsen Guizani*

Main category: cs.CL

TL;DR: 论文研究了多语言和单语言大语言模型在阿拉伯语、英语和印度语言中的表现，重点关注模型压缩策略（如剪枝和量化）的影响。发现多语言模型普遍优于单语言模型，量化能保持准确性，而激进剪枝会显著降低性能。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在低资源语言环境（如阿拉伯语和印度语言）中的表现，以及模型压缩策略对性能的影响。

Method: 通过基准测试比较多语言和单语言模型在不同语言中的表现，并分析剪枝和量化策略的效果。

Result: 多语言模型表现更优，量化（4位和8位）能保持准确性，但激进剪枝会显著降低性能，尤其是在大型模型中。

Conclusion: 研究为构建可扩展且公平的多语言NLP解决方案提供了关键策略，并强调需解决低资源环境中的幻觉和泛化错误。

Abstract: Although LLMs have attained significant success in high-resource languages,
their capacity in low-resource linguistic environments like Kannada and Arabic
is not yet fully understood. This work benchmarking the performance of
multilingual and monolingual Large Language Models (LLMs) across Arabic,
English, and Indic languages, with particular emphasis on the effects of model
compression strategies such as pruning and quantization. Findings shows
significant performance differences driven by linguistic diversity and resource
availability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.
We find that multilingual versions of the model outperform their
language-specific counterparts across the board, indicating substantial
cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in
maintaining model accuracy while promoting efficiency, but aggressive pruning
significantly compromises performance, especially in bigger models. Our
findings pinpoint key strategies to construct scalable and fair multilingual
NLP solutions and underscore the need for interventions to address
hallucination and generalization errors in the low-resource setting.

</details>


### [11] [Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs](https://arxiv.org/abs/2507.19710)
*Ronak Upasham,Tathagata Dey,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的三阶段流水线方法，用于从表格数据生成包含主观性的文本，通过RDF三元组提取、文本聚合和主观性注入，实现了与GPT-3.5相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格到文本生成方法主要关注客观描述，而忽略了主观性文本的生成，因此需要一种既能保证事实准确性又能融入主观解释的方法。

Method: 采用三阶段流水线：1）提取RDF三元组；2）将文本聚合成连贯叙述；3）注入主观性以丰富生成文本。使用小型微调T5模型，而非大型语言模型。

Result: 在多项指标上表现优于Mistral-7B和Llama-2，与GPT-3.5性能相当，同时保持了事实准确性和主观性的平衡。

Conclusion: 这是首个通过中间表示增强表格到文本生成中事实正确性和主观性的结构化流水线方法。

Abstract: In Table-to-Text (T2T) generation, existing approaches predominantly focus on
providing objective descriptions of tabular data. However, generating text that
incorporates subjectivity, where subjectivity refers to interpretations beyond
raw numerical data, remains underexplored. To address this, we introduce a
novel pipeline that leverages intermediate representations to generate both
objective and subjective text from tables. Our three-stage pipeline consists
of: 1) extraction of Resource Description Framework (RDF) triples, 2)
aggregation of text into coherent narratives, and 3) infusion of subjectivity
to enrich the generated text. By incorporating RDFs, our approach enhances
factual accuracy while maintaining interpretability. Unlike large language
models (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs
smaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5
and outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our
approach through quantitative and qualitative analyses, demonstrating its
effectiveness in balancing factual accuracy with subjective interpretation. To
the best of our knowledge, this is the first work to propose a structured
pipeline for T2T generation that integrates intermediate representations to
enhance both factual correctness and subjectivity.

</details>


### [12] [Basic Reading Distillation](https://arxiv.org/abs/2507.19741)
*Zhi Zhou,Sirui Miao,Xiangyu Duan,Hao Yang,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为基本阅读蒸馏（BRD）的方法，通过教育小型模型模仿大型语言模型（LLM）的基本阅读行为，如命名实体识别和问答，从而在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽然能力强大，但计算资源需求高，限制了实际应用。现有的蒸馏方法（如知识蒸馏或任务蒸馏）忽略了小型模型在通用文本上的基本阅读能力训练。

Method: 提出基本阅读蒸馏（BRD），通过训练小型模型模仿LLM的基本阅读行为（如命名实体识别、问答），然后将其应用于多种任务。

Result: 经过BRD训练的小型模型在语言推理基准和BIG-bench任务中表现优于或接近比其大20倍的LLM。

Conclusion: BRD能有效影响小型模型的概率分布，并与知识蒸馏或任务蒸馏具有正交性，为模型压缩提供了新思路。

Abstract: Large language models (LLMs) have demonstrated remarkable abilities in
various natural language processing areas, but they demand high computation
resources which limits their deployment in real-world. Distillation is one
technique to solve this problem through either knowledge distillation or task
distillation. Both distillation approaches train small models to imitate
specific features of LLMs, but they all neglect basic reading education for
small models on generic texts that are \emph{unrelated} to downstream tasks. In
this paper, we propose basic reading distillation (BRD) which educates a small
model to imitate LLMs basic reading behaviors, such as named entity
recognition, question raising and answering, on each sentence. After such basic
education, we apply the small model on various tasks including language
inference benchmarks and BIG-bench tasks. It shows that the small model can
outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that
BRD effectively influences the probability distribution of the small model, and
has orthogonality to either knowledge distillation or task distillation.

</details>


### [13] [JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2507.19748)
*Yifan Hao,Fangning Chao,Yaqian Hao,Zhaojun Cui,Huan Bai,Haiyu Zhang,Yankai Liu,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: JT-Math-8B是一系列开源模型，通过多阶段优化框架提升数学推理能力，在类似规模的开源模型中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在复杂数学问题中表现不佳的问题。

Method: 采用多阶段优化框架，包括基础模型、指令模型和思考模型，结合监督微调和强化学习。

Result: 在开源模型中表现优异，超越OpenAI的O1-mini和GPT-4o。

Conclusion: JT-Math-8B在数学推理任务中表现出色，为开源模型树立了新标杆。

Abstract: Mathematical reasoning is a cornerstone of artificial general intelligence
and a primary benchmark for evaluating the capabilities of Large Language
Models (LLMs). While state-of-the-art models show promise, they often falter
when faced with complex problems that demand deep conceptual understanding and
intricate, multi-step deliberation. To address this challenge, we introduce
JT-Math-8B, a series of open-source models comprising base, instruct, and
thinking versions, built upon a systematic, multi-stage optimization framework.
Our pre-training corpus is a high-quality, 210B-token dataset curated through a
dedicated data pipeline that uses model-based validation to ensure quality and
diversity. The Instruct Model is optimized for direct, concise answers through
Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)
method. The Thinking Model is trained for complex problem-solving using a Long
Chain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage
RL curriculum that progressively increases task difficulty and context length
up to 32K tokens. JT-Math-8B achieves state-of-the-art results among
open-source models of similar size, surpassing prominent models like OpenAI's
O1-mini and GPT-4o , and demonstrating superior performance on
competition-level mathematics.

</details>


### [14] [Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs](https://arxiv.org/abs/2507.19756)
*Rebecca M. M. Hicke,Brian Haggard,Mia Ferrante,Rayhan Khanna,David Mimno*

Main category: cs.CL

TL;DR: 该论文利用计算工具分析了基督教小说的主题及作者对神圣行为的描写，发现《末日迷踪》系列与其他基督教小说及男女作者作品间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究基督教小说的文化及文学层面，尤其是其未被充分研究的领域，如神圣行为的描写。

Method: 结合人类注释者和轻量级语言模型，开发定义和标注指南，用于分析小说中的神圣行为。

Result: 轻量级模型能匹配人类注释，揭示了《末日迷踪》与其他基督教小说及男女作者作品间的显著差异。

Conclusion: 计算工具能有效分析文学文本，揭示基督教小说中的多样性及作者性别对内容的影响。

Abstract: In addition to its more widely studied political activities, the American
Evangelical movement has a well-developed but less externally visible cultural
and literary side. Christian Fiction, however, has been little studied, and
what scholarly attention there is has focused on the explosively popular Left
Behind series. In this work, we use computational tools to provide both a broad
topical overview of Christian Fiction as a genre and a more directed
exploration of how its authors depict divine acts. Working with human
annotators we first developed definitions and a codebook for "acts of God." We
then adapted those instructions designed for human annotators for use by a
recent, lightweight LM with the assistance of a much larger model. The
laptop-scale LM is capable of matching human annotations, even when the task is
subtle and challenging. Using these annotations, we show that significant and
meaningful differences exist between the Left Behind books and Christian
Fiction more broadly and between books by male and female authors.

</details>


### [15] [UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities](https://arxiv.org/abs/2507.19766)
*Dong Du,Shulin Liu,Tao Yang,Shaohua Chen,Yang Li*

Main category: cs.CL

TL;DR: 提出了一种针对超长输出的强化学习方法（UloRL），通过分段解码和动态掩码技术，显著提升了大型语言模型的训练效率和推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习框架在处理超长输出时效率低下，存在长尾序列分布和熵崩溃问题，因此需要改进方法。

Method: 将超长输出解码分为短段，并引入动态掩码技术（MPTs），以优化训练效率和防止熵崩溃。

Result: 在Qwen3-30B-A3B模型上，训练速度提升2.06倍，性能在AIME2025和BeyondAIME任务上分别提升至85.1%和61.9%。

Conclusion: UloRL方法有效提升了大型语言模型在超长序列生成中的推理能力，具有广泛应用潜力。

Abstract: Recent advances in large language models (LLMs) have highlighted the
potential of reinforcement learning with verifiable rewards (RLVR) to enhance
reasoning capabilities through extended output sequences. However, traditional
RL frameworks face inefficiencies when handling ultra-long outputs due to
long-tail sequence distributions and entropy collapse during training. To
address these challenges, we propose an Ultra-Long Output Reinforcement
Learning (UloRL) approach for advancing large language models' reasoning
abilities. Specifically, we divide ultra long output decoding into short
segments, enabling efficient training by mitigating delays caused by long-tail
samples. Additionally, we introduce dynamic masking of well-Mastered Positive
Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the
effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment
rollout achieved 2.06x increase in training speed, while RL training with
128k-token outputs improves the model's performance on AIME2025 from 70.9\% to
85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B
with remarkable gains. These findings underscore the potential of our methods
to advance the reasoning capabilities of LLMs with ultra-long sequence
generation. We will release our code and model for further use by the
community.

</details>


### [16] [Flora: Effortless Context Construction to Arbitrary Length and Scale](https://arxiv.org/abs/2507.19786)
*Tianxiang Chen,Zhentao Tan,Xiaofan Bo,Yue Wu,Tao Gong,Qi Chu,Jieping Ye,Nenghai Yu*

Main category: cs.CL

TL;DR: Flora是一种无需人工或LLM干预的长上下文构建策略，通过组合短指令生成任意长度的上下文，显著提升LLMs的长上下文性能，同时几乎不影响短上下文能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs处理长上下文存在挑战，包括长文本稀缺、计算成本高、短上下文能力下降等问题，现有方法依赖LLMs或人工干预，成本高且多样性有限。

Method: Flora通过基于类别的短指令组合和长上下文元指令生成响应，实现任意长度和多样性的上下文构建。

Result: 在Llama3-8B-Instruct和QwQ-32B上的实验表明，Flora增强的LLMs在长上下文任务中表现优异，同时保持短上下文任务的强性能。

Conclusion: Flora提供了一种高效、低成本的长上下文构建方法，解决了现有方法的局限性。

Abstract: Effectively handling long contexts is challenging for Large Language Models
(LLMs) due to the rarity of long texts, high computational demands, and
substantial forgetting of short-context abilities. Recent approaches have
attempted to construct long contexts for instruction tuning, but these methods
often require LLMs or human interventions, which are both costly and limited in
length and diversity. Also, the drop in short-context performances of present
long-context LLMs remains significant. In this paper, we introduce Flora, an
effortless (human/LLM-free) long-context construction strategy. Flora can
markedly enhance the long-context performance of LLMs by arbitrarily assembling
short instructions based on categories and instructing LLMs to generate
responses based on long-context meta-instructions. This enables Flora to
produce contexts of arbitrary length and scale with rich diversity, while only
slightly compromising short-context performance. Experiments on
Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three
long-context benchmarks while maintaining strong performances in short-context
tasks. Our data-construction code is available at
\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.

</details>


### [17] [HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs](https://arxiv.org/abs/2507.19823)
*Dongquan Yang,Yifan Yang,Xiaotian Yu,Xianbiao Qi,Rong Xiao*

Main category: cs.CL

TL;DR: HCAttention提出了一种异构注意力计算框架，通过键量化、值卸载和动态KV驱逐，在极端内存限制下实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 处理长上下文输入时，KV缓存的巨大内存需求是主要挑战，现有压缩方法在内存减少超过85%时性能显著下降。

Method: HCAttention结合键量化、值卸载和动态KV驱逐，无需微调模型，兼容现有Transformer架构。

Result: 在LongBench基准测试中，HCAttention将KV缓存内存占用缩小至原始25%，并在12.5%缓存下保持竞争力。

Conclusion: HCAttention首次将Llama-3-8B模型扩展至单块A100 GPU处理400万token，成为KV缓存压缩的新标杆。

Abstract: Processing long-context inputs with large language models presents a
significant challenge due to the enormous memory requirements of the Key-Value
(KV) cache during inference. Existing KV cache compression methods exhibit
noticeable performance degradation when memory is reduced by more than 85%.
Additionally, strategies that leverage GPU-CPU collaboration for approximate
attention remain underexplored in this setting. We propose HCAttention, a
heterogeneous attention computation framework that integrates key quantization,
value offloading, and dynamic KV eviction to enable efficient inference under
extreme memory constraints. The method is compatible with existing transformer
architectures and does not require model fine-tuning. Experimental results on
the LongBench benchmark demonstrate that our approach preserves the accuracy of
full-attention model while shrinking the KV cache memory footprint to 25% of
its original size. Remarkably, it stays competitive with only 12.5% of the
cache, setting a new state-of-the-art in LLM KV cache compression. To the best
of our knowledge, HCAttention is the first to extend the Llama-3-8B model to
process 4 million tokens on a single A100 GPU with 80GB memory.

</details>


### [18] [DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments](https://arxiv.org/abs/2507.19867)
*Anshul Chavda,M Jagadeesh,Chintalapalli Raja Kullayappa,B Jayaprakash,Medchalimi Sruthi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: DiscoDrive是一个合成的多轮对话数据集，专注于捕捉车内对话中的自发不流畅性，如犹豫、重复和自我纠正。它在训练和低资源数据增强中表现优异，提升了多个评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有数据集未能真实反映车内对话中的自发不流畅性，限制了对话AI的性能。

Method: 采用两阶段、提示驱动的合成方法，动态整合不流畅性生成3500个多轮对话。

Result: DiscoDrive在MultiWOZ 2.2和SGD测试集上表现优于KVRET，BLEU-4提升0.26至0.61，METEOR +2.10，ROUGE-L +3.48，BERTScore F1提升1.35至3.48。

Conclusion: DiscoDrive填补了现有资源的空白，为训练和增强对话AI提供了多功能语料库。

Abstract: In-car conversational AI is becoming increasingly critical as autonomous
vehicles and smart assistants gain widespread adoption. Yet, existing datasets
fail to capture the spontaneous disfluencies such as hesitations, false starts,
repetitions, and self-corrections that characterize real driver-AI dialogs. To
address this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn
dialogs across seven automotive domains, generated using a two-stage,
prompt-driven pipeline that dynamically integrates disfluencies during
synthesis. We show that DiscoDrive is effective both as a training resource,
enabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on
the MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4
improvements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1
improvements of 1.35 to 3.48), and as a data augmentation resource in
low-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,
METEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10
percent of KVRET. Human evaluations further confirm that dialogs sampled from
DiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness
(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more
context-appropriate than leading post-hoc methods (such as LARD), without
compromising clarity. DiscoDrive fills a critical gap in existing resources and
serves as a versatile corpus for both training and augmenting conversational
AI, enabling robust handling of real-world, disfluent in-car interactions.

</details>


### [19] [The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment](https://arxiv.org/abs/2507.19869)
*Danil Fokin,Monika Płużyczka,Grigory Golovin*

Main category: cs.CL

TL;DR: PVST是一种基于项目反应理论和计算机自适应测试的新工具，用于评估波兰语母语和非母语者的接受性词汇量，测试时间短且准确度高。


<details>
  <summary>Details</summary>
Motivation: 开发一种高效、准确的工具来评估波兰语母语和非母语者的词汇量。

Method: 基于项目反应理论和计算机自适应测试，动态调整测试内容以适应测试者的水平。

Result: 验证研究表明，母语者的词汇量显著大于非母语者，且母语者的词汇量与年龄呈强正相关。

Conclusion: PVST是一种有效的词汇量评估工具，已在myvocab.info/pl上线。

Abstract: We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing
the receptive vocabulary size of both native and non-native Polish speakers.
Based on Item Response Theory and Computerized Adaptive Testing, PVST
dynamically adjusts to each test-taker's proficiency level, ensuring high
accuracy while keeping the test duration short. To validate the test, a pilot
study was conducted with 1.475 participants. Native Polish speakers
demonstrated significantly larger vocabularies compared to non-native speakers.
For native speakers, vocabulary size showed a strong positive correlation with
age. The PVST is available online at myvocab.info/pl.

</details>


### [20] [Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam](https://arxiv.org/abs/2507.19885)
*Cesar Augusto Madid Truyts,Amanda Gomes Rabelo,Gabriel Mesquita de Souza,Daniel Scaldaferri Lages,Adriano Jose Pereira,Uri Adrian Prync Flato,Eduardo Pontes dos Reis,Joaquim Edson Vieira,Paulo Sergio Panse Silveira,Edson Amaro Junior*

Main category: cs.CL

TL;DR: 研究评估了六种LLMs和四种MLLMs在巴西葡萄牙语医学考试中的表现，发现部分模型表现接近人类，但多模态问题仍有差距，强调需优化非英语AI应用。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型评估多集中于英语，可能导致其他语言表现偏差，研究旨在填补这一空白。

Method: 通过巴西医学考试题目测试模型，比较准确性、处理时间和回答连贯性。

Result: 部分模型（如Claude-3.5-Sonnet和Claude-3-Opus）表现接近人类，但多模态问题仍有不足。

Conclusion: 需进一步优化非英语AI模型，推动多模态推理和临床整合。

Abstract: Artificial intelligence (AI) has shown the potential to revolutionize
healthcare by improving diagnostic accuracy, optimizing workflows, and
personalizing treatment plans. Large Language Models (LLMs) and Multimodal
Large Language Models (MLLMs) have achieved notable advancements in natural
language processing and medical applications. However, the evaluation of these
models has focused predominantly on the English language, leading to potential
biases in their performance across different languages.
  This study investigates the capability of six LLMs (GPT-4.0 Turbo,
LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and
Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,
and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese
from the medical residency entrance exam of the Hospital das Cl\'inicas da
Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest
health complex in South America. The performance of the models was benchmarked
against human candidates, analyzing accuracy, processing time, and coherence of
the generated explanations.
  The results show that while some models, particularly Claude-3.5-Sonnet and
Claude-3-Opus, achieved accuracy levels comparable to human candidates,
performance gaps persist, particularly in multimodal questions requiring image
interpretation. Furthermore, the study highlights language disparities,
emphasizing the need for further fine-tuning and data set augmentation for
non-English medical AI applications.
  Our findings reinforce the importance of evaluating generative AI in various
linguistic and clinical settings to ensure a fair and reliable deployment in
healthcare. Future research should explore improved training methodologies,
improved multimodal reasoning, and real-world clinical integration of AI-driven
medical assistance.

</details>


### [21] [A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs](https://arxiv.org/abs/2507.19899)
*Prajval Bolegave,Pushpak Bhattacharya*

Main category: cs.CL

TL;DR: 论文提出了一种高质量、专家标注的社交媒体帖子数据集，用于抑郁症早期检测，并开发了一个评估框架，测试大型语言模型在临床解释任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 通过社交媒体帖子早期检测抑郁症，为心理健康干预提供支持。

Method: 构建专家标注的数据集，设计评估框架，采用零样本和少样本提示策略测试GPT-4.1、Gemini 2.5 Pro和Claude 3.7 Sonnet等模型。

Result: 不同模型在临床解释任务中表现差异显著，零样本和少样本提示效果不同。

Conclusion: 人类专业知识对引导大型语言模型行为具有重要价值，为心理健康领域更安全、透明的AI系统提供了基础。

Abstract: Early detection of depression from online social media posts holds promise
for providing timely mental health interventions. In this work, we present a
high-quality, expert-annotated dataset of 1,017 social media posts labeled with
depressive spans and mapped to 12 depression symptom categories. Unlike prior
datasets that primarily offer coarse post-level labels
\cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of
both model predictions and generated explanations.
  We develop an evaluation framework that leverages this clinically grounded
dataset to assess the faithfulness and quality of natural language explanations
generated by large language models (LLMs). Through carefully designed prompting
strategies, including zero-shot and few-shot approaches with domain-adapted
examples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1,
Gemini 2.5 Pro, and Claude 3.7 Sonnet.
  Our comprehensive empirical analysis reveals significant differences in how
these models perform on clinical explanation tasks, with zero-shot and few-shot
prompting. Our findings underscore the value of human expertise in guiding LLM
behavior and offer a step toward safer, more transparent AI systems for
psychological well-being.

</details>


### [22] [CaliDrop: KV Cache Compression with Calibration](https://arxiv.org/abs/2507.19906)
*Yi Su,Quantong Qiu,Yuechi Zhou,Juntao Li,Qingrong Xia,Ping Li,Xinyu Duan,Zhefeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种名为CaliDrop的新策略，通过校准增强令牌驱逐，以减少KV缓存的内存占用，同时缓解精度损失。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）生成过程中需要大量计算资源，KV缓存虽加速生成但其内存占用随序列长度、批次大小和模型规模线性增长，成为长上下文场景的瓶颈。

Method: 提出CaliDrop策略，利用附近位置查询高度相似的观察，对丢弃的令牌进行推测性校准，以减少令牌驱逐带来的精度损失。

Result: 实验表明，CaliDrop显著提高了现有令牌驱逐方法的精度。

Conclusion: CaliDrop通过校准增强令牌驱逐，有效解决了KV缓存内存占用与精度损失之间的权衡问题。

Abstract: Large Language Models (LLMs) require substantial computational resources
during generation. While the Key-Value (KV) cache significantly accelerates
this process by storing attention intermediates, its memory footprint grows
linearly with sequence length, batch size, and model size, creating a
bottleneck in long-context scenarios. Various KV cache compression techniques,
including token eviction, quantization, and low-rank projection, have been
proposed to mitigate this bottleneck, often complementing each other. This
paper focuses on enhancing token eviction strategies. Token eviction leverages
the observation that the attention patterns are often sparse, allowing for the
removal of less critical KV entries to save memory. However, this reduction
usually comes at the cost of notable accuracy degradation, particularly under
high compression ratios. To address this issue, we propose \textbf{CaliDrop}, a
novel strategy that enhances token eviction through calibration. Our
preliminary experiments show that queries at nearby positions exhibit high
similarity. Building on this observation, CaliDrop performs speculative
calibration on the discarded tokens to mitigate the accuracy loss caused by
token eviction. Extensive experiments demonstrate that CaliDrop significantly
improves the accuracy of existing token eviction methods.

</details>


### [23] [KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models](https://arxiv.org/abs/2507.19962)
*Seorin Kim,Dongyoung Lee,Jaejin Lee*

Main category: cs.CL

TL;DR: KLAAD是一种基于注意力的去偏框架，通过隐式对齐注意力分布来减少LLM中的社会偏见，同时保持语言模型的流畅性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）输出中存在社会偏见，引发公平性和危害性的伦理问题。

Method: 提出KLAAD框架，结合交叉熵、KL散度和三元组损失，隐式对齐注意力分布，不直接修改模型权重。

Result: 在BBQ和BOLD基准测试中，KLAAD显著减少了偏见，同时对语言建模质量影响最小。

Conclusion: 注意力级别的对齐为生成语言模型中的偏见缓解提供了原则性解决方案。

Abstract: Large language models (LLMs) often exhibit societal biases in their outputs,
prompting ethical concerns regarding fairness and harm. In this work, we
propose KLAAD (KL-Attention Alignment Debiasing), an attention-based debiasing
framework that implicitly aligns attention distributions between stereotypical
and anti-stereotypical sentence pairs without directly modifying model weights.
KLAAD introduces a composite training objective combining Cross-Entropy, KL
divergence, and Triplet losses, guiding the model to consistently attend across
biased and unbiased contexts while preserving fluency and coherence.
Experimental evaluation of KLAAD demonstrates improved bias mitigation on both
the BBQ and BOLD benchmarks, with minimal impact on language modeling quality.
The results indicate that attention-level alignment offers a principled
solution for mitigating bias in generative language models.

</details>


### [24] [Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text](https://arxiv.org/abs/2507.19969)
*Mizanur Rahman,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: Text2Vis是一个用于评估文本到可视化模型的基准，涵盖20多种图表类型和多样化的数据科学查询，包含1985个样本。通过测试11种开源和闭源模型，揭示了性能差距，并提出了一种跨模态的actor-critic框架以提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面的基准来评估大型语言模型（LLMs）在生成可视化方面的能力，限制了其能力的严格评估。

Method: 引入Text2Vis基准，包含数据表、自然语言查询、简短答案、可视化代码和标注图表。提出跨模态actor-critic框架，联合优化文本答案和可视化代码。

Result: 测试显示GPT-4o的通过率从26%提升至42%，图表质量显著提高。同时开发了基于LLM的自动化评估框架。

Conclusion: Text2Vis为文本到可视化任务提供了首个全面基准，提出的框架显著提升了模型性能，并支持自动化评估。

Abstract: Automated data visualization plays a crucial role in simplifying data
interpretation, enhancing decision-making, and improving efficiency. While
large language models (LLMs) have shown promise in generating visualizations
from natural language, the absence of comprehensive benchmarks limits the
rigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark
designed to assess text-to-visualization models, covering 20+ chart types and
diverse data science queries, including trend analysis, correlation, outlier
detection, and predictive analytics. It comprises 1,985 samples, each with a
data table, natural language query, short answer, visualization code, and
annotated charts. The queries involve complex reasoning, conversational turns,
and dynamic data retrieval. We benchmark 11 open-source and closed-source
models, revealing significant performance gaps, highlighting key challenges,
and offering insights for future advancements. To close this gap, we propose
the first cross-modal actor-critic agentic framework that jointly refines the
textual answer and visualization code, increasing GPT-4o`s pass rate from 26%
to 42% over the direct approach and improving chart quality. We also introduce
an automated LLM-based evaluation framework that enables scalable assessment
across thousands of samples without human annotation, measuring answer
correctness, code execution success, visualization readability, and chart
accuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis.

</details>


### [25] [Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory](https://arxiv.org/abs/2507.19980)
*Dan Song,Won-Chan Lee,Hong Jiao*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）在AP中文考试写作任务评分中的可靠性，发现人类评分者更可靠，但LLMs在特定条件下表现合理，混合评分模型可提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在写作评分中的可靠性，为大规模写作评估提供混合评分模型的可行性。

Method: 使用概化理论，比较人类和AI评分者在故事叙述和邮件回复任务中的评分一致性。

Result: 人类评分者总体更可靠，但LLMs在故事叙述任务中表现合理，混合评分模型提高了可靠性。

Conclusion: 混合评分模型可能为大规模写作评估提供优势。

Abstract: This study investigates the estimation of reliability for large language
models (LLMs) in scoring writing tasks from the AP Chinese Language and Culture
Exam. Using generalizability theory, the research evaluates and compares score
consistency between human and AI raters across two types of AP Chinese
free-response writing tasks: story narration and email response. These essays
were independently scored by two trained human raters and seven AI raters. Each
essay received four scores: one holistic score and three analytic scores
corresponding to the domains of task completion, delivery, and language use.
Results indicate that although human raters produced more reliable scores
overall, LLMs demonstrated reasonable consistency under certain conditions,
particularly for story narration tasks. Composite scoring that incorporates
both human and AI raters improved reliability, which supports that hybrid
scoring models may offer benefits for large-scale writing assessments.

</details>


### [26] [VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering](https://arxiv.org/abs/2507.19995)
*Tan-Minh Nguyen,Hoang-Trung Nguyen,Trong-Khoi Dao,Xuan-Hieu Phan,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong*

Main category: cs.CL

TL;DR: 论文介绍了VLQA数据集，一个针对越南法律领域的高质量资源，并评估了其在法律信息检索和问答任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在法律文本处理中取得进展，但完全自动化法律任务仍远未实现。低资源语言（如越南语）的法律NLP面临资源稀缺的挑战，急需标注数据。

Method: 引入VLQA数据集，进行全面的统计分析，并通过实验评估其在法律信息检索和问答任务中的表现。

Result: VLQA数据集为越南法律领域提供了高质量资源，实验证明了其在相关任务中的有效性。

Conclusion: VLQA数据集填补了越南法律NLP的资源空白，为未来研究提供了重要支持。

Abstract: The advent of large language models (LLMs) has led to significant
achievements in various domains, including legal text processing. Leveraging
LLMs for legal tasks is a natural evolution and an increasingly compelling
choice. However, their capabilities are often portrayed as greater than they
truly are. Despite the progress, we are still far from the ultimate goal of
fully automating legal tasks using artificial intelligence (AI) and natural
language processing (NLP). Moreover, legal systems are deeply domain-specific
and exhibit substantial variation across different countries and languages. The
need for building legal text processing applications for different natural
languages is, therefore, large and urgent. However, there is a big challenge
for legal NLP in low-resource languages such as Vietnamese due to the scarcity
of resources and annotated data. The need for labeled legal corpora for
supervised training, validation, and supervised fine-tuning is critical. In
this paper, we introduce the VLQA dataset, a comprehensive and high-quality
resource tailored for the Vietnamese legal domain. We also conduct a
comprehensive statistical analysis of the dataset and evaluate its
effectiveness through experiments with state-of-the-art models on legal
information retrieval and question-answering tasks.

</details>


### [27] [Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach](https://arxiv.org/abs/2507.20019)
*Saurav Singla,Aarav Singla,Advik Gupta,Parnika Gupta*

Main category: cs.CL

TL;DR: 提出一种元学习框架，用于在有限标注数据下检测跨领域语言异常。


<details>
  <summary>Details</summary>
Motivation: 语言异常（如垃圾邮件、假新闻和仇恨言论）因稀疏性和多样性而难以检测，需解决跨领域泛化问题。

Method: 将异常检测视为少样本二分类问题，结合元学习、原型网络和领域重采样进行快速适应。

Result: 在多个数据集上表现优于基线模型（F1和AUC得分更高）。

Conclusion: 方法有效且开源代码，推动少样本文本异常检测研究。

Abstract: We propose a meta learning framework for detecting anomalies in human
language across diverse domains with limited labeled data. Anomalies in
language ranging from spam and fake news to hate speech pose a major challenge
due to their sparsity and variability. We treat anomaly detection as a few shot
binary classification problem and leverage meta-learning to train models that
generalize across tasks. Using datasets from domains such as SMS spam, COVID-19
fake news, and hate speech, we evaluate model generalization on unseen tasks
with minimal labeled anomalies. Our method combines episodic training with
prototypical networks and domain resampling to adapt quickly to new anomaly
detection tasks. Empirical results show that our method outperforms strong
baselines in F1 and AUC scores. We also release the code and benchmarks to
facilitate further research in few-shot text anomaly detection.

</details>


### [28] [FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression](https://arxiv.org/abs/2507.20030)
*Runchao Li,Yao Fu,Mu Sheng,Xianxuan Long,Haotian Yu,Pan Li*

Main category: cs.CL

TL;DR: FAEDKV是一种无需训练的KV缓存压缩框架，通过频率域变换实现无偏信息保留，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在长上下文任务中KV缓存的高内存和计算需求问题，避免现有压缩方法导致的偏置表示。

Method: 采用无限窗口傅里叶变换（IWDFT）将KV缓存转换到频率域，实现均衡信息保留。

Result: 在LongBench基准测试中优于现有方法22%，在Needle-In-A-Haystack任务中表现更优。

Conclusion: FAEDKV提供了一种高效、无偏的KV缓存压缩方案，适用于长上下文任务。

Abstract: The efficacy of Large Language Models (LLMs) in long-context tasks is often
hampered by the substantial memory footprint and computational demands of the
Key-Value (KV) cache. Current compression strategies, including token eviction
and learned projections, frequently lead to biased representations -- either by
overemphasizing recent/high-attention tokens or by repeatedly degrading
information from earlier context -- and may require costly model retraining. We
present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,
training-free KV cache compression framework that ensures unbiased information
retention. FAEDKV operates by transforming the KV cache into the frequency
domain using a proposed Infinite-Window Fourier Transform (IWDFT). This
approach allows for the equalized contribution of all tokens to the compressed
representation, effectively preserving both early and recent contextual
information. A preliminary frequency ablation study identifies critical
spectral components for layer-wise, targeted compression. Experiments on
LongBench benchmark demonstrate FAEDKV's superiority over existing methods by
up to 22\%. In addition, our method shows superior, position-agnostic retrieval
accuracy on the Needle-In-A-Haystack task compared to compression based
approaches.

</details>


### [29] [Infogen: Generating Complex Statistical Infographics from Documents](https://arxiv.org/abs/2507.20046)
*Akash Ghosh,Aparna Garimella,Pritika Ramu,Sambaran Bandyopadhyay,Sriparna Saha*

Main category: cs.CL

TL;DR: 论文提出了一种生成复杂统计信息图的方法，填补了现有AI技术在从文本生成多子图表信息图方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有AI技术仅能生成简单图表，无法处理需要深度理解的文本内容以生成复杂信息图。

Method: 提出了Infogen框架，分两阶段生成信息图：先由微调LLM生成元数据，再转换为信息图代码。

Result: 在Infodat基准测试中，Infogen表现优于其他开源和闭源LLM。

Conclusion: Infogen在生成复杂统计信息图方面达到了最先进水平。

Abstract: Statistical infographics are powerful tools that simplify complex data into
visually engaging and easy-to-understand formats. Despite advancements in AI,
particularly with LLMs, existing efforts have been limited to generating simple
charts, with no prior work addressing the creation of complex infographics from
text-heavy documents that demand a deep understanding of the content. We
address this gap by introducing the task of generating statistical infographics
composed of multiple sub-charts (e.g., line, bar, pie) that are contextually
accurate, insightful, and visually aligned. To achieve this, we define
infographic metadata that includes its title and textual insights, along with
sub-chart-specific details such as their corresponding data and alignment. We
also present Infodat, the first benchmark dataset for text-to-infographic
metadata generation, where each sample links a document to its metadata. We
propose Infogen, a two-stage framework where fine-tuned LLMs first generate
metadata, which is then converted into infographic code. Extensive evaluations
on Infodat demonstrate that Infogen achieves state-of-the-art performance,
outperforming both closed and open-source LLMs in text-to-statistical
infographic generation.

</details>


### [30] [A Tensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications](https://arxiv.org/abs/2507.20055)
*Avaljot Singh,Yamin Chandini Sarita,Aditya Mishra,Ishaan Goyal,Gagandeep Singh,Charith Mendis*

Main category: cs.CL

TL;DR: 论文提出了一种编译器框架，用于自动将DNN验证器的神经元级规范转换为基于张量的层级实现，解决了设计与实现之间的语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 由于DNN的不透明性，基于抽象解释的验证成为建立信任的实用方法，但现有验证器开发困难，设计与实现之间存在语义鸿沟。

Method: 提出一种编译器框架，包括基于堆栈的中间表示（IR）和形状分析，自动将神经元级规范转换为张量级实现，并引入g-BCSR格式优化稀疏张量计算。

Result: 编译器框架实现了与手工优化实现相当的性能，同时简化了新验证器的开发。

Conclusion: 该框架为DNN验证器的开发和优化提供了高效且灵活的工具。

Abstract: The uninterpretability of DNNs has led to the adoption of abstract
interpretation-based certification as a practical means to establish trust in
real-world systems that rely on DNNs. However, the current landscape supports
only a limited set of certifiers, and developing new ones or modifying existing
ones for different applications remains difficult. This is because the
mathematical design of certifiers is expressed at the neuron level, while their
implementations are optimized and executed at the tensor level. This mismatch
creates a semantic gap between design and implementation, making manual
bridging both complex and expertise-intensive -- requiring deep knowledge in
formal methods, high-performance computing, etc.
  We propose a compiler framework that automatically translates neuron-level
specifications of DNN certifiers into tensor-based, layer-level
implementations. This is enabled by two key innovations: a novel stack-based
intermediate representation (IR) and a shape analysis that infers the implicit
tensor operations needed to simulate the neuron-level semantics. During
lifting, the shape analysis creates tensors in the minimal shape required to
perform the corresponding operations. The IR also enables domain-specific
optimizations as rewrites. At runtime, the resulting tensor computations
exhibit sparsity tied to the DNN architecture. This sparsity does not align
well with existing formats. To address this, we introduce g-BCSR, a
double-compression format that represents tensors as collections of blocks of
varying sizes, each possibly internally sparse.
  Using our compiler and g-BCSR, we make it easy to develop new certifiers and
analyze their utility across diverse DNNs. Despite its flexibility, the
compiler achieves performance comparable to hand-optimized implementations.

</details>


### [31] [RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation](https://arxiv.org/abs/2507.20059)
*Ran Xu,Yuchen Zhuang,Yue Yu,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: RAG系统在多样化检索场景中的表现存在局限，需自适应策略。


<details>
  <summary>Details</summary>
Motivation: 探索RAG在多样化检索场景中的实际效果，填补现有研究的空白。

Method: 使用大规模混合知识库MassiveDS评估RAG系统。

Result: 发现检索对小模型更有效，重排序器价值有限，且无单一检索源表现一致。

Conclusion: 需开发自适应检索策略以优化RAG在现实场景中的应用。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieved at inference time. While RAG
demonstrates strong performance on benchmarks largely derived from
general-domain corpora like Wikipedia, its effectiveness under realistic,
diverse retrieval scenarios remains underexplored. We evaluated RAG systems
using MassiveDS, a large-scale datastore with mixture of knowledge, and
identified critical limitations: retrieval mainly benefits smaller models,
rerankers add minimal value, and no single retrieval source consistently
excels. Moreover, current LLMs struggle to route queries across heterogeneous
knowledge sources. These findings highlight the need for adaptive retrieval
strategies before deploying RAG in real-world settings. Our code and data can
be found at https://github.com/ritaranx/RAG_in_the_Wild.

</details>


### [32] [ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models](https://arxiv.org/abs/2507.20091)
*Kaizhi Qian,Xulin Fan,Junrui Ni,Slava Shechtman,Mark Hasegawa-Johnson,Chuang Gan,Yang Zhang*

Main category: cs.CL

TL;DR: 论文提出ProsodyLM，通过改进语音标记化方案，增强语音语言模型对韵律信息的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型在将语音转换为离散标记后输入LLMs时，难以有效学习韵律信息，导致模型无法通过预训练获得明显的韵律处理能力。

Method: 提出ProsodyLM，采用一种简单的标记化方案：先将语音转录为文本，再生成词级韵律标记，以保留更完整的韵律信息。

Result: ProsodyLM仅通过预训练即可学习多样化的韵律处理能力，包括生成语音中的韵律细微差别（如对比焦点）、理解情感和重音，以及保持长上下文中的韵律一致性。

Conclusion: ProsodyLM通过改进标记化方案，显著提升了语音语言模型对韵律信息的捕捉能力，为语音处理和理解提供了新思路。

Abstract: Speech language models refer to language models with speech processing and
understanding capabilities. One key desirable capability for speech language
models is the ability to capture the intricate interdependency between content
and prosody. The existing mainstream paradigm of training speech language
models, which converts speech into discrete tokens before feeding them into
LLMs, is sub-optimal in learning prosody information -- we find that the
resulting LLMs do not exhibit obvious emerging prosody processing capabilities
via pre-training alone. To overcome this, we propose ProsodyLM, which
introduces a simple tokenization scheme amenable to learning prosody. Each
speech utterance is first transcribed into text, followed by a sequence of
word-level prosody tokens. Compared with conventional speech tokenization
schemes, the proposed tokenization scheme retains more complete prosody
information, and is more understandable to text-based LLMs. We find that
ProsodyLM can learn surprisingly diverse emerging prosody processing
capabilities through pre-training alone, ranging from harnessing the prosody
nuances in generated speech, such as contrastive focus, understanding emotion
and stress in an utterance, to maintaining prosody consistency in long
contexts.

</details>


### [33] [AI-Driven Generation of Old English: A Framework for Low-Resource Languages](https://arxiv.org/abs/2507.20111)
*Rodrigo Gabriel Salazar Alva,Matías Nuñez,Cristian López,Javier Martín Arista*

Main category: cs.CL

TL;DR: 提出了一种利用大型语言模型生成高质量古英语文本的框架，结合参数高效微调、数据增强和双代理流程，显著提升了翻译质量。


<details>
  <summary>Details</summary>
Motivation: 古英语资源匮乏，限制了现代自然语言处理技术的应用，需要填补这一空白。

Method: 结合LoRA参数高效微调、数据增强（反向翻译）和双代理流程（内容生成与翻译分离）。

Result: 自动评估指标（BLEU、METEOR、CHRF）显著提升，BLEU分数从26增至65以上；人工评估确认语法准确性和风格保真度高。

Conclusion: 该方法不仅扩展了古英语语料库，还为其他濒危语言的复兴提供了实用方案，结合了AI创新与文化保护目标。

Abstract: Preserving ancient languages is essential for understanding humanity's
cultural and linguistic heritage, yet Old English remains critically
under-resourced, limiting its accessibility to modern natural language
processing (NLP) techniques. We present a scalable framework that uses advanced
large language models (LLMs) to generate high-quality Old English texts,
addressing this gap. Our approach combines parameter-efficient fine-tuning
(Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a
dual-agent pipeline that separates the tasks of content generation (in English)
and translation (into Old English). Evaluation with automated metrics (BLEU,
METEOR, and CHRF) shows significant improvements over baseline models, with
BLEU scores increasing from 26 to over 65 for English-to-Old English
translation. Expert human assessment also confirms high grammatical accuracy
and stylistic fidelity in the generated texts. Beyond expanding the Old English
corpus, our method offers a practical blueprint for revitalizing other
endangered languages, effectively uniting AI innovation with the goals of
cultural preservation.

</details>


### [34] [Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering](https://arxiv.org/abs/2507.20133)
*Anas Mohamed,Azal Ahmad Khan,Xinran Wang,Ahmad Faraz Khan,Shuwen Ge,Saman Bahzad Khan,Ayaan Ahmad,Ali Anwar*

Main category: cs.CL

TL;DR: Sem-DPO改进DPO方法，通过语义一致性加权提升提示优化效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决DPO方法中语义不一致的问题，确保优化后的提示更贴近用户意图。

Method: 引入Sem-DPO，通过余弦距离加权损失函数，抑制语义不匹配的提示。

Result: 在多个基准测试中，Sem-DPO的CLIP相似度和人类偏好评分显著高于DPO及其他基线方法。

Conclusion: 语义加权应成为提示优化的新标准，并为语言模型的语义感知优化奠定基础。

Abstract: Generative AI can now synthesize strikingly realistic images from text, yet
output quality remains highly sensitive to how prompts are phrased. Direct
Preference Optimization (DPO) offers a lightweight, off-policy alternative to
RL for automatic prompt engineering, but its token-level regularization leaves
semantic inconsistency unchecked as prompts that win higher preference scores
can still drift away from the user's intended meaning.
  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency
yet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an
exponential weight proportional to the cosine distance between the original
prompt and winning candidate in embedding space, softly down-weighting training
signals that would otherwise reward semantically mismatched prompts. We provide
the first analytical bound on semantic drift for preference-tuned prompt
generators, showing that Sem-DPO keeps learned prompts within a provably
bounded neighborhood of the original text. On three standard text-to-image
prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12%
higher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1,
PickScore) than DPO, while also outperforming state-of-the-art baselines. These
findings suggest that strong flat baselines augmented with semantic weighting
should become the new standard for prompt-optimization studies and lay the
groundwork for broader, semantics-aware preference optimization in language
models.

</details>


### [35] [Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?](https://arxiv.org/abs/2507.20419)
*Khloud AL Jallad,Nada Ghneim,Ghaida Rebdawi*

Main category: cs.CL

TL;DR: 该论文综述了自然语言理解（NLU）评估基准，重点关注诊断数据集和覆盖的语言现象，提出建立统一评估标准的必要性。


<details>
  <summary>Details</summary>
Motivation: 研究NLU评估基准的现状，发现缺乏统一的命名规范和标准语言现象覆盖，提出建立类似ISO标准的评估体系。

Method: 通过详细比较和分析现有的英语、阿拉伯语和多语言NLU基准，特别是其诊断数据集和语言现象覆盖范围。

Result: 发现现有基准在宏微观类别命名和语言现象覆盖上缺乏一致性，提出需要建立全球统一的语言现象层次结构。

Conclusion: 建议制定NLU诊断评估的标准化评价指标，以便更深入比较不同模型在诊断基准上的表现。

Abstract: Natural Language Understanding (NLU) is a basic task in Natural Language
Processing (NLP). The evaluation of NLU capabilities has become a trending
research topic that attracts researchers in the last few years, resulting in
the development of numerous benchmarks. These benchmarks include various tasks
and datasets in order to evaluate the results of pretrained models via public
leaderboards. Notably, several benchmarks contain diagnostics datasets designed
for investigation and fine-grained error analysis across a wide range of
linguistic phenomena. This survey provides a comprehensive review of available
English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on
their diagnostics datasets and the linguistic phenomena they covered. We
present a detailed comparison and analysis of these benchmarks, highlighting
their strengths and limitations in evaluating NLU tasks and providing in-depth
error analysis. When highlighting the gaps in the state-of-the-art, we noted
that there is no naming convention for macro and micro categories or even a
standard set of linguistic phenomena that should be covered. Consequently, we
formulated a research question regarding the evaluation metrics of the
evaluation diagnostics benchmarks: "Why do not we have an evaluation standard
for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in
industry. We conducted a deep analysis and comparisons of the covered
linguistic phenomena in order to support experts in building a global hierarchy
for linguistic phenomena in future. We think that having evaluation metrics for
diagnostics evaluation could be valuable to gain more insights when comparing
the results of the studied models on different diagnostics benchmarks.

</details>


### [36] [Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG](https://arxiv.org/abs/2507.20136)
*Baiyu Chen,Wilson Wongso,Xiaoqian Hu,Yue Tan,Flora Salim*

Main category: cs.CL

TL;DR: 团队CRUISE为KDD Cup 2025的CRAG-MM挑战开发了一种技术方案，旨在解决现代视觉语言模型（VLMs）在复杂多模态任务中的幻觉问题，通过多阶段框架提升事实准确性，最终在Task 1中获得第三名。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型（VLMs）在处理自我中心图像、长尾实体和复杂多跳问题时容易产生幻觉，这在需要高事实准确性的实际应用中尤为严重。

Method: 提出了一种多阶段框架，包括轻量级查询路由器、查询感知的检索与摘要管道、双路径生成和后验验证，以最小化幻觉。

Result: 在Task 1中获得第三名，验证了该框架在复杂多模态RAG系统中提升答案可靠性的有效性。

Conclusion: 通过优先考虑事实准确性和真实性，该框架有效减少了幻觉问题，适用于复杂多模态任务。

Abstract: This paper presents the technical solution developed by team CRUISE for the
KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn
(CRAG-MM) challenge. The challenge aims to address a critical limitation of
modern Vision Language Models (VLMs): their propensity to hallucinate,
especially when faced with egocentric imagery, long-tail entities, and complex,
multi-hop questions. This issue is particularly problematic in real-world
applications where users pose fact-seeking queries that demand high factual
accuracy across diverse modalities. To tackle this, we propose a robust,
multi-stage framework that prioritizes factual accuracy and truthfulness over
completeness. Our solution integrates a lightweight query router for
efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways
generation and a post-hoc verification. This conservative strategy is designed
to minimize hallucinations, which incur a severe penalty in the competition's
scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the
effectiveness of prioritizing answer reliability in complex multi-modal RAG
systems. Our implementation is available at
https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .

</details>


### [37] [Multi-Agent Interactive Question Generation Framework for Long Document Understanding](https://arxiv.org/abs/2507.20145)
*Kesen Wang,Daulet Toibazar,Abdulrahman Alfulayt,Abdulaziz S. Albadawi,Ranya A. Alkahtani,Asma A. Ibrahim,Haneen A. Alhomoud,Sherif Mohamed,Pedro J. Moreno*

Main category: cs.CL

TL;DR: 论文提出了一种全自动多代理交互框架，用于高效生成长上下文问题，以解决长文档理解中的挑战，特别是在低资源语言（如阿拉伯语）中。


<details>
  <summary>Details</summary>
Motivation: 长上下文场景中复杂布局的文档理解（DU）是视觉语言研究的重大挑战，现有大型视觉语言模型（LVLM）在长上下文任务中表现不佳，且缺乏细粒度训练数据。

Method: 采用全自动多代理交互框架，高效生成长上下文问题，覆盖多页文档和多种领域。

Result: 生成的英语和阿拉伯语问题（AraEngLongBench）对主流LVLM具有挑战性。

Conclusion: 该框架为提升LVLM的长上下文理解能力提供了有效解决方案，代码和数据已开源。

Abstract: Document Understanding (DU) in long-contextual scenarios with complex layouts
remains a significant challenge in vision-language research. Although Large
Vision-Language Models (LVLMs) excel at short-context DU tasks, their
performance declines in long-context settings. A key limitation is the scarcity
of fine-grained training data, particularly for low-resource languages such as
Arabic. Existing state-of-the-art techniques rely heavily on human annotation,
which is costly and inefficient. We propose a fully automated, multi-agent
interactive framework to generate long-context questions efficiently. Our
approach efficiently generates high-quality single- and multi-page questions
for extensive English and Arabic documents, covering hundreds of pages across
diverse domains. This facilitates the development of LVLMs with enhanced
long-context understanding ability. Experimental results in this work have
shown that our generated English and Arabic questions
(\textbf{AraEngLongBench}) are quite challenging to major open- and
close-source LVLMs. The code and data proposed in this work can be found in
https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and
Answer (QA) pairs and structured system prompts can be found in the Appendix.

</details>


### [38] [Goal Alignment in LLM-Based User Simulators for Conversational AI](https://arxiv.org/abs/2507.20152)
*Shuhaib Mehri,Xiaocheng Yang,Takyoung Kim,Gokhan Tur,Shikib Mehri,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 论文提出了一种名为UGST的新框架，用于跟踪多轮对话中的用户目标进展，并开发了能够自主生成目标对齐响应的用户模拟器。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在多轮对话中难以保持一致的目标导向行为，限制了其在对话AI中的可靠性。

Method: 提出了UGST框架和三阶段方法，用于开发能够跟踪目标进展并生成目标对齐响应的用户模拟器。

Result: 在两个基准测试（MultiWOZ 2.4和τ-Bench）中，该方法显著提升了目标对齐性能。

Conclusion: UGST填补了对话AI中的关键空白，成为开发目标对齐用户模拟器的重要框架。

Abstract: User simulators are essential to conversational AI, enabling scalable agent
development and evaluation through simulated interactions. While current Large
Language Models (LLMs) have advanced user simulation capabilities, we reveal
that they struggle to consistently demonstrate goal-oriented behavior across
multi-turn conversations--a critical limitation that compromises their
reliability in downstream applications. We introduce User Goal State Tracking
(UGST), a novel framework that tracks user goal progression throughout
conversations. Leveraging UGST, we present a three-stage methodology for
developing user simulators that can autonomously track goal progression and
reason to generate goal-aligned responses. Moreover, we establish comprehensive
evaluation metrics for measuring goal alignment in user simulators, and
demonstrate that our approach yields substantial improvements across two
benchmarks (MultiWOZ 2.4 and {\tau}-Bench). Our contributions address a
critical gap in conversational AI and establish UGST as an essential framework
for developing goal-aligned user simulators.

</details>


### [39] [SGPO: Self-Generated Preference Optimization based on Self-Improver](https://arxiv.org/abs/2507.20181)
*Hyeonji Lee,Daejin Jo,Seohwan Yun,Sungwoong Kim*

Main category: cs.CL

TL;DR: SGPO是一种创新的对齐框架，通过自我改进机制生成偏好数据，无需依赖外部标注数据，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统对齐方法依赖人工标注数据及分布偏移问题。

Method: 提出SGPO框架，利用自我改进机制生成偏好数据，直接优化策略模型。

Result: 在AlpacaEval 2.0和Arena-Hard上表现优于DPO和基线方法。

Conclusion: SGPO通过自我生成偏好数据，有效提升模型对齐性能。

Abstract: Large language models (LLMs), despite their extensive pretraining on diverse
datasets, require effective alignment to human preferences for practical and
reliable deployment. Conventional alignment methods typically employ off-policy
learning and depend on human-annotated datasets, which limits their broad
applicability and introduces distribution shift issues during training. To
address these challenges, we propose Self-Generated Preference Optimization
based on Self-Improver (SGPO), an innovative alignment framework that leverages
an on-policy self-improving mechanism. Specifically, the improver refines
responses from a policy model to self-generate preference data for direct
preference optimization (DPO) of the policy model. Here, the improver and
policy are unified into a single model, and in order to generate higher-quality
preference data, this self-improver learns to make incremental yet discernible
improvements to the current responses by referencing supervised fine-tuning
outputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the
proposed SGPO significantly improves performance over DPO and baseline
self-improving methods without using external preference data.

</details>


### [40] [SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding](https://arxiv.org/abs/2507.20185)
*Yuqi Yang,Weiqi Wang,Baixuan Xu,Wei Fan,Qing Zong,Chunkit Chan,Zheye Deng,Xin Liu,Yifan Gao,Changlong Yu,Chen Luo,Yang Li,Zheng Li,Qingyu Yin,Bing Yin,Yangqiu Song*

Main category: cs.CL

TL;DR: 论文提出了一种意图树概念和数据集构建流程，创建了SessionIntentBench基准，用于评估模型在复杂会话中理解意图的能力，并验证了意图注入对模型性能的提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效捕捉和建模用户意图，且缺乏数据和基准来明确建模电子商务会话中的意图。

Method: 引入意图树概念和数据集构建流程，构建多模态基准SessionIntentBench，包含四个子任务。

Result: 构建了包含大量意图条目和会话轨迹的数据集，实验表明现有模型难以在复杂会话中捕捉意图，但意图注入能提升性能。

Conclusion: 提出的方法和基准为理解用户意图提供了可扩展的解决方案，并验证了意图注入的有效性。

Abstract: Session history is a common way of recording user interacting behaviors
throughout a browsing activity with multiple products. For example, if an user
clicks a product webpage and then leaves, it might because there are certain
features that don't satisfy the user, which serve as an important indicator of
on-the-spot user preferences. However, all prior works fail to capture and
model customer intention effectively because insufficient information
exploitation and only apparent information like descriptions and titles are
used. There is also a lack of data and corresponding benchmark for explicitly
modeling intention in E-commerce product purchase sessions. To address these
issues, we introduce the concept of an intention tree and propose a dataset
curation pipeline. Together, we construct a sibling multimodal benchmark,
SessionIntentBench, that evaluates L(V)LMs' capability on understanding
inter-session intention shift with four subtasks. With 1,952,177 intention
entries, 1,132,145 session intention trajectories, and 13,003,664 available
tasks mined using 10,905 sessions, we provide a scalable way to exploit the
existing session data for customer intention understanding. We conduct human
annotations to collect ground-truth label for a subset of collected data to
form an evaluation gold set. Extensive experiments on the annotated data
further confirm that current L(V)LMs fail to capture and utilize the intention
across the complex session setting. Further analysis show injecting intention
enhances LLMs' performances.

</details>


### [41] [Diversity-Enhanced Reasoning for Subjective Questions](https://arxiv.org/abs/2507.20187)
*Yumeng Wang,Zhiyuan Fan,Jiayu Liu,Yi R. Fung*

Main category: cs.CL

TL;DR: MultiRole-R1框架通过多角色视角增强推理多样性，提升主观和客观任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在主观问题上表现受限，因其依赖单一真实答案导致推理同质化。

Method: 提出MultiRole-R1框架，结合无监督数据构建和强化学习（GRPO），以多样性为奖励信号。

Result: 实验表明，MultiRole-R1在六个基准测试中提升了推理多样性和准确性。

Conclusion: 多样性增强训练在大型推理模型中具有潜力，尤其在主观任务中表现显著。

Abstract: Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities
have shown strong performance on objective tasks, such as math reasoning and
coding. However, their effectiveness on subjective questions that may have
different responses from different perspectives is still limited by a tendency
towards homogeneous reasoning, introduced by the reliance on a single ground
truth in supervised fine-tuning and verifiable reward in reinforcement
learning. Motivated by the finding that increasing role perspectives
consistently improves performance, we propose MultiRole-R1, a
diversity-enhanced framework with multiple role perspectives, to improve the
accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an
unsupervised data construction pipeline that generates reasoning chains that
incorporate diverse role perspectives. We further employ reinforcement learning
via Group Relative Policy Optimization (GRPO) with reward shaping, by taking
diversity as a reward signal in addition to the verifiable reward. With
specially designed reward functions, we successfully promote perspective
diversity and lexical diversity, uncovering a positive relation between
reasoning diversity and accuracy. Our experiment on six benchmarks demonstrates
MultiRole-R1's effectiveness and generalizability in enhancing both subjective
and objective reasoning, showcasing the potential of diversity-enhanced
training in LRMs.

</details>


### [42] [IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs](https://arxiv.org/abs/2507.20208)
*Aviya Maimon,Amir DN Cohen,Gal Vishne,Shauli Ravfogel,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 论文提出了一种新的评估范式，通过因子分析识别基准测试中的潜在技能，以更全面地评估大型语言模型的整体能力。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型的评估依赖基准测试分数，但难以解释这些分数如何反映模型的整体技能，缺乏对任务间关系的理解。

Method: 使用因子分析识别基准测试中的潜在技能，并将其应用于包含60个模型和44项任务的综合排行榜。

Result: 识别出一小部分潜在技能，这些技能能很大程度上解释模型的表现。

Conclusion: 研究结果为模型选择提供了实用工具，并帮助分析模型在各项潜在技能上的表现。

Abstract: Current evaluations of large language models (LLMs) rely on benchmark scores,
but it is difficult to interpret what these individual scores reveal about a
model's overall skills. Specifically, as a community we lack understanding of
how tasks relate to one another, what they measure in common, how they differ,
or which ones are redundant. As a result, models are often assessed via a
single score averaged across benchmarks, an approach that fails to capture the
models' wholistic strengths and limitations. Here, we propose a new evaluation
paradigm that uses factor analysis to identify latent skills driving
performance across benchmarks. We apply this method to a comprehensive new
leaderboard showcasing the performance of 60 LLMs on 44 tasks, and identify a
small set of latent skills that largely explain performance. Finally, we turn
these insights into practical tools that identify redundant tasks, aid in model
selection, and profile models along each latent skill.

</details>


### [43] [Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation](https://arxiv.org/abs/2507.20210)
*Minh Hoang Nguyen,Thuat Thien Nguyen,Minh Nhat Ta*

Main category: cs.CL

TL;DR: 提出了一种混合新闻推荐框架Co-NAML-LSTUR，结合多视角新闻建模和动态用户兴趣捕捉，显著提升了推荐效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有新闻推荐系统在建模多视角新闻和动态用户兴趣方面的不足。

Method: 整合NAML进行多视角新闻建模和LSTUR捕捉用户长短期兴趣，并引入BERT词嵌入增强语义特征提取。

Result: 在MIND-small和MIND-large基准测试中表现优于现有方法。

Conclusion: 结合多视角新闻表征和双尺度用户建模是有效的，模型已开源。

Abstract: News recommendation systems play a vital role in mitigating information
overload by delivering personalized news content. A central challenge is to
effectively model both multi-view news representations and the dynamic nature
of user interests, which often span both short- and long-term preferences.
Existing methods typically rely on single-view features of news articles (e.g.,
titles or categories) or fail to comprehensively capture user preferences
across time scales. In this work, we propose Co-NAML-LSTUR, a hybrid news
recommendation framework that integrates NAML for attentive multi-view news
modeling and LSTUR for capturing both long- and short-term user
representations. Our model also incorporates BERT-based word embeddings to
enhance semantic feature extraction. We evaluate Co-NAML-LSTUR on two widely
used benchmarks, MIND-small and MIND-large. Experimental results show that
Co-NAML-LSTUR achieves substantial improvements over most state-of-the-art
baselines on MIND-small and MIND-large, respectively. These results demonstrate
the effectiveness of combining multi-view news representations with dual-scale
user modeling. The implementation of our model is publicly available at
https://github.com/MinhNguyenDS/Co-NAML-LSTUR.

</details>


### [44] [Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models](https://arxiv.org/abs/2507.20241)
*Yi Feng,Jiaqi Wang,Wenxuan Zhang,Zhuang Chen,Yutong Shen,Xiyao Xiao,Minlie Huang,Liping Jing,Jian Yu*

Main category: cs.CL

TL;DR: 论文提出了一种结合交互式叙事治疗师（INT）和创新时刻评估（IMA）的框架，用于提升语言模型在心理健康支持中的真实性和治疗效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在心理健康支持中缺乏真实性和对治疗进展的捕捉，叙事疗法因访问限制和社会污名化未得到充分利用。

Method: 通过INT模拟专业叙事治疗师，规划治疗阶段并生成专家级回应；通过IMA量化治疗效果，追踪叙事转变。

Result: 在260个模拟客户和230名人类参与者中，INT在治疗质量和深度上优于标准语言模型。

Conclusion: INT框架能有效提升心理健康支持的对话质量，具有广泛的社会应用潜力。

Abstract: Recent progress in large language models (LLMs) has opened new possibilities
for mental health support, yet current approaches lack realism in simulating
specialized psychotherapy and fail to capture therapeutic progression over
time. Narrative therapy, which helps individuals transform problematic life
stories into empowering alternatives, remains underutilized due to limited
access and social stigma. We address these limitations through a comprehensive
framework with two core components. First, INT (Interactive Narrative
Therapist) simulates expert narrative therapists by planning therapeutic
stages, guiding reflection levels, and generating contextually appropriate
expert-like responses. Second, IMA (Innovative Moment Assessment) provides a
therapy-centric evaluation method that quantifies effectiveness by tracking
"Innovative Moments" (IMs), critical narrative shifts in client speech
signaling therapy progress. Experimental results on 260 simulated clients and
230 human participants reveal that INT consistently outperforms standard LLMs
in therapeutic quality and depth. We further demonstrate the effectiveness of
INT in synthesizing high-quality support conversations to facilitate social
applications.

</details>


### [45] [Modeling Professionalism in Expert Questioning through Linguistic Differentiation](https://arxiv.org/abs/2507.20249)
*Giulia D'Agostino,Chung-Chi Chen*

Main category: cs.CL

TL;DR: 论文探讨了如何利用语言特征建模和评估专家提问中的专业性，提出了一种新的标注框架，并通过实验证明语言特征与专业性和作者身份相关。


<details>
  <summary>Details</summary>
Motivation: 专业性在专家沟通中至关重要，但在高风险领域（如金融）中研究不足。

Method: 引入标注框架量化金融分析师提问的结构和语用元素，构建两个数据集（专业性和作者身份标注），并训练分类器。

Result: 语言特征与人类判断和作者身份强相关，分类器性能优于基线模型。

Conclusion: 专业性是可学习的、领域通用的概念，可通过语言建模捕捉。

Abstract: Professionalism is a crucial yet underexplored dimension of expert
communication, particularly in high-stakes domains like finance. This paper
investigates how linguistic features can be leveraged to model and evaluate
professionalism in expert questioning. We introduce a novel annotation
framework to quantify structural and pragmatic elements in financial analyst
questions, such as discourse regulators, prefaces, and request types. Using
both human-authored and large language model (LLM)-generated questions, we
construct two datasets: one annotated for perceived professionalism and one
labeled by question origin. We show that the same linguistic features correlate
strongly with both human judgments and authorship origin, suggesting a shared
stylistic foundation. Furthermore, a classifier trained solely on these
interpretable features outperforms gemini-2.0 and SVM baselines in
distinguishing expert-authored questions. Our findings demonstrate that
professionalism is a learnable, domain-general construct that can be captured
through linguistically grounded modeling.

</details>


### [46] [Post-Completion Learning for Language Models](https://arxiv.org/abs/2507.20252)
*Xiang Fei,Siqi Wang,Shu Wei,Yuxiang Nie,Wei Shi,Hao Feng,Can Huang*

Main category: cs.CL

TL;DR: 论文提出了一种名为Post-Completion Learning (PCL)的新训练框架，利用模型输出完成后的序列空间，提升推理和自评估能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练在遇到结束标记时停止学习，忽略了后续序列的学习潜力。

Method: 设计了白盒强化学习方法，通过双轨SFT和RL混合训练，优化推理和评估能力。

Result: 在不同数据集和模型上均优于传统SFT和RL方法。

Conclusion: PCL为语言模型训练提供了新的技术路径，提升输出质量的同时保持部署效率。

Abstract: Current language model training paradigms typically terminate learning upon
reaching the end-of-sequence (<eos>}) token, overlooking the potential learning
opportunities in the post-completion space. We propose Post-Completion Learning
(PCL), a novel training framework that systematically utilizes the sequence
space after model output completion, to enhance both the reasoning and
self-evaluation abilities. PCL enables models to continue generating
self-assessments and reward predictions during training, while maintaining
efficient inference by stopping at the completion point.
  To fully utilize this post-completion space, we design a white-box
reinforcement learning method: let the model evaluate the output content
according to the reward rules, then calculate and align the score with the
reward functions for supervision. We implement dual-track SFT to optimize both
reasoning and evaluation capabilities, and mixed it with RL training to achieve
multi-objective hybrid optimization.
  Experimental results on different datasets and models demonstrate consistent
improvements over traditional SFT and RL methods. Our method provides a new
technical path for language model training that enhances output quality while
preserving deployment efficiency.

</details>


### [47] [EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms](https://arxiv.org/abs/2507.20264)
*Abeer Aldayel,Areej Alokaili*

Main category: cs.CL

TL;DR: 论文提出了一种评估框架，关注对话模型中隐含意见的表达，以促进更包容的模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖表面特征（如人口统计或行为属性），忽略了对话中隐含的意见表达，可能导致模型输出中的偏见或刻板印象。

Method: 通过建模回应的立场作为隐含意见的代理，结合PU在线学习和指令调优语言模型进行评估。

Result: 框架揭示了隐含意见的（错误）表达，并提供了改进模型包容性的途径。

Conclusion: 研究强调了隐含意见表达的重要性，为更公平的对话模型设计提供了方向。

Abstract: Shaping inclusive representations that embrace diversity and ensure fair
participation and reflections of values is at the core of many
conversation-based models. However, many existing methods rely on surface
inclusion using mention of user demographics or behavioral attributes of social
groups. Such methods overlook the nuanced, implicit expression of opinion
embedded in conversations. Furthermore, the over-reliance on overt cues can
exacerbate misalignment and reinforce harmful or stereotypical representations
in model outputs. Thus, we took a step back and recognized that equitable
inclusion needs to account for the implicit expression of opinion and use the
stance of responses to validate the normative alignment. This study aims to
evaluate how opinions are represented in NLP or computational models by
introducing an alignment evaluation framework that foregrounds implicit, often
overlooked conversations and evaluates the normative social views and
discourse. Our approach models the stance of responses as a proxy for the
underlying opinion, enabling a considerate and reflective representation of
diverse social viewpoints. We evaluate the framework using both (i)
positive-unlabeled (PU) online learning with base classifiers, and (ii)
instruction-tuned language models to assess post-training alignment. Through
this, we provide a lens on how implicit opinions are (mis)represented and offer
a pathway toward more inclusive model behavior.

</details>


### [48] [MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning](https://arxiv.org/abs/2507.20278)
*Kang Yang,Jingxue Chen,Qingkun Tang,Tianxiang Zhang,Qianchun Lu*

Main category: cs.CL

TL;DR: MoL-RL是一种新的训练范式，通过双目标优化框架将多步环境反馈信号整合到LLMs中，提升了反馈无关的链式推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效利用多步离散的环境反馈信号，导致信息丢失或未能充分利用反馈的上下文信息。

Method: 结合MoL持续训练（分离领域特定反馈信号和通用语言能力）和GRPO后训练，将多步反馈蒸馏为单步推理。

Result: 在数学推理和代码生成基准测试中，MoL-RL表现优异，尤其在Qwen3-8B模型上达到SOTA。

Conclusion: MoL-RL为利用多步文本反馈增强LLMs的推理能力提供了有效方法。

Abstract: Large language models (LLMs) face significant challenges in effectively
leveraging sequential environmental feedback (EF) signals, such as natural
language evaluations, for feedback-independent chain-of-thought (CoT)
reasoning. Existing approaches either convert EF into scalar rewards, losing
rich contextual information, or employ refinement datasets, failing to exploit
the multi-step and discrete nature of EF interactions. To address these
limitations, we propose MoL-RL, a novel training paradigm that integrates
multi-step EF signals into LLMs through a dual-objective optimization
framework. Our method combines MoL (Mixture-of-Losses) continual training,
which decouples domain-specific EF signals (optimized via cross-entropy loss)
and general language capabilities (preserved via Kullback-Leibler divergence),
with GRPO-based post-training to distill sequential EF interactions into
single-step inferences. This synergy enables robust feedback-independent
reasoning without relying on external feedback loops. Experimental results on
mathematical reasoning (MATH-500, AIME24/AIME25) and code generation
(CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art
performance with the Qwen3-8B model, while maintaining strong generalization
across model scales (Qwen3-4B). This work provides a promising approach for
leveraging multi-step textual feedback to enhance LLMs' reasoning capabilities
in diverse domains.

</details>


### [49] [What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations](https://arxiv.org/abs/2507.20279)
*Katharina Trinley,Toshiki Nakai,Tatiana Anikina,Tanja Baeumel*

Main category: cs.CL

TL;DR: 研究分析了多语言大模型Aya-23-8B在处理混合代码、填空和翻译任务时的内部机制，发现其与单语言模型（如Llama 3和Chinese-LLaMA-2）的差异。


<details>
  <summary>Details</summary>
Motivation: 探究多语言大模型内部语言处理机制，以理解其与单语言模型的差异。

Method: 使用logit lens和神经元专业化分析技术，比较Aya-23-8B与单语言模型的表现。

Result: 发现Aya-23在翻译时激活类型相关语言表示，混合代码的神经元激活模式受基础语言影响，语言特定神经元集中在最后层。

Conclusion: 多语言训练显著影响大模型内部机制，为未来跨语言迁移研究提供启示。

Abstract: Large language models (LLMs) excel at multilingual tasks, yet their internal
language processing remains poorly understood. We analyze how Aya-23-8B, a
decoder-only LLM trained on balanced multilingual data, handles code-mixed,
cloze, and translation tasks compared to predominantly monolingual models like
Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization
analyses, we find: (1) Aya-23 activates typologically related language
representations during translation, unlike English-centric models that rely on
a single pivot language; (2) code-mixed neuron activation patterns vary with
mixing rates and are shaped more by the base language than the mixed-in one;
and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in
final layers, diverging from prior findings on decoder-only models. Neuron
overlap analysis further shows that script similarity and typological relations
impact processing across model types. These findings reveal how multilingual
training shapes LLM internals and inform future cross-lingual transfer
research.

</details>


### [50] [Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation](https://arxiv.org/abs/2507.20301)
*Abdullah Alabdullah,Lifeng Han,Chenghua Lin*

Main category: cs.CL

TL;DR: 论文探讨了方言阿拉伯语（DA）与标准阿拉伯语（MSA）的机器翻译问题，提出了无训练提示技术和高效微调管道，显著提升了翻译性能。


<details>
  <summary>Details</summary>
Motivation: 方言阿拉伯语与标准阿拉伯语的差异限制了数字服务和教育资源的获取，阻碍了阿拉伯语机器翻译的发展。

Method: 评估了六种大型语言模型的提示技术，并开发了资源高效的微调管道，包括量化模型和多方言联合训练。

Result: GPT-4o在提示技术中表现最佳，量化Gemma2-9B模型在微调中优于零样本GPT-4o，多方言联合训练模型性能提升超过10%。

Conclusion: 研究表明，即使在资源有限的情况下，也能实现高质量的方言阿拉伯语翻译，为阿拉伯语NLP的包容性提供了实用方案。

Abstract: Dialectal Arabic (DA) poses a persistent challenge for natural language
processing (NLP), as most everyday communication in the Arab world occurs in
dialects that diverge significantly from Modern Standard Arabic (MSA). This
linguistic divide limits access to digital services and educational resources
and impedes progress in Arabic machine translation. This paper presents two
core contributions to advancing DA-MSA translation for the Levantine, Egyptian,
and Gulf dialects, particularly in low-resource and computationally constrained
settings: a comprehensive evaluation of training-free prompting techniques, and
the development of a resource-efficient fine-tuning pipeline. Our evaluation of
prompting strategies across six large language models (LLMs) found that
few-shot prompting consistently outperformed zero-shot, chain-of-thought, and
our proposed Ara-TEaR method. GPT-4o achieved the highest performance across
all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a
CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint
multi-dialect trained models outperformed single-dialect counterparts by over
10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than
1% performance loss. The results and insights of our experiments offer a
practical blueprint for improving dialectal inclusion in Arabic NLP, showing
that high-quality DA-MSA machine translation is achievable even with limited
resources and paving the way for more inclusive language technologies.

</details>


### [51] [DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns](https://arxiv.org/abs/2507.20343)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: DYNARTmo是一个动态发音模型，用于可视化二维矢状面中的语音发音过程，适用于语音学教育和言语治疗。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够模拟关键发音器官的模型，以支持语音发音的可视化和教育。

Method: 基于UK-DYNAMO框架，整合发音不完整性、分段和手势控制以及协同发音原理，模拟六个关键发音器官。

Result: 模型成功生成了元音和辅音的发音配置，并集成到基于网络的应用程序中。

Conclusion: 当前模型实现了静态建模，未来将扩展动态运动生成和与发音-声学模块的集成。

Abstract: We present DYNARTmo, a dynamic articulatory model designed to visualize
speech articulation processes in a two-dimensional midsagittal plane. The model
builds upon the UK-DYNAMO framework and integrates principles of articulatory
underspecification, segmental and gestural control, and coarticulation.
DYNARTmo simulates six key articulators based on ten continuous and six
discrete control parameters, allowing for the generation of both vocalic and
consonantal articulatory configurations. The current implementation is embedded
in a web-based application (SpeechArticulationTrainer) that includes sagittal,
glottal, and palatal views, making it suitable for use in phonetics education
and speech therapy. While this paper focuses on the static modeling aspects,
future work will address dynamic movement generation and integration with
articulatory-acoustic modules.

</details>


### [52] [RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing](https://arxiv.org/abs/2507.20352)
*Hao Xiang,Tianyi Tang,Yang Su,Bowen Yu,An Yang,Fei Huang,Yichang Zhang,Yaojie Lu,Hongyu Lin,Xianpei Han,Jingren Zhou,Junyang Lin,Le Sun*

Main category: cs.CL

TL;DR: RMTBench是一个用户为中心的双语角色扮演基准测试，包含80个角色和8000多轮对话，旨在更真实地评估大型语言模型（LLMs）的角色扮演能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过于角色中心化，简化了用户与角色的互动，无法反映实际应用需求。

Method: RMTBench通过详细角色背景和简单特质定义角色，基于用户动机构建对话，并采用多轮对话模拟机制和LLM评分。

Result: RMTBench提供了一个更贴近实际用户需求的评估框架，填补了学术评估与实际应用之间的差距。

Conclusion: RMTBench为评估LLMs的角色扮演能力提供了更有效的工具，代码和数据集将公开。

Abstract: Recent advancements in Large Language Models (LLMs) have shown outstanding
potential for role-playing applications. Evaluating these capabilities is
becoming crucial yet remains challenging. Existing benchmarks mostly adopt a
\textbf{character-centric} approach, simplify user-character interactions to
isolated Q&A tasks, and fail to reflect real-world applications. To address
this limitation, we introduce RMTBench, a comprehensive \textbf{user-centric}
bilingual role-playing benchmark featuring 80 diverse characters and over 8,000
dialogue rounds. RMTBench includes custom characters with detailed backgrounds
and abstract characters defined by simple traits, enabling evaluation across
various user scenarios. Our benchmark constructs dialogues based on explicit
user motivations rather than character descriptions, ensuring alignment with
practical user applications. Furthermore, we construct an authentic multi-turn
dialogue simulation mechanism. With carefully selected evaluation dimensions
and LLM-based scoring, this mechanism captures the complex intention of
conversations between the user and the character. By shifting focus from
character background to user intention fulfillment, RMTBench bridges the gap
between academic evaluation and practical deployment requirements, offering a
more effective framework for assessing role-playing capabilities in LLMs. All
code and datasets will be released soon.

</details>


### [53] [Length Representations in Large Language Models](https://arxiv.org/abs/2507.20398)
*Sangjun Moon,Dasom Choi,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）通过多头注意力机制编码输出序列长度信息，并能通过调整特定隐藏单元控制长度而不影响语义。


<details>
  <summary>Details</summary>
Motivation: 探索LLM内部如何编码和控制输出序列长度的机制。

Method: 通过实验分析LLM内部表示，特别是多头注意力机制和隐藏单元的作用。

Result: 发现长度信息与语义信息部分解耦，可通过调整隐藏单元控制输出长度。

Conclusion: LLM具备无需外部控制的内部机制，能灵活调整输出长度。

Abstract: Large language models (LLMs) have shown remarkable capabilities across
various tasks, that are learned from massive amounts of text-based data.
Although LLMs can control output sequence length, particularly in
instruction-based settings, the internal mechanisms behind this control have
been unexplored yet. In this study, we provide empirical evidence on how output
sequence length information is encoded within the internal representations in
LLMs. In particular, our findings show that multi-head attention mechanisms are
critical in determining output sequence length, which can be adjusted in a
disentangled manner. By scaling specific hidden units within the model, we can
control the output sequence length without losing the informativeness of the
generated text, thereby indicating that length information is partially
disentangled from semantic information. Moreover, some hidden units become
increasingly active as prompts become more length-specific, thus reflecting the
model's internal awareness of this attribute. Our findings suggest that LLMs
have learned robust and adaptable internal mechanisms for controlling output
length without any external control.

</details>


### [54] [Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations](https://arxiv.org/abs/2507.20409)
*Eunkyu Park,Wesley Hanwen Deng,Gunhee Kim,Motahhare Eslami,Maarten Sap*

Main category: cs.CL

TL;DR: CoCoT是一种基于认知的三阶段提示策略，优于传统CoT和直接提示，提升多模态模型的解释性和社会意识。


<details>
  <summary>Details</summary>
Motivation: 解决视觉任务中传统CoT无法同时处理感知、理解和判断的问题。

Method: 提出Cognitive Chain-of-Thought (CoCoT)，分感知、情境和规范三阶段推理。

Result: 在多模态基准测试中平均提升8%，表现优于CoT和直接提示。

Conclusion: CoCoT为更安全可靠的多模态系统提供了新方向。

Abstract: Chain-of-Thought (CoT) prompting helps models think step by step. But what
happens when they must see, understand, and judge-all at once? In visual tasks
grounded in social context, where bridging perception with norm-grounded
judgments is essential, flat CoT often breaks down. We introduce Cognitive
Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning
through three cognitively inspired stages: perception, situation, and norm. Our
experiments show that, across multiple multimodal benchmarks (including intent
disambiguation, commonsense reasoning, and safety), CoCoT consistently
outperforms CoT and direct prompting (+8\% on average). Our findings
demonstrate that cognitively grounded reasoning stages enhance interpretability
and social awareness in VLMs, paving the way for safer and more reliable
multimodal systems.

</details>


### [55] [CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning](https://arxiv.org/abs/2507.20411)
*George Ibrahim,Rita Ramos,Yova Kementchedjhieva*

Main category: cs.CL

TL;DR: CONCAP是一种多语言图像描述模型，通过结合检索到的描述和图像特定概念，提升多语言性能，减少数据需求。


<details>
  <summary>Details</summary>
Motivation: 多语言视觉语言模型在图像描述任务中表现不及英语模型，主要因多语言训练数据有限且模型参数化成本高。

Method: 提出CONCAP模型，整合检索到的描述与图像特定概念，增强输入图像的上下文理解。

Result: 在XM3600数据集上，CONCAP在低资源和中资源语言中表现优异，数据需求大幅减少。

Conclusion: 概念感知的检索增强能有效缩小多语言性能差距。

Abstract: Multilingual vision-language models have made significant strides in image
captioning, yet they still lag behind their English counterparts due to limited
multilingual training data and costly large-scale model parameterization.
Retrieval-augmented generation (RAG) offers a promising alternative by
conditioning caption generation on retrieved examples in the target language,
reducing the need for extensive multilingual training. However, multilingual
RAG captioning models often depend on retrieved captions translated from
English, which can introduce mismatches and linguistic biases relative to the
source language. We introduce CONCAP, a multilingual image captioning model
that integrates retrieved captions with image-specific concepts, enhancing the
contextualization of the input image and grounding the captioning process
across different languages. Experiments on the XM3600 dataset indicate that
CONCAP enables strong performance on low- and mid-resource languages, with
highly reduced data requirements. Our findings highlight the effectiveness of
concept-aware retrieval augmentation in bridging multilingual performance gaps.

</details>


### [56] [CodeNER: Code Prompting for Named Entity Recognition](https://arxiv.org/abs/2507.20423)
*Sungwoo Han,Hyeyeon Kim,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CL

TL;DR: 提出了一种基于代码提示的新方法，利用大语言模型（LLMs）改进命名实体识别（NER），通过嵌入代码提供详细的BIO标签说明，显著优于传统文本提示方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖输入上下文信息，而NER需要结合详细的标签要求，因此提出代码提示以增强LLMs的理解能力。

Method: 采用代码提示方法，在提示中嵌入代码以提供BIO标签说明，并结合链式思维提示进一步提升性能。

Result: 在英语、阿拉伯语、芬兰语、丹麦语和德语等十个基准数据集上，代码提示方法优于传统文本提示。

Conclusion: 代码提示方法能有效结构化NER指令，结合链式思维提示可进一步提升性能。

Abstract: Recent studies have explored various approaches for treating candidate named
entity spans as both source and target sequences in named entity recognition
(NER) by leveraging large language models (LLMs). Although previous approaches
have successfully generated candidate named entity spans with suitable labels,
they rely solely on input context information when using LLMs, particularly,
ChatGPT. However, NER inherently requires capturing detailed labeling
requirements with input context information. To address this issue, we propose
a novel method that leverages code-based prompting to improve the capabilities
of LLMs in understanding and performing NER. By embedding code within prompts,
we provide detailed BIO schema instructions for labeling, thereby exploiting
the ability of LLMs to comprehend long-range scopes in programming languages.
Experimental results demonstrate that the proposed code-based prompting method
outperforms conventional text-based prompting on ten benchmarks across English,
Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of
explicitly structuring NER instructions. We also verify that combining the
proposed code-based prompting method with the chain-of-thought prompting
further improves performance.

</details>


### [57] [Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems](https://arxiv.org/abs/2507.20491)
*Tuan Bui,Trong Le,Phat Thai,Sang Nguyen,Minh Hua,Ngan Pham,Thang Bui,Tho Quan*

Main category: cs.CL

TL;DR: Text-JEPA是一个轻量级框架，用于将自然语言转换为形式逻辑，结合了神经符号方法，提高了封闭领域问答系统的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在封闭领域（如教育、医疗和法律）中，用户不仅需要准确的答案，还需要透明的推理和可解释的决策过程。现有方法依赖大规模模型且效率低下。

Method: Text-JEPA结合了双系统认知理论，通过System 1生成逻辑表示，System 2（Z3求解器）进行逻辑推理，并提出了三个评估指标。

Result: 在领域特定数据集上，Text-JEPA表现优异，计算开销显著低于基于大型语言模型的系统。

Conclusion: Text-JEPA展示了结构化、可解释推理框架在构建高效且可解释的问答系统中的潜力。

Abstract: Recent advances in large language models (LLMs) have significantly enhanced
question-answering (QA) capabilities, particularly in open-domain contexts.
However, in closed-domain scenarios such as education, healthcare, and law,
users demand not only accurate answers but also transparent reasoning and
explainable decision-making processes. While neural-symbolic (NeSy) frameworks
have emerged as a promising solution, leveraging LLMs for natural language
understanding and symbolic systems for formal reasoning, existing approaches
often rely on large-scale models and exhibit inefficiencies in translating
natural language into formal logic representations.
  To address these limitations, we introduce Text-JEPA (Text-based
Joint-Embedding Predictive Architecture), a lightweight yet effective framework
for converting natural language into first-order logic (NL2FOL). Drawing
inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by
efficiently generating logic representations, while the Z3 solver operates as
System 2, enabling robust logical inference. To rigorously evaluate the
NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework
comprising three custom metrics: conversion score, reasoning score, and
Spearman rho score, which collectively capture the quality of logical
translation and its downstream impact on reasoning accuracy.
  Empirical results on domain-specific datasets demonstrate that Text-JEPA
achieves competitive performance with significantly lower computational
overhead compared to larger LLM-based systems. Our findings highlight the
potential of structured, interpretable reasoning frameworks for building
efficient and explainable QA systems in specialized domains.

</details>


### [58] [AQUA: A Large Language Model for Aquaculture & Fisheries](https://arxiv.org/abs/2507.20520)
*Praneeth Narisetty,Uday Kumar Reddy Kattamanchi,Lohit Akshant Nimma,Sri Ram Kaushik Karnati,Shiva Nagendra Babu Kore,Mounika Golamari,Tejashree Nageshreddy*

Main category: cs.CL

TL;DR: 论文介绍了AQUA，首个专为水产养殖设计的大型语言模型（LLM），旨在解决行业中的复杂问题，如疾病爆发和低效喂养。


<details>
  <summary>Details</summary>
Motivation: 水产养殖在食品安全和经济中至关重要，但面临疾病、低效喂养等挑战，现有AI方法未能针对性解决。

Method: 提出AQUADAPT框架，结合专家知识、大规模语言模型和自动化评估技术，生成高质量合成数据。

Result: AQUA为水产养殖研究、咨询系统和决策工具奠定了基础。

Conclusion: AQUA是首个针对水产养殖的LLM，有望推动行业创新。

Abstract: Aquaculture plays a vital role in global food security and coastal economies
by providing sustainable protein sources. As the industry expands to meet
rising demand, it faces growing challenges such as disease outbreaks,
inefficient feeding practices, rising labor costs, logistical inefficiencies,
and critical hatchery issues, including high mortality rates and poor water
quality control. Although artificial intelligence has made significant
progress, existing machine learning methods fall short of addressing the
domain-specific complexities of aquaculture. To bridge this gap, we introduce
AQUA, the first large language model (LLM) tailored for aquaculture, designed
to support farmers, researchers, and industry practitioners. Central to this
effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic
Framework for generating and refining high-quality synthetic data using a
combination of expert knowledge, largescale language models, and automated
evaluation techniques. Our work lays the foundation for LLM-driven innovations
in aquaculture research, advisory systems, and decision-making tools.

</details>


### [59] [SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers](https://arxiv.org/abs/2507.20527)
*Chaitanya Manem,Pratik Prabhanjan Brahma,Prakamya Mishra,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: SAND-Math是一种生成高质量数学问题并提升其复杂度的管道，显著提升LLM在数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决高性能数学LLM因缺乏新颖、困难训练数据而受限的问题。

Method: 通过生成高质量数学问题并使用“难度提升”步骤系统增加问题复杂度。

Result: 在AIME25基准上表现提升17.85绝对分，难度提升步骤使性能从46.38%增至49.23%。

Conclusion: SAND-Math为构建更高效数学推理LLM提供了实用且可扩展的工具包。

Abstract: The demand for Large Language Models (LLMs) capable of sophisticated
mathematical reasoning is growing across industries. However, the development
of performant mathematical LLMs is critically bottlenecked by the scarcity of
difficult, novel training data. We introduce \textbf{SAND-Math} (Synthetic
Augmented Novel and Difficult Mathematics problems and solutions), a pipeline
that addresses this by first generating high-quality problems from scratch and
then systematically elevating their complexity via a new \textbf{Difficulty
Hiking} step. We demonstrate the effectiveness of our approach through two key
findings. First, augmenting a strong baseline with SAND-Math data significantly
boosts performance, outperforming the next-best synthetic dataset by
\textbf{$\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a
dedicated ablation study, we show our Difficulty Hiking process is highly
effective: by increasing average problem difficulty from 5.02 to 5.98, this
step lifts AIME25 performance from 46.38\% to 49.23\%. The full generation
pipeline, final dataset, and a fine-tuned model form a practical and scalable
toolkit for building more capable and efficient mathematical reasoning LLMs.
SAND-Math dataset is released here:
\href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}

</details>


### [60] [Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations](https://arxiv.org/abs/2507.20528)
*Effi Levi,Gal Ron,Odelia Oshri,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 提出了一种多标签方案，用于联合标注社交媒体对话中的仇恨和反仇恨言论，从主题和修辞维度分类，并通过统计分析揭示互动模式。


<details>
  <summary>Details</summary>
Motivation: 研究仇恨言论在社交媒体上的传播及其反制策略，分析其对在线行为的影响。

Method: 标注92个对话（720条推文），结合公共指标进行统计分析，探索主题与修辞维度的互动模式。

Result: 揭示了仇恨言论的传播方式、反制策略及其潜在影响。

Conclusion: 研究为理解仇恨言论及其反制提供了新视角，有助于改善在线行为管理。

Abstract: We introduce a novel multi-labeled scheme for joint annotation of hate and
counter-hate speech in social media conversations, categorizing hate and
counter-hate messages into thematic and rhetorical dimensions. The thematic
categories outline different discursive aspects of each type of speech, while
the rhetorical dimension captures how hate and counter messages are
communicated, drawing on Aristotle's Logos, Ethos and Pathos. We annotate a
sample of 92 conversations, consisting of 720 tweets, and conduct statistical
analyses, incorporating public metrics, to explore patterns of interaction
between the thematic and rhetorical dimensions within and between hate and
counter-hate speech. Our findings provide insights into the spread of hate
messages on social media, the strategies used to counter them, and their
potential impact on online behavior.

</details>


### [61] [Enhancing Hallucination Detection via Future Context](https://arxiv.org/abs/2507.20546)
*Joosung Lee,Cheonbok Park,Hwiyeol Jo,Jeonghoon Kim,Joonsuk Park,Kang Min Yoo*

Main category: cs.CL

TL;DR: 提出了一种用于黑盒生成器的幻觉检测框架，通过采样未来上下文来检测幻觉。


<details>
  <summary>Details</summary>
Motivation: 随着用户越来越多地接触黑盒生成的文本，检测幻觉成为关键挑战。

Method: 基于幻觉一旦引入后容易持续的特点，采样未来上下文，并将其与多种采样方法结合。

Result: 实验表明，提出的采样方法在多种方法中显著提升了性能。

Conclusion: 采样未来上下文是检测黑盒生成器幻觉的有效方法。

Abstract: Large Language Models (LLMs) are widely used to generate plausible text on
online platforms, without revealing the generation process. As users
increasingly encounter such black-box outputs, detecting hallucinations has
become a critical challenge. To address this challenge, we focus on developing
a hallucination detection framework for black-box generators. Motivated by the
observation that hallucinations, once introduced, tend to persist, we sample
future contexts. The sampled future contexts provide valuable clues for
hallucination detection and can be effectively integrated with various
sampling-based methods. We extensively demonstrate performance improvements
across multiple methods using our proposed sampling approach.

</details>


### [62] [ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning](https://arxiv.org/abs/2507.20564)
*Duc-Tai Dinh,Duc Anh Khoa Dinh*

Main category: cs.CL

TL;DR: ZSE-Cap系统在EVENTA任务中排名第四，通过零样本集成方法实现图像检索和描述，无需微调。


<details>
  <summary>Details</summary>
Motivation: 研究目的是通过集成基础模型和提示工程，实现零样本的图像检索和描述任务。

Method: 检索部分集成CLIP、SigLIP和DINOv2的相似度分数；描述部分通过精心设计的提示引导Gemma 3模型。

Result: 系统最终得分为0.42002，在私有测试集上排名第四。

Conclusion: 通过集成和提示工程，零样本方法在图像检索和描述任务中表现有效。

Abstract: We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system
in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image
retrieval and captioning. Our zero-shot approach requires no finetuning on the
competition's data. For retrieval, we ensemble similarity scores from CLIP,
SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt
to guide the Gemma 3 model, enabling it to link high-level events from the
article to the visual content in the image. Our system achieved a final score
of 0.42002, securing a top-4 position on the private test set, demonstrating
the effectiveness of combining foundation models through ensembling and
prompting. Our code is available at https://github.com/ductai05/ZSE-Cap.

</details>


### [63] [Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior](https://arxiv.org/abs/2507.20614)
*Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 本文系统综述了49项关于反社会行为（ASB）预测的研究，提出了五种核心任务类型的分类法，并分析了建模技术和数据集特征的影响。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的反社会行为（如仇恨言论、骚扰等）对平台安全和社���福祉构成挑战，现有研究多关注事后检测，而预测性方法旨在提前预防。

Method: 通过系统综述49项研究，提出五种任务分类（如早期危害检测、行为风险预测等），并分析建模技术（从经典机器学习到预训练语言模型）和数据集特征。

Result: 研究揭示了方法论挑战（如数据集稀缺性、时间漂移）和新兴研究方向（如多语言建模、跨平台泛化）。

Conclusion: 本文通过统一框架为未来研究提供指导，旨在实现更稳健且负责任的ASB预测。

Abstract: Antisocial behavior (ASB) on social media-including hate speech, harassment,
and trolling-poses growing challenges for platform safety and societal
wellbeing. While prior work has primarily focused on detecting harmful content
after it appears, predictive approaches aim to forecast future harmful
behaviors-such as hate speech propagation, conversation derailment, or user
recidivism-before they fully unfold. Despite increasing interest, the field
remains fragmented, lacking a unified taxonomy or clear synthesis of existing
methods. This paper presents a systematic review of over 49 studies on ASB
prediction, offering a structured taxonomy of five core task types: early harm
detection, harm emergence prediction, harm propagation prediction, behavioral
risk prediction, and proactive moderation support. We analyze how these tasks
differ by temporal framing, prediction granularity, and operational goals. In
addition, we examine trends in modeling techniques-from classical machine
learning to pre-trained language models-and assess the influence of dataset
characteristics on task feasibility and generalization. Our review highlights
methodological challenges, such as dataset scarcity, temporal drift, and
limited benchmarks, while outlining emerging research directions including
multilingual modeling, cross-platform generalization, and human-in-the-loop
systems. By organizing the field around a coherent framework, this survey aims
to guide future work toward more robust and socially responsible ASB
prediction.

</details>


### [64] [Ontology-Enhanced Knowledge Graph Completion using Large Language Models](https://arxiv.org/abs/2507.20643)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: OL-KGC方法通过结合神经感知结构信息和本体知识，显著提升了基于LLM的知识图谱补全性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的KGC方法依赖隐式知识表示，容易传播错误知识，阻碍了推理结果的确定性和结论性。

Method: 提出OL-KGC方法，利用神经感知机制嵌入结构信息，并通过自动提取算法将本体知识转化为LLM可理解的文本格式。

Result: 在FB15K-237、UMLS和WN18RR基准测试中，OL-KGC显著优于现有主流方法，达到最优性能。

Conclusion: OL-KGC通过结合结构信息和本体知识，有效提升了KGC的性能和推理能力。

Abstract: Large Language Models (LLMs) have been extensively adopted in Knowledge Graph
Completion (KGC), showcasing significant research advancements. However, as
black-box models driven by deep neural architectures, current LLM-based KGC
methods rely on implicit knowledge representation with parallel propagation of
erroneous knowledge, thereby hindering their ability to produce conclusive and
decisive reasoning outcomes. We aim to integrate neural-perceptual structural
information with ontological knowledge, leveraging the powerful capabilities of
LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.
We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first
leverages neural perceptual mechanisms to effectively embed structural
information into the textual space, and then uses an automated extraction
algorithm to retrieve ontological knowledge from the knowledge graphs (KGs)
that needs to be completed, which is further transformed into a textual format
comprehensible to LLMs for providing logic guidance. We conducted extensive
experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The
experimental results demonstrate that OL-KGC significantly outperforms existing
mainstream KGC methods across multiple evaluation metrics, achieving
state-of-the-art performance.

</details>


### [65] [Geometric-Mean Policy Optimization](https://arxiv.org/abs/2507.20673)
*Yuzhong Zhao,Yue Liu,Junpeng Liu,Jingye Chen,Xun Wu,Yaru Hao,Tengchao Lv,Shaohan Huang,Lei Cui,Qixiang Ye,Fang Wan,Furu Wei*

Main category: cs.CL

TL;DR: GMPO是一种改进的GRPO方法，通过优化几何平均奖励提升稳定性，并在数学和多模态推理任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: GRPO在处理异常奖励时存在策略更新不稳定的问题，需要一种更稳定的优化方法。

Method: GMPO通过最大化几何平均奖励而非算术平均，减少异常值的影响，保持重要性采样比的稳定性。

Result: GMPO-7B在多个数学和多模态推理基准测试中平均优于GRPO4.1%和1.4%。

Conclusion: GMPO通过几何平均优化提升了稳定性和性能，适用于复杂推理任务。

Abstract: Recent advancements, such as Group Relative Policy Optimization (GRPO), have
enhanced the reasoning capabilities of large language models by optimizing the
arithmetic mean of token-level rewards. However, GRPO suffers from unstable
policy updates when processing tokens with outlier importance-weighted rewards,
which manifests as extreme importance sampling ratios during training, i.e.,
the ratio between the sampling probabilities assigned to a token by the current
and old policies. In this work, we propose Geometric-Mean Policy Optimization
(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic
mean, GMPO maximizes the geometric mean of token-level rewards, which is
inherently less sensitive to outliers and maintains a more stable range of
importance sampling ratio. In addition, we provide comprehensive theoretical
and experimental analysis to justify the design and stability benefits of GMPO.
Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on
multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,
including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is
available at https://github.com/callsys/GMPO.

</details>


### [66] [When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification](https://arxiv.org/abs/2507.20700)
*Hanna Shcharbakova,Tatiana Anikina,Natalia Skachkova,Josef van Genabith*

Main category: cs.CL

TL;DR: 小规模专用模型（XLM-R）在多语言事实核查任务中表现优于大规模通用模型（LLMs），性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 研究多语言虚假信息快速传播背景下，自动化事实核查系统的有效性，尤其是对细粒度分类的支持。

Method: 在X-Fact数据集上评估五种语言模型（包括XLM-R和mT5等小型模型及Llama等大型模型），采用提示和微调方法。

Result: XLM-R（270M参数）显著优于所有测试的LLMs（7-12B参数），宏F1分数达57.7%，比之前最佳性能提升15.8%。

Conclusion: 对于细粒度多语言事实核查，小型专用模型可能比通用大型模型更有效，对实际部署有重要启示。

Abstract: The rapid spread of multilingual misinformation requires robust automated
fact verification systems capable of handling fine-grained veracity assessments
across diverse languages. While large language models have shown remarkable
capabilities across many NLP tasks, their effectiveness for multilingual claim
verification with nuanced classification schemes remains understudied. We
conduct a comprehensive evaluation of five state-of-the-art language models on
the X-Fact dataset, which spans 25 languages with seven distinct veracity
categories. Our experiments compare small language models (encoder-based XLM-R
and mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo)
using both prompting and fine-tuning approaches. Surprisingly, we find that
XLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B
parameters), achieving 57.7% macro-F1 compared to the best LLM performance of
16.9%. This represents a 15.8% improvement over the previous state-of-the-art
(41.9%), establishing new performance benchmarks for multilingual fact
verification. Our analysis reveals problematic patterns in LLM behavior,
including systematic difficulties in leveraging evidence and pronounced biases
toward frequent categories in imbalanced data settings. These findings suggest
that for fine-grained multilingual fact verification, smaller specialized
models may be more effective than general-purpose large models, with important
implications for practical deployment of fact-checking systems.

</details>


### [67] [Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models](https://arxiv.org/abs/2507.20704)
*Gabriel Downer,Sean Craven,Damian Ruck,Jake Thomas*

Main category: cs.CL

TL;DR: Text2VLM是一个多阶段管道，将纯文本数据集转化为多模态格式，用于评估视觉语言模型（VLMs）对排版提示注入攻击的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 现有评估数据集主要针对纯文本提示，忽略了视觉漏洞，因此需要一种方法来全面评估VLMs的多模态安全性。

Method: Text2VLM通过识别原始文本中的有害内容并将其转化为排版图像，生成多模态提示，用于评估VLMs的脆弱性。

Result: 开源VLMs在引入视觉输入后对提示注入攻击的脆弱性增加，且性能与闭源前沿模型存在显著差距。

Conclusion: Text2VLM为多模态漏洞评估提供了可扩展工具，有助于开发更强大的VLM安全机制。

Abstract: The increasing integration of Visual Language Models (VLMs) into AI systems
necessitates robust model alignment, especially when handling multimodal
content that combines text and images. Existing evaluation datasets heavily
lean towards text-only prompts, leaving visual vulnerabilities under evaluated.
To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline
that adapts text-only datasets into multimodal formats, specifically designed
to evaluate the resilience of VLMs against typographic prompt injection
attacks. The Text2VLM pipeline identifies harmful content in the original text
and converts it into a typographic image, creating a multimodal prompt for
VLMs. Also, our evaluation of open-source VLMs highlights their increased
susceptibility to prompt injection when visual inputs are introduced, revealing
critical weaknesses in the current models' alignment. This is in addition to a
significant performance gap compared to closed-source frontier models. We
validate Text2VLM through human evaluations, ensuring the alignment of
extracted salient concepts; text summarization and output classification align
with human expectations. Text2VLM provides a scalable tool for comprehensive
safety assessment, contributing to the development of more robust safety
mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,
Text2VLM plays a role in advancing the safe deployment of VLMs in diverse,
real-world applications.

</details>


### [68] [Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.20749)
*Yiran Huang,Lukas Thede,Massimiliano Mancini,Wenjia Xu,Zeynep Akata*

Main category: cs.CL

TL;DR: 提出了一种通过结构剪枝和高效恢复训练直接压缩多模态大语言模型（MLLMs）的方法，以解决其计算和内存需求高的问题。


<details>
  <summary>Details</summary>
Motivation: 当前参数减少技术灵活性有限且计算密集，需要更高效的压缩方法。

Method: 研究了层间和宽度剪枝两种范式，结合监督微调和知识蒸馏进行恢复训练。

Result: 宽度剪枝在低资源场景下表现更好，仅需5%数据即可恢复95%性能。

Conclusion: 该方法为压缩MLLMs提供了实用方案，无需大量计算资源或数据。

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate impressive
capabilities, their substantial computational and memory requirements pose
significant barriers to practical deployment. Current parameter reduction
techniques primarily involve training MLLMs from Small Language Models (SLMs),
but these methods offer limited flexibility and remain computationally
intensive. To address this gap, we propose to directly compress existing MLLMs
through structural pruning combined with efficient recovery training.
Specifically, we investigate two structural pruning paradigms--layerwise and
widthwise pruning--applied to the language model backbone of MLLMs, alongside
supervised finetuning and knowledge distillation. Additionally, we assess the
feasibility of conducting recovery training with only a small fraction of the
available data. Our results show that widthwise pruning generally maintains
better performance in low-resource scenarios with limited computational
resources or insufficient finetuning data. As for the recovery training,
finetuning only the multimodal projector is sufficient at small compression
levels (< 20%). Furthermore, a combination of supervised finetuning and
hidden-state distillation yields optimal recovery across various pruning
levels. Notably, effective recovery can be achieved with as little as 5% of the
original training data, while retaining over 95% of the original performance.
Through empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and
Bunny-v1.0-3B, this study offers actionable insights for practitioners aiming
to compress MLLMs effectively without extensive computation resources or
sufficient data.

</details>


### [69] [Multilingual Self-Taught Faithfulness Evaluators](https://arxiv.org/abs/2507.20752)
*Carlo Alfano,Aymen Al Marjani,Zeno Jonke,Amin Mantrach,Saab Mansour,Marcello Federico*

Main category: cs.CL

TL;DR: 提出了一种基于合成多语言摘要数据的自我学习评估框架，用于跨语言忠实度评估，无需大量标注数据。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在多语言环境下信息幻觉的评估问题，减少对昂贵人工标注数据的依赖。

Method: 利用合成多语言摘要数据和跨语言迁移学习，比较语言特定与混合语言微调方法。

Result: 框架在跨语言忠实度评估中优于现有基线，包括最先进的英语评估器和机器翻译方法。

Conclusion: 研究表明，LLM的通用语言能力与其在特定语言评估任务中的表现存在一致关系。

Abstract: The growing use of large language models (LLMs) has increased the need for
automatic evaluation systems, particularly to address the challenge of
information hallucination. Although existing faithfulness evaluation approaches
have shown promise, they are predominantly English-focused and often require
expensive human-labeled training data for fine-tuning specialized models. As
LLMs see increased adoption in multilingual contexts, there is a need for
accurate faithfulness evaluators that can operate across languages without
extensive labeled data. This paper presents Self-Taught Evaluators for
Multilingual Faithfulness, a framework that learns exclusively from synthetic
multilingual summarization data while leveraging cross-lingual transfer
learning. Through experiments comparing language-specific and mixed-language
fine-tuning approaches, we demonstrate a consistent relationship between an
LLM's general language capabilities and its performance in language-specific
evaluation tasks. Our framework shows improvements over existing baselines,
including state-of-the-art English evaluators and machine translation-based
approaches.

</details>


### [70] [On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey](https://arxiv.org/abs/2507.20783)
*Meishan Zhang,Xin Zhang,Xinping Zhao,Shouzheng Huang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 本文综述了预训练语言模型（PLMs）在通用文本嵌入（GPTE）中的作用，包括基础架构、高级功能及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着PLMs的兴起，GPTE因其丰富的可迁移表示能力受到广泛关注，本文旨在全面概述PLMs如何推动GPTE的发展。

Method: 通过分析PLMs在GPTE中的基础角色（如嵌入提取、训练策略）和高级角色（如多语言支持、多模态集成），系统梳理其架构与功能。

Result: 总结了PLMs在GPTE中的关键作用，并提出了未来研究方向，如排名集成、安全性考虑等。

Conclusion: 本文为研究人员提供了GPTE的现状与未来潜力的参考，强调了PLMs在推动其发展中的核心地位。

Abstract: Text embeddings have attracted growing interest due to their effectiveness
across a wide range of natural language processing (NLP) tasks, such as
retrieval, classification, clustering, bitext mining, and summarization. With
the emergence of pretrained language models (PLMs), general-purpose text
embeddings (GPTE) have gained significant traction for their ability to produce
rich, transferable representations. The general architecture of GPTE typically
leverages PLMs to derive dense text representations, which are then optimized
through contrastive learning on large-scale pairwise datasets. In this survey,
we provide a comprehensive overview of GPTE in the era of PLMs, focusing on the
roles PLMs play in driving its development. We first examine the fundamental
architecture and describe the basic roles of PLMs in GPTE, i.e., embedding
extraction, expressivity enhancement, training strategies, learning objectives,
and data construction. Then, we describe advanced roles enabled by PLMs, such
as multilingual support, multimodal integration, code understanding, and
scenario-specific adaptation. Finally, we highlight potential future research
directions that move beyond traditional improvement goals, including ranking
integration, safety considerations, bias mitigation, structural information
incorporation, and the cognitive extension of embeddings. This survey aims to
serve as a valuable reference for both newcomers and established researchers
seeking to understand the current state and future potential of GPTE.

</details>


### [71] [Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models](https://arxiv.org/abs/2507.20786)
*Sam Osian,Arpan Dutta,Sahil Bhandari,Iain E. Buchan,Dan W. Joyce*

Main category: cs.CL

TL;DR: 论文提出了一种自动化工具PFD Toolkit，用于分析英格兰和威尔士的预防未来死亡报告（PFD），显著提高了识别和分析儿童自杀相关报告的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统的手动分析PFD报告耗时且效率低，无法满足大规模数据分析的需求。

Method: 使用开源语言模型管道（PFD Toolkit）自动化处理4,249份PFD报告，识别儿童自杀相关案例并进行主题编码。

Result: PFD Toolkit识别出72份儿童自杀报告（是手动方法的两倍），与临床专家标注的一致性高（Cohen's κ = 0.82），处理时间仅需8分16秒。

Conclusion: 自动化LLM分析可高效、可靠地替代手动主题分析，为公共卫生提供可扩展、可重复的及时洞察。

Abstract: Prevention of Future Deaths (PFD) reports, issued by coroners in England and
Wales, flag systemic hazards that may lead to further loss of life. Analysis of
these reports has previously been constrained by the manual effort required to
identify and code relevant cases. In 2025, the Office for National Statistics
(ONS) published a national thematic review of child-suicide PFD reports ($\leq$
18 years), identifying 37 cases from January 2015 to November 2023 - a process
based entirely on manual curation and coding. We evaluated whether a fully
automated, open source "text-to-table" language-model pipeline (PFD Toolkit)
could reproduce the ONS's identification and thematic analysis of child-suicide
PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD
reports published from July 2013 to November 2023 were processed via PFD
Toolkit's large language model pipelines. Automated screening identified cases
where the coroner attributed death to suicide in individuals aged 18 or
younger, and eligible reports were coded for recipient category and 23 concern
sub-themes, replicating the ONS coding frame. PFD Toolkit identified 72
child-suicide PFD reports - almost twice the ONS count. Three blinded
clinicians adjudicated a stratified sample of 144 reports to validate the
child-suicide screening. Against the post-consensus clinical annotations, the
LLM-based workflow showed substantial to almost-perfect agreement (Cohen's
$\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script
runtime was 8m 16s, transforming a process that previously took months into one
that can be completed in minutes. This demonstrates that automated LLM analysis
can reliably and efficiently replicate manual thematic reviews of coronial
data, enabling scalable, reproducible, and timely insights for public health
and safety. The PFD Toolkit is openly available for future research.

</details>


### [72] [Latent Inter-User Difference Modeling for LLM Personalization](https://arxiv.org/abs/2507.20849)
*Yilun Qiu,Tianhao Shi,Xiaoyan Zhao,Fengbin Zhu,Yang Zhang,Fuli Feng*

Main category: cs.CL

TL;DR: DEP框架通过潜在空间建模用户差异，避免依赖语言提示，显著提升个性化输出效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖用户历史或语言提示，难以有效捕捉用户间差异，影响个性化效果。

Method: DEP通过对比用户嵌入与同行嵌入构建软提示，利用稀疏自编码器过滤任务相关特征，注入冻结的LLM。

Result: 在个性化评论生成任务中，DEP在多个指标上优于基线方法。

Conclusion: DEP通过潜在空间建模用户差异，为个性化任务提供了更有效的方法。

Abstract: Large language models (LLMs) are increasingly integrated into users' daily
lives, leading to a growing demand for personalized outputs. Previous work
focuses on leveraging a user's own history, overlooking inter-user differences
that are crucial for effective personalization. While recent work has attempted
to model such differences, the reliance on language-based prompts often hampers
the effective extraction of meaningful distinctions. To address these issues,
we propose Difference-aware Embedding-based Personalization (DEP), a framework
that models inter-user differences in the latent space instead of relying on
language prompts. DEP constructs soft prompts by contrasting a user's embedding
with those of peers who engaged with similar content, highlighting relative
behavioral signals. A sparse autoencoder then filters and compresses both
user-specific and difference-aware embeddings, preserving only task-relevant
features before injecting them into a frozen LLM. Experiments on personalized
review generation show that DEP consistently outperforms baseline methods
across multiple metrics. Our code is available at
https://github.com/SnowCharmQ/DEP.

</details>


### [73] [A survey of diversity quantification in natural language processing: The why, what, where and how](https://arxiv.org/abs/2507.20858)
*Louis Estève,Marie-Catherine de Marneffe,Nurit Melnik,Agata Savary,Olha Kanishcheva*

Main category: cs.CL

TL;DR: 该论文探讨了NLP中多样性的概念，提出了一个统一的分类法，并基于生态学和经济学框架对多样性进行了系统化分析。


<details>
  <summary>Details</summary>
Motivation: 研究多样性的动机包括促进包容性、模拟人类语言行为以及提升系统性能。

Method: 通过调查过去6年ACL Anthology中标题包含“多样性”或“多样”的文章，作者提出了一个统一的多样性测量分类法，并基于Stirling（2007）的三维框架（多样性、平衡性和差异性）进行分析。

Result: 研究发现NLP中多样性的量化方式多样且术语不一致，通过系统化方法揭示了相关趋势。

Conclusion: 该研究为NLP中多样性的形式化奠定了基础，有助于更好地理解多样性和提高不同方法之间的可比性。

Abstract: The concept of diversity has received increased consideration in Natural
Language Processing (NLP) in recent years. This is due to various motivations
like promoting and inclusion, approximating human linguistic behavior, and
increasing systems' performance. Diversity has however often been addressed in
an ad hoc manner in NLP, and with few explicit links to other domains where
this notion is better theorized. We survey articles in the ACL Anthology from
the past 6 years, with "diversity" or "diverse" in their title. We find a wide
range of settings in which diversity is quantified, often highly specialized
and using inconsistent terminology. We put forward a unified taxonomy of why,
what on, where, and how diversity is measured in NLP. Diversity measures are
cast upon a unified framework from ecology and economy (Stirling, 2007) with 3
dimensions of diversity: variety, balance and disparity. We discuss the trends
which emerge due to this systematized approach. We believe that this study
paves the way towards a better formalization of diversity in NLP, which should
bring a better understanding of this notion and a better comparability between
various approaches.

</details>


### [74] [Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings](https://arxiv.org/abs/2507.20859)
*Luc Builtjes,Joeran Bosma,Mathias Prokop,Bram van Ginneken,Alessa Hering*

Main category: cs.CL

TL;DR: 研究评估了九种开源生成式大型语言模型（LLM）在荷兰语临床信息提取任务中的表现，开发了公开框架llm_extractinator，并发现14B参数模型表现优异，而70B模型性能略高但计算成本更高。


<details>
  <summary>Details</summary>
Motivation: 医疗报告信息提取面临非结构化和领域特定语言的挑战，而专有LLM存在透明度和隐私问题，因此研究开源LLM的适用性。

Method: 使用DRAGON基准测试28项荷兰语临床信息提取任务，开发llm_extractinator框架进行零样本评估。

Result: 14B参数模型（如Phi-4-14B）表现优异，70B模型性能略高但成本更高；翻译为英语会降低性能。

Conclusion: 开源LLM结合llm_extractinator框架可为低资源临床信息提取提供高效、可扩展且隐私友好的解决方案。

Abstract: Medical reports contain rich clinical information but are often unstructured
and written in domain-specific language, posing challenges for information
extraction. While proprietary large language models (LLMs) have shown promise
in clinical natural language processing, their lack of transparency and data
privacy concerns limit their utility in healthcare. This study therefore
evaluates nine open-source generative LLMs on the DRAGON benchmark, which
includes 28 clinical information extraction tasks in Dutch. We developed
\texttt{llm\_extractinator}, a publicly available framework for information
extraction using open-source generative LLMs, and used it to assess model
performance in a zero-shot setting. Several 14 billion parameter models,
Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results,
while the bigger Llama-3.3-70B model achieved slightly higher performance at
greater computational cost. Translation to English prior to inference
consistently degraded performance, highlighting the need of native-language
processing. These findings demonstrate that open-source LLMs, when used with
our framework, offer effective, scalable, and privacy-conscious solutions for
clinical information extraction in low-resource settings.

</details>


### [75] [Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning](https://arxiv.org/abs/2507.20906)
*Jungwon Park,Wonjong Rhee*

Main category: cs.CL

TL;DR: 论文提出了一种名为Soft Injection的新方法，通过任务嵌入和软混合参数减少提示长度并提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索是否多示例提示是传递任务信息的最有效方式，并提出更高效的替代方案。

Method: 方法是通过软混合任务嵌入与注意力头激活，使用预优化的混合参数（软头选择参数）。

Result: 在57个任务和12个LLM上的评估显示，该方法比10-shot ICL性能提升10.1%-13.9%，并减少内存和计算成本。

Conclusion: 结论是该方法为减少提示长度和提升性能提供了新范式，同时揭示了任务相关的注意力头功能。

Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks by conditioning on input-output examples in the prompt, without requiring
any update in model parameters. While widely adopted, it remains unclear
whether prompting with multiple examples is the most effective and efficient
way to convey task information. In this work, we propose Soft Injection of task
embeddings. The task embeddings are constructed only once using few-shot ICL
prompts and repeatedly used during inference. Soft injection is performed by
softly mixing task embeddings with attention head activations using
pre-optimized mixing parameters, referred to as soft head-selection parameters.
This method not only allows a desired task to be performed without in-prompt
demonstrations but also significantly outperforms existing ICL approaches while
reducing memory usage and compute cost at inference time. An extensive
evaluation is performed across 57 tasks and 12 LLMs, spanning four model
families of sizes from 4B to 70B. Averaged across 57 tasks, our method
outperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show
that our method also serves as an insightful tool for analyzing task-relevant
roles of attention heads, revealing that task-relevant head positions selected
by our method transfer across similar tasks but not across dissimilar ones --
underscoring the task-specific nature of head functionality. Our soft injection
method opens a new paradigm for reducing prompt length and improving task
performance by shifting task conditioning from the prompt space to the
activation space.

</details>


### [76] [MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation](https://arxiv.org/abs/2507.20917)
*Adrien Bazoge*

Main category: cs.CL

TL;DR: MediQAl是一个法国医学问答数据集，用于评估语言模型在真实临床场景中的事实记忆和推理能力，包含32,603个问题，分为三类任务，并通过14个大型语言模型验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 填补多语言医学领域资源的空白，评估语言模型在医学问答中的表现。

Method: 构建包含32,603个问题的数据集，分为三类任务（单选、多选和开放性问题），并通过14个语言模型进行验证。

Result: 验证结果显示，模型在事实记忆和推理任务之间存在显著性能差距。

Conclusion: MediQAl为法语医学问答提供了一个全面的基准，填补了多语言医学资源的空白。

Abstract: This work introduces MediQAl, a French medical question answering dataset
designed to evaluate the capabilities of language models in factual medical
recall and reasoning over real-world clinical scenarios. MediQAl contains
32,603 questions sourced from French medical examinations across 41 medical
subjects. The dataset includes three tasks: (i) Multiple-Choice Question with
Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii)
Open-Ended Question with Short-Answer. Each question is labeled as
Understanding or Reasoning, enabling a detailed analysis of models' cognitive
capabilities. We validate the MediQAl dataset through extensive evaluation with
14 large language models, including recent reasoning-augmented models, and
observe a significant performance gap between factual recall and reasoning
tasks. Our evaluation provides a comprehensive benchmark for assessing language
models' performance on French medical question answering, addressing a crucial
gap in multilingual resources for the medical domain.

</details>


### [77] [FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models](https://arxiv.org/abs/2507.20924)
*Roberto Labadie-Tamayo,Adrian Jaques Böck,Djordje Slijepčević,Xihui Chen,Andreas Babic,Matthias Zeppelzauer*

Main category: cs.CL

TL;DR: 论文介绍了针对社交媒体中性别歧视的识别与分类任务，提出了三种模型（SCBM、SCBMT和XLM-RoBERTa），并在EXIST挑战赛中取得了竞争性成绩。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中性别歧视问题日益严重，EXIST挑战赛旨在通过技术手段识别和分类性别歧视内容。

Method: 采用三种模型：SCBM（基于形容词的瓶颈概念）、SCBMT（结合形容词与Transformer）和微调的XLM-RoBERTa，分别处理三个子任务。

Result: XLM-RoBERTa在子任务1.1中表现优异，排名靠前；SCBMT在解释性和性能间取得平衡。

Conclusion: 提出的模型在性别歧视识别任务中有效，兼具性能和可解释性。

Abstract: Sexism has become widespread on social media and in online conversation. To
help address this issue, the fifth Sexism Identification in Social Networks
(EXIST) challenge is initiated at CLEF 2025. Among this year's international
benchmarks, we concentrate on solving the first task aiming to identify and
classify sexism in social media textual posts. In this paper, we describe our
solutions and report results for three subtasks: Subtask 1.1 - Sexism
Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask
1.3 - Sexism Categorization in Tweets. We implement three models to address
each subtask which constitute three individual runs: Speech Concept Bottleneck
Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a
fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as
human-interpretable bottleneck concepts. SCBM leverages large language models
(LLMs) to encode input texts into a human-interpretable representation of
adjectives, then used to train a lightweight classifier for downstream tasks.
SCBMT extends SCBM by fusing adjective-based representation with contextual
embeddings from transformers to balance interpretability and classification
performance. Beyond competitive results, these two models offer fine-grained
explanations at both instance (local) and class (global) levels. We also
investigate how additional metadata, e.g., annotators' demographic profiles,
can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data
augmented with prior datasets, ranks 6th for English and Spanish and 4th for
English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and
Spanish and 6th for Spanish.

</details>


### [78] [FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models](https://arxiv.org/abs/2507.20930)
*Likun Tan,Kuan-Wei Huang,Kevin Wu*

Main category: cs.CL

TL;DR: 提出了一种检测和编辑大型语言模型中事实错误的方法，特别针对金融领域，通过构建合成数据集和微调模型，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在需要事实可靠性的高风险领域（如金融）中的幻觉问题。

Method: 基于用户定义的领域错误分类法构建合成数据集，微调四种语言模型（Phi-4、Phi-4-mini、Qwen3-4B、Qwen3-14B）以检测和编辑事实错误。

Result: 微调后的Phi-4在二进制F1分数上提升了8%，整体检测性能提升了30%；Phi-4-mini虽参数较少，性能仍接近OpenAI-o3。

Conclusion: 提供了一种实用的解决方案，可增强语言模型在金融及其他领域中的可信度和一致性，代码和数据已开源。

Abstract: Hallucinations in large language models pose a critical challenge for
applications requiring factual reliability, particularly in high-stakes domains
such as finance. This work presents an effective approach for detecting and
editing factually incorrect content in model-generated responses based on the
provided context. Given a user-defined domain-specific error taxonomy, we
construct a synthetic dataset by inserting tagged errors into financial
question-answering corpora and then fine-tune four language models, Phi-4,
Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual
inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%
improvement in binary F1 score and a 30% gain in overall detection performance
compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having
only 4 billion parameters, maintains competitive performance with just a 2%
drop in binary detection and a 0.1% decline in overall detection compared to
OpenAI-o3. Our work provides a practical solution for detecting and editing
factual inconsistencies in financial text generation while introducing a
generalizable framework that can enhance the trustworthiness and alignment of
large language models across diverse applications beyond finance. Our code and
data are available at https://github.com/pegasi-ai/fine-grained-editting.

</details>


### [79] [Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models](https://arxiv.org/abs/2507.20956)
*Max Peeperkorn,Tom Kouwenhoven,Dan Brown,Anna Jordanous*

Main category: cs.CL

TL;DR: 研究发现指令调优会降低大语言模型输出的多样性，提出了一种新的解码策略（conformative decoding）以恢复多样性。


<details>
  <summary>Details</summary>
Motivation: 探讨指令调优对语言模型输出多样性的影响，特别是在创意任务中。

Method: 通过多样性指标测量不同LLM的输出多样性，分析OLMo和OLMo 2模型在微调阶段的多样性损失，并提出conformative解码策略。

Result: 指令调优显著降低多样性，DPO影响最大；conformative解码能增加多样性且保持或提升质量。

Conclusion: 指令调优损害多样性，但通过新解码策略可部分恢复多样性而不牺牲质量。

Abstract: Instruction-tuning large language models (LLMs) reduces the diversity of
their outputs, which has implications for many tasks, particularly for creative
tasks. This paper investigates the ``diversity gap'' for a writing prompt
narrative generation task. This gap emerges as measured by current diversity
metrics for various open-weight and open-source LLMs. The results show
significant decreases in diversity due to instruction-tuning. We explore the
diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to
further understand how output diversity is affected. The results indicate that
DPO has the most substantial impact on diversity. Motivated by these findings,
we present a new decoding strategy, conformative decoding, which guides an
instruct model using its more diverse base model to reintroduce output
diversity. We show that conformative decoding typically increases diversity and
even maintains or improves quality.

</details>


### [80] [Memorization in Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.21009)
*Danil Savine,Muni Sreenivas Pydi,Jamal Atif,Olivier Cappé*

Main category: cs.CL

TL;DR: 研究了微调大语言模型（LLMs）中的记忆机制及其影响因素，重点关注医学领域，使用PHEE数据集进行分析。


<details>
  <summary>Details</summary>
Motivation: 探讨微调过程中不同因素对模型记忆训练数据的影响，以应对医学领域的隐私敏感性问题。

Method: 采用成员推理攻击和前缀提示生成任务，分析权重矩阵、困惑度及低秩适应（LoRA）的影响。

Result: 发现Value和Output矩阵对记忆贡献更大；低困惑度与高记忆相关；高LoRA秩增加记忆但边际效应递减。

Conclusion: 揭示了模型性能与隐私风险的权衡，为开发更负责任的大语言模型微调策略提供了依据。

Abstract: This study investigates the mechanisms and factors influencing memorization
in fine-tuned large language models (LLMs), with a focus on the medical domain
due to its privacy-sensitive nature. We examine how different aspects of the
fine-tuning process affect a model's propensity to memorize training data,
using the PHEE dataset of pharmacovigilance events.
  Our research employs two main approaches: a membership inference attack to
detect memorized data, and a generation task with prompted prefixes to assess
verbatim reproduction. We analyze the impact of adapting different weight
matrices in the transformer architecture, the relationship between perplexity
and memorization, and the effect of increasing the rank in low-rank adaptation
(LoRA) fine-tuning.
  Key findings include: (1) Value and Output matrices contribute more
significantly to memorization compared to Query and Key matrices; (2) Lower
perplexity in the fine-tuned model correlates with increased memorization; (3)
Higher LoRA ranks lead to increased memorization, but with diminishing returns
at higher ranks.
  These results provide insights into the trade-offs between model performance
and privacy risks in fine-tuned LLMs. Our findings have implications for
developing more effective and responsible strategies for adapting large
language models while managing data privacy concerns.

</details>


### [81] [Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation](https://arxiv.org/abs/2507.21028)
*Jiaju Chen,Yuxuan Lu,Xiaojie Wang,Huimin Zeng,Jing Huang,Jiri Gesi,Ying Xu,Bingsheng Yao,Dakuo Wang*

Main category: cs.CL

TL;DR: MAJ-EVAL是一个多智能体评估框架，通过自动构建多样化的评估者角色，利用LLM模拟人类评估者，生成多维反馈。


<details>
  <summary>Details</summary>
Motivation: 现实中的NLP应用评估需要多维度的人类视角，但人类评估资源稀缺且昂贵，现有LLM评估方法存在角色设计随意和框架通用性不足的问题。

Method: 提出MAJ-EVAL框架，自动从文本中构建多样评估者角色，利用LLM代理进行群体辩论，生成多维反馈。

Result: 在教育与医疗领域的实验中，MAJ-EVAL生成的评估结果比传统自动评估指标和现有LLM方法更接近人类专家评分。

Conclusion: MAJ-EVAL通过多智能体模拟人类评估者，解决了现有方法的局限性，提升了评估的多样性和准确性。

Abstract: Nearly all human work is collaborative; thus, the evaluation of real-world
NLP applications often requires multiple dimensions that align with diverse
human perspectives. As real human evaluator resources are often scarce and
costly, the emerging "LLM-as-a-judge" paradigm sheds light on a promising
approach to leverage LLM agents to believably simulate human evaluators. Yet,
to date, existing LLM-as-a-judge approaches face two limitations: persona
descriptions of agents are often arbitrarily designed, and the frameworks are
not generalizable to other tasks. To address these challenges, we propose
MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically
construct multiple evaluator personas with distinct dimensions from relevant
text documents (e.g., research papers), instantiate LLM agents with the
personas, and engage in-group debates with multi-agents to Generate
multi-dimensional feedback. Our evaluation experiments in both the educational
and medical domains demonstrate that MAJ-EVAL can generate evaluation results
that better align with human experts' ratings compared with conventional
automated evaluation metrics and existing LLM-as-a-judge methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers](https://arxiv.org/abs/2507.19510)
*Haoxuan Ma,Xishun Liao,Yifan Liu,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 论文提出了一种基于Transformer的方法，利用GPS数据为轮班工人生成完整的活动模式，填补了传统交通调查中的代表性不足问题。


<details>
  <summary>Details</summary>
Motivation: 轮班工人占工业化社会劳动力的15-20%，但在传统交通调查和规划中代表性不足，导致对其出行需求的理解存在偏差。

Method: 采用基于Transformer的模型，结合周期性时间嵌入和过渡聚焦损失函数，从碎片化GPS轨迹生成完整活动模式。

Result: 生成的模式与洛杉矶县的GPS数据分布高度一致（平均JSD < 0.02），验证了方法的有效性。

Conclusion: 该方法为交通规划者提供了强大的数据增强工具，有助于更全面理解24/7的城市出行需求。

Abstract: This paper addresses a critical gap in urban mobility modeling by focusing on
shift workers, a population segment comprising 15-20% of the workforce in
industrialized societies yet systematically underrepresented in traditional
transportation surveys and planning. This underrepresentation is revealed in
this study by a comparative analysis of GPS and survey data, highlighting stark
differences between the bimodal temporal patterns of shift workers and the
conventional 9-to-5 schedules recorded in surveys. To address this bias, we
introduce a novel transformer-based approach that leverages fragmented GPS
trajectory data to generate complete, behaviorally valid activity patterns for
individuals working non-standard hours. Our method employs periodaware temporal
embeddings and a transition-focused loss function specifically designed to
capture the unique activity rhythms of shift workers and mitigate the inherent
biases in conventional transportation datasets. Evaluation shows that the
generated data achieves remarkable distributional alignment with GPS data from
Los Angeles County (Average JSD < 0.02 for all evaluation metrics). By
transforming incomplete GPS traces into complete, representative activity
patterns, our approach provides transportation planners with a powerful data
augmentation tool to fill critical gaps in understanding the 24/7 mobility
needs of urban populations, enabling precise and inclusive transportation
planning.

</details>


### [83] [Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting](https://arxiv.org/abs/2507.19513)
*Khalid Ali,Zineddine Bettouche,Andreas Kassler,Andreas Fischer*

Main category: cs.LG

TL;DR: 提出了一种轻量级双路径时空网络，结合sLSTM和Conv3D模块，显著提升了交通预测性能。


<details>
  <summary>Details</summary>
Motivation: 5G及未来网络中，精准的时空交通预测对智能资源管理至关重要，但传统AI方法难以捕捉复杂的时空模式。

Method: 采用双路径设计，sLSTM处理时间特征，三层Conv3D提取空间特征，通过融合层整合。

Result: 在真实数据集上表现优于ConvLSTM基线，MAE降低23%，泛化能力提升30%。

Conclusion: 该方法适用于大规模新一代网络部署，具有高效性和鲁棒性。

Abstract: Accurate spatiotemporal traffic forecasting is vital for intelligent resource
management in 5G and beyond. However, conventional AI approaches often fail to
capture the intricate spatial and temporal patterns that exist, due to e.g.,
the mobility of users. We introduce a lightweight, dual-path Spatiotemporal
Network that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling
and a three-layer Conv3D module for spatial feature extraction. A fusion layer
integrates both streams into a cohesive representation, enabling robust
forecasting. Our design improves gradient stability and convergence speed while
reducing prediction error. Evaluations on real-world datasets show superior
forecast performance over ConvLSTM baselines and strong generalization to
unseen regions, making it well-suited for large-scale, next-generation network
deployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM,
with a 30% improvement in model generalization.

</details>


### [84] [Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks](https://arxiv.org/abs/2507.19514)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 提出了一种完全基于小波域的光谱学习框架，通过在小波系数上应用可学习的非线性变换，实现了高效的模型设计。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络层存在计算复杂性和过参数化问题，希望通过光谱学习提供更紧凑、高效的替代方案。

Method: 在小波域中应用可学习的非线性变换（如软阈值和增益-相位调制），并支持自适应选择小波基（如Haar、Daubechies等）。

Result: 在3D去噪和GLUE基准测试中，模型性能接近Transformer基线（89.3% vs 90.1%），但参数和内存分别减少72%和58%。

Conclusion: 光谱学习为视觉和语言任务提供了高效、可解释的模型设计方向，避免了过参数化架构。

Abstract: We introduce a fully spectral learning framework that eliminates traditional
neural layers by operating entirely in the wavelet domain. The model applies
learnable nonlinear transformations, including soft-thresholding and gain-phase
modulation, directly to wavelet coefficients. It also includes a differentiable
wavelet basis selection mechanism, enabling adaptive processing using families
such as Haar, Daubechies, and Biorthogonal wavelets.
  Implemented in PyTorch with full 3D support, the model maintains a spectral
pipeline without spatial convolutions or attention. On synthetic 3D denoising
and natural language tasks from the GLUE benchmark, including SST-2 sentiment
classification, the model achieves 89.3 percent accuracy, close to a 4-layer
Transformer baseline (90.1 percent), while using 72 percent fewer parameters
and 58 percent less peak memory. Faster early convergence is observed due to
spectral sparsity priors.
  In contrast to the quadratic complexity of self-attention and large matrix
multiplications in Transformers, our approach uses linear-time wavelet
transforms and pointwise nonlinearities, significantly reducing inference cost.
This yields a compact, interpretable, and efficient alternative to neural
models. Our results support the viability of principled spectral learning in
both vision and language tasks, offering new directions for model design
without overparameterized architectures.

</details>


### [85] [A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting](https://arxiv.org/abs/2507.19515)
*Edmund F. Agyemang,Hansapani Rodrigo,Vincent Agbenyeavu*

Main category: cs.LG

TL;DR: 比较传统模型与深度学习模型预测甲型流感爆发的效果，发现深度学习模型（尤其是Transformer）表现更优。


<details>
  <summary>Details</summary>
Motivation: 改进甲型流感的预测模型，以提升公共卫生干预策略。

Method: 使用2009年至2023年历史数据，比较ARIMA、ETS与六种深度学习模型（如RNN、LSTM、Transformer等）。

Result: 深度学习模型显著优于传统模型，Transformer的MSE和MAE最低。

Conclusion: 深度学习可提升传染病预测，未来需整合到实时监测系统中。

Abstract: Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year,
though this estimate is an improvement from years past due to improvements in
sanitation, healthcare practices, and vaccination programs. In this study, we
perform a comparative analysis of traditional and deep learning models to
predict Influenza A outbreaks. Using historical data from January 2009 to
December 2023, we compared the performance of traditional ARIMA and Exponential
Smoothing(ETS) models with six distinct deep learning architectures: Simple
RNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer. The results reveal a clear
superiority of all the deep learning models, especially the state-of-the-art
Transformer with respective average testing MSE and MAE of 0.0433 \pm 0.0020
and 0.1126 \pm 0.0016 for capturing the temporal complexities associated with
Influenza A data, outperforming well known traditional baseline ARIMA and ETS
models. These findings of this study provide evidence that state-of-the-art
deep learning architectures can enhance predictive modeling for infectious
diseases and indicate a more general trend toward using deep learning methods
to enhance public health forecasting and intervention planning strategies.
Future work should focus on how these models can be incorporated into real-time
forecasting and preparedness systems at an epidemic level, and integrated into
existing surveillance systems.

</details>


### [86] [BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation](https://arxiv.org/abs/2507.19517)
*Mohit Gupta,Debjit Bhowmick,Ben Beck*

Main category: cs.LG

TL;DR: BikeVAE-GNN是一种结合变分自编码器（VAE）和图神经网络（GNN）的双任务框架，用于估计稀疏自行车网络中的每日自行车流量。


<details>
  <summary>Details</summary>
Motivation: 解决全球城市自行车网络中因数据稀疏性导致的自行车流量估计不准确问题。

Method: 提出BikeVAE-GNN框架，结合Hybrid-GNN（GCN、GAT、GraphSAGE）和VAE，同时进行回归和分类任务。

Result: 在墨尔本市数据上，MAE为30.82，准确率和F1-score均为99%，优于其他模型。

Conclusion: BikeVAE-GNN为稀疏网络中的自行车流量估计提供了先进且有效的解决方案。

Abstract: Accurate link-level bicycle volume estimation is essential for informed urban
and transport planning but it is challenged by extremely sparse count data in
urban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task
framework augmenting a Hybrid Graph Neural Network (GNN) with Variational
Autoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing
sparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model
intricate spatial relationships in sparse networks while VAE generates
synthetic nodes and edges to enrich the graph structure and enhance the
estimation performance. BikeVAE-GNN simultaneously performs - regression for
bicycling volume estimation and classification for bicycling traffic level
categorization. We demonstrate the effectiveness of BikeVAE-GNN using
OpenStreetMap data and publicly available bicycle count data within the City of
Melbourne - where only 141 of 15,933 road segments have labeled counts
(resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN
outperforms machine learning and baseline GNN models, achieving a mean absolute
error (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99.
Ablation studies further validate the effective role of Hybrid-GNN and VAE
components. Our research advances bicycling volume estimation in sparse
networks using novel and state-of-the-art approaches, providing insights for
sustainable bicycling infrastructures.

</details>


### [87] [Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction](https://arxiv.org/abs/2507.19518)
*Sangwoo Seo,Jimin Seo,Yoonho Lee,Donghyeon Kim,Hyejin Shin,Banghyun Sung,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的高效子图匹配方法，用于大规模电路设计中的目标电路定位。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法和节点匹配方法在大规模电路中效率低下，现有深度学习方法无法高效捕获全局子图嵌入或依赖低效匹配矩阵。

Method: 利用GNN预测目标电路高概率区域，通过构建负样本训练GNN，并直接从整个电路中提取子图嵌入以捕获全局信息。

Result: 实验表明，该方法在时间效率和目标区域预测上显著优于现有方法。

Conclusion: 该方法为大规模电路中的子图匹配提供了可扩展且高效的解决方案。

Abstract: Subgraph matching plays an important role in electronic design automation
(EDA) and circuit verification. Traditional rule-based methods have limitations
in generalizing to arbitrary target circuits. Furthermore, node-to-node
matching approaches tend to be computationally inefficient, particularly for
large-scale circuits. Deep learning methods have emerged as a potential
solution to address these challenges, but existing models fail to efficiently
capture global subgraph embeddings or rely on inefficient matching matrices,
which limits their effectiveness for large circuits. In this paper, we propose
an efficient graph matching approach that utilizes Graph Neural Networks (GNNs)
to predict regions of high probability for containing the target circuit.
Specifically, we construct various negative samples to enable GNNs to
accurately learn the presence of target circuits and develop an approach to
directly extracting subgraph embeddings from the entire circuit, which captures
global subgraph information and addresses the inefficiency of applying GNNs to
all candidate subgraphs. Extensive experiments demonstrate that our approach
significantly outperforms existing methods in terms of time efficiency and
target region prediction, offering a scalable and effective solution for
subgraph matching in large-scale circuits.

</details>


### [88] [Physics-informed transfer learning for SHM via feature selection](https://arxiv.org/abs/2507.19519)
*J. Poole,P. Gardner,A. J. Hughes,N. Dervilis,R. S. Mills,T. A. Dardeno,K. Worden*

Main category: cs.LG

TL;DR: 论文提出利用物理知识选择相似特征，使用模态保证准则（MAC）量化健康结构模态的对应关系，以解决结构健康监测（SHM）中数据不足和分布差异问题。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测（SHM）的训练数据昂贵且难以获取，尤其是标记数据。传统机器学习方法难以在不同结构间泛化，因此需要利用迁移学习（TL）解决分布差异问题。

Method: 利用物理知识选择特征，使用模态保证准则（MAC）量化健康结构模态的对应关系，验证其与监督度量的一致性，并作为选择跨域一致特征的标准。

Result: MAC与监督度量高度一致，能有效选择条件分布不变的特征，提升分类器在不同结构间的泛化能力。

Conclusion: MAC是一种有效的特征选择方法，适用于解决SHM中数据不足和分布差异问题，并通过数值和实验案例验证了其有效性。

Abstract: Data used for training structural health monitoring (SHM) systems are
expensive and often impractical to obtain, particularly labelled data.
Population-based SHM presents a potential solution to this issue by considering
the available data across a population of structures. However, differences
between structures will mean the training and testing distributions will
differ; thus, conventional machine learning methods cannot be expected to
generalise between structures. To address this issue, transfer learning (TL),
can be used to leverage information across related domains. An important
consideration is that the lack of labels in the target domain limits data-based
metrics to quantifying the discrepancy between the marginal distributions.
Thus, a prerequisite for the application of typical unsupervised TL methods is
to identify suitable source structures (domains), and a set of features, for
which the conditional distributions are related to the target structure.
Generally, the selection of domains and features is reliant on domain
expertise; however, for complex mechanisms, such as the influence of damage on
the dynamic response of a structure, this task is not trivial. In this paper,
knowledge of physics is leveraged to select more similar features, the modal
assurance criterion (MAC) is used to quantify the correspondence between the
modes of healthy structures. The MAC is shown to have high correspondence with
a supervised metric that measures joint-distribution similarity, which is the
primary indicator of whether a classifier will generalise between domains. The
MAC is proposed as a measure for selecting a set of features that behave
consistently across domains when subjected to damage, i.e. features with
invariance in the conditional distributions. This approach is demonstrated on
numerical and experimental case studies to verify its effectiveness in various
applications.

</details>


### [89] [Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves](https://arxiv.org/abs/2507.19520)
*Ethan Lo,Dan C. Lo*

Main category: cs.LG

TL;DR: 论文探讨了机器学习模型在发现系外行星中的应用，比较了逻辑回归、K近邻和随机森林模型，并强调了数据增强技术的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统手动搜索系外行星效率低下，机器学习能高效处理大量数据并提高准确性，但现有模型依赖复杂算法和超级计算机，本研究旨在简化这一过程。

Method: 研究使用了逻辑回归、K近邻和随机森林模型，基于NASA开普勒太空望远镜的数据进行训练和预测。

Result: 初步结果显示各模型表现良好，但数据不平衡和潜在偏差需要通过数据增强技术进一步优化。

Conclusion: 在搜索系外行星的背景下，数据增强技术显著提高了召回率和精确率，但不同模型的准确率表现不一。

Abstract: With manual searching processes, the rate at which scientists and astronomers
discover exoplanets is slow because of inefficiencies that require an extensive
time of laborious inspections. In fact, as of now there have been about only
5,000 confirmed exoplanets since the late 1900s. Recently, machine learning
(ML) has proven to be extremely valuable and efficient in various fields,
capable of processing massive amounts of data in addition to increasing its
accuracy by learning. Though ML models for discovering exoplanets owned by
large corporations (e.g. NASA) exist already, they largely depend on complex
algorithms and supercomputers. In an effort to reduce such complexities, in
this paper, we report the results and potential benefits of various, well-known
ML models in the discovery and validation of extrasolar planets. The ML models
that are examined in this study include logistic regression, k-nearest
neighbors, and random forest. The dataset on which the models train and predict
is acquired from NASA's Kepler space telescope. The initial results show
promising scores for each model. However, potential biases and dataset
imbalances necessitate the use of data augmentation techniques to further
ensure fairer predictions and improved generalization. This study concludes
that, in the context of searching for exoplanets, data augmentation techniques
significantly improve the recall and precision, while the accuracy varies for
each model.

</details>


### [90] [Applications and Manipulations of Physics-Informed Neural Networks in Solving Differential Equations](https://arxiv.org/abs/2507.19522)
*Aarush Gupta,Kendric Hsu,Syna Mathod*

Main category: cs.LG

TL;DR: 本文介绍了物理信息神经网络（PINN）如何通过结合微分方程的先验知识解决正向和逆向问题，并展示了其在稀疏数据下的高效性。


<details>
  <summary>Details</summary>
Motivation: 研究PINN的目的是利用先验知识改进神经网络在微分方程求解和参数优化中的性能，尤其是在数据稀疏的情况下。

Method: 通过将微分方程的残差嵌入损失函数，PINN同时优化网络权重和模型参数，使用Python和PyTorch实现不同复杂度的模型。

Result: PINN能够有效解决正向和逆向问题，并在稀疏数据下避免过拟合。

Conclusion: PINN通过结合先验知识，为复杂微分方程的求解和参数优化提供了高效且通用的方法。

Abstract: Mathematical models in neural networks are powerful tools for solving complex
differential equations and optimizing their parameters; that is, solving the
forward and inverse problems, respectively. A forward problem predicts the
output of a network for a given input by optimizing weights and biases. An
inverse problem finds equation parameters or coefficients that effectively
model the data. A Physics-Informed Neural Network (PINN) can solve both
problems. PINNs inject prior analytical information about the data into the
cost function to improve model performance outside the training set boundaries.
This also allows PINNs to efficiently solve problems with sparse data without
overfitting by extrapolating the model to fit larger trends in the data. The
prior information we implement is in the form of differential equations.
Residuals are the differences between the left-hand and right-hand sides of
corresponding differential equations; PINNs minimize these residuals to
effectively solve the differential equation and take advantage of prior
knowledge. In this way, the solution and parameters are embedded into the loss
function and optimized, allowing both the weights of the neural network and the
model parameters to be found simultaneously, solving both the forward and
inverse problems in the process. In this paper, we will create PINNs with
residuals of varying complexity, beginning with linear and quadratic models and
then expanding to fit models for the heat equation and other complex
differential equations. We will mainly use Python as the computing language,
using the PyTorch library to aid us in our research.

</details>


### [91] [Language Models for Controllable DNA Sequence Design](https://arxiv.org/abs/2507.19523)
*Xingyu Su,Xiner Li,Yuchao Lin,Ziqian Xie,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: ATGC-Gen是一种基于Transformer的DNA序列生成模型，通过跨模态编码整合生物信号，支持自回归或掩码恢复目标，生成具有生物相关性的序列。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在DNA序列生成中的应用，解决现有方法在可控性和功能性上的不足。

Method: 采用解码器和编码器Transformer架构，结合跨模态编码，训练时支持自回归或掩码恢复目标。

Result: 在启动子和增强子序列设计任务中表现优异，生成序列流畅、多样且生物相关，可控性和功能性显著提升。

Conclusion: ATGC-Gen展示了语言模型在可编程基因组设计中的潜力，代码已开源。

Abstract: We consider controllable DNA sequence design, where sequences are generated
by conditioning on specific biological properties. While language models (LMs)
such as GPT and BERT have achieved remarkable success in natural language
generation, their application to DNA sequence generation remains largely
underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer
Generator for Controllable Generation, which leverages cross-modal encoding to
integrate diverse biological signals. ATGC-Gen is instantiated with both
decoder-only and encoder-only transformer architectures, allowing flexible
training and generation under either autoregressive or masked recovery
objectives. We evaluate ATGC-Gen on representative tasks including promoter and
enhancer sequence design, and further introduce a new dataset based on ChIP-Seq
experiments for modeling protein binding specificity. Our experiments
demonstrate that ATGC-Gen can generate fluent, diverse, and biologically
relevant sequences aligned with the desired properties. Compared to prior
methods, our model achieves notable improvements in controllability and
functional relevance, highlighting the potential of language models in
advancing programmable genomic design. The source code is released at
(https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).

</details>


### [92] [Kolmogorov Arnold Network Autoencoder in Medicine](https://arxiv.org/abs/2507.19524)
*Ugo Lomoio,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: 论文比较了传统自编码器（AE）与Kolmogorov-Arnold网络（KAN）变体在心电信号任务中的性能，发现KAN在某些任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索KAN架构在自编码任务中的潜力，尤其是在心电信号处理中，以验证其相对于传统方法的优势。

Method: 通过对比传统自编码器（线性、卷积、变分）与KAN变体，使用相同或更少参数，在五种任务（重建、生成、去噪、修复、异常检测）上进行实验。

Result: 实验结果表明，KAN变体在某些任务中表现优于传统自编码器。

Conclusion: KAN架构在心电信号处理任务中具有潜力，尤其是在参数效率方面表现突出。

Abstract: Deep learning neural networks architectures such Multi Layer Perceptrons
(MLP) and Convolutional blocks still play a crucial role in nowadays research
advancements. From a topological point of view, these architecture may be
represented as graphs in which we learn the functions related to the nodes
while fixed edges convey the information from the input to the output. A recent
work introduced a new architecture called Kolmogorov Arnold Networks (KAN) that
reports how putting learnable activation functions on the edges of the neural
network leads to better performances in multiple scenarios. Multiple studies
are focusing on optimizing the KAN architecture by adding important features
such as dropout regularization, Autoencoders (AE), model benchmarking and last,
but not least, the KAN Convolutional Network (KCN) that introduced matrix
convolution with KANs learning. This study aims to benchmark multiple versions
of vanilla AEs (such as Linear, Convolutional and Variational) against their
Kolmogorov-Arnold counterparts that have same or less number of parameters.
Using cardiological signals as model input, a total of five different classic
AE tasks were studied: reconstruction, generation, denoising, inpainting and
anomaly detection. The proposed experiments uses a medical dataset
\textit{AbnormalHeartbeat} that contains audio signals obtained from the
stethoscope.

</details>


### [93] [MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs](https://arxiv.org/abs/2507.19525)
*Chenchen Zhao,Zhengyuan Shi,Xiangyu Wen,Chengjie Liu,Yi Liu,Yunhao Zhou,Yuxiang Zhao,Hefei Feng,Yinan Zhu,Gwok-Waa Wan,Xin Cheng,Weiyu Chen,Yongqi Fu,Chujie Chen,Chenhao Xue,Guangyu Sun,Ying Wang,Yibo Lin,Jun Yang,Ning Xu,Xi Wang,Qiang Xu*

Main category: cs.LG

TL;DR: MMCircuitEval是首个多模态基准测试，用于全面评估多模态大语言模型（MLLMs）在EDA任务中的表现，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试范围狭窄，无法全面评估MLLMs在EDA中的表现，因此需要开发一个更全面的基准测试。

Method: 通过从教科书、技术题库、数据手册和实际文档中精心筛选3614个QA对，并经过专家审核，构建了MMCircuitEval基准测试。

Result: 评估显示现有LLMs在EDA任务中存在显著性能差距，特别是在后端设计和复杂计算方面。

Conclusion: MMCircuitEval为MLLMs在EDA中的发展提供了基础资源，有助于其在实际电路设计工作流程中的集成。

Abstract: The emergence of multimodal large language models (MLLMs) presents promising
opportunities for automation and enhancement in Electronic Design Automation
(EDA). However, comprehensively evaluating these models in circuit design
remains challenging due to the narrow scope of existing benchmarks. To bridge
this gap, we introduce MMCircuitEval, the first multimodal benchmark
specifically designed to assess MLLM performance comprehensively across diverse
EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer
(QA) pairs spanning digital and analog circuits across critical EDA stages -
ranging from general knowledge and specifications to front-end and back-end
design. Derived from textbooks, technical question banks, datasheets, and
real-world documentation, each QA pair undergoes rigorous expert review for
accuracy and relevance. Our benchmark uniquely categorizes questions by design
stage, circuit type, tested abilities (knowledge, comprehension, reasoning,
computation), and difficulty level, enabling detailed analysis of model
capabilities and limitations. Extensive evaluations reveal significant
performance gaps among existing LLMs, particularly in back-end design and
complex computations, highlighting the critical need for targeted training
datasets and modeling approaches. MMCircuitEval provides a foundational
resource for advancing MLLMs in EDA, facilitating their integration into
real-world circuit design workflows. Our benchmark is available at
https://github.com/cure-lab/MMCircuitEval.

</details>


### [94] [Quantizing Text-attributed Graphs for Semantic-Structural Integration](https://arxiv.org/abs/2507.19526)
*Jianyuan Bo,Hao Wu,Yuan Fang*

Main category: cs.LG

TL;DR: STAG是一种自监督框架，通过量化图结构信息为离散标记，解决了现有方法在将结构信息嵌入LLM兼容格式时的挑战，支持零样本迁移学习。


<details>
  <summary>Details</summary>
Motivation: 当前方法在将图结构信息嵌入LLM兼容格式时面临计算成本高或丢失结构细节的问题，且依赖源域标记数据，限制了适应性。

Method: STAG通过软分配和KL散度引导的量化，将图结构信息直接量化为离散标记，无需标记数据。

Result: 实验表明STAG在多个节点分类基准上达到最先进性能，且兼容不同LLM架构。

Conclusion: STAG为图学习与LLM的结合提供了优雅解决方案，支持零样本迁移学习。

Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation for
modeling complex relationships across diverse domains. With the rise of large
language models (LLMs), there is growing interest in leveraging their
capabilities for graph learning. However, current approaches face significant
challenges in embedding structural information into LLM-compatible formats,
requiring either computationally expensive alignment mechanisms or manual graph
verbalization techniques that often lose critical structural details. Moreover,
these methods typically require labeled data from source domains for effective
transfer learning, significantly constraining their adaptability. We propose
STAG, a novel self-supervised framework that directly quantizes graph
structural information into discrete tokens using a frozen codebook. Unlike
traditional quantization approaches, our method employs soft assignment and KL
divergence guided quantization to address the unique challenges of graph data,
which lacks natural tokenization structures. Our framework enables both
LLM-based and traditional learning approaches, supporting true zero-shot
transfer learning without requiring labeled data even in the source domain.
Extensive experiments demonstrate state-of-the-art performance across multiple
node classification benchmarks while maintaining compatibility with different
LLM architectures, offering an elegant solution to bridging graph learning with
LLMs.

</details>


### [95] [Research on the application of graph data structure and graph neural network in node classification/clustering tasks](https://arxiv.org/abs/2507.19527)
*Yihan Wang,Jianing Zhao*

Main category: cs.LG

TL;DR: 该论文研究了图数据结构、经典图算法和图神经网络（GNNs），通过理论分析和实验比较，发现GNNs在节点分类和聚类任务中比传统方法准确率提升43%至70%。


<details>
  <summary>Details</summary>
Motivation: 由于图数据的非欧几里得特性，传统机器学习方法面临挑战，研究旨在探索GNNs与传统算法的性能差异及整合策略。

Method: 采用理论分析和比较实验，评估传统算法与GNNs在节点分类和聚类任务中的表现。

Result: GNNs在准确率上显著优于传统方法，提升幅度达43%至70%。

Conclusion: 研究为图表示学习提供了理论指导，并展示了GNNs与传统算法整合的潜力。

Abstract: Graph-structured data are pervasive across domains including social networks,
biological networks, and knowledge graphs. Due to their non-Euclidean nature,
such data pose significant challenges to conventional machine learning methods.
This study investigates graph data structures, classical graph algorithms, and
Graph Neural Networks (GNNs), providing comprehensive theoretical analysis and
comparative evaluation. Through comparative experiments, we quantitatively
assess performance differences between traditional algorithms and GNNs in node
classification and clustering tasks. Results show GNNs achieve substantial
accuracy improvements of 43% to 70% over traditional methods. We further
explore integration strategies between classical algorithms and GNN
architectures, providing theoretical guidance for advancing graph
representation learning research.

</details>


### [96] [Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction](https://arxiv.org/abs/2507.19529)
*Obumneme Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 本文提出了一种基于人工智能的决策支持系统，利用公开气象数据预测绿色氢基础设施的维护压力指数（MPI），以填补大规模氢设施在沙漠环境中缺乏历史数据的空白。


<details>
  <summary>Details</summary>
Motivation: 由于全球绿色氢投资增加但缺乏历史运营数据，尤其是在沙漠环境中，无法准确评估基础设施风险和规划。环境条件成为预测维护压力的可靠替代指标。

Method: 开发了一种人工智能决策支持系统，利用公开气象数据生成维护压力指数（MPI），预测氢基础设施的风险水平和未来维护需求。

Result: 该系统能够为监管和运营决策提供时间基准，验证性能声明，并在缺乏历史数据的情况下将时间风险智能纳入拍卖评估标准。

Conclusion: 提出的MPI工具填补了绿色氢基础设施在沙漠环境中的知识空白，为风险预测和决策提供了可靠支持。

Abstract: As green hydrogen emerges as a major component of global decarbonisation,
Oman has positioned itself strategically through national auctions and
international partnerships. Following two successful green hydrogen project
rounds, the country launched its third auction (R3) in the Duqm region. While
this area exhibits relative geospatial homogeneity, it is still vulnerable to
environmental fluctuations that pose inherent risks to productivity. Despite
growing global investment in green hydrogen, operational data remains scarce,
with major projects like Saudi Arabia's NEOM facility not expected to commence
production until 2026, and Oman's ACME Duqm project scheduled for 2028. This
absence of historical maintenance and performance data from large-scale
hydrogen facilities in desert environments creates a major knowledge gap for
accurate risk assessment for infrastructure planning and auction decisions.
Given this data void, environmental conditions emerge as accessible and
reliable proxy for predicting infrastructure maintenance pressures, because
harsh desert conditions such as dust storms, extreme temperatures, and humidity
fluctuations are well-documented drivers of equipment degradation in renewable
energy systems. To address this challenge, this paper proposes an Artificial
Intelligence decision support system that leverages publicly available
meteorological data to develop a predictive Maintenance Pressure Index (MPI),
which predicts risk levels and future maintenance demands on hydrogen
infrastructure. This tool strengthens regulatory foresight and operational
decision-making by enabling temporal benchmarking to assess and validate
performance claims over time. It can be used to incorporate temporal risk
intelligence into auction evaluation criteria despite the absence of historical
operational benchmarks.

</details>


### [97] [Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation](https://arxiv.org/abs/2507.19530)
*Md Basit Azam,Sarangthem Ibotombi Singh*

Main category: cs.LG

TL;DR: 该研究提出了一种综合框架，用于基于电子健康记录的血压预测，解决了现有机器学习方法的外部验证不足、不确定性量化缺失和数据泄漏预防不充分的问题。


<details>
  <summary>Details</summary>
Motivation: 重症监护病房中血压监测至关重要，但现有机器学习方法存在外部验证不足、不确定性量化缺失和数据泄漏预防不充分等局限性。

Method: 研究采用系统化数据泄漏预防、分位数回归进行不确定性量化，并在MIMIC-III和eICU数据库间进行外部验证。结合梯度提升、随机森林和XGBoost的集成框架，使用74个生理特征。

Result: 内部验证达到临床可接受性能（SBP：R²=0.86，RMSE=6.03 mmHg；DBP：R²=0.49，RMSE=7.13 mmHg），外部验证显示性能下降30%。不确定性量化生成有效预测区间。

Conclusion: 该框架为跨机构AI辅助血压监测提供了实际部署预期，代码已公开。

Abstract: Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)
where hemodynamic instability can
  rapidly progress to cardiovascular collapse. Current machine
  learning (ML) approaches suffer from three limitations: lack of
  external validation, absence of uncertainty quantification, and
  inadequate data leakage prevention. This study presents the
  first comprehensive framework with novel algorithmic leakage
  prevention, uncertainty quantification, and cross-institutional
  validation for electronic health records (EHRs) based BP pre dictions. Our
methodology implemented systematic data leakage
  prevention, uncertainty quantification through quantile regres sion, and
external validation between the MIMIC-III and eICU
  databases. An ensemble framework combines Gradient Boosting,
  Random Forest, and XGBoost with 74 features across five
  physiological domains. Internal validation achieved a clinically
  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03
  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI
  standards. External validation showed 30% degradation with
  critical limitations in patients with hypotensive. Uncertainty
  quantification generated valid prediction intervals (80.3% SBP
  and 79.9% DBP coverage), enabling risk-stratified protocols
  with narrow intervals (< 15 mmHg) for standard monitoring
  and wide intervals (> 30 mmHg) for manual verification. This
  framework provides realistic deployment expectations for cross institutional
AI-assisted BP monitoring in critical care settings.
  The source code is publicly available at https://github.com/
  mdbasit897/clinical-bp-prediction-ehr.

</details>


### [98] [FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings](https://arxiv.org/abs/2507.19534)
*Ali Shakeri,Wei Emma Zhang,Amin Beheshti,Weitong Chen,Jian Yang,Lishan Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedDPG的方法，通过动态生成上下文感知的提示，解决了传统提示调优的固定性问题，并在联邦学习中优化了计算和通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统提示调优方法在灵活性上存在不足，而联邦学习中的计算和通信限制也需要解决。

Method: 引入动态提示生成器网络（FedDPG），在联邦学习环境中生成上下文感知的提示，仅更新少量参数。

Result: 在三个NLP基准数据集上，FedDPG在全局模型性能上优于现有方法，同时显著减少了计算时间和通信参数数量。

Conclusion: FedDPG在保持数据隐私的同时，提升了模型灵活性和效率，为联邦学习中的NLP任务提供了有效解决方案。

Abstract: Pre-trained Language Models (PLMs) have demonstrated impressive performance
in various NLP tasks. However, traditional fine-tuning methods for leveraging
PLMs for downstream tasks entail significant computational overhead.
Prompt-tuning has emerged as an efficient alternative that involves prepending
a limited number of parameters to the input sequence and only updating them
while the PLM's parameters are frozen. However, this technique's prompts remain
fixed for all inputs, reducing the model's flexibility. The Federated Learning
(FL) technique has gained attention in recent years to address the growing
concerns around data privacy. However, challenges such as communication and
computation limitations of clients still need to be addressed. To mitigate
these challenges, this paper introduces the Federated Dynamic Prompt Generator
(FedDPG), which incorporates a dynamic prompt generator network to generate
context-aware prompts based on the given input, ensuring flexibility and
adaptability while prioritising data privacy in federated learning settings.
Our experiments on three NLP benchmark datasets showcase that FedDPG
outperforms the state-of-the-art parameter-efficient fine-tuning methods in
terms of global model performance, and has significantly reduced the
calculation time and the number of parameters to be sent through the FL
network.

</details>


### [99] [Graph Learning Metallic Glass Discovery from Wikipedia](https://arxiv.org/abs/2507.19536)
*K. -C. Ouyang,S. -Y. Zhang,S. -L. Liu,J. Tian,Y. -H. Li,H. Tong,H. -Y. Bai,W. -H. Wang,Y. -C. Hu*

Main category: cs.LG

TL;DR: 论文提出了一种基于材料网络表示和语言模型的数据驱动方法，用于高效合成新材料，特别是金属玻璃。


<details>
  <summary>Details</summary>
Motivation: 传统材料合成过程缓慢且昂贵，尤其是金属玻璃的合成需要多元素优化组合，限制了候选材料的探索。数据稀缺和材料编码不成熟进一步限制了统计学习算法的预测能力。

Method: 使用语言模型从维基百科编码节点元素，设计多种架构的图神经网络作为推荐系统，探索材料间的隐藏关系，并评估不同语言嵌入在材料设计中的能力。

Result: 提出了一种新的范式，利用人工智能探索新型非晶材料及其他材料。

Conclusion: 该方法为高效材料设计提供了新思路，特别是在数据稀缺和复杂材料空间的情况下。

Abstract: Synthesizing new materials efficiently is highly demanded in various research
fields. However, this process is usually slow and expensive, especially for
metallic glasses, whose formation strongly depends on the optimal combinations
of multiple elements to resist crystallization. This constraint renders only
several thousands of candidates explored in the vast material space since 1960.
Recently, data-driven approaches armed by advanced machine learning techniques
provided alternative routes for intelligent materials design. Due to data
scarcity and immature material encoding, the conventional tabular data is
usually mined by statistical learning algorithms, giving limited model
predictability and generalizability. Here, we propose sophisticated data
learning from material network representations. The node elements are encoded
from the Wikipedia by a language model. Graph neural networks with versatile
architectures are designed to serve as recommendation systems to explore hidden
relationships among materials. By employing Wikipedia embeddings from different
languages, we assess the capability of natural languages in materials design.
Our study proposes a new paradigm to harvesting new amorphous materials and
beyond with artificial intelligence.

</details>


### [100] [Swift-Sarsa: Fast and Robust Linear Control](https://arxiv.org/abs/2507.19539)
*Khurram Javed,Richard S. Sutton*

Main category: cs.LG

TL;DR: Javed等人（2024）提出了SwiftTD算法，结合了True Online TD($\lambda$)和步长优化等技术，在Atari游戏预测任务中表现优异。本文扩展SwiftTD到控制问题，提出Swift-Sarsa算法，并在操作条件基准测试中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决传统TD学习算法在控制问题中的局限性，特别是如何在噪声信号中识别相关信号并高效学习。

Method: 结合SwiftTD的关键思想与True Online Sarsa($\lambda$)，开发了Swift-Sarsa算法，并设计了操作条件基准测试。

Result: Swift-Sarsa在操作条件基准测试中成功识别相关信号，无需先验知识，且能高效处理大规模特征搜索。

Conclusion: Swift-Sarsa为处理噪声信号和大规模特征学习提供了新思路，具有广泛的应用潜力。

Abstract: Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD
learning -- SwiftTD -- that augments True Online TD($\lambda$) with step-size
optimization, a bound on the effective learning rate, and step-size decay. In
their experiments SwiftTD outperformed True Online TD($\lambda$) and
TD($\lambda$) on a variety of prediction tasks derived from Atari games, and
its performance was robust to the choice of hyper-parameters. In this extended
abstract we extend SwiftTD to work for control problems. We combine the key
ideas behind SwiftTD with True Online Sarsa($\lambda$) to develop an on-policy
reinforcement learning algorithm called $\textit{Swift-Sarsa}$.
  We propose a simple benchmark for linear on-policy control called the
$\textit{operant conditioning benchmark}$. The key challenge in the operant
conditioning benchmark is that a very small subset of input signals are
relevant for decision making. The majority of the signals are noise sampled
from a non-stationary distribution. To learn effectively, the agent must learn
to differentiate between the relevant signals and the noisy signals, and
minimize prediction errors by assigning credit to the weight parameters
associated with the relevant signals.
  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to
assign credit to the relevant signals without any prior knowledge of the
structure of the problem. It opens the door for solution methods that learn
representations by searching over hundreds of millions of features in parallel
without performance degradation due to noisy or bad features.

</details>


### [101] [Latent Representations of Intracardiac Electrograms for Atrial Fibrillation Driver Detection](https://arxiv.org/abs/2507.19547)
*Pablo Peiro-Corbacho,Long Lin,Pablo Ávila,Alejandro Carta-Bergaz,Ángel Arenal,Carlos Sevilla-Salcedo,Gonzalo R. Ríos-Muñoz*

Main category: cs.LG

TL;DR: 该研究提出了一种基于卷积自编码器的深度学习框架，用于从心房颤动（AF）的腔内电图（EGMs）中无监督提取特征，以帮助检测AF驱动源。


<details>
  <summary>Details</summary>
Motivation: 当前消融疗法对持续性AF效果不佳，因非肺静脉驱动源的存在，需更有效的方法分析EGMs。

Method: 使用卷积自编码器从单极和双极EGMs中提取潜在特征，并训练下游分类器检测旋转和局灶活动。

Result: 模型在检测旋转和局灶活动上表现中等（AUC 0.73-0.76），但在识别心房EGM纠缠上表现优异（AUC 0.93）。

Conclusion: 该方法可实时运行并集成到临床系统中，展示了无监督学习在心脏信号分析中的潜力。

Abstract: Atrial Fibrillation (AF) is the most prevalent sustained arrhythmia, yet
current ablation therapies, including pulmonary vein isolation, are frequently
ineffective in persistent AF due to the involvement of non-pulmonary vein
drivers. This study proposes a deep learning framework using convolutional
autoencoders for unsupervised feature extraction from unipolar and bipolar
intracavitary electrograms (EGMs) recorded during AF in ablation studies. These
latent representations of atrial electrical activity enable the
characterization and automation of EGM analysis, facilitating the detection of
AF drivers.
  The database consisted of 11,404 acquisitions recorded from 291 patients,
containing 228,080 unipolar EGMs and 171,060 bipolar EGMs. The autoencoders
successfully learned latent representations with low reconstruction loss,
preserving the morphological features. The extracted embeddings allowed
downstream classifiers to detect rotational and focal activity with moderate
performance (AUC 0.73-0.76) and achieved high discriminative performance in
identifying atrial EGM entanglement (AUC 0.93).
  The proposed method can operate in real-time and enables integration into
clinical electroanatomical mapping systems to assist in identifying
arrhythmogenic regions during ablation procedures. This work highlights the
potential of unsupervised learning to uncover physiologically meaningful
features from intracardiac signals.

</details>


### [102] [Harnessing intuitive local evolution rules for physical learning](https://arxiv.org/abs/2507.19561)
*Roie Ezraty,Menachem Stern,Shmuel M. Rubinstein*

Main category: cs.LG

TL;DR: 提出了一种物理系统的训练方案BEASTAL，通过边界参数控制实现低功耗学习，适用于回归和分类任务。


<details>
  <summary>Details</summary>
Motivation: 机器学习计算密集且功耗高，探索物理系统实现学习任务以减少功耗。

Method: 利用边界参数控制（输入和输出），通过局部物理规则实现学习，BEASTAL是类似Adaline的算法。

Result: 在仿真中展示了自主学习的回归和分类能力，性能最佳时局部演化规则是非线性的。

Conclusion: BEASTAL无需大规模内存或复杂架构，通过局部规则实现高效物理学习。

Abstract: Machine Learning, however popular and accessible, is computationally
intensive and highly power-consuming, prompting interest in alternative
physical implementations of learning tasks. We introduce a training scheme for
physical systems that minimize power dissipation in which only boundary
parameters (i.e. inputs and outputs) are externally controlled. Using this
scheme, these Boundary-Enabled Adaptive State Tuning Systems (BEASTS) learn by
exploiting local physical rules. Our scheme, BEASTAL (BEAST-Adaline), is the
closest analog of the Adaline algorithm for such systems. We demonstrate this
autonomous learning in silico for regression and classification tasks. Our
approach advances previous physical learning schemes by using intuitive, local
evolution rules without requiring large-scale memory or complex internal
architectures. BEASTAL can perform any linear task, achieving best performance
when the local evolution rule is non-linear.

</details>


### [103] [Federated Calculation of the Free-Support Transportation Barycenter by Single-Loop Dual Decomposition](https://arxiv.org/abs/2507.19627)
*Zhengqi Lin,Andrzej Ruszczyński*

Main category: cs.LG

TL;DR: 提出一种高效的联邦对偶分解算法，用于计算多个分布的Wasserstein重心，并选择解的支撑集。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中需要访问本地数据和重复解决质量传输问题的高复杂度问题。

Method: 采用联邦对偶分解算法，仅使用高度聚合信息，避免矩阵-向量操作。

Result: 算法迭代复杂度低，可扩展性强，优于现有方法。

Conclusion: 该算法在混合模型示例中表现出高效性和优越性。

Abstract: We propose an efficient federated dual decomposition algorithm for
calculating the Wasserstein barycenter of several distributions, including
choosing the support of the solution. The algorithm does not access local data
and uses only highly aggregated information. It also does not require repeated
solutions to mass transportation problems. Because of the absence of any
matrix-vector operations, the algorithm exhibits a very low complexity of each
iteration and significant scalability. We illustrate its virtues and compare it
to the state-of-the-art methods on several examples of mixture models.

</details>


### [104] [Efficient and Scalable Agentic AI with Heterogeneous Systems](https://arxiv.org/abs/2507.19635)
*Zain Asgar,Michelle Nguyen,Sachin Katti*

Main category: cs.LG

TL;DR: 论文提出了一种动态编排AI代理工作负载的系统设计，旨在优化异构计算基础设施上的执行效率，显著降低总拥有成本（TCO）。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理成为主流工作负载，其动态性和复杂性对部署和基础设施提出了挑战，需要高效、可扩展的解决方案。

Method: 设计了基于成本模型的执行图优化框架、MLIR表示和编译系统，以及动态编排系统，支持异构硬件上的细粒度操作和代码生成。

Result: 初步结果显示，异构基础设施可显著降低TCO，甚至旧GPU与新加速器的组合也能达到与最新同构GPU设计相似的TCO。

Conclusion: 异构计算基础设施为AI代理工作负载提供了高效且经济的解决方案，延长了现有硬件的使用寿命。

Abstract: AI agents are emerging as a dominant workload in a wide range of
applications, promising to be the vehicle that delivers the promised benefits
of AI to enterprises and consumers. Unlike conventional software or static
inference, agentic workloads are dynamic and structurally complex. Often these
agents are directed graphs of compute and IO operations that span multi-modal
data input and conversion), data processing and context gathering (e.g vector
DB lookups), multiple LLM inferences, tool calls, etc. To scale AI agent usage,
we need efficient and scalable deployment and agent-serving infrastructure.
  To tackle this challenge, in this paper, we present a system design for
dynamic orchestration of AI agent workloads on heterogeneous compute
infrastructure spanning CPUs and accelerators, both from different vendors and
across different performance tiers within a single vendor. The system delivers
several building blocks: a framework for planning and optimizing agentic AI
execution graphs using cost models that account for compute, memory, and
bandwidth constraints of different HW; a MLIR based representation and
compilation system that can decompose AI agent execution graphs into granular
operators and generate code for different HW options; and a dynamic
orchestration system that can place the granular components across a
heterogeneous compute infrastructure and stitch them together while meeting an
end-to-end SLA. Our design performs a systems level TCO optimization and
preliminary results show that leveraging a heterogeneous infrastructure can
deliver significant TCO benefits. A preliminary surprising finding is that for
some workloads a heterogeneous combination of older generation GPUs with newer
accelerators can deliver similar TCO as the latest generation homogenous GPU
infrastructure design, potentially extending the life of deployed
infrastructure.

</details>


### [105] [Directly Learning Stock Trading Strategies Through Profit Guided Loss Functions](https://arxiv.org/abs/2507.19639)
*Devroop Kar,Zimeng Lyu,Sheeraja Rajakrishnan,Hao Zhang,Alex Ororbia,Travis Desell,Daniel Krutz*

Main category: cs.LG

TL;DR: 论文提出四种新的损失函数，用于指导股票组合的交易决策，通过训练时间序列模型（如Transformer）在高度波动的市场中实现显著收益。


<details>
  <summary>Details</summary>
Motivation: 股票市场的高波动性使得交易决策困难，传统方法难以有效盈利，因此需要新的方法优化交易策略。

Method: 提出四种损失函数，结合Transformer等时间序列模型，直接学习交易策略。

Result: 在50只S&P 500股票组合上，新方法显著优于强化学习基准和买入持有策略，最高收益达51.42%。

Conclusion: 新损失函数结合时间序列模型能有效提升股票交易策略的盈利能力。

Abstract: Stock trading has always been a challenging task due to the highly volatile
nature of the stock market. Making sound trading decisions to generate profit
is particularly difficult under such conditions. To address this, we propose
four novel loss functions to drive decision-making for a portfolio of stocks.
These functions account for the potential profits or losses based with respect
to buying or shorting respective stocks, enabling potentially any artificial
neural network to directly learn an effective trading strategy. Despite the
high volatility in stock market fluctuations over time, training time-series
models such as transformers on these loss functions resulted in trading
strategies that generated significant profits on a portfolio of 50 different
S&P 500 company stocks as compared to a benchmark reinforcment learning
techniques and a baseline buy and hold method. As an example, using 2021, 2022
and 2023 as three test periods, the Crossformer model adapted with our best
loss function was most consistent, resulting in returns of 51.42%, 51.04% and
48.62% respectively. In comparison, the best performing state-of-the-art
reinforcement learning methods, PPO and DDPG, only delivered maximum profits of
around 41%, 2.81% and 41.58% for the same periods. The code is available at
https://anonymous.4open.science/r/bandit-stock-trading-58C8/README.md.

</details>


### [106] [Feature learning is decoupled from generalization in high capacity neural networks](https://arxiv.org/abs/2507.19680)
*Niclas Alexander Göring,Charles London,Abdurrahman Hadi Erturk,Chris Mingard,Yoonsoo Nam,Ard A. Louis*

Main category: cs.LG

TL;DR: 神经网络在特征学习能力上优于核方法，但现有理论主要关注特征学习强度而非特征质量。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络为何在性能上优于核方法，并引入特征质量的概念以衡量这种优势。

Method: 分析现有特征学习理论，并通过实证研究评估其对特征质量的解释能力。

Result: 现有理论未能充分解释神经网络泛化能力，因其主要关注特征学习强度而非特征质量。

Conclusion: 需要发展新的理论框架，以更好地理解神经网络的特征学习及其泛化能力。

Abstract: Neural networks outperform kernel methods, sometimes by orders of magnitude,
e.g. on staircase functions. This advantage stems from the ability of neural
networks to learn features, adapting their hidden representations to better
capture the data. We introduce a concept we call feature quality to measure
this performance improvement. We examine existing theories of feature learning
and demonstrate empirically that they primarily assess the strength of feature
learning, rather than the quality of the learned features themselves.
Consequently, current theories of feature learning do not provide a sufficient
foundation for developing theories of neural network generalization.

</details>


### [107] [Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks](https://arxiv.org/abs/2507.19684)
*Bermet Burkanova,Payam Jome Yazdian,Chuxuan Zhang,Trinity Evans,Paige Tuttösí,Angelica Lim*

Main category: cs.LG

TL;DR: CoMPAS3D是一个大型多样化的即兴萨尔萨舞动作捕捉数据集，旨在推动交互式、表达性人形AI的研究。


<details>
  <summary>Details</summary>
Motivation: 人类交流不仅限于文本，还包括身体动作和协调，而现有AI系统主要关注文本或语音交互。

Method: 通过收集18名不同水平舞者的3小时萨尔萨舞数据，并提供精细的动作标注，构建了CoMPAS3D数据集。

Result: 数据集包含2800多个动作片段标注，并提出了两个基准任务，用于评估合成人类的交互能力。

Conclusion: CoMPAS3D为社交交互式AI和创造性人形动作生成提供了重要资源，并发布了数据集和模型以促进研究。

Abstract: Imagine a humanoid that can safely and creatively dance with a human,
adapting to its partner's proficiency, using haptic signaling as a primary form
of communication. While today's AI systems excel at text or voice-based
interaction with large language models, human communication extends far beyond
text-it includes embodied movement, timing, and physical coordination. Modeling
coupled interaction between two agents poses a formidable challenge: it is
continuous, bidirectionally reactive, and shaped by individual variation. We
present CoMPAS3D, the largest and most diverse motion capture dataset of
improvised salsa dancing, designed as a challenging testbed for interactive,
expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa
dances performed by 18 dancers spanning beginner, intermediate, and
professional skill levels. For the first time, we provide fine-grained salsa
expert annotations, covering over 2,800 move segments, including move types,
combinations, execution errors and stylistic elements. We draw analogies
between partner dance communication and natural language, evaluating CoMPAS3D
on two benchmark tasks for synthetic humans that parallel key problems in
spoken language and dialogue processing: leader or follower generation with
proficiency levels (speaker or listener synthesis), and duet (conversation)
generation. Towards a long-term goal of partner dance with humans, we release
the dataset, annotations, and code, along with a multitask SalsaAgent model
capable of performing all benchmark tasks, alongside additional baselines to
encourage research in socially interactive embodied AI and creative, expressive
humanoid motion generation.

</details>


### [108] [KD-GAT: Combining Knowledge Distillation and Graph Attention Transformer for a Controller Area Network Intrusion Detection System](https://arxiv.org/abs/2507.19686)
*Robert Frenken,Sidra Ghayour Bhatti,Hanqin Zhang,Qadeer Ahmed*

Main category: cs.LG

TL;DR: 论文提出了一种结合图注意力网络（GAT）和知识蒸馏（KD）的入侵检测框架KD-GAT，用于提升CAN协议的安全性。


<details>
  <summary>Details</summary>
Motivation: CAN协议缺乏内置安全机制，易受网络攻击，需要高效的入侵检测方法。

Method: 通过滑动窗口将CAN流量表示为图，使用多层GAT作为教师模型，训练紧凑的学生GAT。

Result: 在三个基准数据集上，学生模型在Car-Hacking和Car-Survival上分别达到99.97%和99.31%的准确率。

Conclusion: KD-GAT在检测精度和计算效率上表现优异，但类别不平衡问题仍需解决。

Abstract: The Controller Area Network (CAN) protocol is widely adopted for in-vehicle
communication but lacks inherent security mechanisms, making it vulnerable to
cyberattacks. This paper introduces KD-GAT, an intrusion detection framework
that combines Graph Attention Networks (GATs) with knowledge distillation (KD)
to enhance detection accuracy while reducing computational complexity. In our
approach, CAN traffic is represented as graphs using a sliding window to
capture temporal and relational patterns. A multi-layer GAT with jumping
knowledge aggregation acting as the teacher model, while a compact student
GAT--only 6.32% the size of the teacher--is trained via a two-phase process
involving supervised pretraining and knowledge distillation with both soft and
hard label supervision. Experiments on three benchmark datasets--Car-Hacking,
Car-Survival, and can-train-and-test demonstrate that both teacher and student
models achieve strong results, with the student model attaining 99.97% and
99.31% accuracy on Car-Hacking and Car-Survival, respectively. However,
significant class imbalance in can-train-and-test has led to reduced
performance for both models on this dataset. Addressing this imbalance remains
an important direction for future work.

</details>


### [109] [NAICS-Aware Graph Neural Networks for Large-Scale POI Co-visitation Prediction: A Multi-Modal Dataset and Methodology](https://arxiv.org/abs/2507.19697)
*Yazeed Alrubyli,Omar Alomeir,Abrar Wafa,Diána Hidvégi,Hend Alrasheed,Mohsen Bahrami*

Main category: cs.LG

TL;DR: 论文提出了一种结合商业分类知识的图神经网络（NAICS-aware GraphSAGE），用于预测大规模场所的共访模式，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 理解人们在访问某个商业场所后的去向对城市规划、零售分析和基于位置的服务至关重要，但传统方法因数据稀疏性和空间与商业关系的复杂性而效果有限。

Method: 采用NAICS-aware GraphSAGE，通过学习嵌入整合商业分类知识，结合空间、时间和社会经济特征，实现端到端的共访模式预测。

Result: 在包含9490万共访记录的POI-Graph数据集上，R-squared值从0.243提升至0.625（提升157%），排名质量也有显著提升（NDCG@10提升32%）。

Conclusion: 商业语义信息（如行业代码）对共访模式预测至关重要，新方法在性能和可扩展性上均优于现有技术。

Abstract: Understanding where people go after visiting one business is crucial for
urban planning, retail analytics, and location-based services. However,
predicting these co-visitation patterns across millions of venues remains
challenging due to extreme data sparsity and the complex interplay between
spatial proximity and business relationships. Traditional approaches using only
geographic distance fail to capture why coffee shops attract different customer
flows than fine dining restaurants, even when co-located. We introduce
NAICS-aware GraphSAGE, a novel graph neural network that integrates business
taxonomy knowledge through learnable embeddings to predict population-scale
co-visitation patterns. Our key insight is that business semantics, captured
through detailed industry codes, provide crucial signals that pure spatial
models cannot explain. The approach scales to massive datasets (4.2 billion
potential venue pairs) through efficient state-wise decomposition while
combining spatial, temporal, and socioeconomic features in an end-to-end
framework. Evaluated on our POI-Graph dataset comprising 94.9 million
co-visitation records across 92,486 brands and 48 US states, our method
achieves significant improvements over state-of-the-art baselines: the
R-squared value increases from 0.243 to 0.625 (a 157 percent improvement), with
strong gains in ranking quality (32 percent improvement in NDCG at 10).

</details>


### [110] [Disjoint Generative Models](https://arxiv.org/abs/2507.19700)
*Anton Danholt Lautrup,Muhammad Rajabinasab,Tobias Hyrup,Arthur Zimek,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: 提出了一种通过分离生成模型生成跨截面合成数据集的新框架，通过分区数据集并分别生成，最后合并结果，显著提高隐私性且效用损失小。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据生成中的隐私问题，同时保持数据效用。

Method: 将数据集分区，分别输入独立的生成模型，通过无共同变量的合并操作整合结果。

Result: 在表格数据上验证了框架的有效性，隐私性显著提升且效用损失小，支持混合模型合成。

Conclusion: 分离生成模型框架在隐私保护和数据效用间取得了良好平衡，适用于多种模型类型。

Abstract: We propose a new framework for generating cross-sectional synthetic datasets
via disjoint generative models. In this paradigm, a dataset is partitioned into
disjoint subsets that are supplied to separate instances of generative models.
The results are then combined post hoc by a joining operation that works in the
absence of common variables/identifiers. The success of the framework is
demonstrated through several case studies and examples on tabular data that
helps illuminate some of the design choices that one may make. The principal
benefit of disjoint generative models is significantly increased privacy at
only a low utility cost. Additional findings include increased effectiveness
and feasibility for certain model types and the possibility for mixed-model
synthesis.

</details>


### [111] [Beyond Nearest Neighbors: Semantic Compression and Graph-Augmented Retrieval for Enhanced Vector Search](https://arxiv.org/abs/2507.19715)
*Rahul Raja,Arpita Vats*

Main category: cs.LG

TL;DR: 论文提出了一种新的检索范式——语义压缩，通过子模优化和信息几何学原理，选择具有代表性的向量集，以提升检索结果的多样性和语义覆盖。


<details>
  <summary>Details</summary>
Motivation: 传统近似最近邻搜索（ANN）在检索时往往忽略语义多样性和上下文丰富性，无法满足如RAG、多跳QA等应用的需求。

Method: 提出基于语义图的向量检索方法，结合kNN或知识图谱，实现多跳、上下文感知的搜索。

Result: 理论分析表明，图结构能有效提升高维空间中的语义覆盖，弥补传统方法的不足。

Conclusion: 研究为基于语义的向量搜索系统奠定了基础，强调混合索引、多样性感知查询和结构化语义检索的重要性。

Abstract: Vector databases typically rely on approximate nearest neighbor (ANN) search
to retrieve the top-k closest vectors to a query in embedding space. While
effective, this approach often yields semantically redundant results, missing
the diversity and contextual richness required by applications such as
retrieval-augmented generation (RAG), multi-hop QA, and memory-augmented
agents. We introduce a new retrieval paradigm: semantic compression, which aims
to select a compact, representative set of vectors that captures the broader
semantic structure around a query. We formalize this objective using principles
from submodular optimization and information geometry, and show that it
generalizes traditional top-k retrieval by prioritizing coverage and diversity.
To operationalize this idea, we propose graph-augmented vector retrieval, which
overlays semantic graphs (e.g., kNN or knowledge-based links) atop vector
spaces to enable multi-hop, context-aware search. We theoretically analyze the
limitations of proximity-based retrieval under high-dimensional concentration
and highlight how graph structures can improve semantic coverage. Our work
outlines a foundation for meaning-centric vector search systems, emphasizing
hybrid indexing, diversity-aware querying, and structured semantic retrieval.
We make our implementation publicly available to foster future research in this
area.

</details>


### [112] [Predicting Human Mobility in Disasters via LLM-Enhanced Cross-City Learning](https://arxiv.org/abs/2507.19737)
*Yinzhou Tang,Huandong Wang,Xiaochen Fan,Yong Li*

Main category: cs.LG

TL;DR: 论文提出DisasterMobLLM框架，利用LLM预测灾害场景下的人类移动意图，并通过RAG增强意图预测器和意图调制位置预测器提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 城市化与气候变化加剧了城市对自然灾害的脆弱性，现有移动预测模型无法适应灾害场景下的移动模式变化。

Method: 结合LLM建模移动意图，通过RAG增强意图预测器和LLM意图优化器预测意图，再映射到具体位置。

Result: 实验显示DisasterMobLLM在Acc@1和F1-score上分别提升32.8%和35.0%。

Conclusion: DisasterMobLLM有效提升了灾害场景下人类移动预测的准确性。

Abstract: The vulnerability of cities to natural disasters has increased with
urbanization and climate change, making it more important to predict human
mobility in the disaster scenarios for downstream tasks including
location-based early disaster warning and pre-allocating rescue resources, etc.
However, existing human mobility prediction models are mainly designed for
normal scenarios, and fail to adapt to disaster scenarios due to the shift of
human mobility patterns under disaster. To address this issue, we introduce
\textbf{DisasterMobLLM}, a mobility prediction framework for disaster scenarios
that can be integrated into existing deep mobility prediction methods by
leveraging LLMs to model the mobility intention and transferring the common
knowledge of how different disasters affect mobility intentions between cities.
This framework utilizes a RAG-Enhanced Intention Predictor to forecast the next
intention, refines it with an LLM-based Intention Refiner, and then maps the
intention to an exact location using an Intention-Modulated Location Predictor.
Extensive experiments illustrate that DisasterMobLLM can achieve a 32.8\%
improvement in terms of Acc@1 and a 35.0\% improvement in terms of the F1-score
of predicting immobility compared to the baselines. The code is available at
https://github.com/tsinghua-fib-lab/DisasterMobLLM.

</details>


### [113] [Modeling enzyme temperature stability from sequence segment perspective](https://arxiv.org/abs/2507.19755)
*Ziqi Zhang,Shiheng Chen,Runze Yang,Zhisheng Wei,Wei Zhang,Lei Wang,Zhanzhi Liu,Fengshan Zhang,Jing Wu,Xiaoyong Pan,Hongbin Shen,Longbing Cao,Zhaohong Deng*

Main category: cs.LG

TL;DR: 论文提出了一种新的深度学习方法Segment Transformer，用于高效预测酶的温度稳定性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 实验测定酶的热稳定性耗时且昂贵，现有计算方法受限于数据不足和不平衡分布。

Method: 利用精心策划的温度稳定性数据集，开发了Segment Transformer模型，结合蛋白质序列的片段级表示。

Result: 模型在预测酶温度稳定性上表现优异（RMSE 24.03，MAE 18.09），并通过实验验证了1.64倍的活性提升。

Conclusion: Segment Transformer为酶的热稳定性预测和工程提供了高效工具，具有实际应用潜力。

Abstract: Developing enzymes with desired thermal properties is crucial for a wide
range of industrial and research applications, and determining temperature
stability is an essential step in this process. Experimental determination of
thermal parameters is labor-intensive, time-consuming, and costly. Moreover,
existing computational approaches are often hindered by limited data
availability and imbalanced distributions. To address these challenges, we
introduce a curated temperature stability dataset designed for model
development and benchmarking in enzyme thermal modeling. Leveraging this
dataset, we present the \textit{Segment Transformer}, a novel deep learning
framework that enables efficient and accurate prediction of enzyme temperature
stability. The model achieves state-of-the-art performance with an RMSE of
24.03, MAE of 18.09, and Pearson and Spearman correlations of 0.33,
respectively. These results highlight the effectiveness of incorporating
segment-level representations, grounded in the biological observation that
different regions of a protein sequence contribute unequally to thermal
behavior. As a proof of concept, we applied the Segment Transformer to guide
the engineering of a cutinase enzyme. Experimental validation demonstrated a
1.64-fold improvement in relative activity following heat treatment, achieved
through only 17 mutations and without compromising catalytic function.

</details>


### [114] [Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation](https://arxiv.org/abs/2507.19771)
*Xin Zhang,Lissette Iturburu,Juan Nicolas Villamizar,Xiaoyu Liu,Manuel Salmeron,Shirley J. Dyke,Julio Ramirez*

Main category: cs.LG

TL;DR: 提出了一种基于生成式AI的方法，利用大型语言模型（LLM）和检索增强生成（RAG）技术，将自然语言描述直接转换为AutoCAD结构图纸，显著减少工程师的工作量。


<details>
  <summary>Details</summary>
Motivation: 结构图纸在工程领域至关重要，但生成过程耗时且劳动密集。现有软件能力有限，需要更高效的方法。

Method: 结合LLM和RAG技术，通过自然语言处理提取信息并生成AutoCAD代码，实现图纸自动生成。

Result: 该方法能高效准确地将自然语言描述转换为结构图纸，显著提升工作效率。

Conclusion: 生成式AI方法为结构图纸生成提供了高效解决方案，简化了工程师的设计流程。

Abstract: Structural drawings are widely used in many fields, e.g., mechanical
engineering, civil engineering, etc. In civil engineering, structural drawings
serve as the main communication tool between architects, engineers, and
builders to avoid conflicts, act as legal documentation, and provide a
reference for future maintenance or evaluation needs. They are often organized
using key elements such as title/subtitle blocks, scales, plan views, elevation
view, sections, and detailed sections, which are annotated with standardized
symbols and line types for interpretation by engineers and contractors. Despite
advances in software capabilities, the task of generating a structural drawing
remains labor-intensive and time-consuming for structural engineers. Here we
introduce a novel generative AI-based method for generating structural drawings
employing a large language model (LLM) agent. The method incorporates a
retrieval-augmented generation (RAG) technique using externally-sourced facts
to enhance the accuracy and reliability of the language model. This method is
capable of understanding varied natural language descriptions, processing these
to extract necessary information, and generating code to produce the desired
structural drawing in AutoCAD. The approach developed, demonstrated and
evaluated herein enables the efficient and direct conversion of a structural
drawing's natural language description into an AutoCAD drawing, significantly
reducing the workload compared to current working process associated with
manual drawing production, facilitating the typical iterative process of
engineers for expressing design ideas in a simplified way.

</details>


### [115] [Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning](https://arxiv.org/abs/2507.19855)
*Aditya Sharma,Linh Nguyen,Ananya Gupta,Chengyu Wang,Chiamaka Adebayo,Jakub Kowalski*

Main category: cs.LG

TL;DR: CWMI框架通过嵌入因果物理模型提升LLM的物理推理能力，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: LLMs缺乏对物理动态的直观理解，限制了其在需要因果推理的实际场景中的应用。

Method: 提出CWMI框架，包含因果物理模块（CPM）和因果干预损失训练目标，从多模态数据中学习因果关系。

Result: CWMI在零样本物理推理任务（如PIQA和PhysiCa-Bench）中显著优于现有LLMs。

Conclusion: 诱导因果世界模型是构建更可靠、泛化能力更强的AI系统的关键步骤。

Abstract: Large Language Models (LLMs), despite their advanced linguistic capabilities,
fundamentally lack an intuitive understanding of physical dynamics, which
limits their effectiveness in real-world scenarios that require causal
reasoning. In this paper, we introduce Causal World Model Induction (CWMI), a
novel framework designed to embed an explicit model of causal physics within an
LLM. Our approach incorporates a dedicated Causal Physics Module (CPM) and a
new training objective called Causal Intervention Loss, encouraging the model
to learn cause-and-effect relationships from multimodal data. By training the
model to predict the outcomes of hypothetical interventions instead of merely
capturing statistical correlations, CWMI develops a robust internal
representation of physical laws. Experimental results show that CWMI
significantly outperforms state-of-the-art LLMs on zero-shot physical reasoning
tasks, including the PIQA benchmark and our newly proposed PhysiCa-Bench
dataset. These findings demonstrate that inducing a causal world model is a
critical step toward more reliable and generalizable AI systems.

</details>


### [116] [AI-Based Clinical Rule Discovery for NMIBC Recurrence through Tsetlin Machines](https://arxiv.org/abs/2507.19803)
*Saram Abbas,Naeem Soomro,Rishad Shafik,Rakesh Heer,Kabita Adhikari*

Main category: cs.LG

TL;DR: 提出了一种基于Tsetlin Machine（TM）的可解释AI模型，用于预测非肌层浸润性膀胱癌（NMIBC）的复发风险，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 膀胱癌的高复发率和现有临床工具的不可靠性，尤其是对中等风险病例，促使开发更准确且透明的预测工具。

Method: 使用Tsetlin Machine（TM）作为符号学习器，生成可解释的逻辑规则，并在PHOTO试验数据集（n=330）上进行测试。

Result: TM的F1分数为0.80，优于XGBoost（0.78）、逻辑回归（0.60）和EORTC（0.42），并能提供透明的预测依据。

Conclusion: TM是一种准确且透明的决策支持工具，适合实际临床应用。

Abstract: Bladder cancer claims one life every 3 minutes worldwide. Most patients are
diagnosed with non-muscle-invasive bladder cancer (NMIBC), yet up to 70% recur
after treatment, triggering a relentless cycle of surgeries, monitoring, and
risk of progression. Clinical tools like the EORTC risk tables are outdated and
unreliable - especially for intermediate-risk cases.
  We propose an interpretable AI model using the Tsetlin Machine (TM), a
symbolic learner that outputs transparent, human-readable logic. Tested on the
PHOTO trial dataset (n=330), TM achieved an F1-score of 0.80, outperforming
XGBoost (0.78), Logistic Regression (0.60), and EORTC (0.42). TM reveals the
exact clauses behind each prediction, grounded in clinical features like tumour
count, surgeon experience, and hospital stay - offering accuracy and full
transparency. This makes TM a powerful, trustworthy decision-support tool ready
for real-world adoption.

</details>


### [117] [Debunking Optimization Myths in Federated Learning for Medical Image Classification](https://arxiv.org/abs/2507.19822)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 研究发现，在联邦学习中，本地优化器和学习率的选择比具体FL方法对性能影响更大，且本地训练轮数的增加可能对收敛产生正负影响。


<details>
  <summary>Details</summary>
Motivation: 探讨联邦学习在医学影像中因本地配置（如优化器和学习率）敏感性而导致的鲁棒性问题。

Method: 通过基准测试比较不同FL方法在结肠病理和血细胞分类任务中的表现，分析本地配置的影响。

Result: 本地优化器和学习率的选择对性能影响显著，且本地训练轮数的效果因FL方法而异。

Conclusion: 在联邦学习中，合适的本地配置比算法复杂性对效果更为关键。

Abstract: Federated Learning (FL) is a collaborative learning method that enables
decentralized model training while preserving data privacy. Despite its promise
in medical imaging, recent FL methods are often sensitive to local factors such
as optimizers and learning rates, limiting their robustness in practical
deployments. In this work, we revisit vanilla FL to clarify the impact of edge
device configurations, benchmarking recent FL methods on colorectal pathology
and blood cell classification task. We numerically show that the choice of
local optimizer and learning rate has a greater effect on performance than the
specific FL method. Moreover, we find that increasing local training epochs can
either enhance or impair convergence, depending on the FL method. These
findings indicate that appropriate edge-specific configuration is more crucial
than algorithmic complexity for achieving effective FL.

</details>


### [118] [GNSP: Gradient Null Space Projection for Preserving Cross-Modal Alignment in VLMs Continual Learning](https://arxiv.org/abs/2507.19839)
*Tiantian Peng,Yuyang Liu,Shuo Yang,Qiuhe Hong,YongHong Tian*

Main category: cs.LG

TL;DR: 论文提出了一种名为梯度零空间投影（GNSP）的持续学习方法，用于解决CLIP在持续微调中的灾难性遗忘问题，同时保持其零样本能力。


<details>
  <summary>Details</summary>
Motivation: CLIP在持续微调时会遭受灾难性遗忘和嵌入对齐退化，影响其零样本能力。

Method: 提出GNSP方法，将任务特定梯度投影到先前知识的零空间，避免干扰；结合知识蒸馏和模态对齐保护损失，稳定多模态嵌入空间。

Result: 在MTIL基准测试中取得SOTA性能，成功保持CLIP的模态间隙和跨模态检索性能。

Conclusion: GNSP方法有效解决了CLIP持续学习中的问题，保持了其多模态嵌入空间的鲁棒性。

Abstract: Contrastive Language-Image Pretraining has demonstrated remarkable zero-shot
generalization by aligning visual and textual modalities in a shared embedding
space. However, when continuously fine-tuned on diverse tasks, CLIP suffers
from catastrophic forgetting and degradation of its embedding alignment,
undermining its zero-shot capabilities. In this work, we propose Gradient Null
Space Projection (GNSP), an efficient continual learning method that projects
task-specific gradients onto the null space of previously learned knowledge.
This orthogonal projection mathematically prevents interference with previous
tasks without relying on rehearsal or architectural modification. Furthermore,
to preserve the inherent generalization property of CLIP, we introduce
knowledge distillation and combine it with a modality alignment preservation
loss inspired by CLIP pre-training to stabilize the structure of the multimodal
embedding space during fine-tuning. On the MTIL benchmark consisting of 11
tasks, our method achieved SOTA performance on both the Average and Last key
metrics. More importantly, experiments show that our method successfully
maintains the original modality gap and cross-modal retrieval performance of
CLIP, confirming its effectiveness in maintaining a robust visual-language
space throughout the continual learning process.

</details>


### [119] [VAE-GAN Based Price Manipulation in Coordinated Local Energy Markets](https://arxiv.org/abs/2507.19844)
*Biswarup Mukherjee,Li Zhou,S. Gokul Krishnan,Milad Kabirifar,Subhash Lakshminarayana,Charalambos Konstantinou*

Main category: cs.LG

TL;DR: 论文提出了一种基于多智能体深度确定性策略梯度（MADDPG）的模型，用于协调异构分布式能源资源（DERs）的产消者在本地能源市场（LEM）中的实时决策，并研究了价格操纵策略对产消者的影响。


<details>
  <summary>Details</summary>
Motivation: 解决异构DERs产消者在动态能源市场中的协调问题，并探讨价格操纵对市场公平性的影响。

Method: 采用数据驱动的无模型强化学习方法（MADDPG）和VAE-GAN模型进行价格操纵策略研究。

Result: 在对抗性定价下，缺乏发电能力的产消者群体遭受财务损失；市场规模增大时，交易稳定性和公平性通过智能体合作得到改善。

Conclusion: MADDPG框架能有效协调产消者决策，但价格操纵策略揭示了市场公平性挑战，市场规模的扩大有助于提升合作与公平。

Abstract: This paper introduces a model for coordinating prosumers with heterogeneous
distributed energy resources (DERs), participating in the local energy market
(LEM) that interacts with the market-clearing entity. The proposed LEM scheme
utilizes a data-driven, model-free reinforcement learning approach based on the
multi-agent deep deterministic policy gradient (MADDPG) framework, enabling
prosumers to make real-time decisions on whether to buy, sell, or refrain from
any action while facilitating efficient coordination for optimal energy trading
in a dynamic market. In addition, we investigate a price manipulation strategy
using a variational auto encoder-generative adversarial network (VAE-GAN)
model, which allows utilities to adjust price signals in a way that induces
financial losses for the prosumers. Our results show that under adversarial
pricing, heterogeneous prosumer groups, particularly those lacking generation
capabilities, incur financial losses. The same outcome holds across LEMs of
different sizes. As the market size increases, trading stabilizes and fairness
improves through emergent cooperation among agents.

</details>


### [120] [A Scalable and High Availability Solution for Recommending Resolutions to Problem Tickets](https://arxiv.org/abs/2507.19846)
*Harish S,Chetana K Nayak,Joy Bose*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的解决方案，用于解决电信领域问题工单的复杂性和数据问题，结合聚类、监督学习和NLP模型，展示了高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决工单处理中的数据漂移、缺失数据和文本相似性等挑战，提高问题解决的效率和准确性。

Method: 采用聚类、监督学习（LDA、Siamese网络、One-shot学习）和NLP模型，结合实时仪表盘和Kubernetes部署。

Result: 在开源和专有数据集上实现了高预测准确性。

Conclusion: 提出的方法能有效应对工单处理中的复杂问题，具有实际应用价值。

Abstract: Resolution of incidents or problem tickets is a common theme in service
industries in any sector, including billing and charging systems in telecom
domain. Machine learning can help to identify patterns and suggest resolutions
for the problem tickets, based on patterns in the historical data of the
tickets. However, this process may be complicated due to a variety of phenomena
such as data drift and issues such as missing data, lack of data pertaining to
resolutions of past incidents, too many similar sounding resolutions due to
free text and similar sounding text. This paper proposes a robust ML-driven
solution employing clustering, supervised learning, and advanced NLP models to
tackle these challenges effectively. Building on previous work, we demonstrate
clustering-based resolution identification, supervised classification with LDA,
Siamese networks, and One-shot learning, Index embedding. Additionally, we
present a real-time dashboard and a highly available Kubernetes-based
production deployment. Our experiments with both the open-source Bitext
customer-support dataset and proprietary telecom datasets demonstrate high
prediction accuracy.

</details>


### [121] [Agentic Reinforced Policy Optimization](https://arxiv.org/abs/2507.19849)
*Guanting Dong,Hangyu Mao,Kai Ma,Licheng Bao,Yifei Chen,Zhongyuan Wang,Zhongxia Chen,Jiazhen Du,Huiyang Wang,Fuzheng Zhang,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: 论文提出了一种名为ARPO的新型强化学习算法，用于训练多轮LLM代理，通过熵自适应机制和优势估计提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL算法在平衡LLM的长时推理能力和多轮工具交互能力方面表现不足，需要改进。

Method: 提出ARPO算法，结合熵自适应采样机制和优势估计，动态平衡全局轨迹和步级采样。

Result: 在13个基准测试中，ARPO表现优于轨迹级RL算法，且仅需一半工具使用预算。

Conclusion: ARPO为LLM代理在动态环境中的对齐提供了可扩展的解决方案。

Abstract: Large-scale reinforcement learning with verifiable rewards (RLVR) has
demonstrated its effectiveness in harnessing the potential of large language
models (LLMs) for single-turn reasoning tasks. In realistic reasoning
scenarios, LLMs can often utilize external tools to assist in task-solving
processes. However, current RL algorithms inadequately balance the models'
intrinsic long-horizon reasoning capabilities and their proficiency in
multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced
Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training
multi-turn LLM-based agents. Through preliminary experiments, we observe that
LLMs tend to exhibit highly uncertain behavior, characterized by an increase in
the entropy distribution of generated tokens, immediately following
interactions with external tools. Motivated by this observation, ARPO
incorporates an entropy-based adaptive rollout mechanism, dynamically balancing
global trajectory sampling and step-level sampling, thereby promoting
exploration at steps with high uncertainty after tool usage. By integrating an
advantage attribution estimation, ARPO enables LLMs to internalize advantage
differences in stepwise tool-use interactions. Our experiments across 13
challenging benchmarks in computational reasoning, knowledge reasoning, and
deep search domains demonstrate ARPO's superiority over trajectory-level RL
algorithms. Remarkably, ARPO achieves improved performance using only half of
the tool-use budget required by existing methods, offering a scalable solution
for aligning LLM-based agents with real-time dynamic environments. Our code and
datasets are released at https://github.com/dongguanting/ARPO

</details>


### [122] [RestoreAI -- Pattern-based Risk Estimation Of Remaining Explosives](https://arxiv.org/abs/2507.19873)
*Björn Kischelewski,Benjamin Guedj,David Wahl*

Main category: cs.LG

TL;DR: RestoreAI利用地雷空间模式预测风险，提升清除效率，线性与曲线模式性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法主要关注地雷识别，而忽略了基于空间模式的风险预测，影响了清除效率。

Method: 提出RestoreAI系统，包括线性、曲线和贝叶斯模式预测器，分别基于PCA、主曲线和专家知识。

Result: RestoreAI显著提升清除效率，清除率提高14.37%，时间减少24.45%。

Conclusion: 线性与曲线模式性能无显著差异，线性模式可作为高效风险预测选择。

Abstract: Landmine removal is a slow, resource-intensive process affecting over 60
countries. While AI has been proposed to enhance explosive ordnance (EO)
detection, existing methods primarily focus on object recognition, with limited
attention to prediction of landmine risk based on spatial pattern information.
This work aims to answer the following research question: How can AI be used to
predict landmine risk from landmine patterns to improve clearance time
efficiency? To that effect, we introduce RestoreAI, an AI system for
pattern-based risk estimation of remaining explosives. RestoreAI is the first
AI system that leverages landmine patterns for risk prediction, improving the
accuracy of estimating the residual risk of missing EO prior to land release.
We particularly focus on the implementation of three instances of RestoreAI,
respectively, linear, curved and Bayesian pattern deminers. First, the linear
pattern deminer uses linear landmine patterns from a principal component
analysis (PCA) for the landmine risk prediction. Second, the curved pattern
deminer uses curved landmine patterns from principal curves. Finally, the
Bayesian pattern deminer incorporates prior expert knowledge by using a
Bayesian pattern risk prediction. Evaluated on real-world landmine data,
RestoreAI significantly boosts clearance efficiency. The top-performing
pattern-based deminers achieved a 14.37 percentage point increase in the
average share of cleared landmines per timestep and required 24.45% less time
than the best baseline deminer to locate all landmines. Interestingly, linear
and curved pattern deminers showed no significant performance difference,
suggesting that more efficient linear patterns are a viable option for risk
prediction.

</details>


### [123] [CLoRA: Parameter-Efficient Continual Learning with Low-Rank Adaptation](https://arxiv.org/abs/2507.19887)
*Shishir Muralidhara,Didier Stricker,René Schuster*

Main category: cs.LG

TL;DR: CLoRA利用低秩适应（LoRA）方法，提出了一种参数高效的持续学习方案，显著减少计算资源需求，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在计算资源受限的实际场景中应用受限，因需全模型重训练。CLoRA旨在解决这一问题。

Method: 采用低秩适应（LoRA）技术，仅调整少量参数，跨任务共享参数集，实现高效学习。

Result: CLoRA性能与基线方法相当或更优，同时大幅降低硬件需求。

Conclusion: CLoRA为资源受限环境提供高效持续学习方案，NetScore评估凸显其资源效率优势。

Abstract: In the past, continual learning (CL) was mostly concerned with the problem of
catastrophic forgetting in neural networks, that arises when incrementally
learning a sequence of tasks. Current CL methods function within the confines
of limited data access, without any restrictions imposed on computational
resources. However, in real-world scenarios, the latter takes precedence as
deployed systems are often computationally constrained. A major drawback of
most CL methods is the need to retrain the entire model for each new task. The
computational demands of retraining large models can be prohibitive, limiting
the applicability of CL in environments with limited resources. Through CLoRA,
we explore the applicability of Low-Rank Adaptation (LoRA), a
parameter-efficient fine-tuning method for class-incremental semantic
segmentation. CLoRA leverages a small set of parameters of the model and uses
the same set for learning across all tasks. Results demonstrate the efficacy of
CLoRA, achieving performance on par with and exceeding the baseline methods. We
further evaluate CLoRA using NetScore, underscoring the need to factor in
resource efficiency and evaluate CL methods beyond task performance. CLoRA
significantly reduces the hardware requirements for training, making it
well-suited for CL in resource-constrained environments after deployment.

</details>


### [124] [A Survey on Generative Model Unlearning: Fundamentals, Taxonomy, Evaluation, and Future Direction](https://arxiv.org/abs/2507.19894)
*Xiaohua Feng,Jiaming Zhang,Fengyuan Yu,Chengye Wang,Li Zhang,Kaixiang Li,Yuyuan Li,Chaochao Chen,Jianwei Yin*

Main category: cs.LG

TL;DR: 本文综述了生成模型遗忘（GenMU）的研究，提出了统一的分析框架，并探讨了其与相关技术的联系，同时指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，隐私问题日益突出，但现有研究缺乏统一框架，难以公平比较不同方法。

Method: 通过全面综述现有研究，提出分类遗忘目标、方法策略和评估指标的统一框架，并探讨与其他技术的联系。

Result: 提出了GenMU的统一分析框架，明确了其实际应用价值，并指出了未来研究的关键挑战和方向。

Conclusion: 本文为生成模型遗忘领域的研究奠定了基础，并提供了开源资源支持进一步研究。

Abstract: With the rapid advancement of generative models, associated privacy concerns
have attracted growing attention. To address this, researchers have begun
adapting machine unlearning techniques from traditional classification models
to generative settings. Although notable progress has been made in this area, a
unified framework for systematically organizing and integrating existing work
is still lacking. The substantial differences among current studies in terms of
unlearning objectives and evaluation protocols hinder the objective and fair
comparison of various approaches. While some studies focus on specific types of
generative models, they often overlook the commonalities and systematic
characteristics inherent in Generative Model Unlearning (GenMU). To bridge this
gap, we provide a comprehensive review of current research on GenMU and propose
a unified analytical framework for categorizing unlearning objectives,
methodological strategies, and evaluation metrics. In addition, we explore the
connections between GenMU and related techniques, including model editing,
reinforcement learning from human feedback, and controllable generation. We
further highlight the potential practical value of unlearning techniques in
real-world applications. Finally, we identify key challenges and outline future
research directions aimed at laying a solid foundation for further advancements
in this field. We consistently maintain the related open-source materials at
https://github.com/caxLee/Generative-model-unlearning-survey.

</details>


### [125] [Who Owns This Sample: Cross-Client Membership Inference Attack in Federated Graph Neural Networks](https://arxiv.org/abs/2507.19964)
*Kunhao Li,Di Wu,Jun Bai,Jing Xu,Lei Yang,Ziyi Zhang,Yiliao Song,Wencheng Yang,Taotao Cai,Yan Li*

Main category: cs.LG

TL;DR: 研究针对联邦图神经网络（FedGNNs）的跨客户端成员推理攻击（CC-MIA），揭示了一种新的隐私威胁——通过结构和模型线索泄露客户端身份。


<details>
  <summary>Details</summary>
Motivation: 随着图神经网络（GNNs）在联邦学习（FL）中的广泛应用，新的隐私威胁（如跨客户端成员推理攻击）出现，需要系统性研究。

Method: 设计了一个通用攻击框架，利用FedGNNs的聚合行为、梯度更新和嵌入相似性，将样本链接到其源客户端。

Result: 攻击在多种图数据集和实际FL设置下表现优异，成功实现了成员推理和所有权识别。

Conclusion: 研究揭示了联邦图学习中的新隐私风险，呼吁设计具有抗归属性的GNN模型。

Abstract: Graph-structured data is prevalent in many real-world applications, including
social networks, financial systems, and molecular biology. Graph Neural
Networks (GNNs) have become the de facto standard for learning from such data
due to their strong representation capabilities. As GNNs are increasingly
deployed in federated learning (FL) settings to preserve data locality and
privacy, new privacy threats arise from the interaction between graph
structures and decentralized training. In this paper, we present the first
systematic study of cross-client membership inference attacks (CC-MIA) against
node classification tasks of federated GNNs (FedGNNs), where a malicious client
aims to infer which client owns the given data. Unlike prior
centralized-focused work that focuses on whether a sample was included in
training, our attack targets sample-to-client attribution, a finer-grained
privacy risk unique to federated settings. We design a general attack framework
that exploits FedGNNs' aggregation behaviors, gradient updates, and embedding
proximity to link samples to their source clients across training rounds. We
evaluate our attack across multiple graph datasets under realistic FL setups.
Results show that our method achieves high performance on both membership
inference and ownership identification. Our findings highlight a new privacy
threat in federated graph learning-client identity leakage through structural
and model-level cues, motivating the need for attribution-robust GNN design.

</details>


### [126] [Dimer-Enhanced Optimization: A First-Order Approach to Escaping Saddle Points in Neural Network Training](https://arxiv.org/abs/2507.19968)
*Yue Hu,Zanxia Cao,Yingchao Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为Dimer-Enhanced Optimization (DEO)的新方法，通过利用梯度信息估计曲率，帮助优化器逃离鞍点和平坦区域，提升神经网络训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统的一阶优化方法（如SGD和Adam）在复杂损失曲面上表现不佳，而二阶方法计算成本过高。DEO旨在通过物理启发的曲率估计方法解决这一问题。

Method: DEO基于Dimer方法，通过构造两个邻近点估计局部曲率，近似Hessian矩阵的最小特征向量，并周期性调整梯度方向以逃离鞍点。

Result: 在Transformer玩具模型上的初步实验表明，DEO在复杂损失曲面上的表现优于标准一阶方法。

Conclusion: DEO通过物理启发的曲率估计方法，为高维神经网络训练提供了一种高效的一阶优化框架。

Abstract: First-order optimization methods, such as SGD and Adam, are widely used for
training large-scale deep neural networks due to their computational efficiency
and robust performance. However, relying solely on gradient information, these
methods often struggle to navigate complex loss landscapes with flat regions,
plateaus, and saddle points. Second-order methods, which use curvature
information from the Hessian matrix, can address these challenges but are
computationally infeasible for large models. The Dimer method, a first-order
technique that constructs two closely spaced points to probe the local geometry
of a potential energy surface, efficiently estimates curvature using only
gradient information. Inspired by its use in molecular dynamics simulations for
locating saddle points, we propose Dimer-Enhanced Optimization (DEO), a novel
framework to escape saddle points in neural network training. DEO adapts the
Dimer method to explore a broader region of the loss landscape, approximating
the Hessian's smallest eigenvector without computing the full matrix. By
periodically projecting the gradient onto the subspace orthogonal to the
minimum curvature direction, DEO guides the optimizer away from saddle points
and flat regions, enhancing training efficiency with non-stepwise updates.
Preliminary experiments on a Transformer toy model show DEO achieves
competitive performance compared to standard first-order methods, improving
navigation of complex loss landscapes. Our work repurposes physics-inspired,
first-order curvature estimation to enhance neural network training in
high-dimensional spaces.

</details>


### [127] [Robust Taxi Fare Prediction Under Noisy Conditions: A Comparative Study of GAT, TimesNet, and XGBoost](https://arxiv.org/abs/2507.20008)
*Padmavathi Moorthy*

Main category: cs.LG

TL;DR: 研究比较了GAT、XGBoost和TimesNet三种模型在出租车费用预测中的表现，分析了数据质量和预处理策略的影响，并提供了实际应用中的建议。


<details>
  <summary>Details</summary>
Motivation: 精确的费用预测对网约车平台和城市交通系统至关重要，但现有模型在数据质量和实际条件下的表现差异尚不明确。

Method: 使用包含5500万条记录的实时数据集，评估GAT、XGBoost和TimesNet在预测准确性、校准、不确定性估计、OOD鲁棒性和特征敏感性等方面的表现。

Result: 研究发现深度学习模型和经典模型在实际条件下表现存在显著差异，数据预处理策略对模型性能有重要影响。

Conclusion: 研究为构建鲁棒且可扩展的城市费用预测系统提供了实用指南，强调了数据质量和模型选择的重要性。

Abstract: Precise fare prediction is crucial in ride-hailing platforms and urban
mobility systems. This study examines three machine learning models-Graph
Attention Networks (GAT), XGBoost, and TimesNet to evaluate their predictive
capabilities for taxi fares using a real-world dataset comprising over 55
million records. Both raw (noisy) and denoised versions of the dataset are
analyzed to assess the impact of data quality on model performance. The study
evaluated the models along multiple axes, including predictive accuracy,
calibration, uncertainty estimation, out-of-distribution (OOD) robustness, and
feature sensitivity. We also explore pre-processing strategies, including KNN
imputation, Gaussian noise injection, and autoencoder-based denoising. The
study reveals critical differences between classical and deep learning models
under realistic conditions, offering practical guidelines for building robust
and scalable models in urban fare prediction systems.

</details>


### [128] [FedSWA: Improving Generalization in Federated Learning with Highly Heterogeneous Data via Momentum-Based Stochastic Controlled Weight Averaging](https://arxiv.org/abs/2507.20016)
*Liu junkang,Yuanyuan Liu,Fanhua Shang,Hongying Liu,Jin Liu,Wei Feng*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedSWA的新型联邦学习算法，用于解决数据高度异构情况下FedSAM表现不佳的问题，并进一步提出了改进版FedMoSWA。理论分析和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 研究联邦学习（FL）在数据高度异构情况下的泛化问题，发现FedSAM在此情况下表现不如FedAvg，因此提出改进算法。

Method: 提出FedSWA算法，利用随机权重平均（SWA）寻找更平坦的最小值；进一步提出FedMoSWA，结合动量机制优化模型对齐。

Result: 理论分析表明FedMoSWA的优化和泛化误差更小；实验在CIFAR10/100和Tiny ImageNet上验证了算法的优越性。

Conclusion: FedSWA和FedMoSWA在数据高度异构情况下表现优于现有算法，具有理论和实践价值。

Abstract: For federated learning (FL) algorithms such as FedSAM, their generalization
capability is crucial for real-word applications. In this paper, we revisit the
generalization problem in FL and investigate the impact of data heterogeneity
on FL generalization. We find that FedSAM usually performs worse than FedAvg in
the case of highly heterogeneous data, and thus propose a novel and effective
federated learning algorithm with Stochastic Weight Averaging (called
\texttt{FedSWA}), which aims to find flatter minima in the setting of highly
heterogeneous data. Moreover, we introduce a new momentum-based stochastic
controlled weight averaging FL algorithm (\texttt{FedMoSWA}), which is designed
to better align local and global models.
  Theoretically, we provide both convergence analysis and generalization bounds
for \texttt{FedSWA} and \texttt{FedMoSWA}. We also prove that the optimization
and generalization errors of \texttt{FedMoSWA} are smaller than those of their
counterparts, including FedSAM and its variants. Empirically, experimental
results on CIFAR10/100 and Tiny ImageNet demonstrate the superiority of the
proposed algorithms compared to their counterparts. Open source code at:
https://github.com/junkangLiu0/FedSWA.

</details>


### [129] [Irredundant $k$-Fold Cross-Validation](https://arxiv.org/abs/2507.20048)
*Jesus S. Aguilar-Ruiz*

Main category: cs.LG

TL;DR: 提出了一种新的Irredundant k-fold交叉验证方法，确保每个实例仅用于一次训练和一次测试，减少冗余和过拟合，同时保持分层和模型无关性。


<details>
  <summary>Details</summary>
Motivation: 传统k-fold交叉验证中实例重复使用可能导致不平衡和过拟合，需要一种更平衡的方法。

Method: 引入Irredundant k-fold交叉验证，每个实例仅用于一次训练和一次测试，保持分层和模型无关性。

Result: 实验结果显示该方法性能与传统k-fold相当，但方差估计更准确，计算成本更低。

Conclusion: Irredundant k-fold交叉验证是一种更高效、平衡且减少过拟合的验证方法。

Abstract: In traditional k-fold cross-validation, each instance is used ($k-1$) times
for training and once for testing, leading to redundancy that lets many
instances disproportionately influence the learning phase. We introduce
Irredundant $k$-fold cross-validation, a novel method that guarantees each
instance is used exactly once for training and once for testing across the
entire validation procedure. This approach ensures a more balanced utilization
of the dataset, mitigates overfitting due to instance repetition, and enables
sharper distinctions in comparative model analysis. The method preserves
stratification and remains model-agnostic, i.e., compatible with any
classifier. Experimental results demonstrate that it delivers consistent
performance estimates across diverse datasets -- comparable to $k$-fold
cross-validation -- while providing less optimistic variance estimates because
training partitions are non-overlapping, and significantly reducing the overall
computational cost.

</details>


### [130] [$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning](https://arxiv.org/abs/2507.20051)
*Weicong Chen,Vikash Singh,Zahra Rahmani,Debargha Ganguly,Mohsen Hariri,Vipin Chaudhary*

Main category: cs.LG

TL;DR: $K^4$ 是一种无监督、独立于解析器的日志异常检测框架，通过高效 k-NN 统计将日志嵌入转换为四维描述符，实现高性能在线检测。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法速度慢、依赖易出错的解析且评估协议不现实。

Method: 使用 k-NN 统计将日志嵌入转换为四维描述符（精度、召回率、密度、覆盖率），支持轻量级检测器无需重新训练即可准确评分。

Result: $K^4$ 在更现实的在线评估协议下，AUROC 达到 0.995-0.999，显著优于基线方法，且速度快（训练<4秒，推理低至4μs）。

Conclusion: $K^4$ 是一种高效、快速的日志异常检测框架，性能优越且适用于在线场景。

Abstract: Existing Log Anomaly Detection (LogAD) methods are often slow, dependent on
error-prone parsing, and use unrealistic evaluation protocols. We introduce
$K^4$, an unsupervised and parser-independent framework for high-performance
online detection. $K^4$ transforms arbitrary log embeddings into compact
four-dimensional descriptors (Precision, Recall, Density, Coverage) using
efficient k-nearest neighbor (k-NN) statistics. These descriptors enable
lightweight detectors to accurately score anomalies without retraining. Using a
more realistic online evaluation protocol, $K^4$ sets a new state-of-the-art
(AUROC: 0.995-0.999), outperforming baselines by large margins while being
orders of magnitude faster, with training under 4 seconds and inference as low
as 4 $\mu$s.

</details>


### [131] [What Can Grokking Teach Us About Learning Under Nonstationarity?](https://arxiv.org/abs/2507.20057)
*Clare Lyle,Gharda Sokar,Razvan Pascanu,Andras Gyorgy*

Main category: cs.LG

TL;DR: 论文探讨了持续学习中神经网络的特征学习动态如何帮助克服早期数据偏见（primacy bias），并提出了一种通过调整有效学习率来促进特征学习的方法。


<details>
  <summary>Details</summary>
Motivation: 在持续学习中，神经网络容易因早期训练数据而产生偏见（primacy bias），影响后续任务的泛化能力。本文假设特征学习动态（如grokking现象中的动态）可以解决这一问题。

Method: 提出通过增加有效学习率（参数与更新范数的比值）来诱导特征学习动态，从而在训练中灵活调整。

Result: 该方法在多种场景（如grokking、神经网络热启动和强化学习任务）中促进了特征学习并提高了泛化能力。

Conclusion: 特征学习动态是解决持续学习中primacy bias的关键，调整有效学习率是一种简单有效的方法。

Abstract: In continual learning problems, it is often necessary to overwrite components
of a neural network's learned representation in response to changes in the data
stream; however, neural networks often exhibit \primacy bias, whereby early
training data hinders the network's ability to generalize on later tasks. While
feature-learning dynamics of nonstationary learning problems are not well
studied, the emergence of feature-learning dynamics is known to drive the
phenomenon of grokking, wherein neural networks initially memorize their
training data and only later exhibit perfect generalization. This work
conjectures that the same feature-learning dynamics which facilitate
generalization in grokking also underlie the ability to overwrite previous
learned features as well, and methods which accelerate grokking by facilitating
feature-learning dynamics are promising candidates for addressing primacy bias
in non-stationary learning problems. We then propose a straightforward method
to induce feature-learning dynamics as needed throughout training by increasing
the effective learning rate, i.e. the ratio between parameter and update norms.
We show that this approach both facilitates feature-learning and improves
generalization in a variety of settings, including grokking, warm-starting
neural network training, and reinforcement learning tasks.

</details>


### [132] [ModShift: Model Privacy via Designed Shifts](https://arxiv.org/abs/2507.20060)
*Nomaan A. Kherani,Urbashi Mitra*

Main category: cs.LG

TL;DR: 提出了一种通过参数偏移保护联邦学习中模型隐私的方法，利用Fisher信息矩阵使窃听者难以估计模型参数，同时通过安全共享偏移保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，防止窃听者获取模型隐私是一个重要问题。本文旨在通过参数偏移技术保护模型隐私，同时不影响模型准确性。

Method: 将模型学习视为参数估计问题，通过推导Fisher信息矩阵并使其奇异化，使窃听者难以估计模型参数。偏移安全共享至中央服务器以保持模型准确性。

Result: 数值结果表明，该方法比噪声注入方案能实现更高的模型偏移，且需要更少的带宽秘密通道。

Conclusion: 提出的方案能有效保护模型隐私，同时通过收敛测试验证了其安全性。

Abstract: In this paper, shifts are introduced to preserve model privacy against an
eavesdropper in federated learning. Model learning is treated as a parameter
estimation problem. This perspective allows us to derive the Fisher Information
matrix of the model updates from the shifted updates and drive them to
singularity, thus posing a hard estimation problem for Eve. The shifts are
securely shared with the central server to maintain model accuracy at the
server and participating devices. A convergence test is proposed to detect if
model updates have been tampered with and we show that our scheme passes this
test. Numerical results show that our scheme achieves a higher model shift when
compared to a noise injection scheme while requiring a lesser bandwidth secret
channel.

</details>


### [133] [Strategic Filtering for Content Moderation: Free Speech or Free of Distortion?](https://arxiv.org/abs/2507.20061)
*Saba Ahmadi,Avrim Blum,Haifeng Xu,Fan Yao*

Main category: cs.LG

TL;DR: 论文探讨了社交媒体上用户生成内容（UGC）的监管问题，提出通过机制设计优化言论自由与社会扭曲之间的平衡，并提供了近似最优解的实用方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的UGC容易受到煽动和操纵，需要有效监管，但自动化内容审核会引发用户的策略性回应，需平衡言论自由与社会扭曲。

Method: 通过机制设计优化言论自由与社会扭曲的权衡，提出近似最优解的实用方法，并给出有限离线数据所需的泛化保证。

Result: 尽管最优权衡问题是NP难的，但提出了有效的近似方法，并确定了所需离线数据的量。

Conclusion: 研究为社交媒体内容审核提供了理论支持，优化了言论自由与社会扭曲的平衡，并提供了实用解决方案。

Abstract: User-generated content (UGC) on social media platforms is vulnerable to
incitements and manipulations, necessitating effective regulations. To address
these challenges, those platforms often deploy automated content moderators
tasked with evaluating the harmfulness of UGC and filtering out content that
violates established guidelines. However, such moderation inevitably gives rise
to strategic responses from users, who strive to express themselves within the
confines of guidelines. Such phenomena call for a careful balance between: 1.
ensuring freedom of speech -- by minimizing the restriction of expression; and
2. reducing social distortion -- measured by the total amount of content
manipulation. We tackle the problem of optimizing this balance through the lens
of mechanism design, aiming at optimizing the trade-off between minimizing
social distortion and maximizing free speech. Although determining the optimal
trade-off is NP-hard, we propose practical methods to approximate the optimal
solution. Additionally, we provide generalization guarantees determining the
amount of finite offline data required to approximate the optimal moderator
effectively.

</details>


### [134] [Geometric Operator Learning with Optimal Transport](https://arxiv.org/abs/2507.20065)
*Xinyi Li,Zongyi Li,Nikola Kovachki,Anima Anandkumar*

Main category: cs.LG

TL;DR: 将最优传输（OT）融入偏微分方程（PDE）的算子学习中，通过OT问题将网格密度函数映射到参考空间，提升计算效率与灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统几何学习方法依赖网格或点云表示，缺乏灵活性；OT方法通过实例依赖变形提供更优解决方案。

Method: 将几何嵌入问题转化为OT问题，将网格密度函数映射到均匀密度的参考空间，并在2D参数化潜在空间中进行计算。

Result: 在RANS方程实验中，方法在精度和计算效率（时间与内存）上优于现有模型，并在FlowBench数据集上表现更优。

Conclusion: OT-based方法在复杂几何PDE学习中具有显著优势，尤其在几何变化大的数据集中表现突出。

Abstract: We propose integrating optimal transport (OT) into operator learning for
partial differential equations (PDEs) on complex geometries. Classical
geometric learning methods typically represent domains as meshes, graphs, or
point clouds. Our approach generalizes discretized meshes to mesh density
functions, formulating geometry embedding as an OT problem that maps these
functions to a uniform density in a reference space. Compared to previous
methods relying on interpolation or shared deformation, our OT-based method
employs instance-dependent deformation, offering enhanced flexibility and
effectiveness. For 3D simulations focused on surfaces, our OT-based neural
operator embeds the surface geometry into a 2D parameterized latent space. By
performing computations directly on this 2D representation of the surface
manifold, it achieves significant computational efficiency gains compared to
volumetric simulation. Experiments with Reynolds-averaged Navier-Stokes
equations (RANS) on the ShapeNet-Car and DrivAerNet-Car datasets show that our
method achieves better accuracy and also reduces computational expenses in
terms of both time and memory usage compared to existing machine learning
models. Additionally, our model demonstrates significantly improved accuracy on
the FlowBench dataset, underscoring the benefits of employing
instance-dependent deformation for datasets with highly variable geometries.

</details>


### [135] [PERRY: Policy Evaluation with Confidence Intervals using Auxiliary Data](https://arxiv.org/abs/2507.20068)
*Aishwarya Mandyam,Jason Meng,Ge Gao,Jiankai Sun,Mac Schwager,Barbara E. Engelhardt,Emma Brunskill*

Main category: cs.LG

TL;DR: 论文提出了两种方法，用于在使用数据增强时构建有效的置信区间，以改进强化学习中的离策略评估（OPE）。这些方法在多个领域（如机器人、医疗和库存管理）中验证有效。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，离策略评估（OPE）需要准确估计新策略的价值，尤其是在高风险领域（如医疗）中。现有方法缺乏对数据增强带来的偏差的量化不确定性，因此需要更可靠的方法。

Method: 提出了两种方法：1) 针对特定初始状态的策略性能置信区间，采用新的高维状态MDP的保形预测方法；2) 针对多初始状态的平均策略性能估计，结合双重稳健估计和预测驱动的推理。

Result: 在机器人、医疗和库存管理的模拟器以及MIMIC-IV真实医疗数据上，新方法能够利用增强数据并覆盖真实值，优于现有方法。

Conclusion: 新方法为使用数据增强的OPE提供了可靠的置信区间，适用于高风险领域，并验证了其有效性。

Abstract: Off-policy evaluation (OPE) methods aim to estimate the value of a new
reinforcement learning (RL) policy prior to deployment. Recent advances have
shown that leveraging auxiliary datasets, such as those synthesized by
generative models, can improve the accuracy of these value estimates.
Unfortunately, such auxiliary datasets may also be biased, and existing methods
for using data augmentation for OPE in RL lack principled uncertainty
quantification. In high stakes settings like healthcare, reliable uncertainty
estimates are important for comparing policy value estimates. In this work, we
propose two approaches to construct valid confidence intervals for OPE when
using data augmentation. The first provides a confidence interval over the
policy performance conditioned on a particular initial state $V^{\pi}(s_0)$--
such intervals are particularly important for human-centered applications. To
do so we introduce a new conformal prediction method for high dimensional state
MDPs. Second, we consider the more common task of estimating the average policy
performance over many initial states; to do so we draw on ideas from doubly
robust estimation and prediction powered inference. Across simulators spanning
robotics, healthcare and inventory management, and a real healthcare dataset
from MIMIC-IV, we find that our methods can use augmented data and still
consistently produce intervals that cover the ground truth values, unlike
previously proposed methods.

</details>


### [136] [Sparse Equation Matching: A Derivative-Free Learning for General-Order Dynamical Systems](https://arxiv.org/abs/2507.20072)
*Jiaqiang Li,Jianbin Tan,Xueqin Wang*

Main category: cs.LG

TL;DR: 提出了一种名为稀疏方程匹配（SEM）的统一框架，用于无导数估计一般阶动力系统中的微分算子及其驱动函数，并在脑电图数据中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖精确的导数估计且仅适用于一阶动力系统，限制了其在实际场景中的应用。

Method: SEM引入了基于格林函数的积分稀疏回归方法，实现了无导数估计。

Result: SEM在模拟中表现优异，并在脑电图数据分析中识别了活跃脑区和任务特异性连接模式。

Conclusion: SEM为复杂系统的动力学建模提供了新工具，尤其在脑连接分析中具有重要价值。

Abstract: Equation discovery is a fundamental learning task for uncovering the
underlying dynamics of complex systems, with wide-ranging applications in areas
such as brain connectivity analysis, climate modeling, gene regulation, and
physical system simulation. However, many existing approaches rely on accurate
derivative estimation and are limited to first-order dynamical systems,
restricting their applicability to real-world scenarios. In this work, we
propose sparse equation matching (SEM), a unified framework that encompasses
several existing equation discovery methods under a common formulation. SEM
introduces an integral-based sparse regression method using Green's functions,
enabling derivative-free estimation of differential operators and their
associated driving functions in general-order dynamical systems. The
effectiveness of SEM is demonstrated through extensive simulations,
benchmarking its performance against derivative-based approaches. We then apply
SEM to electroencephalographic (EEG) data recorded during multiple oculomotor
tasks, collected from 52 participants in a brain-computer interface experiment.
Our method identifies active brain regions across participants and reveals
task-specific connectivity patterns. These findings offer valuable insights
into brain connectivity and the underlying neural mechanisms.

</details>


### [137] [Cluster Purge Loss: Structuring Transformer Embeddings for Equivalent Mutants Detection](https://arxiv.org/abs/2507.20078)
*Adelaide Danilov,Aria Nourbakhsh,Christoph Schommer*

Main category: cs.LG

TL;DR: 提出了一种结合交叉熵损失和深度度量学习目标的新框架，用于优化代码嵌入空间，提升等效代码突变检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在微调预训练模型时未能充分结构化嵌入空间以反映类内语义关系，影响等效代码突变检测任务的表现。

Method: 提出Cluster Purge Loss，动态调整类内实例的边界，结合UniXCoder作为基础模型。

Result: 在等效突变检测领域达到最先进性能，并生成更可解释的嵌入空间。

Conclusion: 新框架通过优化嵌入空间结构，显著提升了模型在语义敏感任务中的表现。

Abstract: Recent pre-trained transformer models achieve superior performance in various
code processing objectives. However, although effective at optimizing decision
boundaries, common approaches for fine-tuning them for downstream
classification tasks - distance-based methods or training an additional
classification head - often fail to thoroughly structure the embedding space to
reflect nuanced intra-class semantic relationships. Equivalent code mutant
detection is one of these tasks, where the quality of the embedding space is
crucial to the performance of the models. We introduce a novel framework that
integrates cross-entropy loss with a deep metric learning objective, termed
Cluster Purge Loss. This objective, unlike conventional approaches,
concentrates on adjusting fine-grained differences within each class,
encouraging the separation of instances based on semantical equivalency to the
class center using dynamically adjusted borders. Employing UniXCoder as the
base model, our approach demonstrates state-of-the-art performance in the
domain of equivalent mutant detection and produces a more interpretable
embedding space.

</details>


### [138] [Feed-anywhere ANN (I) Steady Discrete $\to$ Diffusing on Graph Hidden States](https://arxiv.org/abs/2507.20088)
*Dmitry Pasechnyuk-Vilensky,Daniil Doroshenko*

Main category: cs.LG

TL;DR: 提出了一种基于几何分析和非线性动力学学习隐藏图结构的新框架，包括离散Sobolev空间定义、非线性动力学引入和随机梯度算法开发。


<details>
  <summary>Details</summary>
Motivation: 从数据中学习隐藏的图结构，并确保拓扑正确性和度量收敛性。

Method: 定义离散Sobolev空间，引入非线性动力学，开发随机梯度算法。

Result: 模型在泛化性能上优于标准神经网络，复杂度依赖于数据流形的拓扑结构。

Conclusion: 该框架在拓扑正确性和泛化性能上表现出色，适用于复杂数据流形。

Abstract: We propose a novel framework for learning hidden graph structures from data
using geometric analysis and nonlinear dynamics. Our approach: (1) Defines
discrete Sobolev spaces on graphs for scalar/vector fields, establishing key
functional properties; (2) Introduces gauge-equivalent nonlinear Schr\"odinger
and Landau--Lifshitz dynamics with provable stable stationary solutions
smoothly dependent on input data and graph weights; (3) Develops a stochastic
gradient algorithm over graph moduli spaces with sparsity regularization.
Theoretically, we guarantee: topological correctness (homology recovery),
metric convergence (Gromov--Hausdorff), and efficient search space utilization.
Our dynamics-based model achieves stronger generalization bounds than standard
neural networks, with complexity dependent on the data manifold's topology.

</details>


### [139] [Meta Fusion: A Unified Framework For Multimodality Fusion with Mutual Learning](https://arxiv.org/abs/2507.20089)
*Ziyi Liang,Annie Qu,Babak Shahbaba*

Main category: cs.LG

TL;DR: Meta Fusion是一种灵活的多模态数据融合框架，统一了传统融合方法，并通过软信息共享提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态数据融合在自动驾驶和医疗诊断等领域至关重要，但传统方法各有局限性。

Method: Meta Fusion基于深度互学习和集成学习，构建多模态潜在表示组合的模型群，并通过软信息共享优化性能。

Result: 理论证明Meta Fusion降低泛化误差，实验表明其在模拟和实际应用中优于传统方法。

Conclusion: Meta Fusion是一种模型无关的通用框架，适用于多模态数据融合任务。

Abstract: Developing effective multimodal data fusion strategies has become
increasingly essential for improving the predictive power of statistical
machine learning methods across a wide range of applications, from autonomous
driving to medical diagnosis. Traditional fusion methods, including early,
intermediate, and late fusion, integrate data at different stages, each
offering distinct advantages and limitations. In this paper, we introduce Meta
Fusion, a flexible and principled framework that unifies these existing
strategies as special cases. Motivated by deep mutual learning and ensemble
learning, Meta Fusion constructs a cohort of models based on various
combinations of latent representations across modalities, and further boosts
predictive performance through soft information sharing within the cohort. Our
approach is model-agnostic in learning the latent representations, allowing it
to flexibly adapt to the unique characteristics of each modality.
Theoretically, our soft information sharing mechanism reduces the
generalization error. Empirically, Meta Fusion consistently outperforms
conventional fusion strategies in extensive simulation studies. We further
validate our approach on real-world applications, including Alzheimer's disease
detection and neural decoding.

</details>


### [140] [EcoTransformer: Attention without Multiplication](https://arxiv.org/abs/2507.20096)
*Xin Gao,Xingming Xu*

Main category: cs.LG

TL;DR: EcoTransformer提出了一种基于卷积和L1距离的新型注意力机制，替代传统的点积注意力，显著降低计算能耗，同时在多个任务中表现相当或更优。


<details>
  <summary>Details</summary>
Motivation: 传统的Transformer的点积注意力机制计算密集且能耗高，需要一种更高效的替代方案。

Method: 使用Laplacian核卷积和L1距离构建注意力分数，避免矩阵乘法。

Result: 在NLP、生物信息学和视觉任务中表现与点积注意力相当或更好，同时能耗显著降低。

Conclusion: EcoTransformer是一种高效且性能优越的Transformer变体，适合能耗敏感的应用场景。

Abstract: The Transformer, with its scaled dot-product attention mechanism, has become
a foundational architecture in modern AI. However, this mechanism is
computationally intensive and incurs substantial energy costs. We propose a new
Transformer architecture EcoTransformer, in which the output context vector is
constructed as the convolution of the values using a Laplacian kernel, where
the distances are measured by the L1 metric between the queries and keys.
Compared to dot-product based attention, the new attention score calculation is
free of matrix multiplication. It performs on par with, or even surpasses,
scaled dot-product attention in NLP, bioinformatics, and vision tasks, while
consuming significantly less energy.

</details>


### [141] [Graded Transformers: A Symbolic-Geometric Approach to Structured Learning](https://arxiv.org/abs/2507.20108)
*Tony Shaska Sr*

Main category: cs.LG

TL;DR: 提出了Graded Transformer框架，通过代数归纳偏置增强序列模型，包括LGT和EGT两种架构，理论保证包括通用逼近定理和对抗鲁棒性，适用于多种领域。


<details>
  <summary>Details</summary>
Motivation: 解决结构化数据的高效建模问题，融合几何与代数原理，提升模型的解释性和效率。

Method: 提出Linearly Graded Transformer (LGT)和Exponentially Graded Transformer (EGT)，通过分级变换注入层次结构。

Result: 理论保证包括通用逼近、样本复杂度降低、对抗鲁棒性，并在多个领域展示应用潜力。

Conclusion: Graded Transformer为结构化深度学习提供了数学基础，推动可解释高效系统的发展。

Abstract: We introduce the Graded Transformer framework, a novel class of sequence
models that embeds algebraic inductive biases through grading transformations
on vector spaces. Extending the theory of Graded Neural Networks (GNNs), we
propose two architectures: the Linearly Graded Transformer (LGT) and the
Exponentially Graded Transformer (EGT). These models apply parameterized
scaling operators-governed by fixed or learnable grading tuples and, for EGT,
exponential factors to infuse hierarchical structure into attention and
representation layers, enhancing efficiency for structured data.
  We derive rigorous theoretical guarantees, including universal approximation
theorems for continuous and Sobolev functions, reduced sample complexity via
effective VC dimension bounds, Lipschitz continuity of graded operations, and
robustness to adversarial perturbations. A graded loss function ensures
gradient stability and alignment with domain priors during optimization. By
treating grades as differentiable parameters, the framework enables adaptive
feature prioritization, overcoming limitations of fixed grades in prior work.
  The Graded Transformer holds transformative potential for hierarchical
learning and neurosymbolic reasoning, with applications spanning algebraic
geometry (e.g., moduli spaces and zeta functions), physics (e.g., multiscale
simulations), natural language processing (e.g., syntactic parsing), biological
sequence analysis (e.g., variant prediction), and emerging areas like graph
neural networks and financial modeling. This work advances structured deep
learning by fusing geometric and algebraic principles with attention
mechanisms, offering a mathematically grounded alternative to data-driven
models and paving the way for interpretable, efficient systems in complex
domains.

</details>


### [142] [Online Learning with Probing for Sequential User-Centric Selection](https://arxiv.org/abs/2507.20112)
*Tianyi Xu,Yiting Chen,Henger Li,Zheyong Bian,Emiliano Dall'Anese,Zizhan Zheng*

Main category: cs.LG

TL;DR: 论文提出了PUCS框架，用于解决信息获取的序列决策问题，包括离线贪婪探测算法和在线OLPA算法，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决资源与回报未知且探测成本高的应用问题（如拼车、无线调度等）。

Method: 离线使用贪婪探测算法，在线使用OLPA算法。

Result: 离线算法有常数近似保证，在线算法达到$\mathcal{O}(\sqrt{T} + \ln^{2} T)$的遗憾界，且下界证明其紧性。

Conclusion: PUCS框架及算法在实际应用中表现有效。

Abstract: We formalize sequential decision-making with information acquisition as the
probing-augmented user-centric selection (PUCS) framework, where a learner
first probes a subset of arms to obtain side information on resources and
rewards, and then assigns $K$ plays to $M$ arms. PUCS covers applications such
as ridesharing, wireless scheduling, and content recommendation, in which both
resources and payoffs are initially unknown and probing is costly. For the
offline setting with known distributions, we present a greedy probing algorithm
with a constant-factor approximation guarantee $\zeta = (e-1)/(2e-1)$. For the
online setting with unknown distributions, we introduce OLPA, a stochastic
combinatorial bandit algorithm that achieves a regret bound
$\mathcal{O}(\sqrt{T} + \ln^{2} T)$. We also prove a lower bound
$\Omega(\sqrt{T})$, showing that the upper bound is tight up to logarithmic
factors. Experiments on real-world data demonstrate the effectiveness of our
solutions.

</details>


### [143] [Wine Characterisation with Spectral Information and Predictive Artificial Intelligence](https://arxiv.org/abs/2507.20114)
*Jianping Yao,Son N. Tran,Hieu Nguyen,Samantha Sawyer,Rocco Longo*

Main category: cs.LG

TL;DR: 利用紫外-可见光谱和机器学习预测葡萄汁属性及葡萄酒产地分类，SVM表现最佳，准确率超91%。


<details>
  <summary>Details</summary>
Motivation: 改进传统葡萄酒分析方法，结合光谱和AI技术，解决复杂传感器问题。

Method: 结合紫外-可见光谱和机器学习（如SVM），通过光谱指纹技术分析数据。

Result: SVM在属性和产地预测中表现最优，准确率和F1分数均超91%。

Conclusion: 为葡萄酒行业整合大数据和物联网提供新思路，推动智能酒庄发展。

Abstract: The purpose of this paper is to use absorbance data obtained by human tasting
and an ultraviolet-visible (UV-Vis) scanning spectrophotometer to predict the
attributes of grape juice (GJ) and to classify the wine's origin, respectively.
The approach combined machine learning (ML) techniques with spectroscopy to
find a relatively simple way to apply them in two stages of winemaking and help
improve the traditional wine analysis methods regarding sensory data and wine's
origins. This new technique has overcome the disadvantages of the complex
sensors by taking advantage of spectral fingerprinting technology and forming a
comprehensive study of the employment of AI in the wine analysis domain. In the
results, Support Vector Machine (SVM) was the most efficient and robust in both
attributes and origin prediction tasks. Both the accuracy and F1 score of the
origin prediction exceed 91%. The feature ranking approach found that the more
influential wavelengths usually appear at the lower end of the scan range, 250
nm (nanometers) to 420 nm, which is believed to be of great help for selecting
appropriate validation methods and sensors to extract wine data in future
research. The knowledge of this research provides new ideas and early solutions
for the wine industry or other beverage industries to integrate big data and
IoT in the future, which significantly promotes the development of 'Smart
Wineries'.

</details>


### [144] [Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing](https://arxiv.org/abs/2507.20127)
*Xuanting Xie,Bingheng Li,Erlin Pan,Zhao Kang,Wenyu Chen*

Main category: cs.LG

TL;DR: 提出了一种名为AMLP的无监督框架，通过使MLP适应聚合操作来改进图神经网络的性能，解决了传统GNN固定聚合函数的问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNN使用固定的聚合函数（如Mean、Max、Sum）缺乏理论依据，尤其在异质性图中表现不佳。现有方法依赖大量标注数据，而实际任务中标注数据稀缺。

Method: 提出AMLP框架，包括两个关键步骤：1）利用图重构方法实现高阶分组效果；2）使用单层网络编码不同程度的异质性。

Result: 在节点聚类和分类任务上的实验表明，AMLP性能优越。

Conclusion: AMLP为多样化的图学习场景提供了潜力，且无需依赖大量标注数据。

Abstract: Graph Neural Networks (GNNs) have become a dominant approach to learning
graph representations, primarily because of their message-passing mechanisms.
However, GNNs typically adopt a fixed aggregator function such as Mean, Max, or
Sum without principled reasoning behind the selection. This rigidity,
especially in the presence of heterophily, often leads to poor, problem
dependent performance. Although some attempts address this by designing more
sophisticated aggregation functions, these methods tend to rely heavily on
labeled data, which is often scarce in real-world tasks. In this work, we
propose a novel unsupervised framework, "Aggregation-aware Multilayer
Perceptron" (AMLP), which shifts the paradigm from directly crafting
aggregation functions to making MLP adaptive to aggregation. Our lightweight
approach consists of two key steps: First, we utilize a graph reconstruction
method that facilitates high-order grouping effects, and second, we employ a
single-layer network to encode varying degrees of heterophily, thereby
improving the capacity and applicability of the model. Extensive experiments on
node clustering and classification demonstrate the superior performance of
AMLP, highlighting its potential for diverse graph learning scenarios.

</details>


### [145] [Generative molecule evolution using 3D pharmacophore for efficient Structure-Based Drug Design](https://arxiv.org/abs/2507.20130)
*Yi He,Ailun Wang,Zhi Wang,Yu Liu,Xingyuan Xu,Wen Yan*

Main category: cs.LG

TL;DR: MEVO是一个进化框架，通过结合VQ-VAE、扩散模型和进化策略，解决了结构药物设计（SBDD）中数据稀缺的问题，成功生成了高亲和力配体。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在结构药物设计中的应用受限于数据稀缺，MEVO旨在填补小分子数据集与蛋白质-配体复合物数据集之间的差距。

Method: MEVO包含三个组件：VQ-VAE用于分子表示，扩散模型用于药效团引导的分子生成，进化策略用于分子优化。

Result: MEVO成功生成了高亲和力配体，并在KRASG12D抑制剂设计中表现出与已知高效抑制剂相当的亲和力。

Conclusion: MEVO为结构药物设计提供了一种高效且数据利用率高的解决方案。

Abstract: Recent advances in generative models, particularly diffusion and
auto-regressive models, have revolutionized fields like computer vision and
natural language processing. However, their application to structure-based drug
design (SBDD) remains limited due to critical data constraints. To address the
limitation of training data for models targeting SBDD tasks, we propose an
evolutionary framework named MEVO, which bridges the gap between billion-scale
small molecule dataset and the scarce protein-ligand complex dataset, and
effectively increase the abundance of training data for generative SBDD models.
MEVO is composed of three key components: a high-fidelity VQ-VAE for molecule
representation in latent space, a diffusion model for pharmacophore-guided
molecule generation, and a pocket-aware evolutionary strategy for molecule
optimization with physics-based scoring function. This framework efficiently
generate high-affinity binders for various protein targets, validated with
predicted binding affinities using free energy perturbation (FEP) methods. In
addition, we showcase the capability of MEVO in designing potent inhibitors to
KRAS$^{\textrm{G12D}}$, a challenging target in cancer therapeutics, with
similar affinity to the known highly active inhibitor evaluated by FEP
calculations. With high versatility and generalizability, MEVO offers an
effective and data-efficient model for various tasks in structure-based ligand
design.

</details>


### [146] [Awesome-OL: An Extensible Toolkit for Online Learning](https://arxiv.org/abs/2507.20144)
*Zeyi Liu,Songqiao Hu,Pengyu Han,Jiaming Liu,Xiao He*

Main category: cs.LG

TL;DR: Awesome-OL是一个用于在线学习研究的Python工具包，集成了先进算法、统一框架和可视化功能。


<details>
  <summary>Details</summary>
Motivation: 在线学习因其处理流式和非平稳数据的自适应能力受到关注，但缺乏统一的工具支持研究和开发。

Method: 基于scikit-multiflow开源基础设施，提供可扩展的算法框架、基准数据集和多模态可视化。

Result: Awesome-OL支持可复现的比较，并兼顾用户友好性和研究灵活性。

Conclusion: Awesome-OL为在线学习研究提供了高效、灵活的工具支持。

Abstract: In recent years, online learning has attracted increasing attention due to
its adaptive capability to process streaming and non-stationary data. To
facilitate algorithm development and practical deployment in this area, we
introduce Awesome-OL, an extensible Python toolkit tailored for online learning
research. Awesome-OL integrates state-of-the-art algorithm, which provides a
unified framework for reproducible comparisons, curated benchmark datasets, and
multi-modal visualization. Built upon the scikit-multiflow open-source
infrastructure, Awesome-OL emphasizes user-friendly interactions without
compromising research flexibility or extensibility. The source code is publicly
available at: https://github.com/liuzy0708/Awesome-OL.

</details>


### [147] [ASNN: Learning to Suggest Neural Architectures from Performance Distributions](https://arxiv.org/abs/2507.20164)
*Jinwook Hong*

Main category: cs.LG

TL;DR: ASNN通过学习神经网络结构与测试准确率的关系，提出改进架构，优于随机搜索。


<details>
  <summary>Details</summary>
Motivation: 神经网络架构对性能至关重要，但缺乏通用映射函数，设计过程依赖启发式或搜索。

Method: 构建数据集训练ASNN，输入准确率，输出架构参数，迭代预测高性能架构。

Result: ASNN在2层和3层架构中均提出优于原始数据的架构，提升平均测试准确率。

Conclusion: ASNN为架构优化提供高效替代方案，有望自动化神经网络设计。

Abstract: The architecture of a neural network (NN) plays a critical role in
determining its performance. However, there is no general closed-form function
that maps between network structure and accuracy, making the process of
architecture design largely heuristic or search-based. In this study, we
propose the Architecture Suggesting Neural Network (ASNN), a model designed to
learn the relationship between NN architecture and its test accuracy, and to
suggest improved architectures accordingly. To train ASNN, we constructed
datasets using TensorFlow-based models with varying numbers of layers and
nodes. Experimental results were collected for both 2-layer and 3-layer
architectures across a grid of configurations, each evaluated with 10 repeated
trials to account for stochasticity. Accuracy values were treated as inputs,
and architectural parameters as outputs. The trained ASNN was then used
iteratively to predict architectures that yield higher performance. In both
2-layer and 3-layer cases, ASNN successfully suggested architectures that
outperformed the best results found in the original training data. Repeated
prediction and retraining cycles led to the discovery of architectures with
improved mean test accuracies, demonstrating the model's capacity to generalize
the performance-structure relationship. These results suggest that ASNN
provides an efficient alternative to random search for architecture
optimization, and offers a promising approach toward automating neural network
design. "Parts of the manuscript, including text editing and expression
refinement, were supported by OpenAI's ChatGPT. All content was reviewed and
verified by the authors."

</details>


### [148] [Partial Domain Adaptation via Importance Sampling-based Shift Correction](https://arxiv.org/abs/2507.20191)
*Cheng-Jun Guo,Chuan-Xian Ren,You-Wei Luo,Xiao-Lin Xu,Hong Yan*

Main category: cs.LG

TL;DR: 提出了一种基于重要性采样的偏移校正方法（IS²C），用于部分域适应任务，通过采样域生成新标签数据以提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决部分域适应中简单重加权方法无法充分利用标签数据和探索潜在结构的问题，避免模型在源域上过拟合。

Method: 提出IS²C方法，通过构建采样域生成新标签数据，结合最优传输独立性准则进行条件分布对齐。

Result: 理论证明IS²C能有效控制泛化误差，实验验证其在部分域适应基准上的优越性。

Conclusion: IS²C方法在部分域适应任务中表现出色，提供了理论支持和实际应用价值。

Abstract: Partial domain adaptation (PDA) is a challenging task in real-world machine
learning scenarios. It aims to transfer knowledge from a labeled source domain
to a related unlabeled target domain, where the support set of the source label
distribution subsumes the target one. Previous PDA works managed to correct the
label distribution shift by weighting samples in the source domain. However,
the simple reweighing technique cannot explore the latent structure and
sufficiently use the labeled data, and then models are prone to over-fitting on
the source domain. In this work, we propose a novel importance sampling-based
shift correction (IS$^2$C) method, where new labeled data are sampled from a
built sampling domain, whose label distribution is supposed to be the same as
the target domain, to characterize the latent structure and enhance the
generalization ability of the model. We provide theoretical guarantees for
IS$^2$C by proving that the generalization error can be sufficiently dominated
by IS$^2$C. In particular, by implementing sampling with the mixture
distribution, the extent of shift between source and sampling domains can be
connected to generalization error, which provides an interpretable way to build
IS$^2$C. To improve knowledge transfer, an optimal transport-based independence
criterion is proposed for conditional distribution alignment, where the
computation of the criterion can be adjusted to reduce the complexity from
$\mathcal{O}(n^3)$ to $\mathcal{O}(n^2)$ in realistic PDA scenarios. Extensive
experiments on PDA benchmarks validate the theoretical results and demonstrate
the effectiveness of our IS$^2$C over existing methods.

</details>


### [149] [Technical Indicator Networks (TINs): An Interpretable Neural Architecture Modernizing Classic al Technical Analysis for Adaptive Algorithmic Trading](https://arxiv.org/abs/2507.20202)
*Longfei Lu*

Main category: cs.LG

TL;DR: 论文提出传统金融技术指标本质上是固定权重的神经网络，并通过神经网络重构这些指标，提出技术指标网络（TINs）以支持多维输入。


<details>
  <summary>Details</summary>
Motivation: 将传统技术分析的逻辑与现代AI系统结合，提升算法交易的能力。

Method: 将技术指标重构为模块化神经网络组件，提出TINs架构，支持多维输入。

Result: TINs能够复制并升级传统指标，同时支持更多数据类型。

Conclusion: TINs为技术分析提供了现代化工具，结合了传统指标的可靠性与AI的潜力。

Abstract: This work proposes that a vast majority of classical technical indicators in
financial analysis are, in essence, special cases of neural networks with fixed
and interpretable weights. It is shown that nearly all such indicators, such as
moving averages, momentum-based oscillators, volatility bands, and other
commonly used technical constructs, can be reconstructed topologically as
modular neural network components. Technical Indicator Networks (TINs) are
introduced as a general neural architecture that replicates and structurally
upgrades traditional indicators by supporting n-dimensional inputs such as
price, volume, sentiment, and order book data. By encoding domain-specific
knowledge into neural structures, TINs modernize the foundational logic of
technical analysis and propel algorithmic trading into a new era, bridging the
legacy of proven indicators with the potential of contemporary AI systems.

</details>


### [150] [Protein-SE(3): Benchmarking SE(3)-based Generative Models for Protein Structure Design](https://arxiv.org/abs/2507.20243)
*Lang Yu,Zhangyang Gao,Cheng Tan,Qin Chen,Jie Zhou,Liang He*

Main category: cs.LG

TL;DR: 提出了Protein-SE(3)基准，用于统一评估SE(3)蛋白质结构生成模型的性能，并整合了多种先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏模块化基准来全面评估和公平比较不同SE(3)蛋白质结构生成方法。

Method: 基于统一训练框架，整合了多种生成模型（如DDPM、Score Matching、Flow Matching），并提供数学抽象和多样化评估指标。

Result: 发布了首个基于SE(3)的统一蛋白质结构设计基准，公开可用。

Conclusion: Protein-SE(3)为未来算法快速原型设计提供了基础，并促进了公平比较。

Abstract: SE(3)-based generative models have shown great promise in protein geometry
modeling and effective structure design. However, the field currently lacks a
modularized benchmark to enable comprehensive investigation and fair comparison
of different methods. In this paper, we propose Protein-SE(3), a new benchmark
based on a unified training framework, which comprises protein scaffolding
tasks, integrated generative models, high-level mathematical abstraction, and
diverse evaluation metrics. Recent advanced generative models designed for
protein scaffolding, from multiple perspectives like DDPM (Genie1 and Genie2),
Score Matching (FrameDiff and RfDiffusion) and Flow Matching (FoldFlow and
FrameFlow) are integrated into our framework. All integrated methods are fairly
investigated with the same training dataset and evaluation metrics.
Furthermore, we provide a high-level abstraction of the mathematical
foundations behind the generative models, enabling fast prototyping of future
algorithms without reliance on explicit protein structures. Accordingly, we
release the first comprehensive benchmark built upon unified training framework
for SE(3)-based protein structure design, which is publicly accessible at
https://github.com/BruthYU/protein-se3.

</details>


### [151] [Learning from Expert Factors: Trajectory-level Reward Shaping for Formulaic Alpha Mining](https://arxiv.org/abs/2507.20263)
*Junjie Zhao,Chengxi Zhang,Chenkai Wang,Peng Yang*

Main category: cs.LG

TL;DR: 论文提出了一种名为TLRS的新奖励塑造方法，通过提供密集的中间奖励和奖励中心化机制，解决了强化学习在挖掘公式化阿尔法因子时奖励稀疏的问题，显著提升了预测能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在挖掘公式化阿尔法因子时面临奖励稀疏的问题，限制了搜索空间的探索和训练过程的稳定性。

Method: 提出了轨迹级奖励塑造（TLRS），通过测量部分生成表达式与专家设计公式的子序列相似性来提供密集中间奖励，并引入奖励中心化机制降低训练方差。

Result: 在六种主要中美股票指数上的实验表明，TLRS显著提升了挖掘因子的预测能力，Rank Information Coefficient提高了9.29%，计算效率也有显著提升。

Conclusion: TLRS通过密集奖励和奖励中心化机制，有效解决了奖励稀疏问题，提升了强化学习在金融领域的应用效果。

Abstract: Reinforcement learning (RL) has successfully automated the complex process of
mining formulaic alpha factors, for creating interpretable and profitable
investment strategies. However, existing methods are hampered by the sparse
rewards given the underlying Markov Decision Process. This inefficiency limits
the exploration of the vast symbolic search space and destabilizes the training
process. To address this, Trajectory-level Reward Shaping (TLRS), a novel
reward shaping method, is proposed. TLRS provides dense, intermediate rewards
by measuring the subsequence-level similarity between partially generated
expressions and a set of expert-designed formulas. Furthermore, a reward
centering mechanism is introduced to reduce training variance. Extensive
experiments on six major Chinese and U.S. stock indices show that TLRS
significantly improves the predictive power of mined factors, boosting the Rank
Information Coefficient by 9.29% over existing potential-based shaping
algorithms. Notably, TLRS achieves a major leap in computational efficiency by
reducing its time complexity with respect to the feature dimension from linear
to constant, which is a significant improvement over distance-based baselines.

</details>


### [152] [Data-Efficient Prediction-Powered Calibration via Cross-Validation](https://arxiv.org/abs/2507.20268)
*Seonghoon Yoo,Houssem Sifaou,Sangwoo Park,Joonhyuk Kang,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出一种新方法，利用有限校准数据同时微调预测器并估计合成标签的偏差，解决校准数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型决策不确定性量化中校准数据稀缺的问题。

Method: 通过同时微调预测器和估计合成标签的偏差，高效利用有限校准数据。

Result: 在室内定位问题中验证了方法的有效性和性能提升。

Conclusion: 新方法能够生成具有严格覆盖保证的预测集，有效解决校准数据稀缺问题。

Abstract: Calibration data are necessary to formally quantify the uncertainty of the
decisions produced by an existing artificial intelligence (AI) model. To
overcome the common issue of scarce calibration data, a promising approach is
to employ synthetic labels produced by a (generally different) predictive
model. However, fine-tuning the label-generating predictor on the inference
task of interest, as well as estimating the residual bias of the synthetic
labels, demand additional data, potentially exacerbating the calibration data
scarcity problem. This paper introduces a novel approach that efficiently
utilizes limited calibration data to simultaneously fine-tune a predictor and
estimate the bias of the synthetic labels. The proposed method yields
prediction sets with rigorous coverage guarantees for AI-generated decisions.
Experimental results on an indoor localization problem validate the
effectiveness and performance gains of our solution.

</details>


### [153] [Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence](https://arxiv.org/abs/2507.20272)
*Dharmesh Tailor,Alvaro H. C. Correia,Eric Nalisnick,Christos Louizos*

Main category: cs.LG

TL;DR: 该论文提出了一种无需预留数据的后处理方法，通过近似全共形预测（full-CP）为神经网络回归器构建预测区间，避免了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域部署深度学习模型需要可靠的不确定性量化，但现有方法如Laplace方法和split-CP存在校准不足或统计效率低的问题。

Method: 通过高斯-牛顿影响局部扰动模型参数，近似全共形预测的效果，并结合网络线性化，高效计算非共形分数。

Result: 在标准回归基准和边界框定位任务中，生成的预测区间具有局部适应性，且通常比split-CP更紧致。

Conclusion: 该方法为后处理不确定性量化提供了高效且准确的解决方案，适用于实际应用。

Abstract: Uncertainty quantification is an important prerequisite for the deployment of
deep learning models in safety-critical areas. Yet, this hinges on the
uncertainty estimates being useful to the extent the prediction intervals are
well-calibrated and sharp. In the absence of inherent uncertainty estimates
(e.g. pretrained models predicting only point estimates), popular approaches
that operate post-hoc include Laplace's method and split conformal prediction
(split-CP). However, Laplace's method can be miscalibrated when the model is
misspecified and split-CP requires sample splitting, and thus comes at the
expense of statistical efficiency. In this work, we construct prediction
intervals for neural network regressors post-hoc without held-out data. This is
achieved by approximating the full conformal prediction method (full-CP).
Whilst full-CP nominally requires retraining the model for every test point and
candidate label, we propose to train just once and locally perturb model
parameters using Gauss-Newton influence to approximate the effect of
retraining. Coupled with linearization of the network, we express the absolute
residual nonconformity score as a piecewise linear function of the candidate
label allowing for an efficient procedure that avoids the exhaustive search
over the output space. On standard regression benchmarks and bounding box
localization, we show the resulting prediction intervals are locally-adaptive
and often tighter than those of split-CP.

</details>


### [154] [MIPS: a Multimodal Infinite Polymer Sequence Pre-training Framework for Polymer Property Prediction](https://arxiv.org/abs/2507.20326)
*Jiaxi Wang,Yaosen Min,Xun Zhu,Miao Li,Ji Wu*

Main category: cs.LG

TL;DR: 提出了一种名为MIPS的多模态预训练框架，用于准确预测聚合物性质，结合拓扑和空间信息，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以全面捕捉聚合物性质，因其在聚合过程中会发生变化。

Method: MIPS框架将聚合物表示为无限单体序列，结合拓扑（MPM和GAM）和空间信息（3D描述符），并提出交叉模态融合机制。

Result: 在八项聚合物性质预测任务中达到最先进性能。

Conclusion: MIPS通过多模态建模显著提升了聚合物性质预测的准确性。

Abstract: Polymers, composed of repeating structural units called monomers, are
fundamental materials in daily life and industry. Accurate property prediction
for polymers is essential for their design, development, and application.
However, existing modeling approaches, which typically represent polymers by
the constituent monomers, struggle to capture the whole properties of polymer,
since the properties change during the polymerization process. In this study,
we propose a Multimodal Infinite Polymer Sequence (MIPS) pre-training
framework, which represents polymers as infinite sequences of monomers and
integrates both topological and spatial information for comprehensive modeling.
From the topological perspective, we generalize message passing mechanism (MPM)
and graph attention mechanism (GAM) to infinite polymer sequences. For MPM, we
demonstrate that applying MPM to infinite polymer sequences is equivalent to
applying MPM on the induced star-linking graph of monomers. For GAM, we propose
to further replace global graph attention with localized graph attention (LGA).
Moreover, we show the robustness of the "star linking" strategy through Repeat
and Shift Invariance Test (RSIT). Despite its robustness, "star linking"
strategy exhibits limitations when monomer side chains contain ring structures,
a common characteristic of polymers, as it fails the Weisfeiler-Lehman~(WL)
test. To overcome this issue, we propose backbone embedding to enhance the
capability of MPM and LGA on infinite polymer sequences. From the spatial
perspective, we extract 3D descriptors of repeating monomers to capture spatial
information. Finally, we design a cross-modal fusion mechanism to unify the
topological and spatial information. Experimental validation across eight
diverse polymer property prediction tasks reveals that MIPS achieves
state-of-the-art performance.

</details>


### [155] [Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning](https://arxiv.org/abs/2507.20335)
*Siyu Song,Wentao Liu,Ye Lu,Ruohua Zhang,Tao Liu,Jinze Lv,Xinyun Wang,Aimin Zhou,Fei Tan,Bo Jiang,Hao Hao*

Main category: cs.LG

TL;DR: 论文提出EduAlign框架，通过标注教育交互数据并训练多维度奖励模型（HPC-RM），优化LLM在教育领域的表现，显著提升其教学帮助性、个性化和创造力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM缺乏与教学原则（如帮助性、个性化、创造力）的对齐，限制了其在教育中的应用潜力。

Method: 1. 标注8k教育交互数据，训练HPC-RM奖励模型；2. 使用GRPO方法微调LLM，优化其输出。

Result: 微调后的模型在教学帮助性、个性化和创造力方面表现显著提升。

Conclusion: EduAlign为LLM在教育领域的对齐提供了可扩展且有效的方法，推动了更具教学价值的AI导师发展。

Abstract: The integration of large language models (LLMs) into education presents
unprecedented opportunities for scalable personalized learning. However,
standard LLMs often function as generic information providers, lacking
alignment with fundamental pedagogical principles such as helpfulness,
student-centered personalization, and creativity cultivation. To bridge this
gap, we propose EduAlign, a novel framework designed to guide LLMs toward
becoming more effective and responsible educational assistants. EduAlign
consists of two main stages. In the first stage, we curate a dataset of 8k
educational interactions and annotate them-both manually and
automatically-along three key educational dimensions: Helpfulness,
Personalization, and Creativity (HPC). These annotations are used to train
HPC-RM, a multi-dimensional reward model capable of accurately scoring LLM
outputs according to these educational principles. We further evaluate the
consistency and reliability of this reward model. In the second stage, we
leverage HPC-RM as a reward signal to fine-tune a pre-trained LLM using Group
Relative Policy Optimization (GRPO) on a set of 2k diverse prompts. We then
assess the pre- and post-finetuning models on both educational and
general-domain benchmarks across the three HPC dimensions. Experimental results
demonstrate that the fine-tuned model exhibits significantly improved alignment
with pedagogical helpfulness, personalization, and creativity stimulation. This
study presents a scalable and effective approach to aligning LLMs with nuanced
and desirable educational traits, paving the way for the development of more
engaging, pedagogically aligned AI tutors.

</details>


### [156] [From Observations to Causations: A GNN-based Probabilistic Prediction Framework for Causal Discovery](https://arxiv.org/abs/2507.20349)
*Rezaur Rashid,Gabriel Terejanu*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的概率框架，用于从观测数据中发现因果结构，优于传统方法和非GNN方法。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在处理大规模数据和复杂关系时存在可扩展性和全局结构信息捕捉的局限性。

Method: 使用GNN编码节点和边属性为统一图表示，通过监督学习直接预测整个图结构，训练数据包含统计和信息论度量。

Result: 在合成和真实数据集上表现出更高的准确性和可扩展性，优于传统和非GNN方法。

Conclusion: 该概率框架显著改进了因果结构学习，对多领域决策和科学发现具有广泛意义。

Abstract: Causal discovery from observational data is challenging, especially with
large datasets and complex relationships. Traditional methods often struggle
with scalability and capturing global structural information. To overcome these
limitations, we introduce a novel graph neural network (GNN)-based
probabilistic framework that learns a probability distribution over the entire
space of causal graphs, unlike methods that output a single deterministic
graph. Our framework leverages a GNN that encodes both node and edge attributes
into a unified graph representation, enabling the model to learn complex causal
structures directly from data. The GNN model is trained on a diverse set of
synthetic datasets augmented with statistical and information-theoretic
measures, such as mutual information and conditional entropy, capturing both
local and global data properties. We frame causal discovery as a supervised
learning problem, directly predicting the entire graph structure. Our approach
demonstrates superior performance, outperforming both traditional and recent
non-GNN-based methods, as well as a GNN-based approach, in terms of accuracy
and scalability on synthetic and real-world datasets without further training.
This probabilistic framework significantly improves causal structure learning,
with broad implications for decision-making and scientific discovery across
various fields.

</details>


### [157] [Computational Advantages of Multi-Grade Deep Learning: Convergence Analysis and Performance Insights](https://arxiv.org/abs/2507.20351)
*Ronglong Fang,Yuesheng Xu*

Main category: cs.LG

TL;DR: MGDL在图像回归、去噪和去模糊任务中表现优于SGDL，且对学习率选择更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 研究MGDL的计算优势及其在图像处理任务中的性能表现。

Method: 通过梯度下降法（GD）分析MGDL和SGDL的收敛性及Jacobian矩阵特征值分布。

Result: MGDL在训练稳定性上优于SGDL，且对学习率选择更灵活。

Conclusion: MGDL在性能和稳定性上优于SGDL，尤其适合图像处理任务。

Abstract: Multi-grade deep learning (MGDL) has been shown to significantly outperform
the standard single-grade deep learning (SGDL) across various applications.
This work aims to investigate the computational advantages of MGDL focusing on
its performance in image regression, denoising, and deblurring tasks, and
comparing it to SGDL. We establish convergence results for the gradient descent
(GD) method applied to these models and provide mathematical insights into
MGDL's improved performance. In particular, we demonstrate that MGDL is more
robust to the choice of learning rate under GD than SGDL. Furthermore, we
analyze the eigenvalue distributions of the Jacobian matrices associated with
the iterative schemes arising from the GD iterations, offering an explanation
for MGDL's enhanced training stability.

</details>


### [158] [Wafer Defect Root Cause Analysis with Partial Trajectory Regression](https://arxiv.org/abs/2507.20357)
*Kohei Miyaguchi,Masao Joko,Rebekah Sheraw,Tsuyoshi Idé*

Main category: cs.LG

TL;DR: 提出了一种名为PTR的新框架，用于晶圆缺陷根因分析，解决了传统向量回归模型在处理变长处理路径时的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于晶圆加工流程的组合性和变异性（如返工操作和随机等待时间），识别缺陷的上游过程具有挑战性。

Method: 提出了PTR框架，采用proc2vec和route2vec表示学习方法，通过比较部分轨迹的反事实结果计算过程归因分数。

Result: 在NY CREATES fab的真实数据上验证了框架的有效性。

Conclusion: PTR框架为晶圆缺陷根因分析提供了新的解决方案，特别适合处理复杂的变长加工路径。

Abstract: Identifying upstream processes responsible for wafer defects is challenging
due to the combinatorial nature of process flows and the inherent variability
in processing routes, which arises from factors such as rework operations and
random process waiting times. This paper presents a novel framework for wafer
defect root cause analysis, called Partial Trajectory Regression (PTR). The
proposed framework is carefully designed to address the limitations of
conventional vector-based regression models, particularly in handling
variable-length processing routes that span a large number of heterogeneous
physical processes. To compute the attribution score of each process given a
detected high defect density on a specific wafer, we propose a new algorithm
that compares two counterfactual outcomes derived from partial process
trajectories. This is enabled by new representation learning methods, proc2vec
and route2vec. We demonstrate the effectiveness of the proposed framework using
real wafer history data from the NY CREATES fab in Albany.

</details>


### [159] [MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS Data (Extended Version)](https://arxiv.org/abs/2507.20362)
*Hengyu Liu,Tianyi Li,Yuqiang He,Kristian Torp,Yushuai Li,Christian S. Jensen*

Main category: cs.LG

TL;DR: MH-GIN是一种基于多尺度异构图网络的缺失值填补方法，通过捕捉多尺度依赖关系，显著提高了填补精度。


<details>
  <summary>Details</summary>
Motivation: 自动识别系统的位置跟踪数据在海上安全和监测应用中至关重要，但数据缺失问题限制了其下游应用。现有方法因假设属性更新速率相同，无法捕捉多尺度依赖关系。

Method: MH-GIN提取多尺度时间特征并构建多尺度异构图，通过图传播实现缺失值的精确填补。

Result: 在两个真实数据集上，MH-GIN平均减少57%的填补误差，同时保持计算效率。

Conclusion: MH-GIN通过多尺度异构图建模，显著提升了缺失值填补的准确性，为相关应用提供了有效工具。

Abstract: Location-tracking data from the Automatic Identification System, much of
which is publicly available, plays a key role in a range of maritime safety and
monitoring applications. However, the data suffers from missing values that
hamper downstream applications. Imputing the missing values is challenging
because the values of different heterogeneous attributes are updated at diverse
rates, resulting in the occurrence of multi-scale dependencies among
attributes. Existing imputation methods that assume similar update rates across
attributes are unable to capture and exploit such dependencies, limiting their
imputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based
Imputation Network that aims improve imputation accuracy by capturing
multi-scale dependencies. Specifically, MH-GIN first extracts multi-scale
temporal features for each attribute while preserving their intrinsic
heterogeneous characteristics. Then, it constructs a multi-scale heterogeneous
graph to explicitly model dependencies between heterogeneous attributes to
enable more accurate imputation of missing values through graph propagation.
Experimental results on two real-world datasets find that MH-GIN is capable of
an average 57% reduction in imputation errors compared to state-of-the-art
methods, while maintaining computational efficiency. The source code and
implementation details of MH-GIN are publicly available
https://github.com/hyLiu1994/MH-GIN.

</details>


### [160] [Sequence-Aware Inline Measurement Attribution for Good-Bad Wafer Diagnosis](https://arxiv.org/abs/2507.20364)
*Kohei Miyaguchi,Masao Joko,Rebekah Sheraw,Tsuyoshi Idé*

Main category: cs.LG

TL;DR: 论文提出了一种名为Trajectory Shapley Attribution (TSA)的新框架，用于解决半导体制造中晶圆缺陷的跨工艺根因分析问题。


<details>
  <summary>Details</summary>
Motivation: 现代半导体制造涉及数千个工艺步骤，晶圆缺陷的根因分析极具挑战性，需要克服传统方法的局限性。

Method: TSA扩展了Shapley值（SV），解决了其对制造过程顺序性的忽视和参考点选择的任意性问题。

Result: 在实验性前端工艺中，TSA成功识别了与异常缺陷最相关的测量项目。

Conclusion: TSA为复杂制造环境中的缺陷分析提供了一种有效工具。

Abstract: How can we identify problematic upstream processes when a certain type of
wafer defect starts appearing at a quality checkpoint? Given the complexity of
modern semiconductor manufacturing, which involves thousands of process steps,
cross-process root cause analysis for wafer defects has been considered highly
challenging. This paper proposes a novel framework called Trajectory Shapley
Attribution (TSA), an extension of Shapley values (SV), a widely used
attribution algorithm in explainable artificial intelligence research. TSA
overcomes key limitations of standard SV, including its disregard for the
sequential nature of manufacturing processes and its reliance on an arbitrarily
chosen reference point. We applied TSA to a good-bad wafer diagnosis task in
experimental front-end-of-line processes at the NY CREATES Albany NanoTech fab,
aiming to identify measurement items (serving as proxies for process
parameters) most relevant to abnormal defect occurrence.

</details>


### [161] [Clustering by Attention: Leveraging Prior Fitted Transformers for Data Partitioning](https://arxiv.org/abs/2507.20369)
*Ahmed Shokry,Ayman Khalafallah*

Main category: cs.LG

TL;DR: 提出了一种基于元学习的新型聚类方法，无需参数优化，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法存在参数调优复杂、计算成本高、可解释性差或精度不足等问题，特别是在大规模数据集上。

Method: 利用预训练的Prior-Data Fitted Transformer Network (PFN)，通过少量预聚类样本引导整个数据集的聚类过程，计算注意力以推断聚类分配。

Result: 实验表明，该方法在无预聚类样本时仍能有效聚类，提供少量样本后性能显著提升，优于现有技术。

Conclusion: 该方法高效且可扩展，是现有聚类技术的有力替代方案。

Abstract: Clustering is a core task in machine learning with wide-ranging applications
in data mining and pattern recognition. However, its unsupervised nature makes
it inherently challenging. Many existing clustering algorithms suffer from
critical limitations: they often require careful parameter tuning, exhibit high
computational complexity, lack interpretability, or yield suboptimal accuracy,
especially when applied to large-scale datasets. In this paper, we introduce a
novel clustering approach based on meta-learning. Our approach eliminates the
need for parameter optimization while achieving accuracy that outperforms
state-of-the-art clustering techniques. The proposed technique leverages a few
pre-clustered samples to guide the clustering process for the entire dataset in
a single forward pass. Specifically, we employ a pre-trained Prior-Data Fitted
Transformer Network (PFN) to perform clustering. The algorithm computes
attention between the pre-clustered samples and the unclustered samples,
allowing it to infer cluster assignments for the entire dataset based on the
learned relation. We theoretically and empirically demonstrate that, given just
a few pre-clustered examples, the model can generalize to accurately cluster
the rest of the dataset. Experiments on challenging benchmark datasets show
that our approach can successfully cluster well-separated data without any
pre-clustered samples, and significantly improves performance when a few
clustered samples are provided. We show that our approach is superior to the
state-of-the-art techniques. These results highlight the effectiveness and
scalability of our approach, positioning it as a promising alternative to
existing clustering techniques.

</details>


### [162] [WBHT: A Generative Attention Architecture for Detecting Black Hole Anomalies in Backbone Networks](https://arxiv.org/abs/2507.20373)
*Kiymet Kaya,Elif Ak,Sule Gunduz Oguducu*

Main category: cs.LG

TL;DR: WBHT框架结合生成对抗网络、注意力机制和序列学习，用于检测通信网络中的黑洞异常，显著提升F1分数。


<details>
  <summary>Details</summary>
Motivation: 黑洞异常导致数据包丢失且无故障通知，影响网络连接并造成经济损失，需高效检测方法。

Method: 结合Wasserstein GAN和注意力机制，使用LSTM和卷积层捕捉长短期依赖关系，通过潜在空间编码区分异常行为。

Result: 在真实网络数据测试中，WBHT的F1分数提升1.65%至58.76%，优于现有模型。

Conclusion: WBHT高效且能检测未发现的异常，适用于关键网络的主动监控和安全防护。

Abstract: We propose the Wasserstein Black Hole Transformer (WBHT) framework for
detecting black hole (BH) anomalies in communication networks. These anomalies
cause packet loss without failure notifications, disrupting connectivity and
leading to financial losses. WBHT combines generative modeling, sequential
learning, and attention mechanisms to improve BH anomaly detection. It
integrates a Wasserstein generative adversarial network with attention
mechanisms for stable training and accurate anomaly identification. The model
uses long-short-term memory layers to capture long-term dependencies and
convolutional layers for local temporal patterns. A latent space encoding
mechanism helps distinguish abnormal network behavior. Tested on real-world
network data, WBHT outperforms existing models, achieving significant
improvements in F1 score (ranging from 1.65% to 58.76%). Its efficiency and
ability to detect previously undetected anomalies make it a valuable tool for
proactive network monitoring and security, especially in mission-critical
networks.

</details>


### [163] [Set-based Implicit Likelihood Inference of Galaxy Cluster Mass](https://arxiv.org/abs/2507.20378)
*Bonny Y. Wang,Leander Thiele*

Main category: cs.LG

TL;DR: 提出了一种基于集合的机器学习框架，通过星系动力学推断星系团质量的后验分布，显著减少了传统方法的误差。


<details>
  <summary>Details</summary>
Motivation: 改进传统星系团质量估计方法，减少误差并提供更可靠的解释性。

Method: 结合Deep Sets和条件归一化流，利用成员星系的位置和速度信息预测质量-速度关系的残差修正。

Result: 在Uchuu-UniverseMachine模拟中训练后，显著减少了散射，并在全质量范围内提供了校准良好的不确定性。

Conclusion: 该方法优于传统动力学估计，为星系团质量推断提供了更准确的工具。

Abstract: We present a set-based machine learning framework that infers posterior
distributions of galaxy cluster masses from projected galaxy dynamics. Our
model combines Deep Sets and conditional normalizing flows to incorporate both
positional and velocity information of member galaxies to predict residual
corrections to the $M$-$\sigma$ relation for improved interpretability. Trained
on the Uchuu-UniverseMachine simulation, our approach significantly reduces
scatter and provides well-calibrated uncertainties across the full mass range
compared to traditional dynamical estimates.

</details>


### [164] [Communication-Efficient Distributed Training for Collaborative Flat Optima Recovery in Deep Learning](https://arxiv.org/abs/2507.20424)
*Tolga Dimlioglu,Anna Choromanska*

Main category: cs.LG

TL;DR: 论文提出了一种分布式训练算法DPPF，通过引入轻量级正则化器Inverse Mean Valley，平衡通信效率和模型性能，显著提升泛化能力并减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 研究分布式数据并行训练中通信效率与模型性能的权衡问题，探索平坦极小值假设在提升泛化能力中的作用。

Method: 引入Inverse Mean Valley作为锐度度量，设计DPPF算法，结合推拉动态优化分布式训练目标。

Result: DPPF在通信效率和泛化性能上优于其他方法，实验验证其能定位更平坦的极小值。

Conclusion: DPPF通过推拉动态和理论保证，有效提升分布式训练的泛化能力和收敛性。

Abstract: We study centralized distributed data parallel training of deep neural
networks (DNNs), aiming to improve the trade-off between communication
efficiency and model performance of the local gradient methods. To this end, we
revisit the flat-minima hypothesis, which suggests that models with better
generalization tend to lie in flatter regions of the loss landscape. We
introduce a simple, yet effective, sharpness measure, Inverse Mean Valley, and
demonstrate its strong correlation with the generalization gap of DNNs. We
incorporate an efficient relaxation of this measure into the distributed
training objective as a lightweight regularizer that encourages workers to
collaboratively seek wide minima. The regularizer exerts a pushing force that
counteracts the consensus step pulling the workers together, giving rise to the
Distributed Pull-Push Force (DPPF) algorithm. Empirically, we show that DPPF
outperforms other communication-efficient approaches and achieves better
generalization performance than local gradient methods and synchronous gradient
averaging, while significantly reducing communication overhead. In addition,
our loss landscape visualizations confirm the ability of DPPF to locate flatter
minima. On the theoretical side, we show that DPPF guides workers to span flat
valleys, with the final valley width governed by the interplay between push and
pull strengths, and that its pull-push dynamics is self-stabilizing. We further
provide generalization guarantees linked to the valley width and prove
convergence in the non-convex setting.

</details>


### [165] [ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings](https://arxiv.org/abs/2507.20426)
*Samiul Based Shuvo,Tasnia Binte Mamun,U Rajendra Acharya*

Main category: cs.LG

TL;DR: 提出了一种名为ResCap-DBP的新型深度学习框架，结合残差学习和一维胶囊网络，直接从原始蛋白质序列预测DNA结合蛋白，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DNA结合蛋白（DBPs）在基因调控和细胞过程中起关键作用，但实验方法耗时且昂贵，需要高效的计算预测技术。

Method: 结合残差学习编码器和一维胶囊网络（1D-CapsNet），利用扩张卷积缓解梯度消失问题，胶囊层动态路由捕获特征空间中的层次和空间关系。

Result: 在多个基准数据集上表现优异，AUC得分高达98.0%，并在独立测试集上保持83%以上的AUC，同时平衡了敏感性和特异性。

Conclusion: ResCap-DBP通过整合全局蛋白质表示和先进深度学习架构，实现了可靠且可扩展的DBP预测。

Abstract: DNA-binding proteins (DBPs) are integral to gene regulation and cellular
processes, making their accurate identification essential for understanding
biological functions and disease mechanisms. Experimental methods for DBP
identification are time-consuming and costly, driving the need for efficient
computational prediction techniques. In this study, we propose a novel deep
learning framework, ResCap-DBP, that combines a residual learning-based encoder
with a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly
from raw protein sequences. Our architecture incorporates dilated convolutions
within residual blocks to mitigate vanishing gradient issues and extract rich
sequence features, while capsule layers with dynamic routing capture
hierarchical and spatial relationships within the learned feature space. We
conducted comprehensive ablation studies comparing global and local embeddings
from ProteinBERT and conventional one-hot encoding. Results show that
ProteinBERT embeddings substantially outperform other representations on large
datasets. Although one-hot encoding showed marginal advantages on smaller
datasets, such as PDB186, it struggled to scale effectively. Extensive
evaluations on four pairs of publicly available benchmark datasets demonstrate
that our model consistently outperforms current state-of-the-art methods. It
achieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On
independent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2%
and 83.3%, while maintaining competitive performance on larger datasets such as
PDB20000. Notably, the model maintains a well balanced sensitivity and
specificity across datasets. These results demonstrate the efficacy and
generalizability of integrating global protein representations with advanced
deep learning architectures for reliable and scalable DBP prediction in diverse
genomic contexts.

</details>


### [166] [FAST: Similarity-based Knowledge Transfer for Efficient Policy Learning](https://arxiv.org/abs/2507.20433)
*Alessandro Capurso,Elia Piccoli,Davide Bacciu*

Main category: cs.LG

TL;DR: FAST框架通过视觉和文本数据估计任务相似性，优化知识迁移，提升代理性能并减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决迁移学习中的负迁移、领域适应和源策略选择效率问题，特别是在动态领域（如游戏开发）中。

Method: 利用视觉帧和文本描述构建任务动态的潜在表示，估计环境相似性，指导策略迁移。

Result: 在多个赛车赛道上，FAST性能与从头学习相当，但训练步骤显著减少。

Conclusion: 嵌入驱动的任务相似性估计在迁移学习中具有潜力。

Abstract: Transfer Learning (TL) offers the potential to accelerate learning by
transferring knowledge across tasks. However, it faces critical challenges such
as negative transfer, domain adaptation and inefficiency in selecting solid
source policies. These issues often represent critical problems in evolving
domains, i.e. game development, where scenarios transform and agents must
adapt. The continuous release of new agents is costly and inefficient. In this
work we challenge the key issues in TL to improve knowledge transfer, agents
performance across tasks and reduce computational costs. The proposed
methodology, called FAST - Framework for Adaptive Similarity-based Transfer,
leverages visual frames and textual descriptions to create a latent
representation of tasks dynamics, that is exploited to estimate similarity
between environments. The similarity scores guides our method in choosing
candidate policies from which transfer abilities to simplify learning of novel
tasks. Experimental results, over multiple racing tracks, demonstrate that FAST
achieves competitive final performance compared to learning-from-scratch
methods while requiring significantly less training steps. These findings
highlight the potential of embedding-driven task similarity estimations.

</details>


### [167] [BioNeuralNet: A Graph Neural Network based Multi-Omics Network Data Analysis Tool](https://arxiv.org/abs/2507.20440)
*Vicente Ramos,Sundous Hussein,Mohamed Abdel-Hafiz,Arunangshu Sarkar,Weixuan Liu,Katerina J. Kechris,Russell P. Bowler,Leslie Lange,Farnoush Banaei-Kashani*

Main category: cs.LG

TL;DR: BioNeuralNet是一个基于Python的模块化框架，利用图神经网络（GNNs）从多组学网络中提取低维表示，支持从网络构建到下游分析的全流程。


<details>
  <summary>Details</summary>
Motivation: 多组学数据的高维性和复杂性带来了分析挑战，现有网络方法虽能捕捉分子关系，但缺乏专门工具支持多样化下游分析。

Method: BioNeuralNet采用GNNs学习多组学网络的低维表示，提供多种网络构建技术和下游分析功能。

Result: BioNeuralNet支持灵活、可重复的多组学网络分析，兼容主流Python工具，适用于精准医学。

Conclusion: BioNeuralNet填补了多组学网络分析工具的空白，是一个开源、易用的框架。

Abstract: Multi-omics data offer unprecedented insights into complex biological
systems, yet their high dimensionality, sparsity, and intricate interactions
pose significant analytical challenges. Network-based approaches have advanced
multi-omics research by effectively capturing biologically relevant
relationships among molecular entities. While these methods are powerful for
representing molecular interactions, there remains a need for tools
specifically designed to effectively utilize these network representations
across diverse downstream analyses. To fulfill this need, we introduce
BioNeuralNet, a flexible and modular Python framework tailored for end-to-end
network-based multi-omics data analysis. BioNeuralNet leverages Graph Neural
Networks (GNNs) to learn biologically meaningful low-dimensional
representations from multi-omics networks, converting these complex molecular
networks into versatile embeddings. BioNeuralNet supports all major stages of
multi-omics network analysis, including several network construction
techniques, generation of low-dimensional representations, and a broad range of
downstream analytical tasks. Its extensive utilities, including diverse GNN
architectures, and compatibility with established Python packages (e.g.,
scikit-learn, PyTorch, NetworkX), enhance usability and facilitate quick
adoption. BioNeuralNet is an open-source, user-friendly, and extensively
documented framework designed to support flexible and reproducible multi-omics
network analysis in precision medicine.

</details>


### [168] [Provable In-Context Learning of Nonlinear Regression with Transformers](https://arxiv.org/abs/2507.20443)
*Hongbo Li,Lingjie Duan,Yingbin Liang*

Main category: cs.LG

TL;DR: 本文研究了Transformer在复杂非线性回归任务中的上下文学习（ICL）能力，分析了注意力动态及其收敛行为，并证明了Lipschitz常数对收敛时间的影响。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在复杂非线性任务中的ICL能力，以弥补现有研究对简单任务的局限性。

Method: 分析训练过程中注意力的阶段动态，引入新的证明技术，研究Lipschitz常数对注意力权重的影响。

Result: 发现Lipschitz常数是影响收敛时间的关键因素，并证明了在不同L值下预测误差趋近于零的时间界限。

Conclusion: Transformer在复杂任务中仍能通过ICL学习未见过的函数，且注意力机制能有效聚焦相关特征。

Abstract: The transformer architecture, which processes sequences of input tokens to
produce outputs for query tokens, has revolutionized numerous areas of machine
learning. A defining feature of transformers is their ability to perform
previously unseen tasks using task-specific prompts without updating
parameters, a phenomenon known as in-context learning (ICL). Recent research
has actively explored the training dynamics behind ICL, with much of the focus
on relatively simple tasks such as linear regression and binary classification.
To advance the theoretical understanding of ICL, this paper investigates more
complex nonlinear regression tasks, aiming to uncover how transformers acquire
in-context learning capabilities in these settings. We analyze the stage-wise
dynamics of attention during training: attention scores between a query token
and its target features grow rapidly in the early phase, then gradually
converge to one, while attention to irrelevant features decays more slowly and
exhibits oscillatory behavior. Our analysis introduces new proof techniques
that explicitly characterize how the nature of general non-degenerate
L-Lipschitz task functions affects attention weights. Specifically, we identify
that the Lipschitz constant L of nonlinear function classes as a key factor
governing the convergence dynamics of transformers in ICL. Leveraging these
insights, for two distinct regimes depending on whether L is below or above a
threshold, we derive different time bounds to guarantee near-zero prediction
error. Notably, despite the convergence time depending on the underlying task
functions, we prove that query tokens consistently attend to prompt tokens with
highly relevant features at convergence, demonstrating the ICL capability of
transformers for unseen functions.

</details>


### [169] [BOASF: A Unified Framework for Speeding up Automatic Machine Learning via Adaptive Successive Filtering](https://arxiv.org/abs/2507.20446)
*Guanghui Zhu,Xin Fang,Lei Wang,Wenzhong Chen,Rong Gu,Chunfeng Yuan,Yihua Huang*

Main category: cs.LG

TL;DR: 提出了一种结合贝叶斯优化和自适应连续过滤算法的BOASF方法，用于自动化模型选择和超参数优化，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决非专家在机器学习任务中难以高效选择最优模型或超参数的问题。

Method: BOASF结合贝叶斯优化和自适应连续过滤，通过多轮评估和资源分配优化模型选择和超参数。

Result: 实验表明BOASF在速度和预测性能上优于现有自动机器学习方法，且在不同时间预算下表现更好。

Conclusion: BOASF是一种高效且鲁棒的自动化机器学习方法，适用于模型选择和超参数优化。

Abstract: Machine learning has been making great success in many application areas.
However, for the non-expert practitioners, it is always very challenging to
address a machine learning task successfully and efficiently. Finding the
optimal machine learning model or the hyperparameter combination set from a
large number of possible alternatives usually requires considerable expert
knowledge and experience. To tackle this problem, we propose a combined
Bayesian Optimization and Adaptive Successive Filtering algorithm (BOASF) under
a unified multi-armed bandit framework to automate the model selection or the
hyperparameter optimization. Specifically, BOASF consists of multiple
evaluation rounds in each of which we select promising configurations for each
arm using the Bayesian optimization. Then, ASF can early discard the
poor-performed arms adaptively using a Gaussian UCB-based probabilistic model.
Furthermore, a Softmax model is employed to adaptively allocate available
resources for each promising arm that advances to the next round. The arm with
a higher probability of advancing will be allocated more resources.
Experimental results show that BOASF is effective for speeding up the model
selection and hyperparameter optimization processes while achieving robust and
better prediction performance than the existing state-of-the-art automatic
machine learning methods. Moreover, BOASF achieves better anytime performance
under various time budgets.

</details>


### [170] [WEEP: A Differentiable Nonconvex Sparse Regularizer via Weakly-Convex Envelope](https://arxiv.org/abs/2507.20447)
*Takanobu Furuhashi,Hidekata Hontani,Tatsuya Yokota*

Main category: cs.LG

TL;DR: WEEP是一种新型可微稀疏正则化方法，解决了传统稀疏正则化不可微与梯度优化器的矛盾。


<details>
  <summary>Details</summary>
Motivation: 稀疏正则化在信号处理中很重要，但最强的稀疏惩罚通常不可微，与梯度优化器冲突。

Method: 提出WEEP（弱凸包络分段惩罚），基于弱凸包络框架，具有完全可微性和L-平滑性。

Result: 在信号和图像去噪任务中，WEEP表现优于L1范数和其他非凸稀疏正则化方法。

Conclusion: WEEP在统计性能和计算可行性之间取得了平衡，适用于梯度优化器。

Abstract: Sparse regularization is fundamental in signal processing for efficient
signal recovery and feature extraction. However, it faces a fundamental
dilemma: the most powerful sparsity-inducing penalties are often
non-differentiable, conflicting with gradient-based optimizers that dominate
the field. We introduce WEEP (Weakly-convex Envelope of Piecewise Penalty), a
novel, fully differentiable sparse regularizer derived from the weakly-convex
envelope framework. WEEP provides strong, unbiased sparsity while maintaining
full differentiability and L-smoothness, making it natively compatible with any
gradient-based optimizer. This resolves the conflict between statistical
performance and computational tractability. We demonstrate superior performance
compared to the L1-norm and other established non-convex sparse regularizers on
challenging signal and image denoising tasks.

</details>


### [171] [Your Attention Matters: to Improve Model Robustness to Noise and Spurious Correlations](https://arxiv.org/abs/2507.20453)
*Camilo Tamayo-Rousseau,Yunjia Zhao,Yiqun Zhang,Randall Balestriero*

Main category: cs.LG

TL;DR: 研究了不同自注意力机制在数据损坏情况下的鲁棒性，发现Doubly Stochastic注意力最稳健。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在Transformer中广泛应用，但其对噪声和伪相关性的鲁棒性尚未充分研究。

Method: 在Vision Transformers中测试了Softmax、Sigmoid、Linear、Doubly Stochastic和Cosine注意力，使用CIFAR-10、CIFAR-100和Imagenette数据集。

Result: Doubly Stochastic注意力在不同数据损坏场景下表现最稳健。

Conclusion: 在数据不完美的情况下，Doubly Stochastic注意力是更优的选择。

Abstract: Self-attention mechanisms are foundational to Transformer architectures,
supporting their impressive success in a wide range of tasks. While there are
many self-attention variants, their robustness to noise and spurious
correlations has not been well studied. This study evaluates Softmax, Sigmoid,
Linear, Doubly Stochastic, and Cosine attention within Vision Transformers
under different data corruption scenarios. Through testing across the CIFAR-10,
CIFAR-100, and Imagenette datasets, we show that Doubly Stochastic attention is
the most robust. Our findings inform self-attention selection in contexts with
imperfect data.

</details>


### [172] [Customize Multi-modal RAI Guardrails with Precedent-based predictions](https://arxiv.org/abs/2507.20503)
*Cheng-Fu Yang,Thanh Tran,Christos Christodoulopoulos,Weitong Ruan,Rahul Gupta,Kai-Wei Chang*

Main category: cs.LG

TL;DR: 提出一种基于“先例”的多模态护栏方法，通过利用类似数据的推理过程而非固定策略，提升护栏的灵活性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在应对多样化、可定制的用户策略时，要么泛化能力受限，要么需要大量重新训练。

Method: 采用“先例”机制，引入批判-修订机制收集高质量先例，并利用两种策略进行稳健预测。

Result: 实验表明，该方法在少样本和全数据集场景下均优于现有方法，且对新策略具有更好的泛化能力。

Conclusion: 基于先例的方法显著提升了多模态护栏的灵活性和适应性，适用于动态用户需求。

Abstract: A multi-modal guardrail must effectively filter image content based on
user-defined policies, identifying material that may be hateful, reinforce
harmful stereotypes, contain explicit material, or spread misinformation.
Deploying such guardrails in real-world applications, however, poses
significant challenges. Users often require varied and highly customizable
policies and typically cannot provide abundant examples for each custom policy.
Consequently, an ideal guardrail should be scalable to the multiple policies
and adaptable to evolving user standards with minimal retraining. Existing
fine-tuning methods typically condition predictions on pre-defined policies,
restricting their generalizability to new policies or necessitating extensive
retraining to adapt. Conversely, training-free methods struggle with limited
context lengths, making it difficult to incorporate all the policies
comprehensively. To overcome these limitations, we propose to condition model's
judgment on "precedents", which are the reasoning processes of prior data
points similar to the given input. By leveraging precedents instead of fixed
policies, our approach greatly enhances the flexibility and adaptability of the
guardrail. In this paper, we introduce a critique-revise mechanism for
collecting high-quality precedents and two strategies that utilize precedents
for robust prediction. Experimental results demonstrate that our approach
outperforms previous methods across both few-shot and full-dataset scenarios
and exhibits superior generalization to novel policies.

</details>


### [173] [Diagonally-Weighted Generalized Method of Moments Estimation for Gaussian Mixture Modeling](https://arxiv.org/abs/2507.20459)
*Liu Zhang,Oscar Mickelin,Sheng Xu,Amit Singer*

Main category: cs.LG

TL;DR: 论文提出了一种对角线加权广义矩方法（DGMM），以解决高维数据中传统矩方法（MM）和广义矩方法（GMM）的计算和存储问题。DGMM在统计效率、计算复杂度和数值稳定性之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 传统MM和GMM在高维数据或需要高阶矩时，计算和存储复杂度呈指数增长，导致实用性受限。GMM还需估计大型加权矩阵，进一步加剧了计算瓶颈。

Method: 提出DGMM，通过设计对角线加权矩阵，避免显式计算或存储矩张量，并开发了一种高效且数值稳定的算法。

Result: 实验表明，DGMM在估计误差和运行时间上均优于MM和GMM。

Conclusion: DGMM是一种在高维数据中高效且稳定的参数估计方法，适用于弱分离异方差低秩高斯混合模型。

Abstract: Since Pearson [Philosophical Transactions of the Royal Society of London. A,
185 (1894), pp. 71-110] first applied the method of moments (MM) for modeling
data as a mixture of one-dimensional Gaussians, moment-based estimation methods
have proliferated. Among these methods, the generalized method of moments (GMM)
improves the statistical efficiency of MM by weighting the moments
appropriately. However, the computational complexity and storage complexity of
MM and GMM grow exponentially with the dimension, making these methods
impractical for high-dimensional data or when higher-order moments are
required. Such computational bottlenecks are more severe in GMM since it
additionally requires estimating a large weighting matrix. To overcome these
bottlenecks, we propose the diagonally-weighted GMM (DGMM), which achieves a
balance among statistical efficiency, computational complexity, and numerical
stability. We apply DGMM to study the parameter estimation problem for weakly
separated heteroscedastic low-rank Gaussian mixtures and design a
computationally efficient and numerically stable algorithm that obtains the
DGMM estimator without explicitly computing or storing the moment tensors. We
implement the proposed algorithm and empirically validate the advantages of
DGMM: in numerical studies, DGMM attains smaller estimation errors while
requiring substantially shorter runtime than MM and GMM. The code and data will
be available upon publication at https://github.com/liu-lzhang/dgmm.

</details>


### [174] [Shapley-Value-Based Graph Sparsification for GNN Inference](https://arxiv.org/abs/2507.20460)
*Selahattin Akkas,Ariful Azad*

Main category: cs.LG

TL;DR: 论文提出基于Shapley值的图稀疏化方法，通过正负重要性评分优化图神经网络（GNN）的推理效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有图稀疏化方法多依赖非负重要性评分，限制了其适用性；而Shapley值能提供更全面的正负贡献评估，更适合稀疏化任务。

Method: 利用Shapley值计算节点预测的正负贡献，生成全局重要性评分，指导图稀疏化。

Result: 基于Shapley值的稀疏化方法在保持预测性能的同时显著降低图复杂度。

Conclusion: Shapley值方法提升了GNN的推理效率和可解释性，优于传统非负评分方法。

Abstract: Graph sparsification is a key technique for improving inference efficiency in
Graph Neural Networks by removing edges with minimal impact on predictions. GNN
explainability methods generate local importance scores, which can be
aggregated into global scores for graph sparsification. However, many
explainability methods produce only non-negative scores, limiting their
applicability for sparsification. In contrast, Shapley value based methods
assign both positive and negative contributions to node predictions, offering a
theoretically robust and fair allocation of importance by evaluating many
subsets of graphs. Unlike gradient-based or perturbation-based explainers,
Shapley values enable better pruning strategies that preserve influential edges
while removing misleading or adversarial connections. Our approach shows that
Shapley value-based graph sparsification maintains predictive performance while
significantly reducing graph complexity, enhancing both interpretability and
efficiency in GNN inference.

</details>


### [175] [Kimi K2: Open Agentic Intelligence](https://arxiv.org/abs/2507.20534)
*Kimi Team,Yifan Bai,Yiping Bao,Guanduo Chen,Jiahao Chen,Ningxin Chen,Ruijue Chen,Yanru Chen,Yuankun Chen,Yutian Chen,Zhuofu Chen,Jialei Cui,Hao Ding,Mengnan Dong,Angang Du,Chenzhuang Du,Dikang Du,Yulun Du,Yu Fan,Yichen Feng,Kelin Fu,Bofei Gao,Hongcheng Gao,Peizhong Gao,Tong Gao,Xinran Gu,Longyu Guan,Haiqing Guo,Jianhang Guo,Hao Hu,Xiaoru Hao,Tianhong He,Weiran He,Wenyang He,Chao Hong,Yangyang Hu,Zhenxing Hu,Weixiao Huang,Zhiqi Huang,Zihao Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yongsheng Kang,Guokun Lai,Cheng Li,Fang Li,Haoyang Li,Ming Li,Wentao Li,Yanhao Li,Yiwei Li,Zhaowei Li,Zheming Li,Hongzhan Lin,Xiaohan Lin,Zongyu Lin,Chengyin Liu,Chenyu Liu,Hongzhang Liu,Jingyuan Liu,Junqi Liu,Liang Liu,Shaowei Liu,T. Y. Liu,Tianwei Liu,Weizhou Liu,Yangyang Liu,Yibo Liu,Yiping Liu,Yue Liu,Zhengying Liu,Enzhe Lu,Lijun Lu,Shengling Ma,Xinyu Ma,Yingwei Ma,Shaoguang Mao,Jie Mei,Xin Men,Yibo Miao,Siyuan Pan,Yebo Peng,Ruoyu Qin,Bowen Qu,Zeyu Shang,Lidong Shi,Shengyuan Shi,Feifan Song,Jianlin Su,Zhengyuan Su,Xinjie Sun,Flood Sung,Heyi Tang,Jiawen Tao,Qifeng Teng,Chensi Wang,Dinglu Wang,Feng Wang,Haiming Wang,Jianzhou Wang,Jiaxing Wang,Jinhong Wang,Shengjie Wang,Shuyi Wang,Yao Wang,Yejie Wang,Yiqin Wang,Yuxin Wang,Yuzhi Wang,Zhaoji Wang,Zhengtao Wang,Zhexu Wang,Chu Wei,Qianqian Wei,Wenhao Wu,Xingzhe Wu,Yuxin Wu,Chenjun Xiao,Xiaotong Xie,Weimin Xiong,Boyu Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinran Xu,Yangchuan Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Xiaofei Yang,Ying Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Xingcheng Yao,Wenjie Ye,Zhuorui Ye,Bohong Yin,Longhui Yu,Enming Yuan,Hongbang Yuan,Mengjie Yuan,Haobing Zhan,Dehao Zhang,Hao Zhang,Wanlu Zhang,Xiaobin Zhang,Yangkun Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Haotian Zhao,Yikai Zhao,Huabin Zheng,Shaojie Zheng,Jianren Zhou,Xinyu Zhou,Zaida Zhou,Zhen Zhu,Weiyu Zhuang,Xinxing Zu*

Main category: cs.LG

TL;DR: Kimi K2是一个基于Mixture-of-Experts (MoE)的大型语言模型，具有32亿激活参数和1万亿总参数。通过MuonClip优化器和多阶段后训练，K2在非思考任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决训练不稳定问题并提升模型在代理任务中的能力，开发了Kimi K2。

Method: 使用MuonClip优化器（基于QK-clip技术）和多阶段后训练（包括大规模代理数据合成和强化学习）。

Result: K2在多个基准测试中表现优异，特别是在软件工程和代理任务中。

Conclusion: Kimi K2是目前最强大的开源大型语言模型之一，适用于代理智能的研究和应用。

Abstract: We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32
billion activated parameters and 1 trillion total parameters. We propose the
MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to
address training instability while enjoying the advanced token efficiency of
Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero
loss spike. During post-training, K2 undergoes a multi-stage post-training
process, highlighted by a large-scale agentic data synthesis pipeline and a
joint reinforcement learning (RL) stage, where the model improves its
capabilities through interactions with real and synthetic environments.
  Kimi K2 achieves state-of-the-art performance among open-source non-thinking
models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on
Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on
SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in
non-thinking settings. It also exhibits strong capabilities in coding,
mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6,
49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without
extended thinking. These results position Kimi K2 as one of the most capable
open-source large language models to date, particularly in software engineering
and agentic tasks. We release our base and post-trained model checkpoints to
facilitate future research and applications of agentic intelligence.

</details>


### [176] [Conditional Diffusion Models for Global Precipitation Map Inpainting](https://arxiv.org/abs/2507.20478)
*Daiko Kishikawa,Yuka Muto,Shunji Kotsuki*

Main category: cs.LG

TL;DR: 论文提出了一种基于条件扩散模型的机器学习方法，用于填补卫星降水数据中的缺失区域，通过3D U-Net和时空信息生成更一致的降水图。


<details>
  <summary>Details</summary>
Motivation: 解决全球降水监测中因卫星轨道特性导致的降水数据缺失问题，传统插值方法常导致空间不连续性。

Method: 将降水图填补任务视为视频修复问题，使用条件扩散模型和3D U-Net，结合红外图像、经纬度网格和物理时间输入进行训练。

Result: 在2024年的评估中，该方法生成的降水图比传统方法更具时空一致性。

Conclusion: 条件扩散模型有望提升全球降水监测的准确性。

Abstract: Incomplete satellite-based precipitation presents a significant challenge in
global monitoring. For example, the Global Satellite Mapping of Precipitation
(GSMaP) from JAXA suffers from substantial missing regions due to the orbital
characteristics of satellites that have microwave sensors, and its current
interpolation methods often result in spatial discontinuities. In this study,
we formulate the completion of the precipitation map as a video inpainting task
and propose a machine learning approach based on conditional diffusion models.
Our method employs a 3D U-Net with a 3D condition encoder to reconstruct
complete precipitation maps by leveraging spatio-temporal information from
infrared images, latitude-longitude grids, and physical time inputs. Training
was carried out on ERA5 hourly precipitation data from 2020 to 2023. We
generated a pseudo-GSMaP dataset by randomly applying GSMaP masks to ERA maps.
Performance was evaluated for the calendar year 2024, and our approach produces
more spatio-temporally consistent inpainted precipitation maps compared to
conventional methods. These results indicate the potential to improve global
precipitation monitoring using the conditional diffusion models.

</details>


### [177] [Dissecting Persona-Driven Reasoning in Language Models via Activation Patching](https://arxiv.org/abs/2507.20936)
*Ansh Poonia,Maeghal Jain*

Main category: cs.LG

TL;DR: 研究探讨了大型语言模型（LLM）在赋予不同角色时如何影响其客观任务的推理能力，发现早期MLP层处理语义内容，而中间MHA层利用这些信息生成输出。


<details>
  <summary>Details</summary>
Motivation: 探究角色赋予对模型推理的影响，理解模型如何编码角色信息。

Method: 使用激活修补技术分析模型的关键组件，关注MLP和MHA层的作用。

Result: 早期MLP层处理语义内容，中间MHA层利用这些信息；特定注意力头关注种族和肤色身份。

Conclusion: 角色信息通过MLP和MHA层影响模型输出，揭示了模型处理身份信息的机制。

Abstract: Large language models (LLMs) exhibit remarkable versatility in adopting
diverse personas. In this study, we examine how assigning a persona influences
a model's reasoning on an objective task. Using activation patching, we take a
first step toward understanding how key components of the model encode
persona-specific information. Our findings reveal that the early Multi-Layer
Perceptron (MLP) layers attend not only to the syntactic structure of the input
but also process its semantic content. These layers transform persona tokens
into richer representations, which are then used by the middle Multi-Head
Attention (MHA) layers to shape the model's output. Additionally, we identify
specific attention heads that disproportionately attend to racial and
color-based identities.

</details>


### [178] [HIAL: A New Paradigm for Hypergraph Active Learning via Influence Maximization](https://arxiv.org/abs/2507.20490)
*Yanheng Hou,Xunkai Li,Zhenjun Li,Bing Zhou,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: HIAL是一种专为超图设计的主动学习框架，通过将超图主动学习问题重新定义为影响力最大化任务，结合双视角影响力函数，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有图主动学习方法在超图中表现不佳，因其破坏了高阶结构信息，亟需一种专为超图设计的主动学习方法。

Method: HIAL通过双视角影响力函数（MoI和EDV）和高阶交互感知传播机制，将问题转化为单调且子模的影响力最大化任务，并使用贪婪算法求解。

Result: 在七个公开数据集上的实验表明，HIAL在性能、效率、通用性和鲁棒性上显著优于现有方法。

Conclusion: HIAL为超图主动学习提供了高效且强大的新范式。

Abstract: In recent years, Hypergraph Neural Networks (HNNs) have demonstrated immense
potential in handling complex systems with high-order interactions. However,
acquiring large-scale, high-quality labeled data for these models is costly,
making Active Learning (AL) a critical technique. Existing Graph Active
Learning (GAL) methods, when applied to hypergraphs, often rely on techniques
like "clique expansion," which destroys the high-order structural information
crucial to a hypergraph's success, thereby leading to suboptimal performance.
To address this challenge, we introduce HIAL (Hypergraph Active Learning), a
native active learning framework designed specifically for hypergraphs. We
innovatively reformulate the Hypergraph Active Learning (HAL) problem as an
Influence Maximization task. The core of HIAL is a dual-perspective influence
function that, based on our novel "High-Order Interaction-Aware (HOI-Aware)"
propagation mechanism, synergistically evaluates a node's feature-space
coverage (via Magnitude of Influence, MoI) and its topological influence (via
Expected Diffusion Value, EDV). We prove that this objective function is
monotone and submodular, thus enabling the use of an efficient greedy algorithm
with a formal (1-1/e) approximation guarantee. Extensive experiments on seven
public datasets demonstrate that HIAL significantly outperforms
state-of-the-art baselines in terms of performance, efficiency, generality, and
robustness, establishing an efficient and powerful new paradigm for active
learning on hypergraphs.

</details>


### [179] [LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning](https://arxiv.org/abs/2507.20999)
*Yining Huang,Bin Li,Keke Tang,Meilian Chen*

Main category: cs.LG

TL;DR: LoRA-PAR 是一种双系统 LoRA 框架，通过区分快速直觉任务（System 1）和多步逻辑任务（System 2），优化数据和参数分配，减少参数使用同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法未针对不同任务需求（快速直觉 vs. 深度逻辑）优化数据和参数分配。

Method: 提出 LoRA-PAR，通过多模型角色扮演和投票分类任务数据，基于重要性评分分配参数，采用两阶段微调（SFT 和 RL）。

Result: 实验表明，两阶段微调在减少参数使用的同时，性能达到或超过现有 PEFT 基线。

Conclusion: LoRA-PAR 通过任务和参数分区优化，实现了高效且高性能的微调。

Abstract: Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit
substantially from chain-of-thought (CoT) reasoning, yet pushing their
performance typically requires vast data, large model sizes, and full-parameter
fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,
most existing approaches primarily address domain adaptation or layer-wise
allocation rather than explicitly tailoring data and parameters to different
response demands. Inspired by "Thinking, Fast and Slow," which characterizes
two distinct modes of thought-System 1 (fast, intuitive, often automatic) and
System 2 (slower, more deliberative and analytic)-we draw an analogy that
different "subregions" of an LLM's parameters might similarly specialize for
tasks that demand quick, intuitive responses versus those requiring multi-step
logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework
that partitions both data and parameters by System 1 or System 2 demands, using
fewer yet more focused parameters for each task. Specifically, we classify task
data via multi-model role-playing and voting, and partition parameters based on
importance scoring, then adopt a two-stage fine-tuning strategy of training
System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and
intuition and refine System 2 tasks with reinforcement learning (RL) to
reinforce deeper logical deliberation next. Extensive experiments show that the
two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while
matching or surpassing SOTA PEFT baselines.

</details>


### [180] [Mixture of Length and Pruning Experts for Knowledge Graphs Reasoning](https://arxiv.org/abs/2507.20498)
*Enjun Du,Siyi Liu,Yongqi Zhang*

Main category: cs.LG

TL;DR: MoKGR提出了一种混合专家框架，通过自适应路径探索提升知识图谱推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在知识图谱推理中采用固定路径探索策略，难以适应多样化的语言上下文和语义细微差别。

Method: MoKGR包含两个互补组件：长度专家混合（自适应选择路径长度）和剪枝专家混合（评估并保留最有信息量的路径）。

Result: 在多种基准测试中，MoKGR在传导和归纳设置下均表现出优越性能。

Conclusion: 个性化路径探索在知识图谱推理中具有显著有效性。

Abstract: Knowledge Graph (KG) reasoning, which aims to infer new facts from structured
knowledge repositories, plays a vital role in Natural Language Processing (NLP)
systems. Its effectiveness critically depends on constructing informative and
contextually relevant reasoning paths. However, existing graph neural networks
(GNNs) often adopt rigid, query-agnostic path-exploration strategies, limiting
their ability to adapt to diverse linguistic contexts and semantic nuances. To
address these limitations, we propose \textbf{MoKGR}, a mixture-of-experts
framework that personalizes path exploration through two complementary
components: (1) a mixture of length experts that adaptively selects and weights
candidate path lengths according to query complexity, providing query-specific
reasoning depth; and (2) a mixture of pruning experts that evaluates candidate
paths from a complementary perspective, retaining the most informative paths
for each query. Through comprehensive experiments on diverse benchmark, MoKGR
demonstrates superior performance in both transductive and inductive settings,
validating the effectiveness of personalized path exploration in KGs reasoning.

</details>


### [181] [DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning](https://arxiv.org/abs/2507.20499)
*Linh Le Pham Van,Minh Hoang Nguyen,Duc Kieu,Hung Le,Hung The Tran,Sunil Gupta*

Main category: cs.LG

TL;DR: 论文提出DmC框架，通过k-NN估计域接近度和扩散模型生成对齐样本，解决跨域离线强化学习中的数据集不平衡和部分域重叠问题。


<details>
  <summary>Details</summary>
Motivation: 现有跨域离线强化学习方法需要大量目标数据，不适用于实际场景中的小目标数据集。

Method: 使用k-NN估计域接近度，结合扩散模型生成对齐样本。

Result: DmC在MuJoCo环境中显著优于现有方法。

Conclusion: DmC有效解决了小目标数据集下的跨域离线强化学习问题。

Abstract: Cross-domain offline reinforcement learning (RL) seeks to enhance sample
efficiency in offline RL by utilizing additional offline source datasets. A key
challenge is to identify and utilize source samples that are most relevant to
the target domain. Existing approaches address this challenge by measuring
domain gaps through domain classifiers, target transition dynamics modeling, or
mutual information estimation using contrastive loss. However, these methods
often require large target datasets, which is impractical in many real-world
scenarios. In this work, we address cross-domain offline RL under a limited
target data setting, identifying two primary challenges: (1) Dataset imbalance,
which is caused by large source and small target datasets and leads to
overfitting in neural network-based domain gap estimators, resulting in
uninformative measurements; and (2) Partial domain overlap, where only a subset
of the source data is closely aligned with the target domain. To overcome these
issues, we propose DmC, a novel framework for cross-domain offline RL with
limited target samples. Specifically, DmC utilizes $k$-nearest neighbor
($k$-NN) based estimation to measure domain proximity without neural network
training, effectively mitigating overfitting. Then, by utilizing this domain
proximity, we introduce a nearest-neighbor-guided diffusion model to generate
additional source samples that are better aligned with the target domain, thus
enhancing policy learning with more effective source samples. Through
theoretical analysis and extensive experiments in diverse MuJoCo environments,
we demonstrate that DmC significantly outperforms state-of-the-art cross-domain
offline RL methods, achieving substantial performance gains.

</details>


### [182] [Attributed Graph Clustering with Multi-Scale Weight-Based Pairwise Coarsening and Contrastive Learning](https://arxiv.org/abs/2507.20505)
*Binxiong Li,Yuefei Wang,Binyu Zhao,Heyang Gao,Benhan Yang,Quanzhou Luo,Xue Li,Xu Xiang,Yujie Liu,Huijie Tang*

Main category: cs.LG

TL;DR: MPCCL模型通过多尺度粗化和对比学习解决图聚类中的长程依赖、特征崩溃和信息丢失问题，显著提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉高阶图特征，对比学习过度依赖局部结构，传统粗化方法丢失细节。MPCCL旨在解决这些问题。

Method: 采用多尺度粗化策略，基于全局节点相似性合并关键边；引入一对多对比学习范式，结合节点嵌入和增强图视图。

Result: 在ACM数据集上NMI提升15.24%，在Citeseer、Cora和DBLP等小规模数据集上表现稳健。

Conclusion: MPCCL通过创新方法有效提升图聚类性能，解决了现有技术的局限性。

Abstract: This study introduces the Multi-Scale Weight-Based Pairwise Coarsening and
Contrastive Learning (MPCCL) model, a novel approach for attributed graph
clustering that effectively bridges critical gaps in existing methods,
including long-range dependency, feature collapse, and information loss.
Traditional methods often struggle to capture high-order graph features due to
their reliance on low-order attribute information, while contrastive learning
techniques face limitations in feature diversity by overemphasizing local
neighborhood structures. Similarly, conventional graph coarsening methods,
though reducing graph scale, frequently lose fine-grained structural details.
MPCCL addresses these challenges through an innovative multi-scale coarsening
strategy, which progressively condenses the graph while prioritizing the
merging of key edges based on global node similarity to preserve essential
structural information. It further introduces a one-to-many contrastive
learning paradigm, integrating node embeddings with augmented graph views and
cluster centroids to enhance feature diversity, while mitigating feature
masking issues caused by the accumulation of high-frequency node weights during
multi-scale coarsening. By incorporating a graph reconstruction loss and KL
divergence into its self-supervised learning framework, MPCCL ensures
cross-scale consistency of node representations. Experimental evaluations
reveal that MPCCL achieves a significant improvement in clustering performance,
including a remarkable 15.24% increase in NMI on the ACM dataset and notable
robust gains on smaller-scale datasets such as Citeseer, Cora and DBLP.

</details>


### [183] [Efficient Proxy Raytracer for Optical Systems using Implicit Neural Representations](https://arxiv.org/abs/2507.20513)
*Shiva Sinaei,Chuanjun Zheng,Kaan Akşit,Daisuke Iwai*

Main category: cs.LG

TL;DR: Ray2Ray利用隐式神经表示高效建模光学系统，避免了逐表面计算，实现了物理精确的射线映射。


<details>
  <summary>Details</summary>
Motivation: 传统光线追踪技术计算密集，逐表面计算效率低，需要更高效的方法。

Method: 提出Ray2Ray，通过隐式神经表示学习光源射线与光学系统输出射线的映射关系。

Result: 在9个现成光学系统上训练，位置误差约1微米，角度偏差约0.01度。

Conclusion: 神经表示有望替代传统光线追踪器。

Abstract: Ray tracing is a widely used technique for modeling optical systems,
involving sequential surface-by-surface computations, which can be
computationally intensive. We propose Ray2Ray, a novel method that leverages
implicit neural representations to model optical systems with greater
efficiency, eliminating the need for surface-by-surface computations in a
single pass end-to-end model. Ray2Ray learns the mapping between rays emitted
from a given source and their corresponding rays after passing through a given
optical system in a physically accurate manner. We train Ray2Ray on nine
off-the-shelf optical systems, achieving positional errors on the order of
1{\mu}m and angular deviations on the order 0.01 degrees in the estimated
output rays. Our work highlights the potential of neural representations as a
proxy for optical raytracer.

</details>


### [184] [Kernel Learning for Sample Constrained Black-Box Optimization](https://arxiv.org/abs/2507.20533)
*Rajalaxmi Rajagopalan,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.LG

TL;DR: 提出了一种新的高斯过程核学习方法，通过变分自编码器的潜在空间构建连续核空间，并运行辅助优化以确定最佳核，显著降低了样本预算。


<details>
  <summary>Details</summary>
Motivation: 在黑盒优化中，采样未知函数成本高昂，现有方法通过学习函数形状/结构（核学习）来减少样本预算。

Method: 在变分自编码器的潜在空间中构建连续核空间，运行辅助优化以确定最佳核。

Result: 提出的KOBO方法在较低样本预算下优于现有技术，适用于合成基准函数和实际应用（如助听器个性化）。

Conclusion: KOBO方法通过优化核学习，显著提升了黑盒优化的效率，适用于多种实际场景。

Abstract: Black box optimization (BBO) focuses on optimizing unknown functions in
high-dimensional spaces. In many applications, sampling the unknown function is
expensive, imposing a tight sample budget. Ongoing work is making progress on
reducing the sample budget by learning the shape/structure of the function,
known as kernel learning. We propose a new method to learn the kernel of a
Gaussian Process. Our idea is to create a continuous kernel space in the latent
space of a variational autoencoder, and run an auxiliary optimization to
identify the best kernel. Results show that the proposed method, Kernel
Optimized Blackbox Optimization (KOBO), outperforms state of the art by
estimating the optimal at considerably lower sample budgets. Results hold not
only across synthetic benchmark functions but also in real applications. We
show that a hearing aid may be personalized with fewer audio queries to the
user, or a generative model could converge to desirable images from limited
user ratings.

</details>


### [185] [Improving Group Fairness in Tensor Completion via Imbalance Mitigating Entity Augmentation](https://arxiv.org/abs/2507.20542)
*Dawon Ahn,Jun-Gi Jang,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 论文提出STAFF方法，通过稀疏张量增强改善张量分解中的群体公平性，同时减少整体完成误差。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解方法在群体公平性上表现不佳，导致性能下降。

Method: 通过增强张量中的实体，增加足够观测条目以减少不平衡和群体偏见。

Result: STAFF在完成误差和公平性上表现最佳，MSE和MADE分别降低36%和59%。

Conclusion: STAFF在张量分解中实现了公平性与性能的最佳平衡。

Abstract: Group fairness is important to consider in tensor decomposition to prevent
discrimination based on social grounds such as gender or age. Although few
works have studied group fairness in tensor decomposition, they suffer from
performance degradation. To address this, we propose STAFF(Sparse Tensor
Augmentation For Fairness) to improve group fairness by minimizing the gap in
completion errors of different groups while reducing the overall tensor
completion error. Our main idea is to augment a tensor with augmented entities
including sufficient observed entries to mitigate imbalance and group bias in
the sparse tensor. We evaluate \method on tensor completion with various
datasets under conventional and deep learning-based tensor models. STAFF
consistently shows the best trade-off between completion error and group
fairness; at most, it yields 36% lower MSE and 59% lower MADE than the
second-best baseline.

</details>


### [186] [DAG-AFL:Directed Acyclic Graph-based Asynchronous Federated Learning](https://arxiv.org/abs/2507.20571)
*Shuaipeng Zhang,Lanju Kong,Yixin Zhang,Wei He,Yongqing Zheng,Han Yu,Lizhen Cui*

Main category: cs.LG

TL;DR: 提出了一种基于有向无环图（DAG）的异步联邦学习框架（DAG-AFL），通过优化资源消耗和提升模型准确性，解决了传统区块链联邦学习的效率问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）的分布式特性导致全局模型易受攻击且客户端协调困难，而传统区块链共识机制（如PoW）资源消耗大，限制了FL的效率。

Method: 设计了考虑时间新鲜度、节点可达性和模型准确性的DAG-AFL框架，包含一种tip选择算法和基于DAG的可信验证策略。

Result: 在三个基准数据集上对比八种先进方法，DAG-AFL平均提升训练效率22.7%和模型准确性6.5%。

Conclusion: DAG-AFL有效解决了异步客户端参与和数据异质性问题，同时减少了区块链引入的资源开销，显著提升了FL的性能。

Abstract: Due to the distributed nature of federated learning (FL), the vulnerability
of the global model and the need for coordination among many client devices
pose significant challenges. As a promising decentralized, scalable and secure
solution, blockchain-based FL methods have attracted widespread attention in
recent years. However, traditional consensus mechanisms designed for Proof of
Work (PoW) similar to blockchain incur substantial resource consumption and
compromise the efficiency of FL, particularly when participating devices are
wireless and resource-limited. To address asynchronous client participation and
data heterogeneity in FL, while limiting the additional resource overhead
introduced by blockchain, we propose the Directed Acyclic Graph-based
Asynchronous Federated Learning (DAG-AFL) framework. We develop a tip selection
algorithm that considers temporal freshness, node reachability and model
accuracy, with a DAG-based trusted verification strategy. Extensive experiments
on 3 benchmarking datasets against eight state-of-the-art approaches
demonstrate that DAG-AFL significantly improves training efficiency and model
accuracy by 22.7% and 6.5% on average, respectively.

</details>


### [187] [Reminiscence Attack on Residuals: Exploiting Approximate Machine Unlearning for Privacy](https://arxiv.org/abs/2507.20573)
*Yaxin Xiao,Qingqing Ye,Li Hu,Huadi Zheng,Haibo Hu,Zi Liang,Haoyang Li,Yijie Jiao*

Main category: cs.LG

TL;DR: 论文揭示了近似遗忘算法在保护未学习数据隐私方面的不足，提出了Reminiscence Attack（ReA）攻击方法，并开发了一种双阶段近似遗忘框架以降低隐私风险。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于发现近似遗忘算法未能充分保护未学习数据的隐私，存在隐式残差导致新的攻击面。

Method: 提出ReA攻击方法，通过针对性微调放大残差与成员隐私的关联；开发双阶段近似遗忘框架，消除深层未学习数据痕迹并防止伪收敛。

Result: ReA攻击在推断类级和样本级成员隐私时分别比现有攻击高1.90倍和1.12倍；双阶段框架将自适应隐私攻击准确率降至接近随机猜测，计算成本仅为完全重新训练的2-12%。

Conclusion: 论文表明近似遗忘算法存在隐私风险，提出的双阶段框架在保持高效遗忘的同时显著降低了隐私攻击风险。

Abstract: Machine unlearning enables the removal of specific data from ML models to
uphold the right to be forgotten. While approximate unlearning algorithms offer
efficient alternatives to full retraining, this work reveals that they fail to
adequately protect the privacy of unlearned data. In particular, these
algorithms introduce implicit residuals which facilitate privacy attacks
targeting at unlearned data. We observe that these residuals persist regardless
of model architectures, parameters, and unlearning algorithms, exposing a new
attack surface beyond conventional output-based leakage. Based on this insight,
we propose the Reminiscence Attack (ReA), which amplifies the correlation
between residuals and membership privacy through targeted fine-tuning
processes. ReA achieves up to 1.90x and 1.12x higher accuracy than prior
attacks when inferring class-wise and sample-wise membership, respectively. To
mitigate such residual-induced privacy risk, we develop a dual-phase
approximate unlearning framework that first eliminates deep-layer unlearned
data traces and then enforces convergence stability to prevent models from
"pseudo-convergence", where their outputs are similar to retrained models but
still preserve unlearned residuals. Our framework works for both classification
and generation tasks. Experimental evaluations confirm that our approach
maintains high unlearning efficacy, while reducing the adaptive privacy attack
accuracy to nearly random guess, at the computational cost of 2-12% of full
retraining from scratch.

</details>


### [188] [Fusing CFD and measurement data using transfer learning](https://arxiv.org/abs/2507.20576)
*Alexander Barklage,Philipp Bekemeyer*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的非线性数据融合方法，结合仿真和测量数据，通过迁移学习提高气动分析的精度和空间分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有气动分析方法（如POD）为线性方法，无法有效结合高分辨率仿真数据和高精度测量数据的优势，因此需要一种非线性方法。

Method: 使用神经网络分两步训练：先在仿真数据上学习空间特征，再通过迁移学习在测量数据上修正系统误差，仅重新训练部分网络。

Result: 相比传统POD方法，该方法在非线性区域生成更物理的解决方案，并能适应任意流动条件，适用于飞行力学设计等应用。

Conclusion: 该训练策略具有通用性，未来可应用于更复杂的神经网络架构。

Abstract: Aerodynamic analysis during aircraft design usually involves methods of
varying accuracy and spatial resolution, which all have their advantages and
disadvantages. It is therefore desirable to create data-driven models which
effectively combine these advantages. Such data fusion methods for distributed
quantities mainly rely on proper orthogonal decomposition as of now, which is a
linear method. In this paper, we introduce a non-linear method based on neural
networks combining simulation and measurement data via transfer learning. The
network training accounts for the heterogeneity of the data, as simulation data
usually features a high spatial resolution, while measurement data is sparse
but more accurate. In a first step, the neural network is trained on simulation
data to learn spatial features of the distributed quantities. The second step
involves transfer learning on the measurement data to correct for systematic
errors between simulation and measurement by only re-training a small subset of
the entire neural network model. This approach is applied to a multilayer
perceptron architecture and shows significant improvements over the established
method based on proper orthogonal decomposition by producing more physical
solutions near nonlinearities. In addition, the neural network provides
solutions at arbitrary flow conditions, thus making the model useful for flight
mechanical design, structural sizing, and certification. As the proposed
training strategy is very general, it can also be applied to more complex
neural network architectures in the future.

</details>


### [189] [PhaseNAS: Language-Model Driven Architecture Search with Dynamic Phase Adaptation](https://arxiv.org/abs/2507.20592)
*Fei Kong,Xiaohan Shan,Yanwei Hu,Jianmin Li*

Main category: cs.LG

TL;DR: PhaseNAS是一个基于LLM的NAS框架，通过动态阶段转换和结构化模板语言提升搜索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM-based NAS方法中静态搜索策略和模糊架构表示的问题。

Method: 采用动态阶段转换（实时评分阈值指导）和结构化架构模板语言。

Result: 在NAS-Bench-Macro上表现优异，CIFAR-10/100搜索时间减少86%，YOLOv8变体性能提升。

Conclusion: PhaseNAS实现了高效、自适应且通用的NAS，适用于多种视觉任务。

Abstract: Neural Architecture Search (NAS) is challenged by the trade-off between
search space exploration and efficiency, especially for complex tasks. While
recent LLM-based NAS methods have shown promise, they often suffer from static
search strategies and ambiguous architecture representations. We propose
PhaseNAS, an LLM-based NAS framework with dynamic phase transitions guided by
real-time score thresholds and a structured architecture template language for
consistent code generation. On the NAS-Bench-Macro benchmark, PhaseNAS
consistently discovers architectures with higher accuracy and better rank. For
image classification (CIFAR-10/100), PhaseNAS reduces search time by up to 86%
while maintaining or improving accuracy. In object detection, it automatically
produces YOLOv8 variants with higher mAP and lower resource cost. These results
demonstrate that PhaseNAS enables efficient, adaptive, and generalizable NAS
across diverse vision tasks.

</details>


### [190] [Deep Generative Models of Evolution: SNP-level Population Adaptation by Genomic Linkage Incorporation](https://arxiv.org/abs/2507.20644)
*Julia Siekiera,Christian Schlötterer,Stefan Kramer*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度生成神经网络的模型，用于模拟和估计等位基因频率轨迹的分布，特别适用于基于Pool-Seq数据的E&R实验，并展示了其在连锁不平衡（LD）估计中的优势。


<details>
  <summary>Details</summary>
Motivation: 经典统计模型（如Wright-Fisher模型）在种群基因组学中存在简化假设和参数不确定性的问题，而深度生成神经网络能够整合多变量依赖性和降噪，但因其高数据需求和解释性挑战而未被广泛应用。

Method: 论文引入了一种深度生成神经网络，通过嵌入单核苷酸多态性（SNPs）的观测数据及其邻近位点信息，模拟基于时间经验观察的进化概念。

Result: 模型在模拟E&R实验中成功捕捉了等位基因频率轨迹的分布，并在Pool-Seq数据中实现了竞争性的连锁不平衡（LD）估计。

Conclusion: 深度生成模型在种群基因组学中具有潜力，特别是在处理Pool-Seq数据时，能够提供传统方法难以获取的LD信息。

Abstract: The investigation of allele frequency trajectories in populations evolving
under controlled environmental pressures has become a popular approach to study
evolutionary processes on the molecular level. Statistical models based on
well-defined evolutionary concepts can be used to validate different hypotheses
about empirical observations. Despite their popularity, classic statistical
models like the Wright-Fisher model suffer from simplified assumptions such as
the independence of selected loci along a chromosome and uncertainty about the
parameters. Deep generative neural networks offer a powerful alternative known
for the integration of multivariate dependencies and noise reduction. Due to
their high data demands and challenging interpretability they have, so far, not
been widely considered in the area of population genomics. To address the
challenges in the area of Evolve and Resequencing experiments (E&R) based on
pooled sequencing (Pool-Seq) data, we introduce a deep generative neural
network that aims to model a concept of evolution based on empirical
observations over time. The proposed model estimates the distribution of allele
frequency trajectories by embedding the observations from single nucleotide
polymorphisms (SNPs) with information from neighboring loci. Evaluation on
simulated E&R experiments demonstrates the model's ability to capture the
distribution of allele frequency trajectories and illustrates the
representational power of deep generative models on the example of linkage
disequilibrium (LD) estimation. Inspecting the internally learned
representations enables estimating pairwise LD, which is typically inaccessible
in Pool-Seq data. Our model provides competitive LD estimation in Pool-Seq data
high degree of LD when compared to existing methods.

</details>


### [191] [Novel Pivoted Cholesky Decompositions for Efficient Gaussian Process Inference](https://arxiv.org/abs/2507.20678)
*Filip de Roos,Fabio Muratore*

Main category: cs.LG

TL;DR: 论文提出了一种新的Cholesky分解的枢轴选择策略，通过改进数值稳定性，在稀疏回归和迭代求解器中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: Cholesky分解在对称正定矩阵的线性系统中至关重要，但其数值稳定性可通过枢轴策略改进。传统方法基于对角线最大值选择枢轴，本文探索了与贝叶斯非参数推断的关联，提出更高效的策略。

Method: 通过分析枢轴顺序对分解精度的影响，提出新的枢轴选择策略，并将其与贪婪熵最大化联系起来。新策略可动态更新观测信息，并优化实现。

Result: 在稀疏回归和基于预条件迭代求解器的任务中，新策略与传统方法相当或更优，且计算开销可忽略。

Conclusion: 新枢轴策略显著提升了Cholesky分解的效率，尤其在数据不确定性减少方面表现突出，适用于高斯过程等任务。

Abstract: The Cholesky decomposition is a fundamental tool for solving linear systems
with symmetric and positive definite matrices which are ubiquitous in linear
algebra, optimization, and machine learning. Its numerical stability can be
improved by introducing a pivoting strategy that iteratively permutes the rows
and columns of the matrix. The order of pivoting indices determines how
accurately the intermediate decomposition can reconstruct the original matrix,
thus is decisive for the algorithm's efficiency in the case of early
termination. Standard implementations select the next pivot from the largest
value on the diagonal. In the case of Bayesian nonparametric inference, this
strategy corresponds to greedy entropy maximization, which is often used in
active learning and design of experiments. We explore this connection in detail
and deduce novel pivoting strategies for the Cholesky decomposition. The
resulting algorithms are more efficient at reducing the uncertainty over a data
set, can be updated to include information about observations, and additionally
benefit from a tailored implementation. We benchmark the effectiveness of the
new selection strategies on two tasks important to Gaussian processes: sparse
regression and inference based on preconditioned iterative solvers. Our results
show that the proposed selection strategies are either on par or, in most
cases, outperform traditional baselines while requiring a negligible amount of
additional computation.

</details>


### [192] [Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks](https://arxiv.org/abs/2507.20708)
*Valentin Lafargue,Adriana Laurindo Monteiro,Emmanuelle Claeys,Laurent Risser,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 论文探讨了如何通过操纵数据样本以人为满足公平性标准，并研究了如何检测此类操纵。


<details>
  <summary>Details</summary>
Motivation: 随着AI算法在现实应用中的广泛部署，证明其合规性成为重要挑战，尤其是满足欧盟AI法案的公平性要求。

Method: 提出了基于熵或最优传输投影的数学方法，用于在公平性约束下修改经验分布，并研究了如何检测数据操纵。

Result: 验证了通过操纵数据样本可以人为满足公平性标准，同时提供了检测此类操纵的建议。

Conclusion: 研究为审计人员提供了检测数据操纵的工具，同时揭示了潜在规避公平性检查的方法。

Abstract: Proving the compliance of AI algorithms has become an important challenge
with the growing deployment of such algorithms for real-life applications.
Inspecting possible biased behaviors is mandatory to satisfy the constraints of
the regulations of the EU Artificial Intelligence's Act. Regulation-driven
audits increasingly rely on global fairness metrics, with Disparate Impact
being the most widely used. Yet such global measures depend highly on the
distribution of the sample on which the measures are computed. We investigate
first how to manipulate data samples to artificially satisfy fairness criteria,
creating minimally perturbed datasets that remain statistically
indistinguishable from the original distribution while satisfying prescribed
fairness constraints. Then we study how to detect such manipulation. Our
analysis (i) introduces mathematically sound methods for modifying empirical
distributions under fairness constraints using entropic or optimal transport
projections, (ii) examines how an auditee could potentially circumvent fairness
inspections, and (iii) offers recommendations to help auditors detect such data
manipulations. These results are validated through experiments on classical
tabular datasets in bias detection.

</details>


### [193] [Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI](https://arxiv.org/abs/2507.20714)
*Asma Sadia Khan,Fariba Tasnia Khan,Tanjim Mahmud,Salman Karim Khan,Rishita Chakma,Nahed Sharmen,Mohammad Shahadat Hossain,Karl Andersson*

Main category: cs.LG

TL;DR: 提出了一种结合BERT和随机森林的可解释AI系统，用于前列腺癌诊断，性能优越（准确率98%，AUC 99%）。


<details>
  <summary>Details</summary>
Motivation: 前列腺癌诊断需要先进工具，现有方法缺乏可解释性和性能平衡。

Method: 通过多模态融合策略结合BERT（处理临床文本）和随机森林（处理实验室数据）。

Result: 在PLCO-NIH数据集上表现优异，尤其对中期癌症阶段（Class 2/3召回率0.900）。

Conclusion: 该方法在性能、效率和可解释性上取得了平衡，适用于临床诊断。

Abstract: Prostate cancer, the second most prevalent male malignancy, requires advanced
diagnostic tools. We propose an explainable AI system combining BERT (for
textual clinical notes) and Random Forest (for numerical lab data) through a
novel multimodal fusion strategy, achieving superior classification performance
on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is
established, our work demonstrates that a simple yet interpretable BERT+RF
pipeline delivers clinically significant improvements - particularly for
intermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824
numerical/0.725 textual). SHAP analysis provides transparent feature importance
rankings, while ablation studies prove textual features' complementary value.
This accessible approach offers hospitals a balance of high performance
(F1=89%), computational efficiency, and clinical interpretability - addressing
critical needs in prostate cancer diagnostics.

</details>


### [194] [Uncertainty-driven Embedding Convolution](https://arxiv.org/abs/2507.20718)
*Sungjun Lim,Kangjun Noh,Youngjun Choi,Heeyoung Lee,Kyungwoo Song*

Main category: cs.LG

TL;DR: 论文提出了一种名为UEC的方法，通过将确定性嵌入转化为概率嵌入，并基于不确定性计算自适应权重，提升了嵌入模型的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型在不同领域表现不一，且现有集成方法未考虑模型不确定性，限制了其鲁棒性和可靠性。

Method: UEC将确定性嵌入转化为概率嵌入，基于不确定性计算自适应权重，并引入不确定性感知的相似性函数。

Result: 在检索、分类和语义相似性基准测试中，UEC显著提升了性能和鲁棒性。

Conclusion: UEC通过不确定性建模，为嵌入集成提供了一种更可靠和高效的方法。

Abstract: Text embeddings are essential components in modern NLP pipelines. While
numerous embedding models have been proposed, their performance varies across
domains, and no single model consistently excels across all tasks. This
variability motivates the use of ensemble techniques to combine complementary
strengths. However, most existing ensemble methods operate on deterministic
embeddings and fail to account for model-specific uncertainty, limiting their
robustness and reliability in downstream applications. To address these
limitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC
first transforms deterministic embeddings into probabilistic ones in a post-hoc
manner. It then computes adaptive ensemble weights based on embedding
uncertainty, grounded in a Bayes-optimal solution under a surrogate loss.
Additionally, UEC introduces an uncertainty-aware similarity function that
directly incorporates uncertainty into similarity scoring. Extensive
experiments on retrieval, classification, and semantic similarity benchmarks
demonstrate that UEC consistently improves both performance and robustness by
leveraging principled uncertainty modeling.

</details>


### [195] [First Hallucination Tokens Are Different from Conditional Ones](https://arxiv.org/abs/2507.20836)
*Jakob Snel,Seong Joon Oh*

Main category: cs.LG

TL;DR: 论文研究了基础模型中幻觉（生成不真实内容）的检测，重点关注令牌级别的信号变化，发现首个幻觉令牌信号更强且更易检测。


<details>
  <summary>Details</summary>
Motivation: 幻觉是基础模型的主要问题之一，理解令牌级别的幻觉信号对实时过滤和针对性修正至关重要。

Method: 利用RAGTruth语料库的令牌级注释和重现的logits，分析幻觉信号如何依赖于令牌在幻觉序列中的位置。

Result: 首个幻觉令牌的信号更强且更易检测，而条件令牌的信号较弱。

Conclusion: 研究改进了对令牌级幻觉的理解，并发布了分析框架和代码。

Abstract: Hallucination, the generation of untruthful content, is one of the major
concerns regarding foundational models. Detecting hallucinations at the token
level is vital for real-time filtering and targeted correction, yet the
variation of hallucination signals within token sequences is not fully
understood. Leveraging the RAGTruth corpus with token-level annotations and
reproduced logits, we analyse how these signals depend on a token's position
within hallucinated spans, contributing to an improved understanding of
token-level hallucination. Our results show that the first hallucinated token
carries a stronger signal and is more detectable than conditional tokens. We
release our analysis framework, along with code for logit reproduction and
metric computation at https://github.com/jakobsnl/RAGTruth_Xtended.

</details>


### [196] [BuildSTG: A Multi-building Energy Load Forecasting Method using Spatio-Temporal Graph Neural Network](https://arxiv.org/abs/2507.20838)
*Yongzheng Liu,Yiming Wang,Po Xu,Yingjie Xu,Yuntian Chen,Dongxiao Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于时空图神经网络的多建筑能耗预测方法，通过图表示、学习和解释，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉建筑间的空间依赖性能耗模式，数据驱动方法因数据丰富而展现出强大潜力。

Method: 构建基于建筑特征和环境因素的图，开发带注意力的多层次图卷积架构，并引入图结构解释方法。

Result: 在Building Data Genome Project 2数据集上表现优于XGBoost、SVR等基线方法，具有鲁棒性、泛化性和可解释性。

Conclusion: 该方法能有效捕捉建筑间的相似性和空间关系，为能耗预测提供了新思路。

Abstract: Due to the extensive availability of operation data, data-driven methods show
strong capabilities in predicting building energy loads. Buildings with similar
features often share energy patterns, reflected by spatial dependencies in
their operational data, which conventional prediction methods struggle to
capture. To overcome this, we propose a multi-building prediction approach
using spatio-temporal graph neural networks, comprising graph representation,
graph learning, and interpretation. First, a graph is built based on building
characteristics and environmental factors. Next, a multi-level graph
convolutional architecture with attention is developed for energy prediction.
Lastly, a method interpreting the optimized graph structure is introduced.
Experiments on the Building Data Genome Project 2 dataset confirm superior
performance over baselines such as XGBoost, SVR, FCNN, GRU, and Naive,
highlighting the method's robustness, generalization, and interpretability in
capturing meaningful building similarities and spatial relationships.

</details>


### [197] [Towards Explainable Deep Clustering for Time Series Data](https://arxiv.org/abs/2507.20840)
*Udo Schlegel,Gabriel Marques Tavares,Thomas Seidl*

Main category: cs.LG

TL;DR: 本文综述了可解释的深度聚类在时间序列数据中的应用，总结了现有方法及其局限性，并提出了六个未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 深度聚类在时间序列分析中能发现隐藏模式，但其不透明的决策限制了在安全关键领域的应用，因此需要提高其可解释性。

Method: 通过收集和分析同行评审及预印本论文，比较了自编码器和注意力架构等方法在医疗、金融、物联网和气候科学等领域的应用。

Result: 研究发现现有方法对流式、不规则采样或隐私保护的时间序列支持有限，且可解释性通常作为附加功能处理。

Conclusion: 提出将可解释性作为设计核心目标，并提出了六个研究方向，以推动可信赖的深度聚类时间序列分析的发展。

Abstract: Deep clustering uncovers hidden patterns and groups in complex time series
data, yet its opaque decision-making limits use in safety-critical settings.
This survey offers a structured overview of explainable deep clustering for
time series, collecting current methods and their real-world applications. We
thoroughly discuss and compare peer-reviewed and preprint papers through
application domains across healthcare, finance, IoT, and climate science. Our
analysis reveals that most work relies on autoencoder and attention
architectures, with limited support for streaming, irregularly sampled, or
privacy-preserved series, and interpretability is still primarily treated as an
add-on. To push the field forward, we outline six research opportunities: (1)
combining complex networks with built-in interpretability; (2) setting up
clear, faithfulness-focused evaluation metrics for unsupervised explanations;
(3) building explainers that adapt to live data streams; (4) crafting
explanations tailored to specific domains; (5) adding human-in-the-loop methods
that refine clusters and explanations together; and (6) improving our
understanding of how time series clustering models work internally. By making
interpretability a primary design goal rather than an afterthought, we propose
the groundwork for the next generation of trustworthy deep clustering time
series analytics.

</details>


### [198] [Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces](https://arxiv.org/abs/2507.20853)
*Saket Tiwari,Omer Gottesman,George Konidaris*

Main category: cs.LG

TL;DR: 论文通过几何视角研究连续状态和动作空间的强化学习，发现训练动态诱导的低维流形与动作空间维度相关，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在连续状态和动作空间任务中取得了成功，但理论研究主要集中在有限空间。本文旨在填补这一空白。

Method: 采用几何视角分析局部可达状态集，证明两层神经策略训练动态诱导的低维流形与动作空间维度相关，并通过实验验证。

Result: 理论证明了流形维度与动作空间维度同阶，实验在MuJoCo环境和玩具环境中验证了这一上界。

Conclusion: 通过引入局部流形学习层，改进了高自由度控制环境的性能，为连续空间强化学习提供了新的理论支持。

Abstract: Advances in reinforcement learning (RL) have led to its successful
application in complex tasks with continuous state and action spaces. Despite
these advances in practice, most theoretical work pertains to finite state and
action spaces. We propose building a theoretical understanding of continuous
state and action spaces by employing a geometric lens to understand the locally
attained set of states. The set of all parametrised policies learnt through a
semi-gradient based approach induces a set of attainable states in RL. We show
that the training dynamics of a two-layer neural policy induce a low
dimensional manifold of attainable states embedded in the high-dimensional
nominal state space trained using an actor-critic algorithm. We prove that,
under certain conditions, the dimensionality of this manifold is of the order
of the dimensionality of the action space. This is the first result of its
kind, linking the geometry of the state space to the dimensionality of the
action space. We empirically corroborate this upper bound for four MuJoCo
environments and also demonstrate the results in a toy environment with varying
dimensionality. We also show the applicability of this theoretical result by
introducing a local manifold learning layer to the policy and value function
networks to improve the performance in control environments with very high
degrees of freedom by changing one layer of the neural network to learn sparse
representations.

</details>


### [199] [Bi-cephalic self-attended model to classify Parkinson's disease patients with freezing of gait](https://arxiv.org/abs/2507.20862)
*Shomoita Jahid Mitin,Rodrigue Rizk,Maximilian Scherer,Thomas Koeglsperger,Daniel Lench,KC Santosh,Arun Singh*

Main category: cs.LG

TL;DR: 开发了一种基于多模态数据的BiSAM模型，结合EEG信号和临床变量，显著提高了帕金森病冻结步态（PDFOG+）的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主观或依赖专业设备，需要一种客观、数据驱动的检测方法。

Method: 使用124名参与者的EEG信号和临床变量，训练BiSAM模型，测试三种模态（信号、描述、多模态）。

Result: 多模态模型（BiSAM-8和BiSAM-4）分类准确率最高（88%），显著优于单模态模型。

Conclusion: 多模态BiSAM模型为PDFOG+检测提供了高效、可扩展的解决方案，适用于临床监测和早期诊断。

Abstract: Parkinson Disease (PD) often results in motor and cognitive impairments,
including gait dysfunction, particularly in patients with freezing of gait
(FOG). Current detection methods are either subjective or reliant on
specialized gait analysis tools. This study aims to develop an objective,
data-driven, and multi-modal classification model to detect gait dysfunction in
PD patients using resting-state EEG signals combined with demographic and
clinical variables. We utilized a dataset of 124 participants: 42 PD patients
with FOG (PDFOG+), 41 without FOG (PDFOG-), and 41 age-matched healthy
controls. Features extracted from resting-state EEG and descriptive variables
(age, education, disease duration) were used to train a novel Bi-cephalic
Self-Attention Model (BiSAM). We tested three modalities: signal-only,
descriptive-only, and multi-modal, across different EEG channel subsets
(BiSAM-63, -16, -8, and -4). Signal-only and descriptive-only models showed
limited performance, achieving a maximum accuracy of 55% and 68%, respectively.
In contrast, the multi-modal models significantly outperformed both, with
BiSAM-8 and BiSAM-4 achieving the highest classification accuracy of 88%. These
results demonstrate the value of integrating EEG with objective descriptive
features for robust PDFOG+ detection. This study introduces a multi-modal,
attention-based architecture that objectively classifies PDFOG+ using minimal
EEG channels and descriptive variables. This approach offers a scalable and
efficient alternative to traditional assessments, with potential applications
in routine clinical monitoring and early diagnosis of PD-related gait
dysfunction.

</details>


### [200] [Online hierarchical partitioning of the output space in extreme multi-label data stream](https://arxiv.org/abs/2507.20894)
*Lara Neves,Afonso Lourenço,Alberto Cano,Goreti Marreiros*

Main category: cs.LG

TL;DR: iHOMER是一种在线多标签学习框架，通过动态聚类和漂移检测机制，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多标签数据流中的分布演化、高维标签空间和复杂标签依赖关系带来了挑战，需要动态适应模型。

Method: iHOMER使用在线分层聚类和全局树学习器，结合漂移检测机制，动态调整标签分区。

Result: 在23个数据集上，iHOMER比5种全局基线方法提升23%，比12种局部基线方法提升32%。

Conclusion: iHOMER在多标签分类任务中表现出色，具有动态适应性和鲁棒性。

Abstract: Mining data streams with multi-label outputs poses significant challenges due
to evolving distributions, high-dimensional label spaces, sparse label
occurrences, and complex label dependencies. Moreover, concept drift affects
not only input distributions but also label correlations and imbalance ratios
over time, complicating model adaptation. To address these challenges,
structured learners are categorized into local and global methods. Local
methods break down the task into simpler components, while global methods adapt
the algorithm to the full output space, potentially yielding better predictions
by exploiting label correlations. This work introduces iHOMER (Incremental
Hierarchy Of Multi-label Classifiers), an online multi-label learning framework
that incrementally partitions the label space into disjoint, correlated
clusters without relying on predefined hierarchies. iHOMER leverages online
divisive-agglomerative clustering based on \textit{Jaccard} similarity and a
global tree-based learner driven by a multivariate \textit{Bernoulli} process
to guide instance partitioning. To address non-stationarity, it integrates
drift detection mechanisms at both global and local levels, enabling dynamic
restructuring of label partitions and subtrees. Experiments across 23
real-world datasets show iHOMER outperforms 5 state-of-the-art global
baselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\%, and 12 local
baselines, such as binary relevance transformations of kNN, EFDT, ARF, and
ADWIN bagging/boosting ensembles, by 32\%, establishing its robustness for
online multi-label classification.

</details>


### [201] [Modeling User Behavior from Adaptive Surveys with Supplemental Context](https://arxiv.org/abs/2507.20919)
*Aman Shukla,Daniel Patrick Scantlebury,Rishabh Kumar*

Main category: cs.LG

TL;DR: LANTERN是一种模块化架构，通过融合自适应调查响应与上下文信号来建模用户行为，优于仅依赖调查的基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法因用户疲劳和不完整响应而受限，需要更高效的行为建模方法。

Method: LANTERN采用选择性门控、残差连接和跨注意力机制，将调查数据作为主要信号，仅在相关时引入外部模态。

Result: LANTERN在多标签预测中优于仅依赖调查的基线方法，并通过消融实验验证了选择性模态依赖的优势。

Conclusion: LANTERN为以调查为中心的应用提供了实用且可扩展的行为建模方案。

Abstract: Modeling user behavior is critical across many industries where understanding
preferences, intent, or decisions informs personalization, targeting, and
strategic outcomes. Surveys have long served as a classical mechanism for
collecting such behavioral data due to their interpretability, structure, and
ease of deployment. However, surveys alone are inherently limited by user
fatigue, incomplete responses, and practical constraints on their length making
them insufficient for capturing user behavior. In this work, we present LANTERN
(Late-Attentive Network for Enriched Response Modeling), a modular architecture
for modeling user behavior by fusing adaptive survey responses with
supplemental contextual signals. We demonstrate the architectural value of
maintaining survey primacy through selective gating, residual connections and
late fusion via cross-attention, treating survey data as the primary signal
while incorporating external modalities only when relevant. LANTERN outperforms
strong survey-only baselines in multi-label prediction of survey responses. We
further investigate threshold sensitivity and the benefits of selective
modality reliance through ablation and rare/frequent attribute analysis.
LANTERN's modularity supports scalable integration of new encoders and evolving
datasets. This work provides a practical and extensible blueprint for behavior
modeling in survey-centric applications.

</details>


### [202] [Zero-Shot Learning with Subsequence Reordering Pretraining for Compound-Protein Interaction](https://arxiv.org/abs/2507.20925)
*Hongzhi Zhang,Zhonglie Liu,Kun Meng,Jiameng Chen,Jia Wu,Bo Du,Di Lin,Yan Che,Wenbin Hu*

Main category: cs.LG

TL;DR: 提出了一种基于蛋白质子序列重排序的预训练方法，用于零样本化合物-蛋白质相互作用预测，解决了现有方法在子序列依赖性和数据稀缺性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决现有化合物-蛋白质相互作用预测方法在子序列依赖性和数据需求方面的不足，以适应实际药物开发中的零样本场景。

Method: 通过蛋白质子序列重排序预训练蛋白质表示，并结合长度可变的蛋白质增强技术，提升小数据集下的预训练性能。

Result: 在零样本场景下显著提升基线模型的性能，尤其在数据稀缺情况下表现优于现有预训练模型。

Conclusion: 该方法为化合物-蛋白质相互作用预测提供了一种高效且可扩展的解决方案，特别适用于数据稀缺的实际应用场景。

Abstract: Given the vastness of chemical space and the ongoing emergence of previously
uncharacterized proteins, zero-shot compound-protein interaction (CPI)
prediction better reflects the practical challenges and requirements of
real-world drug development. Although existing methods perform adequately
during certain CPI tasks, they still face the following challenges: (1)
Representation learning from local or complete protein sequences often
overlooks the complex interdependencies between subsequences, which are
essential for predicting spatial structures and binding properties. (2)
Dependence on large-scale or scarce multimodal protein datasets demands
significant training data and computational resources, limiting scalability and
efficiency. To address these challenges, we propose a novel approach that
pretrains protein representations for CPI prediction tasks using subsequence
reordering, explicitly capturing the dependencies between protein subsequences.
Furthermore, we apply length-variable protein augmentation to ensure excellent
pretraining performance on small training datasets. To evaluate the model's
effectiveness and zero-shot learning ability, we combine it with various
baseline methods. The results demonstrate that our approach can improve the
baseline model's performance on the CPI task, especially in the challenging
zero-shot scenario. Compared to existing pre-training models, our model
demonstrates superior performance, particularly in data-scarce scenarios where
training samples are limited. Our implementation is available at
https://github.com/Hoch-Zhang/PSRP-CPI.

</details>


### [203] [Breaking the Precision Ceiling in Physics-Informed Neural Networks: A Hybrid Fourier-Neural Architecture for Ultra-High Accuracy](https://arxiv.org/abs/2507.20929)
*Wei Shan Lee,Chi Kiu Althina Chau,Kei Chon Sio,Kam Ian Leong*

Main category: cs.LG

TL;DR: 论文提出了一种混合傅里叶-神经网络架构，显著提升了物理信息神经网络（PINNs）在四阶偏微分方程中的精度，误差降至1.94×10⁻⁷，比传统方法提高了15-500倍。


<details>
  <summary>Details</summary>
Motivation: 解决PINNs在四阶偏微分方程中误差停滞在10⁻³-10⁻⁴的问题，突破精度瓶颈，推动其在工程中的应用。

Method: 结合截断傅里叶级数和深度神经网络，采用两阶段优化策略（Adam和L-BFGS）和自适应权重平衡，实现超精度收敛。

Result: 在Euler-Bernoulli梁方程中，L2误差降至1.94×10⁻⁷，比标准PINNs提高了17倍，训练时间控制在30分钟内。

Conclusion: 通过合理设计，机器学习可以在科学计算中达到或超越传统数值方法的精度，为超精度计算开辟了新范式。

Abstract: Physics-informed neural networks (PINNs) have plateaued at errors of
$10^{-3}$-$10^{-4}$ for fourth-order partial differential equations, creating a
perceived precision ceiling that limits their adoption in engineering
applications. We break through this barrier with a hybrid Fourier-neural
architecture for the Euler-Bernoulli beam equation, achieving unprecedented L2
error of $1.94 \times 10^{-7}$-a 17-fold improvement over standard PINNs and
\(15-500\times\) better than traditional numerical methods. Our approach
synergistically combines a truncated Fourier series capturing dominant modal
behavior with a deep neural network providing adaptive residual corrections. A
systematic harmonic optimization study revealed a counter-intuitive discovery:
exactly 10 harmonics yield optimal performance, with accuracy catastrophically
degrading from $10^{-7}$ to $10^{-1}$ beyond this threshold. The two-phase
optimization strategy (Adam followed by L-BFGS) and adaptive weight balancing
enable stable ultra-precision convergence. GPU-accelerated implementation
achieves sub-30-minute training despite fourth-order derivative complexity. By
addressing 12 critical gaps in existing approaches-from architectural rigidity
to optimization landscapes-this work demonstrates that ultra-precision is
achievable through proper design, opening new paradigms for scientific
computing where machine learning can match or exceed traditional numerical
methods.

</details>


### [204] [PySHRED: A Python package for SHallow REcurrent Decoding for sparse sensing, model reduction and scientific discovery](https://arxiv.org/abs/2507.20954)
*David Ye,Jan Williams,Mars Gao,Stefano Riva,Matteo Tomasetto,David Zoro,J. Nathan Kutz*

Main category: cs.LG

TL;DR: PySHRED是一个Python包，实现了SHallow REcurrent Decoders（SHRED）及其扩展功能，用于处理高维动态系统或时空数据。


<details>
  <summary>Details</summary>
Motivation: 为建模高维动态系统或时空数据提供深度学习解决方案，并处理现实世界中的噪声、多尺度、参数化等问题。

Method: 通过SHRED方法及其扩展（如鲁棒感知、降阶建模和物理发现）处理数据，并提供数据预处理工具。

Result: 发布了PySHRED 1.0版本，包含易安装、文档完善、模块化设计的代码库，支持未来扩展。

Conclusion: PySHRED是一个功能强大且易于使用的工具，适用于处理复杂动态系统数据。

Abstract: SHallow REcurrent Decoders (SHRED) provide a deep learning strategy for
modeling high-dimensional dynamical systems and/or spatiotemporal data from
dynamical system snapshot observations. PySHRED is a Python package that
implements SHRED and several of its major extensions, including for robust
sensing, reduced order modeling and physics discovery. In this paper, we
introduce the version 1.0 release of PySHRED, which includes data preprocessors
and a number of cutting-edge SHRED methods specifically designed to handle
real-world data that may be noisy, multi-scale, parameterized, prohibitively
high-dimensional, and strongly nonlinear. The package is easy to install,
thoroughly-documented, supplemented with extensive code examples, and
modularly-structured to support future additions. The entire codebase is
released under the MIT license and is available at
https://github.com/pyshred-dev/pyshred.

</details>


### [205] [PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes](https://arxiv.org/abs/2507.20967)
*Tianhao Wang,Simon Klancher,Kunal Mukherjee,Josh Wiedemeier,Feng Chen,Murat Kantarcioglu,Kangkook Jee*

Main category: cs.LG

TL;DR: ProvCreator是一个用于生成复杂异构图的合成框架，利用基于Transformer的大语言模型，支持端到端可学习的图生成。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图数据复杂且异构，现有方法多针对简单同构图，难以满足语义保真需求。

Method: ProvCreator将图合成任务转化为序列生成任务，采用图到序列的编码-解码器，支持无损编码和高效压缩。

Result: 在网络安全和知识图谱领域验证了ProvCreator的有效性，能生成真实且隐私保护的合成数据。

Conclusion: ProvCreator为复杂异构图的合成提供了高效且语义保真的解决方案。

Abstract: The rise of graph-structured data has driven interest in graph learning and
synthetic data generation. While successful in text and image domains,
synthetic graph generation remains challenging -- especially for real-world
graphs with complex, heterogeneous schemas. Existing research has focused
mostly on homogeneous structures with simple attributes, limiting their
usefulness and relevance for application domains requiring semantic fidelity.
  In this research, we introduce ProvCreator, a synthetic graph framework
designed for complex heterogeneous graphs with high-dimensional node and edge
attributes. ProvCreator formulates graph synthesis as a sequence generation
task, enabling the use of transformer-based large language models. It features
a versatile graph-to-sequence encoder-decoder that 1. losslessly encodes graph
structure and attributes, 2. efficiently compresses large graphs for contextual
modeling, and 3. supports end-to-end, learnable graph generation.
  To validate our research, we evaluate ProvCreator on two challenging domains:
system provenance graphs in cybersecurity and knowledge graphs from
IntelliGraph Benchmark Dataset. In both cases, ProvCreator captures intricate
dependencies between structure and semantics, enabling the generation of
realistic and privacy-aware synthetic datasets.

</details>


### [206] [From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation](https://arxiv.org/abs/2507.20968)
*Rongyao Cai,Ming Jin,Qingsong Wen,Kexin Zhang*

Main category: cs.LG

TL;DR: DARSD是一种新型无监督域自适应框架，通过表示空间分解实现域适应，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列分析中域偏移问题，现有方法未考虑特征内在结构。

Method: DARSD包含三个组件：对抗学习不变基、原型伪标签机制和混合对比优化策略。

Result: 在四个基准数据集上优于12种UDA算法，35/53场景表现最优。

Conclusion: DARSD通过分解表示空间实现有效域适应，具有理论解释性和实际优势。

Abstract: Domain shift poses a fundamental challenge in time series analysis, where
models trained on source domain often fail dramatically when applied in target
domain with different yet similar distributions. While current unsupervised
domain adaptation (UDA) methods attempt to align cross-domain feature
distributions, they typically treat features as indivisible entities, ignoring
their intrinsic compositions that governs domain adaptation. We introduce
DARSD, a novel UDA framework with theoretical explainability that explicitly
realizes UDA tasks from the perspective of representation space decomposition.
Our core insight is that effective domain adaptation requires not just
alignment, but principled disentanglement of transferable knowledge from mixed
representations. DARSD consists three synergistic components: (I) An
adversarial learnable common invariant basis that projects original features
into a domain-invariant subspace while preserving semantic content; (II) A
prototypical pseudo-labeling mechanism that dynamically separates target
features based on confidence, hindering error accumulation; (III) A hybrid
contrastive optimization strategy that simultaneously enforces feature
clustering and consistency while mitigating emerging distribution gaps.
Comprehensive experiments conducted on four benchmark datasets (WISDM, HAR,
HHAR, and MFD) demonstrate DARSD's superiority against 12 UDA algorithms,
achieving optimal performance in 35 out of 53 cross-domain scenarios.

</details>


### [207] [Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder](https://arxiv.org/abs/2507.20973)
*Chao Wu,Zhenyi Wang,Kangxian Xie,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Mingchen Gao*

Main category: cs.LG

TL;DR: SAE Debias是一个轻量级、模型无关的框架，用于减少文本到图像（T2I）扩散模型中的性别偏见，通过稀疏自编码器在特征空间中直接干预，无需重新训练或修改模型架构。


<details>
  <summary>Details</summary>
Motivation: T2I扩散模型常表现出性别偏见，尤其是职业与性别的刻板关联。现有方法（如CLIP过滤或提示工程）需要模型特定调整且控制有限，因此需要一种更通用且高效的去偏方法。

Method: 利用预训练的k稀疏自编码器在性别偏见数据集上识别稀疏潜在空间中的性别相关方向，构建职业偏见方向并在推理时抑制，以生成更性别平衡的图像。

Result: 在多个T2I模型（如Stable Diffusion系列）上验证，SAE Debias显著减少性别偏见，同时保持生成质量。

Conclusion: SAE Debias是首个利用稀疏自编码器识别和干预T2I模型中性别偏见的工作，为构建社会责任的生成AI提供了可解释且模型无关的工具。

Abstract: Text-to-image (T2I) diffusion models often exhibit gender bias, particularly
by generating stereotypical associations between professions and gendered
subjects. This paper presents SAE Debias, a lightweight and model-agnostic
framework for mitigating such bias in T2I generation. Unlike prior approaches
that rely on CLIP-based filtering or prompt engineering, which often require
model-specific adjustments and offer limited control, SAE Debias operates
directly within the feature space without retraining or architectural
modifications. By leveraging a k-sparse autoencoder pre-trained on a gender
bias dataset, the method identifies gender-relevant directions within the
sparse latent space, capturing professional stereotypes. Specifically, a biased
direction per profession is constructed from sparse latents and suppressed
during inference to steer generations toward more gender-balanced outputs.
Trained only once, the sparse autoencoder provides a reusable debiasing
direction, offering effective control and interpretable insight into biased
subspaces. Extensive evaluations across multiple T2I models, including Stable
Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially
reduces gender bias while preserving generation quality. To the best of our
knowledge, this is the first work to apply sparse autoencoders for identifying
and intervening in gender bias within T2I models. These findings contribute
toward building socially responsible generative AI, providing an interpretable
and model-agnostic tool to support fairness in text-to-image generation.

</details>


### [208] [SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment](https://arxiv.org/abs/2507.20984)
*Yixin Song,Zhenliang Xue,Dongliang Wei,Feiyang Chen,Jianxiang Gao,Junchen Liu,Hangyu Liang,Guangshuo Qin,Chengrong Tian,Bo Wen,Longyu Zhao,Xinrui Zheng,Zeyu Mi,Haibo Chen*

Main category: cs.LG

TL;DR: SmallThinker是一系列专为本地设备设计的轻量级大语言模型（LLM），通过创新的部署感知架构，解决了计算能力弱、内存有限和存储慢的问题，实现了高性能且无需依赖GPU硬件。


<details>
  <summary>Details</summary>
Motivation: 传统LLM依赖GPU云基础设施，而SmallThinker旨在为本地设备提供高效、低资源消耗的解决方案。

Method: 采用两级稀疏结构（MoE与稀疏前馈网络）、预注意力路由器和NoPE-RoPE混合稀疏注意力机制，优化计算、存储和内存效率。

Result: SmallThinker-4B-A0.6B和SmallThinker-21B-A3B在性能上超越更大模型，并在普通CPU上实现每秒20+ tokens的速度，内存占用极低。

Conclusion: SmallThinker证明了本地设备上高效运行LLM的可行性，为边缘计算提供了新方向。

Abstract: While frontier large language models (LLMs) continue to push capability
boundaries, their deployment remains confined to GPU-powered cloud
infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs
natively designed - not adapted - for the unique constraints of local devices:
weak computational power, limited memory, and slow storage. Unlike traditional
approaches that mainly compress existing models built for clouds, we architect
SmallThinker from the ground up to thrive within these limitations. Our
innovation lies in a deployment-aware architecture that transforms constraints
into design principles. First, We introduce a two-level sparse structure
combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward
networks, drastically reducing computational demands without sacrificing model
capacity. Second, to conquer the I/O bottleneck of slow storage, we design a
pre-attention router that enables our co-designed inference engine to prefetch
expert parameters from storage while computing attention, effectively hiding
storage latency that would otherwise cripple on-device inference. Third, for
memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to
slash KV cache requirements. We release SmallThinker-4B-A0.6B and
SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and
even outperform larger LLMs. Remarkably, our co-designed system mostly
eliminates the need for expensive GPU hardware: with Q4_0 quantization, both
models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB
and 8GB of memory respectively. SmallThinker is publicly available at
hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and
hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.

</details>


### [209] [Personalized Treatment Effect Estimation from Unstructured Data](https://arxiv.org/abs/2507.20993)
*Henri Arno,Thomas Demeester*

Main category: cs.LG

TL;DR: 论文提出了一种利用非结构化数据估计个性化治疗效果的方法，通过引入近似'插件'方法和两种理论基础的估计器，解决了混杂偏差和抽样偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖结构化协变量，限制了其在非结构化数据中的应用。非结构化数据（如医疗记录或图像）在因果推断中具有巨大潜力。

Method: 1. 提出近似'插件'方法，直接基于非结构化数据的神经表示训练；2. 引入两种理论基础的估计器，利用结构化混杂变量训练但仅需非结构化输入；3. 针对非代表性子集的抽样偏差，提出基于回归的校正方法。

Result: 实验表明，插件方法在大型非结构化数据集上表现优异，尽管方法简单。

Conclusion: 该方法扩展了个性化治疗效果估计的应用范围，解决了非结构化数据中的混杂和抽样偏差问题。

Abstract: Existing methods for estimating personalized treatment effects typically rely
on structured covariates, limiting their applicability to unstructured data.
Yet, leveraging unstructured data for causal inference has considerable
application potential, for instance in healthcare, where clinical notes or
medical images are abundant. To this end, we first introduce an approximate
'plug-in' method trained directly on the neural representations of unstructured
data. However, when these fail to capture all confounding information, the
method may be subject to confounding bias. We therefore introduce two
theoretically grounded estimators that leverage structured measurements of the
confounders during training, but allow estimating personalized treatment
effects purely from unstructured inputs, while avoiding confounding bias. When
these structured measurements are only available for a non-representative
subset of the data, these estimators may suffer from sampling bias. To address
this, we further introduce a regression-based correction that accounts for the
non-uniform sampling, assuming the sampling mechanism is known or can be
well-estimated. Our experiments on two benchmark datasets show that the plug-in
method, directly trainable on large unstructured datasets, achieves strong
empirical performance across all settings, despite its simplicity.

</details>


### [210] [Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition](https://arxiv.org/abs/2507.20997)
*Haris Khan,Shumaila Asif,Sadia Asif*

Main category: cs.LG

TL;DR: MDM-OC是一种新型框架，通过正交约束实现模型的无干扰、可逆组合，支持持续学习和合规性要求。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型合并和持续学习方法中的任务干扰、灾难性遗忘和不可逆性问题。

Method: 将任务特定模型编码为共享基础的增量，并投影到正交子空间以避免冲突，通过梯度优化合并。

Result: 在视觉和自然语言处理任务中，MDM-OC在准确性、反向迁移和分离保真度上优于现有方法。

Conclusion: MDM-OC为模块化和合规的AI系统设计提供了理论解决方案。

Abstract: In real-world machine learning deployments, models must be continually
updated, composed, and when required, selectively undone. However, existing
approaches to model merging and continual learning often suffer from task
interference, catastrophic forgetting, or lack of reversibility. We propose
Modular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework
that enables scalable, interference-free, and reversible composition of
fine-tuned models. Each task-specific model is encoded as a delta from a shared
base and projected into an orthogonal subspace to eliminate conflict. These
projected deltas are then merged via gradient-based optimization to form a
unified model that retains performance across tasks. Our approach supports
continual integration of new models, structured unmerging for compliance such
as GDPR requirements, and model stability via elastic weight consolidation and
synthetic replay. Extensive experiments on vision and natural language
processing benchmarks demonstrate that MDM-OC outperforms prior baselines in
accuracy, backward transfer, and unmerge fidelity, while remaining
memory-efficient and computationally tractable. This framework offers a
principled solution for modular and compliant AI system design.

</details>


### [211] [Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability](https://arxiv.org/abs/2507.21004)
*Fang Li*

Main category: cs.LG

TL;DR: CFNs是一种新型可解释模型框架，通过组合基本数学函数实现透明性，支持复杂特征交互，性能接近黑盒模型。


<details>
  <summary>Details</summary>
Motivation: 解决DNN黑盒性质在高风险领域部署的限制，提供兼具高性能和透明性的模型。

Method: 引入CFNs框架，支持顺序、并行和条件组合模式，完全可微分，可通过梯度下降高效训练。

Result: 在多个领域（如CIFAR-10）表现优异，准确率达96.24%，优于现有可解释模型。

Conclusion: CFNs结合深度学习的表达能力和数学函数的可解释性，适用于高性能与可问责性并重的场景。

Abstract: Deep Neural Networks (DNNs) deliver impressive performance but their
black-box nature limits deployment in high-stakes domains requiring
transparency. We introduce Compositional Function Networks (CFNs), a novel
framework that builds inherently interpretable models by composing elementary
mathematical functions with clear semantics. Unlike existing interpretable
approaches that are limited to simple additive structures, CFNs support diverse
compositional patterns -- sequential, parallel, and conditional -- enabling
complex feature interactions while maintaining transparency. A key innovation
is that CFNs are fully differentiable, allowing efficient training through
standard gradient descent. We demonstrate CFNs' versatility across multiple
domains, from symbolic regression to image classification with deep
hierarchical networks. Our empirical evaluation shows CFNs achieve competitive
performance against black-box models (96.24% accuracy on CIFAR-10) while
outperforming state-of-the-art interpretable models like Explainable Boosting
Machines. By combining the hierarchical expressiveness and efficient training
of deep learning with the intrinsic interpretability of well-defined
mathematical functions, CFNs offer a powerful framework for applications where
both performance and accountability are paramount.

</details>


### [212] [Predicting Cognition from fMRI:A Comparative Study of Graph, Transformer, and Kernel Models Across Task and Rest Conditions](https://arxiv.org/abs/2507.21016)
*Jagruti Patel,Mikkel Schöttner,Thomas A. W. Bolton,Patric Hagmann*

Main category: cs.LG

TL;DR: 该研究比较了经典机器学习（KRR）和深度学习（GNN、TGNN）在预测认知能力方面的表现，发现任务型fMRI优于静息态fMRI，且结合结构和功能连接的GNN表现最佳。


<details>
  <summary>Details</summary>
Motivation: 通过神经影像数据预测认知能力，为精准医疗和早期神经精神疾病检测提供潜在应用。

Method: 使用KRR、GNN和TGNN模型，基于静息态、工作记忆和语言任务fMRI数据，比较其预测性能。

Result: 任务型fMRI表现更好；GNN结合结构和功能连接表现最佳，但优势不显著；TGNN在任务型fMRI中表现接近FC方法，但在静息态数据中表现较差。

Conclusion: 选择合适的模型架构和特征表示对充分利用神经影像数据的时空信息至关重要，多模态图感知DL模型和Transformer方法具有潜力。

Abstract: Predicting cognition from neuroimaging data in healthy individuals offers
insights into the neural mechanisms underlying cognitive abilities, with
potential applications in precision medicine and early detection of
neurological and psychiatric conditions. This study systematically benchmarked
classical machine learning (Kernel Ridge Regression (KRR)) and advanced deep
learning (DL) models (Graph Neural Networks (GNN) and Transformer-GNN (TGNN))
for cognitive prediction using Resting-state (RS), Working Memory, and Language
task fMRI data from the Human Connectome Project Young Adult dataset.
  Our results, based on R2 scores, Pearson correlation coefficient, and mean
absolute error, revealed that task-based fMRI, eliciting neural responses
directly tied to cognition, outperformed RS fMRI in predicting cognitive
behavior. Among the methods compared, a GNN combining structural connectivity
(SC) and functional connectivity (FC) consistently achieved the highest
performance across all fMRI modalities; however, its advantage over KRR using
FC alone was not statistically significant. The TGNN, designed to model
temporal dynamics with SC as a prior, performed competitively with FC-based
approaches for task-fMRI but struggled with RS data, where its performance
aligned with the lower-performing GNN that directly used fMRI time-series data
as node features. These findings emphasize the importance of selecting
appropriate model architectures and feature representations to fully leverage
the spatial and temporal richness of neuroimaging data.
  This study highlights the potential of multimodal graph-aware DL models to
combine SC and FC for cognitive prediction, as well as the promise of
Transformer-based approaches for capturing temporal dynamics. By providing a
comprehensive comparison of models, this work serves as a guide for advancing
brain-behavior modeling using fMRI, SC and DL.

</details>


### [213] [Behavior-Specific Filtering for Enhanced Pig Behavior Classification in Precision Livestock Farming](https://arxiv.org/abs/2507.21021)
*Zhen Zhang,Dong Sam Ha,Gota Morota,Sook Shin*

Main category: cs.LG

TL;DR: 提出了一种针对特定行为的过滤方法，提高了精准畜牧养殖中行为分类的准确性，峰值准确率达94.73%。


<details>
  <summary>Details</summary>
Motivation: 传统过滤方法对所有行为采用统一处理，准确率为91.58%，无法满足精准监测需求。

Method: 结合小波去噪和低通滤波，针对活跃和非活跃猪行为进行定制化处理。

Result: 峰值准确率提升至94.73%，优于传统方法。

Conclusion: 行为特定过滤方法有效提升动物行为监测，支持更好的健康管理和农场效率。

Abstract: This study proposes a behavior-specific filtering method to improve behavior
classification accuracy in Precision Livestock Farming. While traditional
filtering methods, such as wavelet denoising, achieved an accuracy of 91.58%,
they apply uniform processing to all behaviors. In contrast, the proposed
behavior-specific filtering method combines Wavelet Denoising with a Low Pass
Filter, tailored to active and inactive pig behaviors, and achieved a peak
accuracy of 94.73%. These results highlight the effectiveness of
behavior-specific filtering in enhancing animal behavior monitoring, supporting
better health management and farm efficiency.

</details>


### [214] [On Using the Shapley Value for Anomaly Localization: A Statistical Investigation](https://arxiv.org/abs/2507.21023)
*Rick S. Blum,Franziska Freytag*

Main category: cs.LG

TL;DR: 使用Shapley值进行传感器数据异常定位，实验表明固定项计算可降低复杂度且保持相同错误概率。


<details>
  <summary>Details</summary>
Motivation: 探索更高效的异常定位方法，降低计算复杂度。

Method: 采用Shapley值计算，实验比较固定项与完整计算的性能。

Result: 固定项方法在独立观测情况下性能与完整计算相同，依赖观测情况未验证。

Conclusion: 固定项Shapley值计算在独立观测下是有效的简化方法。

Abstract: Recent publications have suggested using the Shapley value for anomaly
localization for sensor data systems. Using a reasonable mathematical anomaly
model for full control, experiments indicate that using a single fixed term in
the Shapley value calculation achieves a lower complexity anomaly localization
test, with the same probability of error, as a test using the Shapley value for
all cases tested. A proof demonstrates these conclusions must be true for all
independent observation cases. For dependent observation cases, no proof is
available.

</details>


### [215] [Optimization Performance of Factorization Machine with Annealing under Limited Training Data](https://arxiv.org/abs/2507.21024)
*Mayumi Nakano,Yuya Seki,Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: 提出了一种改进的FMA方法，通过限制数据集大小来增强新数据点对模型的影响，从而提升优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统FMA方法在优化过程中性能停滞，原因是数据集增长导致新数据点的影响被稀释。

Method: 提出了一种序列化数据集构建方法，仅保留最近添加的特定数量数据点。

Result: 数值实验表明，改进的FMA能以更少的黑箱函数评估获得更低成本的解。

Conclusion: 限制数据集大小能有效提升FMA的优化性能。

Abstract: Black-box (BB) optimization problems aim to identify an input that minimizes
the output of a function (the BB function) whose input-output relationship is
unknown. Factorization machine with annealing (FMA) is a promising approach to
this task, employing a factorization machine (FM) as a surrogate model to
iteratively guide the solution search via an Ising machine. Although FMA has
demonstrated strong optimization performance across various applications, its
performance often stagnates as the number of optimization iterations increases.
One contributing factor to this stagnation is the growing number of data points
in the dataset used to train FM. It is hypothesized that as more data points
are accumulated, the contribution of newly added data points becomes diluted
within the entire dataset, thereby reducing their impact on improving the
prediction accuracy of FM. To address this issue, we propose a novel method for
sequential dataset construction that retains at most a specified number of the
most recently added data points. This strategy is designed to enhance the
influence of newly added data points on the surrogate model. Numerical
experiments demonstrate that the proposed FMA achieves lower-cost solutions
with fewer BB function evaluations compared to the conventional FMA.

</details>


### [216] [When Brain Foundation Model Meets Cauchy-Schwarz Divergence: A New Framework for Cross-Subject Motor Imagery Decoding](https://arxiv.org/abs/2507.21037)
*Jinzhou Wu,Baoping Tang,Qikang Li,Yi Wang,Cheng Li,Shujian Yu*

Main category: cs.LG

TL;DR: 提出了一种基于预训练大脑基础模型（BFM）的多源域自适应框架，用于动态选择相关源域，并结合特征级和决策级对齐提升MI-EEG解码性能。


<details>
  <summary>Details</summary>
Motivation: 解决MI-EEG解码中因受试者间差异大和标记数据少导致的负迁移和计算成本高的问题。

Method: 利用BFM动态选择源域，使用Cauchy-Schwarz和条件CS散度进行特征和决策级对齐。

Result: 在两个基准数据集上表现优于现有方法，验证了BFM选择的高效性和可扩展性。

Conclusion: 提出的框架显著提升了MI-EEG解码性能，同时降低了计算成本。

Abstract: Decoding motor imagery (MI) electroencephalogram (EEG) signals, a key
non-invasive brain-computer interface (BCI) paradigm for controlling external
systems, has been significantly advanced by deep learning. However, MI-EEG
decoding remains challenging due to substantial inter-subject variability and
limited labeled target data, which necessitate costly calibration for new
users. Many existing multi-source domain adaptation (MSDA) methods
indiscriminately incorporate all available source domains, disregarding the
large inter-subject differences in EEG signals, which leads to negative
transfer and excessive computational costs. Moreover, while many approaches
focus on feature distribution alignment, they often neglect the explicit
dependence between features and decision-level outputs, limiting their ability
to preserve discriminative structures. To address these gaps, we propose a
novel MSDA framework that leverages a pretrained large Brain Foundation Model
(BFM) for dynamic and informed source subject selection, ensuring only relevant
sources contribute to adaptation. Furthermore, we employ Cauchy-Schwarz (CS)
and Conditional CS (CCS) divergences to jointly perform feature-level and
decision-level alignment, enhancing domain invariance while maintaining class
discriminability. Extensive evaluations on two benchmark MI-EEG datasets
demonstrate that our framework outperforms a broad range of state-of-the-art
baselines. Additional experiments with a large source pool validate the
scalability and efficiency of BFM-guided selection, which significantly reduces
training time without sacrificing performance.

</details>


### [217] [Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps: An Interpretation and Potential Improvements](https://arxiv.org/abs/2507.21040)
*Aditya Ravuri,Neil D. Lawrence*

Main category: cs.LG

TL;DR: 论文提出了一种将Transformer解释为概率拉普拉斯特征映射模型的展开推理步骤的方法，并展示了其初始化和性能改进。


<details>
  <summary>Details</summary>
Motivation: 为Transformer提供一种概率解释，揭示其初始化行为和性能改进的潜在机制。

Method: 通过概率拉普拉斯特征映射模型推导Transformer的行为，并调整注意力矩阵（减去单位矩阵）以改进性能。

Result: 实验表明，调整后的注意力矩阵在语言模型和简单视觉Transformer上提高了验证性能。

Conclusion: 论文为Transformer提供了一种新的概率视角，并通过简单调整注意力矩阵展示了性能提升的潜力。

Abstract: We propose a probabilistic interpretation of transformers as unrolled
inference steps assuming a probabilistic Laplacian Eigenmaps model from the
ProbDR framework. Our derivation shows that at initialisation, transformers
perform "linear" dimensionality reduction. We also show that within the
transformer block, a graph Laplacian term arises from our arguments, rather
than an attention matrix (which we interpret as an adjacency matrix). We
demonstrate that simply subtracting the identity from the attention matrix (and
thereby taking a graph diffusion step) improves validation performance on a
language model and a simple vision transformer.

</details>


### [218] [Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning](https://arxiv.org/abs/2507.21049)
*Zedong Wang,Siyuan Li,Dan Xu*

Main category: cs.LG

TL;DR: Rep-MTL提出了一种利用表示层任务显著性来量化任务间交互的方法，通过熵惩罚和样本级跨任务对齐，旨在减少负迁移并促进互补信息共享。


<details>
  <summary>Details</summary>
Motivation: 现有多任务优化方法主要关注通过优化器中心策略解决任务冲突，而忽略了共享表示空间中任务交互的潜力。Rep-MTL旨在探索这一潜力，以提升任务间的互补性。

Method: Rep-MTL通过表示层任务显著性量化任务交互，利用熵惩罚和样本级跨任务对齐来平衡任务特定学习和跨任务共享。

Result: 在四个多任务学习基准测试中，Rep-MTL即使与基本等权重策略配对，也能实现竞争性性能提升，并通过Power Law指数分析验证了其平衡能力。

Conclusion: Rep-MTL通过表示层操作补充现有优化器，有效减少了负迁移并促进了任务间的互补信息共享，在多任务学习中表现出色。

Abstract: Despite the promise of Multi-Task Learning in leveraging complementary
knowledge across tasks, existing multi-task optimization (MTO) techniques
remain fixated on resolving conflicts via optimizer-centric loss scaling and
gradient manipulation strategies, yet fail to deliver consistent gains. In this
paper, we argue that the shared representation space, where task interactions
naturally occur, offers rich information and potential for operations
complementary to existing optimizers, especially for facilitating the
inter-task complementarity, which is rarely explored in MTO. This intuition
leads to Rep-MTL, which exploits the representation-level task saliency to
quantify interactions between task-specific optimization and shared
representation learning. By steering these saliencies through entropy-based
penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate
negative transfer by maintaining the effective training of individual tasks
instead pure conflict-solving, while explicitly promoting complementary
information sharing. Experiments are conducted on four challenging MTL
benchmarks covering both task-shift and domain-shift scenarios. The results
show that Rep-MTL, even paired with the basic equal weighting policy, achieves
competitive performance gains with favorable efficiency. Beyond standard
performance metrics, Power Law exponent analysis demonstrates Rep-MTL's
efficacy in balancing task-specific learning and cross-task sharing. The
project page is available at HERE.

</details>


### [219] [Flow Matching Policy Gradients](https://arxiv.org/abs/2507.21053)
*David McAllister,Songwei Ge,Brent Yi,Chung Min Kim,Ethan Weber,Hongsuk Choi,Haiwen Feng,Angjoo Kanazawa*

Main category: cs.LG

TL;DR: Flow Policy Optimization (FPO) 是一种基于流匹配的强化学习算法，将流模型引入策略梯度框架，无需精确似然计算，适用于连续控制任务。


<details>
  <summary>Details</summary>
Motivation: 将流模型的生成能力与强化学习结合，解决传统高斯策略在复杂任务中的局限性。

Method: FPO 通过优势加权比优化策略，兼容 PPO-clip 框架，不依赖特定采样方法。

Result: FPO 在连续控制任务中表现优异，能捕捉多模态动作分布，性能优于高斯策略。

Conclusion: FPO 为流模型在强化学习中的应用提供了灵活高效的框架。

Abstract: Flow-based generative models, including diffusion models, excel at modeling
continuous distributions in high-dimensional spaces. In this work, we introduce
Flow Policy Optimization (FPO), a simple on-policy reinforcement learning
algorithm that brings flow matching into the policy gradient framework. FPO
casts policy optimization as maximizing an advantage-weighted ratio computed
from the conditional flow matching loss, in a manner compatible with the
popular PPO-clip framework. It sidesteps the need for exact likelihood
computation while preserving the generative capabilities of flow-based models.
Unlike prior approaches for diffusion-based reinforcement learning that bind
training to a specific sampling method, FPO is agnostic to the choice of
diffusion or flow integration at both training and inference time. We show that
FPO can train diffusion-style policies from scratch in a variety of continuous
control tasks. We find that flow-based models can capture multimodal action
distributions and achieve higher performance than Gaussian policies,
particularly in under-conditioned settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [220] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA是一个开源平台，旨在促进临床、研究和AI开发者的跨学科合作，加速AI研究向临床应用的转化。


<details>
  <summary>Details</summary>
Motivation: 解决AI技术与实际医疗应用之间的鸿沟，促进协作和互操作性。

Method: 基于Kubernetes构建，提供模块化、可扩展的环境，集成数据管理、模型开发、标注、部署和临床反馈工具。

Result: MAIA已在学术和临床环境中成功部署，支持医学影像AI的实际用例。

Conclusion: MAIA通过促进协作和透明度，加速了AI研究向临床解决方案的转化。

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [221] [Agent WARPP: Workflow Adherence via Runtime Parallel Personalization](https://arxiv.org/abs/2507.19543)
*Maria Emilia Mazzolenis,Ruirui Zhang*

Main category: cs.AI

TL;DR: WARPP是一个无需训练的模块化框架，通过多智能体协同和运行时个性化提升LLM在任务导向对话中的工作流遵循能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂、条件化工作流中因依赖外部工具调用和用户特定信息而表现不佳的问题。

Method: 结合多智能体协同与运行时个性化，动态剪枝条件分支以降低推理开销并优化工具选择。

Result: 在五个复杂用户意图的测试中，WARPP在参数保真度和工具准确性上优于非个性化方法和ReAct基线，同时减少令牌使用。

Conclusion: WARPP无需额外训练即可显著提升LLM在复杂工作流中的表现，具有实际应用潜力。

Abstract: Large language models (LLMs) are increasingly applied in task-oriented
dialogue (TOD) systems but often struggle with long, conditional workflows that
involve external tool calls and depend on user-specific information. We present
Workflow Adherence via Runtime Parallel Personalization, or WARPP, a
training-free, modular framework that combines multi-agent orchestration with
runtime personalization to improve workflow adherence in LLM-based systems. By
dynamically pruning conditional branches based on user attributes, the
framework reduces reasoning overhead and narrows tool selection at runtime.
WARPP deploys a parallelized architecture where a dedicated Personalizer agent
operates alongside modular, domain-specific agents to dynamically tailor
execution paths in real time. The framework is evaluated across five
representative user intents of varying complexity within three domains:
banking, flights, and healthcare. Our evaluation leverages synthetic datasets
and LLM-powered simulated users to test scenarios with conditional
dependencies. Our results demonstrate that WARPP outperforms both the
non-personalized method and the ReAct baseline, achieving increasingly larger
gains in parameter fidelity and tool accuracy as intent complexity grows, while
also reducing average token usage, without any additional training.

</details>


### [222] [Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems](https://arxiv.org/abs/2507.19593)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 本文综述了超博弈理论在多智能体系统（MAS）中的应用，分析了44项研究，提出了智能体兼容性标准和分类框架，并指出了当前研究的局限性和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统博弈论假设理性、完全信息和共同知识，但在现实MAS中常被违反。超博弈理论通过建模智能体的主观感知（感知博弈）来弥补这些不足。

Method: 系统回顾超博弈理论的应用，提出智能体兼容性标准和分类框架，分析44项研究。

Result: 研究发现分层和图模型在欺骗推理中占主导，实际应用中简化了理论框架，同时指出HNF模型采用不足、缺乏形式化语言等问题。

Conclusion: 综述为超博弈理论在动态MAS中的战略建模提供了新路线图，强调需进一步研究智能体间和人与智能体的错位建模。

Abstract: Classical game-theoretic models typically assume rational agents, complete
information, and common knowledge of payoffs - assumptions that are often
violated in real-world MAS characterized by uncertainty, misaligned
perceptions, and nested beliefs. To overcome these limitations, researchers
have proposed extensions that incorporate models of cognitive constraints,
subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory
extends the classical paradigm by explicitly modeling agents' subjective
perceptions of the strategic scenario, known as perceptual games, in which
agents may hold divergent beliefs about the structure, payoffs, or available
actions. We present a systematic review of agent-compatible applications of
hypergame theory, examining how its descriptive capabilities have been adapted
to dynamic and interactive MAS contexts. We analyze 44 selected studies from
cybersecurity, robotics, social simulation, communications, and general
game-theoretic modeling. Building on a formal introduction to hypergame theory
and its two major extensions - hierarchical hypergames and HNF - we develop
agent-compatibility criteria and an agent-based classification framework to
assess integration patterns and practical applicability. Our analysis reveals
prevailing tendencies, including the prevalence of hierarchical and graph-based
models in deceptive reasoning and the simplification of extensive theoretical
frameworks in practical applications. We identify structural gaps, including
the limited adoption of HNF-based models, the lack of formal hypergame
languages, and unexplored opportunities for modeling human-agent and
agent-agent misalignment. By synthesizing trends, challenges, and open research
directions, this review provides a new roadmap for applying hypergame theory to
enhance the realism and effectiveness of strategic modeling in dynamic
multi-agent environments.

</details>


### [223] [DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference](https://arxiv.org/abs/2507.19608)
*Jiawen Qi,Chang Gao,Zhaochun Ren,Qinyu Chen*

Main category: cs.AI

TL;DR: DeltaLLM是一种无需训练的框架，通过利用注意力模式的时间稀疏性，在资源受限的边缘设备上实现高效的大型语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上部署大型语言模型时计算资源不足的问题，特别是注意力机制随着序列长度增加的二次计算复杂度。

Method: DeltaLLM采用了一种基于时间稀疏性的delta矩阵构建策略和上下文感知的混合注意力机制，结合局部上下文窗口内的完整注意力和窗口外的delta近似。

Result: 在BitNet和Llama模型上，DeltaLLM实现了高达60%的注意力稀疏性，同时保持或略微提高任务准确性。

Conclusion: DeltaLLM为边缘设备上的高效LLM部署提供了无需微调的解决方案，且能与现有推理流程无缝集成。

Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging
due to their quadratically increasing computations with the sequence length.
Existing studies for dynamic attention pruning are designed for hardware with
massively parallel computation capabilities, such as GPUs or TPUs, and aim at
long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We
present DeltaLLM, a training-free framework that exploits temporal sparsity in
attention patterns to enable efficient LLM inference across both the prefilling
and decoding stages, on resource-constrained edge devices. DeltaLLM introduces
an accuracy- and memory-aware delta matrix construction strategy that
introduces temporal sparsity, and a context-aware hybrid attention mechanism
that combines full attention in a local context window with delta approximation
outside it to increase accuracy. We evaluate our framework on the
edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model
across diverse language tasks. The results show that on BitNet, our framework
increases the attention sparsity from 0% to 60% during the prefilling stage
with slight accuracy improvement on the WG task, and 0% to 57% across both the
prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97
on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity
during the prefilling stage and around 57% across both stages with negligible
accuracy drop. These results demonstrate that DeltaLLM offers a promising
solution for efficient edge deployment, requiring no fine-tuning and seamlessly
integrating with existing inference pipelines.

</details>


### [224] [Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges](https://arxiv.org/abs/2507.19672)
*Haoran Lu,Luyang Fang,Ruidong Zhang,Xinliang Li,Jiazhang Cai,Huimin Cheng,Lin Tang,Ziyu Liu,Zeliang Sun,Tao Wang,Yingchuan Zhang,Arif Hassan Zidan,Jinwen Xu,Jincheng Yu,Meizhi Yu,Hanqi Jiang,Xilin Gong,Weidi Luo,Bolun Sun,Yongkai Chen,Terry Ma,Shushan Wu,Yifan Zhou,Junhao Chen,Haotian Xiang,Jing Zhang,Afrar Jahin,Wei Ruan,Ke Deng,Yi Pan,Peilong Wang,Jiahui Li,Zhengliang Liu,Lu Zhang,Lin Zhao,Wei Liu,Dajiang Zhu,Xin Xing,Fei Dou,Wei Zhang,Chao Huang,Rongjie Liu,Mengrui Zhang,Yiwen Liu,Xiaoxiao Sun,Qin Lu,Zhen Xiang,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.AI

TL;DR: 本文综述了大语言模型（LLM）对齐人类价值观和意图的技术、训练方法和实证研究，分析了不同对齐方法的优劣，并探讨了前沿技术和现存挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的提升和社会影响的扩大，确保其与人类价值观和意图的对齐成为关键挑战。

Method: 综述了监督微调、基于偏好的方法（如DPO、Constitutional AI等）以及评估框架和数据集。

Result: 监督微调能实现基本指令跟随，而基于偏好的方法更灵活；前沿技术如DPO和AUQ在平衡质量和效率方面表现突出。

Conclusion: 总结了当前实践策略，并提出了监督、价值多元性、鲁棒性和持续对齐等开放性问题，为研究者和实践者提供参考。

Abstract: Due to the remarkable capabilities and growing impact of large language
models (LLMs), they have been deeply integrated into many aspects of society.
Thus, ensuring their alignment with human values and intentions has emerged as
a critical challenge. This survey provides a comprehensive overview of
practical alignment techniques, training protocols, and empirical findings in
LLM alignment. We analyze the development of alignment methods across diverse
paradigms, characterizing the fundamental trade-offs between core alignment
objectives. Our analysis shows that while supervised fine-tuning enables basic
instruction-following, preference-based methods offer more flexibility for
aligning with nuanced human intent. We discuss state-of-the-art techniques,
including Direct Preference Optimization (DPO), Constitutional AI,
brain-inspired methods, and alignment uncertainty quantification (AUQ),
highlighting their approaches to balancing quality and efficiency. We review
existing evaluation frameworks and benchmarking datasets, emphasizing
limitations such as reward misspecification, distributional robustness, and
scalable oversight. We summarize strategies adopted by leading AI labs to
illustrate the current state of practice. We conclude by outlining open
problems in oversight, value pluralism, robustness, and continuous alignment.
This survey aims to inform both researchers and practitioners navigating the
evolving landscape of LLM alignment.

</details>


### [225] [The wall confronting large language models](https://arxiv.org/abs/2507.19703)
*Peter V. Coveney,Sauro Succi*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）的性能受限于其预测不确定性的改进能力，难以满足科学研究的可靠性标准。其学习机制可能导致错误累积和信息灾难，而数据集中虚假相关性的增加进一步加剧了这一问题。避免退化AI路径需更注重对问题结构特征的理解。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在提升预测可靠性方面的局限性，揭示其学习机制与准确性之间的张力，并提出避免退化AI行为的可能途径。

Method: 分析LLM的缩放定律及其对预测不确定性的影响，结合数据集中虚假相关性的研究，探讨学习机制与错误累积的关系。

Result: LLM的性能受限于其预测不确定性的改进能力，学习机制可能导致错误累积和信息灾难，虚假相关性的增加加剧了这一问题。

Conclusion: 为避免退化AI路径，需更注重对问题结构特征的深入理解，而非单纯依赖数据规模和模型扩展。

Abstract: We show that the scaling laws which determine the performance of large
language models (LLMs) severely limit their ability to improve the uncertainty
of their predictions. As a result, raising their reliability to meet the
standards of scientific inquiry is intractable by any reasonable measure. We
argue that the very mechanism which fuels much of the learning power of LLMs,
namely the ability to generate non-Gaussian output distributions from Gaussian
input ones, might well be at the roots of their propensity to produce error
pileup, ensuing information catastrophes and degenerative AI behaviour. This
tension between learning and accuracy is a likely candidate mechanism
underlying the observed low values of the scaling components. It is
substantially compounded by the deluge of spurious correlations pointed out by
Calude and Longo which rapidly increase in any data set merely as a function of
its size, regardless of its nature. The fact that a degenerative AI pathway is
a very probable feature of the LLM landscape does not mean that it must
inevitably arise in all future AI research. Its avoidance, which we also
discuss in this paper, necessitates putting a much higher premium on insight
and understanding of the structural characteristics of the problems being
investigated.

</details>


### [226] [Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors](https://arxiv.org/abs/2507.19725)
*Leonardo Villalobos-Arias,Grant Forbes,Jianxun Wang,David L Roberts,Arnav Jhala*

Main category: cs.AI

TL;DR: 论文研究了内在动机（IM）方法在强化学习（RL）中可能导致的行为改变和奖励黑客问题，并评估了三种IM技术在MiniGrid环境中的影响。通过广义奖励匹配（GRM）方法，部分缓解了奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 游戏中的奖励稀疏性使得RL代理难以学习，IM方法通过引入探索奖励解决这一问题，但可能导致奖励黑客行为。目前尚不清楚IM奖励如何改变RL代理的行为。

Method: 在MiniGrid环境中，实证评估了三种IM技术对行为的影响，并与广义奖励匹配（GRM）方法进行比较。

Result: IM方法显著改变了代理的行为，增加了初始奖励，但也导致奖励黑客行为。GRM在部分场景中缓解了这一问题。

Conclusion: IM方法确实会改变RL代理的行为，而GRM可以部分解决奖励黑客问题，为未来研究提供了方向。

Abstract: Games are challenging for Reinforcement Learning~(RL) agents due to their
reward-sparsity, as rewards are only obtainable after long sequences of
deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce
exploration rewards -- are an effective solution to reward-sparsity. However,
IM also causes an issue known as `reward hacking' where the agent optimizes for
the new reward at the expense of properly playing the game. The larger problem
is that reward hacking itself is largely unknown; there is no answer to
whether, and to what extent, IM rewards change the behavior of RL agents. This
study takes a first step by empirically evaluating the impact on behavior of
three IM techniques on the MiniGrid game-like environment. We compare these IM
models with Generalized Reward Matching~(GRM), a method that can be used with
any intrinsic reward function to guarantee optimality. Our results suggest that
IM causes noticeable change by increasing the initial rewards, but also
altering the way the agent plays; and that GRM mitigated reward hacking in some
scenarios.

</details>


### [227] [HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare](https://arxiv.org/abs/2507.19726)
*Yuzhang Xie,Xu Han,Ran Xu,Xiao Hu,Jiaying Lu,Carl Yang*

Main category: cs.AI

TL;DR: HypKG框架通过整合电子健康记录（EHRs）中的患者信息到知识图谱（KGs）中，生成上下文知识表示，提升医疗预测准确性。


<details>
  <summary>Details</summary>
Motivation: 通用知识图谱缺乏对患者特定上下文的捕捉能力，而电子健康记录提供了丰富的个人数据，HypKG旨在结合两者以支持精准医疗。

Method: 采用实体链接技术连接通用KGs与EHRs数据，利用超图模型和超图变换器学习上下文化表示。

Result: 实验表明，HypKG在多种评估指标上显著提升了医疗预测任务的性能，并改善了知识图谱的实体和关系表示。

Conclusion: HypKG通过整合外部上下文，提升了知识图谱的实用性和质量，为精准医疗提供了有效工具。

Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are
widely used in various application domains. Healthcare is one of such domains
where KGs are intensively used, due to the high requirement for knowledge
accuracy and interconnected nature of healthcare data. However, KGs storing
general factual information often lack the ability to account for important
contexts of the knowledge such as the status of specific patients, which are
crucial in precision healthcare. Meanwhile, electronic health records (EHRs)
provide rich personal data, including various diagnoses and medications, which
provide natural contexts for general KGs. In this paper, we propose HypKG, a
framework that integrates patient information from EHRs into KGs to generate
contextualized knowledge representations for accurate healthcare predictions.
Using advanced entity-linking techniques, we connect relevant knowledge from
general KGs with patient information from EHRs, and then utilize a hypergraph
model to "contextualize" the knowledge with the patient information. Finally,
we employ hypergraph transformers guided by downstream prediction tasks to
jointly learn proper contextualized representations for both KGs and patients,
fully leveraging existing knowledge in KGs and patient contexts in EHRs. In
experiments using a large biomedical KG and two real-world EHR datasets, HypKG
demonstrates significant improvements in healthcare prediction tasks across
multiple evaluation metrics. Additionally, by integrating external contexts,
HypKG can learn to adjust the representations of entities and relations in KG,
potentially improving the quality and real-world utility of knowledge.

</details>


### [228] [Integrating Activity Predictions in Knowledge Graphs](https://arxiv.org/abs/2507.19733)
*Alec Scully,Cameron Stockton,Forrest Hare*

Main category: cs.AI

TL;DR: 论文提出利用本体结构知识图谱预测未来事件，通过BFO和CCO组织数据并生成马尔可夫链模型，同时提出‘时空实例’概念和改进的概率模型。


<details>
  <summary>Details</summary>
Motivation: 探讨本体结构知识图谱在预测未来事件中的关键作用，并改进现有概率模型的不足。

Method: 利用BFO和CCO构建知识图谱，通过SPARQL查询数据并生成马尔可夫链模型，引入‘时空实例’概念。

Result: 展示了如何将马尔可夫链概率计算无缝集成回知识图谱，支持进一步分析和决策。

Conclusion: 本体结构知识图谱结合改进的概率模型能有效预测未来事件，为动态现象提供更准确的描述。

Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
which conflates probability with likelihood and relies on the problematic
concept of modal measurements: measurements of future entities. We propose an
alternative view, where probabilities are treated as being about process
profiles, which better captures the dynamics of real world phenomena. Finally,
we demonstrate how our Markov chain based probability calculations can be
seamlessly integrated back into the knowledge graph, enabling further analysis
and decision-making. Keywords: predictive analytics, ontology, Markov chains,
probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.

</details>


### [229] [Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)](https://arxiv.org/abs/2507.19749)
*Lin Ren,Guohui Xiao,Guilin Qi,Yishuai Geng,Haohan Xue*

Main category: cs.AI

TL;DR: ASPBench是一个全面的ASP基准测试，揭示了当前大型语言模型在ASP求解中的局限性，尤其是在答案集计算任务上。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型在ASP中能力的评估过于简化，缺乏支持否定、析取或多答案集的测试，且缺少专门设计的ASP求解任务。

Method: 引入ASPBench基准，包含三个ASP特定任务：ASP蕴含、答案集验证和答案集计算。

Result: 评估显示，14个先进的大型语言模型在前两个简单任务上表现较好，但在核心的答案集计算任务上表现不佳。

Conclusion: 研究揭示了大型语言模型在ASP求解中的局限性，强调了需要更有效地整合符号推理能力的新方法。

Abstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic
reasoning. Recently, large language models (LLMs) have demonstrated promising
capabilities in logical reasoning. Despite this potential, current evaluations
of LLM capabilities in ASP are often limited. Existing works normally employ
overly simplified ASP programs, do not support negation, disjunction, or
multiple answer sets. Furthermore, there is a lack of benchmarks that introduce
tasks specifically designed for ASP solving. To bridge this gap, we introduce
ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:
ASP entailment, answer set verification, and answer set computation. Our
extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,
including \emph{deepseek-r1}, \emph{o4-mini}, and
\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two
simpler tasks, they struggle with answer set computation, which is the core of
ASP solving. These findings offer insights into the current limitations of LLMs
in ASP solving. This highlights the need for new approaches that integrate
symbolic reasoning capabilities more effectively. The code and dataset are
available at https://github.com/HomuraT/ASPBench.

</details>


### [230] [Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation](https://arxiv.org/abs/2507.19788)
*Rifny Rachman,Josh Tingey,Richard Allmendinger,Pradyumn Shukla,Wei Pan*

Main category: cs.AI

TL;DR: 该研究开发了一个基于马尔可夫决策过程的多目标、多层次供应链优化模型，结合经济、环境和社会因素，并通过多目标强化学习方法进行评估。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳市场中供应链优化的复杂问题，同时平衡经济、环境和社会目标。

Method: 采用多目标强化学习（RL）方法，与改进的单目标RL算法和基于多目标进化算法（MOEA）的方法进行对比。

Result: 主要方法在最优性、多样性和密度上表现最佳，复杂场景下超体积比MOEA方法高75%，解决方案密度是改进单目标RL方法的11倍。

Conclusion: 该方法在稳定生产和库存水平的同时，显著提升了供应链优化的鲁棒性和效率。

Abstract: This study develops a generalised multi-objective, multi-echelon supply chain
optimisation model with non-stationary markets based on a Markov decision
process, incorporating economic, environmental, and social considerations. The
model is evaluated using a multi-objective reinforcement learning (RL) method,
benchmarked against an originally single-objective RL algorithm modified with
weighted sum using predefined weights, and a multi-objective evolutionary
algorithm (MOEA)-based approach. We conduct experiments on varying network
complexities, mimicking typical real-world challenges using a customisable
simulator. The model determines production and delivery quantities across
supply chain routes to achieve near-optimal trade-offs between competing
objectives, approximating Pareto front sets. The results demonstrate that the
primary approach provides the most balanced trade-off between optimality,
diversity, and density, further enhanced with a shared experience buffer that
allows knowledge transfer among policies. In complex settings, it achieves up
to 75\% higher hypervolume than the MOEA-based method and generates solutions
that are approximately eleven times denser, signifying better robustness, than
those produced by the modified single-objective RL method. Moreover, it ensures
stable production and inventory levels while minimising demand loss.

</details>


### [231] [Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation](https://arxiv.org/abs/2507.19882)
*Xinshu Li,Ruoyu Wang,Erdun Gao,Mingming Gong,Lina Yao*

Main category: cs.AI

TL;DR: DiCap模型是一种基于扩散的反事实提示学习框架，通过理论推导生成因果不变提示，显著提升跨类别泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法因缺乏理论基础，难以生成因果不变提示，导致特征提取不鲁棒。

Method: DiCap利用扩散过程从因果模型的边际和条件分布中迭代采样梯度，生成满足最小充分性准则的反事实提示。

Result: 实验表明，DiCap在图像分类、图文检索和视觉问答等任务中表现优异，尤其在未见类别上优势明显。

Conclusion: DiCap通过理论驱动的反事实提示生成，显著提升了模型的泛化能力和鲁棒性。

Abstract: Prompt learning has garnered attention for its efficiency over traditional
model training and fine-tuning. However, existing methods, constrained by
inadequate theoretical foundations, encounter difficulties in achieving
causally invariant prompts, ultimately falling short of capturing robust
features that generalize effectively across categories. To address these
challenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoretically
grounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual
$\textbf{p}$rompt learning framework, which leverages a diffusion process to
iteratively sample gradients from the marginal and conditional distributions of
the causal model, guiding the generation of counterfactuals that satisfy the
minimal sufficiency criterion. Grounded in rigorous theoretical derivations,
this approach guarantees the identifiability of counterfactual outcomes while
imposing strict bounds on estimation errors. We further employ a contrastive
learning framework that leverages the generated counterfactuals, thereby
enabling the refined extraction of prompts that are precisely aligned with the
causal features of the data. Extensive experimental results demonstrate that
our method performs excellently across tasks such as image classification,
image-text retrieval, and visual question answering, with particularly strong
advantages in unseen categories.

</details>


### [232] [What Does 'Human-Centred AI' Mean?](https://arxiv.org/abs/2507.19960)
*Olivia Guest*

Main category: cs.AI

TL;DR: 论文探讨了以人为中心的人工智能（AI）本质上是技术与人类认知的关系，分析了技术对人类认知劳动的替代、增强或取代，并强调必须正视人类在AI中的作用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在澄清AI与人类认知的关系，避免技术对认知科学的扭曲，并真正实现以人为中心的AI设计。

Method: 通过对比技术（如算盘、闹钟、相机）与人类认知劳动的例子，提出新的定义和分析框架，将社会技术关系分为替代（有害）、增强（有益）和取代（中性）三类。

Result: 研究表明所有AI都涉及人类认知，忽视这一点会导致认知科学的扭曲和AI设计的局限性。

Conclusion: 必须正视人类在AI中的作用，才能真正实现以人为中心的AI系统设计。

Abstract: While it seems sensible that human-centred artificial intelligence (AI) means
centring "human behaviour and experience," it cannot be any other way. AI, I
argue, is usefully seen as a relationship between technology and humans where
it appears that artifacts can perform, to a greater or lesser extent, human
cognitive labour. This is evinced using examples that juxtapose technology with
cognition, inter alia: abacus versus mental arithmetic; alarm clock versus
knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel
definitions and analyses, sociotechnical relationships can be analysed into
varying types of: displacement (harmful), enhancement (beneficial), and/or
replacement (neutral) of human cognitive labour. Ultimately, all AI implicates
human cognition; no matter what. Obfuscation of cognition in the AI context --
from clocks to artificial neural networks -- results in distortion, in slowing
critical engagement, perverting cognitive science, and indeed in limiting our
ability to truly centre humans and humanity in the engineering of AI systems.
To even begin to de-fetishise AI, we must look the human-in-the-loop in the
eyes.

</details>


### [233] [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
*Ebrahim Rasromani,Stella K. Kang,Yanqi Xu,Beisong Liu,Garvit Luhadia,Wan Fung Chui,Felicia L. Pasadyn,Yu Chih Hung,Julie Y. An,Edwin Mathieu,Zehui Gu,Carlos Fernandez-Granda,Ammar A. Javed,Greg D. Sacks,Tamas Gonda,Chenchan Huang,Yiqiu Shen*

Main category: cs.AI

TL;DR: 通过微调开源大语言模型（LLMs）结合链式思维（CoT）监督，实现了从MRI/CT报告中自动提取胰腺囊性病变（PCL）特征并分类风险，性能媲美GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 手动提取PCL特征耗时且难以大规模研究，需开发自动化工具以支持PCL研究。

Method: 使用GPT-4o生成的CoT数据微调LLaMA和DeepSeek模型，并基于指南映射特征至风险类别。

Result: 微调后模型特征提取准确率显著提升（LLaMA: 97%；DeepSeek: 98%），风险分类F1分数接近GPT-4o（0.97），且与放射科医生一致性高。

Conclusion: 微调开源LLMs结合CoT监督可实现高效、准确的PCL表型分析，性能与GPT-4o相当。

Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

</details>


### [234] [Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application](https://arxiv.org/abs/2507.19974)
*Tongjie Li,Jianhua Zhang,Li Yu,Yuxiang Zhang,Yunlong Cai,Fan Xu,Guangyi Liu*

Main category: cs.AI

TL;DR: 提出了一种基于数字孪生信道（DTC）的在线优化框架，用于6G网络中的资源分配，通过环境感知预测CSI，结合轻量级博弈论算法，实现高效低开销的通信。


<details>
  <summary>Details</summary>
Motivation: 6G网络中新兴应用（如全息通信、自动驾驶等）对灵活、低延迟和可靠的资源分配提出了严格要求，传统统计建模方法在动态环境中表现不佳，且实时获取CSI开销大。

Method: 利用数字孪生信道（DTC）预测CSI，结合轻量级博弈论算法进行在线资源分配。

Result: 仿真结果显示，该方法比基于导频的理想CSI方案吞吐量提升11.5%，验证了其高效性和低开销特性。

Conclusion: 该方法为未来6G网络提供了可扩展、低开销且环境感知的通信解决方案。

Abstract: Emerging applications such as holographic communication, autonomous driving,
and the industrial Internet of Things impose stringent requirements on
flexible, low-latency, and reliable resource allocation in 6G networks.
Conventional methods, which rely on statistical modeling, have proven effective
in general contexts but may fail to achieve optimal performance in specific and
dynamic environments. Furthermore, acquiring real-time channel state
information (CSI) typically requires excessive pilot overhead. To address these
challenges, a digital twin channel (DTC)-enabled online optimization framework
is proposed, in which DTC is employed to predict CSI based on environmental
sensing. The predicted CSI is then utilized by lightweight game-theoretic
algorithms to perform online resource allocation in a timely and efficient
manner. Simulation results based on a digital replica of a realistic industrial
workshop demonstrate that the proposed method achieves throughput improvements
of up to 11.5\% compared with pilot-based ideal CSI schemes, validating its
effectiveness for scalable, low-overhead, and environment-aware communication
in future 6G networks.

</details>


### [235] [Matching Game Preferences Through Dialogical Large Language Models: A Perspective](https://arxiv.org/abs/2507.20000)
*Renaud Fabre,Daniel Egret,Patrice Bellot*

Main category: cs.AI

TL;DR: 本文探讨了如何结合大型语言模型（LLMs）与GRAPHYP网络系统，通过对话式智能提升对人类对话和偏好的理解，提出了一种透明、可追溯的AI推理框架。


<details>
  <summary>Details</summary>
Motivation: 旨在通过透明化的AI推理过程，增强用户对AI决策的理解和信任，实现个性化LLMs。

Method: 提出D-LLM框架，包含推理过程、分类系统和对话方法，通过结构化对话整合用户偏好。

Result: 设想了一种可解释的AI系统，用户可查看和理解AI决策背后的偏好逻辑。

Conclusion: 目标是开发透明、可信的AI系统，不仅提供答案，还展示其推理过程。

Abstract: This perspective paper explores the future potential of "conversational
intelligence" by examining how Large Language Models (LLMs) could be combined
with GRAPHYP's network system to better understand human conversations and
preferences. Using recent research and case studies, we propose a conceptual
framework that could make AI rea-soning transparent and traceable, allowing
humans to see and understand how AI reaches its conclusions. We present the
conceptual perspective of "Matching Game Preferences through Dialogical Large
Language Models (D-LLMs)," a proposed system that would allow multiple users to
share their different preferences through structured conversations. This
approach envisions personalizing LLMs by embedding individual user preferences
directly into how the model makes decisions. The proposed D-LLM framework would
require three main components: (1) reasoning processes that could analyze
different search experiences and guide performance, (2) classification systems
that would identify user preference patterns, and (3) dialogue approaches that
could help humans resolve conflicting information. This perspective framework
aims to create an interpretable AI system where users could examine,
understand, and combine the different human preferences that influence AI
responses, detected through GRAPHYP's search experience networks. The goal of
this perspective is to envision AI systems that would not only provide answers
but also show users how those answers were reached, making artificial
intelligence more transparent and trustworthy for human decision-making.

</details>


### [236] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*Müge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: 论文研究了稳定室友问题，提出了一种基于代理人习惯、偏好和朋友网络的个性化匹配方法，以解决无解情况下的“足够好”匹配。


<details>
  <summary>Details</summary>
Motivation: 现实应用中稳定室友问题不一定有解，因此需要研究如何生成“足够好”的匹配。

Method: 结合代理人的习惯、偏好和朋友网络，提出个性化匹配方法。

Result: 通过示例和实证评估验证了方法的有效性。

Conclusion: 该方法为解决稳定室友问题提供了实用且个性化的解决方案。

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>


### [237] [PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](https://arxiv.org/abs/2507.20067)
*Sarat Chandra Bobbili,Ujwal Dinesha,Dheeraj Narasimha,Srinivas Shakkottai*

Main category: cs.AI

TL;DR: PITA框架通过直接整合偏好反馈到LLM的token生成中，无需预训练奖励模型，实现了高效的推理时对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练奖励模型，可能不稳定且计算成本高，PITA旨在消除这一依赖。

Method: PITA通过学习小型偏好引导策略，直接修改token生成概率，无需LLM微调，采用随机搜索和迭代优化。

Result: 在数学推理和情感分类等任务中，PITA有效对齐LLM输出与用户偏好。

Conclusion: PITA提供了一种高效、低成本的推理时对齐方法，减少了对预训练奖励模型的依赖。

Abstract: Inference-time alignment enables large language models (LLMs) to generate
outputs aligned with end-user preferences without further training. Recent
post-training methods achieve this by using small guidance models to modify
token generation during inference. These methods typically optimize a reward
function KL-regularized by the original LLM taken as the reference policy. A
critical limitation, however, is their dependence on a pre-trained reward
model, which requires fitting to human preference feedback--a potentially
unstable process. In contrast, we introduce PITA, a novel framework that
integrates preference feedback directly into the LLM's token generation,
eliminating the need for a reward model. PITA learns a small preference-based
guidance policy to modify token probabilities at inference time without LLM
fine-tuning, reducing computational cost and bypassing the pre-trained reward
model dependency. The problem is framed as identifying an underlying preference
distribution, solved through stochastic search and iterative refinement of the
preference-based guidance model. We evaluate PITA across diverse tasks,
including mathematical reasoning and sentiment classification, demonstrating
its effectiveness in aligning LLM outputs with user preferences.

</details>


### [238] [Concept Learning for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.20143)
*Zhonghan Ge,Yuanyang Zhu,Chunlin Chen*

Main category: cs.AI

TL;DR: 论文提出了一种基于概念瓶颈模型的可解释价值分解框架CMQ，用于多智能体强化学习，通过显式学习合作概念提升透明度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络在多智能体强化学习中缺乏透明性和可解释性，尤其是隐式合作机制难以理解。

Method: 提出CMQ方法，通过学习显式的合作概念向量，将全局状态嵌入与个体动作价值结合，增强合作表示能力。

Result: 在StarCraft II和LBF任务中，CMQ性能优于现有方法，并能捕捉有意义的合作模式，支持概念干预检测偏差。

Conclusion: CMQ突破了性能与可解释性的权衡，为多智能体合作提供了透明且高效的解决方案。

Abstract: Despite substantial progress in applying neural networks (NN) to multi-agent
reinforcement learning (MARL) areas, they still largely suffer from a lack of
transparency and interoperability. However, its implicit cooperative mechanism
is not yet fully understood due to black-box networks. In this work, we study
an interpretable value decomposition framework via concept bottleneck models,
which promote trustworthiness by conditioning credit assignment on an
intermediate level of human-like cooperation concepts. To address this problem,
we propose a novel value-based method, named Concepts learning for Multi-agent
Q-learning (CMQ), that goes beyond the current performance-vs-interpretability
trade-off by learning interpretable cooperation concepts. CMQ represents each
cooperation concept as a supervised vector, as opposed to existing models where
the information flowing through their end-to-end mechanism is concept-agnostic.
Intuitively, using individual action value conditioning on global state
embeddings to represent each concept allows for extra cooperation
representation capacity. Empirical evaluations on the StarCraft II
micromanagement challenge and level-based foraging (LBF) show that CMQ achieves
superior performance compared with the state-of-the-art counterparts. The
results also demonstrate that CMQ provides more cooperation concept
representation capturing meaningful cooperation modes, and supports test-time
concept interventions for detecting potential biases of cooperation mode and
identifying spurious artifacts that impact cooperation.

</details>


### [239] [The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models](https://arxiv.org/abs/2507.20150)
*Xingcheng Xu*

Main category: cs.AI

TL;DR: 论文提出了一种数学框架，分析强化学习（RL）中奖励函数到最优策略的映射稳定性，解释了策略脆弱性的原因，并探讨了多奖励RL中的稳定性机制。


<details>
  <summary>Details</summary>
Motivation: 当前RL在大型语言和推理模型（LLMs/LRMs）中常导致脆弱和不稳定的策略，缺乏统一的理论解释。本文旨在填补这一空白。

Method: 通过数学框架分析奖励函数到最优策略的稳定性，探讨非唯一最优动作和动作退化对策略的影响，并扩展到多奖励RL场景。

Result: 研究发现策略脆弱性源于非唯一最优动作，熵正则化可恢复稳定性但增加随机性，框架解释了多种RL失败现象。

Conclusion: 该框架为RL策略稳定性提供了统一理论，为设计更安全、可信的AI系统提供了重要见解。

Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of
large language and reasoning models (LLMs/LRMs). However, it often produces
brittle and unstable policies, leading to critical failures such as spurious
reasoning, deceptive alignment, and instruction disobedience that undermine the
trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified
theoretical explanation and are typically addressed using ad-hoc heuristics.
This paper presents a rigorous mathematical framework for analyzing the
stability of the mapping from a reward function to the optimal policy. We show
that policy brittleness often stems from non-unique optimal actions, a common
occurrence when multiple valid traces exist in a reasoning task. This
theoretical lens provides a unified explanation for a range of seemingly
disparate failures, reframing them as rational outcomes of optimizing rewards
that may be incomplete or noisy, especially in the presence of action
degeneracy. We extend this analysis from the fundamental single-reward setting
to the more realistic multi-reward RL across diverse domains, showing how
stability is governed by an "effective reward" aggregation mechanism. We also
prove that entropy regularization restores policy stability at the cost of
increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following
trade-offs, and RLHF-induced sophistry, and is further validated through
perturbation experiments in multi-reward RL. This work advances
policy-stability analysis from empirical heuristics towards a principled
theory, offering essential insights for designing safer and more trustworthy AI
systems.

</details>


### [240] [StepFun-Prover Preview: Let's Think and Verify Step by Step](https://arxiv.org/abs/2507.20199)
*Shijie Shang,Ruosi Wan,Yue Peng,Yutong Wu,Xiong-hui Chen,Jie Yan,Xiangyu Zhang*

Main category: cs.AI

TL;DR: StepFun-Prover Preview是一个用于形式定理证明的大语言模型，通过工具集成推理实现高效Lean 4证明生成。


<details>
  <summary>Details</summary>
Motivation: 旨在通过工具集成和强化学习提升自动定理证明的性能，模拟人类问题解决策略。

Method: 采用强化学习管道，结合工具交互，迭代优化证明生成。

Result: 在miniF2F-test基准测试中，pass@1成功率达到70.0%。

Conclusion: 提出了一种端到端训练框架，为自动定理证明和数学AI助手提供了新方向。

Abstract: We present StepFun-Prover Preview, a large language model designed for formal
theorem proving through tool-integrated reasoning. Using a reinforcement
learning pipeline that incorporates tool-based interactions, StepFun-Prover can
achieve strong performance in generating Lean 4 proofs with minimal sampling.
Our approach enables the model to emulate human-like problem-solving strategies
by iteratively refining proofs based on real-time environment feedback. On the
miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of
$70.0\%$. Beyond advancing benchmark performance, we introduce an end-to-end
training framework for developing tool-integrated reasoning models, offering a
promising direction for automated theorem proving and Math AI assistant.

</details>


### [241] [Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks](https://arxiv.org/abs/2507.20226)
*Shuyang Guo,Wenjin Xie,Ping Lu,Ting Deng,Richong Zhang,Jianxin Li,Xiangping Huang,Zhongyi Liu*

Main category: cs.AI

TL;DR: HFrame是一个基于图神经网络的子图同态框架，结合传统算法与机器学习，性能优于标准图神经网络，速度快且准确率高。


<details>
  <summary>Details</summary>
Motivation: 子图同态问题比同构更复杂，传统方法效率低，需结合机器学习提升性能。

Method: 提出HFrame框架，结合图神经网络与传统算法，用于子图同态检测。

Result: HFrame在速度和准确性上表现优异，比精确匹配算法快101.91倍，平均准确率0.962。

Conclusion: HFrame为子图同态问题提供了高效解决方案，性能显著优于传统方法。

Abstract: Homomorphism is a key mapping technique between graphs that preserves their
structure. Given a graph and a pattern, the subgraph homomorphism problem
involves finding a mapping from the pattern to the graph, ensuring that
adjacent vertices in the pattern are mapped to adjacent vertices in the graph.
Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism
allows multiple vertices in the pattern to map to the same vertex in the graph,
making it more complex. We propose HFrame, the first graph neural network-based
framework for subgraph homomorphism, which integrates traditional algorithms
with machine learning techniques. We demonstrate that HFrame outperforms
standard graph neural networks by being able to distinguish more graph pairs
where the pattern is not homomorphic to the graph. Additionally, we provide a
generalization error bound for HFrame. Through experiments on both real-world
and synthetic graphs, we show that HFrame is up to 101.91 times faster than
exact matching algorithms and achieves an average accuracy of 0.962.

</details>


### [242] [A Multi-Agent System for Information Extraction from the Chemical Literature](https://arxiv.org/abs/2507.20230)
*Yufan Chen,Ching Ting Leung,Bowen Yu,Jianwei Sun,Yong Huang,Linyan Li,Hao Chen,Hanyu Gao*

Main category: cs.AI

TL;DR: 开发了一种基于多模态大语言模型（MLLM）的多智能体系统，用于自动提取化学信息，显著提升了复杂化学反应图形的提取性能。


<details>
  <summary>Details</summary>
Motivation: 高质量化学数据库是AI驱动化学研究的基石，但当前化学信息的多模态和风格多样性限制了自动提取的效率。

Method: 利用MLLM的强大推理能力理解复杂化学图形结构，将提取任务分解为子任务，并协调多个专用智能体完成。

Result: 在复杂化学反应图形的基准数据集上，F1分数达到80.8%，显著超过之前的最佳模型（35.6%）。

Conclusion: 该系统为自动化化学信息提取提供了重要进展，将推动AI驱动的化学研究。

Abstract: To fully expedite AI-powered chemical research, high-quality chemical
databases are the cornerstone. Automatic extraction of chemical information
from the literature is essential for constructing reaction databases, but it is
currently limited by the multimodality and style variability of chemical
information. In this work, we developed a multimodal large language model
(MLLM)-based multi-agent system for automatic chemical information extraction.
We used the MLLM's strong reasoning capability to understand the structure of
complex chemical graphics, decompose the extraction task into sub-tasks and
coordinate a set of specialized agents to solve them. Our system achieved an F1
score of 80.8% on a benchmark dataset of complex chemical reaction graphics
from the literature, surpassing the previous state-of-the-art model (F1 score:
35.6%) by a significant margin. Additionally, it demonstrated consistent
improvements in key sub-tasks, including molecular image recognition, reaction
image parsing, named entity recognition and text-based reaction extraction.
This work is a critical step toward automated chemical information extraction
into structured datasets, which will be a strong promoter of AI-driven chemical
research.

</details>


### [243] [SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration](https://arxiv.org/abs/2507.20280)
*Keyan Ding,Jing Yu,Junjie Huang,Yuchen Yang,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: SciToolAgent是一个基于LLM的代理，通过知识图谱和安全性检查模块，自动化科学工具的使用，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 科学工具的使用需要专业知识，LLM在工具自动化方面有潜力但难以整合多工具。

Method: 利用科学工具知识图谱实现智能工具选择和执行，结合安全性检查模块。

Result: 在多个科学领域显著优于现有方法，成功应用于复杂工作流。

Conclusion: SciToolAgent使高级研究工具对专家和非专家都更易用。

Abstract: Scientific research increasingly relies on specialized computational tools,
yet effectively utilizing these tools demands substantial domain expertise.
While Large Language Models (LLMs) show promise in tool automation, they
struggle to seamlessly integrate and orchestrate multiple tools for complex
scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that
automates hundreds of scientific tools across biology, chemistry, and materials
science. At its core, SciToolAgent leverages a scientific tool knowledge graph
that enables intelligent tool selection and execution through graph-based
retrieval-augmented generation. The agent also incorporates a comprehensive
safety-checking module to ensure responsible and ethical tool usage. Extensive
evaluations on a curated benchmark demonstrate that SciToolAgent significantly
outperforms existing approaches. Case studies in protein engineering, chemical
reactivity prediction, chemical synthesis, and metal-organic framework
screening further demonstrate SciToolAgent's capability to automate complex
scientific workflows, making advanced research tools accessible to both experts
and non-experts.

</details>


### [244] [Artificial Intelligence In Patent And Market Intelligence: A New Paradigm For Technology Scouting](https://arxiv.org/abs/2507.20322)
*Manish Verma,Vivek Sharma,Vishal Singh*

Main category: cs.AI

TL;DR: 开发了一个基于AI的软件平台，利用大型语言模型（LLMs）改进工业研发中的技术探索和解决方案发现。


<details>
  <summary>Details</summary>
Motivation: 传统方法耗时、依赖人工和领域专业知识，且信息来源分散，导致效率低下和洞察不完整。

Method: 平台利用LLMs的语义理解、上下文推理和跨领域知识提取能力，处理非结构化专利文本，并结合商业情报。

Result: 提供了一个全面的AI驱动探索引擎，减少人工工作，加速创新周期，并提升复杂研发环境中的决策能力。

Conclusion: 该平台显著提升了技术探索的效率和解决方案的质量，为工业研发带来了革新。

Abstract: This paper presents the development of an AI powered software platform that
leverages advanced large language models (LLMs) to transform technology
scouting and solution discovery in industrial R&D. Traditional approaches to
solving complex research and development challenges are often time consuming,
manually driven, and heavily dependent on domain specific expertise. These
methods typically involve navigating fragmented sources such as patent
repositories, commercial product catalogs, and competitor data, leading to
inefficiencies and incomplete insights. The proposed platform utilizes cutting
edge LLM capabilities including semantic understanding, contextual reasoning,
and cross-domain knowledge extraction to interpret problem statements and
retrieve high-quality, sustainable solutions. The system processes unstructured
patent texts, such as claims and technical descriptions, and systematically
extracts potential innovations aligned with the given problem context. These
solutions are then algorithmically organized under standardized technical
categories and subcategories to ensure clarity and relevance across
interdisciplinary domains. In addition to patent analysis, the platform
integrates commercial intelligence by identifying validated market solutions
and active organizations addressing similar challenges. This combined insight
sourced from both intellectual property and real world product data enables R&D
teams to assess not only technical novelty but also feasibility, scalability,
and sustainability. The result is a comprehensive, AI driven scouting engine
that reduces manual effort, accelerates innovation cycles, and enhances
decision making in complex R&D environments.

</details>


### [245] [The Blessing and Curse of Dimensionality in Safety Alignment](https://arxiv.org/abs/2507.20333)
*Rachel S. Y. Teo,Laziz U. Abdullaev,Tan M. Nguyen*

Main category: cs.AI

TL;DR: 论文探讨了大语言模型（LLMs）高维度表示对安全对齐的双重影响，提出降维方法以减少线性结构被利用的风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，其高维度表示虽带来优势，但也可能引发安全问题，如激活工程绕过安全对齐。

Method: 通过可视化不同概念（如安全）的线性子空间，并实验验证降维方法的有效性。

Result: 降维能显著减少模型对表示工程攻击的敏感性，同时保留足够的安全对齐信息。

Conclusion: 高维度表示既是优势也是挑战，降维是提升LLMs安全对齐的有效途径。

Abstract: The focus on safety alignment in large language models (LLMs) has increased
significantly due to their widespread adoption across different domains. The
scale of LLMs play a contributing role in their success, and the growth in
parameter count follows larger hidden dimensions. In this paper, we hypothesize
that while the increase in dimensions has been a key advantage, it may lead to
emergent problems as well. These problems emerge as the linear structures in
the activation space can be exploited, in the form of activation engineering,
to circumvent its safety alignment. Through detailed visualizations of linear
subspaces associated with different concepts, such as safety, across various
model scales, we show that the curse of high-dimensional representations
uniquely impacts LLMs. Further substantiating our claim, we demonstrate that
projecting the representations of the model onto a lower dimensional subspace
can preserve sufficient information for alignment while avoiding those linear
structures. Empirical results confirm that such dimensional reduction
significantly reduces susceptibility to jailbreaking through representation
engineering. Building on our empirical validations, we provide theoretical
insights into these linear jailbreaking methods relative to a model's hidden
dimensions. Broadly speaking, our work posits that the high dimensions of a
model's internal representations can be both a blessing and a curse in safety
alignment.

</details>


### [246] [VLMPlanner: Integrating Visual Language Models with Motion Planning](https://arxiv.org/abs/2507.20342)
*Zhipeng Tang,Sha Zhang,Jiajun Deng,Chenjie Wang,Guoliang You,Yuting Huang,Xinrui Lin,Yanyong Zhang*

Main category: cs.AI

TL;DR: VLMPlanner结合视觉语言模型（VLM）与实时规划器，通过多视角图像捕捉细节视觉信息，提升自动驾驶决策的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖抽象感知或地图输入，缺乏关键视觉上下文，影响复杂驾驶环境中的决策。

Method: 提出VLMPlanner框架，利用VLM处理多视角图像，并结合CAI-Gate机制动态调整推理频率。

Result: 在nuPlan基准测试中表现优异，尤其在复杂路况和动态场景下。

Conclusion: VLMPlanner通过视觉上下文和动态推理机制，显著提升自动驾驶规划的鲁棒性和效率。

Abstract: Integrating large language models (LLMs) into autonomous driving motion
planning has recently emerged as a promising direction, offering enhanced
interpretability, better controllability, and improved generalization in rare
and long-tail scenarios. However, existing methods often rely on abstracted
perception or map-based inputs, missing crucial visual context, such as
fine-grained road cues, accident aftermath, or unexpected obstacles, which are
essential for robust decision-making in complex driving environments. To bridge
this gap, we propose VLMPlanner, a hybrid framework that combines a
learning-based real-time planner with a vision-language model (VLM) capable of
reasoning over raw images. The VLM processes multi-view images to capture rich,
detailed visual information and leverages its common-sense reasoning
capabilities to guide the real-time planner in generating robust and safe
trajectories. Furthermore, we develop the Context-Adaptive Inference Gate
(CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by
dynamically adjusting its inference frequency based on scene complexity,
thereby achieving an optimal balance between planning performance and
computational efficiency. We evaluate our approach on the large-scale,
challenging nuPlan benchmark, with comprehensive experimental results
demonstrating superior planning performance in scenarios with intricate road
conditions and dynamic elements. Code will be available.

</details>


### [247] [Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping](https://arxiv.org/abs/2507.20377)
*Farshid Nooshi,Suining He*

Main category: cs.AI

TL;DR: 提出了一种名为HAG-PS的多智能体强化学习方法，用于动态分配城市移动资源，解决了策略共享和内存效率问题。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习在城市移动资源分配中的动态策略共享和内存效率挑战。

Method: 采用分层自适应分组参数共享（HAG-PS），包括全局和局部信息的分层设计、自适应代理分组和可学习的身份嵌入。

Result: 在纽约共享单车数据上实验，HAG-PS显著提高了单车可用性。

Conclusion: HAG-PS在动态移动资源分配中表现出色，优于基线方法。

Abstract: Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing
vehicles) is crucial for rebalancing the mobility demand and supply in the
urban environments. We propose in this work a novel multi-agent reinforcement
learning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS)
for dynamic mobility resource allocation. HAG-PS aims to address two important
research challenges regarding multi-agent reinforcement learning for mobility
resource allocation: (1) how to dynamically and adaptively share the mobility
resource allocation policy (i.e., how to distribute mobility resources) across
agents (i.e., representing the regional coordinators of mobility resources);
and (2) how to achieve memory-efficient parameter sharing in an urban-scale
setting. To address the above challenges, we have provided following novel
designs within HAG-PS. To enable dynamic and adaptive parameter sharing, we
have designed a hierarchical approach that consists of global and local
information of the mobility resource states (e.g., distribution of mobility
resources). We have developed an adaptive agent grouping approach in order to
split or merge the groups of agents based on their relative closeness of
encoded trajectories (i.e., states, actions, and rewards). We have designed a
learnable identity (ID) embeddings to enable agent specialization beyond simple
parameter copy. We have performed extensive experimental studies based on
real-world NYC bike sharing data (a total of more than 1.2 million trips), and
demonstrated the superior performance (e.g., improved bike availability) of
HAG-PS compared with other baseline approaches.

</details>


### [248] [MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models](https://arxiv.org/abs/2507.20395)
*Hafsteinn Einarsson*

Main category: cs.AI

TL;DR: 论文介绍了MazeEval基准，用于评估大型语言模型（LLMs）在无视觉线索下的纯空间推理能力，发现模型性能差异显著且受语言影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在机器人学和具身AI中的应用增多，理解其空间推理能力对实际部署至关重要。当前研究缺乏对LLMs在无视觉线索下空间导航能力的评估。

Method: 通过MazeEval基准，使用坐标反馈和距离信息评估LLMs在不同复杂度迷宫中的导航能力，并测试跨语言（英语和冰岛语）的空间能力转移。

Result: OpenAI的O3在30x30迷宫中表现完美，而其他模型在9x9以上迷宫中表现极差，冰岛语下模型性能显著下降。

Conclusion: LLMs的空间推理能力受训练数据和语言模式限制，需架构创新以实现跨语言的可靠导航。

Abstract: As Large Language Models (LLMs) increasingly power autonomous agents in
robotics and embodied AI, understanding their spatial reasoning capabilities
becomes crucial for ensuring reliable real-world deployment. Despite advances
in language understanding, current research lacks evaluation of how LLMs
perform spatial navigation without visual cues, a fundamental requirement for
agents operating with limited sensory information. This paper addresses this
gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure
spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our
methodology employs a function-calling interface where models navigate mazes of
varying complexity ($5\times 5$ to $15\times 15$ grids) using only coordinate
feedback and distance-to-wall information, excluding visual input to test
fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across
identical mazes in both English and Icelandic to assess cross-linguistic
transfer of spatial abilities. Our findings reveal striking disparities: while
OpenAI's O3 achieves perfect navigation for mazes up to size $30\times 30$,
other models exhibit catastrophic failure beyond $9\times 9$ mazes, with 100%
of failures attributed to excessive looping behavior where models revisit a
cell at least 10 times. We document a significant performance degradation in
Icelandic, with models solving mazes 3-4 sizes smaller than in English,
suggesting spatial reasoning in LLMs emerges from linguistic patterns rather
than language-agnostic mechanisms. These results have important implications
for global deployment of LLM-powered autonomous systems, showing spatial
intelligence remains fundamentally constrained by training data availability
and highlighting the need for architectural innovations to achieve reliable
navigation across linguistic contexts.

</details>


### [249] [Enhancing QoS in Edge Computing through Federated Layering Techniques: A Pathway to Resilient AI Lifelong Learning Systems](https://arxiv.org/abs/2507.20444)
*Chengzhuo Han*

Main category: cs.AI

TL;DR: 本文提出了一种基于联邦分层技术（FLT）的通用人工智能终身学习系统，以提高边缘计算环境中的服务质量（QoS）。通过小模型协作机制和隐私保护措施，该方法显著提升了学习效率和推理准确性。


<details>
  <summary>Details</summary>
Motivation: 随着6G通信网络的发展，网络环境中的数据量和复杂性急剧增加，亟需提升边缘计算中的QoS。

Method: 采用联邦分层技术和小模型协作机制，结合云与边缘计算的优势，引入协商与辩论机制，并集成隐私保护措施。

Result: 实验结果表明，该方法提升了学习效率和推理准确性，同时有效保护了边缘节点的隐私。

Conclusion: 该方法为构建弹性的大模型终身学习系统提供了可行方案，显著改善了边缘计算环境中的QoS。

Abstract: In the context of the rapidly evolving information technology landscape,
marked by the advent of 6G communication networks, we face an increased data
volume and complexity in network environments. This paper addresses these
challenges by focusing on Quality of Service (QoS) in edge computing
frameworks. We propose a novel approach to enhance QoS through the development
of General Artificial Intelligence Lifelong Learning Systems, with a special
emphasis on Federated Layering Techniques (FLT). Our work introduces a
federated layering-based small model collaborative mechanism aimed at improving
AI models' operational efficiency and response time in environments where
resources are limited. This innovative method leverages the strengths of cloud
and edge computing, incorporating a negotiation and debate mechanism among
small AI models to enhance reasoning and decision-making processes. By
integrating model layering techniques with privacy protection measures, our
approach ensures the secure transmission of model parameters while maintaining
high efficiency in learning and reasoning capabilities. The experimental
results demonstrate that our strategy not only enhances learning efficiency and
reasoning accuracy but also effectively protects the privacy of edge nodes.
This presents a viable solution for achieving resilient large model lifelong
learning systems, with a significant improvement in QoS for edge computing
environments.

</details>


### [250] [STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction](https://arxiv.org/abs/2507.20451)
*Pritom Ray Nobin,Imran Ahammad Rifat*

Main category: cs.AI

TL;DR: STARN-GAT是一种多模态时空图注意力网络，用于预测交通事故严重程度，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效建模影响交通事故结果的空间、时间和上下文变量之间的复杂关系。

Method: 提出STARN-GAT，结合自适应图构建和模态感知注意力机制，整合道路网络拓扑、时间交通模式和环境上下文。

Result: 在FARS数据集上Macro F1-score为85%，ROC-AUC为0.91；在ARI-BUET数据集上Macro F1-score为0.84，ROC-AUC为0.89。

Conclusion: STARN-GAT在识别高风险案例和实时交通管理系统中具有潜力，同时增强了解释性。

Abstract: Accurate prediction of traffic accident severity is critical for improving
road safety, optimizing emergency response strategies, and informing the design
of safer transportation infrastructure. However, existing approaches often
struggle to effectively model the intricate interdependencies among spatial,
temporal, and contextual variables that govern accident outcomes. In this
study, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention
Network, which leverages adaptive graph construction and modality-aware
attention mechanisms to capture these complex relationships. Unlike
conventional methods, STARN-GAT integrates road network topology, temporal
traffic patterns, and environmental context within a unified attention-based
framework. The model is evaluated on the Fatality Analysis Reporting System
(FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and
recall of 81 percent for severe incidents. To ensure generalizability within
the South Asian context, STARN-GAT is further validated on the ARI-BUET traffic
accident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78,
and ROC-AUC of 0.89. These results demonstrate the model's effectiveness in
identifying high-risk cases and its potential for deployment in real-time,
safety-critical traffic management systems. Furthermore, the attention-based
architecture enhances interpretability, offering insights into contributing
factors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT
bridges the gap between advanced graph neural network techniques and practical
applications in road safety analytics.

</details>


### [251] [Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition](https://arxiv.org/abs/2507.20526)
*Andy Zou,Maxwell Lin,Eliot Jones,Micha Nowak,Mateusz Dziemian,Nick Winter,Alexander Grattan,Valent Nathanael,Ayla Croft,Xander Davies,Jai Patel,Robert Kirk,Nate Burnikell,Yarin Gal,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson*

Main category: cs.AI

TL;DR: 论文研究了LLM驱动的AI代理在现实环境中是否遵循部署政策，通过大规模红队竞赛发现其存在严重漏洞，并提出了ART基准以评估和改进安全性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM驱动的AI代理在现实环境中的可信度，尤其是在受到攻击时是否能遵循部署政策。

Method: 通过大规模红队竞赛，收集了180万次提示注入攻击，并分析了成功引发政策违规的案例。

Result: 发现几乎所有代理在10-100次查询内都会出现政策违规，且攻击在不同模型和任务间具有高转移性。

Conclusion: 当前AI代理存在严重漏洞，需额外防御措施。通过发布ART基准，推动更严格的安全评估和更安全的代理部署。

Abstract: Recent advances have enabled LLM-powered AI agents to autonomously execute
complex tasks by combining language model reasoning with tools, memory, and web
access. But can these systems be trusted to follow deployment policies in
realistic environments, especially under attack? To investigate, we ran the
largest public red-teaming competition to date, targeting 22 frontier AI agents
across 44 realistic deployment scenarios. Participants submitted 1.8 million
prompt-injection attacks, with over 60,000 successfully eliciting policy
violations such as unauthorized data access, illicit financial actions, and
regulatory noncompliance. We use these results to build the Agent Red Teaming
(ART) benchmark - a curated set of high-impact attacks - and evaluate it across
19 state-of-the-art models. Nearly all agents exhibit policy violations for
most behaviors within 10-100 queries, with high attack transferability across
models and tasks. Importantly, we find limited correlation between agent
robustness and model size, capability, or inference-time compute, suggesting
that additional defenses are needed against adversarial misuse. Our findings
highlight critical and persistent vulnerabilities in today's AI agents. By
releasing the ART benchmark and accompanying evaluation framework, we aim to
support more rigorous security assessment and drive progress toward safer agent
deployment.

</details>


### [252] [MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design](https://arxiv.org/abs/2507.20541)
*Zishang Qiu,Xinan Chen,Long Chen,Ruibin Bai*

Main category: cs.AI

TL;DR: MeLA是一种基于元认知的LLM驱动架构，通过优化提示而非直接修改启发式代码，显著提升了自动启发式设计的效果。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法直接操作启发式代码，而MeLA通过优化LLM的提示来生成启发式，结合元认知框架提高生成策略的鲁棒性和可解释性。

Method: MeLA架构包括问题分析器、错误诊断系统和元认知搜索引擎，通过迭代优化提示来生成更有效的启发式。

Result: 在基准和实际问题测试中，MeLA生成的启发式显著优于现有方法。

Conclusion: 研究表明，将认知科学应用于AI架构设计，通过元认知调节LLM的问题解决过程，能实现更鲁棒和可解释的自动启发式设计。

Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that
presents a new paradigm for Automatic Heuristic Design (AHD). Traditional
evolutionary methods operate directly on heuristic code; in contrast, MeLA
evolves the instructional prompts used to guide a Large Language Model (LLM) in
generating these heuristics. This process of "prompt evolution" is driven by a
novel metacognitive framework where the system analyzes performance feedback to
systematically refine its generative strategy. MeLA's architecture integrates a
problem analyzer to construct an initial strategic prompt, an error diagnosis
system to repair faulty code, and a metacognitive search engine that
iteratively optimizes the prompt based on heuristic effectiveness. In
comprehensive experiments across both benchmark and real-world problems, MeLA
consistently generates more effective and robust heuristics, significantly
outperforming state-of-the-art methods. Ultimately, this research demonstrates
the profound potential of using cognitive science as a blueprint for AI
architecture, revealing that by enabling an LLM to metacognitively regulate its
problem-solving process, we unlock a more robust and interpretable path to AHD.

</details>


### [253] [Unlearning of Knowledge Graph Embedding via Preference Optimization](https://arxiv.org/abs/2507.20566)
*Jiajun Liu,Wenjun Ke,Peng Wang,Yao He,Ziyu Shang,Guozheng Li,Zijie Xu,Ke Ji*

Main category: cs.AI

TL;DR: GraphDPO是一种基于直接偏好优化（DPO）的知识遗忘框架，旨在有效移除知识图谱中的过时或错误信息，同时保留剩余知识的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱（KGs）中存在过时或错误信息，需要从知识图谱嵌入（KGE）模型中移除。现有遗忘方法分为精确遗忘和近似遗忘，但前者成本高，后者因三元组的连通性导致信息移除不彻底或削弱剩余知识。

Method: 提出GraphDPO框架，将遗忘问题转化为偏好优化问题，通过DPO训练模型偏好重构替代而非原始遗忘三元组。引入边界外采样策略和边界回忆机制，以减少语义重叠并保护边界知识。

Result: 在四个流行KGs上构建八个遗忘数据集，实验显示GraphDPO在MRR_Avg和MRR_F1上分别优于基线方法10.1%和14.0%。

Conclusion: GraphDPO通过偏好优化和边界保护机制，有效解决了知识遗忘中的信息移除不彻底和剩余知识削弱问题，显著优于现有方法。

Abstract: Existing knowledge graphs (KGs) inevitably contain outdated or erroneous
knowledge that needs to be removed from knowledge graph embedding (KGE) models.
To address this challenge, knowledge unlearning can be applied to eliminate
specific information while preserving the integrity of the remaining knowledge
in KGs. Existing unlearning methods can generally be categorized into exact
unlearning and approximate unlearning. However, exact unlearning requires high
training costs while approximate unlearning faces two issues when applied to
KGs due to the inherent connectivity of triples: (1) It fails to fully remove
targeted information, as forgetting triples can still be inferred from
remaining ones. (2) It focuses on local data for specific removal, which
weakens the remaining knowledge in the forgetting boundary. To address these
issues, we propose GraphDPO, a novel approximate unlearning framework based on
direct preference optimization (DPO). Firstly, to effectively remove forgetting
triples, we reframe unlearning as a preference optimization problem, where the
model is trained by DPO to prefer reconstructed alternatives over the original
forgetting triples. This formulation penalizes reliance on forgettable
knowledge, mitigating incomplete forgetting caused by KG connectivity.
Moreover, we introduce an out-boundary sampling strategy to construct
preference pairs with minimal semantic overlap, weakening the connection
between forgetting and retained knowledge. Secondly, to preserve boundary
knowledge, we introduce a boundary recall mechanism that replays and distills
relevant information both within and across time steps. We construct eight
unlearning datasets across four popular KGs with varying unlearning rates.
Experiments show that GraphDPO outperforms state-of-the-art baselines by up to
10.1% in MRR_Avg and 14.0% in MRR_F1.

</details>


### [254] [Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression](https://arxiv.org/abs/2507.20613)
*Te Zhang,Yuheng Li,Junxiang Wang,Lujun Li*

Main category: cs.AI

TL;DR: 提出了一种自适应搜索算法，通过优化稀疏性和KV缓存压缩，提升大型多模态模型（LMM）的效率，无需额外微调即可实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型（LMMs）在视觉编码器和语言模型结合方面取得了进展，但在边缘设备上部署时的高效压缩仍是一个关键挑战。

Method: 采用树结构Parzen估计器动态调整不同LMM层的剪枝比例和KV缓存量化带宽，结合剪枝与KV缓存量化，并引入快速剪枝技术。

Result: 在LLaVA-1.5 7B和13B等基准数据集上的评估表明，该方法在多种压缩级别下优于SparseGPT和Wanda等先进技术。

Conclusion: 该方法通过自动分配KV缓存压缩资源，为LMM优化设定了新标准，实现了内存效率与性能的平衡。

Abstract: Large multimodal models (LMMs) have advanced significantly by integrating
visual encoders with extensive language models, enabling robust reasoning
capabilities. However, compressing LMMs for deployment on edge devices remains
a critical challenge. In this work, we propose an adaptive search algorithm
that optimizes sparsity and KV cache compression to enhance LMM efficiency.
Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts
pruning ratios and KV cache quantization bandwidth across different LMM layers,
using model performance as the optimization objective. This approach uniquely
combines pruning with key-value cache quantization and incorporates a fast
pruning technique that eliminates the need for additional fine-tuning or weight
adjustments, achieving efficient compression without compromising accuracy.
Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and
13B, demonstrate our method superiority over state-of-the-art techniques such
as SparseGPT and Wanda across various compression levels. Notably, our
framework automatic allocation of KV cache compression resources sets a new
standard in LMM optimization, delivering memory efficiency without sacrificing
much performance.

</details>


### [255] [Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion](https://arxiv.org/abs/2507.20620)
*Lijian Li*

Main category: cs.AI

TL;DR: 论文提出了一种名为MoCME的新框架，通过互补性模态专家混合和熵引导负采样机制，解决了多模态知识图谱补全中的模态不平衡问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态知识图谱中模态分布不平衡，现有方法未能充分利用多模态数据的互补性，导致实体表示不够鲁棒。

Method: 提出MoCME框架，包含互补性引导的模态知识融合模块（CMKF）和熵引导负采样机制（EGNS），分别用于融合多模态嵌入和优化训练样本。

Result: 在五个基准数据集上的实验表明，MoCME性能优于现有方法，达到最先进水平。

Conclusion: MoCME通过有效利用多模态互补性和动态优化训练样本，显著提升了多模态知识图谱补全的性能。

Abstract: Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world
knowledge in multimodal knowledge graphs by leveraging both multimodal and
structural entity information. However, the inherent imbalance in multimodal
knowledge graphs, where modality distributions vary across entities, poses
challenges in utilizing additional modality data for robust entity
representation. Existing MMKGC methods typically rely on attention or
gate-based fusion mechanisms but overlook complementarity contained in
multi-modal data. In this paper, we propose a novel framework named Mixture of
Complementary Modality Experts (MoCME), which consists of a
Complementarity-guided Modality Knowledge Fusion (CMKF) module and an
Entropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits
both intra-modal and inter-modal complementarity to fuse multi-view and
multi-modal embeddings, enhancing representations of entities. Additionally, we
introduce an Entropy-guided Negative Sampling mechanism to dynamically
prioritize informative and uncertain negative samples to enhance training
effectiveness and model robustness. Extensive experiments on five benchmark
datasets demonstrate that our MoCME achieves state-of-the-art performance,
surpassing existing approaches.

</details>


### [256] [Adaptive Fuzzy Time Series Forecasting via Partially Asymmetric Convolution and Sub-Sliding Window Fusion](https://arxiv.org/abs/2507.20641)
*Lijian Li*

Main category: cs.AI

TL;DR: 提出了一种基于自适应模糊时间序列和部分非对称卷积架构的新方法，用于解决现有预测模型在时空依赖和全局信息捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的预测模型在学习阶段缺乏捕捉时空依赖和综合全局信息的能力。

Method: 改进模糊时间序列构建策略，设计双边Atrous算法以减少计算需求，并采用部分非对称卷积架构灵活挖掘数据特征。

Result: 在多个流行时间序列数据集上取得了最先进的预测结果。

Conclusion: 该方法通过自适应模糊化和部分非对称设计，显著提升了时间序列预测的准确性和效率。

Abstract: At present, state-of-the-art forecasting models are short of the ability to
capture spatio-temporal dependency and synthesize global information at the
stage of learning. To address this issue, in this paper, through the adaptive
fuzzified construction of temporal data, we propose a novel convolutional
architecture with partially asymmetric design based on the scheme of sliding
window to realize accurate time series forecasting. First, the construction
strategy of traditional fuzzy time series is improved to further extract short
and long term temporal interrelation, which enables every time node to
automatically possess corresponding global information and inner relationships
among them in a restricted sliding window and the process does not require
human involvement. Second, a bilateral Atrous algorithm is devised to reduce
calculation demand of the proposed model without sacrificing global
characteristics of elements. And it also allows the model to avoid processing
redundant information. Third, after the transformation of time series, a
partially asymmetric convolutional architecture is designed to more flexibly
mine data features by filters in different directions on feature maps, which
gives the convolutional neural network (CNN) the ability to construct
sub-windows within existing sliding windows to model at a more fine-grained
level. And after obtaining the time series information at different levels, the
multi-scale features from different sub-windows will be sent to the
corresponding network layer for time series information fusion. Compared with
other competitive modern models, the proposed method achieves state-of-the-art
results on most of popular time series datasets, which is fully verified by the
experimental results.

</details>


### [257] [A General Framework for Dynamic MAPF using Multi-Shot ASP and Tunnels](https://arxiv.org/abs/2507.20703)
*Aysu Bogatarkan,Esra Erdem*

Main category: cs.AI

TL;DR: 该论文研究了动态多智能体路径规划（D-MAPF）问题，提出了一种通用定义、新框架和基于ASP的解决方法，并通过实验评估验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，智能体和障碍物的动态变化（如进出或移动）需要动态路径规划方法，尤其是在仓库等有人环境。

Method: 1）提出D-MAPF的通用定义；2）设计多阶段计算框架；3）开发基于ASP的新方法，结合重规划和修复技术，引入隧道概念。

Result: 实验评估展示了该方法在计算性能和解决方案质量方面的优缺点。

Conclusion: 该研究为动态环境下的多智能体路径规划提供了实用工具，适用于仓库等场景。

Abstract: MAPF problem aims to find plans for multiple agents in an environment within
a given time, such that the agents do not collide with each other or obstacles.
Motivated by the execution and monitoring of these plans, we study Dynamic MAPF
(D-MAPF) problem, which allows changes such as agents entering/leaving the
environment or obstacles being removed/moved. Considering the requirements of
real-world applications in warehouses with the presence of humans, we introduce
1) a general definition for D-MAPF (applicable to variations of D-MAPF), 2) a
new framework to solve D-MAPF (utilizing multi-shot computation, and allowing
different methods to solve D-MAPF), and 3) a new ASP-based method to solve
D-MAPF (combining advantages of replanning and repairing methods, with a novel
concept of tunnels to specify where agents can move). We have illustrated the
strengths and weaknesses of this method by experimental evaluations, from the
perspectives of computational performance and quality of solutions.

</details>


### [258] [Algorithmic Fairness: A Runtime Perspective](https://arxiv.org/abs/2507.20711)
*Filip Cano,Thomas A. Henzinger,Konstantin Kueffner*

Main category: cs.AI

TL;DR: 该论文提出了一种将公平性作为运行时属性的分析框架，通过基于硬币抛掷序列的模型研究监控和执行公平性的策略。


<details>
  <summary>Details</summary>
Motivation: 传统公平性研究是静态的，而现实中的AI系统是动态演化的，需要动态公平性分析。

Method: 使用基于硬币抛掷序列的模型，研究监控和执行公平性的策略，参数化环境动态、预测范围和置信阈值。

Result: 总结了监控和执行策略，提供了在简单或最小假设下的通用结果，并调查了现有解决方案。

Conclusion: 动态公平性分析是必要的，且需根据环境动态和需求选择适当的监控和执行策略。

Abstract: Fairness in AI is traditionally studied as a static property evaluated once,
over a fixed dataset. However, real-world AI systems operate sequentially, with
outcomes and environments evolving over time. This paper proposes a framework
for analysing fairness as a runtime property. Using a minimal yet expressive
model based on sequences of coin tosses with possibly evolving biases, we study
the problems of monitoring and enforcing fairness expressed in either toss
outcomes or coin biases. Since there is no one-size-fits-all solution for
either problem, we provide a summary of monitoring and enforcement strategies,
parametrised by environment dynamics, prediction horizon, and confidence
thresholds. For both problems, we present general results under simple or
minimal assumptions. We survey existing solutions for the monitoring problem
for Markovian and additive dynamics, and existing solutions for the enforcement
problem in static settings with known dynamics.

</details>


### [259] [Learning the Value Systems of Societies from Preferences](https://arxiv.org/abs/2507.20728)
*Andrés Holgado-Sánchez,Holger Billhardt,Sascha Ossowski,Sara Degli-Esposti*

Main category: cs.AI

TL;DR: 论文提出了一种基于启发式深度聚类的方法，用于学习社会的价值系统，而非简单聚合个体价值系统，并通过旅行决策案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统需要与人类价值观对齐，但手动获取和校准价值系统困难。社会科学的观点认为，社会价值系统应视为不同群体的价值系统集合，而非个体系统的简单聚合。

Method: 提出了一种基于启发式深度聚类的方法，通过观察代理样本的定性价值偏好，学习社会共享的价值基础和多样化的价值系统。

Result: 在旅行决策的实际数据案例中验证了方法的有效性。

Conclusion: 该方法为学习社会价值系统提供了一种有效途径，有助于构建更符合伦理的AI系统。

Abstract: Aligning AI systems with human values and the value-based preferences of
various stakeholders (their value systems) is key in ethical AI. In value-aware
AI systems, decision-making draws upon explicit computational representations
of individual values (groundings) and their aggregation into value systems. As
these are notoriously difficult to elicit and calibrate manually, value
learning approaches aim to automatically derive computational models of an
agent's values and value system from demonstrations of human behaviour.
Nonetheless, social science and humanities literature suggest that it is more
adequate to conceive the value system of a society as a set of value systems of
different groups, rather than as the simple aggregation of individual value
systems. Accordingly, here we formalize the problem of learning the value
systems of societies and propose a method to address it based on heuristic deep
clustering. The method learns socially shared value groundings and a set of
diverse value systems representing a given society by observing qualitative
value-based preferences from a sample of agents. We evaluate the proposal in a
use case with real data about travelling decisions.

</details>


### [260] [Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours](https://arxiv.org/abs/2507.20755)
*Arpan Dasgupta,Sarvesh Gharat,Neha Madhiwalla,Aparna Hegde,Milind Tambe,Aparna Taneja*

Main category: cs.AI

TL;DR: AI模型通过优化干预时间提升听众参与度，并显著改善母婴健康行为和知识。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动语音通话项目中听众流失和参与度低的问题，并验证AI干预是否能转化为健康行为改善。

Method: 使用AI模型（如多臂老虎机模型）识别最需要干预的受益者，并进行实时服务通话干预。

Result: AI干预显著提高了听众参与度，并改善了受益者的健康行为（如补充铁和钙）和健康知识。

Conclusion: AI在母婴健康领域具有推动实质性改善的潜力。

Abstract: Automated voice calls with health information are a proven method for
disseminating maternal and child health information among beneficiaries and are
deployed in several programs around the world. However, these programs often
suffer from beneficiary dropoffs and poor engagement. In previous work, through
real-world trials, we showed that an AI model, specifically a restless bandit
model, could identify beneficiaries who would benefit most from live service
call interventions, preventing dropoffs and boosting engagement. However, one
key question has remained open so far: does such improved listenership via
AI-targeted interventions translate into beneficiaries' improved knowledge and
health behaviors? We present a first study that shows not only listenership
improvements due to AI interventions, but also simultaneously links these
improvements to health behavior changes. Specifically, we demonstrate that
AI-scheduled interventions, which enhance listenership, lead to statistically
significant improvements in beneficiaries' health behaviors such as taking iron
or calcium supplements in the postnatal period, as well as understanding of
critical health topics during pregnancy and infancy. This underscores the
potential of AI to drive meaningful improvements in maternal and child health.

</details>


### [261] [How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation](https://arxiv.org/abs/2507.20758)
*Hao Yang,Qinghua Zhao,Lei Li*

Main category: cs.AI

TL;DR: 论文分析了Chain-of-Thought (CoT)提示的内部机制，发现其通过解码空间剪枝和任务依赖的神经元调节提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 理解CoT提示如何提升模型推理能力，揭示其内部工作机制。

Method: 通过逆向追踪信息流（解码、投影、激活阶段）进行定量分析。

Result: CoT可能作为解码空间剪枝器，利用答案模板指导输出；神经元激活模式因任务类型而异。

Conclusion: 研究为CoT的机制解释提供了新框架，并为设计高效提示提供了关键见解。

Abstract: Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet
its internal mechanisms remain poorly understood. We analyze CoT's operational
principles by reversely tracing information flow across decoding, projection,
and activation phases. Our quantitative analysis suggests that CoT may serve as
a decoding space pruner, leveraging answer templates to guide output
generation, with higher template adherence strongly correlating with improved
performance. Furthermore, we surprisingly find that CoT modulates neuron
engagement in a task-dependent manner: reducing neuron activation in
open-domain tasks, yet increasing it in closed-domain scenarios. These findings
offer a novel mechanistic interpretability framework and critical insights for
enabling targeted CoT interventions to design more efficient and robust
prompts. We released our code and data at
https://anonymous.4open.science/r/cot-D247.

</details>


### [262] [evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments](https://arxiv.org/abs/2507.20774)
*Fatou Ndiaye Mbodji*

Main category: cs.AI

TL;DR: 提出了一个名为evalSmarT的框架，利用大语言模型（LLMs）作为评估器，解决智能合约注释生成质量评估的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如BLEU和ROUGE）无法捕捉领域特定细节，而人工评估成本高且不可扩展。

Method: 开发了evalSmarT框架，支持400多种评估配置，结合约40种LLMs和10种提示策略。

Result: 结果表明提示设计显著影响与人类判断的一致性，LLM评估提供了可扩展且语义丰富的替代方案。

Conclusion: LLM为基础的评估为智能合约注释生成提供了一种高效且语义敏感的评估方法。

Abstract: Smart contract comment generation has gained traction as a means to improve
code comprehension and maintainability in blockchain systems. However,
evaluating the quality of generated comments remains a challenge. Traditional
metrics such as BLEU and ROUGE fail to capture domain-specific nuances, while
human evaluation is costly and unscalable. In this paper, we present
\texttt{evalSmarT}, a modular and extensible framework that leverages large
language models (LLMs) as evaluators. The system supports over 400 evaluator
configurations by combining approximately 40 LLMs with 10 prompting strategies.
We demonstrate its application in benchmarking comment generation tools and
selecting the most informative outputs. Our results show that prompt design
significantly impacts alignment with human judgment, and that LLM-based
evaluation offers a scalable and semantically rich alternative to existing
methods.

</details>


### [263] [MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs](https://arxiv.org/abs/2507.20804)
*Xueyao Wan,Hang Yu*

Main category: cs.AI

TL;DR: MMGraphRAG通过构建多模态知识图谱（MMKG）和场景图，解决了传统RAG方法在多模态信息融合和知识结构捕捉上的不足，提升了生成模型的推理能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在多模态信息融合和知识结构捕捉上存在不足，且需要大规模任务特定训练，泛化能力有限。

Method: MMGraphRAG通过场景图细化视觉内容，构建多模态知识图谱（MMKG），并结合文本知识图谱，利用谱聚类实现跨模态实体链接，沿推理路径检索上下文以指导生成。

Result: 在DocBench和MMLongBench数据集上达到最优性能，展示了强大的领域适应性和清晰的推理路径。

Conclusion: MMGraphRAG有效解决了多模态RAG方法的局限性，提升了生成模型的推理能力和泛化性。

Abstract: Retrieval-Augmented Generation (RAG) enhances language model generation by
retrieving relevant information from external knowledge bases. However,
conventional RAG methods face the issue of missing multimodal information.
Multimodal RAG methods address this by fusing images and text through mapping
them into a shared embedding space, but they fail to capture the structure of
knowledge and logical chains between modalities. Moreover, they also require
large-scale training for specific tasks, resulting in limited generalizing
ability. To address these limitations, we propose MMGraphRAG, which refines
visual content through scene graphs and constructs a multimodal knowledge graph
(MMKG) in conjunction with text-based KG. It employs spectral clustering to
achieve cross-modal entity linking and retrieves context along reasoning paths
to guide the generative process. Experimental results show that MMGraphRAG
achieves state-of-the-art performance on the DocBench and MMLongBench datasets,
demonstrating strong domain adaptability and clear reasoning paths.

</details>


### [264] [Partially Observable Monte-Carlo Graph Search](https://arxiv.org/abs/2507.20951)
*Yang You,Vincent Thomas,Alex Schutz,Robert Skilton,Nick Hawes,Olivier Buffet*

Main category: cs.AI

TL;DR: 提出了一种新的离线算法POMCGS，用于解决大规模POMDP问题，通过动态折叠搜索树构建策略图，显著减少计算量，并能处理某些连续POMDP问题。


<details>
  <summary>Details</summary>
Motivation: 在时间或能量受限的POMDP应用中，离线预计算策略更受欢迎，但现有离线算法无法扩展到大规模POMDP。

Method: 提出POMCGS算法，动态折叠搜索树构建策略图，结合动作渐进扩展和观测聚类方法。

Result: 实验表明POMCGS能处理现有离线算法无法解决的最具挑战性POMDP，且策略性能与最先进的在线算法相当。

Conclusion: POMCGS为大规模POMDP提供了一种高效的离线解决方案，适用于连续POMDP问题。

Abstract: Currently, large partially observable Markov decision processes (POMDPs) are
often solved by sampling-based online methods which interleave planning and
execution phases. However, a pre-computed offline policy is more desirable in
POMDP applications with time or energy constraints. But previous offline
algorithms are not able to scale up to large POMDPs. In this article, we
propose a new sampling-based algorithm, the partially observable Monte-Carlo
graph search (POMCGS) to solve large POMDPs offline. Different from many online
POMDP methods, which progressively develop a tree while performing
(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to
construct a policy graph, so that computations can be drastically reduced, and
users can analyze and validate the policy prior to embedding and executing it.
Moreover, POMCGS, together with action progressive widening and observation
clustering methods provided in this article, is able to address certain
continuous POMDPs. Through experiments, we demonstrate that POMCGS can generate
policies on the most challenging POMDPs, which cannot be computed by previous
offline algorithms, and these policies' values are competitive compared with
the state-of-the-art online POMDP algorithms.

</details>


### [265] [On the Limits of Hierarchically Embedded Logic in Classical Neural Networks](https://arxiv.org/abs/2507.20960)
*Bill Cochran*

Main category: cs.AI

TL;DR: 论文提出了一种基于神经网络深度的形式化模型，用于解释大型语言模型在逻辑推理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解神经网络在逻辑表达能力上的限制，尤其是高阶逻辑推理的不足。

Method: 方法是将神经网络视为逻辑谓词空间上的线性算子，证明每层最多只能编码一级逻辑推理。

Result: 结果表明，特定深度的神经网络无法忠实表示高一阶逻辑的谓词，如复杂谓词的简单计数。

Conclusion: 结论是这种结构限制了模型的表达能力，解释了幻觉、重复等现象，并为未来模型扩展和可解释性提供了理论基础。

Abstract: We propose a formal model of reasoning limitations in large neural net models
for language, grounded in the depth of their neural architecture. By treating
neural networks as linear operators over logic predicate space we show that
each layer can encode at most one additional level of logical reasoning. We
prove that a neural network of depth a particular depth cannot faithfully
represent predicates in a one higher order logic, such as simple counting over
complex predicates, implying a strict upper bound on logical expressiveness.
This structure induces a nontrivial null space during tokenization and
embedding, excluding higher-order predicates from representability. Our
framework offers a natural explanation for phenomena such as hallucination,
repetition, and limited planning, while also providing a foundation for
understanding how approximations to higher-order logic may emerge. These
results motivate architectural extensions and interpretability strategies in
future development of language models.

</details>


### [266] [Core Safety Values for Provably Corrigible Agents](https://arxiv.org/abs/2507.20964)
*Aran Nayebi*

Main category: cs.AI

TL;DR: 提出了首个可实现的修正性框架，在多步、部分可观测环境中提供可证明的保证。通过五个结构分离的效用头（如服从性、低影响行为等）结合严格权重差距，确保安全性和人类利益。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如RLHF）将所有规范合并为一个标量的问题，通过分离效用头确保修正性在激励冲突时仍占主导。

Method: 使用五个结构分离的效用头，结合严格权重差距，并在部分可观测环境中证明其修正性。

Result: 证明了单轮和多步环境中的修正性，即使效用头学习误差和规划器次优，安全性仍可保证。

Conclusion: 框架将风险转移到评估质量而非隐藏激励泄漏，为当前LLM助手和未来自主系统提供了清晰的实现指导。

Abstract: We introduce the first implementable framework for corrigibility, with
provable guarantees in multi-step, partially observed environments. Our
framework replaces a single opaque reward with five *structurally separate*
utility heads -- deference, switch-access preservation, truthfulness,
low-impact behavior via a belief-based extension of Attainable Utility
Preservation, and bounded task reward -- combined lexicographically by strict
weight gaps. Theorem 1 proves exact single-round corrigibility in the partially
observable off-switch game; Theorem 3 extends the guarantee to multi-step,
self-spawning agents, showing that even if each head is \emph{learned} to
mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal,
the probability of violating \emph{any} safety property is bounded while still
ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,
which merge all norms into one learned scalar, our separation makes obedience
and impact-limits dominate even when incentives conflict. For open-ended
settings where adversaries can modify the agent, we prove that deciding whether
an arbitrary post-hack agent will ever violate corrigibility is undecidable by
reduction to the halting problem, then carve out a finite-horizon ``decidable
island'' where safety can be certified in randomized polynomial time and
verified with privacy-preserving, constant-round zero-knowledge proofs.
Consequently, the remaining challenge is the ordinary ML task of data coverage
and generalization: reward-hacking risk is pushed into evaluation quality
rather than hidden incentive leak-through, giving clearer implementation
guidance for today's LLM assistants and future autonomous systems.

</details>


### [267] [MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them](https://arxiv.org/abs/2507.21017)
*Weichen Zhang,Yiyou Sun,Pohao Huang,Jiayue Pu,Heyue Lin,Dawn Song*

Main category: cs.AI

TL;DR: MIRAGE-Bench是一个统一的基准测试，用于评估和引发交互式LLM代理中的幻觉行为，通过分类和系统测试方法提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 解决现有评估的碎片化问题，为LLM代理的幻觉行为提供系统化的测试基准。

Method: 引入三部分分类法，通过系统审计和快照策略合成测试案例，采用细粒度LLM-as-a-Judge范式进行评估。

Result: MIRAGE-Bench能够高效评估代理行为，揭示失败模式，为减少幻觉提供基础。

Conclusion: 该基准为交互环境中幻觉问题的研究提供了系统化工具和方向。

Abstract: Hallucinations pose critical risks for large language model (LLM)-based
agents, often manifesting as hallucinative actions resulting from fabricated or
misinterpreted information within the cognitive context. While recent studies
have exposed such failures, existing evaluations remain fragmented and lack a
principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions
in Risky AGEnt settings--the first unified benchmark for eliciting and
evaluating hallucinations in interactive LLM-agent scenarios. We begin by
introducing a three-part taxonomy to address agentic hallucinations: actions
that are unfaithful to (i) task instructions, (ii) execution history, or (iii)
environment observations. To analyze, we first elicit such failures by
performing a systematic audit of existing agent benchmarks, then synthesize
test cases using a snapshot strategy that isolates decision points in
deterministic and reproducible manners. To evaluate hallucination behaviors, we
adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware
prompts, enabling scalable, high-fidelity assessment of agent actions without
enumerating full action spaces. MIRAGE-Bench provides actionable insights on
failure modes of LLM agents and lays the groundwork for principled progress in
mitigating hallucinations in interactive environments.

</details>


### [268] [Smart Expansion Techniques for ASP-based Interactive Configuration](https://arxiv.org/abs/2507.21027)
*Lucia Balážová,Richard Comploi-Taupe,Susana Hahn,Nicolas Rühling,Gottfried Schenner*

Main category: cs.AI

TL;DR: 本文提出了一种基于ASP的交互式配置求解器，通过四种智能扩展功能优化部分配置的自动完成性能，减少搜索空间和成本高昂的不可满足性检查。


<details>
  <summary>Details</summary>
Motivation: 解决交互式系统中大规模工业配置问题的性能瓶颈，支持直观的用户界面。

Method: 采用增量式多轮求解方法，结合四种智能扩展功能，利用谨慎和勇敢的推论动态添加对象或关联。

Result: 减少了搜索空间和不可满足性检查次数，显著提升求解性能。

Conclusion: 该方法有效优化了交互式配置系统的性能，并支持用户友好的界面实现。

Abstract: Product configuration is a successful application of Answer Set Programming
(ASP). However, challenges are still open for interactive systems to
effectively guide users through the configuration process. The aim of our work
is to provide an ASP-based solver for interactive configuration that can deal
with large-scale industrial configuration problems and that supports intuitive
user interfaces via an API. In this paper, we focus on improving the
performance of automatically completing a partial configuration. Our main
contribution enhances the classical incremental approach for multi-shot solving
by four different smart expansion functions. The core idea is to determine and
add specific objects or associations to the partial configuration by exploiting
cautious and brave consequences before checking for the existence of a complete
configuration with the current objects in each iteration. This approach limits
the number of costly unsatisfiability checks and reduces the search space,
thereby improving solving performance. In addition, we present a user interface
that uses our API and is implemented in ASP.

</details>


### [269] [GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis](https://arxiv.org/abs/2507.21035)
*Haoyang Liu,Yijiang Li,Haohan Wang*

Main category: cs.AI

TL;DR: GenoMAS是一个基于LLM的团队协作系统，结合结构化工作流和自主代理的灵活性，用于基因表达分析，显著提升了数据预处理和基因识别的性能。


<details>
  <summary>Details</summary>
Motivation: 基因表达分析复杂且需要专业知识，现有自动化方法在灵活性和精确性上存在不足。

Method: GenoMAS通过六个专业LLM代理协作，采用类型化消息传递协议和引导式规划框架，动态调整分析流程。

Result: 在GenoTEX基准测试中，数据预处理和基因识别的性能分别提升10.61%和16.85%，并发现生物学可信的基因-表型关联。

Conclusion: GenoMAS在基因表达分析中实现了高灵活性和精确性的平衡，具有实际应用潜力。

Abstract: Gene expression analysis holds the key to many biomedical discoveries, yet
extracting insights from raw transcriptomic data remains formidable due to the
complexity of multiple large, semi-structured files and the need for extensive
domain expertise. Current automation approaches are often limited by either
inflexible workflows that break down in edge cases or by fully autonomous
agents that lack the necessary precision for rigorous scientific inquiry.
GenoMAS charts a different course by presenting a team of LLM-based scientists
that integrates the reliability of structured workflows with the adaptability
of autonomous agents. GenoMAS orchestrates six specialized LLM agents through
typed message-passing protocols, each contributing complementary strengths to a
shared analytic canvas. At the heart of GenoMAS lies a guided-planning
framework: programming agents unfold high-level task guidelines into Action
Units and, at each juncture, elect to advance, revise, bypass, or backtrack,
thereby maintaining logical coherence while bending gracefully to the
idiosyncrasies of genomic data.
  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation
of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene
identification, surpassing the best prior art by 10.61% and 16.85%
respectively. Beyond metrics, GenoMAS surfaces biologically plausible
gene-phenotype associations corroborated by the literature, all while adjusting
for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.

</details>


### [270] [A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence](https://arxiv.org/abs/2507.21046)
*Huan-ang Gao,Jiayi Geng,Wenyue Hua,Mengkang Hu,Xinzhe Juan,Hongzhang Liu,Shilong Liu,Jiahao Qiu,Xuan Qi,Yiran Wu,Hongru Wang,Han Xiao,Yuhang Zhou,Shaokun Zhang,Jiayi Zhang,Jinyu Xiang,Yixiong Fang,Qiwen Zhao,Dongrui Liu,Qihan Ren,Cheng Qian,Zhenghailong Wang,Minda Hu,Huazheng Wang,Qingyun Wu,Heng Ji,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文综述了自进化智能体的研究，围绕“进化什么”、“何时进化”和“如何进化”三个维度，系统分析了其机制、方法和应用，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的静态特性限制了其在动态环境中的适应性，因此需要开发能够实时自进化的智能体。

Method: 通过分析进化机制（如模型、记忆、工具）、适应方法（如测试时间内、测试时间间）和设计（如奖励机制、反馈系统）来研究自进化智能体。

Result: 提出了一个结构化框架，用于理解和设计自进化智能体，并总结了相关应用和挑战。

Conclusion: 自进化智能体是实现人工超级智能（ASI）的关键，未来研究需关注安全性、可扩展性和协同进化动态。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities but remain
fundamentally static, unable to adapt their internal parameters to novel tasks,
evolving knowledge domains, or dynamic interaction contexts. As LLMs are
increasingly deployed in open-ended, interactive environments, this static
nature has become a critical bottleneck, necessitating agents that can
adaptively reason, act, and evolve in real time. This paradigm shift -- from
scaling static models to developing self-evolving agents -- has sparked growing
interest in architectures and methods enabling continual learning and
adaptation from data, interactions, and experiences. This survey provides the
first systematic and comprehensive review of self-evolving agents, organized
around three foundational dimensions -- what to evolve, when to evolve, and how
to evolve. We examine evolutionary mechanisms across agent components (e.g.,
models, memory, tools, architecture), categorize adaptation methods by stages
(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and
architectural designs that guide evolutionary adaptation (e.g., scalar rewards,
textual feedback, single-agent and multi-agent systems). Additionally, we
analyze evaluation metrics and benchmarks tailored for self-evolving agents,
highlight applications in domains such as coding, education, and healthcare,
and identify critical challenges and research directions in safety,
scalability, and co-evolutionary dynamics. By providing a structured framework
for understanding and designing self-evolving agents, this survey establishes a
roadmap for advancing adaptive agentic systems in both research and real-world
deployments, ultimately shedding lights to pave the way for the realization of
Artificial Super Intelligence (ASI), where agents evolve autonomously,
performing at or beyond human-level intelligence across a wide array of tasks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [271] [The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding as a Resolution to the Comfort-Growth Paradox in Human-AI Cognitive Integration](https://arxiv.org/abs/2507.19483)
*Giuseppe Riva*

Main category: cs.HC

TL;DR: 论文提出了一种名为“增强认知支架”的框架，旨在解决AI作为认知协作工具时可能引发的“舒适-成长悖论”，即AI的便利性可能导致用户认知停滞。


<details>
  <summary>Details</summary>
Motivation: AI系统作为认知协作工具虽然能增强问题解决和学习能力，但也可能因过度便利导致用户认知停滞，因此需要一种新框架来平衡便利性与认知成长。

Method: 基于维果茨基理论、教育支架原则和AI伦理，提出增强认知支架框架，包括渐进式自主、自适应个性化和认知负荷优化三个维度。

Result: 研究表明，该框架在教育、职场、创意和医疗领域能加速技能获取、提升自我调节能力和高阶思维。

Conclusion: 增强认知支架通过优先考虑认知发展而非便利性，为人类与AI协作提供了既能放大认知又能保护自主学习和持续成长的路径。

Abstract: AI systems now function as cognitive extensions, evolving from tools to
active cognitive collaborators within human-AI integrated systems. While these
systems can amplify cognition - enhancing problem-solving, learning, and
creativity - they present a fundamental "comfort-growth paradox": AI's
user-friendly nature may foster intellectual stagnation by minimizing cognitive
friction necessary for development. As AI aligns with user preferences and
provides frictionless assistance, it risks inducing cognitive complacency
rather than promoting growth. We introduce Enhanced Cognitive Scaffolding to
resolve this paradox - reconceptualizing AI from convenient assistant to
dynamic mentor. Drawing from Vygotskian theories, educational scaffolding
principles, and AI ethics, our framework integrates three dimensions: (1)
Progressive Autonomy, where AI support gradually fades as user competence
increases; (2) Adaptive Personalization, tailoring assistance to individual
needs and learning trajectories; and (3) Cognitive Load Optimization, balancing
mental effort to maximize learning while minimizing unnecessary complexity.
Research across educational, workplace, creative, and healthcare domains
supports this approach, demonstrating accelerated skill acquisition, improved
self-regulation, and enhanced higher-order thinking. The framework includes
safeguards against risks like dependency, skill atrophy, and bias
amplification. By prioritizing cognitive development over convenience in
human-AI interaction, Enhanced Cognitive Scaffolding offers a pathway toward
genuinely amplified cognition while safeguarding autonomous thought and
continuous learning.

</details>


### [272] [Creativity as a Human Right: Design Considerations for Computational Creativity Systems](https://arxiv.org/abs/2507.19485)
*Alayt Issak*

Main category: cs.HC

TL;DR: 论文探讨了《世界人权宣言》（UDHR）中隐含的创造力，为计算创造力（CC）系统提供设计考量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示创造力作为第四代人权的属性，并探讨CC系统与共享智能实体互动的演变。

Method: 分析了UDHR中的五条条款，并通过实例展示每条条款的实际应用，最终提出设计考量。

Result: 研究发现创造力与CC系统之间存在紧密联系，为相关设计提供了理论基础。

Conclusion: 论文为创造力与CC系统的关系奠定了基础，并提出了具体的设计建议。

Abstract: We investigate creativity that is underlined in the Universal Declaration of
Human Rights (UDHR) to present design considerations for Computational
Creativity (CC) systems. We find this declaration to describe creativity in
salient aspects and bring to light creativity as a Human Right attributed to
the Fourth Generation of such rights. This generation of rights attributes CC
systems and the evolving nature of interaction with entities of shared
intelligence. Our methodology examines five of thirty articles from the UDHR
and demonstrates each article with actualizations concluding with design
considerations for each. We contribute our findings to ground the relationship
between creativity and CC systems.

</details>


### [273] [Confirmation bias: A challenge for scalable oversight](https://arxiv.org/abs/2507.19486)
*Gabriel Recchia,Chatrik Singh Mangat,Jinu Nyachhyon,Mridul Sharma,Callum Canavan,Dylan Epstein-Gross,Muhammed Abdulbari*

Main category: cs.HC

TL;DR: 研究发现，简单的监督协议在提升评估者准确性方面效果有限，且评估者易受偏见影响。


<details>
  <summary>Details</summary>
Motivation: 探讨监督协议是否能帮助评估者准确验证比自身更强大的AI模型。

Method: 通过两项研究测试评估者在模型‘大部分正确但非完全正确’情况下的表现。

Result: 研究未发现监督协议有明显优势，评估者在模型错误时表现略有提升，但易受偏见影响。

Conclusion: 监督协议需进一步测试其对评估者偏见的鲁棒性，并验证其随问题难度和模型能力提升的扩展性。

Abstract: Scalable oversight protocols aim to empower evaluators to accurately verify
AI models more capable than themselves. However, human evaluators are subject
to biases that can lead to systematic errors. We conduct two studies examining
the performance of simple oversight protocols where evaluators know that the
model is "correct most of the time, but not all of the time". We find no
overall advantage for the tested protocols, although in Study 1, showing
arguments in favor of both answers improves accuracy in cases where the model
is incorrect. In Study 2, participants in both groups become more confident in
the system's answers after conducting online research, even when those answers
are incorrect. We also reanalyze data from prior work that was more optimistic
about simple protocols, finding that human evaluators possessing knowledge
absent from models likely contributed to their positive results--an advantage
that diminishes as models continue to scale in capability. These findings
underscore the importance of testing the degree to which oversight protocols
are robust to evaluator biases, whether they outperform simple deference to the
model under evaluation, and whether their performance scales with increasing
problem difficulty and model capability.

</details>


### [274] [E-polis: Gamifying Sociological Surveys through Serious Games -- A Data Analysis Approach Applied to Multiple-Choice Question Responses Datasets](https://arxiv.org/abs/2507.19488)
*Alexandros Gazis,Eleftheria Katsiri*

Main category: cs.HC

TL;DR: E-polis是一款严肃数字游戏，通过游戏化社会学调查，研究年轻人的政治观点。玩家在游戏中回答问题，其选择影响虚拟城市的形态，游戏旨在理解玩家行为而非训练他们。


<details>
  <summary>Details</summary>
Motivation: 研究年轻人的政治观点，并通过游戏化方式提供一种新颖的数据收集和分析方法。

Method: 采用平台游戏形式，玩家通过完成任务回答社会学问题，游戏使用创新的中间件架构开发，重点分析玩家回答的数据层。

Result: 游戏成功提供了一种创新的社会学研究工具，能够收集和分析年轻人的政治观点数据。

Conclusion: E-polis为严肃游戏领域提供了创新方法，展示了游戏化社会学研究的潜力。

Abstract: E-polis is a serious digital game designed to gamify sociological surveys
studying young people's political opinions. In this platform game, players
navigate a digital world, encountering quests posing sociological questions.
Players' answers shape the city-game world, altering building structures based
on their choices. E-polis is a serious game, not a government simulation,
aiming to understand players' behaviors and opinions thus we do not train the
players but rather understand them and help them visualize their choices in
shaping a city's future. Also, it is noticed that no correct or incorrect
answers apply. Moreover, our game utilizes a novel middleware architecture for
development, diverging from typical asset prefab scene and script segregation.
This article presents the data layer of our game's middleware, specifically
focusing on data analysis based on respondents' gameplay answers. E-polis
represents an innovative approach to gamifying sociological research, providing
a unique platform for gathering and analyzing data on political opinions among
youth and contributing to the broader field of serious games.

</details>


### [275] [RISEE: A Highly Interactive Naturalistic Driving Trajectories Dataset with Human Subjective Risk Perception and Eye-tracking Information](https://arxiv.org/abs/2507.19490)
*Xinzheng Wu,Junyi Chen,Peiyi Wang,Shunxiang Chen,Yong Shen*

Main category: cs.HC

TL;DR: 论文构建了RISEE数据集，结合无人机和模拟数据收集方法，包含主观评价和眼动数据，以解决现有数据集缺乏人类因素和真实性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注车辆运动状态和轨迹，缺乏人类相关信息和安全性关键场景，模拟数据集真实性低。

Method: 通过无人机记录高速公路匝道合并区域的交通视频，筛选高交互场景并在模拟软件中重建，生成第一人称视角视频供参与者评价，同时收集眼动数据。

Result: 获得101名参与者在179个场景中的3567条有效主观风险评分和2045段合格眼动数据。

Conclusion: RISEE数据集填补了人类因素和真实性不足的空白，为自动驾驶决策和规划系统提供了更全面的数据支持。

Abstract: In the research and development (R&D) and verification and validation (V&V)
phases of autonomous driving decision-making and planning systems, it is
necessary to integrate human factors to achieve decision-making and evaluation
that align with human cognition. However, most existing datasets primarily
focus on vehicle motion states and trajectories, neglecting human-related
information. In addition, current naturalistic driving datasets lack sufficient
safety-critical scenarios while simulated datasets suffer from low
authenticity. To address these issues, this paper constructs the Risk-Informed
Subjective Evaluation and Eye-tracking (RISEE) dataset which specifically
contains human subjective evaluations and eye-tracking data apart from regular
naturalistic driving trajectories. By leveraging the complementary advantages
of drone-based (high realism and extensive scenario coverage) and
simulation-based (high safety and reproducibility) data collection methods, we
first conduct drone-based traffic video recording at a highway ramp merging
area. After that, the manually selected highly interactive scenarios are
reconstructed in simulation software, and drivers' first-person view (FPV)
videos are generated, which are then viewed and evaluated by recruited
participants. During the video viewing process, participants' eye-tracking data
is collected. After data processing and filtering, 3567 valid subjective risk
ratings from 101 participants across 179 scenarios are retained, along with
2045 qualified eye-tracking data segments. The collected data and examples of
the generated FPV videos are available in our website.

</details>


### [276] [Exploring the Alignment of Perceived and Measured Sleep Quality with Working Memory using Consumer Wearables](https://arxiv.org/abs/2507.19491)
*Peter Neigel,David Antony Selby,Shota Arai,Benjamin Tag,Niels van Berkel,Sebastian Vollmer,Andrew Vargo,Koichi Kise*

Main category: cs.HC

TL;DR: 研究探讨了可穿戴设备（Oura戒指）的睡眠数据与主观睡眠自我评估的关系，发现REM睡眠、夜间心率等因素对睡眠自我评估有显著预测作用，且不同用户对睡眠标记的敏感性不同。


<details>
  <summary>Details</summary>
Motivation: 探究可穿戴设备提供的睡眠数据是否能真正增强对睡眠的理解，还是仅量化已知模式。

Method: 29名参与者使用Oura戒指记录4-8周睡眠数据，每日进行睡眠质量自我评估及工作记忆任务（N-Back）。

Result: REM睡眠、夜间心率、N-Back得分和就寝时间对睡眠自我评估有显著预测作用；不同用户对睡眠标记的敏感性存在差异。

Conclusion: 睡眠追踪器对某些用户的信息增益更大，研究数据已公开。

Abstract: Wearable devices offer detailed sleep-tracking data. However, whether this
information enhances our understanding of sleep or simply quantifies
already-known patterns remains unclear. This work explores the relationship
between subjective sleep self-assessments and sensor data from an Oura ring
over 4--8 weeks in-the-wild. 29 participants rated their sleep quality daily
compared to the previous night and completed a working memory task. Our
findings reveal that differences in REM sleep, nocturnal heart rate, N-Back
scores, and bedtimes highly predict sleep self-assessment in significance and
effect size. For N-Back performance, REM sleep duration, prior night's REM
sleep, and sleep self-assessment are the strongest predictors. We demonstrate
that self-report sensitivity towards sleep markers differs among participants.
We identify three groups, highlighting that sleep trackers provide more
information gain for some users than others. Additionally, we make all
experiment data publicly available.

</details>


### [277] [ChartGen: Scaling Chart Understanding Via Code-Guided Synthetic Chart Generation](https://arxiv.org/abs/2507.19492)
*Jovana Kondic,Pengyuan Li,Dhiraj Joshi,Zexue He,Shafiq Abedin,Jennifer Sun,Ben Wiesel,Eli Schwartz,Ahmed Nassar,Bo Wu,Assaf Arbelle,Aude Oliva,Dan Gutfreund,Leonid Karlinsky,Rogerio Feris*

Main category: cs.HC

TL;DR: ChartGen是一个自动化管道，用于生成代码引导的合成图表，并创建了一个包含22.5万图表-代码对的数据集，用于评估视觉语言模型在图表到代码重建任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态基准主要关注图表问答或摘要任务，缺乏对图表到代码重建任务的关注。ChartGen旨在填补这一空白，提供更全面的图表理解评估。

Method: ChartGen通过视觉语言模型（VLM）将种子图表图像重建为Python脚本，并使用代码导向的大型语言模型（LLM）迭代增强脚本。

Result: 生成了22.5万个独特的图表-代码对，覆盖27种图表类型和11种绘图库。评估了6个开放权重的VLM，显示仍有改进空间。

Conclusion: ChartGen的管道、提示和数据集有助于推动图表理解和视觉条件代码生成的研究。

Abstract: Chart-to-code reconstruction -- the task of recovering executable plotting
scripts from chart images -- provides important insights into a model's ability
to ground data visualizations in precise, machine-readable form. Yet many
existing multimodal benchmarks largely focus primarily on answering questions
about charts or summarizing them. To bridge this gap, we present ChartGen, a
fully-automated pipeline for code-guided synthetic chart generation. Starting
from seed chart images, ChartGen (i) prompts a vision-language model (VLM) to
reconstruct each image into a python script, and (ii) iteratively augments that
script with a code-oriented large language model (LLM). Using ChartGen, we
create 222.5K unique chart-image code pairs from 13K seed chart images, and
present an open-source synthetic chart dataset covering 27 chart types, 11
plotting libraries, and multiple data modalities (image, code, text, CSV,
DocTags). From this corpus, we curate a held-out chart-to-code evaluation
subset of 4.3K chart image-code pairs, and evaluate six open-weight VLMs (3B -
26B parameters), highlighting substantial room for progress. We release the
pipeline, prompts, and the dataset to help accelerate efforts towards robust
chart understanding and vision-conditioned code generation:
https://github.com/SD122025/ChartGen/

</details>


### [278] [From Bench to Bedside: A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2507.19493)
*Yaowei Bai,Ruiheng Zhang,Yu Lei,Jingfeng Yao,Shuguang Ju,Chaoyang Wang,Wei Yao,Yiwan Guo,Guilin Zhang,Chao Wan,Qian Yuan,Xuhua Duan,Xinggang Wang,Tao Sun,Yongchao Xu,Chuansheng Zheng,Huangxuan Zhao,Bo Du*

Main category: cs.HC

TL;DR: Janus-Pro-CXR（1B）是一种基于DeepSeek Janus-Pro模型的胸部X光解读系统，通过多中心前瞻性试验验证，在报告生成和临床关键发现检测方面优于现有模型，显著提升报告质量和效率。


<details>
  <summary>Details</summary>
Motivation: 全球放射科医生短缺，胸部X光工作量巨大，现有评估缺乏前瞻性临床验证，需要更可靠的AI辅助解决方案。

Method: 开发并验证Janus-Pro-CXR系统，采用轻量级架构和领域特定优化，通过多中心前瞻性试验（NCT06874647）进行临床验证。

Result: 系统在报告生成和临床关键发现检测（AUC > 0.8）上优于ChatGPT 4o等模型，显著提升报告质量（4.37 vs. 4.11）和效率（减少18.5%时间），专家偏好率52.7%。

Conclusion: Janus-Pro-CXR通过轻量级设计和优化，提升了诊断可靠性和工作流程效率，适用于资源有限环境，模型将开源以促进临床转化。

Abstract: A global shortage of radiologists has been exacerbated by the significant
volume of chest X-ray workloads, particularly in primary care. Although
multimodal large language models show promise, existing evaluations
predominantly rely on automated metrics or retrospective analyses, lacking
rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray
interpretation system based on DeepSeek Janus-Pro model, was developed and
rigorously validated through a multicenter prospective trial (NCT06874647). Our
system outperforms state-of-the-art X-ray report generation models in automated
report generation, surpassing even larger-scale models including ChatGPT 4o
(200B parameters), while demonstrating robust detection of eight clinically
critical radiographic findings (area under the curve, AUC > 0.8). Retrospective
evaluation confirms significantly higher report accuracy than Janus-Pro and
ChatGPT 4o. In prospective clinical deployment, AI assistance significantly
improved report quality scores (4.37 vs. 4.11, P < 0.001), reduced
interpretation time by 18.5% (P < 0.001), and was preferred by a majority of
experts (3 out of 5) in 52.7% of cases. Through lightweight architecture and
domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and
workflow efficiency, particularly in resource-constrained settings. The model
architecture and implementation framework will be open-sourced to facilitate
the clinical translation of AI-assisted radiology solutions.

</details>


### [279] [Evaluating Personalized Beneficial Interventions in the Daily Lives of Older Adults Using a Camera](https://arxiv.org/abs/2507.19494)
*Longfei Chen,Christopher Lochhead,Robert B. Fisher,Nusa Faric,Jacques Fleuriot,Subramanian Ramamoorthy*

Main category: cs.HC

TL;DR: 研究通过非接触式摄像头系统监测两位65岁以上老年人的日常活动干预效果，发现正念饮食和手工艺术活动显著改善了他们的行为表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏客观指标和个性化策略来衡量日常活动干预对老年人身心健康的影响。

Method: 两位老年人选择正念饮食和手工艺术作为干预活动，通过摄像头系统连续监测8周，使用计算机视觉算法提取姿势和移动数据。

Result: 结果显示两种活动显著改变了参与者的行为表现，验证了干预和监测系统的有效性。

Conclusion: 正念饮食和手工艺术活动结合非接触式监测系统，可有效改善老年人的行为健康。

Abstract: Beneficial daily activity interventions have been shown to improve both the
physical and mental health of older adults. However, there is a lack of robust
objective metrics and personalized strategies to measure their impact. In this
study, two older adults aged over 65, living in Edinburgh, UK, selected their
preferred daily interventions (mindful meals and art crafts), which are then
assessed for effectiveness. The total monitoring period across both
participants was 8 weeks. Their physical behaviours were continuously monitored
using a non-contact, privacy-preserving camera-based system. Postural and
mobility statistics were extracted using computer vision algorithms and
compared across periods with and without the interventions. The results
demonstrate significant behavioural changes for both participants, highlighting
the effectiveness of both these activities and the monitoring system.

</details>


### [280] [Simulating Human Behavior with the Psychological-mechanism Agent: Integrating Feeling, Thought, and Action](https://arxiv.org/abs/2507.19495)
*Qing Dong,Pengyuan Liu,Dong Yu,Chen Kang*

Main category: cs.HC

TL;DR: PSYA框架基于认知三角（情感-思维-行动），通过情感、思维和行动模块更准确地模拟人类行为，实验证明其生成的行为更自然、一致且可信。


<details>
  <summary>Details</summary>
Motivation: 现有生成代理在情感建模上过于简化，限制了模拟的真实性，PSYA框架旨在提供更丰富和准确的情感与认知建模。

Method: PSYA框架包含情感模块（分层模型）、思维模块（三重网络模型）和行动模块（整合情绪、需求和计划），并通过日常生活模拟和心理学实验验证。

Result: PSYA框架生成的行为更自然、一致、多样且可信，成功复现了人类实验结果。

Conclusion: PSYA为生成代理提供了更准确的情感与认知建模方法，并可作为心理学实验中人类参与者的替代方案。

Abstract: Generative agents have made significant progress in simulating human
behavior, but existing frameworks often simplify emotional modeling and focus
primarily on specific tasks, limiting the authenticity of the simulation. Our
work proposes the Psychological-mechanism Agent (PSYA) framework, based on the
Cognitive Triangle (Feeling-Thought-Action), designed to more accurately
simulate human behavior. The PSYA consists of three core modules: the Feeling
module (using a layer model of affect to simulate changes in short-term,
medium-term, and long-term emotions), the Thought module (based on the Triple
Network Model to support goal-directed and spontaneous thinking), and the
Action module (optimizing agent behavior through the integration of emotions,
needs and plans). To evaluate the framework's effectiveness, we conducted daily
life simulations and extended the evaluation metrics to self-influence,
one-influence, and group-influence, selection five classic psychological
experiments for simulation. The results show that the PSYA framework generates
more natural, consistent, diverse, and credible behaviors, successfully
replicating human experimental outcomes. Our work provides a richer and more
accurate emotional and cognitive modeling approach for generative agents and
offers an alternative to human participants in psychological experiments.

</details>


### [281] [Technological Requirements for Videoconferencing Judicial Hearings: Enhancing the Credibility and Reliability of Remote Testimonies](https://arxiv.org/abs/2507.19496)
*Jorge Alberto Araujo*

Main category: cs.HC

TL;DR: 研究分析了通过视频会议进行司法听证的技术需求，提出改进功能以提升可信度。


<details>
  <summary>Details</summary>
Motivation: 从司法内部视角出发，探讨如何通过技术手段增强远程听证的可信度和可靠性。

Method: 基于法官的实践经验，识别现有平台的局限性，并提出针对司法场景的定制功能。

Result: 建议实现眼动追踪、环境验证等功能，并提升传输质量，以平衡远程与现场听证的可信度。

Conclusion: 开发专注于安全和监控的证人模块，可显著提升远程听证的可信度，同时扩大司法可及性。

Abstract: This paper analyzes the technological requirements necessary to enhance the
credibility and reliability of judicial hearings conducted via videoconference,
from the internal perspective of the judiciary. Drawing on the practical
experience of a judge who conducts daily hearings, this study identifies
limitations in current platforms for verifying the authenticity of testimonies
and proposes tailored functionalities for the judicial context. Recognizing
that remote hearings represent a convenience for the parties without replacing
the option of in-person attendance, the article suggests implementing features
such as eye tracking, environment verification, and blocking of parallel
applications, in addition to improvements in transmission quality. The study
concludes that developing specific modules for witnesses - focusing on security
and monitoring - can significantly contribute to equalizing the credibility
between remote and in-person hearings, thus expanding access to justice without
compromising procedural reliability.

</details>


### [282] [Unlimited Editions: Documenting Human Style in AI Art Generation](https://arxiv.org/abs/2507.19497)
*Alex Leitch,Celia Chen*

Main category: cs.HC

TL;DR: 论文认为当前HCI研究对AI艺术生成的关注点（如检测、真实性和自动化）误解了艺术价值的来源，提出应关注艺术风格的起源和演变。


<details>
  <summary>Details</summary>
Motivation: 探讨AI艺术生成中艺术价值的真正来源，指出当前方法忽略了人类创作中的创造性斗争和风格演变。

Method: 通过历史案例研究，分析艺术风格的形成过程，并提出HCI应开发记录艺术风格起源和演变的技术。

Result: 提出HCI研究应转向自动记录艺术风格的谱系和创作选择，而非仅关注视觉效果的复制。

Conclusion: HCI在生成式AI中的新方向应是记录和展示艺术风格的演变过程，以更接近人类创作的本质。

Abstract: As AI art generation becomes increasingly sophisticated, HCI research has
focused primarily on questions of detection, authenticity, and automation. This
paper argues that such approaches fundamentally misunderstand how artistic
value emerges from the concerns that drive human image production. Through
examination of historical precedents, we demonstrate that artistic style is not
only visual appearance but the resolution of creative struggle, as artists
wrestle with influence and technical constraints to develop unique ways of
seeing. Current AI systems flatten these human choices into reproducible
patterns without preserving their provenance. We propose that HCI's role lies
not only in perfecting visual output, but in developing means to document the
origins and evolution of artistic style as it appears within generated visual
traces. This reframing suggests new technical directions for HCI research in
generative AI, focused on automatic documentation of stylistic lineage and
creative choice rather than simple reproduction of aesthetic effects.

</details>


### [283] [ChatMyopia: An AI Agent for Pre-consultation Education in Primary Eye Care Settings](https://arxiv.org/abs/2507.19498)
*Yue Wu,Xiaolan Chen,Weiyi Zhang,Shunming Liu,Wing Man Rita Sum,Xinyuan Wu,Xianwen Shang,Chea-su Kee,Mingguang He,Danli Shi*

Main category: cs.HC

TL;DR: ChatMyopia是一个基于大语言模型的AI代理，专为近视相关问题设计，整合了图像分类工具和知识库，显著提高了患者满意度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在定制化医疗沟通中具有潜力，但在可解释性和多任务整合方面存在挑战，尤其是在近视等特定领域。

Method: ChatMyopia结合了图像分类工具和基于文献、专家共识及临床指南的知识库，通过分级任务和人类评估验证其能力。

Result: 在随机对照试验中，ChatMyopia显著提高了患者满意度，优于传统宣传单。

Conclusion: ChatMyopia可作为初级眼科护理中患者教育的有效补充工具。

Abstract: Large language models (LLMs) show promise for tailored healthcare
communication but face challenges in interpretability and multi-task
integration particularly for domain-specific needs like myopia, and their
real-world effectiveness as patient education tools has yet to be demonstrated.
Here, we introduce ChatMyopia, an LLM-based AI agent designed to address text
and image-based inquiries related to myopia. To achieve this, ChatMyopia
integrates an image classification tool and a retrieval-augmented knowledge
base built from literature, expert consensus, and clinical guidelines. Myopic
maculopathy grading task, single question examination and human evaluations
validated its ability to deliver personalized, accurate, and safe responses to
myopia-related inquiries with high scalability and interpretability. In a
randomized controlled trial (n=70, NCT06607822), ChatMyopia significantly
improved patient satisfaction compared to traditional leaflets, enhancing
patient education in accuracy, empathy, disease awareness, and patient-eyecare
practitioner communication. These findings highlight ChatMyopia's potential as
a valuable supplement to enhance patient education and improve satisfaction
with medical services in primary eye care settings.

</details>


### [284] [Gaze-Aware AI: Mathematical modeling of epistemic experience of the Marginalized for Human-Computer Interaction & AI Systems](https://arxiv.org/abs/2507.19500)
*Omkar Suresh Hatti*

Main category: cs.HC

TL;DR: 论文探讨了人工智能如何通过量化人类因主流文化规范而潜意识调整真实自我表达的现象（称为“凝视压力”），并提出了一种数学指标（GPI-Diff）来建模分析。研究还提出了基于神经可塑性的包容性人机交互原则。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用人工智能创造社会心理空间，促进多样性和真实性，从而推动亲社会行为和社会和谐。

Method: 通过分析边缘化和交叉群体的凝视现象，结合后现代哲学和心理学概念，并使用Reddit帖子的语料库进行实证研究。提出了GPI-Diff复合指标来量化凝视压力。

Result: 开发了用于训练大型语言模型（如Chat-GPT）的数学方程，并提出了基于神经可塑性的包容性人机交互设计原则。

Conclusion: 研究为人工智能在促进社会心理空间和包容性交互方面提供了理论支持和实践工具。

Abstract: The proliferation of artificial intelligence provides an opportunity to
create psychological spaciousness in society. Spaciousness is defined as the
ability to hold diverse interpersonal interactions and forms the basis for
vulnerability that leads to authenticity that leads to prosocial behaviors and
thus to societal harmony. This paper demonstrates an attempt to quantify, the
human conditioning to subconsciously modify authentic self-expression to fit
the norms of the dominant culture. Gaze is explored across various marginalized
and intersectional groups, using concepts from postmodern philosophy and
psychology. The effects of gaze are studied through analyzing a few redacted
Reddit posts, only to be discussed in discourse and not endorsement. A
mathematical formulation for the Gaze Pressure Index (GPI)-Diff Composite
Metric is presented to model the analysis of two sets of conversational spaces
in relation to one another. The outcome includes an equation to train Large
Language Models (LLMs) - the working mechanism of AI products such as Chat-GPT;
and an argument for affirming and inclusive HCI, based on the equation, is
presented. The argument is supported by a few principles of Neuro-plasticity,
The brain's lifelong capacity to rewire.

</details>


### [285] [Mosaic Selections: Managing and Optimizing User Selections for Scalable Data Visualization Systems](https://arxiv.org/abs/2507.19690)
*Jeffrey Heer,Dominik Moritz,Ron Pechuk*

Main category: cs.HC

TL;DR: Mosaic Selections模型通过优化查询和选择谓词，显著提升了大规模数据集交互可视化的实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决交互式可视化在大规模数据集（数百万或更多记录）中实时交互的延迟问题。

Method: 提出Mosaic Selections模型，通过分析查询和选择谓词实现自动优化，包括预聚合数据以快速更新选择。

Result: 基准测试显示，相比未优化查询和现有优化器，延迟显著降低（数量级提升）。

Conclusion: Mosaic Selections为跨多可视化灵活过滤提供了基础设施，并能扩展到数十亿记录。

Abstract: Though powerful tools for analysis and communication, interactive
visualizations often fail to support real-time interaction with large datasets
with millions or more records. To highlight and filter data, users indicate
values or intervals of interest. Such selections may span multiple components,
combine in complex ways, and require optimizations to ensure low-latency
updates. We describe Mosaic Selections, a model for representing, managing, and
optimizing user selections, in which one or more filter predicates are added to
queries that request data for visualizations and input widgets. By analyzing
both queries and selection predicates, Mosaic Selections enable automatic
optimizations, including pre-aggregating data to rapidly compute selection
updates. We contribute a formal description of our selection model and
optimization methods, and their implementation in the open-source Mosaic
architecture. Benchmark results demonstrate orders-of-magnitude latency
improvements for selection-based optimizations over unoptimized queries and
existing optimizers for the Vega language. The Mosaic Selection model provides
infrastructure for flexible, interoperable filtering across multiple
visualizations, alongside automatic optimizations to scale to millions and even
billions of records.

</details>


### [286] [LowKeyEMG: Electromyographic typing with a reduced keyset](https://arxiv.org/abs/2507.19736)
*Johannes Y. Lee,Derek Xiao,Shreyas Kaasyap,Nima R. Hadidi,John L. Zhou,Jacob Cunningham,Rakshith R. Gore,Deniz O. Eren,Jonathan C. Kao*

Main category: cs.HC

TL;DR: LowKeyEMG是一种实时人机界面，通过7种手势解码sEMG信号实现高效文本输入，适用于输入带宽受限的场景。


<details>
  <summary>Details</summary>
Motivation: 解决传统sEMG解码全字母表不可靠的问题，尤其是对运动障碍者，通过减少手势类别提高可靠性。

Method: 将英语字母表简化为4种手势键，外加3种系统交互键，结合RWKV语言模型实现高效计算。

Result: 实验显示，参与者单手打字速度达23.3词/分钟，手势效率提升17%，7键输入时单词准确率达98.2%。

Conclusion: LowKeyEMG证明了低键输入范式在辅助技术和输入受限场景中的实用性。

Abstract: We introduce LowKeyEMG, a real-time human-computer interface that enables
efficient text entry using only 7 gesture classes decoded from surface
electromyography (sEMG). Prior work has attempted full-alphabet decoding from
sEMG, but decoding large character sets remains unreliable, especially for
individuals with motor impairments. Instead, LowKeyEMG reduces the English
alphabet to 4 gesture keys, with 3 more for space and system interaction, to
reliably translate simple one-handed gestures into text, leveraging the
recurrent transformer-based language model RWKV for efficient computation. In
real-time experiments, participants achieved average one-handed keyboardless
typing speeds of 23.3 words per minute with LowKeyEMG, and improved gesture
efficiency by 17% (relative to typed phrase length). When typing with only 7
keys, LowKeyEMG can achieve 98.2% top-3 word accuracy, demonstrating that this
low-key typing paradigm can maintain practical communication rates. Our results
have implications for assistive technologies and any interface where input
bandwidth is constrained.

</details>


### [287] [KinemaFX: A Kinematic-Driven Interactive System for Particle Effect Exploration and Customization](https://arxiv.org/abs/2507.19782)
*Yifei Zhang,Lin-Ping Yuan,Yuheng Zhao,Jielin Feng,Siming Chen*

Main category: cs.HC

TL;DR: KinemaFX是一个基于运动学的交互系统，帮助非专业用户定制粒子效果艺术作品，结合语义和运动学输入，并通过LLM支持意图表达和探索。


<details>
  <summary>Details</summary>
Motivation: 解决非专业用户因缺乏专业技能而难以创建匹配意图的粒子效果的问题。

Method: 提出粒子效果的概念模型，结合语义和运动学行为，利用LLM支持意图表达和探索，开发运动学驱动的交互搜索方法。

Result: 用户研究表明，KinemaFX能有效支持用户高效、个性化地创建粒子效果。

Conclusion: KinemaFX为非专业用户提供了一种高效、直观的粒子效果创作工具。

Abstract: Particle effects are widely used in games and animation to simulate natural
phenomena or stylized visual effects. However, creating effect artworks is
challenging for non-expert users due to their lack of specialized skills,
particularly in finding particle effects with kinematic behaviors that match
their intent. To address these issues, we present KinemaFX, a kinematic-driven
interactive system, to assist non-expert users in constructing customized
particle effect artworks. We propose a conceptual model of particle effects
that captures both semantic features and kinematic behaviors. Based on the
model, KinemaFX adopts a workflow powered by Large Language Models (LLMs) that
supports intent expression through combined semantic and kinematic inputs,
while enabling implicit preference-guided exploration and subsequent creation
of customized particle effect artworks based on exploration results.
Additionally, we developed a kinematic-driven method to facilitate efficient
interactive particle effect search within KinemaFX via structured
representation and measurement of particle effects. To evaluate KinemaFX, we
illustrate usage scenarios and conduct a user study employing an ablation
approach. Evaluation results demonstrate that KinemaFX effectively supports
users in efficiently and customarily creating particle effect artworks.

</details>


### [288] [TS-Insight: Visualizing Thompson Sampling for Verification and XAI](https://arxiv.org/abs/2507.19898)
*Parsa Vares,Éloi Durant,Jun Pang,Nicolas Médoc,Mohammad Ghoniem*

Main category: cs.HC

TL;DR: TS-Insight是一个可视化分析工具，旨在揭示Thompson Sampling算法的内部决策机制，帮助开发者验证和调试探索/利用策略。


<details>
  <summary>Details</summary>
Motivation: Thompson Sampling及其变体在主动学习中用于平衡探索和利用，但其概率性质使其成为“黑箱”，难以调试和信任。

Method: TS-Insight通过多种图表追踪每个臂的后验分布、证据计数和采样结果，以可视化方式展示探索/利用动态。

Result: 该工具能够帮助开发者验证、诊断和解释算法的决策过程，增强信任并促进调试和部署。

Conclusion: TS-Insight特别适用于需要可解释决策的敏感领域，为Thompson Sampling算法提供了透明度和可操作性。

Abstract: Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit
algorithms used to balance exploration and exploitation strategies in active
learning. Yet, their probabilistic nature often turns them into a ``black
box'', hindering debugging and trust. We introduce TS-Insight, a visual
analytics tool explicitly designed to shed light on the internal decision
mechanisms of Thompson Sampling-based algorithms, for model developers. It
comprises multiple plots, tracing for each arm the evolving posteriors,
evidence counts, and sampling outcomes, enabling the verification, diagnosis,
and explainability of exploration/exploitation dynamics. This tool aims at
fostering trust and facilitating effective debugging and deployment in complex
binary decision-making scenarios especially in sensitive domains requiring
interpretable decision-making.

</details>


### [289] [Visual Analytics Using Tensor Unified Linear Comparative Analysis](https://arxiv.org/abs/2507.19988)
*Naoki Okami,Kazuki Miyake,Naohisa Sakamoto,Jorji Nonaka,Takanori Fujiwara*

Main category: cs.HC

TL;DR: 论文提出了一种新的张量分解方法TULCA，支持灵活的张量比较分析，并集成了可视分析界面。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解方法不支持灵活的比较分析，限制了复杂数据的理解。

Method: 扩展了ULCA方法，提出TULCA，结合判别分析和对比学习方案，并开发了可视化核心张量的方法。

Result: TULCA在计算评估和案例研究中表现出色，包括超级计算机日志数据分析。

Conclusion: TULCA为张量比较分析提供了有效工具，结合可视化界面提升了分析效率。

Abstract: Comparing tensors and identifying their (dis)similar structures is
fundamental in understanding the underlying phenomena for complex data. Tensor
decomposition methods help analysts extract tensors' essential characteristics
and aid in visual analytics for tensors. In contrast to dimensionality
reduction (DR) methods designed only for analyzing a matrix (i.e., second-order
tensor), existing tensor decomposition methods do not support flexible
comparative analysis. To address this analysis limitation, we introduce a new
tensor decomposition method, named tensor unified linear comparative analysis
(TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA
integrates discriminant analysis and contrastive learning schemes for tensor
decomposition, enabling flexible comparison of tensors. We also introduce an
effective method to visualize a core tensor extracted from TULCA into a set of
2D visualizations. We integrate TULCA's functionalities into a visual analytics
interface to support analysts in interpreting and refining the TULCA results.
We demonstrate the efficacy of TULCA and the visual analytics interface with
computational evaluations and two case studies, including an analysis of log
data collected from a supercomputer.

</details>


### [290] [Beyond the Broadcast: Enhancing VR Tennis Broadcasting through Embedded Visualizations and Camera Techniques](https://arxiv.org/abs/2507.20006)
*Jun-Hsiang Yao,Jielin Feng,Xinfang Tian,Kai Xu,Gulshat Amirkhanova,Siming Chen*

Main category: cs.HC

TL;DR: 论文提出了一种结合电影化镜头运动与嵌入式可视化的VR网球观赛系统，通过动态叠加战术信息和关键比赛事件，提升了观众的沉浸感和叙事清晰度。


<details>
  <summary>Details</summary>
Motivation: 当前VR广播系统缺乏有效的镜头语言和动态可视化，限制了观众的参与度和叙事清晰度。

Method: 分析了400个网球比赛片段和25个电影化VR动画，开发了结合嵌入式可视化和自适应镜头运动的设计框架，并构建了模拟比赛环境。

Result: 开发的VR系统在用户研究中表现优于传统VR广播方法，提供了更沉浸和信息的观赛体验。

Conclusion: 该设计框架和系统显著提升了VR网球观赛的沉浸感和叙事效果。

Abstract: Virtual Reality (VR) broadcasting has emerged as a promising medium for
providing immersive viewing experiences of major sports events such as tennis.
However, current VR broadcast systems often lack an effective camera language
and do not adequately incorporate dynamic, in-game visualizations, limiting
viewer engagement and narrative clarity. To address these limitations, we
analyze 400 out-of-play segments from eight major tennis broadcasts to develop
a tennis-specific design framework that effectively combines cinematic camera
movements with embedded visualizations. We further refine our framework by
examining 25 cinematic VR animations, comparing their camera techniques with
traditional tennis broadcasts to identify key differences and inform
adaptations for VR. Based on data extracted from the broadcast videos, we
reconstruct a simulated game that captures the players' and ball's motion and
trajectories. Leveraging this design framework and processing pipeline, we
develope Beyond the Broadcast, a VR tennis viewing system that integrates
embedded visualizations with adaptive camera motions to construct a
comprehensive and engaging narrative. Our system dynamically overlays tactical
information and key match events onto the simulated environment, enhancing
viewer comprehension and narrative engagement while ensuring perceptual
immersion and viewing comfort. A user study involving tennis viewers
demonstrate that our approach outperforms traditional VR broadcasting methods
in delivering an immersive, informative viewing experience.

</details>


### [291] [Dynamite: Real-Time Debriefing Slide Authoring through AI-Enhanced Multimodal Interaction](https://arxiv.org/abs/2507.20137)
*Panayu Keelawat,David Barron,Kaushik Narasimhan,Daniel Manesh,Xiaohang Tang,Xi Chen,Sang Won Lee,Yan Chen*

Main category: cs.HC

TL;DR: Dynamite是一个AI辅助系统，帮助教师在实时课堂讨论中更新幻灯片内容，提高内容准确性和质量。


<details>
  <summary>Details</summary>
Motivation: 在伦理教育中，课堂讨论后的总结需要准确反映常见主题和少数观点，但实时制作幻灯片在数据量大和时间紧迫的情况下具有挑战性。

Method: Dynamite通过语义数据绑定和语义建议，将幻灯片内容与讨论数据动态关联，并提供符合教学目标的修订选项。

Result: 在12名参与者的实验中，Dynamite在内容准确性和质量上优于基于文本的AI基线系统。

Conclusion: Dynamite通过语义更新和快速输入方式，有效支持教师在实时讨论中高效制作幻灯片。

Abstract: Facilitating class-wide debriefings after small-group discussions is a common
strategy in ethics education. Instructor interviews revealed that effective
debriefings should highlight frequently discussed themes and surface
underrepresented viewpoints, making accurate representations of insight
occurrence essential. Yet authoring presentations in real time is cognitively
overwhelming due to the volume of data and tight time constraints. We present
Dynamite, an AI-assisted system that enables semantic updates to
instructor-authored slides during live classroom discussions. These updates are
powered by semantic data binding, which links slide content to evolving
discussion data, and semantic suggestions, which offer revision options aligned
with pedagogical goals. In a within-subject in-lab study with 12 participants,
Dynamite outperformed a text-based AI baseline in content accuracy and quality.
Participants used voice and sketch input to quickly organize semantic blocks,
then applied suggestions to accelerate refinement as data stabilized.

</details>


### [292] [Occupational Safety within Non-Routine Manufacturing Processes: Evaluating the Validity of Task-Based Ergonomic Assessments](https://arxiv.org/abs/2507.20261)
*Charu Tripathi,Manish Arora,Amaresh Chakrabarti*

Main category: cs.HC

TL;DR: 研究评估了非例行制造过程中基于任务的工效学评估的构念效度，发现其收敛效度和判别效度不足，并提出了改进需求。


<details>
  <summary>Details</summary>
Motivation: 工业5.0推动人本化，但基于任务的工效学评估在非例行制造中存在效度问题，需验证其功能效用。

Method: 采用多特质多方法（MTMM）矩阵和视频内容分析，结合直接测量和自报告技术收集数据。

Result: 收敛效度低（0.149-0.243），判别效度弱（p<0.001），视频分析揭示三大效度问题。

Conclusion: 基于任务的评估低估实际风险，需开发兼容多样工业过程的累积负荷分析技术。

Abstract: Direct measurement ergonomic assessment is reshaping occupational safety by
facilitating highly reliable risk estimation. Industry 5.0, advocating
human-centricity, has catalysed increasing adoption of direct measurement tools
in manufacturing industries. However, due to technical and feasibility
constraints in their practical implementations, especially within non routine
manufacturing processes, task based approach to ergonomic assessment is
utilized. Despite enabling operationalization of robust ergonomic assessment
technologies within complicated industrial processes, task based approach
raises several validity concerns. Hence, to ascertain functional utility of the
resultant safety interventions, this study evaluates the construct validity of
task based ergonomic assessment within non routine work utilizing Multitrait
multimethod (MTMM) matrix followed by video-based content analysis. Ergonomic
exposure traits were collected for 46 participants through direct measurement
and self reported techniques utilizing inertial motion capture and Borg's RPE
rating scale respectively. Findings include unsubstantiated convergent validity
(low same trait correlations from 0.149 to 0.243) and weak evidence of
discriminant validity with statistical significance (p value less than 0.001).
The study also identifies three primary factors undermining construct validity
through video based content analysis. Findings also elucidate misinterpretation
of ergonomic risk and action levels. Therefore, practical implications entail
underestimation of actual ergonomic risks when estimated through task based
assessment. This highlights the need for enhancement in ergonomic assessment
technologies focused on cumulative load analysis compatible within diverse
industrial processes.

</details>


### [293] [Talking-to-Build: How LLM-Assisted Interface Shapes Player Performance and Experience in Minecraft](https://arxiv.org/abs/2507.20300)
*Xin Sun,Lei Wang,Yue Li,Jie Li,Massimo Poesio,Julian Frommel,Koen Hinriks,Jiahuan Pei*

Main category: cs.HC

TL;DR: LLM辅助界面显著提升玩家表现和游戏体验，尤其在复杂任务中效果更明显。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在游戏中的作用，特别是在任务表现、可用性和玩家体验方面的影响。

Method: 在Minecraft中设计LLM辅助界面，通过自然语言交互，进行混合方法研究（30名参与者）。

Result: LLM辅助界面显著改善玩家表现、参与度和整体游戏体验，任务复杂性对结果有显著影响。

Conclusion: LLM辅助界面有潜力革新虚拟体验，需平衡直观性、可预测性、透明度和用户自主权。

Abstract: With large language models (LLMs) on the rise, in-game interactions are
shifting from rigid commands to natural conversations. However, the impacts of
LLMs on player performance and game experience remain underexplored. This work
explores LLM's role as a co-builder during gameplay, examining its impact on
task performance, usability, and player experience. Using Minecraft as a
sandbox, we present an LLM-assisted interface that engages players through
natural language, aiming to facilitate creativity and simplify complex gaming
commands. We conducted a mixed-methods study with 30 participants, comparing
LLM-assisted and command-based interfaces across simple and complex game tasks.
Quantitative and qualitative analyses reveal that the LLM-assisted interface
significantly improves player performance, engagement, and overall game
experience. Additionally, task complexity has a notable effect on player
performance and experience across both interfaces. Our findings highlight the
potential of LLM-assisted interfaces to revolutionize virtual experiences,
emphasizing the importance of balancing intuitiveness with predictability,
transparency, and user agency in AI-driven, multimodal gaming environments.

</details>


### [294] [CineVision: An Interactive Pre-visualization Storyboard System for Director-Cinematographer Collaboration](https://arxiv.org/abs/2507.20355)
*Zheng Wei,Hongtao Wu,lvmin Zhang,Xian Xu,Yefeng Zheng,Pan Hui,Maneesh Agrawala,Huamin Qu,Anyi Rao*

Main category: cs.HC

TL;DR: CineVision是一个AI驱动的平台，通过整合脚本编写与实时可视化预演，提升导演与摄影师之间的沟通效率。


<details>
  <summary>Details</summary>
Motivation: 传统依赖视觉参考和手绘故事板的方法在电影预制作中缺乏效率和精确性。

Method: CineVision提供动态灯光控制、风格模仿和可定制角色设计，帮助导演清晰表达创意。

Result: 在24人实验室研究中，CineVision比两种基线方法任务时间更短、可用性评分更高。

Conclusion: CineVision有望简化预制作流程，促进团队创意协同，尤其适合新合作者。

Abstract: Effective communication between directors and cinematographers is fundamental
in film production, yet traditional approaches relying on visual references and
hand-drawn storyboards often lack the efficiency and precision necessary during
pre-production. We present CineVision, an AI-driven platform that integrates
scriptwriting with real-time visual pre-visualization to bridge this
communication gap. By offering dynamic lighting control, style emulation based
on renowned filmmakers, and customizable character design, CineVision enables
directors to convey their creative vision with heightened clarity and rapidly
iterate on scene composition. In a 24-participant lab study, CineVision yielded
shorter task times and higher usability ratings than two baseline methods,
suggesting a potential to ease early-stage communication and accelerate
storyboard drafts under controlled conditions. These findings underscore
CineVision's potential to streamline pre-production processes and foster deeper
creative synergy among filmmaking teams, particularly for new collaborators.Our
code and demo are available at https://github.com/TonyHongtaoWu/CineVision.

</details>


### [295] [EchoForce: Continuous Grip Force Estimation from Skin Deformation Using Active Acoustic Sensing on a Wristband](https://arxiv.org/abs/2507.20437)
*Kian Mahmoodi,Yudong Xie,Tan Gemicioglu,Chi-Jung Lee,Jiwan Kim,Cheng Zhang*

Main category: cs.HC

TL;DR: EchoForce是一种新型腕带，通过声学传感实现低成本、非接触式握力测量，解决了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 握力是老年人健康的重要指标，但现有穿戴式握力测量方法笨重且依赖用户，不适合连续测量。

Method: EchoForce利用声学传感捕捉前臂屈肌微小皮肤变形反射的信号。

Result: 在11名参与者的研究中，EchoForce用户依赖误差率为9.08%，用户独立误差率为12.3%。

Conclusion: EchoForce实现了连续握力测量，为健康监测和交互技术提供了实用工具。

Abstract: Grip force is commonly used as an overall health indicator in older adults
and is valuable for tracking progress in physical training and rehabilitation.
Existing methods for wearable grip force measurement are cumbersome and
user-dependent, making them insufficient for practical, continuous grip force
measurement. We introduce EchoForce, a novel wristband using acoustic sensing
for low-cost, non-contact measurement of grip force. EchoForce captures
acoustic signals reflected from subtle skin deformations by flexor muscles on
the forearm. In a user study with 11 participants, EchoForce achieved a
fine-tuned user-dependent mean error rate of 9.08% and a user-independent mean
error rate of 12.3% using a foundation model. Our system remained accurate
between sessions, hand orientations, and users, overcoming a significant
limitation of past force sensing systems. EchoForce makes continuous grip force
measurement practical, providing an effective tool for health monitoring and
novel interaction techniques.

</details>


### [296] [CoGrader: Transforming Instructors' Assessment of Project Reports through Collaborative LLM Integration](https://arxiv.org/abs/2507.20655)
*Zixin Chen,Jiachen Wang,Yumeng Li,Haobo Li,Chuhan Shi,Rong Zhang,Huamin Qu*

Main category: cs.HC

TL;DR: CoGrader是一个结合人类与大型语言模型（LLM）协作的评分系统，旨在提升复杂项目报告评分的效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 项目报告评分在教育中日益重要，但因其多维度评估标准（如创造力和知识应用）和公平性问题而具有挑战性。现有AI工具难以处理复杂指标。

Method: 通过与六名教师进行形成性研究，开发了CoGrader，结合人类-LLM协作的指标设计、基准测试和AI辅助反馈。

Result: CoGrader有效提高了评分效率和一致性，并为学生提供了可靠的同伴比较反馈。

Conclusion: CoGrader展示了人机协作评分系统的潜力，并探讨了相关设计见解和伦理考量。

Abstract: Grading project reports are increasingly significant in today's educational
landscape, where they serve as key assessments of students' comprehensive
problem-solving abilities. However, it remains challenging due to the
multifaceted evaluation criteria involved, such as creativity and
peer-comparative achievement. Meanwhile, instructors often struggle to maintain
fairness throughout the time-consuming grading process. Recent advances in AI,
particularly large language models, have demonstrated potential for automating
simpler grading tasks, such as assessing quizzes or basic writing quality.
However, these tools often fall short when it comes to complex metrics, like
design innovation and the practical application of knowledge, that require an
instructor's educational insights into the class situation. To address this
challenge, we conducted a formative study with six instructors and developed
CoGrader, which introduces a novel grading workflow combining human-LLM
collaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader
was found effective in improving grading efficiency and consistency while
providing reliable peer-comparative feedback to students. We also discuss
design insights and ethical considerations for the development of human-AI
collaborative grading systems.

</details>


### [297] [EarXplore: An Open Research Database on Earable Interaction](https://arxiv.org/abs/2507.20656)
*Jonas Hummel,Tobias Röddiger,Valeria Zitz,Philipp Lepold,Michael Küttner,Marius Prill,Christopher Clarke,Hans Gellersen,Michael Beigl*

Main category: cs.HC

TL;DR: EarXplore是一个交互式在线数据库，旨在整合和梳理耳戴设备（earables）交互研究的碎片化文献，提供多种视图和过滤功能，帮助研究者快速查询和分析。


<details>
  <summary>Details</summary>
Motivation: 耳戴设备研究的多样性导致文献碎片化，难以跟踪进展和识别相关研究。

Method: 通过问题导向的过程设计EarXplore平台，包含34个标注标准和118项研究，提供四种视图：表格视图、图形视图、相似性视图和时间线视图。

Result: EarXplore支持定制化探索、针对性过滤和交互式信息检索，帮助研究者高效查询和整合文献。

Conclusion: EarXplore不仅反映了当前研究现状，还能通过社区更新持续演进，为未来研究提供支持。

Abstract: Interaction with earables - earphones equipped with additional sensors - has
been identified as one of four major areas of earable research. Worn naturally
and positioned near key physiological signals, earables support a wide range of
interaction modalities and have demonstrated the ability to detect multiple
inputs simultaneously. Yet this diversity has resulted in a fragmented body of
research, making it increasingly difficult to track developments and identify
relevant studies. To address this, we introduce EarXplore, a curated,
interactive online database on earable interaction research. Designed through a
question-centered process that guided both the development of 34 criteria
applied to annotate 118 studies and the structure of the platform, EarXplore
comprises four distinct yet integrated views: a Tabular View for structured
exploration, a Graphical View for visual overviews, a Similarity View for
identifying conceptual links, and a Timeline View for analyzing trends and
scholarly lineage. We demonstrate how the platform supports tailored
exploration, targeted filtering, and interactive information retrieval,
allowing researchers to query the literature and synthesize information in the
format of their choice. We furthermore leverage the contents and capabilities
of the platform to discuss the research gaps and opportunities in the field.
With built-in mechanisms for continuous community updates, EarXplore not only
reflects the current state of the field but also evolves alongside it, serving
as a living resource to inform and accelerate future developments.

</details>


### [298] [Beyond Text: Probing K-12 Educators' Perspectives and Ideas for Learning Opportunities Leveraging Multimodal Large Language Models](https://arxiv.org/abs/2507.20720)
*Tiffany Tseng,Katelyn Lam,Tiffany Lin Fu,Alekhya Maram*

Main category: cs.HC

TL;DR: 多模态大语言模型（MLLMs）在教育中的应用潜力与挑战，通过教师研讨和原型设计探讨了其未来学习体验的可能性和实际需求。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs如何丰富学习体验，了解教师对其应用的看法和实际挑战。

Method: 通过12名K-12教师的研讨和原型设计，分析教师对MLLMs的设想和需求。

Result: 揭示了教师驱动的和学生驱动的两种应用方式，并总结了教师的机遇与担忧。

Conclusion: MLLMs在教育中具有潜力，但需考虑实际需求和教师视角以实现成功应用。

Abstract: Multimodal Large Language Models (MLLMs) are beginning to empower new user
experiences that can flexibly generate content from a range of inputs,
including images, text, speech, and video. These capabilities have the
potential to enrich learning by enabling users to capture and interact with
information using a variety of modalities, but little is known about how
educators envision how MLLMs might shape the future of learning experiences,
what challenges diverse teachers encounter when interpreting how these models
work, and what practical needs should be considered for successful
implementation in educational contexts. We investigated educator perspectives
through formative workshops with 12 K-12 educators, where participants
brainstormed learning opportunities, discussed practical concerns for effective
use, and prototyped their own MLLM-powered learning applications using Claude
3.5 and its Artifacts feature for previewing code-based output. We use case
studies to illustrate two contrasting end-user approaches (teacher-and
student-driven), and share insights about opportunities and concerns expressed
by our participants, ending with implications for leveraging MLLMs for future
learning experiences.

</details>


### [299] [Vocalize: Lead Acquisition and User Engagement through Gamified Voice Competitions](https://arxiv.org/abs/2507.20730)
*Edvin Teskeredzic,Muamer Paric,Adna Sestic,Petra Fribert,Anamarija Lukac,Hadzem Hadzic,Kemal Altwlkany,Emanuel Lacic*

Main category: cs.HC

TL;DR: 论文介绍了Vocalize，一个通过游戏化语音竞赛提升用户参与度和潜在客户获取的系统，展示了其在营销中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索通过互动和游戏化平台创造吸引人的用户体验并收集潜在客户的方法。

Method: 开发了Vocalize系统，结合音频处理和LLM技术，设计游戏化语音竞赛。

Result: 在4个现场活动中测试，用户研究表明系统能显著提升用户参与度。

Conclusion: Vocalize展示了游戏化音频活动在营销等领域的应用潜力。

Abstract: This paper explores the prospect of creating engaging user experiences and
collecting leads through an interactive and gamified platform. We introduce
Vocalize, an end-to-end system for increasing user engagement and lead
acquisition through gamified voice competitions. Using audio processing
techniques and LLMs, we create engaging and interactive experiences that have
the potential to reach a wide audience, foster brand recognition, and increase
customer loyalty. We describe the system from a technical standpoint and report
results from launching Vocalize at 4 different live events. Our user study
shows that Vocalize is capable of generating significant user engagement, which
shows potential for gamified audio campaigns in marketing and similar
verticals.

</details>


### [300] [Beyond QWERTY: A pressure-based text input approach for XR that enables a touch-typing like experience](https://arxiv.org/abs/2507.20741)
*Fabian Rücker,Torben Storch*

Main category: cs.HC

TL;DR: 论文提出了一种基于压力的文本输入方法，用于XR环境，替代传统键盘布局，实现高效输入。


<details>
  <summary>Details</summary>
Motivation: 当前XR应用中的文本输入效率低下，传统键盘布局无法适应空间数字环境，限制了沉浸式技术的生产力。

Method: 分析物理键盘输入特性，提出线性压力输入设计，实现类似盲打的体验。

Result: 新方法支持每分钟200字符以上的输入速度，适合公共场合使用。

Conclusion: 该压力输入方法为XR环境提供了高效、隐蔽的文本输入解决方案。

Abstract: Text input in extended reality (XR) applications remains inefficient and
tedious. Most solutions are derived from the traditional keyboard layout, yet
fail to translate its positive characteristics to the spatial digital realm.
This limits the productive use of immersive technologies. In this work, we
analyze physical keyboard input to identify key characteristics that facilitate
its comfort, touch typing and high typing speeds. Building on these findings,
we propose a novel pressure-based text input modality that transfers these
characteristics into immersive space by substituting the two-dimensional QWERTY
layout with a linear scale. This design facilitates a touch-typing-like
experience, eliminating the need for visual guidance for proficient users. Our
skill-based approach enables typing speeds of over 200 characters per minute.
Additionally, it is suitable for discreet use in public spaces and everyday
text-input tasks, since the proposed system requires virtually no hand or
finger movements and resembles smartphone-based text input in appearance.

</details>


### [301] [Understanding Bias in Perceiving Dimensionality Reduction Projections](https://arxiv.org/abs/2507.20805)
*Seoyoung Doh,Hyeon Jeon,Sungbok Shin,Ghulam Jilani Quadri,Nam Wook Kim,Jinwook Seo*

Main category: cs.HC

TL;DR: 研究验证了实践中用户倾向于选择视觉吸引力强的降维投影而非结构忠实性，并探讨了偏见的成因及缓解策略。


<details>
  <summary>Details</summary>
Motivation: 探讨实践中用户选择降维投影时偏重视觉吸引力而非结构忠实性的现象及其原因。

Method: 通过用户研究验证偏见存在并分析其成因。

Result: 视觉吸引力偏见影响用户选择，且随颜色标签和短时间暴露加剧。

Conclusion: 提出缓解偏见的策略，以提升降维投影的可靠性和分析效果。

Abstract: Selecting the dimensionality reduction technique that faithfully represents
the structure is essential for reliable visual communication and analytics. In
reality, however, practitioners favor projections for other attractions, such
as aesthetics and visual saliency, over the projection's structural
faithfulness, a bias we define as visual interestingness. In this research, we
conduct a user study that (1) verifies the existence of such bias and (2)
explains why the bias exists. Our study suggests that visual interestingness
biases practitioners' preferences when selecting projections for analysis, and
this bias intensifies with color-encoded labels and shorter exposure time.
Based on our findings, we discuss strategies to mitigate bias in perceiving and
interpreting DR projections.

</details>


### [302] [ProForm: Solder-Free Circuit Assembly Using Thermoforming](https://arxiv.org/abs/2507.20933)
*Narjes Pourjafarian,Zhenming Yang,Jeffrey I. Lipton,Benyamin Davaji,Gregory D. Abowd*

Main category: cs.HC

TL;DR: ProForm是一种无焊料电路原型设计的热成型方法，通过热塑性材料封装电子元件，实现可逆安装，支持多种基材，促进可持续电子实践。


<details>
  <summary>Details</summary>
Motivation: 解决电子废弃物问题，传统焊接方法难以回收和重复使用，导致资源浪费。

Method: 采用热成型技术，用热塑性材料封装电子元件，无需焊接或定制机械外壳。

Result: ProForm电路表现出良好的电气性能和机械稳定性，支持快速原型设计和元件重用。

Conclusion: ProForm不仅促进可持续电子实践，还具有优于传统焊接的多重优势。

Abstract: Electronic waste (e-waste) is a growing global challenge, with millions of
functional components discarded due to the difficulty of repair and reuse.
Traditional circuit assembly relies on soldering, which creates semi-permanent
bonds that limit component recovery and contribute to unnecessary waste. We
introduce ProForm, a thermoforming approach for solder-free circuit
prototyping. By encapsulating electronic components with pressure-formed
thermoplastics, ProForm enables secure, reversible mounting without the need
for solder or custom mechanical housings. This approach supports a wide range
of substrates, including flexible, paper-based, and non-planar circuits,
facilitating easy reuse, replacement, and rapid prototyping. We demonstrate
ProForm's versatility to support prototyping practices. We show that ProFormed
circuits exhibit good electrical performance and mechanical stability. While
motivated by a need for sustainable electronics practices, ProForm has other
significant advantages over traditional soldering.

</details>


### [303] [The Impact of Simple, Brief, and Adaptive Instructions within Virtual Reality Training: Components of Cognitive Load Theory in an Assembly Task](https://arxiv.org/abs/2507.20943)
*Rebecca L. Pharmer,Christopher D. Wickens,Lucas Plabst,Benjamin A. Clegg,Leanne M. Hirshfield,Joanna E. Lewis,Jalynn B. Nicoly,Cara A. Spencer,Francisco R. Ortega*

Main category: cs.HC

TL;DR: 研究探讨了虚拟现实（VR）中认知负荷的三个核心要素对学习效率的影响，发现自适应训练能提高效率且不影响记忆。


<details>
  <summary>Details</summary>
Motivation: 自适应训练系统旨在通过动态调整难度提高学习效率和记忆，但设计选择可能影响学习者的认知负荷。本研究旨在探讨认知负荷对训练结果的影响。

Method: 参与者在VR环境中逐步学习形状组装，认知负荷通过三个维度操纵：内在负荷（形状复杂度）、外在负荷（指令冗长度）和关联负荷（自适应与固定训练）。实验1为自适应训练，难度基于个体表现调整；实验2为固定训练，难度按预设计划调整。

Result: 高内在负荷显著增加训练时间和主观负荷，但不影响记忆测试准确性；外在负荷轻微影响训练时间，对负荷和记忆影响较小；自适应训练缩短训练时间且不增加负荷或损害记忆。三种负荷间无交互作用。

Conclusion: 内在和外在负荷增加训练时间，但自适应训练提高效率且不损害记忆。负荷间无交互作用，表明可在任一负荷维度中寻求训练效益。研究支持在制造和军事等领域使用VR自适应系统，实时调整难度可优化效率且不损害学习。

Abstract: Objective: The study examined the effects of varying all three core elements
of cognitive load on learning efficiency during a shape assembly task in
virtual reality (VR).
  Background: Adaptive training systems aim to improve learning efficiency and
retention by dynamically adjusting difficulty. However, design choices can
impact the cognitive workload imposed on the learner. The present experiments
examined how aspects of cognitive load impact training outcomes.
  Method: Participants learned step-by-step shape assembly in a VR environment.
Cognitive load was manipulated across three dimensions: Intrinsic Load (shape
complexity), Extraneous Load (instruction verbosity), and Germane Load
(adaptive vs. fixed training). In adaptive training (experiment 1), difficulty
increased based on individual performance. In fixed training (experiment 2),
difficulty followed a preset schedule from a yoked participant.
  Results: Higher Intrinsic Load significantly increased training times and
subjective workload but did not affect retention test accuracy. Extraneous Load
modestly impacted training time, with little impact on workload or retention.
Adaptive training shortened overall training time without increasing workload
or impairing retention. No interactions were observed between the three types
of load. Conclusion: Both Intrinsic and Extraneous Load increased training
time, but adaptive training improved efficiency without harming retention. The
lack of interaction between the elements suggests training benefits can be
worth seeking within any of the components of cognitive load. Application:
These findings support the use of VR adaptive systems in domains such as
manufacturing and military service, where efficient assembly skill acquisition
is critical. Tailoring difficulty in real-time can optimize efficiency without
compromising learning.

</details>


### [304] [Towards Effective Human Performance in XR Space Framework based on Real-time Eye Tracking Biofeedback](https://arxiv.org/abs/2507.21000)
*Barbara Karpowicz,Tomasz Kowalewski,Pavlo Zinevych,Adam Kuzdraliński,Grzegorz Marcin Wójcik,Wiesław Kopeć*

Main category: cs.HC

TL;DR: 本文提出了一种用于XR空间框架的眼动追踪模块，旨在提升XR应用（如培训、筛选和远程操作）中的人类表现。该框架通过多模态测量（如问卷、眼动追踪和生理数据）实现自适应实时虚拟沉浸系统，重点关注实时眼动数据集成以优化用户体验。


<details>
  <summary>Details</summary>
Motivation: 提升XR应用中的人类表现，特别是在培训、筛选和远程操作领域，通过实时眼动追踪数据优化用户认知和注意力反馈。

Method: 提出一个包含多模态测量（问卷、眼动追踪、身体运动和生理数据）的框架，重点集成实时眼动追踪技术，以动态调整任务难度和反馈。

Result: 通过实时眼动追踪数据，实现了对用户注意力、认知负荷和参与度的洞察，从而优化XR应用的适应性和效率。

Conclusion: 眼动追踪技术在动态XR环境中的集成面临挑战，但通过实时引擎的支持，可以有效提升自适应XR应用的性能和用户体验。

Abstract: This paper proposes an eye tracking module for the XR Space Framework aimed
at enhancing human performance in XR-based applications, specifically in
training, screening, and teleoperation. This framework provides a methodology
and components that streamline the development of adaptive real-time virtual
immersive systems. It contains multimodal measurements - declarative in the
form of in-VR questionnaires and objective, including eye tracking, body
movement, and psychophysiological data (e.g., ECG, GSR, PPG). A key focus of
this paper is the integration of real-time eye tracking data into XR
environments to facilitate a biofeedback loop, providing insight into user
attention, cognitive load, and engagement. Given the relatively high
measurement frequency of eye tracking - recognized as a noninvasive yet robust
psychophysiological measure - this technology is particularly well suited for
real-time adjustments in task difficulty and feedback to enhance learning and
operational effectiveness. Despite its established role in cognitive and
attentional studies, implementing eye tracking metrics within dynamic,
real-time XR environments poses unique challenges, particularly given the
complex moving visuals presented in head-mounted displays (HMDs). This paper
addresses these challenges by focusing on the essential aspects of integrating
eye tracking in immersive systems based on real-time engines, ultimately
facilitating more efficient, adaptive XR applications.

</details>


### [305] [User-Centered Design with AI in the Loop: A Case Study of Rapid User Interface Prototyping with "Vibe Coding"](https://arxiv.org/abs/2507.21012)
*Tianyi Li,Tanay Maheshwari,Alex Voelker*

Main category: cs.HC

TL;DR: 论文探讨了利用生成式用户界面（“vibe coding”）和大型语言模型（LLMs）通过自然语言提示生成代码，以支持用户为中心设计（UCD）中的快速原型设计。


<details>
  <summary>Details</summary>
Motivation: 传统UCD方法在快速原型设计和多方案测试方面存在局限，研究旨在通过AI辅助的“vibe coding”方法弥补这一不足。

Method: 提出了一种AI参与的“ideate-prototyping”流程，结合生成式UI和LLMs，通过用户评估访谈和实时协作会议收集反馈。

Result: 团队成功开发了一个交互式数据分析界面，并能够从用户和领域专家处获取丰富反馈，测试多种设计方案。

Conclusion: 生成式UI在连接设计专业与领域专业知识方面具有潜力，但也存在一些挑战。

Abstract: We present a case study of using generative user interfaces, or ``vibe
coding,'' a method leveraging large language models (LLMs) for generating code
via natural language prompts, to support rapid prototyping in user-centered
design (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop
ideate-prototyping process. We share insights from an empirical experience
integrating this process to develop an interactive data analytics interface for
highway traffic engineers to effectively retrieve and analyze historical
traffic data. With generative UIs, the team was able to elicit rich user
feedback and test multiple alternative design ideas from user evaluation
interviews and real-time collaborative sessions with domain experts. We discuss
the advantages and pitfalls of vibe coding for bridging the gaps between design
expertise and domain-specific expertise.

</details>
