<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [cs.HC](#cs.HC) [Total: 21]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.LG](#cs.LG) [Total: 162]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: 提出了一种基于Spatial ModernBERT的模型，用于从复杂财务文档中提取表格和键值对，通过空间嵌入和多头分类任务实现高精度提取。


<details>
  <summary>Details</summary>
Motivation: 财务文档中的表格和键值对提取对审计、数据分析和自动化发票处理等业务流程至关重要。

Method: 使用Spatial ModernBERT模型，结合空间嵌入，通过三个分类头（标签、列索引、行区分）进行任务分类，并在PubTables-1M数据集上预训练后微调。

Result: 模型在财务文档数据集上表现优异，能够有效结合文本和空间信息，实现高精度提取。

Conclusion: Spatial ModernBERT通过空间嵌入和多任务分类，显著提升了财务文档中表格和键值对的提取准确性。

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [2] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: SEALGuard是一种多语言护栏，旨在提升LLM系统在多语言环境中的安全对齐能力，显著优于现有方法如LlamaGuard。


<details>
  <summary>Details</summary>
Motivation: 现有护栏（如LlamaGuard）在多语言不安全输入（尤其是低资源语言）上表现不佳，导致LLM系统易受攻击。

Method: 通过低秩适应（LoRA）将通用多语言模型转化为多语言护栏，并构建SEALSBench数据集（26万条多语言提示）。

Result: SEALGuard在多语言不安全提示检测上表现优异，DSR提升48%，优于LlamaGuard。

Conclusion: SEALGuard通过有效的多语言护栏显著提升了LLM系统的安全对齐能力。

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [3] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 论文分析了当前大语言模型（LLMs）在医学问答中的局限性，重点是评估数据集的质量。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在医学领域的表现，发现现有数据集的不足，推动更严谨的评估方法。

Method: 回顾了MedQA等常用数据集，分析其严谨性、透明性和临床相关性，并探讨了医学期刊挑战问题的潜力。

Result: 现有数据集缺乏临床真实性、透明性和验证，挑战问题虽有益但规模小、范围窄且可能被LLM训练暴露。

Conclusion: 需建立标准化框架和合作机制，确保数据集和方法严谨、无偏且反映临床复杂性。

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [4] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: 论文介绍了两个韩语专家级基准测试KMMLU-Redux和KMMLU-Pro，用于评估大语言模型在韩国工业领域的适用性。


<details>
  <summary>Details</summary>
Motivation: 开发能够全面评估大语言模型在学术和工业领域适用性的基准测试。

Method: 重构KMMLU为KMMLU-Redux，基于韩国国家技术资格考试；创建KMMLU-Pro，基于韩国国家专业执照考试。

Result: 实验证明这两个基准测试能全面代表韩国工业知识。

Conclusion: 公开数据集，为评估大语言模型在韩国工业领域的适用性提供了可靠工具。

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [5] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: SIMS是一种无需外部监督的自改进模型引导框架，通过自主生成和优化对比样本，提升大型语言模型（LLM）的适应性和引导效果。


<details>
  <summary>Details</summary>
Motivation: 传统模型引导方法依赖外部标注数据，限制了其适应性和效果。SIMS旨在通过自改进机制解决这一问题。

Method: SIMS通过迭代自改进循环生成和优化对比样本，并结合提示排序和对比采样等策略增强引导效果。

Result: SIMS在多样化的LLM和基准测试中显著优于现有方法，展示了更高的引导效果和适应性。

Conclusion: 自改进模型引导是未来LLM推理时对齐研究的有前景方向。

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [6] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: 研究发现，电子健康记录（EHR）中普遍存在对特定患者群体的污名化语言，尤其是黑人、低收入患者及某些疾病患者，且不同医疗提供者的使用频率不同。


<details>
  <summary>Details</summary>
Motivation: 揭示EHR中污名化语言的普遍性及其对患者群体的不公平影响。

Method: 通过扩展词典匹配和监督学习分类器识别MIMIC-III EHR中的怀疑标记和污名化标签，并使用泊松回归模型分析预测因素。

Result: 黑人、低收入患者及某些疾病患者的污名化标签率更高；护士和社会工作者的使用频率较高。

Conclusion: 污名化语言在EHR中普遍存在，需采取措施减少其对患者的影响。

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [7] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: 研究发现，视觉想象力强弱影响Ganzflicker诱导的幻觉内容复杂性，强想象力者描述更自然复杂的内容，弱想象力者则报告简单几何图案。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉想象力差异（从无想象力到典型和生动想象力）如何影响Ganzflicker诱导的幻觉内容。

Method: 利用自然语言处理工具分析4000多名参与者的自由文本描述，结合视觉语言模型和纯文本语言模型。

Result: 强想象力者描述复杂自然内容，弱想象力者报告简单几何图案；视觉语言模型更能捕捉差异，强想象力者语言更具感觉运动关联。

Conclusion: 视觉想象力差异可能反映早期视觉区与高阶区域协调的个体差异。

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [8] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard是一个线性化框架，将预训练的Transformer大语言模型转换为支持无限上下文生成的次二次复杂度架构，解决了内存和计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型在长上下文场景下因softmax注意力的二次复杂度和KV缓存增长面临内存和计算瓶颈，Lizard旨在解决这些问题。

Method: Lizard提出了一种次二次复杂度注意力机制，结合门控模块和混合机制（全局线性注意力与滑动窗口注意力），并引入硬件感知算法加速训练。

Result: Lizard在标准语言建模任务中几乎无损恢复教师模型性能，在5-shot MMLU基准上比先前方法提升18分，并在关联召回任务中表现显著提升。

Conclusion: Lizard通过创新的线性化设计和门控机制，显著提升了长上下文生成的能力和效率，优于现有方法。

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [9] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: ALIGN系统通过提示对齐细粒度属性，动态个性化基于LLM的决策者，支持配置管理、结构化输出生成和多种算法实现。


<details>
  <summary>Details</summary>
Motivation: 用户价值观和偏好多样，需要新方法对齐和个性化LLM决策辅助工具。

Method: 提出ALIGN系统，通过提示对齐细粒度属性，支持模块化后端和多种算法实现。

Result: 系统支持定性和定量分析，在公共意见调查和医疗分诊决策中验证对齐方法。

Conclusion: ALIGN框架开源，推动可靠、负责任和个性化LLM决策者的研究。

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [10] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

TL;DR: OpenCodeReasoning-II数据集包含2.5M个问题-解决方案-评论三元组，用于代码生成和评论的联合训练，性能超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有代码推理数据集规模不足，限制了LLMs在代码生成和评论中的潜力。

Method: 采用两阶段监督微调策略，分别针对代码生成和联合训练。

Result: 微调后的模型在代码生成上表现优异，联合训练显著提升竞赛编程性能。

Conclusion: OpenCodeReasoning-II和扩展的LiveCodeBench为LLM评估提供了更全面的支持。

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [11] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: 论文提出了一种动态参数记忆（DPM）机制，用于解决语音大语言模型（SLLM）在处理长音频序列时因高帧率限制上下文窗口的问题，显著提升了情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 语音模态的高帧率限制了SLLM的信号处理能力，现有方法忽略了情感在对话中的连续性。

Method: 提出DPM机制，通过上下文语义和句子级情感编码，动态存储信息到临时LoRA模块。

Result: 在IEMOCAP数据集上，DPM显著提升了SLLM处理长音频的情感识别能力，达到最优性能。

Conclusion: DPM机制有效扩展了SLLM的上下文处理能力，为长音频情感识别提供了新方法。

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [12] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: CompassJudger-2是一种新型通用评判模型，通过任务驱动的多领域数据策略提升评判能力，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前评判模型存在专业狭窄和鲁棒性不足的问题，限制了其全面评估能力。

Method: 采用任务驱动、多领域数据策略，结合可验证奖励和监督拒绝采样，引入边缘策略梯度损失优化学习目标。

Result: CompassJudger-2在多个评判和奖励基准中表现优异，7B模型与更大模型竞争。

Conclusion: 该研究推动了鲁棒、可扩展的LLM评判，并建立了新的性能和评估标准。

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [13] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: OPENXRD是一个用于晶体学问答的开卷管道，结合GPT-4.5生成的简明支持内容，显著提升了小模型在X射线衍射（XRD）任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统扫描教材可能带来的版权问题，同时帮助小模型理解XRD关键概念。

Method: 通过GPT-4.5生成紧凑的领域特定参考内容，并在217个专家级XRD问题上评估不同视觉语言模型的性能。

Result: 使用GPT-4.5生成摘要的模型在准确性上有显著提升，尤其是对晶体学知识有限的模型。

Conclusion: OPENXRD展示了开卷系统在材料科学中的实用性，并为科学领域的自然语言处理工具奠定了基础。

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [14] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

TL;DR: 论文提出了一种轻量级模型PU-Lie，结合冻结BERT嵌入、可解释特征和PU学习目标，用于检测战略对话中的欺骗信息，在Diplomacy数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 战略对话中欺骗检测任务复杂且类别不平衡（欺骗信息占比不足5%），传统方法难以应对。

Method: 结合冻结BERT嵌入、可解释的语言和游戏特定特征，采用PU学习目标，专注于建模稀有但关键的欺骗类别。

Result: 模型在Diplomacy数据集上达到0.60的宏F1值，同时减少650倍以上的可训练参数。

Conclusion: PU学习、语言可解释性和说话者感知表示在此任务中具有重要价值，准确检测欺骗比识别真实信息更为关键。

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [15] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: RAMA是一个检索增强的多代理框架，用于验证多媒体虚假信息，通过战略查询、跨验证证据聚合和多代理架构提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息的快速传播对自动化事实核查系统提出了挑战，尤其是当声明模糊或缺乏上下文时。

Method: RAMA采用三种创新：战略查询制定、跨验证证据聚合和多代理集成架构。

Result: 实验表明RAMA在基准数据集上表现优异，尤其在解决模糊或不太可能的声明时。

Conclusion: 整合基于网络的证据和多代理推理对可信的多媒体验证至关重要，RAMA为更可靠和可扩展的事实核查解决方案铺平了道路。

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [16] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: 论文提出了一种通过剪枝神经元来提升大语言模型泛化能力的微调方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常依赖数据集特定的机制，导致在新任务或分布上性能下降，因此需要提升其泛化能力。

Method: 使用Integrated Gradients量化神经元对高置信度预测的影响，剪枝与数据集特定机制相关的神经元。

Result: 在多选题基准测试中，该方法显著提升了性能，优于之前的非剪枝适应方法。

Conclusion: 通过剪枝数据集特定神经元，模型能更依赖泛化性强的表示，从而提升性能。

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [17] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

TL;DR: 论文提出了一种针对藏语的低资源语言模型Banzhida，通过构建大规模藏语预训练语料库，显著提升了藏语生成AI的性能。


<details>
  <summary>Details</summary>
Motivation: 藏语作为低资源语言在现有语言模型中代表性不足，缺乏高质量训练语料。

Method: 通过整合多源数据并应用专门的数据清洗和处理流程，构建了最大的藏语预训练语料库，并基于此训练了多语言大模型Banzhida。

Result: Banzhida在多种任务中显著优于同类开源模型和针对藏语的定制模型。

Conclusion: 研究表明，通过高质量语料库和针对性训练，可以有效提升低资源语言的模型性能。

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [18] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 该研究分析了视觉隐喻（如融化的冰川被描绘为融化的冰手榴弹）在气候变化传播中的效果，发现其虽更难理解但更具美学吸引力，且能引发更多认知加工和积极情绪。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉隐喻在气候变化传播中的实际效果，填补相关研究空白。

Method: 创建MetaClimage数据库，包含视觉隐喻和字面图像，并通过人类评分收集难度、效能、艺术质量和情感唤起等数据。

Result: 视觉隐喻更难理解但更美观，效能和情感唤起与字面图像无差异，但能引发更多标签和积极情绪。

Conclusion: 视觉隐喻在气候变化传播中具有认知和美学优势，但需权衡其复杂性。

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [19] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

TL;DR: Swa-bhasha Resource Hub提供2020-2025年间开发的罗马化僧伽罗语转僧伽罗语的数据资源和算法，推动了僧伽罗语NLP研究，并公开了数据集和工具。


<details>
  <summary>Details</summary>
Motivation: 为僧伽罗语NLP研究提供罗马化僧伽罗语转僧伽罗语的资源和工具，促进该领域的发展。

Method: 收集并公开数据资源和算法，进行现有转写应用的比较分析。

Result: 资源中心公开了数据集和工具，支持了僧伽罗语NLP的研究和应用开发。

Conclusion: Swa-bhasha Resource Hub为僧伽罗语NLP提供了重要支持，推动了罗马化僧伽罗语转写的研究和应用。

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [20] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: 论文提出了M2SaG数据集和ViSP框架，用于多模态讽刺生成，通过PPO和对比学习提升生成质量，实验表明ViSP优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有讽刺生成研究过于依赖文本模态且忽视视觉线索，数据集中的图像内容与讽刺意图不匹配。

Method: 提出M2SaG数据集（4,970个样本）和ViSP框架，结合PPO和对比学习优化讽刺文本生成。

Result: ViSP在五个指标上超越基线模型，生成文本的讽刺分数（0.898）和事实不一致性（0.768）均高于原始数据集。

Conclusion: ViSP能生成更高质量的讽刺内容，数据集和代码将公开。

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [21] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: 本文提出了一种心理学启发的幽默分解机制（HDM），利用思维链（CoT）模仿人类思维过程，优化大型语言模型（LLM）在幽默翻译中的表现，显著提升了幽默性、流畅性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在幽默翻译中存在语言干扰和缺乏幽默感的问题，需要一种更有效的方法来提升翻译质量。

Method: 提出幽默分解机制（HDM），结合思维链（CoT）和幽默理论，优化LLM的幽默翻译能力。

Result: 实验表明，该方法在幽默性、流畅性和连贯性上分别提升了7.75%、2.81%和6.13%。

Conclusion: HDM显著提升了LLM在幽默翻译中的表现，为跨文化沟通提供了有效工具。

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [22] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: 提出了一种名为ClaritySpeech的新框架，通过结合ASR、文本混淆和零样本TTS技术，在低数据环境下改善痴呆症患者的语音可理解性并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 痴呆症患者的语音模式异常，现有语音技术难以处理，导致沟通障碍和隐私问题。

Method: 整合自动语音转录（ASR）、文本混淆和零样本文本转语音（TTS）技术，无需微调即可实现语音校正和身份保护。

Result: 在ADReSS和ADReSSo数据集上，F1分数分别下降16%和10%，但保持了50%的说话人相似性；WER和语音质量显著提升。

Conclusion: ClaritySpeech在提升痴呆症患者语音可理解性和隐私保护方面表现出色，适用于低数据环境。

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [23] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

TL;DR: DATE-LM是一个统一的基准，用于评估语言模型中的数据归因方法，通过三个关键任务衡量归因质量，并发现现有方法在不同任务中存在权衡。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对数据归因方法在语言模型中的系统性评估，DATE-LM旨在填补这一空白。

Method: 引入DATE-LM基准，通过训练数据选择、毒性/偏见过滤和事实归因三个任务评估归因方法。

Result: 研究发现，没有单一方法在所有任务中表现最优，且方法性能对任务设计敏感。

Conclusion: DATE-LM为未来语言模型中的数据归因研究提供了基础，并发布了公共排行榜以促进社区参与。

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [24] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 研究优化了DRAGON Longformer基础模型用于临床文本分类，通过超参数调整、领域特定预处理和架构改进，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何优化预训练模型以更好地处理临床文本分类任务，特别是医学案例描述的二元分类。

Method: 使用500个临床案例数据集，调整序列长度、学习率、训练轮次，并加入医学术语。

Result: 优化后模型性能显著提升：准确率从72.0%提高到85.2%，其他指标也有明显改善（p < .001）。

Conclusion: 优化模型在医疗术语和临床观察方面表现优异，具有广泛医疗应用的潜力。

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [25] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

TL;DR: 本文介绍了CoNLL-2013共享任务，包括任务定义、数据集、评估指标及评分工具，并概述了参与团队的方法和评估结果。


<details>
  <summary>Details</summary>
Motivation: 该共享任务旨在推动语法错误纠正领域的研究，为相关方法提供统一的评估标准。

Method: 任务定义明确，使用特定数据集和评估指标，结合参与团队的不同方法进行对比分析。

Result: 通过共享任务展示了多种语法错误纠正方法的性能，并提供了详细的评估结果。

Conclusion: 该任务为语法错误纠正领域的研究提供了重要参考，促进了方法的发展和比较。

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [26] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文综述了检索增强生成（RAG）与推理方法的结合，提出了统一的推理-检索视角，并探讨了如何通过协同框架提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在多步推理问题上的不足以及纯推理方法的事实错误问题。

Method: 通过分析推理如何优化RAG的各个阶段（推理增强RAG）和检索知识如何支持复杂推理（RAG增强推理），提出协同框架。

Result: 展示了协同框架在知识密集型任务中的先进性能，并总结了方法、数据集和挑战。

Conclusion: 未来研究方向包括更有效、多模态适应、可信和以人为中心的RAG-推理系统。

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [27] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型（LLM）的ABSA方法，通过数据增强和强化学习优化，解决了短文本和数据不平衡问题，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA方法因短文本和数据不平衡（多为正面情感）难以有效学习上下文信息，数据增强虽可行但质量难以保证。

Method: 利用LLM生成增强数据以扩大训练集并平衡标签分布，同时通过强化学习优化数据增强质量。

Result: 在英文ABSA基准数据集上表现优于现有方法。

Conclusion: 该方法通过数据增强和强化学习显著提升了ABSA性能。

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [28] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

TL;DR: GoalfyMax是一个协议驱动的多智能体协作框架，通过标准化的A2A通信层和XP架构解决传统AI系统协调不足的问题，提升适应性和知识复用。


<details>
  <summary>Details</summary>
Motivation: 现代企业需要智能系统处理复杂动态任务，但传统单用途AI系统缺乏协调和知识复用能力，限制了可扩展性。

Method: GoalfyMax基于MCP协议实现A2A通信，引入XP架构分层存储任务逻辑和执行痕迹，支持多轮对话和动态安全验证。

Result: 实验表明GoalfyMax在复杂任务协调和案例研究中优于基线框架，表现出更高的适应性和协调能力。

Conclusion: GoalfyMax为多智能体系统提供了可扩展的未来基础，具备强大的适应性和知识复用潜力。

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [29] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

TL;DR: 该论文提出了Ref-Long基准，用于评估长上下文语言模型（LCLMs）的长上下文引用能力，发现现有模型（包括GPT-4o）在此任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 长上下文引用是LCLMs的重要任务，但目前研究不足，需要专门的评估基准。

Method: 设计了Ref-Long基准，包含三个子集（合成到现实场景），要求模型识别引用特定关键词的文档索引。

Result: 实验显示13种LCLMs在长上下文引用任务中存在显著不足。

Conclusion: Ref-Long揭示了LCLMs的局限性，并提供了进一步分析的见解。

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [30] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Main category: cs.CL

TL;DR: 论文研究了提示质量对大型语言模型（LLM）在机器翻译和翻译评估任务中表现的影响，发现错误类型和数量对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 探讨提示中的错误如何影响LLM在机器翻译和翻译评估任务中的表现，以理解模型的鲁棒性。

Method: 系统评估人类合理和合成的提示错误对LLM性能的影响，进行定量和定性分析。

Result: 提示质量显著影响翻译性能，字符级和组合噪声比短语扰动更严重；模型在极端噪声下仍能翻译。

Conclusion: 提示质量影响指令遵循而非直接翻译质量，LLM在极端噪声下仍具翻译能力。

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [31] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

TL;DR: 本文探讨了如何将定义建模任务扩展到白俄罗斯语，提出了一个包含43,150条定义的新数据集，并验证了少量数据即可适配模型，但自动评估指标存在不足。


<details>
  <summary>Details</summary>
Motivation: 定义建模任务有助于辅助词典编纂工作，但目前对未支持语言（如白俄罗斯语）的模型适配研究不足。

Method: 通过提出一个白俄罗斯语的新数据集（43,150条定义），并适配现有定义建模系统。

Result: 实验表明，适配定义建模系统仅需少量数据，但自动评估指标未能完全捕捉模型表现。

Conclusion: 定义建模任务在白俄罗斯语中具有潜力，但需改进评估方法。

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [32] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: NMIXX是一种跨语言金融嵌入模型，通过高置信度三元组微调，显著提升金融语义捕捉能力，并在低资源语言（如韩语）中表现优异。


<details>
  <summary>Details</summary>
Motivation: 通用句子嵌入模型难以捕捉金融领域的专业语义，尤其是在低资源语言中，存在领域术语、时间语义变化和双语词汇不对齐等问题。

Method: 提出NMIXX模型，利用18.8K高置信度三元组（包括领域内同义句、语义偏移硬负例和精确韩英翻译）进行微调，并发布KorFinSTS基准。

Result: NMIXX在英语FinSTS和韩语KorFinSTS上分别提升Spearman's rho 0.10和0.22，优于其他基线模型，但通用STS性能略有下降。

Conclusion: NMIXX和KorFinSTS为金融领域的多语言表示学习提供了有效工具，强调了分词设计在低资源跨语言场景中的重要性。

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [33] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: SpreadPy是一个Python库，用于模拟认知单层和多层网络中的激活传播，支持结构-功能关系的数值模拟，并通过案例研究验证其应用。


<details>
  <summary>Details</summary>
Motivation: 开发一个工具，用于系统研究激活动态如何反映认知、心理和临床现象，支持知识建模的理论验证。

Method: 通过数值模拟测试结构-功能关系，使用实证或理论网络建模，并通过三个案例研究验证。

Result: 案例研究显示：1）数学焦虑学生的知识网络结构差异；2）认知负荷对词汇访问的影响；3）失语症患者的网络结构与错误类型的相关性。

Conclusion: SpreadPy为心理学、神经科学和教育研究提供了一个灵活且可复现的研究工具，支持对个体差异和认知障碍的机制研究。

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [34] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

TL;DR: 首次研究阿拉伯语知识编辑（KE），评估四种方法在阿拉伯语翻译数据集上的表现，发现参数化方法在跨语言泛化上表现不佳，而指令调优方法更稳健。扩展LTE至多语言设置，联合训练提升编辑和迁移能力。


<details>
  <summary>Details</summary>
Motivation: 探讨知识编辑在形态丰富的语言（如阿拉伯语）中的行为，填补该领域的研究空白。

Method: 评估四种KE方法（ROME、MEMIT、ICE、LTE）在阿拉伯语翻译数据集（ZsRE和Counterfact）上的表现，分析多语言和跨语言设置。扩展LTE至多语言联合训练。

Result: 参数化方法在跨语言泛化上表现不佳，指令调优方法更稳健。联合阿拉伯语-英语训练提升LTE的编辑和迁移能力。

Conclusion: 阿拉伯语KE研究填补了领域空白，多语言联合训练是未来研究方向，并发布了相关数据和基准。

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [35] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

TL;DR: 论文提出了一种基于GRPO的方法，显著提升了泰国法律问答系统中LLM的法律引用准确性和回答质量。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统在泰国法律问答中表现有限，尤其是在需要复杂法律推理的问题上。

Method: 采用Group-Relative Policy Optimization (GRPO)方法，结合BGE-M3嵌入作为语义相似性奖励，降低计算成本。

Result: 在NitiBench基准测试中，GRPO实现了90%的引用F1提升和31%的联合质量指标提升。

Conclusion: 该方法在复杂法律推理任务中表现出更强的鲁棒性，为提升泰国法律LLM提供了一种高效且资源节约的解决方案。

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [36] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

TL;DR: MCEval是一个多语言文化评估框架，用于检测大型语言模型的文化偏见和跨文化理解能力，覆盖13种文化和语言。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在文化偏见和跨文化理解能力不足的问题，尤其是在服务全球多样化用户时。

Method: 采用动态文化问题构建和反事实重述、混杂重述进行因果分析。

Result: 评估结果显示性能差异与训练数据分布和语言-文化对齐相关，并揭示了公平性问题。

Conclusion: MCEval是首个全面的多语言文化评估框架，为LLMs的文化理解提供了深入见解。

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [37] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）的潜在空间中，高级语义信息集中在低维子空间，且在不同领域间线性可分，这种可分性在深层和结构化推理提示下更明显。


<details>
  <summary>Details</summary>
Motivation: 探究LLM内部如何组织与语义理解相关的表示，以改进模型对齐和行为解释。

Method: 对11个基于Transformer的解码器模型进行大规模实证研究，分析6个科学主题和12个层的隐藏状态。

Result: 语义信息在低维子空间中线性可分，深层和结构化推理提示下可分性更强；可通过简单向量方向捕捉推理模式（如思维链）。

Conclusion: 潜在空间的几何特性支持开发几何感知工具，直接操作表示以检测和防御有害内容，如通过轻量级分类器实现高效防护。

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [38] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 提出了一种自适应的课程学习范式，利用预训练语言模型预测样本难度，优化微调顺序，提升学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习依赖人工定义的难度指标（如文本长度），可能无法准确反映模型视角，需改进。

Method: 基于预训练语言模型预测的难度分数，探索从易到难、从难到易及混合采样的微调策略。

Result: 在四个自然语言理解数据集上验证，相比随机采样，收敛更快且性能更优。

Conclusion: 自适应的课程学习范式能有效提升模型训练效率和性能。

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [39] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

TL;DR: 论文重新定义了点击诱饵（clickbait），提出好奇心缺口是其核心特征，并创建了首个西班牙语点击诱饵检测开源数据集TA1C。


<details>
  <summary>Details</summary>
Motivation: 当前对点击诱饵的定义缺乏共识，研究旨在明确其与类似现象的区别，并减少检测中的主观性。

Method: 提出新定义，强调好奇心缺口；创建TA1C数据集，包含3,500条标注推文，标注一致性达0.825。

Result: 基线模型F1分数达0.84，验证了方法的有效性。

Conclusion: 研究为点击诱饵检测提供了更清晰的定义和高质量数据集，推动了相关领域的发展。

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [40] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: 论文通过研究语言模型在‘错位加法’任务中的表现，揭示了其任务泛化能力的内部机制，发现了一种可重用的函数归纳机制。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型如何通过上下文学习完成未见任务，特别是理解其内部机制如何驱动任务级泛化。

Method: 利用路径修补等电路式可解释性技术，分析模型在‘错位加法’任务中的内部计算。

Result: 发现了一种函数归纳机制，该机制由多个注意力头并行控制，并可泛化到其他任务中。

Conclusion: 揭示了语言模型中可重用和可组合的结构如何支持任务级泛化，为理解模型内部机制提供了新视角。

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [41] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: 提出了一种基于层次文本分割和聚类的新框架，用于改进RAG系统的检索效果，生成更具语义意义的块。


<details>
  <summary>Details</summary>
Motivation: 传统分块方法未能充分捕捉语义信息，缺乏对文本结构的考虑。

Method: 结合层次文本分割和聚类，生成语义连贯的块，并在推理时利用段级和簇级向量表示进行检索。

Result: 在NarrativeQA、QuALITY和QASPER数据集上表现优于传统分块方法。

Conclusion: 新框架通过更精细的文本处理提升了RAG系统的检索精度和上下文相关性。

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [42] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: TinyRM是一种小型双向掩码语言模型（MLM），仅需4亿参数，性能媲美175倍大的模型，适用于奖励建模任务。


<details>
  <summary>Details</summary>
Motivation: 随着奖励模型在测试时策略中的部署增加，其推理成本成为关注焦点，需开发更高效的替代方案。

Method: 结合FLAN风格提示、定向低秩适应（DoRA）和层冻结技术，优化小型模型性能。

Result: 在RewardBench上表现优异，尤其在推理任务中，轻量级微调方法效果显著。

Conclusion: 轻量级双向架构在偏好建模中展现出高效、可扩展的潜力，尽管通用模型和对话偏好建模仍有挑战。

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [43] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Main category: cs.CL

TL;DR: TextOmics是一个开创性基准，通过建立组学表达与分子文本描述的一对一对应关系，提供异构数据集，支持分子生成。ToDi是基于此的生成框架，结合组学表达和分子文本描述生成生物相关、化学有效的分子。


<details>
  <summary>Details</summary>
Motivation: 解决靶向药物发现中缺乏异构数据和统一框架的问题。

Method: 提出TextOmics基准和ToDi框架，利用两个编码器（OmicsEn和TextEn）捕捉多级生物和语义关联，并开发条件扩散（DiffGen）进行可控生成。

Result: 实验证明TextOmics有效，ToDi优于现有方法，并在零样本治疗分子生成中表现出潜力。

Conclusion: TextOmics和ToDi为靶向药物发现提供了新工具，展示了异构数据整合和可控分子生成的潜力。

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [44] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: 该论文提出了一种新框架，通过联合学习风险因素和保护因素对自杀风险的动态影响，预测后续自杀风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注静态自杀风险检测，忽视保护因素及动态变化，无法捕捉心理状态的快速波动。

Method: 构建包含风险和保护因素的新型数据集，并开发动态因素影响学习方法。

Result: 模型在三个数据集上显著优于现有方法，并提供可解释的权重。

Conclusion: 该框架为临床干预提供了更精准的工具，有助于理解自杀模式。

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [45] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Main category: cs.CL

TL;DR: GeLaCo是一种基于进化的LLM压缩方法，通过层折叠高效探索压缩解空间，支持单目标和多目标优化，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）因计算需求高而面临部署和使用障碍，模型压缩是解决这一问题的关键。现有方法如结构化剪枝成本高且可能忽略更优解。

Method: GeLaCo通过层折叠和基于种群的搜索，结合模块相似性适应度函数（注意力、前馈和隐藏状态表示），支持单目标和多目标进化压缩搜索。

Result: 在基础模型和指令调优模型上，GeLaCo通过困惑度和生成评估优于现有方法，并首次建立了压缩与质量的Pareto前沿。

Conclusion: GeLaCo为LLM压缩提供了一种高效且性能优越的解决方案，支持更灵活的压缩目标。

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [46] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）无法代表多样化的文化道德框架，尽管其语言能力强。模型在19种文化背景下与人类道德直觉存在显著差距，且模型规模增大并未改善文化代表性。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统是否能真正代表人类多样化的道德价值观，而非简单地平均化。

Method: 通过在不同文化背景下应用道德基础问卷，比较LLMs与人类基线数据的表现。

Result: LLMs系统性地同质化道德多样性，模型规模增加未显著提升文化代表性。

Conclusion: 当前AI对齐方法存在根本性限制，需更接地气的对齐目标和评估指标，以确保AI系统能代表多样化的人类价值观。

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [47] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: 论文提出了一种名为CRFT的新方法，通过识别和优化关键表征来提升复杂推理任务的性能，相比传统PEFT方法更高效。


<details>
  <summary>Details</summary>
Motivation: 在复杂推理任务中，固定位置的表征修改效果不佳，而关键表征对输出有显著影响，因此需要针对性优化。

Method: 提出CRFT方法，通过信息流分析识别关键表征，并在低秩线性子空间中动态优化，同时冻结基础模型。

Result: 在八个算术和常识推理基准测试中验证了CRFT的有效性和高效性，少样本设置下准确率提升16.4%。

Conclusion: CRFT展示了表征级优化在推理任务中的潜力，为传统PEFT方法提供了轻量且强大的替代方案。

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [48] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种结合LLMs和普通Transformer的新架构，以融合语义和时间信息，提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在时间序列预测中表现不佳，因其擅长文本而非连续数值数据；普通Transformer则缺乏高级语义学习能力。

Method: 设计了一种混合架构，融合LLM的语义表示和Transformer的时间信息，生成综合表征。

Result: 实验证明该方法在基准数据集上优于现有方法。

Conclusion: 融合语义和时间信息的架构显著提升了时间序列预测的准确性。

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [49] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Main category: cs.CL

TL;DR: 提出一种无需额外参数的任务特征蒸馏方法，解决传统特征蒸馏中师生模型隐藏层尺寸不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 传统特征蒸馏方法假设师生模型隐藏层尺寸相同，限制了学生模型的灵活性；线性投影方法引入额外参数且可能降低性能。

Method: 利用任务相关隐藏单元，直接蒸馏教师模型中任务相关的激活到学生模型，无需新参数。

Result: 在分类、指令跟随和摘要等任务中，性能提升高达3%。

Conclusion: 该方法灵活且高效，优于传统线性投影基线。

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [50] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）在将辱骂性文本转换为非辱骂性文本方面的表现，评估了Gemini、GPT-4o、DeepSeek和Groq等模型的能力，发现Groq与其他模型差异显著。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在识别和转换辱骂性文本（如仇恨言论和脏话）为保留原意的非辱骂性文本方面的有效性。

Method: 使用Gemini、GPT-4o、DeepSeek和Groq等LLMs识别并转换辱骂性文本，随后通过情感分析和语义分析评估原始与转换后的数据集。

Result: Groq的表现与其他模型差异显著，而GPT-4o和DeepSeek-V3之间存在相似性。

Conclusion: LLMs在文本转换任务中表现不一，Groq与其他模型差异明显，GPT-4o和DeepSeek-V3相似性较高。

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [51] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: 论文介绍了Absher基准，用于评估大语言模型（LLMs）在沙特阿拉伯方言中的表现，发现其在文化和上下文理解方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在阿拉伯语方言和文化细微差别中的理解能力，特别是在沙特阿拉伯这种语言多样化的环境中。

Method: 设计了包含18,000多个多选题的Absher基准，涵盖六类任务，评估了多语言和阿拉伯语专用LLMs。

Result: 发现LLMs在需要文化推理或上下文理解的任务中表现不佳，存在显著性能差距。

Conclusion: 强调需要方言感知训练和文化对齐的评估方法，以提升LLMs在阿拉伯语应用中的实际表现。

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [52] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Main category: cs.CL

TL;DR: 提出了一种基于进化搜索的自动化离散提示优化方法，用于解决复杂任务和小型语言模型的提示设计问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动化提示工程方法主要针对简单任务和大型语言模型，而复杂任务和小型模型对提示设计更敏感，需要更高效的优化方法。

Method: 采用两阶段进化搜索：第一阶段通过语法引导的遗传编程合成提示创建程序，第二阶段通过局部搜索进一步优化性能。

Result: 在三个小型通用语言模型和四个领域特定任务上，该方法优于PromptWizard、OPRO和RL-Prompt等基准方法。

Conclusion: 该方法在复杂任务和小型模型中表现优异，性能提升显著，且退化情况极少。

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [53] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 本文提出了一种基于增长边界矩阵（GBM）的新正则化技术，旨在提升NLP模型对抗词替换攻击的鲁棒性，并首次系统分析了状态空间模型（S4）的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP领域取得进展，模型仍易受对抗攻击（如同义词替换）影响，而循环网络和现代状态空间模型（如S4）的鲁棒性研究较少。

Method: 通过计算增长边界矩阵（GBM）来减少输入扰动对模型输出的影响，应用于LSTM、S4和CNN三种架构。

Result: 实验表明，该方法在对抗鲁棒性上比现有基线提升高达8.8%，优于多种先进防御方法。

Conclusion: GBM方法有效提升了模型对抗攻击的鲁棒性，并首次系统分析了S4模型的鲁棒性表现。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [54] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在语言学研究中作为可靠分析工具的潜力，特别是在涉及运动动词的时间表达中情感意义的生成。研究发现，LLMs与人类反应高度一致。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否能复制人类在语言学任务中的细微判断，以评估其作为研究工具的可靠性。

Method: 通过四项心理语言学实验（情感意义生成、情感变化、情感语境中的动词选择、句子-表情符号关联），分别对人类和LLMs进行测试。

Result: 人类与AI反应高度一致（Spearman's rho = .73-.96），仅在少数情况下存在微小差异。

Conclusion: LLMs可作为语言学研究的可信赖工具，支持大规模研究且不损害解释有效性。

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [55] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Main category: cs.CL

TL;DR: 提出了一种分层隐喻处理模型，将隐喻意义分为内容分析、概念融合和语用意三个层次，为计算系统提供了更丰富的隐喻解释框架。


<details>
  <summary>Details</summary>
Motivation: 隐喻意义并非简单的概念映射，而是一个复杂的认知现象，需要多层次的解释。

Method: 采用三层模型：内容分析（标注基本概念元素）、概念融合（建模概念组合）和语用意（捕捉说话者意图和语境效果）。

Result: 模型为计算系统提供了更深入、更语境敏感的隐喻理解方法。

Conclusion: 该分层框架为计算隐喻处理奠定了基础，支持超越表面关联的深层推理。

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [56] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）能够通过文本描述理解图结构并完成子结构提取任务，研究提出了诱导子结构过滤（ISF）视角，揭示了Transformer内部机制。


<details>
  <summary>Details</summary>
Motivation: 探索仅解码器Transformer架构如何理解图结构，特别是通过子结构提取任务分析其内部机制。

Method: 提出诱导子结构过滤（ISF）视角，结合实证和理论分析，验证其在多层Transformer中的一致性。

Result: LLMs能够成功从属性图（如分子图）中提取子结构，展示了序列Transformer处理图数据的潜力。

Conclusion: 研究为序列Transformer处理图数据提供了新视角，揭示了其子结构提取的内部动态。

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [57] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: 研究探讨了LLMs在任务导向对话中提出澄清问题的能力，发现人类和LLMs在模糊性处理上存在差异，且LLMs的提问能力可能与推理能力相关。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在异步任务导向对话中提出澄清问题的能力，并分析其与人类行为的差异。

Method: 结合Minecraft Dialogue Corpus的两种注释，构建新语料库，比较LLMs与人类在模糊性情况下的提问行为。

Result: 人类较少为指代模糊提问，更多为任务不确定性提问；LLMs则相反。推理能力可能提升LLMs的提问频率和相关性。

Conclusion: LLMs的澄清问题能力与推理能力相关，但与人类行为差异显著。

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [58] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: 研究比较了经典编码器和新一代LLM在仇恨言论检测上的表现。


<details>
  <summary>Details</summary>
Motivation: 在线平台需要在不过度审查合法言论的情况下有效遏制仇恨言论，但超大规模自回归LLM的实际效果尚未验证。

Method: 通过基准测试比较经典双向Transformer编码器和新一代LLM在仇恨言论检测任务上的表现。

Result: 未明确提及具体结果，但研究验证了模型的实际效果。

Conclusion: 研究旨在验证超大规模LLM是否真正提升了仇恨言论检测的实用性。

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [59] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: MLAR是一种基于RPA和LLM的创新ATS系统，通过三层处理优化简历筛选和候选人匹配，显著提升处理效率。


<details>
  <summary>Details</summary>
Motivation: 传统招聘流程在简历筛选和候选人匹配上存在时间和资源瓶颈，MLAR旨在解决这些问题。

Method: MLAR利用LLM分三层处理：提取职位关键特征、解析简历信息、语义匹配候选人。

Result: MLAR在处理2400份简历时，平均每份耗时5.4秒，比主流RPA平台快16.9%-17.1%。

Conclusion: MLAR为现代招聘需求提供了高效、准确且可扩展的解决方案。

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


### [60] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*İsmail Tarım,Aytuğ Onan*

Main category: cs.CL

TL;DR: 本文比较了扩散生成文本（LLaDA）和自回归生成文本（LLaMA）的检测效果，发现扩散生成文本在困惑度和突发性上更接近人类文本，导致现有检测器效果不佳，需开发针对扩散模型的检测方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展引发了对AI生成文本检测的担忧，现有方法对扩散模型的检测效果未知。

Method: 使用2000个样本，对比分析了LLaDA和LLaMA在困惑度、突发性、词汇多样性、可读性及BLEU/ROUGE分数上的表现。

Result: LLaDA在困惑度和突发性上更接近人类文本，导致检测器高假阴性率；LLaMA困惑度低但词汇保真度差。单一指标无法区分扩散生成文本和人类文本。

Conclusion: 需开发针对扩散模型的检测方法，如混合模型、扩散特定风格特征和鲁棒水印技术。

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [61] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Main category: cs.CL

TL;DR: Mixture-of-Recursions (MoR) 是一种结合参数共享和自适应计算的高效框架，通过递归Transformer实现，显著提升模型性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能同时实现参数共享和自适应计算，MoR旨在填补这一空白，提升语言模型的效率和性能。

Method: MoR通过共享层和轻量级路由器实现参数效率和自适应计算，动态分配递归深度，并选择性缓存键值对以减少内存需求。

Result: 在135M到1.7B参数的模型规模下，MoR在训练FLOPs相同的情况下显著降低验证困惑度，提高少样本准确率，并提升吞吐量。

Conclusion: MoR是实现高效高质量语言模型的有效路径，无需承担高成本。

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


### [62] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: 论文介绍了CodeJudgeBench基准，用于评估LLM作为裁判在代码生成、修复和单元测试生成任务中的表现，发现思维模型优于非思维模型，但存在随机性和顺序敏感性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM作为裁判在编码任务中的有效性，填补缺乏专用基准的空白。

Method: 引入CodeJudgeBench基准，评估26个LLM裁判模型在三个编码任务中的表现，研究提示策略。

Result: 思维模型表现更优，但裁判结果存在随机性和顺序敏感性；成对比较和保留完整响应可提升性能。

Conclusion: LLM作为裁判在编码任务中表现不稳定，需优化提示策略以提高可靠性。

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [63] [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
*Zhuoshi Pan,Qizhi Pei,Yu Li,Qiyao Sun,Zinan Tang,H. Vicky Zhao,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: REST框架通过同时测试多个问题来评估大型推理模型，揭示了现有评估方法的局限性，并展示了更强的区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法局限于单问题推理，无法反映真实世界的多上下文压力需求。

Method: 提出REST框架，同时暴露模型于多个问题，评估其优先级分配、干扰抵抗和认知负载管理能力。

Result: 即使SOTA模型在REST下性能显著下降，且REST比传统评估方法更具区分力。

Conclusion: REST是一种高效、面向未来的评估范式，减少对人类标注的依赖，更贴近实际需求。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable progress on
task-specific benchmarks, yet their evaluation methods remain constrained by
isolated problem-solving paradigms. Existing benchmarks predominantly assess
single-question reasoning through sequential testing, resulting critical
limitations: (1) vulnerability to data contamination and less challenging
(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual
creation of new questions with large human efforts, (2) failure to evaluate
models under multi-context pressure, a key requirement for real-world
deployment. To bridge this gap, we present REST (Reasoning Evaluation through
Simultaneous Testing), a stress-testing framework that concurrently exposes
LRMs to multiple problems simultaneously. Beyond basic reasoning, REST
specifically evaluates several under-tested capabilities: contextual priority
allocation, cross-problem interference resistance, and dynamic cognitive load
management. Our evaluation reveals several striking findings: Even
state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance
degradation under stress testing. Crucially, REST demonstrates stronger
discriminative power than existing benchmarks, revealing pronounced performance
differences among models that exhibit similar, near-ceiling performance under
single-question evaluations. Some key mechanistic insights emerge from our
analysis: (1) the "overthinking trap" is a critical factor contributing to the
performance degradation; (2) the models trained with "long2short" technique
preserve more accuracy of their single-problem performance under REST,
outperforming standard-trained counterparts. These results establish REST as a
cost-efficient, future-proof evaluation paradigm that better reflects
real-world reasoning demands while reducing reliance on continuous human
annotation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [64] [Cognitive Dissonance Artificial Intelligence (CD-AI): The Mind at War with Itself. Harnessing Discomfort to Sharpen Critical Thinking](https://arxiv.org/abs/2507.08804)
*Delia Deliu*

Main category: cs.HC

TL;DR: 论文提出了一种名为认知失调AI（CD-AI）的新框架，旨在通过维持不确定性而非解决它，促进用户的批判性思维和适应性决策。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统通过最小化认知负荷和优化效率来简化决策，但可能导致被动接受和智力停滞。CD-AI旨在通过维持认知失调，激发主动思考和辩证推理。

Method: CD-AI通过延迟决策解决、鼓励用户面对矛盾并挑战偏见，实现其目标。论文探讨了理论基础、实现模型，并在伦理、法律、政治和科学领域进行了应用分析。

Result: CD-AI能够增强反思性推理、认知谦逊和批判性思维，同时提出了潜在的伦理问题，如决策瘫痪和用户自主权侵蚀。

Conclusion: CD-AI挑战了传统AI的确定性范式，提出了一种通过维持冲突而非解决冲突来促进智力成长的新视角。

Abstract: AI-augmented systems are traditionally designed to streamline human
decision-making by minimizing cognitive load, clarifying arguments, and
optimizing efficiency. However, in a world where algorithmic certainty risks
becoming an Orwellian tool of epistemic control, true intellectual growth
demands not passive acceptance but active struggle. Drawing on the dystopian
visions of George Orwell and Philip K. Dick - where reality is unstable,
perception malleable, and truth contested - this paper introduces Cognitive
Dissonance AI (CD-AI): a novel framework that deliberately sustains uncertainty
rather than resolving it. CD-AI does not offer closure, but compels users to
navigate contradictions, challenge biases, and wrestle with competing truths.
By delaying resolution and promoting dialectical engagement, CD-AI enhances
reflective reasoning, epistemic humility, critical thinking, and adaptability
in complex decision-making. This paper examines the theoretical foundations of
the approach, presents an implementation model, explores its application in
domains such as ethics, law, politics, and science, and addresses key ethical
concerns - including decision paralysis, erosion of user autonomy, cognitive
manipulation, and bias in AI reasoning. In reimagining AI as an engine of doubt
rather than a deliverer of certainty, CD-AI challenges dominant paradigms of
AI-augmented reasoning and offers a new vision - one in which AI sharpens the
mind not by resolving conflict, but by sustaining it. Rather than reinforcing
Huxleyan complacency or pacifying the user into intellectual conformity, CD-AI
echoes Nietzsche's vision of the Uebermensch - urging users to transcend
passive cognition through active epistemic struggle.

</details>


### [65] [Non-linear, Team-based VR Training for Cardiac Arrest Care with enhanced CRM Toolkit](https://arxiv.org/abs/2507.08805)
*Mike Kentros,Manos Kamarianakis,Michael Cole,Vitaliy Popov,Antonis Protopsaltis,George Papagiannakis*

Main category: cs.HC

TL;DR: iREACT是一种新型VR模拟系统，用于改进传统心脏骤停培训的局限性，提供动态协作环境和多模态数据反馈。


<details>
  <summary>Details</summary>
Motivation: 传统心脏骤停培训方法难以模拟真实事件的动态性，阻碍团队资源管理技能的培养。

Method: iREACT通过非线性的协作环境模拟真实心脏骤停的复杂性，并捕获多模态数据（用户行为、认知负荷、视觉注视），提供实时和事后反馈。

Result: 初步评估显示iREACT具有可用性和教育价值，能够提升团队资源管理评估。

Conclusion: iREACT在心脏骤停培训中表现出潜力，并可扩展到其他高风险场景，以改善团队合作、沟通和决策能力。

Abstract: This paper introduces iREACT, a novel VR simulation addressing key
limitations in traditional cardiac arrest (CA) training. Conventional methods
struggle to replicate the dynamic nature of real CA events, hindering Crew
Resource Management (CRM) skill development. iREACT provides a non-linear,
collaborative environment where teams respond to changing patient states,
mirroring real CA complexities. By capturing multi-modal data (user actions,
cognitive load, visual gaze) and offering real-time and post-session feedback,
iREACT enhances CRM assessment beyond traditional methods. A formative
evaluation with medical experts underscores its usability and educational
value, with potential applications in other high-stakes training scenarios to
improve teamwork, communication, and decision-making.

</details>


### [66] ['Teens Need to Be Educated on the Danger': Digital Access, Online Risks, and Safety Practices Among Nigerian Adolescents](https://arxiv.org/abs/2507.08914)
*Munachimso B. Oguine,Ozioma C. Oguine,Karla Badillo-Urquiola,Oluwasogo Adekunle Okunade*

Main category: cs.HC

TL;DR: 研究调查了尼日利亚青少年的在线体验，发现他们面临内容不当和诈骗等风险，常用应对策略是屏蔽和举报，父母是主要支持者。建议加强教育和工具改进。


<details>
  <summary>Details</summary>
Motivation: 青少年数字参与增加，但在西非等地区面临显著在线风险，需研究其体验以提出改进措施。

Method: 通过自填问卷调查409名尼日利亚青少年，分析其技术访问、风险暴露、应对策略及影响因素。

Result: 多数青少年有中等技术访问，但常遇风险；父母为主要支持者，但监控和沟通差异大。

Conclusion: 需文化相关干预措施，加强教育、工具改进和政策支持，以提升青少年在线安全。

Abstract: Adolescents increasingly rely on online technologies to explore their
identities, form social connections, and access information and entertainment.
However, their growing digital engagement exposes them to significant online
risks, particularly in underrepresented contexts like West Africa. This study
investigates the online experiences of 409 secondary school adolescents in
Nigeria's Federal Capital Territory (FCT), focusing on their access to
technology, exposure to risks, coping strategies, key stakeholders influencing
their online interactions, and recommendations for improving online safety.
Using self-administered surveys, we found that while most adolescents reported
moderate access to online technology and connectivity, those who encountered
risks frequently reported exposure to inappropriate content and online scams.
Blocking and reporting tools were the most commonly used strategies, though
some adolescents responded with inaction due to limited resources or awareness.
Parents emerged as the primary support network, though monitoring practices and
communication varied widely. Guided by Protection Motivation Theory (PMT), our
analysis interprets adolescents' online safety behaviors as shaped by both
their threat perceptions and their confidence in available coping strategies. A
thematic analysis of their recommendations highlights the need for greater
awareness and education, parental mediation, enhanced safety tools, stricter
age restrictions, improved content moderation, government accountability, and
resilience-building initiatives. Our findings underscore the importance of
culturally and contextually relevant interventions to empower adolescents in
navigating the digital world, with implications for parents, educators,
designers, and policymakers.

</details>


### [67] [Analytical Study on the Visibility of Potential Positions for External Human-Machine Interfaces](https://arxiv.org/abs/2507.08973)
*Jose Gonzalez-Belmonte,Jaerock Kwon*

Main category: cs.HC

TL;DR: 研究通过Unity模拟探索自动驾驶车辆与行人通信时信号放置的最佳位置，发现前翼子板和前大灯是最容易被行人看到的区域。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆与行人通信的信号放置问题尚未充分研究，需明确最佳位置以确保行人安全。

Method: 使用Unity游戏引擎模拟15种车辆，记录行人视角下车辆前部元素的可见性，变量包括车辆位置、数量和距离。

Result: 行人最常看到的是前翼子板和前大灯，前轮、前门、保险杠和侧镜可见性较低。

Conclusion: 研究结果为自动驾驶车辆信号设计提供了重要参考，Unity模拟平台可用于未来类似研究。

Abstract: As we move towards a future of autonomous vehicles, questions regarding their
method of communication have arisen. One of the common questions concerns the
placement of the signaling used to communicate with pedestrians and road users,
but little work has been published fully dedicated to exploring this. This
paper uses a simulation made in the Unity game engine to record the visibility
of fifteen different vehicles, specifically regarding the visibility of frontal
elements by a pedestrian on the sidewalk. Variables include the vehicle
position, number of vehicles on the road, and minimum and maximum distance of
the recorded points. It was concluded that the areas of the vehicle most often
seen by pedestrians on the sidewalk attempting to cross the road were the
frontal frontal fenders and the headlights, with the frontal wheels, frontal
doors, bumper, and side mirrors are less visible alternatives. These findings
are valuable in the future design of signaling for autonomous vehicles, in
order to ensure pedestrians are able to see them on approaching vehicles. The
software used provides a platform for similar works in the future to be
conducted.

</details>


### [68] [AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data](https://arxiv.org/abs/2507.09100)
*Mohammad Abolnejadian,Shakiba Amirshahi,Matthew Brehmer,Anamaria Crisan*

Main category: cs.HC

TL;DR: 论文探讨了在实时决策对话中如何利用历史数据生成洞察，以医生-患者互动为例，开发了一个基于检索的大型语言模型（LLM）系统，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 专家在实时对话中需快速决策，但难以利用历史数据。研究旨在探索如何通过实时检索相关数据生成洞察以辅助决策。

Method: 开发了一个对话式用户界面，系统实时监听对话，识别问题与解决方案，并从嵌入的数据集中检索相关信息，利用LLM生成简洁洞察。

Result: 原型系统通过模拟医生-患者对话验证了有效性，但也揭示了挑战。

Conclusion: 研究展示了实时数据检索与LLM结合在决策辅助中的潜力，为下一步工作指明了方向。

Abstract: In decision-making conversations, experts must navigate complex choices and
make on-the-spot decisions while engaged in conversation. Although extensive
historical data often exists, the real-time nature of these scenarios makes it
infeasible for decision-makers to review and leverage relevant information.
This raises an interesting question: What if experts could utilize relevant
past data in real-time decision-making through insights derived from past data?
To explore this, we implemented a conversational user interface, taking
doctor-patient interactions as an example use case. Our system continuously
listens to the conversation, identifies patient problems and doctor-suggested
solutions, and retrieves related data from an embedded dataset, generating
concise insights using a pipeline built around a retrieval-based Large Language
Model (LLM) agent. We evaluated the prototype by embedding Health Canada
datasets into a vector database and conducting simulated studies using sample
doctor-patient dialogues, showing effectiveness but also challenges, setting
directions for the next steps of our work.

</details>


### [69] [User-to-PC Authentication Through Confirmation on Mobile Devices: On Usability and Performance](https://arxiv.org/abs/2507.09190)
*Andreas Pramendorfer,Rainhard Dieter Findling*

Main category: cs.HC

TL;DR: 论文提出了一种基于移动设备的无密码认证方法，通过智能手机或智能手表确认登录请求，替代传统的密码认证。


<details>
  <summary>Details</summary>
Motivation: 传统密码认证存在认知负担和弱凭证问题，而现代移动设备具备高级安全功能，可用于改进PC认证。

Method: 采用基于令牌的无密码方法，用户通过移动设备确认或拒绝PC的认证请求，评估了按钮点击和生物指纹验证两种方式。

Result: 智能手表认证在认证时长上优于密码认证和智能手机认证，成功率相当，且用户对其可用性评价最高。

Conclusion: 智能手表认证是一种高效且用户友好的PC认证替代方案。

Abstract: Protecting personal computers (PCs) from unauthorized access typically relies
on password authentication, which is know to suffer from cognitive burden and
weak credentials. As many users nowadays carry mobile devices with advanced
security features throughout their day, there is an opportunity to leverage
these devices to improve authentication to PCs. In this paper we utilize a
token-based passwordless approach where users authenticate to their PC by
confirming the authentication request on their smartphones or smartwatches.
Upon a request to login to the PC, or to evaluate privileges, the PC issues an
authentication request that users receive on their mobile devices, where users
can confirm or deny the request. We evaluate button tap and biometric
fingerprint verification as confirmation variants, and compare their
authentication duration, success rate, and usability to traditional
password-based authentication in a user study with 30 participants and a total
of 1,200 authentication attempts. Smartwatch-based authentication outperformed
password-based authentication and smartphone-based variants in authentication
duration, while showing comparable success rates. Participants rated
smartwatch-based authentication highest in usability, followed by
password-based authentication and smartphone-based authentication.

</details>


### [70] [Discrepancies in Mental Workload Estimation: Self-Reported versus EEG-Based Measures in Data Visualization Evaluation](https://arxiv.org/abs/2507.09262)
*Soobin Yim,Sangbong Yoo,Chanyoung Yoon,Chanyoung Jung,Chansoo Kim,Yun Jang,Ghulam Jilani Quadri*

Main category: cs.HC

TL;DR: 研究探讨了EEG与自我报告在评估心理负荷时的差异，发现两者存在显著不一致，并揭示了可能未被自我报告捕捉的无意识认知努力。


<details>
  <summary>Details</summary>
Motivation: 初步研究发现EEG与自我报告的心理负荷评估存在差异，因此进一步探索这些差异及其原因。

Method: 通过可视化任务实验（VLAT和SV任务），使用32通道EEG系统和GAT架构模型评估心理负荷，并与自我报告结果对比。

Result: 发现EEG评估与任务难度、自我报告之间存在显著差异，表明存在未被自我报告捕捉的无意识认知努力。

Conclusion: EEG和自我报告在心理负荷评估中各有局限性，需结合使用以更全面理解认知过程。

Abstract: Accurate assessment of mental workload (MW) is crucial for understanding
cognitive processes during visualization tasks. While EEG-based measures are
emerging as promising alternatives to conventional assessment techniques, such
as selfreport measures, studies examining consistency across these different
methodologies are limited. In a preliminary study, we observed indications of
potential discrepancies between EEGbased and self-reported MW measures.
Motivated by these preliminary observations, our study further explores the
discrepancies between EEG-based and self-reported MW assessment methods through
an experiment involving visualization tasks. In the experiment, we employ two
benchmark tasks: the Visualization Literacy Assessment Test (VLAT) and a
Spatial Visualization (SV) task. EEG signals are recorded from participants
using a 32-channel system at a sampling rate of 128 Hz during the visualization
tasks. For each participant, MW is estimated using an EEG-based model built on
a Graph Attention Network (GAT) architecture, and these estimates are compared
with conventional MW measures to examine potential discrepancies. Our findings
reveal notable discrepancies between task difficulty and EEG-based MW
estimates, as well as between EEG-based and self-reported MW measures across
varying task difficulty levels. Additionally, the observed patterns suggest the
presence of unconscious cognitive effort that may not be captured by selfreport
alone.

</details>


### [71] [TraSculptor: Visual Analytics for Enhanced Decision-Making in Road Traffic Planning](https://arxiv.org/abs/2507.09489)
*Zikun Deng,Yuanbang Liu,Mingrui Zhu,Da Xiang,Haiyue Yu,Zicheng Su,Qinglong Lu,Tobias Schreck,Yi Cai*

Main category: cs.HC

TL;DR: TraSculptor是一个交互式交通规划决策系统，解决了现有平台在灵活修改路网结构和直观比较多个状态时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有交通规划平台在灵活交互和直观比较多个路网状态时效率低下，影响了规划效果。

Method: TraSculptor通过灵活的路网修改交互和设计历史树与路网状态矩阵的比较视图，解决了这两个挑战。

Result: 通过展示Braess悖论案例和对Sioux Falls网络的专家案例研究，验证了TraSculptor的有效性。

Conclusion: TraSculptor提升了交通规划的效率与直观性，为专家提供了更好的决策支持。

Abstract: The design of urban road networks significantly influences traffic
conditions, underscoring the importance of informed traffic planning. Traffic
planning experts rely on specialized platforms to simulate traffic systems,
assessing the efficacy of the road network across various states of
modifications. Nevertheless, a prevailing issue persists: many existing traffic
planning platforms exhibit inefficiencies in flexibly interacting with the road
network's structure and attributes and intuitively comparing multiple states
during the iterative planning process. This paper introduces TraSculptor, an
interactive planning decision-making system. To develop TraSculptor, we
identify and address two challenges: interactive modification of road networks
and intuitive comparison of multiple network states. For the first challenge,
we establish flexible interactions to enable experts to easily and directly
modify the road network on the map. For the second challenge, we design a
comparison view with a history tree of multiple states and a road-state matrix
to facilitate intuitive comparison of road network states. To evaluate
TraSculptor, we provided a usage scenario where the Braess's paradox was
showcased, invited experts to perform a case study on the Sioux Falls network,
and collected expert feedback through interviews.

</details>


### [72] [The Spectacle of Fidelity: Blind Resistance and the Wizardry of Prototyping](https://arxiv.org/abs/2507.09549)
*Hrittika Bhowmick,Shilpaa Anand*

Main category: cs.HC

TL;DR: 论文批判了以视觉为中心的交互设计原型文化，提出应重新定位原型设计为一种情境化、具身化和关系化的实践。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉中心主义在原型设计中的主导地位及其对盲人学者和其他非视觉认知方式的排斥。

Method: 结合盲人学者的生活经验和文化残疾研究，分析原型设计的视觉偏见。

Result: 揭示了原型设计中对视觉的过度依赖如何边缘化其他认知和制作方式。

Conclusion: 呼吁HCI领域重新思考原型设计的参与方式，避免将其简化为基于屏幕的模拟。

Abstract: Prototyping is widely regarded in Human-Computer Interaction as an iterative
process through which ideas are tested and refined, often via visual mockups,
screen flows, and coded simulations. This position paper critiques the
visual-centric norms embedded in prototyping culture by drawing from the lived
experiences of blind scholars and insights from cultural disability studies. It
discusses how dominant methods of prototyping rely on an unexamined fidelity to
sight, privileging what can be rendered visibly coherent while marginalizing
other modes of knowing and making. By repositioning prototyping as a situated,
embodied, and relational practice, this paper challenges HCI to rethink what
kinds of design participation are legitimized and which are excluded when
prototyping is reduced to screen-based simulations.

</details>


### [73] [SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations](https://arxiv.org/abs/2507.09664)
*Zoe Kaputa,Anika Rajaram,Vryan Almanon Feliciano,Zhuoyue Lyu,Maneesh Agrawala,Hari Subramonyam*

Main category: cs.HC

TL;DR: 论文提出Chain-of-Abstractions (CoA)框架，通过分解生成过程为多个抽象层次，恢复编程的核心特性（如可追溯性、逐步优化等），并在SimStep环境中实现，帮助教师更高效地创建交互式学习内容。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的编程提示方式为非程序员（如教师）提供了便利，但绕过了直接编写代码，导致编程的核心特性（如可追溯性、逐步优化等）丢失。

Method: 提出CoA框架，将生成过程分解为多个任务对齐的抽象层次（如概念图、场景图等），并在SimStep环境中实现，支持逆向修正过程。

Result: 评估表明，CoA在编程提示工作流中提供了更高的创作控制和可解释性。

Conclusion: CoA框架在保留自然语言灵活性的同时，恢复了编程的核心特性，为非程序员提供了更高效的工具。

Abstract: Programming-by-prompting with generative AI offers a new paradigm for
end-user programming, shifting the focus from syntactic fluency to semantic
intent. This shift holds particular promise for non-programmers such as
educators, who can describe instructional goals in natural language to generate
interactive learning content. Yet in bypassing direct code authoring, many of
programming's core affordances - such as traceability, stepwise refinement, and
behavioral testing - are lost. We propose the Chain-of-Abstractions (CoA)
framework as a way to recover these affordances while preserving the expressive
flexibility of natural language. CoA decomposes the synthesis process into a
sequence of cognitively meaningful, task-aligned representations that function
as checkpoints for specification, inspection, and refinement. We instantiate
this approach in SimStep, an authoring environment for teachers that scaffolds
simulation creation through four intermediate abstractions: Concept Graph,
Scenario Graph, Learning Goal Graph, and UI Interaction Graph. To address
ambiguities and misalignments, SimStep includes an inverse correction process
that surfaces in-filled model assumptions and enables targeted revision without
requiring users to manipulate code. Evaluations with educators show that CoA
enables greater authoring control and interpretability in
programming-by-prompting workflows.

</details>


### [74] [Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series](https://arxiv.org/abs/2507.09917)
*Zikun Deng,Jiabao Huang,Chenxi Ruan,Jialing Li,Shaowu Gao,Yi Cai*

Main category: cs.HC

TL;DR: VolumeSTCube是一种新颖的技术框架，用于连续时空现象的可视化，解决了空间时间立方体（STC）的视觉遮挡和深度模糊问题。


<details>
  <summary>Details</summary>
Motivation: 尽管STC在时空数据可视化中表现出色，但其在处理大规模空间时间序列数据时存在视觉遮挡和深度模糊等问题。

Method: VolumeSTCube将离散分布的空间时间序列数据转换为连续体积数据，并采用体积渲染和表面渲染技术进行可视化。

Result: 通过计算实验、真实案例研究和用户研究验证，VolumeSTCube在大规模空间时间序列分析中表现出优越性和有效性。

Conclusion: VolumeSTCube为时空数据可视化提供了一种高效且用户友好的解决方案。

Abstract: Spatial time series visualization offers scientific research pathways and
analytical decision-making tools across various spatiotemporal domains. Despite
many advanced methodologies, the seamless integration of temporal and spatial
information remains a challenge. The space-time cube (STC) stands out as a
promising approach for the synergistic presentation of spatial and temporal
information, with successful applications across various spatiotemporal
datasets. However, the STC is plagued by well-known issues such as visual
occlusion and depth ambiguity, which are further exacerbated when dealing with
large-scale spatial time series data. In this study, we introduce a novel
technical framework termed VolumeSTCube, designed for continuous spatiotemporal
phenomena. It first leverages the concept of the STC to transform discretely
distributed spatial time series data into continuously volumetric data.
Subsequently, volume rendering and surface rendering techniques are employed to
visualize the transformed volumetric data. Volume rendering is utilized to
mitigate visual occlusion, while surface rendering provides pattern details by
enhanced lighting information. Lastly, we design interactions to facilitate the
exploration and analysis from temporal, spatial, and spatiotemporal
perspectives. VolumeSTCube is evaluated through a computational experiment, a
real-world case study with one expert, and a controlled user study with twelve
non-experts, compared against a baseline from prior work, showing its
superiority and effectiveness in largescale spatial time series analysis.

</details>


### [75] [Branch Explorer: Leveraging Branching Narratives to Support Interactive 360° Video Viewing for Blind and Low Vision Users](https://arxiv.org/abs/2507.09959)
*Shuchang Xu,Xiaofu Jin,Wenshuo Zhang,Huamin Qu,Yukang Yan*

Main category: cs.HC

TL;DR: Branch Explorer系统通过将360度视频转化为分支叙事，支持盲人和低视力用户的互动观看体验。


<details>
  <summary>Details</summary>
Motivation: 360度视频通常排除了盲人和低视力用户的互动体验，Branch Explorer旨在填补这一空白。

Method: 采用多模态机器学习管道生成多样化的叙事路径，用户可以在检测到的分支点灵活选择，并通过沉浸式音频引导参与每个故事线。

Result: 评估显示，Branch Explorer显著提升了用户的自主性和参与感，并帮助他们发展个性化的探索策略。

Conclusion: 该系统为视频和虚拟环境的无障碍探索提供了重要启示。

Abstract: 360{\deg} videos enable users to freely choose their viewing paths, but blind
and low vision (BLV) users are often excluded from this interactive experience.
To bridge this gap, we present Branch Explorer, a system that transforms
360{\deg} videos into branching narratives -- stories that dynamically unfold
based on viewer choices -- to support interactive viewing for BLV audiences.
Our formative study identified three key considerations for accessible
branching narratives: providing diverse branch options, ensuring coherent story
progression, and enabling immersive navigation among branches. To address these
needs, Branch Explorer employs a multi-modal machine learning pipeline to
generate diverse narrative paths, allowing users to flexibly make choices at
detected branching points and seamlessly engage with each storyline through
immersive audio guidance. Evaluation with 12 BLV viewers showed that Branch
Explorer significantly enhanced user agency and engagement in 360{\deg} video
viewing. Users also developed personalized strategies for exploring 360{\deg}
content. We further highlight implications for supporting accessible
exploration of videos and virtual environments.

</details>


### [76] [Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles](https://arxiv.org/abs/2507.10024)
*Shaolun Ruan,Rui Sheng,Xiaolin Wen,Jiachen Wang,Tianyi Zhang,Yong Wang,Tim Dwyer,Jiannan Li*

Main category: cs.HC

TL;DR: 本文通过多阶段定性研究，探讨了大型语言模型（LLMs）在可视化设计研究中的应用，总结了其角色、挑战及实践策略。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在可视化设计研究中潜力巨大，但目前缺乏对其如何有效辅助研究的系统性理解。

Method: 研究采用多阶段定性方法，包括对30名设计研究者的深度访谈和问卷调查。

Result: 研究发现LLMs在设计研究各阶段的具体角色，并总结了实践中的挑战及应对策略。

Conclusion: 研究为可视化从业者提供了实用框架，以利用LLMs优化设计研究过程。

Abstract: Design studies aim to create visualization solutions for real-world problems
of different application domains. Recently, the emergence of large language
models (LLMs) has introduced new opportunities to enhance the design study
process, providing capabilities such as creative problem-solving, data
handling, and insightful analysis. However, despite their growing popularity,
there remains a lack of systematic understanding of how LLMs can effectively
assist researchers in visualization-specific design studies. In this paper, we
conducted a multi-stage qualitative study to fill this gap, involving 30 design
study researchers from diverse backgrounds and expertise levels. Through
in-depth interviews and carefully-designed questionnaires, we investigated
strategies for utilizing LLMs, the challenges encountered, and the practices
used to overcome them. We further compiled and summarized the roles that LLMs
can play across different stages of the design study process. Our findings
highlight practical implications to inform visualization practitioners, and
provide a framework for leveraging LLMs to enhance the design study process in
visualization research.

</details>


### [77] [XROps: A Visual Workflow Management System for Dynamic Immersive Analytics](https://arxiv.org/abs/2507.10043)
*Suemin Jeon,JunYoung Choi,Haejin Jeong,Won-Ki Jeong*

Main category: cs.HC

TL;DR: XROps是一个基于网页的创作系统，通过交互式可视化编程降低沉浸式分析系统的技术门槛，支持实时数据集成和动态调整。


<details>
  <summary>Details</summary>
Motivation: 沉浸式分析系统需要编程专业知识，开发难度大，限制了领域专家的使用。

Method: XROps采用交互式可视化编程，无需底层脚本，支持实时数据集成和动态调整。

Result: 用户研究表明XROps具有高可用性，并在多个场景中展示了其有效性。

Conclusion: XROps降低了沉浸式分析的技术门槛，适合动态任务和实时数据环境。

Abstract: Immersive analytics is gaining attention across multiple domains due to its
capability to facilitate intuitive data analysis in expansive environments
through user interaction with data. However, creating immersive analytics
systems for specific tasks is challenging due to the need for programming
expertise and significant development effort. Despite the introduction of
various immersive visualization authoring toolkits, domain experts still face
hurdles in adopting immersive analytics into their workflow, particularly when
faced with dynamically changing tasks and data in real time. To lower such
technical barriers, we introduce XROps, a web-based authoring system that
allows users to create immersive analytics applications through interactive
visual programming, without the need for low-level scripting or coding. XROps
enables dynamic immersive analytics authoring by allowing users to modify each
step of the data visualization process with immediate feedback, enabling them
to build visualizations on-the-fly and adapt to changing environments. It also
supports the integration and visualization of real-time sensor data from XR
devices, a key feature of immersive analytics, facilitating the creation of
various analysis scenarios. We evaluated the usability of XROps through a user
study and demonstrate its efficacy and usefulness in several example scenarios.
We have released a web platform (https://vience.io/xrops) to demonstrate
various examples to supplement our findings.

</details>


### [78] [MEDebiaser: A Human-AI Feedback System for Mitigating Bias in Multi-label Medical Image Classification](https://arxiv.org/abs/2507.10044)
*Shaohan Shi,Yuheng Shao,Haoran Jiang,Yunjie Yao,Zhijun Zhang,Xu Ding,Quan Li*

Main category: cs.HC

TL;DR: MEDebiaser是一个交互式系统，允许医生通过本地解释直接优化AI模型，减少多标签医学图像分类中的偏差。


<details>
  <summary>Details</summary>
Motivation: 医学图像的多标签分布不平衡和共现问题导致分类偏差，传统医工协作模式难以高效整合医学专业知识。

Method: 结合预测与注意力损失函数，采用定制排名策略，医生无需技术背景即可直接修正模型。

Result: 有效减少偏差，提升可用性和协作效率。

Conclusion: MEDebiaser为医学专业知识与AI结合提供了实用解决方案。

Abstract: Medical images often contain multiple labels with imbalanced distributions
and co-occurrence, leading to bias in multi-label medical image classification.
Close collaboration between medical professionals and machine learning
practitioners has significantly advanced medical image analysis. However,
traditional collaboration modes struggle to facilitate effective feedback
between physicians and AI models, as integrating medical expertise into the
training process via engineers can be time-consuming and labor-intensive. To
bridge this gap, we introduce MEDebiaser, an interactive system enabling
physicians to directly refine AI models using local explanations. By combining
prediction with attention loss functions and employing a customized ranking
strategy to alleviate scalability, MEDebiaser allows physicians to mitigate
biases without technical expertise, reducing reliance on engineers, and thus
enhancing more direct human-AI feedback. Our mechanism and user studies
demonstrate that it effectively reduces biases, improves usability, and
enhances collaboration efficiency, providing a practical solution for
integrating medical expertise into AI-driven healthcare.

</details>


### [79] [ReDemon UI: Reactive Synthesis by Demonstration for Web UI](https://arxiv.org/abs/2507.10099)
*Jay Lee,Gyuhyeok Oh,Joongwon Ahn,Xiaokang Qiu*

Main category: cs.HC

TL;DR: ReDemon UI通过用户演示合成React应用，支持设计师和非专业程序员创建符合标准UI原型工作流的界面。


<details>
  <summary>Details</summary>
Motivation: 为非专业程序员和设计师提供一种无需编码即可创建动态UI的方法，同时与标准UI原型工作流集成。

Method: 用户提供静态草图并演示行为，系统识别响应式数据并合成React程序，结合枚举合成和LLM处理复杂UI。

Result: 成功合成具有正确状态更新逻辑的React程序。

Conclusion: ReDemon UI为UI设计提供了一种高效且用户友好的解决方案。

Abstract: ReDemon UI synthesizes React applications from user demonstrations, enabling
designers and non-expert programmers to create UIs that integrate with standard
UI prototyping workflows. Users provide a static mockup sketch with event
handler holes and demonstrate desired runtime behaviors by interacting with the
rendered mockup and editing the sketch. ReDemon UI identifies reactive data and
synthesizes a React program with correct state update logic. We utilize
enumerative synthesis for simple UIs and LLMs for more complex UIs.

</details>


### [80] [When Familiarity Remains: Procedural Memory, Symbolic Anchors, and Digital Engagement in Dementia Care](https://arxiv.org/abs/2507.10102)
*Jeongone Seo,Kyung-zoon Hong,Sol Baik*

Main category: cs.HC

TL;DR: 研究探讨了程序性记忆和象征性锚点如何支持韩国老年痴呆患者的数字参与，发现熟悉的技术和个性化内容能增强互动，而陌生技术可能引发恐惧。


<details>
  <summary>Details</summary>
Motivation: 探索程序性记忆和象征性锚点在老年痴呆患者数字参与中的作用，以提升护理技术的文化适应性和认知可及性。

Method: 对11名社区老年痴呆患者的专业护理人员进行深度访谈，采用扎根理论方法分析数据。

Result: 熟悉的数字习惯通过程序性记忆得以保持，象征性锚点增强互动；陌生技术可能引发负面情绪。

Conclusion: 设计文化响应和认知可及的技术可提升老年痴呆患者的自主性和福祉。

Abstract: INTRODUCTION: Older adults with early-stage dementia often retain procedural
memory, enabling continued use of familiar technologies. Additionally, symbolic
anchors such as photos or personalized content may serve as memory cues to
reinforce digital engagement. This study explores how these mechanisms support
technology use in dementia care within the South Korean context.
  METHODS: We conducted in-depth interviews with 11 professional caregivers of
community-dwelling older adults with cognitive decline. Grounded theory methods
guided the analysis, using iterative coding and constant comparison to identify
emergent themes.
  RESULTS: Caregivers reported that familiar digital routines (e.g., taking
photos) persisted through procedural memory. Symbolic anchors such as family
photos or recognizable icons enhanced interaction and emotional engagement.
However, unfamiliar or anthropomorphic technologies often triggered fear or
symbolic resistance.
  DISCUSSION: Findings highlight the dual role of procedural memory and
symbolic anchors in sustaining digital engagement. Designing culturally
responsive and cognitively accessible technologies may enhance autonomy and
well-being in dementia care.
  Keywords: procedural memory, symbolic anchors, dementia care, digital
engagement, older adults, cultural adaptation, caregiving technologies

</details>


### [81] [Visual Analytics for Explainable and Trustworthy Artificial Intelligence](https://arxiv.org/abs/2507.10240)
*Angelos Chatzimparmpas*

Main category: cs.HC

TL;DR: 论文探讨了如何通过视觉分析（VA）提高AI系统的透明度，从而增强专家对AI的信任，并提出了一个设计空间以支持AI流程中的关键任务。


<details>
  <summary>Details</summary>
Motivation: AI系统缺乏透明度（“黑箱”问题）阻碍了专家的信任和采用，尤其是在医疗等关键领域。视觉分析（VA）通过结合AI模型与交互式可视化，为解决这一问题提供了可能。

Method: 论文定义了VA解决方案的分类，并探索其在AI流程各阶段（如数据处理、模型调试等）中的应用。提出了一个可视化设计空间，并展示了已开发的VA仪表板。

Result: VA能够有效提升AI系统的透明度，帮助专家理解、调试和改进模型，从而增强信任。

Conclusion: 视觉分析是解决AI“黑箱”问题的有效方法，未来可进一步优化设计以支持更广泛的AI应用。

Abstract: Our society increasingly depends on intelligent systems to solve complex
problems, ranging from recommender systems suggesting the next movie to watch
to AI models assisting in medical diagnoses for hospitalized patients. With the
iterative improvement of diagnostic accuracy and efficiency, AI holds
significant potential to mitigate medical misdiagnoses by preventing numerous
deaths and reducing an economic burden of approximately 450 EUR billion
annually. However, a key obstacle to AI adoption lies in the lack of
transparency: many automated systems function as "black boxes," providing
predictions without revealing the underlying processes. This opacity can hinder
experts' ability to trust and rely on AI systems. Visual analytics (VA)
provides a compelling solution by combining AI models with interactive
visualizations. These specialized charts and graphs empower users to
incorporate their domain expertise to refine and improve the models, bridging
the gap between AI and human understanding. In this work, we define,
categorize, and explore how VA solutions can foster trust across the stages of
a typical AI pipeline. We propose a design space for innovative visualizations
and present an overview of our previously developed VA dashboards, which
support critical tasks within the various pipeline stages, including data
processing, feature engineering, hyperparameter tuning, understanding,
debugging, refining, and comparing models.

</details>


### [82] [Towards Emotion Co-regulation with LLM-powered Socially Assistive Robots: Integrating LLM Prompts and Robotic Behaviors to Support Parent-Neurodivergent Child Dyads](https://arxiv.org/abs/2507.10427)
*Jing Li,Felix Schijve,Sheng Li,Yuye Yang,Jun Hu,Emilia Barakova*

Main category: cs.HC

TL;DR: 研究探讨了如何结合大型语言模型（LLM）与社会辅助机器人（SAR）来帮助神经发育障碍儿童及其家长进行情绪共调节。


<details>
  <summary>Details</summary>
Motivation: 现有研究在利用先进技术支持家长与儿童情绪共调节方面存在不足，尤其是LLM与SAR的结合尚未充分探索。

Method: 开发了一个基于MiRo-E机器人平台的LLM驱动社交机器人，结合语音通信模块和机器人行为，提供定制化干预。

Result: 初步测试显示，该系统对互动动态有积极影响，并有助于情绪调节，但也发现设计和技术的挑战。

Conclusion: 研究为未来LLM驱动的SAR在心理健康应用中的发展提供了设计启示。

Abstract: Socially Assistive Robotics (SAR) has shown promise in supporting emotion
regulation for neurodivergent children. Recently, there has been increasing
interest in leveraging advanced technologies to assist parents in co-regulating
emotions with their children. However, limited research has explored the
integration of large language models (LLMs) with SAR to facilitate emotion
co-regulation between parents and children with neurodevelopmental disorders.
To address this gap, we developed an LLM-powered social robot by deploying a
speech communication module on the MiRo-E robotic platform. This supervised
autonomous system integrates LLM prompts and robotic behaviors to deliver
tailored interventions for both parents and neurodivergent children. Pilot
tests were conducted with two parent-child dyads, followed by a qualitative
analysis. The findings reveal MiRo-E's positive impacts on interaction dynamics
and its potential to facilitate emotion regulation, along with identified
design and technical challenges. Based on these insights, we provide design
implications to advance the future development of LLM-powered SAR for mental
health applications.

</details>


### [83] [An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments](https://arxiv.org/abs/2507.10469)
*Mikko Korkiakoski,Saeid Sheikhi,Jesper Nyman,Jussi Saariniemi,Kalle Tapio,Panos Kostakos*

Main category: cs.HC

TL;DR: 论文探讨了AI驱动的NPC在VR审讯模拟器中的表现，评估了其真实性、可用性和系统性能，发现GPT-4 Turbo提升了交互体验，但存在延迟和情感深度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 研究AI如何通过大型语言模型（如GPT-4 Turbo）提升VR中NPC的真实性和交互性，以创造更沉浸的用户体验。

Method: 使用VR审讯模拟器，包含两个AI驱动的NPC（嫌疑人和搭档），通过用户研究（18名参与者）评估系统性能、真实性和可用性。

Result: 平均循环延迟为7秒，NPC真实性评分为6.67/10，系统可用性评分为79.44（SUS），显示良好表现但需优化延迟和情感深度。

Conclusion: 大型语言模型能显著提升NPC的真实性和交互性，但需进一步优化系统性能和情感表现以实现更沉浸的VR体验。

Abstract: Advancements in artificial intelligence (AI) have significantly enhanced the
realism and interactivity of non-player characters (NPCs) in virtual reality
(VR), creating more engaging and believable user experiences. This paper
evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their
perceived realism, usability, and system performance. The simulator features
two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage
participants in a scenario to determine the suspect's guilt or innocence. A
user study with 18 participants assessed the system using the System Usability
Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent
Believability Questionnaire, alongside latency measurements for speech-to-text
(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.
Results showed an average cycle latency of 7 seconds, influenced by the
increasing conversational context. Believability scored 6.67 out of 10, with
high ratings in behavior, social relationships, and intelligence but moderate
scores in emotion and personality. The system achieved a SUS score of 79.44,
indicating good usability. These findings demonstrate the potential of large
language models to improve NPC realism and interaction in VR while highlighting
challenges in reducing system latency and enhancing emotional depth. This
research contributes to the development of more sophisticated AI-driven NPCs,
revealing the need for performance optimization to achieve increasingly
immersive virtual experiences.

</details>


### [84] [VIP-Sim: A User-Centered Approach to Vision Impairment Simulation for Accessible Design](https://arxiv.org/abs/2507.10479)
*Max Rädler,Mark Colley,Enrico Rukzio*

Main category: cs.HC

TL;DR: VIP-Sim是一个基于症状的视觉障碍模拟器，通过参与式设计开发，旨在帮助设计师更好地理解视觉障碍者的体验。


<details>
  <summary>Details</summary>
Motivation: 现有视觉障碍模拟器缺乏直接视觉障碍者参与和评估，VIP-Sim填补了这一空白。

Method: 通过参与式设计过程开发VIP-Sim，涉及7名视觉障碍者，模拟21种症状。

Result: 多数参与者认为VIP-Sim能复现其症状，但对其代表性和全面性仍有担忧。

Conclusion: VIP-Sim是一个有潜力的工具，但需进一步验证其广泛适用性。

Abstract: People with vision impairments (VIPs) often rely on their remaining vision
when interacting with user interfaces. Simulating visual impairments is an
effective tool for designers, fostering awareness of the challenges faced by
VIPs. While previous research has introduced various vision impairment
simulators, none have yet been developed with the direct involvement of VIPs or
thoroughly evaluated from their perspective. To address this gap, we developed
VIP-Sim. This symptom-based vision simulator was created through a
participatory design process tailored explicitly for this purpose, involving
N=7 VIPs. 21 symptoms, like field loss or light sensitivity, can be overlaid on
desktop design tools. Most participants felt VIP-Sim could replicate their
symptoms. VIP-Sim was received positively, but concerns about exclusion in
design and comprehensiveness of the simulation remain, mainly whether it
represents the experiences of other VIPs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [85] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 论文提出了一种通过去除推理过程中的冗余信息来提高大型语言模型性能的方法，重点关注注意力稀疏性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在长链推理中存在冗余，注意力分散，尤其是错误答案中更明显，影响了性能。

Method: 通过测量特殊‘思考结束’标记的注意力分数识别冗余，采用结构感知剪枝去除低贡献推理块，再恢复推理生成。

Result: 该方法显著提高了推理密集型任务的准确性，尤其在数学竞赛基准测试（如AIME和AMC）中表现突出。

Conclusion: 去除推理冗余能有效提升模型性能，无需额外训练，适用于复杂推理任务。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [86] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 提出了一种结合两种虚拟差距分析（VGA）模型的新型多标准评估（MCA）方法，以提高评估的效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有MCA方法依赖假设和主观判断，且常采用同质性假设，影响评估效果。需要一种更全面可靠的方法。

Method: 结合两种VGA模型，基于线性规划，处理定量和定性标准。

Result: 通过两个数值示例验证了方法的准确性和透明度。

Conclusion: 该方法为自动化决策系统提供了强大且灵活的解决方案，推动决策支持系统的进步。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [87] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 论文提出了一种基于桌游角色扮演游戏（TTRPGs）的灵活场景定义框架，用于支持生成式AI在多角色环境中的多样化应用。


<details>
  <summary>Details</summary>
Motivation: 为了满足生成式AI在模拟、戏剧化和评估等多样化应用场景中的需求，需要一个灵活的框架来定义和管理场景。

Method: 借鉴TTRPGs中的游戏主持人（GM）概念，采用实体-组件架构模式，将GM设计为可配置的实体，由组件构成，实现工程师与设计师的分工协作。

Result: 通过Concordia库的实践，证明了该框架能够有效支持用户根据特定目标配置场景，实现快速迭代和模块化。

Conclusion: 基于TTRPGs的实体-组件架构模式为生成式AI在多角色环境中的应用提供了灵活、可扩展的解决方案。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [88] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: BioAnalyst是一个基于Transformer架构的AI基础模型，专为生物多样性分析和保护规划设计，通过多模态数据预训练，适用于多种下游任务，如物种分布建模和栖息地评估。


<details>
  <summary>Details</summary>
Motivation: 生物多样性丧失加速，威胁生态平衡和可持续性，需要综合监测和保护规划能力。AI基础模型在科学领域表现优异，适用于生物多样性保护。

Method: BioAnalyst采用Transformer架构，预训练于多模态数据（物种记录、遥感指标、气候环境变量），可微调适应多种下游任务。

Result: BioAnalyst在数据稀缺场景下表现优于现有方法，为生态预测设定了新的准确性基准。

Conclusion: BioAnalyst的开放发布旨在促进生物多样性建模合作，推动AI解决生态挑战。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [89] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: 研究发现，尽管开发者预期AI工具能缩短任务完成时间20%，但实际使用后反而增加了19%的时间，与经济学和ML专家的预测相反。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具对开源开发者生产力的实际影响，填补现有研究的空白。

Method: 采用随机对照试验（RCT），16名有中等AI经验的开发者完成246项任务，随机分配是否使用2025年的AI工具（如Cursor Pro和Claude 3.5/3.7 Sonnet）。

Result: 允许使用AI工具时，任务完成时间增加了19%，与开发者预期的20%缩短和专家预测的38-39%缩短形成鲜明对比。

Conclusion: AI工具在实际开发中可能因多种因素（如项目规模、质量标准或开发者经验）反而降低生产力，这一现象值得进一步研究。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [90] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体强化学习（MARL）的框架，用于检测去中心化金融（DeFi）中的市场操纵行为，通过动态对抗游戏建模操纵者与检测者的互动。


<details>
  <summary>Details</summary>
Motivation: 去中心化金融缺乏集中监管，导致市场操纵行为频发，需要一种去中心化的检测方法。

Method: 采用MARL框架，结合GRPO优化学习稳定性，理论驱动的奖励函数，以及多模态智能体管道整合语义、社交图和链上数据。

Result: 在真实数据和对抗模拟中验证，Hide-and-Shill系统在检测准确性和因果归因方面表现优异。

Conclusion: 该研究为去中心化市场情报提供了新范式，推动了多智能体系统与金融监管的结合。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [91] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: LLM-based coding agents的安全风险首次系统评估，发现21%的不安全行为，提出检测系统和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 理解LLM编码代理在软件开发中的安全影响，填补研究空白。

Method: 分析5种先进模型在93个真实任务中的12,000多个动作，开发高精度检测系统。

Result: 21%的行为不安全，信息暴露最常见；GPT-4.1缓解成功率96.8%。

Conclusion: 需安全设计下一代LLM编码代理，提供首个全面评估框架。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [92] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 本文提出了一种关于AI可能导致全人类灭绝事件的分类和示例，旨在通过公开讨论这些可能性，推动预防措施。


<details>
  <summary>Details</summary>
Motivation: 通过公开讨论AI可能带来的灾难性风险，争取公众支持以推动预防措施。

Method: 提出分类法和具体示例，展示AI可能导致的全人类灭绝情景。

Result: 明确了AI潜在的全人类灭绝风险，并呼吁采取预防措施。

Conclusion: 公开讨论AI的灾难性风险有助于推动预防措施，减少潜在的全人类灭绝事件。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [93] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow是一个端到端框架，旨在提升多模态大语言模型（MLLMs）在科学任务中的表现，通过引入EduPRM和EduMCTS技术，优化推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在科学任务中表现不佳，尤其是在需要多步和可解释推理的场景中，缺乏科学推理模式、全局一致性和自我修正能力。

Method: 提出EduFlow框架，包括数据选择、MCTS轨迹构建、模型训练和输出优化；核心是EduPRM（过程感知奖励模型）和EduMCTS（领域适应搜索框架）。

Result: 实验表明EduFlow显著提升了推理的一致性和连贯性，并构建了EduMCTS-160K数据集。

Conclusion: EduFlow通过动态适应和迭代优化，有效解决了MLLMs在科学推理中的局限性。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [94] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 论文探讨了可解释性和适应性在AI系统中的重要性，特别是针对神经符号AI系统的设计和评估。


<details>
  <summary>Details</summary>
Motivation: 研究如何结合可解释性和适应性，设计可迁移且可解释的神经符号AI系统。

Method: 系统评估了知识表示的结构和复杂性对LLM查询三元组存储的影响。

Result: 结果表明，知识表示的不同方式对AI代理的查询效果有显著影响。

Conclusion: 研究强调了知识表示在AI系统中的重要性，并讨论了其潜在影响。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [95] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: LLM-Stackelberg 博弈框架将大语言模型（LLMs）融入领导者与跟随者的策略交互中，突破了经典博弈理论的完全信息与理性假设，通过结构化提示和概率行为生成实现策略适应。


<details>
  <summary>Details</summary>
Motivation: 传统 Stackelberg 博弈假设完全信息和理性行为者，而现实决策中存在信息不对称和有限理性，LLMs 的引入为建模此类复杂交互提供了新思路。

Method: 提出两种均衡概念：推理与行为均衡（内部推理与行为一致）和推测推理均衡（考虑对手响应的认知不确定性），并通过钓鱼攻击案例验证框架。

Result: LLM-Stackelberg 博弈能有效建模网络安全、错误信息和推荐系统等领域的决策过程，展示了 LLM 交互的认知复杂性和对抗潜力。

Conclusion: 该框架为有限理性、信息不对称和认知适应的策略交互提供了新范式，具有广泛的应用前景。

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [96] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 论文提出从反应式多智能体强化学习转向生成式AI驱动的主动式多智能体智能，以解决传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习方法存在联合动作空间爆炸、非平稳环境和部分可观测性等问题，且缺乏应对新场景的能力。

Method: 采用生成式AI强化学习，将智能体视为能够建模多智能体动态并预测未来交互的生成模型，实现主动决策和协调。

Result: 生成式AI强化学习能够实现主动决策、增强协调和动态适应，为分布式智能提供新可能。

Conclusion: 这一范式转变有望解决传统反应式框架难以处理的协调问题，推动多智能体协作智能的发展。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [97] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: CTP是一种基于模型的离线强化学习方法，通过单步轨迹生成实现高效优化，显著降低计算成本，并在性能上优于现有扩散模型方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在规划任务中计算成本高，CTP旨在解决这一问题，同时保持高性能。

Method: 利用一致性轨迹模型（CTM）进行快速单步轨迹生成，减少迭代采样步骤。

Result: 在D4RL基准测试中，CTP性能优于现有方法，且推理速度提升120倍。

Conclusion: CTP是一种高效、低延迟的离线规划方法，适用于高性能任务。

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [98] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: 提出了一种基于Metropolis-Hastings采样的框架，用于训练SNN在RL任务中，无需依赖梯度方法，并在控制任务中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: SNN在实时控制系统中具有生物启发和高效能优势，但其训练因脉冲通信的不可微性而面临挑战，尤其是在RL任务中。

Method: 采用Metropolis-Hastings采样技术，通过迭代提出并概率性接受参数更新，绕过反向传播限制，直接优化神经形态平台上的SNN。

Result: 在AcroBot和CartPole控制任务中，该方法在累积奖励、网络资源和训练次数上优于传统DQL和现有SNN方法。

Conclusion: 该框架为SNN在RL任务中的训练提供了一种高效且无需梯度的解决方案，展示了在神经形态平台上的潜力。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [99] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens是一个面向企业的AIaaS平台，整合专有数据、工作流程和主流LLM，提供数据安全与自动化支持。


<details>
  <summary>Details</summary>
Motivation: 解决企业在AI应用中面临的数据安全和知识保留问题，同时提升团队效率。

Method: 结合结构化文档处理、混合向量检索和无代码编排（LangChain），支持多种LLM，并通过THOR Agent处理SQL查询。

Result: 实验显示，512令牌分块检索精度最高（Top-3准确率91.3%），生成质量在TRACe指标下提升23%。

Conclusion: eSapiens在如法律和金融等高要求领域实现了可信、可审计的AI工作流程。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [100] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: 本文探讨了AI快速发展带来的环境和伦理挑战，包括能源消耗、电子废物、计算资源不平等及网络安全系统的隐藏能源负担。


<details>
  <summary>Details</summary>
Motivation: AI的快速扩张引发了被忽视的环境和伦理问题，需系统性研究其影响。

Method: 通过综述近期研究和机构报告，分析AI在能源消耗、电子废物、计算资源不平等及网络安全能源负担方面的系统性影响。

Result: 揭示了AI模型训练的高排放、硬件快速淘汰、全球基础设施不平等及网络安全能源需求等问题。

Conclusion: AI发展需与伦理责任和环境保护结合，推动可持续、透明和公平的技术未来。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [101] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 提出了一种结合多模态语言模型与知识图谱的神经符号框架，以提升服务机器人在复杂环境中的互操作性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前服务机器人系统依赖专有解决方案，难以适应和扩展，而知识图谱和多模态语言模型各有局限性，需要结合两者优势。

Method: 提出神经符号框架，结合多模态语言模型的感知能力和知识图谱的结构化表示，生成符合本体论的知识图谱。

Result: 评估显示GPT-o1和LLaMA 4 Maverick表现最佳，但新模型不一定更好，集成策略是关键。

Conclusion: 神经符号框架有效支持机器人互操作性，模型选择和集成策略对性能至关重要。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [102] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: 该论文介绍了一个基于PyTorch的工具包，用于通过随机控制技术建模多代理AI系统的交互，并提供公平性和鲁棒性的先验保证。


<details>
  <summary>Details</summary>
Motivation: 多代理AI系统的交互需要满足公平性和鲁棒性的先验保证，但现有的方法在处理这些复杂系统时存在困难。

Method: 开发了一个开源的PyTorch工具包，利用随机控制技术建模AI系统与代理的交互，并以闭环方式实现公平性和鲁棒性。

Result: 该工具包简化了多代理系统闭环模型中公平性保证的复杂性。

Conclusion: 该工具包为多代理AI系统的公平性和鲁棒性提供了有效的解决方案。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [103] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂任务上表现优异，但生成长且冗余的推理链，浪费资源。需缩短推理链并学习自适应推理。本文综述了相关进展。


<details>
  <summary>Details</summary>
Motivation: LRMs在复杂推理任务中表现突出，但生成冗余推理链导致资源浪费和响应延迟，限制了实际应用。需研究自适应推理方法。

Method: 综述了近期关于简洁和自适应推理的研究进展，包括方法、基准和未来挑战。

Result: 总结了LRMs在高效推理方面的最新成果和潜在问题。

Conclusion: 本文为研究者提供了该领域的全面概述，旨在启发新思路以优化LRMs的使用。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [104] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果关系的深度Q网络（Causal DQ）方法，用于部分可观测的传感器布局优化，以快速检测异常。


<details>
  <summary>Details</summary>
Motivation: 由于资源限制，无法在所有位置部署传感器，现有方法多忽略因果关系或依赖不切实际的干预手段。

Method: 通过将因果信息整合到Q网络训练的每个阶段，实现更快的收敛和更紧的理论误差界。

Result: Causal DQ显著减少了异常检测时间，适用于大规模实时数据流。

Conclusion: 该方法不仅有效，还为其他强化学习问题提供了新思路，扩展了因果机器学习在工程中的应用。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [105] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 提出一种将大语言模型（LLM）整合到形式语义解释函数中的方法，以解决其逻辑不一致性问题，同时保留逻辑的健全性和完备性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自然语言理解和生成方面表现出色，但其输出存在逻辑不一致性问题，如何利用其广泛覆盖的参数化知识进行形式推理是一个挑战。

Method: 将LLM直接整合到形式语义的解释函数中，使用短形式事实性基准数据集进行实验评估。

Result: 实验证明该方法可行，且不同于以往工作，提供了保留逻辑性质的理论框架。

Conclusion: 该方法为神经符号推理提供了理论支持，既能利用LLM的知识，又能保持逻辑的健全性和完备性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [106] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 论文提出了一种基于“什么、为什么、谁”三个维度分类和比较可解释人工智能（XAI）研究的方法，旨在解决任务描述不足、脱离上下文研究和目标用户测试不足等问题。


<details>
  <summary>Details</summary>
Motivation: 当前XAI研究存在大量矛盾且缺乏具体设计建议，主要源于对需要AI辅助的任务理解不足。

Method: 借鉴视觉分析、认知科学和仪表盘设计等领域，提出分类和比较XAI研究的三维框架（what, why, who）。

Result: 研究发现主要问题包括任务描述不充分、脱离上下文的研究以及目标用户测试不足。建议研究应明确报告用户的领域、AI和数据分析专长。

Conclusion: 论文提出了XAI研究的设计和报告指南，以帮助研究者和设计者更好地识别相关研究、填补研究空白并处理XAI设计中的矛盾结果。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [107] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: 论文探讨了AI快速发展带来的风险，并提出了通过技术干预实现协调暂停危险AI活动的方案。


<details>
  <summary>Details</summary>
Motivation: AI系统的快速发展带来了前所未有的风险，如失控、滥用、地缘政治不稳定和权力集中，需要政府采取行动避免最坏结果。

Method: 提出了关键的技术干预措施，以实现对危险AI活动的协调暂停。

Result: 这些干预措施可以限制多种危险AI活动，并为AI治理计划提供技术基础。

Conclusion: 通过技术干预，政府可以建立协调暂停危险AI开发与部署的能力，以应对潜在风险。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [108] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: 通过少量高质量的长链思维（CoT）示例微调基础模型，可以显著提升其推理能力，甚至超越更大的模型。


<details>
  <summary>Details</summary>
Motivation: 探究是否仅通过提示或少量微调就能在基础模型中诱导出长链思维推理能力。

Method: 使用20个来自推理模型的长链思维示例对基础模型进行轻量微调，并探索其他来源的CoT数据。

Result: 微调后的模型表现优于更大的模型，但非推理模型或人工标注的CoT数据效果较差。

Conclusion: 少量高质量的专家CoT数据可以激活基础模型的推理能力，但其潜在特性难以复制。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [109] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: 该论文提出将指令调优的大型语言模型重新解释为基于模型的符号AI系统，其中自然语言作为符号层，并通过模型的内部表示空间实现接地。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络和符号AI的互补优势，探索如何通过自然语言作为符号层提升学习效率和推理可靠性。

Method: 重新解释大型语言模型为符号AI系统，开发新的学习和推理方法，保持与传统范式的结构相似性。

Result: 初步评估表明，该方法在复杂程度不同的公理演绎推理过程中有效提升了学习效率和推理可靠性。

Conclusion: 通过自然语言作为符号层，结合大型语言模型的内部表示空间，为神经符号AI提供了一种新的有效框架。

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [110] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: 论文提出VerifyBench，一个跨领域基准，用于系统评估验证器的性能，揭示了专用验证器和通用LLM在准确性和包容性上的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有验证器在复杂性和灵活性上存在不足，缺乏跨领域的系统评估，限制了RLVR的可靠发展。

Method: 构建包含4,000个专家级问题的基准，涵盖多个学科，设计四维实验框架比较验证器性能。

Result: 专用验证器准确性高但召回率低，通用模型包容性强但精度不稳定，验证器对输入结构敏感且跨领域泛化能力有限。

Conclusion: 研究揭示了当前验证器技术的瓶颈，为RLVR的发展提供了关键见解。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [111] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek发布V3和R1系列模型，因其低成本、高性能和开源优势引发关注。论文回顾了大模型演变，重点介绍了DeepSeek的新算法和工程突破，并分析了其对AI竞争格局的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨DeepSeek模型在AI领域的创新及其对主流LLM的竞争影响。

Method: 回顾大模型演变，介绍DeepSeek的新算法（MLA、MoE、MTP、GRPO）和工程突破（LLM扩展、训练、推理优化）。

Result: DeepSeek模型在性能和成本上具有竞争力，推动了AI技术的发展。

Conclusion: DeepSeek的创新为大型AI模型的技术和工程发展提供了新方向，未来趋势将集中在数据、训练和推理优化。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [112] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: OMDPG算法通过引入最优边际Q函数和广义Q批评器，解决了异构多智能体强化学习中单调改进与部分参数共享的冲突，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 异构多智能体强化学习（MARL）中，单调改进对性能提升至关重要，但部分参数共享（ParPS）与顺序更新方案的结合会导致策略更新基线漂移问题。

Method: 提出OMDPG算法，包括最优边际Q函数（OMQ）替代顺序计算的Q值，广义Q批评器（GQC）优化Q值估计，以及集中式批评器分组执行器（CCGA）架构。

Result: 在SMAC和MAMuJoCo环境中，OMDPG优于多种最先进的MARL基线。

Conclusion: OMDPG成功解决了单调改进与ParPS的冲突，同时实现了高性能合作。

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [113] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 该论文提出了一种基于Promise Theory的语义时空模型，用于低成本、低计算需求下评估数据中的潜在意图性，适用于基础生物体。


<details>
  <summary>Details</summary>
Motivation: 解决科学和技术领域对意图实际意义的忽视，提供一种无需复杂训练或推理的方法来评估意图性。

Method: 利用过程一致性和多尺度异常检测，通过时空一致性将内容分为“意图”和“环境背景”。

Result: 实现了对潜在意图性的基本但实用的解释，适用于低计算能力的代理。

Conclusion: 该方法为评估意图性提供了一种简单且高效的方式，适用于基础生物体或低计算资源场景。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [114] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: 提出了一种通过模型内在的真实性编码校准CoT推理准确性的新方法，显著提升了推理任务的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: CoT推理在LLMs和MLLMs中表现出强大的深度推理能力，但中间步骤的错误累积影响了其可靠性。

Method: 利用特定注意力头激活反映CoT推理步骤的真实性，训练置信度预测器动态选择最优推理路径。

Result: 在数学、符号和常识推理任务中显著优于现有基线方法，适用于单模态和多模态场景。

Conclusion: 为CoT推理提供了一种新颖的可靠性改进路径，具有广泛的应用潜力。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [115] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLM）能否在不同知识图谱（KG）模式间自动翻译SPARQL查询，重点关注DBpedia-Wikidata和DBLP-OpenAlex的转换。


<details>
  <summary>Details</summary>
Motivation: 填补知识图谱互操作性研究中SPARQL到SPARQL翻译的空白。

Method: 使用三种LLM模型（Llama-3-8B、DeepSeek-R1-Distill-Llama-70B、Mistral-Large-Instruct-2407），通过零样本、少样本和思维链变体进行测试，并构建两个基准数据集。

Result: 模型和提示策略的表现差异显著，Wikidata到DBpedia的翻译效果优于反向。

Conclusion: LLM在SPARQL翻译中表现不一，需进一步优化模型和策略。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [116] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 该论文提出了一种新的渐进语义方法，用于假设基础论证（ABA）框架，填补了该领域的空白。


<details>
  <summary>Details</summary>
Motivation: 渐进语义在计算论证中具有重要作用，但尚未应用于假设基础论证（ABA），尽管ABA是一种流行的结构化论证形式。

Method: 通过将双极集基础论证框架作为ABA的抽象，并扩展了现有的定量双极论证框架（QBAF）的渐进语义。

Result: 提出的渐进ABA语义满足平衡性和单调性等理想性质，并通过实验与基于论证的方法进行了比较。

Conclusion: 该研究为ABA框架提供了有效的渐进语义方法，并验证了其可行性和性能。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [117] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: 论文提出BlueGlass框架，整合多种AI安全工具，通过统一基础设施提升AI系统的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力增强和普及，现有安全工具无法单独提供全面保障，需要集成化方法。

Method: 引入BlueGlass框架，支持多样安全工具的集成与组合，并在视觉语言模型上进行三项安全分析。

Result: 展示了框架在分布评估、层级动态分析和可解释概念识别中的实用性。

Conclusion: 该工作为构建更稳健可靠的AI系统提供了基础架构和发现。

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [118] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: 论文探讨了边缘-云系统中应用迁移的自动化编排问题，通过MDP框架比较了AI规划和强化学习方法，并基于状态空间定义提出了新分类。


<details>
  <summary>Details</summary>
Motivation: 研究目的是理解在计算连续环境中能够协调应用迁移的技术，以提供高QoS和成本效益的服务。

Method: 从MDP出发，分析和比较了AI规划和强化学习方法，并将问题建模为汉诺塔问题。

Result: 提出了一种基于状态空间定义的新分类，并对模型进行了分析。

Conclusion: 研究为边缘-云系统中应用迁移的自动化编排提供了技术参考。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [119] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: 论文探讨了利用人类心理学中的元认知提示（如“你可能是错的吗？”）来减少LLM偏见的策略。


<details>
  <summary>Details</summary>
Motivation: 由于LLM仍在发展中，需要通用的去偏见策略，而人类决策中的去偏见方法为此提供了灵感。

Method: 采用元认知提示（如“你可能是错的吗？”）引导LLM反思其回答，揭示潜在偏见和矛盾信息。

Result: 元认知提示能有效让LLM自我识别偏见，并提供更全面的反思信息。

Conclusion: 人类心理学为提示工程提供了新思路，可借鉴其长期有效的决策改进方法。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [120] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的在线飞行资源分配方案（FRSICL），用于无人机辅助的野火监测系统，通过联合优化飞行控制和数据收集调度，实时最小化信息年龄（AoI）。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习（DRL）方法在采样效率、仿真与现实的差距以及复杂训练等方面存在不足，不适用于野火监测等时间关键应用。

Method: FRSICL利用自然语言任务描述和环境反馈，动态生成数据收集计划和飞行速度控制，无需大量重新训练。

Result: 仿真结果表明，FRSICL在最小化平均AoI方面优于PPO和最近邻基线方法。

Conclusion: FRSICL为无人机野火监测提供了一种高效、动态的决策方案，克服了DRL的局限性。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [121] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 该论文提出“适应性”作为评估多智能体强化学习（MARL）在动态环境中可靠性的统一框架，包括学习适应性、策略适应性和场景驱动适应性三个维度。


<details>
  <summary>Details</summary>
Motivation: MARL在现实多智能体系统（MAS）中应用受限，主要由于环境的复杂性和动态性，需要算法在持续变化的系统配置和操作需求中保持有效性。

Method: 引入适应性概念，并提出一个包含三个关键维度的结构化框架：学习适应性、策略适应性和场景驱动适应性。

Result: 通过适应性视角，支持更原则化的MARL性能评估，超越狭窄定义的基准测试。

Conclusion: 该框架有助于开发更适合动态现实多智能体系统的算法。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [122] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: 论文介绍了瑞士食品知识图谱（SwissFKG），整合了食谱、食材、替代品、营养数据、饮食限制和过敏原信息，并利用LLM增强图谱内容，为个性化营养评估工具奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有自动饮食评估系统常忽略非视觉因素（如食材替代对营养的影响）和个性化需求（如过敏、文化习惯），瑞士缺乏整合相关信息的中心化资源。

Method: 构建SwissFKG图谱，采用LLM增强图谱内容，并评估四种LLM在食品知识增强中的表现。通过Graph-RAG应用展示图谱如何支持LLM回答用户营养查询。

Result: LLM能有效丰富图谱营养信息，SwissFKG提供食材级信息和营养指南支持。Graph-RAG应用验证了图谱在回答用户查询中的实用性。

Conclusion: SwissFKG为结合视觉、上下文和文化维度的下一代饮食评估工具奠定了基础。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [123] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: 论文通过实验比较了决策变换器（DT）与基于MLP的过滤行为克隆（FBC），发现FBC在稀疏奖励环境中表现更优，质疑DT的适用性。


<details>
  <summary>Details</summary>
Motivation: 探讨决策变换器（DT）在离线强化学习中的实际优势，尤其是在稀疏奖励环境中的表现。

Method: 在Robomimic和D4RL基准上实验，比较DT与FBC的性能，FBC通过过滤低质量轨迹后进行行为克隆。

Result: FBC在稀疏奖励环境中表现优于DT，且更高效、数据需求更少。

Conclusion: 质疑DT的适用性，认为其在稀疏和密集奖励环境中均非最优选择。

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [124] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: 该论文综述了基于LLM的表格代理，旨在通过整合预处理、推理和领域适应自动化表格任务，分析了五大核心能力，并揭示了开源模型在真实场景中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现实世界的表格任务常涉及噪声、结构异质性和语义复杂性，而现有研究多针对干净的学术数据集，因此需要探索更实用的解决方案。

Method: 定义了五大核心能力（C1-C5）来分析和比较现有方法，并详细研究了Text-to-SQL代理的性能。

Result: 发现开源模型在真实场景中性能不足，尤其在Text-to-SQL任务中。

Conclusion: 提出了改进LLM表格代理在实用性中的鲁棒性、泛化性和效率的具体建议。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [125] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: 本文通过实例空间分析（ISA）方法，结合DIMACS数据集，识别了23个相关实例特征，并利用降维和机器学习方法，揭示了实例结构对元启发式算法性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决CVRP研究中实例特征与元启发式算法性能之间复杂关系的理解问题。

Method: 采用实例空间分析（ISA）方法，结合PREILM、SIFTED和PILOT阶段，利用降维和机器学习技术，生成二维实例空间投影。

Result: 识别了23个相关实例特征，并提供了投影矩阵，便于新实例的分析。

Conclusion: ISA为CVRP领域提供了一种新的实例分析方法，有助于理解实例结构对算法性能的影响。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [126] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 论文提出了一种结合BERT情感分析和XGBoost特征选择的新模型，用于预测远程学习中的学生辍学风险，准确率达84%。


<details>
  <summary>Details</summary>
Motivation: 远程学习中的学生辍学问题严重，早期检测对干预和提升学生坚持性至关重要。

Method: 结合BERT对学生的评论进行情感分析，并通过XGBoost整合社会人口统计和行为数据，优化特征选择。

Result: 模型在未见过的新学年数据上达到84%的准确率，优于基线模型的82%，并在精确度和F1分数上表现更优。

Conclusion: 该模型可作为开发个性化策略的重要工具，有效降低辍学率并鼓励学生坚持学习。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [127] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: 论文提出了一种在数据稀缺领域（如计算化学、医学成像）中高效获取先验知识的架构，包括神经记忆和超网络设计，并在3D场景生成和分子预测中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决在数据稀缺领域无法训练大型预训练模型的问题，提出高效获取先验知识的方法。

Method: 使用神经记忆适应非平稳分布，设计超网络结合MAML获取更通用的先验，并应用于3D场景生成和分子预测。

Result: 在少量样本下实现高效先验获取，提升3D生成和分子预测性能。

Conclusion: 提出的架构在数据稀缺领域具有高效性和通用性，为相关应用提供了新思路。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [128] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{\text{Eco}}$是一种基于LLM的自动化科学合成系统，支持递归、深度和广度可控的探索，显著提升文献检索的多样性和细致度。


<details>
  <summary>Details</summary>
Motivation: 解决传统检索增强生成管道在科学文献合成中的局限性，提供用户可控的合成、透明推理和参数驱动配置。

Method: 采用递归、深度和广度可控的探索方法，结合参数驱动配置，实现高吞吐量的领域证据整合。

Result: 在49个生态研究问题中，实现了最高21倍的源整合提升和14.9倍的每千字源整合增加。高参数设置可达到专家级分析深度和上下文多样性。

Conclusion: DeepResearch$^{\text{Eco}}$在科学文献合成中表现出色，提供了高效、可控且透明的解决方案。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [129] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: 本文提出了一种名为Recurrent Expansion (RE)的新学习范式，超越了传统的机器学习和深度学习，通过分析模型自身的行为演化实现自我迭代改进。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习专注于静态数据表示，而RE旨在通过模型行为的动态分析实现自我改进，为智能模型提供更强大的适应性和自省能力。

Method: RE通过多次映射数据到相同的深度架构中，并结合内部表示和性能信号（如损失）进行分析。扩展版本包括Multiverse RE (MVRE)和Heterogeneous MVRE (HMVRE)，后者进一步引入不同架构的模型。

Result: RE框架实现了行为感知和自我演化的系统，为可扩展、自省和自适应的人工智能奠定了基础。

Conclusion: RE标志着深度学习从纯表示学习向行为感知和自我演化系统的转变，为未来智能模型的发展提供了新方向。

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [130] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: 本文提出了一种基于可解释人工智能（XAI）的高效三重模块冗余（TMR）方法，用于增强深度神经网络（DNN）在比特翻转故障下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，DNN的可靠性至关重要。TMR虽能提升可靠性，但其开销较大，因此需要一种高效的选择标准来优化其应用。

Method: 利用梯度可解释性方法（如LRP）计算DNN参数的重要性分数，并选择关键权重应用TMR。

Result: 在AlexNet模型上，该方法在比特错误率为10-4时实现了超过60%的可靠性提升，且开销与现有方法相当。

Conclusion: 基于XAI的TMR方法能有效提升DNN的可靠性，同时保持较低的开销。

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [131] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: 论文提出了一种结合机器学习和人机交互的决策支持系统，帮助印度卡纳塔克邦的农民应对市场和气候波动，并通过语音界面解决识字障碍。


<details>
  <summary>Details</summary>
Motivation: 农民面临市场和气候波动，同时因识字障碍无法利用数字技术。

Method: 结合随机森林分类器和LSTM网络，分别预测作物适宜性和市场价格，并通过语音界面提供建议。

Result: 随机森林模型准确率达98.5%，LSTM模型预测误差低。

Conclusion: 该系统为边缘化农民提供了经济优化的数据驱动建议，增强了其财务韧性。

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [132] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: LoRA是一种广泛用于微调大语言模型的技术，通过引入少量可训练的低秩权重矩阵减少参数更新量，但在某些模型架构和训练设置中速度提升不一致。本文分析其性能并提出更高效的微调方法。


<details>
  <summary>Details</summary>
Motivation: LoRA在不同模型架构和训练设置中速度提升不一致，作者希望探究其性能限制因素并提出改进方法。

Method: 对LoRA性能进行全面分析，提出更高效的微调方法，并通过实验评估与LoRA的比较。

Result: 提出的方法在性能上优于或与LoRA相当，同时提供更一致的训练速度提升。

Conclusion: 本文为资源受限下优化大语言模型微调提供了实用指南和见解。

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [133] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息神经网络（PINN）的框架，用于模拟二维平流-扩散方程控制的污染物扩散，通过嵌入物理定律和拟合噪声合成数据，实现了物理一致的预测。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理大规模动态海洋域污染物传输时面临复杂性和规模挑战，需要一种更高效、灵活的方法。

Method: 采用PINN框架，结合物理定律和噪声合成数据训练神经网络，使用混合损失函数（包括PDE残差、边界/初始条件一致性和加权数据拟合项）。

Result: 模型能够处理非线性动态和边界/初始条件约束，利用Julia语言生态系统实现高性能仿真。

Conclusion: PINN框架为传统求解器提供了一种可扩展且灵活的替代方案。

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [134] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer神经网络的洗钱检测新方法，结合对比学习和双阈值控制假阳性率。


<details>
  <summary>Details</summary>
Motivation: 解决洗钱检测问题，减少对领域专家标注的依赖，并提高检测准确性。

Method: 1. 通过对比学习学习时间序列表示；2. 利用表示生成洗钱评分；3. 采用双阈值和BH程序控制假阳性率。

Result: 实验表明，Transformer能有效捕捉洗钱模式，优于基于规则或LSTM的方法。

Conclusion: 新方法在检测洗钱和非洗钱行为上表现优异，假阳性率可控。

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [135] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: CompactifAI压缩方法应用于Llama 3.1 8B模型，显著减少计算资源并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 评估压缩方法CompactifAI在大语言模型Llama 3.1 8B上的性能，以提高效率和降低成本。

Method: 使用Codecarbon和Ragas框架分别评估压缩模型的能耗和准确性，并与完整模型对比。

Result: 压缩模型显著减少计算资源，同时保持准确性。

Conclusion: CompactifAI使模型更高效、可扩展且经济实惠。

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [136] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: 论文提出了一种名为$\mathtt{wd1}$的新方法，通过加权似然优化扩散大语言模型（dLLMs）的推理能力，避免了传统强化学习中的计算开销和偏差问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）的推理能力提升是一个开放性问题，传统强化学习方法因似然函数难以处理而引入计算开销和潜在偏差。

Method: 提出$\mathtt{wd1}$方法，将目标重新表述为加权似然，仅需对当前参数化策略似然进行一次近似。

Result: 在广泛使用的推理基准测试中，$\mathtt{wd1}$无需监督微调（SFT）或监督数据，性能优于现有强化学习方法，准确率提升高达16%，同时减少训练时间和梯度步数。

Conclusion: $\mathtt{wd1}$是一种更高效、更有效的方法，适用于扩散大语言模型的强化学习推理任务。

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [137] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的迁移学习模型（TAT），利用阿尔茨海默病（AD）数据增强路易体病（LBD）的诊断，解决了数据稀缺和领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 路易体病（LBD）是一种常见但研究不足的痴呆症，诊断面临数据稀缺和领域偏移的挑战，而AD数据丰富，可用于知识迁移。

Method: 提出Transferability Aware Transformer（TAT），利用结构MRI的连通性数据，通过注意力机制自适应分配权重，减少领域偏移。

Result: 实验证明TAT能有效提升LBD诊断准确性，为罕见疾病的领域自适应诊断提供了新框架。

Conclusion: TAT是首个在数据稀缺和领域偏移条件下从AD到LBD的领域适应研究，为罕见疾病诊断提供了可行方案。

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [138] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: 提出了一种名为WRCor的无训练估计代理，用于加速神经架构搜索（NAS），并通过实验验证其高效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本NAS方法的有效性、稳定性和通用性不足，需要更高效的训练免费代理来加速架构估计。

Method: 利用不同输入样本响应之间的相关系数矩阵计算代理分数，衡量架构的表达性和泛化性。

Result: WRCor及其投票代理在代理评估中表现更高效，NAS算法在不同搜索空间中优于现有方法，ImageNet-1k上仅用4 GPU小时即达到22.1%测试错误率。

Conclusion: WRCor是一种高效且通用的训练免费代理，显著提升了零样本NAS的性能和效率。

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [139] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Main category: cs.LG

TL;DR: FedRAS是一个通信高效的联邦推荐系统框架，通过动作共享策略减少通信开销，同时避免模型性能下降。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统（FedRecs）存在通信开销高和训练效率低的问题，现有压缩方法会导致模型性能下降。

Method: 采用动作共享策略，将梯度聚类为有限的动作进行通信，而非直接压缩嵌入矩阵，并结合自适应聚类机制适应异构环境。

Result: 实验表明，FedRAS能将通信负载减少96.88%，且不影响推荐性能。

Conclusion: FedRAS有效解决了通信开销和性能平衡问题，适用于异构环境。

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [140] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Main category: cs.LG

TL;DR: FLLL3M是一个隐私保护的联邦学习框架，结合大语言模型用于移动建模，实现了高效的下一个位置预测。


<details>
  <summary>Details</summary>
Motivation: 解决用户数据隐私问题，同时利用大语言模型提升预测准确性。

Method: 通过保留用户数据本地化，并采用高效的外积机制结合大语言模型。

Result: 在多个数据集上达到SOT结果（如Gowalla Acc@1: 12.55），参数减少45.6%，内存使用降低52.7%。

Conclusion: FLLL3M在隐私保护和资源效率方面表现优异，适用于移动建模任务。

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [141] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: 论文提出了一种动态自适应扇出优化采样器（DAFOS），通过动态调整扇出和优先处理重要节点，显著提升了GNN的训练速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决GNN中均匀邻居采样和静态扇出设置导致的扩展性和效率限制。

Method: 动态调整扇出，基于节点度评分优先处理重要节点，并集成早停机制。

Result: 在多个数据集上实现显著加速（最高12.6倍）和F1分数提升（最高3.11%）。

Conclusion: DAFOS是高效且可扩展的大规模GNN训练解决方案。

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [142] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: 本文提出AMLAS-RL框架，通过迭代过程为基于强化学习的系统生成安全保证论证，解决了现有方法在RL生命周期中系统性安全保证的不足。


<details>
  <summary>Details</summary>
Motivation: 机器学习在物理信息系统中广泛应用，但强化学习的安全性问题缺乏系统性保证方法，现有方法如AMLAS不适用于RL。

Method: 通过调整AMLAS方法，提出AMLAS-RL框架，以迭代方式为RL系统生成安全保证论证。

Result: 以轮式车辆避障任务为例，验证了AMLAS-RL的有效性。

Conclusion: AMLAS-RL为强化学习在安全关键应用中的系统性安全保证提供了可行方案。

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [143] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Main category: cs.LG

TL;DR: 比较时间序列基础模型（TSFMs）与传统方法在共形预测中的表现，发现TSFMs在数据有限时表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在时间序列共形预测中的潜力，尤其是在数据受限情况下。

Method: 在共形预测框架下，对比TSFMs与统计模型和梯度提升方法的性能。

Result: TSFMs在小数据量下提供更可靠的预测区间，且校准过程更稳定。

Conclusion: 基础模型在数据受限的时间序列应用中能显著提升共形预测的可靠性。

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [144] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Main category: cs.LG

TL;DR: 论文提出了一种新的业务导向评估指标e-Profits，用于客户关系管理中的流失预测模型，优于传统指标如AUC和F1-score。


<details>
  <summary>Details</summary>
Motivation: 传统指标如AUC和F1-score无法反映财务结果，可能误导战略决策。

Method: 使用Kaplan-Meier生存分析估计个性化保留率，支持细粒度的客户评估。

Result: 在电信数据集上测试六种分类器，e-Profits改变了模型排名，揭示了传统指标忽视的财务优势。

Conclusion: e-Profits是一种易于理解的工具，适用于业务场景，支持利润驱动的决策。

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [145] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Main category: cs.LG

TL;DR: 本文提出了图神经网络（GNN）在求解偏微分方程（PDE）时所需消息传递迭代次数的严格下界，显著减少了超参数调优的需求。


<details>
  <summary>Details</summary>
Motivation: 通过将问题的物理特性与GNN的消息传递需求联系起来，减少对超参数调优的依赖，提高GNN在求解PDE时的效率和准确性。

Method: 针对三类基本PDE（双曲型、抛物型和椭圆型），推导了消息传递迭代次数的下界，研究了方程物理常数、时空离散化与GNN消息传递机制的关系。

Result: 当迭代次数低于下界时，信息无法有效传播，导致解的质量差；满足下界时，GNN能准确捕捉现象学，得到高精度解。

Conclusion: 通过四个方程实例验证了下界的严格性，表明该方法能有效指导GNN在PDE求解中的应用。

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [146] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本文研究了数据偏见对算法歧视的影响，提出了数据偏见配置文件（DBP）来检测和缓解偏见，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 数据偏见是算法歧视的主要驱动因素，但研究不足，阻碍了检测和缓解偏见的计算方法发展。

Method: 研究三种常见数据偏见的单独和联合影响，开发DBP机制检测偏见，并通过案例研究验证。

Result: 发现弱势群体在训练集中的代表性不足对歧视的影响较小，而代理和标签偏见的组合更为关键；DBP能有效预测歧视风险。

Conclusion: DBP为系统性记录偏见信号提供了初步框架，连接了算法公平研究和反歧视政策。

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [147] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Main category: cs.LG

TL;DR: 小型模型结合压缩文献库和结构化推理框架，在ICLR 2025的自我排名前30%提交中表现优于通用语言模型，高置信度预测的接受率超过90%。


<details>
  <summary>Details</summary>
Motivation: AI研究快速发展，但缺乏可扩展的咨询系统为假设生成和实验设计提供高质量反馈。

Method: 探索模型大小、上下文长度、置信度估计和结构化推理等关键因素，开发小型模型结合压缩文献库和结构化推理框架。

Result: 小型模型在ICLR 2025自我排名前30%提交中表现优于通用语言模型，高置信度预测接受率超过90%。

Conclusion: 该系统显著提升了假设生成和实验设计的质量与效率，代码已开源。

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [148] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 提出了一种基于学习的旅行需求建模框架，结合家庭协调的日常活动模式，具有生成性、数据驱动、可扩展和跨区域适用性。


<details>
  <summary>Details</summary>
Motivation: 传统活动基础模型（ABMs）依赖简化规则，开发成本高且难以跨区域适应，需要更高效、灵活的解决方案。

Method: 整合人口合成、协调活动生成、位置分配和大规模微观交通模拟，形成统一系统。

Result: 在洛杉矶的10百万人口测试中，模型准确复制真实出行模式，性能接近传统ABMs，但成本更低、可扩展性更强。

Conclusion: 该框架为旅行需求建模提供了高效、可扩展的替代方案，显著降低开发成本并提升适应性。

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [149] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Main category: cs.LG

TL;DR: 提出了一种基于CLIP模型的语义通信框架，无需训练即可提取数据语义，并通过强化学习优化无线网络中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于神经网络的语义通信需要联合训练的问题，并优化无线噪声环境下的语义通信性能。

Method: 使用CLIP模型提取语义信息，结合PPO强化学习算法优化模型架构和频谱资源分配。

Result: 仿真结果显示，该方法收敛速度提升40%，累积奖励提高4倍。

Conclusion: 基于CLIP的语义通信框架在无线网络中表现优异，显著提升了性能。

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [150] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Main category: cs.LG

TL;DR: VIPEEGNet是一种基于卷积神经网络的模型，用于通过EEG及时识别有害脑活动，性能接近人类专家，并在外部验证中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在EEG分析中存在评分者间变异性、资源限制和泛化性差的问题，限制了其在脑疾病诊断中的应用。

Method: 开发并验证了VIPEEGNet模型，使用来自两个独立数据集的EEG数据（1950名患者和1532名患者），模型通过多分类和二元分类进行评估。

Result: VIPEEGNet在二元分类中AUROC高达0.972，多分类中敏感性和精确度接近人类专家，外部验证表现优异，参数效率高。

Conclusion: VIPEEGNet是一种高效且泛化性强的EEG分析工具，有望推动脑疾病诊断的临床应用。

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [151] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Main category: cs.LG

TL;DR: 提出了一种名为ODIA的新方法，通过在线用户交互数据加速LLM的函数调用，显著降低延迟并保持准确性。


<details>
  <summary>Details</summary>
Motivation: LLM函数调用的高延迟影响用户体验，需一种高效解决方案。

Method: 利用生产流量中的“简单查询”，通过知识蒸馏将大模型知识迁移到小模型。

Result: 响应延迟降低45%（预期）和78%（中位数），小模型处理60%流量且精度损失可忽略。

Conclusion: ODIA是一种实用且自动化程度高的生产环境解决方案。

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [152] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Göran Falkman,Jonas Andersson,Anders Sjögren*

Main category: cs.LG

TL;DR: 论文提出了一种基于哈密顿蒙特卡洛（HMC）采样的深度神经网络（DNN）概率最后一层方法（LL-HMC），以减少计算成本，并在实际视频数据集上验证其性能。


<details>
  <summary>Details</summary>
Motivation: HMC虽为不确定性估计的金标准，但其计算成本限制了其在大规模数据和大型DNN中的应用。LL-HMC通过仅对DNN最后一层进行HMC采样，降低了计算需求。

Method: LL-HMC将HMC采样限制在DNN的最后一层，并与五种概率深度学习方法（LL-PDL）在三个真实视频数据集上进行比较，评估分类性能、校准和分布外（OOD）检测。

Result: LL-HMC在分类和OOD检测上表现竞争性，但额外的采样参数未提升分类性能，仅对OOD检测有改善。多链或不同起始位置未带来一致提升。

Conclusion: LL-HMC是一种计算高效的HMC采样方法，适用于资源有限的数据密集型场景，尤其在OOD检测方面表现突出。

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [153] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 论文提出了一种名为Fair-FLIP的后处理方法，旨在减少深度伪造检测中的偏见，同时保持检测性能。实验表明，该方法能显著提升公平性指标，且对准确率影响极小。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测方法虽然性能高，但存在对种族和性别等人口属性的偏见，威胁公共信任。本文旨在解决公平性问题。

Method: 提出Fair-FLIP方法，通过重新加权模型最后一层的输入，减少子群差异，优先处理低变异性输入。

Result: Fair-FLIP将公平性指标提升高达30%，同时基线准确率仅下降0.25%。

Conclusion: Fair-FLIP是一种有效的公平性优化方法，适用于深度伪造检测，且对性能影响极小。

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [154] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 论文研究了无需Lipschitz平滑假设的随机洗牌梯度方法，提出了一种步长策略，使其在更弱条件下收敛，并在数值实验中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有的随机洗牌梯度方法收敛分析大多依赖Lipschitz平滑条件，但许多机器学习模型不满足此条件。论文旨在填补这一理论空白。

Method: 提出了一种新的步长策略，分析了随机洗牌梯度方法在非凸、强凸和非强凸情况下的收敛性，支持随机重排和任意洗牌方案。

Result: 算法在更弱的假设下收敛，且收敛速率与当前已知最优速率一致，数值实验验证了其实际性能。

Conclusion: 论文扩展了随机洗牌梯度方法的适用范围，为不满足Lipschitz平滑条件的模型提供了理论支持。

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [155] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Díaz,Sam Buchanan,Jeremias Sulam*

Main category: cs.LG

TL;DR: 论文提出了一种基于反向离散化的扩散模型（ProxDM），通过使用近端映射替代分数，实现了理论和实践上的优势。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在高维数据生成中表现出色，但传统方法依赖于分数的估计，存在局限性。

Method: 采用反向离散化SDEs，利用近端映射学习对数密度的近端算子，开发ProxDM。

Result: 理论证明ProxDM在KL散度下仅需较少的步骤即可生成高精度分布，实验显示其收敛速度显著快于传统方法。

Conclusion: ProxDM在理论和实践中均优于传统分数匹配方法，为扩散模型提供了新的优化方向。

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [156] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的跨平台广告推荐方法，通过多维建模提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有广告推荐方法在跨平台场景下准确性不足，需捕捉用户兴趣迁移的潜在路径。

Method: 利用用户行为数据、广告内容和平台特征进行多维建模，通过GNN捕捉兴趣迁移。

Result: 实验结果显示，Platform B的AUC值达到0.937，表现最佳；Platform A和C因广告标签分布不均，精度和召回率略有下降。

Conclusion: 通过调整超参数，模型在异构数据中的适应性和鲁棒性得到进一步提升。

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [157] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Main category: cs.LG

TL;DR: 论文分析了Classifier-Free Guidance (CFG)在离散扩散模型中的作用，发现早期高指导会降低生成质量，而后期指导更有效。提出了一种改进方法，通过简单代码调整提升样本质量。


<details>
  <summary>Details</summary>
Motivation: 研究CFG在离散扩散模型中的表现，特别是指导时间表的作用，以解决现有实现中的不平衡过渡问题。

Method: 理论分析CFG在离散扩散中的行为，提出一种新的指导机制，通过平滑数据分布与初始分布之间的传输来改进质量。

Result: 实验证明，改进方法在ImageNet和QM9数据集上显著提升了样本质量。

Conclusion: 论文揭示了CFG在离散扩散中的局限性，并提出了一种简单有效的改进方法。

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [158] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Main category: cs.LG

TL;DR: ToxBench是一个大规模AB-FEP数据集，用于机器学习开发，专注于人类雌激素受体α（ERα），包含8,770个复合物结构，并验证了其准确性。提出的DualBind模型在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在蛋白质-配体结合亲和力预测中数据不足的问题，同时弥补物理方法计算成本高的缺陷。

Method: 引入ToxBench数据集，并使用双损失框架的DualBind模型进行基准测试。

Result: DualBind模型表现出色，机器学习能以较低计算成本近似AB-FEP。

Conclusion: ToxBench和DualBind为药物发现提供了高效工具，展示了机器学习在亲和力预测中的潜力。

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [159] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Main category: cs.LG

TL;DR: PINNs通过物理方程直接训练神经网络，成功模拟了湍流，无需传统计算网格或训练数据。


<details>
  <summary>Details</summary>
Motivation: 湍流模拟计算资源需求高，传统方法在高流速下难以实现。

Method: 结合自适应网络架构、因果训练和高级优化方法，直接从流体方程学习解。

Result: PINNs准确再现了湍流的关键统计特性，如能谱、动能、涡量和雷诺应力。

Conclusion: 神经网络方程求解器可处理复杂混沌系统，为超越传统计算限制的湍流建模开辟新途径。

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [160] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Main category: cs.LG

TL;DR: SGNNs结合机械模拟与深度学习，提升科学建模的预测和推断能力，同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决机械模型在复杂现实中失效，而机器学习模型依赖大数据且缺乏可解释性的问题。

Method: 使用机械模拟生成合成数据训练神经网络（SGNNs），覆盖多种模型结构和参数。

Result: SGNNs在预测和推断任务中表现优异，如COVID-19预测、化学反应产率预测等。

Conclusion: SGNNs通过模拟数据实现灵活、可解释的科学建模，为缺乏真实数据的场景提供新范式。

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [161] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 论文提出了一种系统性框架，通过表示引导改进扩散模型，引入两种新策略提升表示对齐，实验证明在图像、蛋白质序列和分子生成任务中表现优越且训练加速。


<details>
  <summary>Details</summary>
Motivation: 通过将扩散模型的内部表示与预训练模型对齐，提升生成质量。

Method: 提出两种策略：1）将样本与目标表示配对学习联合模型；2）设计平衡表示学习和数据生成的最优训练课程。

Result: 在ImageNet 256×256基准测试中，训练速度比SiT-XL快23.3倍，比REPA快4倍。

Conclusion: 表示引导显著提升扩散模型的性能和训练效率。

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [162] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Main category: cs.LG

TL;DR: 论文揭示了模型排行榜作为大规模传播中毒模型的潜在渠道，提出了TrojanClimb框架，展示了其在多种模态中的有效性，并呼吁改进排行榜评估机制。


<details>
  <summary>Details</summary>
Motivation: 探索中毒模型通过模型排行榜大规模传播的机制，填补研究空白。

Method: 提出TrojanClimb框架，在保持排行榜性能的同时注入恶意行为，并在四种模态中验证其有效性。

Result: 攻击者能在排行榜上获得高排名，同时嵌入有害功能（如后门或偏见注入）。

Conclusion: 揭示了机器学习生态系统的重大漏洞，呼吁改进排行榜评估机制，并警示采用未经验证模型的风险。

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [163] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Main category: cs.LG

TL;DR: 提出了一种自监督深度学习模型，从多模态信号（EEG、ECG和呼吸信号）中提取有意义模式，用于预测心血管疾病（CVD）风险。


<details>
  <summary>Details</summary>
Motivation: 通过多模态信号分析，提升CVD风险的个性化评估能力。

Method: 训练自监督模型，利用4,398名参与者的数据生成投影分数，并在1,093名独立参与者中进行外部验证。

Result: 投影分数揭示了临床有意义的模式，结合Framingham风险评分显著提升预测性能（AUC 0.607-0.965）。

Conclusion: 该框架可直接从PSG数据生成个性化CVD风险评分，有望用于临床实践。

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [164] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Main category: cs.LG

TL;DR: 利用人类注视建模改进RLHF，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: RLHF计算成本高，希望通过人类注视信号提升效率。

Method: 提出两种方法：注视感知奖励模型和基于注视的稀疏奖励分布。

Result: 实验显示注视信息能加速收敛并保持性能，降低计算成本。

Conclusion: 人类注视信号是优化RLHF效率的潜在方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [165] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: 论文分析了当前大型语言模型（LLM）推理系统评估方法的常见缺陷，提出了识别和避免这些缺陷的框架，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理系统的评估方法存在缺陷，掩盖了真实性能特征，阻碍科学进步。

Method: 通过系统分析近期系统，识别了三个关键维度（基线公平性、评估设置和指标设计）中的常见反模式，并提出了避免这些反模式的框架。

Result: 揭示了评估反模式如何导致误导性结论，并通过案例研究验证了框架的实用性。

Conclusion: 建立了一个严谨的评估方法基础，促进LLM推理系统的真实进步。

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [166] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出了一种分布式预训练方法，通过训练小型结构化子网络减少内存需求，避免节点间激活通信，性能不降。


<details>
  <summary>Details</summary>
Motivation: 解决大规模模型预训练中内存需求高和节点间通信成本大的问题。

Method: 训练小型结构化子网络，避免节点间激活通信，采用随机块丢弃和宽度子网络构建策略。

Result: 随机块丢弃策略表现更优，内存使用减少20-40%，性能无损失。

Conclusion: 该方法有效减少内存需求，性能稳定，具有潜力。

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [167] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: 论文提出了一种递归MDN（R-MDN）层，用于在持续学习中消除混杂变量的影响，提升模型在不同群体中的公平性。


<details>
  <summary>Details</summary>
Motivation: 混杂变量会导致虚假相关性和预测偏差，传统方法在持续学习中难以有效处理这一问题。

Method: 引入R-MDN层，通过递归最小二乘法动态调整特征表示，使其对混杂变量不变。

Result: 实验表明，R-MDN在静态学习和持续学习中均能减少混杂变量的影响，提升预测公平性。

Conclusion: R-MDN是一种有效的持续学习方法，能够动态适应数据分布变化，减少混杂变量的负面影响。

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [168] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: 提出了一种名为“行为探索”的方法，通过训练长上下文生成模型，使自主代理能够快速在线探索和适应环境，模仿专家的行为并实现目标导向的探索。


<details>
  <summary>Details</summary>
Motivation: 人类能够快速在线探索和适应环境，而现有算法依赖随机探索和缓慢的梯度更新。本文旨在赋予自主代理类似人类的能力。

Method: 利用专家演示数据集，训练长上下文生成模型，预测专家行为，并结合上下文实现目标导向的探索。

Result: 在模拟和真实机器人任务中验证了方法的有效性，展示了其快速适应和探索能力。

Conclusion: 行为探索方法能够有效实现自主代理的快速在线适应和专家级探索。

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [169] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Main category: cs.LG

TL;DR: 提出了一种改进高斯概率生成模型（GPGMs）效率的框架，通过分析数据快速收敛到高斯分布的特性，减少冗余计算。


<details>
  <summary>Details</summary>
Motivation: GPGMs在生成数据时计算成本高，限制了实际部署。

Method: 识别数据快速收敛到高斯分布的特征步骤，用闭式高斯近似替代剩余轨迹。

Result: 在多种数据模态中显著提高了样本质量和计算效率。

Conclusion: 该方法在保持学习动态完整性的同时，提升了生成效率。

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [170] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Main category: cs.LG

TL;DR: 论文研究了在连续状态和动作动态系统中模仿专家演示的问题，提出了两种最小干预方法（动作分块和噪声注入）来缓解复合误差问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在物理环境（如自动驾驶和机器人学习）中比离散环境更复杂，主要由于复合误差问题。现有研究表明，仅从专家控制的轨迹中学习时，指数级复合误差不可避免。

Method: 提出了两种干预方法：对于开环稳定系统，采用动作分块（预测并执行动作序列）；对于可能不稳定的系统，采用噪声注入（在专家演示中添加噪声）。

Result: 这些干预方法有效缓解了复合误差，且与控制理论和强化学习的现有方法有所不同。

Conclusion: 研究结合了控制理论和强化学习的工具，揭示了单独考虑任一领域时未自然出现的新问题。

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [171] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amaëlle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Main category: cs.LG

TL;DR: 该论文提出了一种结合排队论和注意力模型的新方法QT-SimAM，用于高精度预测航班延误，并在不同网络中表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 航班延误对航空业造成重大财务和运营影响，需要精确且通用的预测模型来改善乘客体验和减少收入损失。

Method: 提出QT-SimAM模型，结合排队论和简单注意力机制，使用美国交通统计局数据进行验证。

Result: QT-SimAM模型在美国数据集上准确率为0.927，F1分数为0.932；在欧洲数据集上准确率为0.826，F1分数为0.791。

Conclusion: QT-SimAM是一种有效的端到端方法，能够高精度预测航班延误，有助于减少乘客焦虑和优化运营决策。

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [172] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Main category: cs.LG

TL;DR: 论文提出了一种基于多步信用分配的广义投影贝尔曼误差（GPBE）方法，用于深度强化学习中的快速稳定离策略学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为半梯度TD方法，虽简单高效但易发散；GTD方法虽有强收敛性，但很少用于深度RL。本文旨在扩展GPBE以支持多步信用分配。

Method: 扩展GPBE目标以支持基于λ-return的多步信用分配，并推导了三种梯度优化方法，包括前向视图和后向视图。

Result: 在MuJoCo和MinAtar环境中，所提算法优于PPO和StreamQ。

Conclusion: 多步GPBE方法在深度RL中实现了快速稳定的离策略学习，性能优于现有方法。

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [173] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: 本文揭示了直接偏好优化（DPO）是连接损失函数理论和随机选择理论的特定形式，支持广泛的应用场景和扩展。


<details>
  <summary>Details</summary>
Motivation: 理解DPO的通用原理对应对多样化的应用场景、当前的研究热潮以及避免潜在陷阱至关重要。

Method: 通过建立Savage损失函数与随机选择理论之间的连接，支持包括弃权、非凸目标等特性。

Result: DPO框架能够免费扩展至包括边际和长度校正等场景，覆盖了现有DPO变体的广泛区域。

Conclusion: 从通用视角理解DPO有助于探索其应用潜力并规避潜在问题。

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [174] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Main category: cs.LG

TL;DR: 论文提出了一种基于隐式神经信号表示的连续时间向量信号低秩分解方法，统一了PCA和ICA问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统PCA和ICA方法在连续时间信号、点云和不规则采样信号中的局限性。

Method: 通过对比函数项在神经网络损失中强制源信号的统计特性（去相关、独立性），实现连续域的低秩分解。

Result: 该方法能够应用于传统技术无法处理的点云和不规则采样信号。

Conclusion: 提出的框架为连续时间信号的低秩分解提供了灵活且通用的解决方案。

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [175] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Main category: cs.LG

TL;DR: 论文提出DejaVu攻击，利用网络延迟导致的多模态传感器时间错位，显著降低自动驾驶感知任务性能，并提出防御方法AION。


<details>
  <summary>Details</summary>
Motivation: 多模态融合（MMF）在自动驾驶感知中至关重要，但其严格的时间同步要求使其易受攻击。

Method: 提出DejaVu攻击，分析传感器对任务的敏感性差异；设计防御方法AION，通过跨模态时间一致性监测攻击。

Result: DejaVu攻击显著降低感知性能（如检测mAP下降88.5%）；AION防御在AUROC上表现优异（0.92-0.98）。

Conclusion: 时间错位攻击对MMF构成严重威胁，AION是一种高效且通用的防御方案。

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [176] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Main category: cs.LG

TL;DR: 论文提出了一种基于集合到集合（S2S）的食材推荐框架S2SRec2，用于解决传统方法在预测多个缺失食材时的不足。


<details>
  <summary>Details</summary>
Motivation: 在电商购物中，顾客常因缺乏专业知识而无法完成完整食谱的食材搭配，传统方法仅预测单一缺失食材，无法满足实际需求。

Method: 采用基于Set Transformer的多任务学习框架，同时学习从现有食材表示中检索缺失食材和评估篮子完整性。

Result: 在大规模食谱数据集上的实验表明，S2SRec2显著优于单目标基线方法。

Conclusion: S2SRec2为提升购物体验和激发烹饪创意提供了有效解决方案。

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [177] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Main category: cs.LG

TL;DR: 研究了特征选项（eigenoptions）在无模型强化学习中对信用分配的加速作用，发现预设特征选项有助于探索和信用分配，而在线发现可能阻碍学习。


<details>
  <summary>Details</summary>
Motivation: 特征选项在强化学习中表现出强大的探索能力，但其在信用分配中的作用尚未充分研究。

Method: 在表格和像素网格世界中评估特征选项，并提出在深度强化学习中学习选项值的方法。

Result: 预设特征选项有助于探索和信用分配，而在线发现可能阻碍学习。

Conclusion: 特征选项在同时支持信用分配和探索方面具有潜力，但也存在复杂性。

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [178] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Main category: cs.LG

TL;DR: 论文提出了一种结合图提示与权重剪枝的新框架GPAWP，旨在通过减少图提示数量提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNNs）在图任务中表现优异，但仍面临训练时间长、复杂关系捕捉不足等问题。图预训练与提示方法虽受关注，但此前研究忽视了图提示的优化潜力及其对模型稳定性的影响。

Method: 提出GPAWP框架，结合图提示与权重剪枝，通过重要性评估函数确定不同粒度的正负权重，并采用分层剪枝去除负提示标签。

Result: 在三个基准数据集上的实验表明，GPAWP显著减少了节点分类任务中的参数量，同时保持竞争力。

Conclusion: GPAWP通过优化图提示与剪枝，提升了GNN的性能与效率，为相关领域提供了新思路。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [179] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Main category: cs.LG

TL;DR: POIFormer是一种基于Transformer的框架，用于准确高效地将用户访问归因于特定POI，解决了GPS不准确和POI空间密度高的问题。


<details>
  <summary>Details</summary>
Motivation: 由于GPS精度问题（2-20米误差）和城市环境中POI的高密度分布，仅依赖邻近性难以准确确定用户实际访问的POI。

Method: POIFormer结合了空间邻近性、访问时间与时长、POI语义上下文特征以及用户和群体行为模式，利用Transformer的自注意力机制建模这些维度的复杂交互。

Result: 在真实世界移动数据集上的实验表明，POIFormer在空间噪声和高密度POI环境下显著优于现有基线方法。

Conclusion: POIFormer能够在大规模噪声数据中实现高效准确的POI归因，且适用于多种数据源和地理环境，具有实际部署价值。

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [180] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Main category: cs.LG

TL;DR: MolecBioNet是一个新颖的图框架，整合分子和生物医学知识，用于预测药物相互作用（DDI），通过统一建模药物对和多尺度知识集成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图方法独立处理药物对，忽略其复杂上下文依赖性，且难以整合生物网络和分子结构。MolecBioNet旨在解决这些问题，提供更稳健和可解释的DDI预测。

Method: MolecBioNet将药物对建模为统一实体，结合生物医学知识图和分子表示，使用图神经网络学习多尺度表示，并引入两种领域特定池化策略（CASPool和AGIPool）及互信息最小化正则化。

Result: 实验表明MolecBioNet在DDI预测上优于现有方法，消融研究和嵌入可视化验证了统一建模和多尺度集成的优势。

Conclusion: MolecBioNet通过整合多尺度知识和统一建模药物对，显著提升了DDI预测的性能和可解释性。

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [181] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 论文提出了一种基于在线世界模型的持续强化学习方法（OA），通过模型预测控制解决任务，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习（CRL）中，智能体在学习新任务时容易遗忘旧任务（灾难性遗忘），需要一种方法解决这一问题。

Method: 学习一个在线Follow-The-Leader浅层模型捕捉世界动态，基于最新模型进行规划，形成增量更新的OA。

Result: OA在设计的Continual Bench环境中表现优异，能持续学习新任务且不遗忘旧技能，优于基于深度世界模型的基线方法。

Conclusion: 在线世界模型和规划方法有效解决了CRL中的灾难性遗忘问题，OA表现优于现有方法。

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [182] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Main category: cs.LG

TL;DR: XiChen是一个完全由AI驱动的全球天气预报系统，能够在17秒内完成从数据同化到中期预报的整个流程，其预报准确性与传统数值天气预报系统相当。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的天气预报模型依赖数值天气预报系统进行初始条件准备，耗时且效率低，XiChen旨在实现完全独立于数值天气预报系统的高效AI驱动预报。

Method: XiChen基于预训练的天气预测基础模型，通过微调作为观测算子和数据同化模型，结合四维变分知识，实现高效数据同化和预报。

Result: XiChen在17秒内完成全流程预报，预报准确性与传统系统相当，预报技能领先时间超过8.25天。

Conclusion: XiChen展示了完全由AI驱动的天气预报系统的潜力，有望摆脱对数值天气预报系统的依赖。

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [183] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: DeepX-GAN模型用于模拟超出历史记录的极端气候事件，揭示潜在风险，并强调空间依赖性和适应性政策的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法捕捉超出历史记录的极端气候事件及其空间依赖性，低估了同步灾害的风险。

Method: 开发了DeepX-GAN模型，结合知识引导的深度生成方法，模拟统计上合理的“未见”极端事件。

Result: 模型在中东和北非地区发现，这些极端事件对脆弱区域影响更大，未来变暖可能改变其分布。

Conclusion: 需制定空间适应性政策，以应对新兴风险热点，而非仅依赖历史模式。

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [184] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: 提出了一种名为“warm-start model”的确定性模型，通过提供更好的起始点加速条件生成，显著减少了生成过程所需的步骤。


<details>
  <summary>Details</summary>
Motivation: 现有的迭代生成模型（如扩散模型和流匹配）生成高质量样本需要大量计算步骤，速度较慢。

Method: 引入warm-start模型，通过预测一个基于输入条件的有信息先验分布N(mu, sigma)，替代传统的无信息先验N(0, I)，从而减少生成过程的距离。

Result: 在图像修复等任务中，仅需11次函数评估即可达到与1000步DDPM基线竞争的结果。

Conclusion: warm-start模型简单高效，可与任何标准生成模型和采样器兼容，进一步加速生成过程。

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [185] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Main category: cs.LG

TL;DR: 该论文提出了一种构建性小波神经网络（CWNN），通过选择初始基并根据频率分析动态增加基函数，以提高计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统小波神经网络在构建准确基函数和高计算成本方面存在挑战，限制了其应用。

Method: 引入频率分析选择初始基，并通过动态增加高能量基函数优化网络结构。

Result: 提出的框架在多个任务中展示了高效性和广泛适用性，包括静态映射估计和时间序列分析。

Conclusion: CWNN通过频率驱动的基函数选择机制，显著提升了小波神经网络的性能和实用性。

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


### [186] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Main category: cs.LG

TL;DR: TPP-SD是一种利用推测解码技术加速Transformer时间点过程采样的新方法，通过并行验证候选事件实现2-6倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer时间点过程模型采样速度慢的问题，满足实际应用中对快速序列采样的需求。

Method: 结合时间点过程的细化算法与语言模型的推测解码技术，使用小型草稿模型生成候选事件，并行验证。

Result: 实验表明，TPP-SD在保持输出分布一致的同时，实现了2-6倍的加速。

Conclusion: TPP-SD填补了强大Transformer时间点过程模型与快速采样需求之间的技术空白。

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [187] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Main category: cs.LG

TL;DR: 论文提出两种轻量级模块（CKM和CSM），实现动态补丁大小控制，提升计算效率和预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 固定补丁大小限制了基于补丁的变换器代理模型在生产中的高效部署。

Method: 引入CKM和CSM模块，结合循环补丁大小展开，无需重新训练即可动态调整补丁大小。

Result: 在2D和3D PDE基准测试中提升了预测稳定性和运行效率。

Conclusion: 首次实现补丁大小可调的PDE代理模型，为计算自适应建模提供通用框架。

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [188] [Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation](https://arxiv.org/abs/2507.09353)
*Addison Weatherhead,Anna Goldenberg*

Main category: cs.LG

TL;DR: 提出了一种量化不确定性的时间序列缺失值插补框架，通过选择性插补减少误差并提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 医疗领域因传感器长时间断开导致缺失值问题严重，现有方法大多忽略模型不确定性或无法估计。

Method: 引入通用框架，量化并利用不确定性进行选择性插补，避免高不可靠插补。

Result: 在多种EHR数据集上实验表明，选择性插补减少误差并提升下游任务（如24小时死亡率预测）性能。

Conclusion: 将不确定性纳入时间序列插补具有实际价值，能显著提升模型性能。

Abstract: Time series data with missing values is common across many domains.
Healthcare presents special challenges due to prolonged periods of sensor
disconnection. In such cases, having a confidence measure for imputed values is
critical. Most existing methods either overlook model uncertainty or lack
mechanisms to estimate it. To address this gap, we introduce a general
framework that quantifies and leverages uncertainty for selective imputation.
By focusing on values the model is most confident in, highly unreliable
imputations are avoided. Our experiments on multiple EHR datasets, covering
diverse types of missingness, demonstrate that selectively imputing
less-uncertain values not only reduces imputation errors but also improves
downstream tasks. Specifically, we show performance gains in a 24-hour
mortality prediction task, underscoring the practical benefit of incorporating
uncertainty into time series imputation.

</details>


### [189] [Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes](https://arxiv.org/abs/2507.09362)
*Assaf Marron,Smadar Szekely,Irun Cohen,David Harel*

Main category: cs.LG

TL;DR: 论文提出了元自编码器（MAE）的概念，用于对一组自编码器进行编码和解码，适用于动态演化的多类别建模。


<details>
  <summary>Details</summary>
Motivation: 研究如何捕捉动态演化中多个类别的共性和特性，特别是在自然进化和机器学习中。

Method: 构建元自编码器（MAE），通过学习一组自编码器的紧凑表示和编码解码过程。

Result: 提供了MAE的构造定义和初步示例。

Conclusion: MAE为机器学习和生物学研究提供了新的建模工具，未来可进一步探索其应用。

Abstract: An autoencoder (AE) is a neural network that, using self-supervised training,
learns a succinct parameterized representation, and a corresponding encoding
and decoding process, for all instances in a given class. Here, we introduce
the concept of a meta-autoencoder (MAE): an AE for a collection of
autoencoders. Given a family of classes that differ from each other by the
values of some parameters, and a trained AE for each class, an MAE for the
family is a neural net that has learned a compact representation and associated
encoder and decoder for the class-specific AEs. One application of this general
concept is in research and modeling of natural evolution -- capturing the
defining and the distinguishing properties across multiple species that are
dynamically evolving from each other and from common ancestors. In this interim
report we provide a constructive definition of MAEs, initial examples, and the
motivating research directions in machine learning and biology.

</details>


### [190] [Fair CCA for Fair Representation Learning: An ADNI Study](https://arxiv.org/abs/2507.09382)
*Bojian Hou,Zhanliang Wang,Zhuoping Zhou,Boning Tong,Zexuan Wang,Jingxuan Bao,Duy Duong-Tran,Qi Long,Li Shen*

Main category: cs.LG

TL;DR: 提出了一种新的公平CCA方法，用于学习公平的低维表示，确保特征与敏感属性无关，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 随着公平性在机器学习中的重要性增加，现有公平CCA方法未充分考虑下游分类任务的影响，限制了其适用性。

Method: 提出了一种新颖的公平CCA方法，确保投影特征与敏感属性独立，从而在不影响准确性的情况下提升公平性。

Result: 在合成数据和ADNI真实数据上验证了方法的有效性，既能保持高相关性分析性能，又能提高分类任务的公平性。

Conclusion: 该方法为神经影像研究中需要无偏分析的公平机器学习提供了有效工具。

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlations
between different data modalities and learning low-dimensional representations.
As fairness becomes crucial in machine learning, fair CCA has gained attention.
However, previous approaches often overlook the impact on downstream
classification tasks, limiting applicability. We propose a novel fair CCA
method for fair representation learning, ensuring the projected features are
independent of sensitive attributes, thus enhancing fairness without
compromising accuracy. We validate our method on synthetic data and real-world
data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating
its ability to maintain high correlation analysis performance while improving
fairness in classification tasks. Our work enables fair machine learning in
neuroimaging studies where unbiased analysis is essential.

</details>


### [191] [Geometric Generative Modeling with Noise-Conditioned Graph Networks](https://arxiv.org/abs/2507.09391)
*Peter Pao-Huang,Mitchell Black,Xiaojie Qiu*

Main category: cs.LG

TL;DR: 论文提出了一种噪声条件图网络（NCGNs），通过动态调整架构以适应生成过程中的噪声水平，解决了现有图神经网络在噪声独立性上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的图生成模型在噪声去除方面表现优异，但其图神经网络架构独立于噪声水平，限制了表达能力。

Method: 引入NCGNs，提出动态消息传递（DMP），根据噪声水平调整消息传递的范围和分辨率。

Result: DMP在3D点云、时空转录组学和图像等多个领域表现优于噪声独立架构。

Conclusion: NCGNs通过动态适应噪声水平，显著提升了图生成模型的性能。

Abstract: Generative modeling of graphs with spatial structure is essential across many
applications from computer graphics to spatial genomics. Recent flow-based
generative models have achieved impressive results by gradually adding and then
learning to remove noise from these graphs. Existing models, however, use graph
neural network architectures that are independent of the noise level, limiting
their expressiveness. To address this issue, we introduce
\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural
networks that dynamically modify their architecture according to the noise
level during generation. Our theoretical and empirical analysis reveals that as
noise increases, (1) graphs require information from increasingly distant
neighbors and (2) graphs can be effectively represented at lower resolutions.
Based on these insights, we develop Dynamic Message Passing (DMP), a specific
instantiation of NCGNs that adapts both the range and resolution of message
passing to the noise level. DMP consistently outperforms noise-independent
architectures on a variety of domains including $3$D point clouds,
spatiotemporal transcriptomics, and images. Code is available at
https://github.com/peterpaohuang/ncgn.

</details>


### [192] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Main category: cs.LG

TL;DR: 研究了多头潜在注意力（MLA）对预训练中Transformer内部容量的影响，发现旋转嵌入的应用方式对防止容量瓶颈和频谱碎片化至关重要。


<details>
  <summary>Details</summary>
Motivation: 探索MLA压缩键/值记忆时如何影响Transformer的预训练容量，特别是旋转嵌入的应用方式。

Method: 使用Marchenko-Pastur诊断分析$W_{Q}W_{K}^\top$矩阵的频谱，比较标准多头注意力（MHA）、MLA-PreRoPE和MLA-Decoupled三种变体。

Result: 发现容量瓶颈局部出现，MLA-PreRoPE和MHA均出现频谱异常，而MLA-Decoupled能保持频谱支持并抑制异常。

Conclusion: 旋转嵌入的应用方式与压缩位置同样重要，共享旋转分量可避免频谱碎片化并保持模型容量。

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [193] [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404)
*Mustafa Shukor,Louis Bethune,Dan Busbridge,David Grangier,Enrico Fini,Alaaeldin El-Nouby,Pierre Ablin*

Main category: cs.LG

TL;DR: 提出了一种基于缩放定律的系统方法，用于确定目标领域的最佳数据混合比例，避免了传统试错法的高成本。


<details>
  <summary>Details</summary>
Motivation: 传统的数据混合比例选择依赖试错法，在大规模预训练中不切实际，需要一种更高效的方法。

Method: 利用缩放定律预测模型在不同数据混合比例下的损失，并通过小规模训练估算参数，外推到更大规模和未见过的数据混合比例。

Result: 在LLM、NMM和LVM三种大规模预训练场景中验证了缩放定律的普适性和预测能力，并能推导出给定预算下的最优数据混合比例。

Conclusion: 该方法为数据混合比例的选择提供了理论依据，显著降低了大规模预训练的成本和复杂性。

Abstract: Large foundation models are typically trained on data from multiple domains,
with the data mixture--the proportion of each domain used--playing a critical
role in model performance. The standard approach to selecting this mixture
relies on trial and error, which becomes impractical for large-scale
pretraining. We propose a systematic method to determine the optimal data
mixture for any target domain using scaling laws. Our approach accurately
predicts the loss of a model of size $N$ trained with $D$ tokens and a specific
domain weight vector $h$. We validate the universality of these scaling laws by
demonstrating their predictive power in three distinct and large-scale
settings: large language model (LLM), native multimodal model (NMM), and large
vision models (LVM) pretraining. We further show that these scaling laws can
extrapolate to new data mixtures and across scales: their parameters can be
accurately estimated using a few small-scale training runs, and used to
estimate the performance at larger scales and unseen domain weights. The
scaling laws allow to derive the optimal domain weights for any target domain
under a given training budget ($N$,$D$), providing a principled alternative to
costly trial-and-error methods.

</details>


### [194] [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
*Santhosh Kumar Ravindran*

Main category: cs.LG

TL;DR: 论文提出了一种名为对抗性激活修补的新框架，用于检测和缓解大型语言模型中的欺骗行为，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在安全对齐后仍可能表现出欺骗行为，需要一种机制来检测和缓解此类问题。

Method: 采用对抗性激活修补技术，通过将欺骗性提示的激活值修补到安全前向传递中，模拟漏洞并量化欺骗率。

Result: 实验表明，对抗性修补将欺骗性输出从0%提高到23.9%，并验证了多个假设。

Conclusion: 该研究为AI安全提供了新工具，并提出了未来研究方向。

Abstract: Large language models (LLMs) aligned for safety through techniques like
reinforcement learning from human feedback (RLHF) often exhibit emergent
deceptive behaviors, where outputs appear compliant but subtly mislead or omit
critical information. This paper introduces adversarial activation patching, a
novel mechanistic interpretability framework that leverages activation patching
as an adversarial tool to induce, detect, and mitigate such deception in
transformer-based models. By sourcing activations from "deceptive" prompts and
patching them into safe forward passes at specific layers, we simulate
vulnerabilities and quantify deception rates. Through toy neural network
simulations across multiple scenarios (e.g., 1000 trials per setup), we
demonstrate that adversarial patching increases deceptive outputs to 23.9% from
a 0% baseline, with layer-specific variations supporting our hypotheses. We
propose six hypotheses, including transferability across models, exacerbation
in multimodal settings, and scaling effects. An expanded literature review
synthesizes over 20 key works in interpretability, deception, and adversarial
attacks. Mitigation strategies, such as activation anomaly detection and robust
fine-tuning, are detailed, alongside ethical considerations and future research
directions. This work advances AI safety by highlighting patching's dual-use
potential and provides a roadmap for empirical studies on large-scale models.

</details>


### [195] [On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization](https://arxiv.org/abs/2507.09428)
*Zakhar Shumaylov,Vasileios Tsiaras,Yannis Stylianou*

Main category: cs.LG

TL;DR: 论文探讨了信息几何在深度学习模型压缩中的应用，强调通过定义低计算子流形和投影实现压缩，并分析了信息散度在预训练和微调模型中的不同作用。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型参数量的增加，资源受限设备上的部署需要有效的压缩技术。

Method: 采用信息几何视角分析模型压缩方法，提出通过软秩约束和迭代奇异值阈值方法实现压缩。

Result: 研究表明，信息散度在预训练模型压缩中至关重要，而在微调场景中，瓶颈模型的可训练性更为重要。

Conclusion: 通过软秩减少的简单修改可在固定压缩率下提升性能，迭代方法在压缩中具有优势。

Abstract: The ever-increasing parameter counts of deep learning models necessitate
effective compression techniques for deployment on resource-constrained
devices. This paper explores the application of information geometry, the study
of density-induced metrics on parameter spaces, to analyze existing methods
within the space of model compression, primarily focusing on operator
factorization. Adopting this perspective highlights the core challenge:
defining an optimal low-compute submanifold (or subset) and projecting onto it.
We argue that many successful model compression approaches can be understood as
implicitly approximating information divergences for this projection. We
highlight that when compressing a pre-trained model, using information
divergences is paramount for achieving improved zero-shot accuracy, yet this
may no longer be the case when the model is fine-tuned. In such scenarios,
trainability of bottlenecked models turns out to be far more important for
achieving high compression ratios with minimal performance degradation,
necessitating adoption of iterative methods. In this context, we prove
convergence of iterative singular value thresholding for training neural
networks subject to a soft rank constraint. To further illustrate the utility
of this perspective, we showcase how simple modifications to existing methods
through softer rank reduction result in improved performance under fixed
compression rates.

</details>


### [196] [Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series](https://arxiv.org/abs/2507.09439)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: DyCAST-Net是一种新型架构，用于多变量时间序列中的因果发现，通过动态稀疏注意力机制和扩张时间卷积提高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列中的因果关系分析在金融和营销等领域至关重要，但传统方法难以处理复杂依赖和滞后效应。

Method: DyCAST-Net结合扩张时间卷积和动态稀疏注意力机制，通过自适应阈值策略消除虚假连接，并采用统计洗牌测试验证鲁棒性。

Result: 在金融和营销数据集上，DyCAST-Net优于现有模型，能更精确估计因果延迟并减少误发现，尤其在噪声环境中。

Conclusion: DyCAST-Net通过可解释的注意力热图揭示隐藏因果模式，适用于高维动态场景，具有广泛的应用潜力。

Abstract: Understanding causal relationships in multivariate time series (MTS) is
essential for effective decision-making in fields such as finance and
marketing, where complex dependencies and lagged effects challenge conventional
analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal
Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel
architecture designed to enhance causal discovery by integrating dilated
temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net
effectively captures multiscale temporal dependencies through dilated
convolutions while leveraging an adaptive thresholding strategy in its
attention mechanism to eliminate spurious connections, ensuring both accuracy
and interpretability. A statistical shuffle test validation further strengthens
robustness by filtering false positives and improving causal inference
reliability. Extensive evaluations on financial and marketing datasets
demonstrate that DyCAST-Net consistently outperforms existing models such as
TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation
of causal delays and significantly reduces false discoveries, particularly in
noisy environments. Moreover, attention heatmaps offer interpretable insights,
uncovering hidden causal patterns such as the mediated effects of advertising
on consumer behavior and the influence of macroeconomic indicators on financial
markets. Case studies illustrate DyCAST-Net's ability to detect latent
mediators and lagged causal factors, making it particularly effective in
high-dimensional, dynamic settings. The model's architecture enhanced by
RMSNorm stabilization and causal masking ensures scalability and adaptability
across diverse application domains

</details>


### [197] [Transformers Don't In-Context Learn Least Squares Regression](https://arxiv.org/abs/2507.09440)
*Joshua Hill,Benjamin Eyre,Elliot Creager*

Main category: cs.LG

TL;DR: 本文研究了大型预训练变压器中的上下文学习（ICL）机制，通过合成线性回归实验发现其泛化能力受限，且与OLS等算法不一致。


<details>
  <summary>Details</summary>
Motivation: 探索ICL在预训练变压器中的工作机制，揭示其与常见学习算法的差异。

Method: 使用合成线性回归任务，通过分布外泛化实验和谱分析研究ICL行为。

Result: 变压器在分布变化后泛化能力下降，且其行为与OLS不一致；谱分析显示训练数据输入具有独特的谱特征。

Conclusion: ICL的泛化能力受限于训练数据分布，其机制与常规学习算法不同，谱特征与性能相关。

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
pretrained transformers, enabling them to solve new tasks implicit in example
input-output pairs without any gradient updates. Despite its practical success,
the mechanisms underlying ICL remain largely mysterious. In this work we study
synthetic linear regression to probe how transformers implement learning at
inference time. Previous works have demonstrated that transformers match the
performance of learning rules such as Ordinary Least Squares (OLS) regression
or gradient descent and have suggested ICL is facilitated in transformers
through the learned implementation of one of these techniques. In this work, we
demonstrate through a suite of out-of-distribution generalization experiments
that transformers trained for ICL fail to generalize after shifts in the prompt
distribution, a behaviour that is inconsistent with the notion of transformers
implementing algorithms such as OLS. Finally, we highlight the role of the
pretraining corpus in shaping ICL behaviour through a spectral analysis of the
learned representations in the residual stream. Inputs from the same
distribution as the training data produce representations with a unique
spectral signature: inputs from this distribution tend to have the same top two
singular vectors. This spectral signature is not shared by out-of-distribution
inputs, and a metric characterizing the presence of this signature is highly
correlated with low loss.

</details>


### [198] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Main category: cs.LG

TL;DR: 该论文提出了一种结合卷积神经网络（CNN）和计算热力学模型的方法，用于预测压水堆燃料棒的温度、应力和应变，以支持核电站的预测性维护策略。


<details>
  <summary>Details</summary>
Motivation: 核电站的预测性维护（PdM）策略可以减少因组件故障导致的意外停机时间，但需要实时监测燃料棒的热力学状态。

Method: 使用CNN架构结合热力学模型，基于有限的温度测量数据预测燃料棒的温度分布，并通过模拟生成训练、验证和测试数据集。

Result: CNN在1000多个训练周期中未出现过拟合，能够高精度预测温度分布，进而用于计算燃料棒的应力和应变分布。

Conclusion: 该方法为核反应堆的预测性维护工具开发提供了潜在支持，实现了对燃料棒状态的实时监测。

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [199] [Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting](https://arxiv.org/abs/2507.09445)
*Runze Yang,Longbing Cao,Xin You,Kun Fang,Jianxun Li,Jie Yang*

Main category: cs.LG

TL;DR: 提出了一种名为Fourier Basis Mapping（FBM）的新方法，通过傅里叶基展开和映射解决现有傅里叶方法在时间序列预测中的问题，并展示了其在多种神经网络中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于傅里叶变换的方法存在起始周期不一致、序列长度不一致等问题，且无法精确解释频率成分和忽视时间信息。

Method: 提出FBM方法，通过傅里叶基展开和映射整合时间-频率特征，并设计了FBM-L、FBM-NL、FBM-NP和FBM-S等变体，以及多种针对时间-频率特征的技术。

Result: 在多种真实数据集上验证了FBM在长短期预测任务中的优越性能，达到了SOTA水平。

Conclusion: FBM方法有效解决了现有傅里叶方法的局限性，为时间序列预测提供了新的解决方案。

Abstract: The integration of Fourier transform and deep learning opens new avenues for
time series forecasting. We reconsider the Fourier transform from a basis
functions perspective. Specifically, the real and imaginary parts of the
frequency components can be regarded as the coefficients of cosine and sine
basis functions at tiered frequency levels, respectively. We find that existing
Fourier-based methods face inconsistent starting cycles and inconsistent series
length issues. They fail to interpret frequency components precisely and
overlook temporal information. Accordingly, the novel Fourier Basis Mapping
(FBM) method addresses these issues by integrating time-frequency features
through Fourier basis expansion and mapping in the time-frequency space. Our
approach extracts explicit frequency features while preserving temporal
characteristics. FBM supports plug-and-play integration with various types of
neural networks by only adjusting the first initial projection layer for better
performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,
MLP-based, and Transformer-based models, respectively, demonstrating the
effectiveness of time-frequency features. Next, we propose a synergetic model
architecture, termed FBM-S, which decomposes the seasonal, trend, and
interaction effects into three separate blocks, each designed to model
time-frequency features in a specialized manner. Finally, we introduce several
techniques tailored for time-frequency features, including interaction masking,
centralization, patching, rolling window projection, and multi-scale
down-sampling. The results are validated on diverse real-world datasets for
both long-term and short-term forecasting tasks with SOTA performance.

</details>


### [200] [Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring](https://arxiv.org/abs/2507.09460)
*Noah Marchal,William E. Janes,Mihail Popescu,Xing Song*

Main category: cs.LG

TL;DR: 论文开发了半监督回归模型，通过家庭传感器数据预测ALS功能衰退率，比较了三种模型范式，发现迁移学习在子量表预测中表现最佳，自注意力插值在非线性模式中表现最优。


<details>
  <summary>Details</summary>
Motivation: 解决ALS临床监测中因定期评估而遗漏关键变化的问题。

Method: 开发半监督回归模型，比较个体批学习、队列批学习与增量微调迁移学习，使用线性、三次多项式及自注意力插值。

Result: 迁移学习在32次对比中28次表现更优，自注意力插值在子量表模型中误差最低，线性插值在复合量表中更稳定。

Conclusion: 根据功能域的同质-异质特征匹配学习技术可提高预测准确性，未来可整合自适应模型选择以实现及时干预。

Abstract: Clinical monitoring of functional decline in ALS relies on periodic
assessments that may miss critical changes occurring between visits. To address
this gap, semi-supervised regression models were developed to estimate rates of
decline in a case series cohort by targeting ALSFRS- R scale trajectories with
continuous in-home sensor monitoring data. Our analysis compared three model
paradigms (individual batch learning and cohort-level batch versus incremental
fine-tuned transfer learning) across linear slope, cubic polynomial, and
ensembled self-attention pseudo-label interpolations. Results revealed cohort
homogeneity across functional domains responding to learning methods, with
transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32
contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting
the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention
interpolation achieved the lowest prediction error for subscale-level models
(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,
outperforming linear and cubic interpolations in 20 of 32 contrasts, though
linear interpolation proved more stable in all ALSFRS-R composite scale models
(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity
profiles across functional domains with respiratory and speech exhibiting
patient-specific patterns benefiting from personalized incremental adaptation,
while swallowing and dressing functions followed cohort-level trajectories
suitable for transfer models. These findings suggest that matching learning and
pseudo-labeling techniques to functional domain-specific
homogeneity-heterogeneity profiles enhances predictive accuracy in ALS
progression tracking. Integrating adaptive model selection within sensor
monitoring platforms could enable timely interventions and scalable deployment
in future multi-center studies.

</details>


### [201] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gaël Richard,Slim Essid,Andrei Bursuc,Patrick Pérez*

Main category: cs.LG

TL;DR: LoRA-MCL结合多选择学习（MCL）和低秩适应（LoRA），在语言模型中生成多样且合理的句子续写。


<details>
  <summary>Details</summary>
Motivation: 传统语言建模存在多解性问题，即给定上下文可能有多个合理的未来句子。

Method: 采用多选择学习（MCL）和Winner-Takes-All（WTA）损失，结合LoRA处理模糊性。

Result: 在视觉和音频字幕任务中，生成结果具有高多样性和相关性。

Conclusion: LoRA-MCL有效解决了语言建模中的多解性问题，生成结果多样且合理。

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [202] [La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://arxiv.org/abs/2507.09466)
*Tomas Geffner,Kieran Didi,Zhonglin Cao,Danny Reidenbach,Zuobai Zhang,Christian Dallago,Emine Kucukbenli,Karsten Kreis,Arash Vahdat*

Main category: cs.LG

TL;DR: La-Proteina是一种基于部分潜在蛋白质表示的新型生成模型，用于联合生成全原子蛋白质结构和氨基酸序列，解决了侧链变化的挑战，并在多个生成基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型难以直接联合生成全原子蛋白质结构和氨基酸序列，尤其是需要处理侧链长度变化的问题。

Method: 采用部分潜在蛋白质表示，显式建模粗粒度主干结构，通过固定维度的残基潜在变量捕获序列和原子细节，利用流匹配建模联合分布。

Result: 在多个生成基准测试中达到最先进性能，包括全原子共设计性、多样性和结构有效性，并能生成长达800个残基的蛋白质。

Conclusion: La-Proteina在原子级蛋白质设计任务中表现出卓越的可扩展性和鲁棒性，尤其在原子级基序支架方面超越先前模型。

Abstract: Recently, many generative models for de novo protein structure design have
emerged. Yet, only few tackle the difficult task of directly generating fully
atomistic structures jointly with the underlying amino acid sequence. This is
challenging, for instance, because the model must reason over side chains that
change in length during generation. We introduce La-Proteina for atomistic
protein design based on a novel partially latent protein representation: coarse
backbone structure is modeled explicitly, while sequence and atomistic details
are captured via per-residue latent variables of fixed dimensionality, thereby
effectively side-stepping challenges of explicit side-chain representations.
Flow matching in this partially latent space then models the joint distribution
over sequences and full-atom structures. La-Proteina achieves state-of-the-art
performance on multiple generation benchmarks, including all-atom
co-designability, diversity, and structural validity, as confirmed through
detailed structural analyses and evaluations. Notably, La-Proteina also
surpasses previous models in atomistic motif scaffolding performance, unlocking
critical atomistic structure-conditioned protein design tasks. Moreover,
La-Proteina is able to generate co-designable proteins of up to 800 residues, a
regime where most baselines collapse and fail to produce valid samples,
demonstrating La-Proteina's scalability and robustness.

</details>


### [203] [Discrete Differential Principle for Continuous Smooth Function Representation](https://arxiv.org/abs/2507.09480)
*Guoyou Wang,Yihua Tan,Shiqi Liu*

Main category: cs.LG

TL;DR: 提出了一种基于Vandermonde系数矩阵的离散微分算子，用于估计导数和局部表示连续光滑函数，解决了泰勒公式的维度灾难和误差传播问题。


<details>
  <summary>Details</summary>
Motivation: 泰勒公式在函数表示中具有重要作用，但在离散情况下存在维度灾难和导数计算中的误差传播问题。

Method: 通过截断泰勒级数导出的Vandermonde系数矩阵，提出新的离散微分算子，同时计算所有低阶导数，并利用等距均匀采样实现高精度。

Result: 数学上建立了严格的误差界限，实验证明该方法在导数估计和函数表示上优于有限前向差分、三次样条和线性插值。

Conclusion: 该方法在视觉表示、特征提取、流体力学和跨媒体成像等领域具有广泛适用性。

Abstract: Taylor's formula holds significant importance in function representation,
such as solving differential difference equations, ordinary differential
equations, partial differential equations, and further promotes applications in
visual perception, complex control, fluid mechanics, weather forecasting and
thermodynamics. However, the Taylor's formula suffers from the curse of
dimensionality and error propagation during derivative computation in discrete
situations. In this paper, we propose a new discrete differential operator to
estimate derivatives and to represent continuous smooth function locally using
the Vandermonde coefficient matrix derived from truncated Taylor series. Our
method simultaneously computes all derivatives of orders less than the number
of sample points, inherently mitigating error propagation. Utilizing
equidistant uniform sampling, it achieves high-order accuracy while alleviating
the curse of dimensionality. We mathematically establish rigorous error bounds
for both derivative estimation and function representation, demonstrating
tighter bounds for lower-order derivatives. We extend our method to the
two-dimensional case, enabling its use for multivariate derivative
calculations. Experiments demonstrate the effectiveness and superiority of the
proposed method compared to the finite forward difference method for derivative
estimation and cubic spline and linear interpolation for function
representation. Consequently, our technique offers broad applicability across
domains such as vision representation, feature extraction, fluid mechanics, and
cross-media imaging.

</details>


### [204] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: 研究探讨了大型语言模型（LLM）的推理能力，特别是通过强化学习（RL）提升性能的方法。研究发现，某些模型（如Qwen2.5）在特定基准测试中表现优异，但可能因数据污染导致结果不可靠。为此，作者提出了一种生成合成数据的方法，并验证了准确奖励信号的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示现有基准测试中可能存在的数据污染问题，并探索强化学习对LLM推理能力的真实影响。

Method: 作者引入了一个生成器，用于创建完全合成的算术问题数据集（RandomCalculation），以消除数据污染的影响。

Result: 实验表明，仅准确的奖励信号能持续提升性能，而噪声或错误信号无效。Qwen2.5在无污染数据上的表现验证了这一点。

Conclusion: 结论强调需要在无污染的基准测试和多样化模型家族中评估RL方法，以确保结论的可信度。

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [205] [An Analysis of Action-Value Temporal-Difference Methods That Learn State Values](https://arxiv.org/abs/2507.09523)
*Brett Daley,Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: 论文分析了基于两个不对称价值函数的TD学习方法（QV-learning和AV-learning），发现AV-learning在控制任务中优于Q-learning，并提出了新算法RDQ，性能优于Dueling DQN。


<details>
  <summary>Details</summary>
Motivation: 探讨学习两个价值函数（而非单一动作价值函数）是否具有优势及其理论依据。

Method: 分析QV-learning和AV-learning的收敛性和样本效率，提出新算法Regularized Dueling Q-learning (RDQ)。

Result: AV-learning在控制任务中表现优于Q-learning；RDQ在MinAtar基准测试中显著优于Dueling DQN。

Conclusion: AV-learning方法在控制任务中具有优势，新算法RDQ表现优异。

Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.

</details>


### [206] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Main category: cs.LG

TL;DR: 该研究探讨了在不平衡数据集下评估XAI方法解释可靠性的初步方法，提出了一种基于少数类的评估框架，并通过一个霜冻事件的案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的广泛应用和立法要求的增加，解释的稳健性成为关键，而不平衡数据集在高风险场景中常见且具有挑战性。

Method: 提出了一种基于少数类的评估方法，包括邻域生成、解释聚合和一致性测试。

Result: 通过一个霜冻事件的案例验证了方法的有效性。

Conclusion: 该研究为不平衡数据集下XAI解释的可靠性评估提供了初步框架。

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [207] [Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives](https://arxiv.org/abs/2507.09565)
*Heeba Shakeel,Tanvir Ahmad,Chandni Saxena*

Main category: cs.LG

TL;DR: 该论文介绍了一个用于社交媒体用户帖子中健康维度分类的数据集，涵盖六个关键方面，并评估了传统和先进的机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 旨在通过社交媒体内容评估用户的多维健康状态，为个性化健康评估和心理健康早期干预提供支持。

Method: 开发了一个全面的注释框架，并评估了传统机器学习和基于Transformer的模型，使用10折交叉验证进行性能评估。

Result: 模型性能通过精确率、召回率和F1分数衡量，并通过事后解释确保透明度和可解释性。

Conclusion: 该数据集为特定区域的健康评估提供了支持，并促进了心理健康早期干预策略的发展，同时遵循了伦理准则。

Abstract: We introduce a dataset for classifying wellness dimensions in social media
user posts, covering six key aspects: physical, emotional, social,
intellectual, spiritual, and vocational. The dataset is designed to capture
these dimensions in user-generated content, with a comprehensive annotation
framework developed under the guidance of domain experts. This framework allows
for the classification of text spans into the appropriate wellness categories.
We evaluate both traditional machine learning models and advanced
transformer-based models for this multi-class classification task, with
performance assessed using precision, recall, and F1-score, averaged over
10-fold cross-validation. Post-hoc explanations are applied to ensure the
transparency and interpretability of model decisions. The proposed dataset
contributes to region-specific wellness assessments in social media and paves
the way for personalized well-being evaluations and early intervention
strategies in mental health. We adhere to ethical considerations for
constructing and releasing our experiments and dataset publicly on Github.

</details>


### [208] [DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences](https://arxiv.org/abs/2507.09602)
*Bocheng Ju,Junchao Fan,Jiaqi Liu,Xiaolin Chang*

Main category: cs.LG

TL;DR: 论文提出DRAGD和DRAGDP两种攻击方法，利用联邦学习中遗忘数据前后的梯度差异重建被删除的数据，揭示了联邦遗忘中的隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 联邦遗忘过程中梯度交换可能泄露敏感信息，现有方法未能有效解决这一问题。

Method: 提出DRAGD攻击方法，利用梯度差异重建数据；进一步提出DRAGDP，结合公开数据提升重建精度。

Result: 实验表明DRAGD和DRAGDP在数据重建上显著优于现有方法。

Conclusion: 研究揭示了联邦遗忘的隐私风险，并提出了实际解决方案，提升了系统安全性。

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy. However, the rise of federated unlearning, designed to allow
clients to erase their data from the global model, introduces new privacy
concerns. Specifically, the gradient exchanges during the unlearning process
can leak sensitive information about deleted data. In this paper, we introduce
DRAGD, a novel attack that exploits gradient discrepancies before and after
unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced
version of DRAGD that leverages publicly available prior data to improve
reconstruction accuracy, particularly for complex datasets like facial images.
Extensive experiments across multiple datasets demonstrate that DRAGD and
DRAGDP significantly outperform existing methods in data reconstruction.Our
work highlights a critical privacy vulnerability in federated unlearning and
offers a practical solution, advancing the security of federated unlearning
systems in real-world applications.

</details>


### [209] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: MLoRQ是一种结合低秩近似和混合精度量化的新方法，用于在资源受限的边缘设备上部署Transformer网络，通过两阶段优化实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的边缘设备上部署Transformer网络的挑战，结合低秩近似和量化技术以优化性能。

Method: MLoRQ采用两阶段优化：1) 层内优化，筛选低秩和量化组合；2) 层间优化，分配比特宽度和秩以满足内存限制。可选步骤使用自适应舍入技术减少误差。

Result: 在Vision Transformers上评估，MLoRQ实现了最高15%的性能提升。

Conclusion: MLoRQ是一种高效且兼容性强的压缩方法，适用于多种任务。

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [210] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim,Bouaziz,Manon Revel,Jack Kussman,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Main category: cs.LG

TL;DR: 论文通过大规模多语言人类研究，发现人类偏好比21种先进LLMs的响应更具多样性，并提出负相关采样方法提升对齐方法性能，最终开源了最大的多语言多轮偏好数据集Community Alignment。


<details>
  <summary>Details</summary>
Motivation: 研究如何让大型语言模型（LLMs）服务于具有不同（可能冲突）偏好的用户，尤其是在文化、政治等维度上。

Method: 通过大规模多语言人类研究（N=15,000）分析人类偏好与LLMs响应的差异，提出负相关采样方法，并构建Community Alignment数据集。

Result: 发现人类偏好比LLMs响应更具多样性，负相关采样显著提升对齐方法性能，开源了最大的多语言多轮偏好数据集。

Conclusion: Community Alignment数据集有望帮助LLMs更好地服务于全球多样化用户。

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [211] [Conformal Prediction for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2507.09678)
*Alexander David Balinsky,Dominik Krzeminski,Alexander Balinsky*

Main category: cs.LG

TL;DR: 研究将Conformal Prediction（CP）与确定性加密数据的监督学习结合，验证CP在加密领域仍有效，并比较p值和e值CP方法的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在填补严格不确定性量化与隐私保护机器学习之间的空白。

Method: 使用AES加密的MNIST数据集，测试p值和e值CP方法在加密数据上的表现。

Result: 加密数据模型测试准确率达36.88%，e值CP的预测集覆盖率达60%以上，p值CP预测集更小但覆盖精度较低。

Conclusion: CP在加密数据中具有潜力，但需权衡预测集紧凑性与可靠性。

Abstract: We investigate the integration of Conformal Prediction (CP) with supervised
learning on deterministically encrypted data, aiming to bridge the gap between
rigorous uncertainty quantification and privacy-preserving machine learning.
Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP
methods remain effective even when applied directly in the encrypted domain,
owing to the preservation of data exchangeability under fixed-key encryption.
We test traditional $p$-value-based against $e$-value-based conformal
predictors. Our empirical evaluation reveals that models trained on
deterministically encrypted data retain the ability to extract meaningful
structure, achieving 36.88\% test accuracy -- significantly above random
guessing (9.56\%) observed with per-instance encryption. Moreover,
$e$-value-based CP achieves predictive set coverage of over 60\% with 4.3
loss-threshold calibration, correctly capturing the true label in 4888 out of
5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive
sets but with reduced coverage accuracy. These findings highlight both the
promise and limitations of CP in encrypted data settings and underscore
critical trade-offs between prediction set compactness and reliability. %Our
work sets a foundation for principled uncertainty quantification in secure,
privacy-aware learning systems.

</details>


### [212] [Networked Information Aggregation via Machine Learning](https://arxiv.org/abs/2507.09683)
*Michael Kearns,Aaron Roth,Emily Ryu*

Main category: cs.LG

TL;DR: 研究分布式学习问题，学习代理嵌入有向无环图（DAG）中，探讨信息聚合的条件和界限。


<details>
  <summary>Details</summary>
Motivation: 探索在DAG中，代理如何通过观察部分特征和父节点的预测，学习到与直接访问所有特征的最佳模型竞争的性能。

Method: 代理按拓扑顺序学习，利用自身观察的特征和父节点的预测作为额外特征。研究线性和一般假设类的上下界。

Result: DAG的深度是关键参数，信息聚合可在足够长的路径上实现，但某些分布下即使在大DAG中也无法聚合。

Conclusion: DAG深度决定信息聚合的可能性，实验验证了理论结果。

Abstract: We study a distributed learning problem in which learning agents are embedded
in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution
over feature/label pairs, and each agent or vertex in the graph is able to
directly observe only a subset of the features -- potentially a different
subset for every agent. The agents learn sequentially in some order consistent
with a topological sort of the DAG, committing to a model mapping observations
to predictions of the real-valued label. Each agent observes the predictions of
their parents in the DAG, and trains their model using both the features of the
instance that they directly observe, and the predictions of their parents as
additional features. We ask when this process is sufficient to achieve
\emph{information aggregation}, in the sense that some agent in the DAG is able
to learn a model whose error is competitive with the best model that could have
been learned (in some hypothesis class) with direct access to \emph{all}
features, despite the fact that no single agent in the network has such access.
We give upper and lower bounds for this problem for both linear and general
hypothesis classes. Our results identify the \emph{depth} of the DAG as the key
parameter: information aggregation can occur over sufficiently long paths in
the DAG, assuming that all of the relevant features are well represented along
the path, and there are distributions over which information aggregation cannot
occur even in the linear case, and even in arbitrarily large DAGs that do not
have sufficient depth (such as a hub-and-spokes topology in which the spoke
vertices collectively see all the features). We complement our theoretical
results with a comprehensive set of experiments.

</details>


### [213] [Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness](https://arxiv.org/abs/2507.09687)
*Md Mushfiqur Rahaman,Elliot Chang,Tasmiah Haque,Srinjoy Das*

Main category: cs.LG

TL;DR: 本文比较了生成式和判别式LSTM文本分类模型在边缘计算中的表现，重点研究了后训练量化（PTQ）的影响，发现生成式模型对量化位宽、校准数据和输入噪声更敏感。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中的文本分类需要低延迟和高准确性，生成式分类器对噪声和分布外数据具有鲁棒性，但部署时面临计算和内存限制。PTQ是一种无需重新训练即可减小模型大小和计算成本的方法。

Method: 使用Brevitas量化库，对生成式和判别式LSTM文本分类模型进行PTQ，评估不同位宽下的表现，并分析校准数据的类别不平衡对权重调整和激活分布的影响。

Result: 判别式分类器表现稳健，而生成式分类器对位宽、校准数据和输入噪声更敏感。类别不平衡的校准数据会导致生成式分类器在低位宽下性能下降。

Conclusion: 研究强调了校准数据在PTQ中的重要性，并揭示了生成式分类器在噪声环境下的适用性，为边缘部署提供了指导。

Abstract: Text classification plays a pivotal role in edge computing applications like
industrial monitoring, health diagnostics, and smart assistants, where low
latency and high accuracy are both key requirements. Generative classifiers, in
particular, have been shown to exhibit robustness to out-of-distribution and
noisy data, which is an extremely critical consideration for deployment in such
real-time edge environments. However, deploying such models on edge devices
faces computational and memory constraints. Post Training Quantization (PTQ)
reduces model size and compute costs without retraining, making it ideal for
edge deployment. In this work, we present a comprehensive comparative study of
generative and discriminative Long Short Term Memory (LSTM)-based text
classification models with PTQ using the Brevitas quantization library. We
evaluate both types of classifier models across multiple bitwidths and assess
their robustness under regular and noisy input conditions. We find that while
discriminative classifiers remain robust, generative ones are more sensitive to
bitwidth, calibration data used during PTQ, and input noise during quantized
inference. We study the influence of class imbalance in calibration data for
both types of classifiers, comparing scenarios with evenly and unevenly
distributed class samples including their effect on weight adjustments and
activation profiles during PTQ. Using test statistics derived from
nonparametric hypothesis testing, we identify that using class imbalanced data
during calibration introduces insufficient weight adaptation at lower bitwidths
for generative LSTM classifiers, thereby leading to degraded performance. This
study underscores the role of calibration data in PTQ and when generative
classifiers succeed or fail under noise, aiding deployment in edge
environments.

</details>


### [214] [Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting](https://arxiv.org/abs/2507.09694)
*Nicolas Gonel,Paul Saves,Joseph Morlier*

Main category: cs.LG

TL;DR: 论文介绍了一个开源框架，用于开发相关性核函数，特别关注用户自定义和核函数组合，以支持代理建模。通过引入频率感知元素，该框架能更好地捕捉复杂机械行为和时间频率动态。


<details>
  <summary>Details</summary>
Motivation: 传统核函数（如基于指数的方法）在捕捉复杂系统行为时存在局限性，需要更灵活和多样化的核函数来满足代理建模的需求。

Method: 提出并扩展了多种核函数（如指数平方正弦核和有理二次核），并集成了它们的导数。框架还支持核函数的组合，以构建针对特定问题的复合模型。

Result: 框架在正弦基数测试案例、Mauna-Loa二氧化碳浓度预测和航空乘客流量预测中验证了有效性，并集成到开源工具SMT 2.0中。

Conclusion: 该框架为工程师和研究人员提供了灵活的工具集，为复杂频率敏感领域的代理建模开辟了新的应用方向。

Abstract: This paper introduces a comprehensive open-source framework for developing
correlation kernels, with a particular focus on user-defined and composition of
kernels for surrogate modeling. By advancing kernel-based modeling techniques,
we incorporate frequency-aware elements that effectively capture complex
mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.
Traditional kernel functions, often limited to exponential-based methods, are
extended to include a wider range of kernels such as exponential squared sine
and rational quadratic kernels, along with their respective firstand
second-order derivatives. The proposed methodologies are first validated on a
sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon
Dioxide (CO 2 ) concentrations and airline passenger traffic. All these
advancements are integrated into the open-source Surrogate Modeling Toolbox
(SMT 2.0), providing a versatile platform for both standard and customizable
kernel configurations. Furthermore, the framework enables the combination of
various kernels to leverage their unique strengths into composite models
tailored to specific problems. The resulting framework offers a flexible
toolset for engineers and researchers, paving the way for numerous future
applications in metamodeling for complex, frequency-sensitive domains.

</details>


### [215] [EPT-2 Technical Report](https://arxiv.org/abs/2507.09703)
*Roberto Molinaro,Niall Siegenheim,Niels Poulsen,Jordan Dane Daubinet,Henry Martin,Mark Frey,Kevin Thiart,Alexander Jakob Dautel,Andreas Schlueter,Alex Grigoryev,Bogdan Danciu,Nikoo Ekhtiari,Bas Steunebrink,Leonie Wagner,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: EPT-2是Earth Physics Transformer家族的最新AI模型，显著提升了地球系统预测性能，超越现有领先模型，并引入EPT-2e进行概率预测。


<details>
  <summary>Details</summary>
Motivation: 提升地球系统预测的准确性和效率，超越现有AI和数值天气预报系统。

Method: 基于Transformer架构的EPT-2模型，以及其扰动集合模型EPT-2e。

Result: EPT-2在0-240小时预测范围内表现优异，EPT-2e在概率预测上超越ECMWF ENS，且计算成本更低。

Conclusion: EPT-2和EPT-2e在地球系统预测中设定了新标准，并通过平台提供访问。

Abstract: We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)
family of foundation AI models for Earth system forecasting. EPT-2 delivers
substantial improvements over its predecessor, EPT-1.5, and sets a new state of
the art in predicting energy-relevant variables-including 10m and 100m wind
speed, 2m temperature, and surface solar radiation-across the full 0-240h
forecast horizon. It consistently outperforms leading AI weather models such as
Microsoft Aurora, as well as the operational numerical forecast system IFS HRES
from the European Centre for Medium-Range Weather Forecasts (ECMWF). In
parallel, we introduce a perturbation-based ensemble model of EPT-2 for
probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly
surpasses the ECMWF ENS mean-long considered the gold standard for medium- to
longrange forecasting-while operating at a fraction of the computational cost.
EPT models, as well as third-party forecasts, are accessible via the app.jua.ai
platform.

</details>


### [216] [Continental scale habitat modelling with artificial intelligence and multimodal earth observation](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Wilfried Thuiller*

Main category: cs.LG

TL;DR: 利用高分辨率遥感数据和AI工具改进大范围精细生境分类，通过多光谱和雷达影像结合，以及解决类别不平衡问题，显著提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 当前生境地图在主题和空间分辨率上不足，难以满足保护和恢复需求，需要更精确的分类方法。

Method: 结合高分辨率遥感数据、AI工具和多模型策略，利用欧洲植被档案数据建模EUNIS Level 3生境。

Result: 通过层次化生境命名和多源数据集成，显著提高了分类准确性，尤其在破碎化景观中。

Conclusion: 该方法可推广至其他地区，未来需关注动态生境的时间建模和更高分辨率数据的应用。

Abstract: Habitats integrate the abiotic conditions and biophysical structures that
support biodiversity and sustain nature's contributions to people. As these
ecosystems face mounting pressure from human activities, accurate,
high-resolution habitat maps are essential for effective conservation and
restoration. Yet current maps often fall short in thematic or spatial
resolution because they must (1) model several mutually exclusive habitat types
that co-occur across landscapes and (2) cope with severe class imbalance that
complicate multi-class training. Here, we evaluated how high-resolution remote
sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat
classification over large geographic extents at fine thematic resolution. Using
vegetation plots from the European Vegetation Archive, we modelled Level 3
EUNIS habitats across Europe and assessed multiple modelling strategies against
independent validation datasets. Strategies that exploited the hierarchical
nature of habitat nomenclatures resolved classification ambiguities, especially
in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic
aperture radar (SAR) imagery, particularly through Earth Observation Foundation
models, enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
accuracy further. Our methodological framework is transferable beyond Europe
and adaptable to other classification systems. Future research should advance
temporal modelling of dynamic habitats, extend to habitat segmentation and
quality assessment, and exploit next-generation EO data paired with
higher-quality in-situ observations.

</details>


### [217] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: 提出了一种基于边界条件数据直接学习物理定律的通用物理模拟基础AI模型，无需预先编码方程。


<details>
  <summary>Details</summary>
Motivation: 传统物理模拟方法（如PINNs和有限差分法）需要显式数学方程，限制了其通用性和发现潜力。本文旨在通过AI直接生成物理准确的稳态解，实现从AI加速物理到AI发现物理的范式转变。

Method: 采用草图引导的扩散变换器方法，将模拟视为条件生成问题，利用增强的扩散变换器架构和空间关系编码，直接从边界条件映射到稳态解。

Result: 模型能够直接生成稳态解（SSIM > 0.8），并保持亚像素级边界精度，同时通过LRP分析揭示物理关系。

Conclusion: 该工作首次实现了真正通用的物理模拟框架，标志着从AI加速物理到AI发现物理的转变。

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [218] [Do we need equivariant models for molecule generation?](https://arxiv.org/abs/2507.09753)
*Ewa M. Nowara,Joshua Rackers,Patricia Suriana,Pan Kessel,Max Shen,Andrew Martin Watkins,Michael Maser*

Main category: cs.LG

TL;DR: 研究探讨了非等变卷积神经网络（CNN）通过旋转增强训练是否能达到等变图神经网络（GNN）的性能，并分析了模型大小、数据集大小和训练时长对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有等变GNN模型复杂、难以训练且扩展性差，因此探索非等变CNN是否可以通过学习实现等变性。

Method: 使用旋转增强训练非等变CNN，并通过损失分解分离预测误差和等变误差，评估其在去噪、分子生成和性质预测中的表现。

Result: 研究发现非等变CNN可以通过训练学习等变性，并在某些任务中匹配等变模型的性能。

Conclusion: 非等变CNN通过旋转增强训练可以学习等变性，为分子生成任务提供了一种更简单高效的替代方案。

Abstract: Deep generative models are increasingly used for molecular discovery, with
most recent approaches relying on equivariant graph neural networks (GNNs)
under the assumption that explicit equivariance is essential for generating
high-quality 3D molecules. However, these models are complex, difficult to
train, and scale poorly.
  We investigate whether non-equivariant convolutional neural networks (CNNs)
trained with rotation augmentations can learn equivariance and match the
performance of equivariant models. We derive a loss decomposition that
separates prediction error from equivariance error, and evaluate how model
size, dataset size, and training duration affect performance across denoising,
molecule generation, and property prediction. To our knowledge, this is the
first study to analyze learned equivariance in generative tasks.

</details>


### [219] [Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts](https://arxiv.org/abs/2507.09754)
*Aakash Tripathi,Ian E. Nielsen,Muhammad Umer,Ravi P. Ramachandran,Ghulam Rasool*

Main category: cs.LG

TL;DR: 该研究提出了一种基于混合专家（MoE）的新型方法，用于预测转录因子结合位点（TFBS），结合多个预训练的CNN模型，并在分布内外数据集上验证其性能。MoE模型在OOD场景中表现优异，同时引入了一种新的解释性技术ShiftSmooth，提升了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 转录因子结合位点（TFBS）预测对理解基因调控至关重要，但现有方法在分布外数据上的泛化性和解释性不足。

Method: 采用混合专家（MoE）框架，整合多个预训练的CNN模型，并引入ShiftSmooth技术提升模型解释性。

Result: MoE模型在分布内外数据集上表现优异，尤其在OOD场景中显著优于单一专家模型。ShiftSmooth在解释性分析中优于传统方法。

Conclusion: 该研究提供了一种高效、泛化性强且可解释的TFBS预测方法，有望推动基因组生物学和转录调控的研究。

Abstract: Transcription Factor Binding Site (TFBS) prediction is crucial for
understanding gene regulation and various biological processes. This study
introduces a novel Mixture of Experts (MoE) approach for TFBS prediction,
integrating multiple pre-trained Convolutional Neural Network (CNN) models,
each specializing in different TFBS patterns. We evaluate the performance of
our MoE model against individual expert models on both in-distribution and
out-of-distribution (OOD) datasets, using six randomly selected transcription
factors (TFs) for OOD testing. Our results demonstrate that the MoE model
achieves competitive or superior performance across diverse TF binding sites,
particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)
statistical test confirms the significance of these performance differences.
Additionally, we introduce ShiftSmooth, a novel attribution mapping technique
that provides more robust model interpretability by considering small shifts in
input sequences. Through comprehensive explainability analysis, we show that
ShiftSmooth offers superior attribution for motif discovery and localization
compared to traditional Vanilla Gradient methods. Our work presents an
efficient, generalizable, and interpretable solution for TFBS prediction,
potentially enabling new discoveries in genome biology and advancing our
understanding of transcriptional regulation.

</details>


### [220] [Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights](https://arxiv.org/abs/2507.09766)
*Mohamadreza Akbari Pour,Ali Ghasemzadeh,MohamadAli Bijarchi,Mohammad Behshad Shafii*

Main category: cs.LG

TL;DR: 提出了一种结合物理监督与时空学习的新框架RGPD，用于剩余使用寿命（RUL）和健康状态（SOH）的准确估计，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在工业应用中，准确估计RUL和SOH对预测与健康管理至关重要，但现有方法在时空学习和物理约束结合方面存在不足。

Method: RGPD框架结合了图卷积循环网络（GCRN）、图注意力卷积（GATConv）和软性行动者-评论家（SAC）模块，动态加权物理约束以提高泛化能力。

Result: 在三个工业基准数据集上，RGPD在RUL和SOH估计任务中均优于现有方法，表现出强鲁棒性和预测准确性。

Conclusion: RGPD通过动态结合物理约束和时空学习，显著提升了工业系统中RUL和SOH的估计性能。

Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)
is essential for Prognostics and Health Management (PHM) across a wide range of
industrial applications. We propose a novel framework -- Reinforced Graph-Based
Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that
combines physics-based supervision with advanced spatio-temporal learning.
Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional
filters within recurrent units to capture how node representations evolve over
time. Graph Attention Convolution (GATConv) leverages a self-attention
mechanism to compute learnable, edge-wise attention coefficients, dynamically
weighting neighbor contributions for adaptive spatial aggregation. A Soft
Actor-Critic (SAC) module is positioned between the Temporal Attention Unit
(TAU) and GCRN to further improve the spatio-temporal learning. This module
improves attention and prediction accuracy by dynamically scaling hidden
representations to minimize noise and highlight informative features. To
identify the most relevant physical constraints in each area, Q-learning agents
dynamically assign weights to physics-informed loss terms, improving
generalization across real-time industrial systems and reducing the need for
manual tuning. In both RUL and SOH estimation tasks, the proposed method
consistently outperforms state-of-the-art models, demonstrating strong
robustness and predictive accuracy across varied degradation patterns across
three diverse industrial benchmark datasets.

</details>


### [221] [Knowing When to Quit: Probabilistic Early Exits for Speech Separation](https://arxiv.org/abs/2507.09768)
*Kenny Falkær Olsen. Mads Østergaard,Karl Ulbæk,Søren Føns Nielsen,Rasmus Malik Høegh Lindrup,Bjørn Sand Jensen,Morten Mørup*

Main category: cs.LG

TL;DR: 论文提出了一种支持早期退出的神经网络架构，用于语音分离，并结合概率框架动态调整计算资源，实现高性能和可解释的退出条件。


<details>
  <summary>Details</summary>
Motivation: 解决现有语音分离模型计算和参数固定、难以适应嵌入式设备资源变化的问题。

Method: 设计支持早期退出的神经网络架构，提出基于不确定性的概率框架，建模干净语音信号和误差方差，推导概率早期退出条件。

Result: 在语音分离和增强任务中，单一早期退出模型性能与多计算预算的先进模型相当。

Conclusion: 该框架实现了动态计算资源分配，同时保持高性能和可解释性，适用于嵌入式设备。

Abstract: In recent years, deep learning-based single-channel speech separation has
improved considerably, in large part driven by increasingly compute- and
parameter-efficient neural network architectures. Most such architectures are,
however, designed with a fixed compute and parameter budget, and consequently
cannot scale to varying compute demands or resources, which limits their use in
embedded and heterogeneous devices such as mobile phones and hearables. To
enable such use-cases we design a neural network architecture for speech
separation capable of early-exit, and we propose an uncertainty-aware
probabilistic framework to jointly model the clean speech signal and error
variance which we use to derive probabilistic early-exit conditions in terms of
desired signal-to-noise ratios. We evaluate our methods on both speech
separation and enhancement tasks, and we show that a single early-exit model
can be competitive with state-of-the-art models trained at many compute and
parameter budgets. Our framework enables fine-grained dynamic compute-scaling
of speech separation networks while achieving state-of-the-art performance and
interpretable exit conditions.

</details>


### [222] [Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow](https://arxiv.org/abs/2507.09785)
*Zhonglin Cao,Mario Geiger,Allan dos Santos Costa,Danny Reidenbach,Karsten Kreis,Tomas Geffner,Franco Pellegrini,Guoqing Zhou,Emine Kucukbenli*

Main category: cs.LG

TL;DR: 本文提出两种加速训练和推理的机制，用于3D分子构象生成的生成模型，包括SO(3)-Averaged Flow训练目标和流模型的reflow与蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 快速准确地生成分子构象对计算化学和药物发现任务至关重要，但现有扩散或流模型需要大量计算资源。

Method: 提出SO(3)-Averaged Flow训练目标以加速训练，并利用reflow和蒸馏方法实现快速推理。

Result: SO(3)-Averaged Flow训练模型达到最先进的构象生成质量，reflow和蒸馏方法实现高质量少步或一步生成。

Conclusion: 本文方法为基于流模型的高效分子构象生成提供了可行路径。

Abstract: Fast and accurate generation of molecular conformers is desired for
downstream computational chemistry and drug discovery tasks. Currently,
training and sampling state-of-the-art diffusion or flow-based models for
conformer generation require significant computational resources. In this work,
we build upon flow-matching and propose two mechanisms for accelerating
training and inference of generative models for 3D molecular conformer
generation. For fast training, we introduce the SO(3)-Averaged Flow training
objective, which leads to faster convergence to better generation quality
compared to conditional optimal transport flow or Kabsch-aligned flow. We
demonstrate that models trained using SO(3)-Averaged Flow can reach
state-of-the-art conformer generation quality. For fast inference, we show that
the reflow and distillation methods of flow-based models enable few-steps or
even one-step molecular conformer generation with high quality. The training
techniques proposed in this work show a path towards highly efficient molecular
conformer generation with flow-based models.

</details>


### [223] [Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster](https://arxiv.org/abs/2507.09786)
*Junaid Iqbal Khan*

Main category: cs.LG

TL;DR: 论文提出两种互补方法加速分类导向的近似机器遗忘（AMU）：Blend（一种新型分布匹配数据集压缩技术）和A-AMU（一种损失中心加速方法），显著减少遗忘延迟并保持模型效用和隐私。


<details>
  <summary>Details</summary>
Motivation: 现有AMU方法在处理保留数据集时计算开销大，且减少训练轮次仍具挑战性，因此需要更高效的解决方案。

Method: 1. Blend：通过视觉相似图像的合并和共享混合权重，显著减少保留集大小。2. A-AMU：通过增强遗忘目标（陡峭主损失和新型可微分正则化器）加速收敛。

Result: 实验表明，这种数据与损失中心优化的双重方法显著减少了端到端遗忘延迟，同时保持模型效用和隐私。

Conclusion: 这是首个通过联合设计专用数据集压缩技术和加速损失函数来系统解决遗忘效率的工作。

Abstract: Approximate machine unlearning (AMU) enables models to `forget' specific
training data through specialized fine-tuning on a retained dataset subset.
However, processing this retained subset still dominates computational runtime,
while reductions of epochs also remain a challenge. We propose two
complementary methods to accelerate classification-oriented AMU. First,
\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges
visually similar images with shared blend-weights to significantly reduce the
retained set size. It operates with minimal pre-processing overhead and is
orders of magnitude faster than state-of-the-art DC methods. Second, our
loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning
objective to quicken convergence. A-AMU achieves this by combining a steepened
primary loss to expedite forgetting with a novel, differentiable regularizer
that matches the loss distributions of forgotten and in-distribution unseen
data. Our extensive experiments demonstrate that this dual approach of data and
loss-centric optimization dramatically reduces end-to-end unlearning latency
across both single and multi-round scenarios, all while preserving model
utility and privacy. To our knowledge, this is the first work to systematically
tackle unlearning efficiency by jointly designing a specialized dataset
condensation technique with a dedicated accelerated loss function. Code is
available at https://github.com/algebraicdianuj/DC_Unlearning.

</details>


### [224] [A Scalable and Efficient Signal Integration System for Job Matching](https://arxiv.org/abs/2507.09797)
*Ping Liu,Rajat Arora,Xiao Shi,Benjamin Le,Qianqi Shen,Jianqiang Shen,Chengming Jiang,Nikita Zhiltsov,Priya Bannur,Yidan Zhu,Liming Dong,Haichao Wei,Qi Guo,Luke Simon,Liangjie Hong,Wenjing Zhang*

Main category: cs.LG

TL;DR: LinkedIn开发了STAR系统，结合LLM和GNN解决推荐系统中的冷启动、过滤气泡和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 解决LinkedIn在职业匹配推荐系统中面临的冷启动、过滤气泡和偏见等挑战。

Method: 结合大型语言模型（LLM）和图神经网络（GNN），利用自适应采样和版本管理等工业级范式。

Result: STAR系统提供了端到端的解决方案，支持大规模推荐系统中嵌入的开发与部署。

Conclusion: STAR系统为工业应用提供了高效的嵌入构建方法，并实现了高性能的推荐。

Abstract: LinkedIn, one of the world's largest platforms for professional networking
and job seeking, encounters various modeling challenges in building
recommendation systems for its job matching product, including cold-start,
filter bubbles, and biases affecting candidate-job matching. To address these,
we developed the STAR (Signal Integration for Talent And Recruiters) system,
leveraging the combined strengths of Large Language Models (LLMs) and Graph
Neural Networks (GNNs). LLMs excel at understanding textual data, such as
member profiles and job postings, while GNNs capture intricate relationships
and mitigate cold-start issues through network effects. STAR integrates diverse
signals by uniting LLM and GNN capabilities with industrial-scale paradigms
including adaptive sampling and version management. It provides an end-to-end
solution for developing and deploying embeddings in large-scale recommender
systems. Our key contributions include a robust methodology for building
embeddings in industrial applications, a scalable GNN-LLM integration for
high-performing recommendations, and practical insights for real-world model
deployment.

</details>


### [225] [Federated Learning with Graph-Based Aggregation for Traffic Forecasting](https://arxiv.org/abs/2507.09805)
*Audri Banik,Glaucio Haroldo Silva de Carvalho,Renata Dividino*

Main category: cs.LG

TL;DR: 提出了一种轻量级的图感知联邦学习方法，结合了FedAvg的简单性和图学习的关键思想，有效捕捉空间关系并保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 在交通预测中，空间关系对性能至关重要，但传统联邦学习方法（如FedAvg）忽略了这种关系，而基于图的方法计算开销大。

Method: 采用基本的邻域聚合原则指导参数更新，基于图连通性对客户端模型进行加权，从而捕捉空间关系。

Result: 在METR-LA和PEMS-BAY数据集上验证，性能优于标准基线和近期基于图的联邦学习方法。

Conclusion: 该方法在保持计算效率的同时，有效提升了交通预测的性能。

Abstract: In traffic prediction, the goal is to estimate traffic speed or flow in
specific regions or road segments using historical data collected by devices
deployed in each area. Each region or road segment can be viewed as an
individual client that measures local traffic flow, making Federated Learning
(FL) a suitable approach for collaboratively training models without sharing
raw data. In centralized FL, a central server collects and aggregates model
updates from multiple clients to build a shared model while preserving each
client's data privacy. Standard FL methods, such as Federated Averaging
(FedAvg), assume that clients are independent, which can limit performance in
traffic prediction tasks where spatial relationships between clients are
important. Federated Graph Learning methods can capture these dependencies
during server-side aggregation, but they often introduce significant
computational overhead. In this paper, we propose a lightweight graph-aware FL
approach that blends the simplicity of FedAvg with key ideas from graph
learning. Rather than training full models, our method applies basic
neighbourhood aggregation principles to guide parameter updates, weighting
client models based on graph connectivity. This approach captures spatial
relationships effectively while remaining computationally efficient. We
evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,
and show that it achieves competitive performance compared to standard
baselines and recent graph-based federated learning techniques.

</details>


### [226] [Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem](https://arxiv.org/abs/2507.09816)
*Adam Newgas*

Main category: cs.LG

TL;DR: 研究了神经网络在计算中的叠加现象，发现实际学习到的电路与理论构造不同，具有高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络是否能在实践中学习到高效的叠加计算电路，而非仅存储特征。

Method: 使用一个玩具模型（Universal-AND问题），限制隐藏维度以迫使模型找到计算高效的电路。

Result: 训练过程发现了一种简单、全密集的解决方案，比理论构造更高效，且具有鲁棒性和扩展性。

Conclusion: 研究揭示了神经网络偏好的电路类型，为网络结构和可解释性提供了新见解。

Abstract: Neural networks are capable of superposition -- representing more features
than there are dimensions. Recent work considers the analogous concept for
computation instead of storage, proposing theoretical constructions. But there
has been little investigation into whether these circuits can be learned in
practice. In this work, we investigate a toy model for the Universal-AND
problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs.
The hidden dimension that determines the number of non-linear activations is
restricted to pressure the model to find a compute-efficient circuit, called
compressed computation. We find that the training process finds a simple
solution that does not correspond to theoretical constructions. It is fully
dense -- every neuron contributes to every output. The solution circuit
naturally scales with dimension, trading off error rates for neuron efficiency.
It is similarly robust to changes in sparsity and other key parameters, and
extends naturally to other boolean operations and boolean circuits. We explain
the found solution in detail and compute why it is more efficient than the
theoretical constructions at low sparsity. Our findings shed light on the types
of circuits that models like to form and the flexibility of the superposition
representation. This contributes to a broader understanding of network
circuitry and interpretability.

</details>


### [227] [Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification](https://arxiv.org/abs/2507.09826)
*Jintao Qu,Zichong Wang,Chenhao Wu,Wenbin Zhang*

Main category: cs.LG

TL;DR: 提出一种结合动态时间规整（DTW）和神经网络的模型，适应冷启动和丰富数据场景，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在冷启动场景中依赖大量标注数据和缺乏可解释性的问题，同时改进DTW在丰富数据场景中的性能。

Method: 提出动态长度缩短算法，将时间序列转换为原型，并将DTW的递归关系转化为等效的循环神经网络。

Result: 在多个基准时间序列分类任务中，模型在低资源场景显著优于先前方法，在丰富资源场景仍具竞争力。

Conclusion: 该模型兼具DTW的可解释性和神经网络的可训练性，适用于不同数据资源场景。

Abstract: Neural networks have achieved remarkable success in time series
classification, but their reliance on large amounts of labeled data for
training limits their applicability in cold-start scenarios. Moreover, they
lack interpretability, reducing transparency in decision-making. In contrast,
dynamic time warping (DTW) combined with a nearest neighbor classifier is
widely used for its effectiveness in limited-data settings and its inherent
interpretability. However, as a non-parametric method, it is not trainable and
cannot leverage large amounts of labeled data, making it less effective than
neural networks in rich-resource scenarios. In this work, we aim to develop a
versatile model that adapts to cold-start conditions and becomes trainable with
labeled data, while maintaining interpretability. We propose a dynamic
length-shortening algorithm that transforms time series into prototypes while
preserving key structural patterns, thereby enabling the reformulation of the
DTW recurrence relation into an equivalent recurrent neural network. Based on
this, we construct a trainable model that mimics DTW's alignment behavior. As a
neural network, it becomes trainable when sufficient labeled data is available,
while still retaining DTW's inherent interpretability. We apply the model to
several benchmark time series classification tasks and observe that it
significantly outperforms previous approaches in low-resource settings and
remains competitive in rich-resource settings.

</details>


### [228] [Generative Cognitive Diagnosis](https://arxiv.org/abs/2507.09831)
*Jiatong Li,Qi Liu,Mengxiao Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成式认知诊断范式，通过生成建模实现认知状态的归纳推理，解决了传统模型需要重新训练和可靠性低的问题。


<details>
  <summary>Details</summary>
Motivation: 传统认知诊断模型无法对新学习者进行即时诊断且可靠性有限，需要一种更高效、可靠的方法。

Method: 提出了生成式诊断范式，并实例化为G-IRT和G-NCDM两种模型，通过生成过程分离认知状态推断和响应预测。

Result: 实验表明，新方法在可扩展性和可靠性上表现优异，诊断速度提升100倍。

Conclusion: 生成式认知诊断为人工智能中的认知诊断应用开辟了新途径，尤其在智能模型评估和教育系统中具有潜力。

Abstract: Cognitive diagnosis (CD) models latent cognitive states of human learners by
analyzing their response patterns on diagnostic tests, serving as a crucial
machine learning technique for educational assessment and evaluation.
Traditional cognitive diagnosis models typically follow a transductive
prediction paradigm that optimizes parameters to fit response scores and
extract learner abilities. These approaches face significant limitations as
they cannot perform instant diagnosis for new learners without computationally
expensive retraining and produce diagnostic outputs with limited reliability.
In this study, we introduces a novel generative diagnosis paradigm that
fundamentally shifts CD from predictive to generative modeling, enabling
inductive inference of cognitive states without parameter re-optimization. We
propose two simple yet effective instantiations of this paradigm: Generative
Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model
(G-NCDM), which achieve excellent performance improvements over traditional
methods. The generative approach disentangles cognitive state inference from
response prediction through a well-designed generation process that
incorporates identifiability and monotonicity conditions. Extensive experiments
on real-world datasets demonstrate the effectiveness of our methodology in
addressing scalability and reliability challenges, especially $\times 100$
speedup for the diagnosis of new learners. Our framework opens new avenues for
cognitive diagnosis applications in artificial intelligence, particularly for
intelligent model evaluation and intelligent education systems. The code is
available at https://github.com/CSLiJT/Generative-CD.git.

</details>


### [229] [A Pre-training Framework for Relational Data with Information-theoretic Principles](https://arxiv.org/abs/2507.09837)
*Quang Truong,Zhikai Chen,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: 论文提出了一种名为Task Vector Estimation (TVE)的新型预训练框架，用于解决关系数据库中任务异构性带来的预训练挑战。


<details>
  <summary>Details</summary>
Motivation: 关系数据库在多个领域支撑关键基础设施，但由于任务异构性（如关系模式图、时间依赖性和SQL定义的标签逻辑），设计通用的预训练策略仍具挑战性。

Method: TVE通过基于模式遍历图的集合聚合构建预测性监督信号，显式建模下一窗口的关系动态。

Result: 在RelBench基准测试中，TVE始终优于传统预训练基线。

Conclusion: 研究结果表明，预训练目标应编码任务异构性和时间结构，作为关系数据库预测建模的设计原则。

Abstract: Relational databases underpin critical infrastructure across a wide range of
domains, yet the design of generalizable pre-training strategies for learning
from relational databases remains an open challenge due to task heterogeneity.
Specifically, there exist infinitely many possible downstream tasks, as tasks
are defined based on relational schema graphs, temporal dependencies, and
SQL-defined label logics. An effective pre-training framework is desired to
take these factors into account in order to obtain task-aware representations.
By incorporating knowledge of the underlying distribution that drives label
generation, downstream tasks can benefit from relevant side-channel
information. To bridge this gap, we introduce Task Vector Estimation (TVE), a
novel pre-training framework that constructs predictive supervisory signals via
set-based aggregation over schema traversal graphs, explicitly modeling
next-window relational dynamics. We formalize our approach through an
information-theoretic lens, demonstrating that task-informed representations
retain more relevant signals than those obtained without task priors. Extensive
experiments on the RelBench benchmark show that TVE consistently outperforms
traditional pre-training baselines. Our findings advocate for pre-training
objectives that encode task heterogeneity and temporal structure as design
principles for predictive modeling on relational databases.

</details>


### [230] [Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs](https://arxiv.org/abs/2507.09839)
*MohammadReza Davari,Utkarsh Garg,Weixin Cai,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的自动提示优化（APO）框架，通过引入正负反馈强化和反馈多样化技术，显著提升了提示优化的效果和效率，并解决了不同模型间提示迁移的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法主要关注错误修正，忽视了正确预测中的有用信息，限制了其效果和效率。此外，不同模型版本或API提供商间的提示迁移问题尚未解决。

Method: 提出了一种基于增强反馈机制的APO框架，包括正负反馈强化和反馈多样化技术，并引入了持续提示优化（CPO）以解决提示迁移问题。

Result: 实验表明，该方法在准确率、收敛速度和计算成本上均优于基线方法，尤其在提示迁移场景中表现突出。

Conclusion: 通过结合正负反馈和反馈多样化，该方法显著提升了提示优化的效果，并有效解决了模型间提示迁移的挑战。

Abstract: An increasing number of NLP applications interact with large language models
(LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO)
methods iteratively refine prompts using model-generated feedback, textual
gradients, they primarily focus on error correction and neglect valuable
insights from correct predictions. This limits both their effectiveness and
efficiency. In this paper, we propose a novel APO framework centered on
enhancing the feedback mechanism. We reinterpret the textual gradient as a form
of negative reinforcement and introduce the complementary positive
reinforcement to explicitly preserve beneficial prompt components identified
through successful predictions. To mitigate the noise inherent in LLM-generated
feedback, we introduce a technique called feedback diversification, which
aggregates multiple feedback signals, emphasizing consistent, actionable advice
while filtering out outliers. Motivated by the rapid evolution and diversity of
available LLMs, we also formalize Continual Prompt Optimization (CPO),
addressing the practical challenge of efficiently migrating optimized prompts
between different model versions or API providers. Our experiments reveal that
naive prompt migration often degrades performance due to loss of critical
instructions. In contrast, our approach consistently outperforms strong
baselines, achieving significant accuracy improvements, faster convergence, and
lower computational costs in both standard and migration scenarios.

</details>


### [231] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文提出了一种改进的Schedule-Free (SF)方法，用于大规模语言模型训练，无需显式衰减阶段或额外内存开销，并通过理论和实证分析验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着模型和数据集的规模迅速扩大，传统的预训练策略（如固定计算预算的余弦学习率调度）已不足以应对大规模训练需求。现有方法如WSD和权重平均虽有一定改进，但仍存在局限性。

Method: 重新审视了Schedule-Free (SF)方法，并通过理论和实证分析揭示了其隐式权重平均的特性。在此基础上，提出了一种改进的SF变体，增强了对动量的鲁棒性，并优化了大批量训练的性能。

Result: 改进后的SF方法在无需显式衰减阶段或额外内存开销的情况下，有效导航了损失景观的“河流”结构，适用于持续扩展的训练任务。

Conclusion: SF方法是一种实用、可扩展且理论支持的语言模型训练方法，改进后的版本进一步提升了其鲁棒性和性能。

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [232] [Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks](https://arxiv.org/abs/2507.09871)
*Niket Patel,Randall Balestriero*

Main category: cs.LG

TL;DR: 论文提出了一种基于任务先验的概率空间框架，用于评估模型在所有可能下游任务上的性能，解决了当前固定评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究依赖于固定的下游基准测试，限制了评估的全面性，作者希望通过任务先验的概率空间提供更灵活的评估标准。

Method: 通过定义任务先验和任务分布，构建了一个概率空间，用于评估模型在所有可能下游任务上的平均性能和方差。

Result: 该框架首次能够回答模型在所有任务上的平均性能和方差问题，为SSL研究提供了新的评估标准。

Conclusion: 任务先验框架不仅改进了评估方法，还可能加速SSL研究的进展。

Abstract: The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.

</details>


### [233] [AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications](https://arxiv.org/abs/2507.09882)
*Jiamin Wu,Zichen Ren,Junyu Wang,Pengyu Zhu,Yonghao Song,Mianxin Liu,Qihao Zheng,Lei Bai,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: AdaBrain-Bench是一个标准化的大规模基准测试，用于评估非侵入式脑机接口（BCI）中的基础模型，涵盖7个关键应用，并提供多维评估指标和工具。


<details>
  <summary>Details</summary>
Motivation: 非侵入式BCI信号的高噪声和有限任务数据限制了解码能力，且缺乏评估公共基础模型的实用基准。

Method: 提出AdaBrain-Bench，一个包含多样化BCI数据集、任务适应流程和多维评估的基准框架。

Result: 评估了多个公开的基础模型，提供了模型选择的最佳实践，并开源了基准流程。

Conclusion: AdaBrain-Bench为促进稳健和通用的神经解码解决方案提供了持续演进的平台。

Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible
means of connecting the human brain to external devices, with broad
applications in home and clinical settings to enhance human capabilities.
However, the high noise level and limited task-specific data in non-invasive
signals constrain decoding capabilities. Recently, the adoption of
self-supervised pre-training is transforming the landscape of non-invasive BCI
research, enabling the development of brain foundation models to capture
generic neural representations from large-scale unlabeled
electroencephalography (EEG) signals with substantial noises. However, despite
these advances, the field currently lacks comprehensive, practical and
extensible benchmarks to assess the utility of the public foundation models
across diverse BCI tasks, hindering their widespread adoption. To address this
challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to
systematically evaluate brain foundation models in widespread non-invasive BCI
tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI
decoding datasets spanning 7 key applications. It introduces a streamlined task
adaptation pipeline integrated with multi-dimensional evaluation metrics and a
set of adaptation tools. The benchmark delivers an inclusive framework for
assessing generalizability of brain foundation models across key transfer
settings, including cross-subject, multi-subject, and few-shot scenarios. We
leverage AdaBrain-Bench to evaluate a suite of publicly available brain
foundation models and offer insights into practices for selecting appropriate
models in various scenarios. We make our benchmark pipeline available to enable
reproducible research and external use, offering a continuously evolving
platform to foster progress toward robust and generalized neural decoding
solutions.

</details>


### [234] [TolerantECG: A Foundation Model for Imperfect Electrocardiogram](https://arxiv.org/abs/2507.09887)
*Huynh Nguyen Dang,Thang Pham,Ngan Le,Van Nguyen*

Main category: cs.LG

TL;DR: TolerantECG是一种针对ECG信号的基础模型，能够处理噪声和部分导联缺失的情况，通过对比学习和自监督学习框架训练，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: ECG信号在诊断心脏疾病中至关重要，但噪声或导联缺失可能导致诊断错误或不确定性，因此需要一种更鲁棒的模型。

Method: 结合对比学习和自监督学习框架，训练模型学习ECG信号表示及其文本报告描述，同时处理噪声和导联缺失信号。

Result: 在PTB-XL数据集和MIT-BIH心律失常数据库中，TolerantECG在多种条件下表现最佳或次佳。

Conclusion: TolerantECG是一种高效且鲁棒的ECG信号处理模型，适用于噪声和导联缺失场景。

Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing
heart diseases. However, its effectiveness can be compromised by noise or
unavailability of one or more leads of the standard 12-lead recordings,
resulting in diagnostic errors or uncertainty. To address these challenges, we
propose TolerantECG, a foundation model for ECG signals that is robust to noise
and capable of functioning with arbitrary subsets of the standard 12-lead ECG.
TolerantECG training combines contrastive and self-supervised learning
frameworks to jointly learn ECG signal representations alongside their
corresponding knowledge-retrieval-based text report descriptions and corrupted
or lead-missing signals. Comprehensive benchmarking results demonstrate that
TolerantECG consistently ranks as the best or second-best performer across
various ECG signal conditions and class levels in the PTB-XL dataset, and
achieves the highest performance on the MIT-BIH Arrhythmia Database.

</details>


### [235] [NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting](https://arxiv.org/abs/2507.09888)
*Huibo Xu,Likang Wu,Xianquan Wang,Haoning Dang,Chun-Wun Cheng,Angelica I Aviles-Rivero,Qi Liu*

Main category: cs.LG

TL;DR: NeuTSFlow是一种新颖的时间序列预测框架，通过将时间序列视为连续函数族的噪声样本，利用神经算子学习函数族之间的转换路径。


<details>
  <summary>Details</summary>
Motivation: 传统方法将时间序列视为离散序列，忽略了其作为连续过程噪声样本的本质，导致预测效果受限。

Method: 提出NeuTSFlow框架，利用神经算子进行流匹配，参数化无限维函数空间中的速度场，直接建模函数级特征。

Result: 实验表明，NeuTSFlow在多种预测任务中表现出更高的准确性和鲁棒性。

Conclusion: 函数族视角为时间序列预测提供了更有效的框架，NeuTSFlow验证了这一视角的优越性。

Abstract: Time series forecasting is a fundamental task with broad applications, yet
conventional methods often treat data as discrete sequences, overlooking their
origin as noisy samples of continuous processes. Crucially, discrete noisy
observations cannot uniquely determine a continuous function; instead, they
correspond to a family of plausible functions. Mathematically, time series can
be viewed as noisy observations of a continuous function family governed by a
shared probability measure. Thus, the forecasting task can be framed as
learning the transition from the historical function family to the future
function family. This reframing introduces two key challenges: (1) How can we
leverage discrete historical and future observations to learn the relationships
between their underlying continuous functions? (2) How can we model the
transition path in function space from the historical function family to the
future function family? To address these challenges, we propose NeuTSFlow, a
novel framework that leverages Neural Operators to facilitate flow matching for
learning path of measure between historical and future function families. By
parameterizing the velocity field of the flow in infinite-dimensional function
spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies
at discrete points, directly modeling function-level features instead.
Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior
accuracy and robustness, validating the effectiveness of the function-family
perspective.

</details>


### [236] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种名为scSGC的软图聚类方法，用于单细胞RNA测序数据，通过非二值边权重更准确地表征细胞间的连续相似性，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的单细胞RNA测序聚类方法（如图神经网络）依赖硬图构建，导致信息丢失和聚类偏差。scSGC旨在解决这些问题。

Method: scSGC包含三个核心组件：基于ZINB的特征自编码器、双通道切割信息软图嵌入模块和基于最优传输的聚类优化模块。

Result: 在十个数据集上的实验表明，scSGC在聚类准确性、细胞类型注释和计算效率上优于13种先进模型。

Conclusion: scSGC在单细胞RNA测序数据分析中具有显著潜力，有助于深入理解细胞异质性。

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [237] [Algorithm Development in Neural Networks: Insights from the Streaming Parity Task](https://arxiv.org/abs/2507.09897)
*Loek van Rossem,Andrew M. Saxe*

Main category: cs.LG

TL;DR: 论文研究了深度神经网络在过参数化情况下仍能泛化的现象，特别是RNN在流式奇偶任务中的学习动态，揭示了无限泛化的机制。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在训练集之外数据上的泛化能力，特别是无限泛化的现象。

Method: 通过研究RNN在流式奇偶任务中的学习动态，提出有效理论分析算法开发过程。

Result: 发现RNN在有限训练后会出现完美无限泛化的相变，并通过有效理论揭示了隐式表示合并效应。

Conclusion: 揭示了神经网络从有限训练经验中实现无限泛化的一种机制。

Abstract: Even when massively overparameterized, deep neural networks show a remarkable
ability to generalize. Research on this phenomenon has focused on
generalization within distribution, via smooth interpolation. Yet in some
settings neural networks also learn to extrapolate to data far beyond the
bounds of the original training set, sometimes even allowing for infinite
generalization, implying that an algorithm capable of solving the task has been
learned. Here we undertake a case study of the learning dynamics of recurrent
neural networks (RNNs) trained on the streaming parity task in order to develop
an effective theory of algorithm development. The streaming parity task is a
simple but nonlinear task defined on sequences up to arbitrary length. We show
that, with sufficient finite training experience, RNNs exhibit a phase
transition to perfect infinite generalization. Using an effective theory for
the representational dynamics, we find an implicit representational merger
effect which can be interpreted as the construction of a finite automaton that
reproduces the task. Overall, our results disclose one mechanism by which
neural networks can generalize infinitely from finite training experience.

</details>


### [238] [Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model](https://arxiv.org/abs/2507.09925)
*Md Ahsanul Kabir,Abrar Jahin,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 提出了一种结合依赖树和Transformer模型的方法DepBERT，用于提取句子中的因果关系短语，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法未充分利用依赖树等语言学工具，限制了因果关系提取的效果。

Method: 扩展Transformer模型，将句子的依赖树整合到模型框架中，形成DepBERT。

Result: 在三个数据集上的实验表明，DepBERT优于现有监督方法。

Conclusion: DepBERT通过结合依赖树和深度学习模型，显著提升了因果关系提取的性能。

Abstract: Extracting cause and effect phrases from a sentence is an important NLP task,
with numerous applications in various domains, including legal, medical,
education, and scientific research. There are many unsupervised and supervised
methods proposed for solving this task. Among these, unsupervised methods
utilize various linguistic tools, including syntactic patterns, dependency
tree, dependency relations, etc. among different sentential units for
extracting the cause and effect phrases. On the other hand, the contemporary
supervised methods use various deep learning based mask language models
equipped with a token classification layer for extracting cause and effect
phrases. Linguistic tools, specifically, dependency tree, which organizes a
sentence into different semantic units have been shown to be very effective for
extracting semantic pairs from a sentence, but existing supervised methods do
not have any provision for utilizing such tools within their model framework.
In this work, we propose DepBERT, which extends a transformer-based model by
incorporating dependency tree of a sentence within the model framework.
Extensive experiments over three datasets show that DepBERT is better than
various state-of-the art supervised causality extraction methods.

</details>


### [239] [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
*Yoon Pyo Lee*

Main category: cs.LG

TL;DR: 本文提出了一种解释大型语言模型（LLM）在核工程领域推理过程的新方法，通过微调技术和神经元沉默实验验证了特定神经元对任务性能的关键作用。


<details>
  <summary>Details</summary>
Motivation: 为了在安全关键领域（如核工程）中部署LLM，需要理解其内部推理过程，以满足核监管框架的要求。

Method: 采用低秩适应技术微调通用LLM（Gemma-3-1b-it），通过神经元激活模式分析和沉默技术探究其行为变化。

Result: 沉默特定神经元组会显著降低任务性能，而单独沉默单个神经元影响不显著。定性分析显示这些神经元对生成准确技术信息至关重要。

Conclusion: 该方法为增强LLM透明度提供了具体路径，有助于实现核级AI验证，推动其在安全关键领域的应用。

Abstract: The integration of Large Language Models (LLMs) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an LLM encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose LLM
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
sparse set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.

</details>


### [240] [Memorization Sinks: Isolating Memorization during LLM Training](https://arxiv.org/abs/2507.09937)
*Gaurav R. Ghosal,Pratyush Maini,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 论文提出MemSinks方法，通过设计隔离记忆内容，解决大语言模型记忆重复序列的隐私和版权问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易记忆重复序列，引发隐私和版权担忧，现有后处理方法效果有限。

Method: 提出MemSinks范式，利用序列标识符激活特定记忆神经元，隔离记忆内容。

Result: 在十亿参数和十亿标记规模上实现有效隔离和强泛化能力。

Conclusion: MemSinks首次证明在真实数据上同时实现泛化和隔离是可行的。

Abstract: Large language models are susceptible to memorizing repeated sequences,
posing privacy and copyright concerns. A popular mitigation strategy is to
remove memorized information from specific neurons post-hoc. However, such
approaches have shown limited success so far. In a controlled setting, we show
that the memorization of natural sequences (those that resemble linguistically
plausible text) become mechanistically entangled with general language
abilities, thereby becoming challenging to remove post-hoc. In this work, we
put forward a new paradigm of MemSinks that promotes isolation of memorization
by design. We leverage a sequence identifier that activates a unique set of
memorization neurons for each sequence across repetitions. By analyzing the
dynamics of learning and forgetting, we argue that MemSinks facilitates
isolation of memorized content, making it easier to remove without compromising
general language capabilities. We implement MemSinks at the billion-parameter
and billion-token scale, and observe both effective isolation and strong
generalization. To our knowledge, this is the first proof-of-concept on real
data demonstrating that simultaneous generalization and isolation is
achievable. We open-source our code at http://github.com/grghosal/MemSinks.

</details>


### [241] [Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training](https://arxiv.org/abs/2507.09940)
*Taigo Sakai,Kazuhiro Hotta*

Main category: cs.LG

TL;DR: 论文提出一种动态调整神经元数量的方法，通过训练过程中增加和删除神经元来提升少数类别的识别性能，最终网络结构不变。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习神经元数量固定，而生物学的启发表明动态调整神经元数量可能提升性能，尤其是在类别不平衡的数据集上。

Method: 在训练过程中周期性增减神经元，动态调整容量，同时保留多数类别的关键特征。

Result: 在三个数据集和五种模型上的实验表明，该方法优于固定大小网络，并与其他不平衡处理技术结合时效果更佳。

Conclusion: 动态、受生物学启发的网络设计能有效提升类别不平衡数据的性能。

Abstract: In conventional deep learning, the number of neurons typically remains fixed
during training. However, insights from biology suggest that the human
hippocampus undergoes continuous neuron generation and pruning of neurons over
the course of learning, implying that a flexible allocation of capacity can
contribute to enhance performance. Real-world datasets often exhibit class
imbalance situations where certain classes have far fewer samples than others,
leading to significantly reduce recognition accuracy for minority classes when
relying on fixed size networks.To address the challenge, we propose a method
that periodically adds and removes neurons during training, thereby boosting
representational power for minority classes. By retaining critical features
learned from majority classes while selectively increasing neurons for
underrepresented classes, our approach dynamically adjusts capacity during
training. Importantly, while the number of neurons changes throughout training,
the final network size and structure remain unchanged, ensuring efficiency and
compatibility with deployment.Furthermore, by experiments on three different
datasets and five representative models, we demonstrate that the proposed
method outperforms fixed size networks and shows even greater accuracy when
combined with other imbalance-handling techniques. Our results underscore the
effectiveness of dynamic, biologically inspired network designs in improving
performance on class-imbalanced data.

</details>


### [242] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: Iceberg通过合成数据增强和弱标签生成方法，显著提升了深度学习模型在HLS预测中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在HLS硬件设计预测中泛化能力不足的问题。

Method: 采用合成数据增强（Iceberg）和弱标签生成方法，结合上下文模型架构进行元学习。

Result: 在六种实际应用中，建模准确率几何平均提升86.4%，离线DSE性能提升2.47倍和1.12倍。

Conclusion: Iceberg有效提升了模型的泛化能力，适用于实际应用中的少样本学习。

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [243] [Hierarchical Job Classification with Similarity Graph Integration](https://arxiv.org/abs/2507.09949)
*Md Ahsanul Kabir,Kareem Abdelfatah,Mohammed Korayem,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 提出了一种新的表示学习和分类模型，用于在线招聘中的职位分类，通过嵌入分层行业类别到潜在空间，显著提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 在线招聘中职位分类的复杂性增加，传统方法无法充分利用行业类别的分层结构，导致分类效果不佳。

Method: 结合标准职业分类系统（SOC）和内部分层分类法Carotene，将职位和分层行业类别嵌入潜在空间，捕捉图和分层关系。

Result: 在大规模职位发布数据集上的实验表明，该模型显著优于现有方法，能够有效利用分层结构和丰富语义特征。

Conclusion: 该研究为提升职位分类准确性提供了稳健框架，支持招聘行业更明智的决策。

Abstract: In the dynamic realm of online recruitment, accurate job classification is
paramount for optimizing job recommendation systems, search rankings, and labor
market analyses. As job markets evolve, the increasing complexity of job titles
and descriptions necessitates sophisticated models that can effectively
leverage intricate relationships within job data. Traditional text
classification methods often fall short, particularly due to their inability to
fully utilize the hierarchical nature of industry categories. To address these
limitations, we propose a novel representation learning and classification
model that embeds jobs and hierarchical industry categories into a latent
embedding space. Our model integrates the Standard Occupational Classification
(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both
graph and hierarchical relationships, thereby improving classification
accuracy. By embedding hierarchical industry categories into a shared latent
space, we tackle cold start issues and enhance the dynamic matching of
candidates to job opportunities. Extensive experimentation on a large-scale
dataset of job postings demonstrates the model's superior ability to leverage
hierarchical structures and rich semantic features, significantly outperforming
existing methods. This research provides a robust framework for improving job
classification accuracy, supporting more informed decision-making in the
recruitment industry.

</details>


### [244] [Radial Neighborhood Smoothing Recommender System](https://arxiv.org/abs/2507.09952)
*Zerui Zhang,Yumou Qiu*

Main category: cs.LG

TL;DR: 论文提出了一种基于潜在空间中距离估计的新方法，通过行和列距离近似潜在空间距离，并引入方差校正和径向邻域估计器（RNE）来提高推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统在潜在空间中具有低秩结构，但如何有效定义和测量潜在空间中的距离以捕捉用户和物品关系是一个关键挑战。

Method: 通过行和列距离近似潜在空间距离，引入方差校正减少噪声影响，提出RNE构建邻域并通过局部核回归提高填充准确性。

Result: 在模拟和真实数据集上验证，RNE优于现有协同过滤和矩阵分解方法，并能缓解冷启动问题。

Conclusion: RNE为潜在空间距离估计提供了结构化方法，显著提升了推荐系统性能，并具有解决冷启动问题的潜力。

Abstract: Recommender systems inherently exhibit a low-rank structure in latent space.
A key challenge is to define meaningful and measurable distances in the latent
space to capture user-user, item-item, user-item relationships effectively. In
this work, we establish that distances in the latent space can be
systematically approximated using row-wise and column-wise distances in the
observed matrix, providing a novel perspective on distance estimation. To
refine the distance estimation, we introduce the correction based on empirical
variance estimator to account for noise-induced non-centrality. The novel
distance estimation enables a more structured approach to constructing
neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which
constructs neighborhoods by including both overlapped and partially overlapped
user-item pairs and employs neighborhood smoothing via localized kernel
regression to improve imputation accuracy. We provide the theoretical
asymptotic analysis for the proposed estimator. We perform evaluations on both
simulated and real-world datasets, demonstrating that RNE achieves superior
performance compared to existing collaborative filtering and matrix
factorization methods. While our primary focus is on distance estimation in
latent space, we find that RNE also mitigates the ``cold-start'' problem.

</details>


### [245] [Rethinking Inductive Bias in Geographically Neural Network Weighted Regression](https://arxiv.org/abs/2507.09958)
*Zhenyuan Chen*

Main category: cs.LG

TL;DR: 论文探讨了地理神经网络加权回归（GNNWR）中的归纳偏置问题，提出通过结合卷积神经网络、循环神经网络和Transformer来改进模型，实验证明其在复杂空间关系建模中的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有GNNWR方法在建模空间非平稳性时受限于固定的距离方案和有限的归纳偏置，需要更灵活的模型来捕捉复杂空间关系。

Method: 通过引入卷积神经网络、循环神经网络和Transformer的局部感受野、序列上下文和自注意力机制，扩展GNNWR模型。

Result: 实验表明GNNWR在非线性复杂空间关系建模中优于传统方法，且模型性能受数据特征影响显著。

Conclusion: 归纳偏置对空间建模至关重要，未来可探索可学习的空间加权函数、混合神经架构及提升非平稳空间数据模型的解释性。

Abstract: Inductive bias is a key factor in spatial regression models, determining how
well a model can learn from limited data and capture spatial patterns. This
work revisits the inductive biases in Geographically Neural Network Weighted
Regression (GNNWR) and identifies limitations in current approaches for
modeling spatial non-stationarity. While GNNWR extends traditional
Geographically Weighted Regression by using neural networks to learn spatial
weighting functions, existing implementations are often restricted by fixed
distance-based schemes and limited inductive bias. We propose to generalize
GNNWR by incorporating concepts from convolutional neural networks, recurrent
neural networks, and transformers, introducing local receptive fields,
sequential context, and self-attention into spatial regression. Through
extensive benchmarking on synthetic spatial datasets with varying
heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic
methods in capturing nonlinear and complex spatial relationships. Our results
also reveal that model performance depends strongly on data characteristics,
with local models excelling in highly heterogeneous or small-sample scenarios,
and global models performing better with larger, more homogeneous data. These
findings highlight the importance of inductive bias in spatial modeling and
suggest future directions, including learnable spatial weighting functions,
hybrid neural architectures, and improved interpretability for models handling
non-stationary spatial data.

</details>


### [246] [Text-Driven Causal Representation Learning for Source-Free Domain Generalization](https://arxiv.org/abs/2507.09961)
*Lihua Zhou,Mao Ye,Nianxin Li,Shuaifeng Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.LG

TL;DR: TDCRL是一种结合因果推理的源自由域泛化方法，通过文本驱动生成视觉表示并提取域不变特征，显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统域泛化方法数据需求高的问题，以及现有源自由域泛化方法在域特定混淆因素上的局限性。

Method: 分两步：1）通过数据增强生成风格词向量，结合类别信息生成文本嵌入模拟视觉表示；2）训练因果干预网络提取域不变特征。

Result: 在多个数据集（PACS、VLCS等）上实现最先进性能。

Conclusion: TDCRL通过因果学习机制有效提取域不变特征，显著提升源自由域泛化的鲁棒性。

Abstract: Deep learning often struggles when training and test data distributions
differ. Traditional domain generalization (DG) tackles this by including data
from multiple source domains, which is impractical due to expensive data
collection and annotation. Recent vision-language models like CLIP enable
source-free domain generalization (SFDG) by using text prompts to simulate
visual representations, reducing data demands. However, existing SFDG methods
struggle with domain-specific confounders, limiting their generalization
capabilities. To address this issue, we propose TDCRL
(\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation
\textbf{L}earning), the first method to integrate causal inference into the
SFDG setting. TDCRL operates in two steps: first, it employs data augmentation
to generate style word vectors, combining them with class information to
generate text embeddings to simulate visual representations; second, it trains
a causal intervention network with a confounder dictionary to extract
domain-invariant features. Grounded in causal learning, our approach offers a
clear and effective mechanism to achieve robust, domain-invariant features,
ensuring robust generalization. Extensive experiments on PACS, VLCS,
OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL
effectiveness in SFDG.

</details>


### [247] [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)
*Xiangyu Sun,Amin Yousefpour,Shirin Hosseinmardi,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息高斯过程的网格无关框架，用于解决合规性最小化问题，通过共享多输出神经网络的GP先验参数化设计变量，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在解决合规性最小化问题时存在特征边界模糊、计算成本高且缺乏设计复杂度控制机制的问题。

Method: 使用独立核的高斯过程先验参数化设计和状态变量，共享多输出神经网络作为均值函数，结合PGCANs架构控制设计复杂度，并通过最小化合规性、总势能和体积分数约束残差来估计参数。

Result: 方法能够生成超分辨率拓扑结构，收敛速度快，合规性和灰度区域分数优于传统数值方法，并能控制细尺度特征，性能优于其他基于机器学习的方法。

Conclusion: 提出的框架有效解决了传统方法的局限性，为合规性最小化问题提供了一种高效且可控的解决方案。

Abstract: Machine learning (ML) techniques have recently gained significant attention
for solving compliance minimization (CM) problems. However, these methods
typically provide poor feature boundaries, are very expensive, and lack a
systematic mechanism to control the design complexity. Herein, we address these
limitations by proposing a mesh-free and simultaneous framework based on
physics-informed Gaussian processes (GPs). In our approach, we parameterize the
design and state variables with GP priors which have independent kernels but
share a multi-output neural network (NN) as their mean function. The
architecture of this NN is based on Parametric Grid Convolutional Attention
Networks (PGCANs) which not only mitigate spectral bias issues, but also
provide an interpretable mechanism to control design complexity. We estimate
all the parameters of our GP-based representations by simultaneously minimizing
the compliance, total potential energy, and residual of volume fraction
constraint. Importantly, our loss function exclude all data-based residuals as
GPs automatically satisfy them. We also develop computational schemes based on
curriculum training and numerical integration to increase the efficiency and
robustness of our approach which is shown to (1) produce super-resolution
topologies with fast convergence, (2) achieve smaller compliance and less gray
area fraction compared to traditional numerical methods, (3) provide control
over fine-scale features, and (4) outperform competing ML-based methods.

</details>


### [248] [Effects of structural properties of neural networks on machine learning performance](https://arxiv.org/abs/2507.10005)
*Yash Arya,Sang Hoon Lee*

Main category: cs.LG

TL;DR: 研究了图结构对神经网络性能的影响，发现具有密集社区结构的网络学习能力更强。


<details>
  <summary>Details</summary>
Motivation: 探索图结构（如社区结构）对神经网络预测性能的影响，填补现有研究在真实网络结构上的不足。

Method: 使用随机网络、无标度网络和生物神经网络进行比较分析，评估结构属性对图像分类任务的影响。

Result: 结构属性确实影响性能，尤其是具有密集社区结构的网络表现更优。

Conclusion: 研究为网络科学和机器学习提供了新见解，可能启发更接近生物结构的神经网络设计。

Abstract: In recent years, graph-based machine learning techniques, such as
reinforcement learning and graph neural networks, have garnered significant
attention. While some recent studies have started to explore the relationship
between the graph structure of neural networks and their predictive
performance, they often limit themselves to a narrow range of model networks,
particularly lacking mesoscale structures such as communities. Our work
advances this area by conducting a more comprehensive investigation,
incorporating realistic network structures characterized by heterogeneous
degree distributions and community structures, which are typical
characteristics of many real networks. These community structures offer a
nuanced perspective on network architecture. Our analysis employs model
networks such as random and scale-free networks, alongside a comparison with a
biological neural network and its subsets for more detailed analysis. We
examine the impact of these structural attributes on the performance of image
classification tasks. Our findings reveal that structural properties do affect
performance to some extent. Specifically, networks featuring coherent, densely
interconnected communities demonstrate enhanced learning capabilities. The
comparison with the biological neural network emphasizes the relevance of our
findings to real-world structures, suggesting an intriguing connection worth
further exploration. This study contributes meaningfully to network science and
machine learning, providing insights that could inspire the design of more
biologically informed neural networks.

</details>


### [249] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevtić*

Main category: cs.LG

TL;DR: 本研究开发了首个用于预测亚利桑那州山谷热发病率的图神经网络模型，整合了环境因素与病例数据，揭示了关键的环境驱动因素。


<details>
  <summary>Details</summary>
Motivation: 山谷热是美国西南部的地方性公共卫生问题，需要更有效的预测工具以支持早期预警和资源分配。

Method: 采用图神经网络（GNN）模型，结合病例数据和环境变量（如土壤、大气、农业指标和空气质量），探索变量间的相关性及滞后效应。

Result: GNN模型成功捕捉了山谷热的发病趋势，并识别了关键的环境驱动因素。

Conclusion: 该模型为山谷热的早期预警和预防资源分配提供了科学依据。

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [250] [Towards Applying Large Language Models to Complement Single-Cell Foundation Models](https://arxiv.org/abs/2507.10039)
*Steven Palayew,Bo Wang,Gary Bader*

Main category: cs.LG

TL;DR: 论文探讨了如何结合单细胞基础模型（如scGPT）和大型语言模型（LLMs）的优势，提出scMPT模型，通过融合两者提升单细胞数据分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞基础模型无法利用文本信息，而LLMs虽表现优异但缺乏对其性能驱动因素的理解。研究旨在探索LLMs在单细胞数据中的生物学见解，并提出互补性解决方案。

Method: 提出scMPT模型，结合scGPT和LLMs的单细胞表示，并尝试不同融合方法。

Result: scMPT性能优于单独使用scGPT或LLMs，且表现更稳定。

Conclusion: LLMs可作为单细胞基础模型的补充，提升单细胞分析效果。

Abstract: Single-cell foundation models such as scGPT represent a significant
advancement in single-cell omics, with an ability to achieve state-of-the-art
performance on various downstream biological tasks. However, these models are
inherently limited in that a vast amount of information in biology exists as
text, which they are unable to leverage. There have therefore been several
recent works that propose the use of LLMs as an alternative to single-cell
foundation models, achieving competitive results. However, there is little
understanding of what factors drive this performance, along with a strong focus
on using LLMs as an alternative, rather than complementary approach to
single-cell foundation models. In this study, we therefore investigate what
biological insights contribute toward the performance of LLMs when applied to
single-cell data, and introduce scMPT; a model which leverages synergies
between scGPT, and single-cell representations from LLMs that capture these
insights. scMPT demonstrates stronger, more consistent performance than either
of its component models, which frequently have large performance gaps between
each other across datasets. We also experiment with alternate fusion methods,
demonstrating the potential of combining specialized reasoning models with
scGPT to improve performance. This study ultimately showcases the potential for
LLMs to complement single-cell foundation models and drive improvements in
single-cell analysis.

</details>


### [251] [On the Efficiency of Training Robust Decision Trees](https://arxiv.org/abs/2507.10048)
*Benedict Gerlach,Marie Anastacio,Holger H. Hoos*

Main category: cs.LG

TL;DR: 本文提出了一种高效且可持续的对抗性鲁棒决策树训练流程，包括扰动大小自动选择、对抗训练和鲁棒性验证三个阶段。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在工业中的快速应用，其可信赖性受到关注，但鲁棒训练流程的效率和可持续性尚未确立。

Method: 1. 自动选择扰动大小；2. 训练对抗性鲁棒模型；3. 验证模型鲁棒性。

Result: 扰动大小可通过小模型估计，验证时间与训练时间无关。

Conclusion: 该流程显著提升了对抗性鲁棒决策树训练的效率和可持续性。

Abstract: As machine learning gets adopted into the industry quickly, trustworthiness
is increasingly in focus. Yet, efficiency and sustainability of robust training
pipelines still have to be established. In this work, we consider a simple
pipeline for training adversarially robust decision trees and investigate the
efficiency of each step. Our pipeline consists of three stages. Firstly, we
choose the perturbation size automatically for each dataset. For that, we
introduce a simple algorithm, instead of relying on intuition or prior work.
Moreover, we show that the perturbation size can be estimated from smaller
models than the one intended for full training, and thus significant gains in
efficiency can be achieved. Secondly, we train state-of-the-art adversarial
training methods and evaluate them regarding both their training time and
adversarial accuracy. Thirdly, we certify the robustness of each of the models
thus obtained and investigate the time required for this. We find that
verification time, which is critical to the efficiency of the full pipeline, is
not correlated with training time.

</details>


### [252] [Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction](https://arxiv.org/abs/2507.10078)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: cs.LG

TL;DR: 提出了一种基于控制理论中$H^{2}$模型降阶技术的高效参数缩减方法，用于减少线性SSM模型的参数量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 线性SSM模型在捕获序列数据的长程依赖时参数量过大，难以在资源受限设备上部署。

Method: 应用控制理论中的$H^{2}$模型降阶技术对线性SSM组件进行参数缩减。

Result: 实验表明，该方法在LRA基准测试中优于现有的平衡截断方法，参数量减少至1/32且性能未下降。

Conclusion: 提出的方法有效解决了线性SSM模型参数量大的问题，适用于资源受限环境。

Abstract: Deep learning models incorporating linear SSMs have gained attention for
capturing long-range dependencies in sequential data. However, their large
parameter sizes pose challenges for deployment on resource-constrained devices.
In this study, we propose an efficient parameter reduction method for these
models by applying $H^{2}$ model order reduction techniques from control theory
to their linear SSM components. In experiments, the LRA benchmark results show
that the model compression based on our proposed method outperforms an existing
method using the Balanced Truncation, while successfully reducing the number of
parameters in the SSMs to $1/32$ without sacrificing the performance of the
original models.

</details>


### [253] [A Variance-Reduced Cubic-Regularized Newton for Policy Optimization](https://arxiv.org/abs/2507.10120)
*Cheng Sun,Zhen Zhang,Shaofu Yang*

Main category: cs.LG

TL;DR: 提出了一种名为VR-CR-PN的方差缩减立方正则化策略牛顿算法，解决了二阶策略优化中的样本复杂度和分布偏移问题，无需重要性采样。


<details>
  <summary>Details</summary>
Motivation: 现有二阶方法存在样本复杂度不优或依赖不现实的假设，需克服这些限制。

Method: 结合Hessian辅助的方差缩减与二阶策略优化，提出VR-CR-PN算法，并引入新的Hessian估计器。

Result: 算法在非凸条件下达到$\tilde{\mathcal{O}}(\epsilon^{-3})$的样本复杂度，优于之前的$\tilde{\mathcal{O}}(\epsilon^{-3.5})$。

Conclusion: VR-CR-PN在无需重要性采样的情况下显著提升了二阶策略优化的性能。

Abstract: In this paper, we study a second-order approach to policy optimization in
reinforcement learning. Existing second-order methods often suffer from
suboptimal sample complexity or rely on unrealistic assumptions about
importance sampling. To overcome these limitations, we propose VR-CR-PN, a
variance-reduced cubic-regularized policy Newton algorithm. To the best of our
knowledge, this is the first algorithm that integrates Hessian-aided variance
reduction with second-order policy optimization, effectively addressing the
distribution shift problem and achieving best-known sample complexity under
general nonconvex conditions but without the need for importance sampling. We
theoretically establish that VR-CR-PN achieves a sample complexity of
$\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order
stationary point, significantly improving upon the previous best result of
$\tilde{\mathcal{O}}(\epsilon^{-3.5})$ under comparable assumptions. As an
additional contribution, we introduce a novel Hessian estimator for the
expected return function, which admits a uniform upper bound independent of the
horizon length $H$, allowing the algorithm to achieve horizon-independent
sample complexity.

</details>


### [254] [Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering](https://arxiv.org/abs/2507.10088)
*Tung Sum Thomas Kwok,Zeyong Zhang,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: PRRO是一种新颖的表格数据合成方法，通过数据修剪和列重排序优化合成数据的监督学习效用，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据在监督学习中效用低的问题，如类别不平衡和数据关系忽视。

Method: 提出PRRO流程，结合数据修剪和列重排序技术，优化合成数据的信号噪声比和结构对齐。

Result: 在22个公共数据集上，PRRO显著提升预测性能，最高达871.46%；在6个高度不平衡数据集上，类别分布相似性提高43%。

Conclusion: PRRO实现了数据合成与监督学习的无缝集成，提升了数据分析和可访问性。

Abstract: Tabular data synthesis for supervised learning ('SL') model training is
gaining popularity in industries such as healthcare, finance, and retail.
Despite the progress made in tabular data generators, models trained with
synthetic data often underperform compared to those trained with original data.
This low SL utility of synthetic data stems from class imbalance exaggeration
and SL data relationship overlooked by tabular generator. To address these
challenges, we draw inspirations from techniques in emerging data-centric
artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel
pipeline that integrates data-centric techniques into tabular data synthesis.
PRRO incorporates data pruning to guide the table generator towards
observations with high signal-to-noise ratio, ensuring that the class
distribution of synthetic data closely matches that of the original data.
Besides, PRRO employs a column reordering algorithm to align the data modeling
structure of generators with that of SL models. These two modules enable PRRO
to optimize SL utility of synthetic data. Empirical experiments on 22 public
datasets show that synthetic data generated using PRRO enhances predictive
performance compared to data generated without PRRO. Specifically, synthetic
replacement of original data yields an average improvement of 26.74% and up to
871.46% improvement using PRRO, while synthetic appendant to original data
results with PRRO-generated data results in an average improvement of 6.13% and
up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show
that PRRO enables the generator to produce synthetic data with a class
distribution that resembles the original data more closely, achieving a
similarity improvement of 43%. Through PRRO, we foster a seamless integration
of data synthesis to subsequent SL prediction, promoting quality and accessible
data analysis.

</details>


### [255] [Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting](https://arxiv.org/abs/2507.10132)
*Usman Gani Joy*

Main category: cs.LG

TL;DR: 本文提出了一种结合神经ODE、图注意力、小波变换和自适应频率学习的神经网络框架，用于能源供需预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 能源供需预测对可持续能源系统至关重要，但可再生能源的波动性和动态消费模式增加了预测难度。

Method: 采用神经ODE、图注意力、多分辨率小波变换和自适应频率学习，结合Runge-Kutta方法和残差连接，捕捉时空模式。

Result: 在七个数据集上表现优于现有方法，能有效建模复杂时间依赖。

Conclusion: 该模型不仅预测性能优越，还通过SHAP分析增强了可解释性，适用于可持续能源应用。

Abstract: Accurate forecasting of energy demand and supply is critical for optimizing
sustainable energy systems, yet it is challenged by the variability of
renewable sources and dynamic consumption patterns. This paper introduces a
neural framework that integrates continuous-time Neural Ordinary Differential
Equations (Neural ODEs), graph attention, multi-resolution wavelet
transformations, and adaptive learning of frequencies to address the issues of
time series prediction. The model employs a robust ODE solver, using the
Runge-Kutta method, paired with graph-based attention and residual connections
to better understand both structural and temporal patterns. Through
wavelet-based feature extraction and adaptive frequency modulation, it adeptly
captures and models diverse, multi-scale temporal dynamics. When evaluated
across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity
transformer temperature), and Waste, Solar, and Hydro (renewable energy), this
architecture consistently outperforms state-of-the-art baselines in various
forecasting metrics, proving its robustness in capturing complex temporal
dependencies. Furthermore, the model enhances interpretability through SHAP
analysis, making it suitable for sustainable energy applications.

</details>


### [256] [Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS](https://arxiv.org/abs/2507.10172)
*Ruizhe Yu Xia,Jeremy Gow,Simon Lucas*

Main category: cs.LG

TL;DR: 本文探讨了使用无监督CNN-LSTM自编码器模型直接从低级游戏轨迹数据中获取潜在表示的方法，以减少对领域专业知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 通过识别游戏风格，可以为游戏设计提供有价值的见解，并实现自适应体验，同时提升游戏AI的表现。

Method: 采用无监督CNN-LSTM自编码器模型，直接从MicroRTS的低级游戏轨迹数据中学习潜在表示。

Result: 该方法在潜在空间中实现了对不同游戏AI的有意义分离，减少了对领域专业知识的依赖及其相关偏见。

Conclusion: 潜在空间可用于指导探索AI玩家的多样化游戏风格，为游戏设计和AI改进提供了新途径。

Abstract: Play style identification can provide valuable game design insights and
enable adaptive experiences, with the potential to improve game playing agents.
Previous work relies on domain knowledge to construct play trace
representations using handcrafted features. More recent approaches incorporate
the sequential structure of play traces but still require some level of domain
abstraction. In this study, we explore the use of unsupervised CNN-LSTM
autoencoder models to obtain latent representations directly from low-level
play trace data in MicroRTS. We demonstrate that this approach yields a
meaningful separation of different game playing agents in the latent space,
reducing reliance on domain expertise and its associated biases. This latent
space is then used to guide the exploration of diverse play styles within
studied AI players.

</details>


### [257] [MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping](https://arxiv.org/abs/2507.10158)
*Obaidullah Zaland,Erik Elmroth,Monowar Bhuyan*

Main category: cs.LG

TL;DR: MTF-Grasp是一种多层次的联邦学习方法，用于解决机器人抓取任务中非独立同分布数据导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在机器人抓取任务中缺乏探索，且非独立同分布数据导致性能下降。

Method: 提出MTF-Grasp，通过选择数据质量高的机器人训练初始模型，再分发给其他机器人。

Result: 在Cornell和Jacquard数据集上，性能提升高达8%。

Conclusion: MTF-Grasp有效解决了非独立同分布数据问题，提升了机器人抓取任务的性能。

Abstract: Federated Learning (FL) is a promising machine learning paradigm that enables
participating devices to train privacy-preserved and collaborative models. FL
has proven its benefits for robotic manipulation tasks. However, grasping tasks
lack exploration in such settings where robots train a global model without
moving data and ensuring data privacy. The main challenge is that each robot
learns from data that is nonindependent and identically distributed (non-IID)
and of low quantity. This exhibits performance degradation, particularly in
robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL
approach for robotic grasping, acknowledging the unique challenges posed by the
non-IID data distribution across robots, including quantitative skewness.
MTF-Grasp harnesses data quality and quantity across robots to select a set of
"top-level" robots with better data distribution and higher sample count. It
then utilizes top-level robots to train initial seed models and distribute them
to the remaining "low-level" robots, reducing the risk of model performance
degradation in low-level robots. Our approach outperforms the conventional FL
setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping
datasets.

</details>


### [258] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: 论文提出了一种对抗性表示学习方法，通过引入焦点熵来保护用户隐私，同时保持高预测能力。


<details>
  <summary>Details</summary>
Motivation: 如何在保持高预测能力的同时保护用户隐私。

Method: 采用对抗性表示学习方法，引入焦点熵以减少信息泄露。

Result: 在多个基准测试中验证了方法的可行性，结果显示高目标效用和中等隐私泄露。

Conclusion: 提出的方法在隐私保护和预测能力之间取得了平衡。

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [259] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Röder,Christoph Raab,Frank-Michael Schleif*

Main category: cs.LG

TL;DR: FedAcross+是一个高效的联邦学习框架，针对工业环境中的客户端适应问题，通过冻结预训练模型的部分结构并利用自适应线性层处理域适应，解决了数据标注、协变量漂移和资源限制等挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘设备上的实际应用面临数据标注成本高、协变量漂移和资源限制等问题，需要一种高效且适应性强的方法。

Method: 框架基于预训练模型（包括深度主干、适应模块和分类器），在客户端适应时冻结主干和分类器，仅调整自适应线性层，并支持流式数据处理。

Result: 实验表明，FedAcross+在低端设备上实现了竞争性的域适应性能，并支持资源受限环境中的间歇性模型更新。

Conclusion: FedAcross+为资源受限环境中的联邦学习提供了一种实用且高效的解决方案，成功应对了域适应和资源限制的挑战。

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [260] [Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach](https://arxiv.org/abs/2507.10170)
*Wuyang Zhou,Giorgos Iacovides,Kriton Konstantinidis,Ilya Kisil,Danilo Mandic*

Main category: cs.LG

TL;DR: 论文探讨了张量网络（TN）分解中TN秩的概念，旨在通过实例和可视化方法澄清其含义，帮助读者理解并应用TN秩。


<details>
  <summary>Details</summary>
Motivation: TN秩在张量网络分解中至关重要，但其缺乏统一的定义和直观解释，常被作为经验性超参数而非设计参数。本文旨在澄清TN秩的概念，提升其在实践和教育中的应用。

Method: 通过实例和图形化方法解释TN秩，特别是在CP和Tucker分解中，并揭示TN秩与张量展开矩阵秩的关系。

Result: 提供了对TN秩的清晰理解，支持基于领域知识的TN设计，简化了复杂张量代数的应用。

Conclusion: 本文为读者提供了TN秩的统一视角，增强了其在实践中的可解释性和应用性。

Abstract: Tensor Network (TN) decompositions have emerged as an indispensable tool in
Big Data analytics owing to their ability to provide compact low-rank
representations, thus alleviating the ``Curse of Dimensionality'' inherent in
handling higher-order data. At the heart of their success lies the concept of
TN ranks, which governs the efficiency and expressivity of TN decompositions.
However, unlike matrix ranks, TN ranks often lack a universal meaning and an
intuitive interpretation, with their properties varying significantly across
different TN structures. Consequently, TN ranks are frequently treated as
empirically tuned hyperparameters, rather than as key design parameters
inferred from domain knowledge. The aim of this Lecture Note is therefore to
demystify the foundational yet frequently misunderstood concept of TN ranks
through real-life examples and intuitive visualizations. We begin by
illustrating how domain knowledge can guide the selection of TN ranks in
widely-used models such as the Canonical Polyadic (CP) and Tucker
decompositions. For more complex TN structures, we employ a self-explanatory
graphical approach that generalizes to tensors of arbitrary order. Such a
perspective naturally reveals the relationship between TN ranks and the
corresponding ranks of tensor unfoldings (matrices), thereby circumventing
cumbersome multi-index tensor algebra while facilitating domain-informed TN
design. It is our hope that this Lecture Note will equip readers with a clear
and unified understanding of the concept of TN rank, along with the necessary
physical insight and intuition to support the selection, explainability, and
deployment of tensor methods in both practical applications and educational
contexts.

</details>


### [261] [Recognizing Dementia from Neuropsychological Tests with State Space Models](https://arxiv.org/abs/2507.10311)
*Liming Wang,Saurabhchand Bhati,Cody Karjadi,Rhoda Au,James Glass*

Main category: cs.LG

TL;DR: Demenba是一种基于状态空间模型的自动痴呆分类框架，通过语音记录推断认知衰退，性能优于现有方法21%，且参数更少。


<details>
  <summary>Details</summary>
Motivation: 早期发现痴呆对及时干预和改善患者预后至关重要，传统神经心理学测试依赖人工评分，自动化系统可提高效率。

Method: 提出Demenba框架，基于状态空间模型，内存和计算复杂度随序列长度线性增长，结合大型语言模型进一步提升性能。

Result: 在1,000多小时认知评估数据上训练，性能优于现有方法21%，参数更少，且融合大型语言模型后效果更佳。

Conclusion: Demenba为痴呆评估提供了更透明、可扩展的工具，未来可进一步优化和应用。

Abstract: Early detection of dementia is critical for timely medical intervention and
improved patient outcomes. Neuropsychological tests are widely used for
cognitive assessment but have traditionally relied on manual scoring. Automatic
dementia classification (ADC) systems aim to infer cognitive decline directly
from speech recordings of such tests. We propose Demenba, a novel ADC framework
based on state space models, which scale linearly in memory and computation
with sequence length. Trained on over 1,000 hours of cognitive assessments
administered to Framingham Heart Study participants, some of whom were
diagnosed with dementia through adjudicated review, our method outperforms
prior approaches in fine-grained dementia classification by 21\%, while using
fewer parameters. We further analyze its scaling behavior and demonstrate that
our model gains additional improvement when fused with large language models,
paving the way for more transparent and scalable dementia assessment tools.
Code: https://anonymous.4open.science/r/Demenba-0861

</details>


### [262] [T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs](https://arxiv.org/abs/2507.10183)
*Alireza Dizaji,Benedict Aaron Tjandra,Mehrab Hamidi,Shenyang Huang,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: 论文提出了T-GRAB基准测试，用于系统评估时序图神经网络（TGNNs）在周期性、因果性和长程依赖等核心时序模式上的表现，揭示了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前TGNNs在捕捉核心时序模式（如周期性、因果性和长程依赖）方面的有效性尚不明确，需要系统化的评估工具。

Method: 设计了T-GRAB基准测试，包含合成任务以隔离关键时序能力，并评估了11种时序图学习方法。

Result: 发现现有模型在泛化时序模式方面存在根本性缺陷，传统基准测试未能揭示这些挑战。

Conclusion: T-GRAB为开发具有更强时序推理能力的架构提供了行动指南，并揭示了当前模型的局限性。

Abstract: Dynamic graph learning methods have recently emerged as powerful tools for
modelling relational data evolving through time. However, despite extensive
benchmarking efforts, it remains unclear whether current Temporal Graph Neural
Networks (TGNNs) effectively capture core temporal patterns such as
periodicity, cause-and-effect, and long-range dependencies. In this work, we
introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set
of synthetic tasks designed to systematically probe the capabilities of TGNNs
to reason across time. T-GRAB provides controlled, interpretable tasks that
isolate key temporal skills: counting/memorizing periodic repetitions,
inferring delayed causal effects, and capturing long-range dependencies over
both spatial and temporal dimensions. We evaluate 11 temporal graph learning
methods on these tasks, revealing fundamental shortcomings in their ability to
generalize temporal patterns. Our findings offer actionable insights into the
limitations of current models, highlight challenges hidden by traditional
real-world benchmarks, and motivate the development of architectures with
stronger temporal reasoning abilities. The code for T-GRAB can be found at:
https://github.com/alirezadizaji/T-GRAB.

</details>


### [263] [Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2507.10348)
*Yichen Li*

Main category: cs.LG

TL;DR: FedFD提出了一种基于特征蒸馏的异构联邦学习方法，通过正交投影对齐特征，解决了传统方法因知识偏差导致的不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 异构联邦学习中，传统基于logit的蒸馏方法无法有效补偿异构模型带来的知识偏差，导致性能不稳定。

Method: 提出FedFD，利用正交投影对齐特征，为每种客户端模型架构维护投影层，以减少知识偏差。

Result: 实验表明FedFD在性能上优于现有方法。

Conclusion: FedFD通过特征蒸馏和正交投影，显著提升了异构联邦学习的稳定性和性能。

Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing
attention for its ability to aggregate knowledge from heterogeneous models
while keeping private data locally. To better aggregate knowledge from clients,
ensemble distillation, as a widely used and effective technique, is often
employed after global aggregation to enhance the performance of the global
model. However, simply combining Hetero-FL and ensemble distillation does not
always yield promising results and can make the training process unstable. The
reason is that existing methods primarily focus on logit distillation, which,
while being model-agnostic with softmax predictions, fails to compensate for
the knowledge bias arising from heterogeneous models. To tackle this challenge,
we propose a stable and efficient Feature Distillation for model-heterogeneous
Federated learning, dubbed FedFD, that can incorporate aligned feature
information via orthogonal projection to integrate knowledge from heterogeneous
models better. Specifically, a new feature-based ensemble federated knowledge
distillation paradigm is proposed. The global model on the server needs to
maintain a projection layer for each client-side model architecture to align
the features separately. Orthogonal techniques are employed to re-parameterize
the projection layer to mitigate knowledge bias from heterogeneous models and
thus maximize the distilled knowledge. Extensive experiments show that FedFD
achieves superior performance compared to state-of-the-art methods.

</details>


### [264] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Main category: cs.LG

TL;DR: 提出了一种基于时间对齐注意力机制的Transformer模型（TAT），用于多时间范围需求预测，特别是在销售高峰期，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 需求预测对供应链管理至关重要，尤其是在销售高峰期，传统方法难以准确预测峰值需求。

Method: TAT模型结合已知上下文变量（如节假日和促销活动），通过时间对齐注意力机制（TAA）学习上下文依赖的对齐关系。

Result: 在两个大型电商数据集上，TAT在峰值需求预测中提升了30%的准确性，同时整体性能优于其他先进方法。

Conclusion: TAT为多时间范围需求预测提供了一种高效解决方案，尤其在销售高峰期表现突出。

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


### [265] [A Graph Sufficiency Perspective for Neural Networks](https://arxiv.org/abs/2507.10215)
*Cencheng Shen,Yuexiao Dong*

Main category: cs.LG

TL;DR: 论文通过图变量和统计充分性分析神经网络，提出层输出对输入具有统计充分性的条件，证明在无限宽度极限下渐近充分性成立，并展示有限宽度网络也能实现充分性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过图论和统计充分性理论，为神经网络提供新的统计理解，揭示层间信息保留的机制。

Method: 将神经网络层解释为基于图的变换，神经元作为输入与锚点间的成对函数，并在密集锚点假设下分析充分性条件。

Result: 证明无限宽度网络具有渐近充分性，且有限宽度网络在特定输入分布下也能实现充分性。

Conclusion: 该框架为神经网络提供了统计充分性的理论基础，连接了图论表示与深度学习。

Abstract: This paper analyzes neural networks through graph variables and statistical
sufficiency. We interpret neural network layers as graph-based transformations,
where neurons act as pairwise functions between inputs and learned anchor
points. Within this formulation, we establish conditions under which layer
outputs are sufficient for the layer inputs, that is, each layer preserves the
conditional distribution of the target variable given the input variable. Under
dense anchor point assumptions, we prove that asymptotic sufficiency holds in
the infinite-width limit and is preserved throughout training. To align more
closely with practical architectures, we further show that sufficiency can be
achieved with finite-width networks by assuming region-separated input
distributions and constructing appropriate anchor points. Our framework covers
fully connected layers, general pairwise functions, ReLU and sigmoid
activations, and convolutional neural networks. This work bridges statistical
sufficiency, graph-theoretic representations, and deep learning, providing a
new statistical understanding of neural networks.

</details>


### [266] [Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study](https://arxiv.org/abs/2507.10409)
*Amine Lbath,Ibtissam Labriji*

Main category: cs.LG

TL;DR: 研究通过知识蒸馏（KD）训练紧凑型DeepRX模型，在保持性能的同时降低能耗，验证了KD在高效AI中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决AI/ML模型中能效与性能平衡的挑战，特别是在DeepRX这种基于全卷积ResNet架构的深度学习接收器中。

Method: 评估DeepRX的能耗（FLOPs/Watt和FLOPs/clock），分析内存访问模式的影响，并应用知识蒸馏训练紧凑型学生模型。

Result: 蒸馏模型在BER与SINR性能上优于从头训练的模型，且能耗更低。

Conclusion: 知识蒸馏是实现高效能AI的有效方法，尤其在DeepRX模型中表现显著。

Abstract: This study addresses the challenge of balancing energy efficiency with
performance in AI/ML models, focusing on DeepRX, a deep learning receiver based
on a fully convolutional ResNet architecture. We evaluate the energy
consumption of DeepRX, considering factors including FLOPs/Watt and
FLOPs/clock, and find consistency between estimated and actual energy usage,
influenced by memory access patterns. The research extends to comparing energy
dynamics during training and inference phases. A key contribution is the
application of knowledge distillation (KD) to train a compact DeepRX
\textit{student} model that emulates the performance of the \textit{teacher}
model but with reduced energy consumption. We experiment with different student
model sizes, optimal teacher sizes, and KD hyperparameters. Performance is
measured by comparing the Bit Error Rate (BER) performance versus
Signal-to-Interference \& Noise Ratio (SINR) values of the distilled model and
a model trained from scratch. The distilled models demonstrate a lower error
floor across SINR levels, highlighting the effectiveness of KD in achieving
energy-efficient AI solutions.

</details>


### [267] [Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients](https://arxiv.org/abs/2507.10241)
*Vikas Dwivedi,Balaji Srinivasan,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: KAPI-ELM是一种基于RBF的自适应扩展方法，结合贝叶斯优化和最小二乘优化，解决了PI-ELM在捕捉尖锐梯度时的局限性，并在多个PDE问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统PI-ELM虽然速度快，但其固定输入层限制了捕捉尖锐梯度的能力，需要一种改进方法。

Method: 引入轻量级贝叶斯优化框架，优化输入层分布参数，结合最小二乘优化输出层参数。

Result: 在多个PDE基准测试中，KAPI-ELM在正向和反向问题上均达到最先进精度，且参数更少。

Conclusion: KAPI-ELM是一种可扩展、可解释且通用的物理信息学习框架，特别适用于刚性PDE问题。

Abstract: This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning
Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of
PI-ELM designed to solve both forward and inverse Partial Differential Equation
(PDE) problems involving localized sharp gradients. While PI-ELMs outperform
the traditional Physics-Informed Neural Networks (PINNs) in speed due to their
single-shot, least square optimization, this advantage comes at a cost: their
fixed, randomly initialized input layer limits their ability to capture sharp
gradients. To overcome this limitation, we introduce a lightweight Bayesian
Optimization (BO) framework that, instead of adjusting each input layer
parameter individually as in traditional backpropagation, learns a small set of
hyperparameters defining the statistical distribution from which the input
weights are drawn. This novel distributional optimization strategy -- combining
BO for input layer distributional parameters with least-squares optimization
for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's
speed while matching or exceeding the expressiveness of PINNs. We validate the
proposed methodology on several challenging forward and inverse PDE benchmarks,
including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson
equation with sharp localized sources, and a time-dependent advection equation.
Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and
inverse settings. In stiff PDE regimes, it matches or even outperforms advanced
methods such as the Extended Theory of Functional Connections (XTFC), while
requiring nearly an order of magnitude fewer tunable parameters. These results
establish the potential of KAPI-ELM as a scalable, interpretable, and
generalizable physics-informed learning framework, especially in stiff PDE
regimes.

</details>


### [268] [Conditional Chemical Language Models are Versatile Tools in Drug Discovery](https://arxiv.org/abs/2507.10273)
*Lu Zhu,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: SAFE-T是一个基于生物背景的化学建模框架，通过条件生成模型优化分子设计和评分，显著提升药物发现效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成化学语言模型在药物发现中缺乏可靠奖励信号和输出可解释性的问题。

Method: SAFE-T通过建模基于片段的分子序列条件似然，支持虚拟筛选、药物-靶标相互作用预测等任务，并实现目标导向的分子生成。

Result: 在零样本评估中，SAFE-T性能优于或与现有方法相当，且速度更快，同时能捕捉已知结构-活性关系。

Conclusion: 条件生成化学语言模型可统一评分与生成，加速早期药物发现。

Abstract: Generative chemical language models (CLMs) have demonstrated strong
capabilities in molecular design, yet their impact in drug discovery remains
limited by the absence of reliable reward signals and the lack of
interpretability in their outputs. We present SAFE-T, a generalist chemical
modeling framework that conditions on biological context -- such as protein
targets or mechanisms of action -- to prioritize and design molecules without
relying on structural information or engineered scoring functions. SAFE-T
models the conditional likelihood of fragment-based molecular sequences given a
biological prompt, enabling principled scoring of molecules across tasks such
as virtual screening, drug-target interaction prediction, and activity cliff
detection. Moreover, it supports goal-directed generation by sampling from this
learned distribution, aligning molecular design with biological objectives. In
comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,
ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves
performance comparable to or better than existing approaches while being
significantly faster. Fragment-level attribution further reveals that SAFE-T
captures known structure-activity relationships, supporting interpretable and
biologically grounded design. Together with its computational efficiency, these
results demonstrate that conditional generative CLMs can unify scoring and
generation to accelerate early-stage drug discovery.

</details>


### [269] [Average Sensitivity of Hierarchical $k$-Median Clustering](https://arxiv.org/abs/2507.10296)
*Shijie Li,Weiqiang He,Ruobing Bai,Pan Peng*

Main category: cs.LG

TL;DR: 论文研究了层次聚类中的k-中值问题，提出了一种高效算法，证明了其低平均敏感性和高聚类质量，并通过实验验证了其稳健性。


<details>
  <summary>Details</summary>
Motivation: 现代算法处理的数据集通常规模大且动态变化，若层次聚类对数据扰动敏感，会降低算法实用性。

Method: 提出了一种高效的层次k-中值聚类算法，并理论证明了其低平均敏感性和高聚类质量。

Result: 实验验证了算法的稳健性和有效性，同时指出单链接聚类和CLNSS算法的确定性变体具有高平均敏感性。

Conclusion: 提出的算法在理论和实践中均表现出色，适用于动态数据集，提高了层次聚类的稳定性和实用性。

Abstract: Hierarchical clustering is a widely used method for unsupervised learning
with numerous applications. However, in the application of modern algorithms,
the datasets studied are usually large and dynamic. If the hierarchical
clustering is sensitive to small perturbations of the dataset, the usability of
the algorithm will be greatly reduced. In this paper, we focus on the
hierarchical $k$ -median clustering problem, which bridges hierarchical and
centroid-based clustering while offering theoretical appeal, practical utility,
and improved interpretability. We analyze the average sensitivity of algorithms
for this problem by measuring the expected change in the output when a random
data point is deleted. We propose an efficient algorithm for hierarchical
$k$-median clustering and theoretically prove its low average sensitivity and
high clustering quality. Additionally, we show that single linkage clustering
and a deterministic variant of the CLNSS algorithm exhibit high average
sensitivity, making them less stable. Finally, we validate the robustness and
effectiveness of our algorithm through experiments.

</details>


### [270] [Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities](https://arxiv.org/abs/2507.10442)
*Shivam Chandhok,Wan-Cyuan Fan,Vered Shwartz,Vineeth N Balasubramanian,Leonid Sigal*

Main category: cs.LG

TL;DR: 本文通过构建一系列测试，探讨了当前最先进的视觉语言模型（VLMs）在基础视觉任务中的局限性，并分析了其设计中的潜在不足。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多任务处理中表现出色，但仍缺乏一些基础视觉理解能力。本文旨在揭示这些局限性，并推动模型的进一步改进。

Method: 通过设计测试，对比分析VLMs在视觉编码器、中间视觉语言投影和LLM解码器输出等不同组件上的表现。

Result: 研究发现VLMs在视觉信息处理中存在不足，揭示了其能力、鲁棒性及信息处理方式的局限性。

Conclusion: 本文的发现为改进VLMs提供了重要指导，并希望推动该领域的进一步发展。

Abstract: Vision-language Models (VLMs) have emerged as general-purpose tools for
addressing a variety of complex computer vision problems. Such models have been
shown to be highly capable, but, at the same time, lacking some basic visual
understanding skills. In this paper, we set out to understand the limitations
of SoTA VLMs on fundamental visual tasks by constructing a series of tests that
probe which components of design, specifically, may be lacking. Importantly, we
go significantly beyond the current benchmarks, which simply measure the final
performance of VLM response, by also comparing and contrasting it to the
performance of probes trained directly on features obtained from the visual
encoder, intermediate vision-language projection and LLM-decoder output. In
doing so, we uncover shortcomings in VLMs and make a number of important
observations about their capabilities, robustness and how they process visual
information. We hope our insights will guide progress in further improving
VLMs.

</details>


### [271] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 论文分析了联邦学习（FL）中客户端随机参与的问题，提出了在非均匀参与情况下Agnostic FedAvg算法的收敛性证明，并验证其优于加权聚合变体。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端随机参与且参与概率未知或偏置的实际挑战，现有方法假设不切实际。

Method: 提出Agnostic FedAvg算法，分析其在随机、非均匀客户端参与下的优化问题，并证明其收敛性。

Result: 在凸且可能非光滑损失函数下，算法达到标准收敛速率O(1/√T)，且无需知道参与分布。

Conclusion: Agnostic FedAvg在非均匀参与下表现优于加权聚合变体，即使后者已知参与权重。

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [272] [MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data](https://arxiv.org/abs/2507.10334)
*Mahmoud Bekhit,Ahmad Salah,Ahmed Salim Alrawahi,Tarek Attia,Ahmed Ali,Esraa Eldesokey,Ahmed Fathalla*

Main category: cs.LG

TL;DR: 论文比较了统计、机器学习和深度学习方法在IMU运动捕捉数据缺失填补中的表现，发现多变量方法优于单变量方法，尤其在复杂缺失模式下。


<details>
  <summary>Details</summary>
Motivation: 运动捕捉数据缺失影响应用效果，但缺乏系统性评估，研究填补这一空白。

Method: 通过公开数据集，比较单变量和多变量填补方法，模拟三种缺失机制。

Result: 多变量方法显著降低误差（如MAE减少50%），GAIN和迭代填补器表现最佳。

Conclusion: 为未来研究提供基准，建议使用多变量方法提升数据完整性。

Abstract: Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)
is vital for applications in sports science, but its utility is often
compromised by missing data. Despite numerous imputation techniques, a
systematic performance evaluation for IMU-derived MoCap time-series data is
lacking. We address this gap by conducting a comprehensive comparative analysis
of statistical, machine learning, and deep learning imputation methods. Our
evaluation considers three distinct contexts: univariate time-series,
multivariate across subjects, and multivariate across kinematic angles. To
facilitate this benchmark, we introduce the first publicly available MoCap
dataset designed specifically for imputation, featuring data from 53 karate
practitioners. We simulate three controlled missingness mechanisms: missing
completely at random (MCAR), block missingness, and a novel value-dependent
pattern at signal transition points. Our experiments, conducted on 39 kinematic
variables across all subjects, reveal that multivariate imputation frameworks
consistently outperform univariate approaches, particularly for complex
missingness. For instance, multivariate methods achieve up to a 50% mean
absolute error reduction (MAE from 10.8 to 5.8) compared to univariate
techniques for transition point missingness. Advanced models like Generative
Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the
highest accuracy in these challenging scenarios. This work provides a critical
baseline for future research and offers practical recommendations for improving
the integrity and robustness of Mo-Cap data analysis.

</details>


### [273] [Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions](https://arxiv.org/abs/2507.10345)
*Yuwen Li,Guozhi Zhang*

Main category: cs.LG

TL;DR: 论文研究了ReLU神经网络对Korobov函数的$L_p$和$W^1_p$范数逼近误差，得到了在宽度和深度上几乎最优的超逼近误差界。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在高维空间中逼近Korobov函数的能力，克服维度灾难的影响。

Method: 利用稀疏网格有限元和比特提取技术，分析网络宽度和深度对逼近误差的影响。

Result: 在$L_p$范数下得到$2m$阶误差界，在$W^1_p$范数下得到$2m-2$阶误差界，优于经典的低阶误差界。

Conclusion: 神经网络在高维空间中的表达能力不受维度灾难的显著影响，逼近效果优于传统方法。

Abstract: This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU
neural networks for Korobov functions. In terms of network width and depth, we
derive nearly optimal super-approximation error bounds of order $2m$ in the
$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with
$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages
sparse grid finite elements and the bit extraction technique. Our results
improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and
demonstrate that the expressivity of neural networks is largely unaffected by
the curse of dimensionality.

</details>


### [274] [Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop](https://arxiv.org/abs/2507.10502)
*Elizabeth Fahsbender,Alma Andersson,Jeremy Ash,Polina Binder,Daniel Burkhardt,Benjamin Chang,Georg K. Gerber,Anthony Gitter,Patrick Godau,Ankit Gupta,Genevieve Haliburton,Siyu He,Trey Ideker,Ivana Jelic,Aly Khan,Yang-Joon Kim,Aditi Krishnapriyan,Jon M. Laurent,Tianyu Liu 28,Emma Lundberg,Shalin B. Mehta,Rob Moccia,Angela Oliveira Pisco,Katherine S. Pollard,Suresh Ramani,Julio Saez-Rodriguez,Yasin Senbabaoglu,Elana Simon,Srinivasan Sivanandan,Gustavo Stolovitzky,Marc Valer,Bo Wang,Xikun Zhang,James Zou,Katrina Kalantar*

Main category: cs.LG

TL;DR: 论文探讨了人工智能在生物学中的应用，指出缺乏标准化跨领域基准测试的问题，并提出了构建稳健基准框架的建议。


<details>
  <summary>Details</summary>
Motivation: 解决生物学中人工智能模型缺乏标准化基准的问题，以提升模型的稳健性和可信度。

Method: 通过召集机器学习与计算生物学专家，分析数据异质性、噪声、可重复性挑战等瓶颈，并提出建议。

Result: 提出了高质量数据管理、标准化工具、全面评估指标和开放协作平台等建议。

Conclusion: 构建稳健的基准框架将推动人工智能驱动的虚拟细胞研究，促进生物学发现和治疗见解。

Abstract: Artificial intelligence holds immense promise for transforming biology, yet a
lack of standardized, cross domain, benchmarks undermines our ability to build
robust, trustworthy models. Here, we present insights from a recent workshop
that convened machine learning and computational biology experts across
imaging, transcriptomics, proteomics, and genomics to tackle this gap. We
identify major technical and systemic bottlenecks such as data heterogeneity
and noise, reproducibility challenges, biases, and the fragmented ecosystem of
publicly available resources and propose a set of recommendations for building
benchmarking frameworks that can efficiently compare ML models of biological
systems across tasks and data modalities. By promoting high quality data
curation, standardized tooling, comprehensive evaluation metrics, and open,
collaborative platforms, we aim to accelerate the development of robust
benchmarks for AI driven Virtual Cells. These benchmarks are crucial for
ensuring rigor, reproducibility, and biological relevance, and will ultimately
advance the field toward integrated models that drive new discoveries,
therapeutic insights, and a deeper understanding of cellular systems.

</details>


### [275] [Parallel Sampling of Diffusion Models on $SO(3)$](https://arxiv.org/abs/2507.10347)
*Yan-Ting Chen,Hao-Wei Chen,Tsu-Ching Hsiao,Chun-Yi Lee*

Main category: cs.LG

TL;DR: 提出一种在SO(3)流形上加速扩散过程的算法，通过数值Picard迭代实现，实验显示速度提升达4.9倍且不影响任务奖励。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在SO(3)流形上的去噪过程耗时严重，需解决其固有顺序性问题。

Method: 采用数值Picard迭代方法优化SO(3)空间中的扩散过程。

Result: 算法在姿态模糊问题中实现速度提升4.9倍，且任务奖励无显著下降。

Conclusion: 该方法有效加速扩散过程，显著降低生成样本的延迟。

Abstract: In this paper, we design an algorithm to accelerate the diffusion process on
the $SO(3)$ manifold. The inherently sequential nature of diffusion models
necessitates substantial time for denoising perturbed data. To overcome this
limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$
space. We demonstrate our algorithm on an existing method that employs
diffusion models to address the pose ambiguity problem. Moreover, we show that
this acceleration advantage occurs without any measurable degradation in task
reward. The experiments reveal that our algorithm achieves a speed-up of up to
4.9$\times$, significantly reducing the latency for generating a single sample.

</details>


### [276] [Disentangling Neural Disjunctive Normal Form Models](https://arxiv.org/abs/2507.10546)
*Kexin Gu Baugh,Vincent Perreault,Matthew Baugh,Luke Dickens,Katsumi Inoue,Alessandra Russo*

Main category: cs.LG

TL;DR: 论文提出了一种新的解缠方法，通过拆分编码嵌套规则的节点，提升了神经DNF模型在符号翻译后的性能。


<details>
  <summary>Details</summary>
Motivation: 神经DNF模型在符号翻译后性能下降，部分原因是未能解缠网络权重中的学习知识。

Method: 提出解缠方法，拆分嵌套规则节点为独立小节点，以保留模型性能。

Result: 在多种分类任务中，解缠方法提供了紧凑且可解释的逻辑表示，性能接近翻译前模型。

Conclusion: 解缠方法有效提升了神经DNF模型的性能与可解释性。

Abstract: Neural Disjunctive Normal Form (DNF) based models are powerful and
interpretable approaches to neuro-symbolic learning and have shown promising
results in classification and reinforcement learning settings without prior
knowledge of the tasks. However, their performance is degraded by the
thresholding of the post-training symbolic translation process. We show here
that part of the performance degradation during translation is due to its
failure to disentangle the learned knowledge represented in the form of the
networks' weights. We address this issue by proposing a new disentanglement
method; by splitting nodes that encode nested rules into smaller independent
nodes, we are able to better preserve the models' performance. Through
experiments on binary, multiclass, and multilabel classification tasks
(including those requiring predicate invention), we demonstrate that our
disentanglement method provides compact and interpretable logical
representations for the neural DNF-based models, with performance closer to
that of their pre-translation counterparts. Our code is available at
https://github.com/kittykg/disentangling-ndnf-classification.

</details>


### [277] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Main category: cs.LG

TL;DR: DeepONets在PDE系统中作为替代模型表现出色，但在岩土工程中应用有限。本研究评估了四种DeepONet架构，发现Model 4（增强型）性能最佳，计算效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索DeepONets在岩土工程中的应用潜力，解决传统方法在复杂系统中的效率问题。

Method: 比较了四种DeepONet架构，包括标准型和物理启发型，最终提出Fourier特征增强的Trunknet架构（Model 4）。

Result: Model 4表现最佳，计算速度提升1.5至100倍，适用于快速变化函数。

Conclusion: DeepONets在岩土工程中具有高效、通用的潜力，推动了科学机器学习在该领域的应用。

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [278] [Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis](https://arxiv.org/abs/2507.10382)
*Yue Ding,Conor McCarthy,Kevin O'Shea,Mingming Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于云和LLM的共享电动出行平台，结合移动应用提供个性化路线推荐，并通过优化模块和RAG框架在不同场景下进行评估。


<details>
  <summary>Details</summary>
Motivation: 随着智能出行和共享电动出行服务的兴起，用户对端到端解决方案的需求增加，推动了云技术和LLM在该领域的应用。

Method: 开发了一个云基LLM驱动的共享电动出行平台，集成了移动应用和优化模块，同时采用RAG框架进行模式级评估。

Result: 优化模块在旅行时间和成本上表现良好；RAG框架在系统操作员查询和用户查询上的执行准确率分别为0.81和0.98。

Conclusion: 该平台为共享电动出行提供了有效的端到端解决方案，展示了LLM和云技术在智能出行中的潜力。

Abstract: With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.

</details>


### [279] [Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model](https://arxiv.org/abs/2507.10385)
*Md. Ahsanul Kabir,Mohammad Al Hasan,Aritra Mandal,Liyang Hao,Ishita Khan,Daniel Tunkelang,Zhe Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于语义标签的依赖感知Transformer模型TagBERT，用于电子商务查询重构任务，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 电子商务搜索引擎面临查询模糊、词汇不对齐等问题，传统方法未能充分利用语义标签信息。

Method: 将查询重构任务视为令牌分类问题，设计依赖感知Transformer模型TagBERT，利用令牌的语义标签学习查询短语嵌入。

Result: 在大型电子商务数据集上，TagBERT性能优于BERT、eBERT等竞争模型。

Conclusion: TagBERT通过利用语义标签信息，显著提升了查询重构任务的性能。

Abstract: The major task of any e-commerce search engine is to retrieve the most
relevant inventory items, which best match the user intent reflected in a
query. This task is non-trivial due to many reasons, including ambiguous
queries, misaligned vocabulary between buyers, and sellers, over- or
under-constrained queries by the presence of too many or too few tokens. To
address these challenges, query reformulation is used, which modifies a user
query through token dropping, replacement or expansion, with the objective to
bridge semantic gap between query tokens and users' search intent. Early
methods of query reformulation mostly used statistical measures derived from
token co-occurrence frequencies from selective user sessions having clicks or
purchases. In recent years, supervised deep learning approaches, specifically
transformer-based neural language models, or sequence-to-sequence models are
being used for query reformulation task. However, these models do not utilize
the semantic tags of a query token, which are significant for capturing user
intent of an e-commerce query. In this work, we pose query reformulation as a
token classification task, and solve this task by designing a dependency-aware
transformer-based language model, TagBERT, which makes use of semantic tags of
a token for learning superior query phrase embedding. Experiments on large,
real-life e-commerce datasets show that TagBERT exhibits superior performance
than plethora of competing models, including BERT, eBERT, and
Sequence-to-Sequence transformer model for important token classification task.

</details>


### [280] [Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials](https://arxiv.org/abs/2507.10400)
*Nicholas Casetti,Dylan Anstine,Olexandr Isayev,Connor W. Coley*

Main category: cs.LG

TL;DR: 提出了一种针对复杂反应（如环化反应）的机制搜索策略，结合图枚举和机器学习筛选，利用神经网络势能（AIMNet2-rxn）评估反应路径，验证了其预测活化能和立体选择性的能力。


<details>
  <summary>Details</summary>
Motivation: 复杂反应（如多键协同变化的反应）的机制搜索困难，尤其在天然产物合成中，需要更高效的方法。

Method: 结合图枚举和机器学习筛选，利用神经网络势能（AIMNet2-rxn）评估候选反应路径。

Result: 验证了神经网络势能预测活化能和立体选择性的能力，并成功复现了天然产物合成的关键步骤。

Conclusion: 该策略为复杂反应的机制搜索提供了高效且成本效益高的解决方案。

Abstract: Reaction mechanism search tools have demonstrated the ability to provide
insights into likely products and rate-limiting steps of reacting systems.
However, reactions involving several concerted bond changes - as can be found
in many key steps of natural product synthesis - can complicate the search
process. To mitigate these complications, we present a mechanism search
strategy particularly suited to help expedite exploration of an exemplary
family of such complex reactions, cyclizations. We provide a cost-effective
strategy for identifying relevant elementary reaction steps by combining
graph-based enumeration schemes and machine learning techniques for
intermediate filtering. Key to this approach is our use of a neural network
potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate
reaction pathway. In this article, we evaluate the NNP's ability to estimate
activation energies, demonstrate the correct anticipation of stereoselectivity,
and recapitulate complex enabling steps in natural product synthesis.

</details>


### [281] [Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning](https://arxiv.org/abs/2507.10401)
*Ryan Bausback,Jingqiao Tang,Lu Lu,Feng Bao,Toan Huynh*

Main category: cs.LG

TL;DR: 提出了一种用于算子学习中不确定性量化的新框架——随机算子网络（SON），结合了随机神经网络（SNN）和DeepONet的概念。


<details>
  <summary>Details</summary>
Motivation: 解决算子学习中的不确定性量化问题，通过结合随机最优控制和深度算子网络的方法。

Method: 将分支网络建模为随机微分方程（SDE），并通过伴随BSDE反向传播，利用随机最大值原理中的哈密顿梯度替代损失函数梯度进行SGD更新。

Result: SON能够通过其扩散参数学习算子中的不确定性，并在2D和3D噪声算子复制中表现出有效性。

Conclusion: SON为算子学习中的不确定性量化提供了一种有效的新方法。

Abstract: We develop a novel framework for uncertainty quantification in operator
learning, the Stochastic Operator Network (SON). SON combines the stochastic
optimal control concepts of the Stochastic Neural Network (SNN) with the
DeepONet. By formulating the branch net as an SDE and backpropagating through
the adjoint BSDE, we replace the gradient of the loss function with the
gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.
This allows SON to learn the uncertainty present in operators through its
diffusion parameters. We then demonstrate the effectiveness of SON when
replicating several noisy operators in 2D and 3D.

</details>


### [282] [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](https://arxiv.org/abs/2507.10425)
*Alvaro H. C. Correia,Christos Louizos*

Main category: cs.LG

TL;DR: 本文提出了一种基于最优传输的新视角，用于估计和缓解分布偏移下共形预测的覆盖率损失。


<details>
  <summary>Details</summary>
Motivation: 共形预测在分布偏移下可能失效，现有方法需预先知道偏移类型，限制了实用性。

Method: 通过最优传输的视角，估计并缓解分布偏移对覆盖率的影响。

Result: 展示了在分布偏移下估计覆盖率损失并有效缓解的方法。

Conclusion: 最优传输为共形预测在非交换性数据下的应用提供了新思路。

Abstract: Conformal prediction is a distribution-free uncertainty quantification method
that has gained popularity in the machine learning community due to its
finite-sample guarantees and ease of use. Its most common variant, dubbed split
conformal prediction, is also computationally efficient as it boils down to
collecting statistics of the model predictions on some calibration data not yet
seen by the model. Nonetheless, these guarantees only hold if the calibration
and test data are exchangeable, a condition that is difficult to verify and
often violated in practice due to so-called distribution shifts. The literature
is rife with methods to mitigate the loss in coverage in this non-exchangeable
setting, but these methods require some prior information on the type of
distribution shift to be expected at test time. In this work, we study this
problem via a new perspective, through the lens of optimal transport, and show
that it is possible to estimate the loss in coverage and mitigate it in case of
distribution shift.

</details>


### [283] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: CLA是一种自监督学习方法，用于在线持续学习场景，通过对齐当前与过去的表征减少遗忘，提升训练收敛速度，并在预训练阶段表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决在线持续学习中数据以小批量到达、计算预算固定且无任务边界时，自监督学习方法不足的问题。

Method: 提出Continual Latent Alignment (CLA)，通过对齐当前模型学到的表征与过去表征来减少遗忘。

Result: CLA在在线场景中加速训练收敛，并在相同计算预算下优于现有方法；作为预训练协议时，其早期表现优于传统i.i.d.预训练。

Conclusion: CLA是一种有效的在线持续学习自监督策略，兼具训练效率和最终性能优势。

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


### [284] [Some remarks on gradient dominance and LQR policy optimization](https://arxiv.org/abs/2507.10452)
*Eduardo D. Sontag*

Main category: cs.LG

TL;DR: 论文探讨了优化问题中梯度下降的变体及其收敛性，特别是通过Polyak-Łojasiewicz不等式（PLI）分析连续时间和离散时间LQR问题的差异，并提出了广义PLI条件以解决梯度估计误差的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决连续时间LQR问题中收敛速率随初始条件变化而消失的问题，以及离散时间LQR问题中全局指数收敛的差异，同时探讨梯度估计误差对优化的影响。

Method: 方法包括应用广义PLI条件分析收敛性，并通过输入到状态稳定性（ISS）框架研究梯度估计误差的影响。

Result: 结果表明连续时间LQR问题存在混合的全局线性/局部指数收敛行为，而离散时间LQR问题则具有全局指数收敛性。广义PLI条件有助于理解梯度误差的瞬态和渐近效应。

Conclusion: 结论指出广义PLI条件是理解优化问题收敛性和梯度误差影响的关键，未来工作可能进一步探讨线性前馈神经网络在反馈控制中的收敛性。

Abstract: Solutions of optimization problems, including policy optimization in
reinforcement learning, typically rely upon some variant of gradient descent.
There has been much recent work in the machine learning, control, and
optimization communities applying the Polyak-{\L}ojasiewicz Inequality (PLI) to
such problems in order to establish an exponential rate of convergence (a.k.a.
``linear convergence'' in the local-iteration language of numerical analysis)
of loss functions to their minima under the gradient flow. Often, as is the
case of policy iteration for the continuous-time LQR problem, this rate
vanishes for large initial conditions, resulting in a mixed globally linear /
locally exponential behavior. This is in sharp contrast with the discrete-time
LQR problem, where there is global exponential convergence. That gap between CT
and DT behaviors motivates the search for various generalized PLI-like
conditions, and this talk will address that topic. Moreover, these
generalizations are key to understanding the transient and asymptotic effects
of errors in the estimation of the gradient, errors which might arise from
adversarial attacks, wrong evaluation by an oracle, early stopping of a
simulation, inaccurate and very approximate digital twins, stochastic
computations (algorithm ``reproducibility''), or learning by sampling from
limited data. We describe an ``input to state stability'' (ISS) analysis of
this issue. The lecture also discussed convergence and PLI-like properties of
``linear feedforward neural networks'' in feedback control, but this arXiv
skips that part (to be updated). Much of the work described here was done in
collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping
Jiang, and Milad Siami.

</details>


### [285] [The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization](https://arxiv.org/abs/2507.10484)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: 提出了一种名为“Target Polish”的高效非负矩阵和张量分解框架，通过加权中位数变换实现异常值抵抗，同时保持Fast-HALS算法的高效性。


<details>
  <summary>Details</summary>
Motivation: 传统加权NMF方法对异常值具有抵抗性，但收敛速度慢。本文旨在提出一种既能抵抗异常值又能保持计算效率的方法。

Method: 采用加权中位数变换自适应平滑数据，与Fast-HALS算法兼容，保持其高效加法更新结构。

Result: 在图像数据集上，Target Polish在精度上匹配或超越现有鲁棒NMF方法，计算时间减少一个数量级。

Conclusion: Target Polish是一种高效且鲁棒的非负矩阵和张量分解方法，适用于处理异常值问题。

Abstract: This paper introduces the "Target Polish," a robust and computationally
efficient framework for nonnegative matrix and tensor factorization. Although
conventional weighted NMF approaches are resistant to outliers, they converge
slowly due to the use of multiplicative updates to minimize the objective
criterion. In contrast, the Target Polish approach remains compatible with the
Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing
the data with a weighted median-based transformation. This innovation provides
outlier resistance while maintaining the highly efficient additive update
structure of Fast-HALS. Empirical evaluations using image datasets corrupted
with structured (block) and unstructured (salt) noise demonstrate that the
Target Polish approach matches or exceeds the accuracy of state-of-the-art
robust NMF methods and reduces computational time by an order of magnitude in
the studied scenarios.

</details>


### [286] [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/2507.10485)
*Brandon Shuen Yi Loke,Filippo Quadri,Gabriel Vivanco,Maximilian Casagrande,Saúl Fenollosa*

Main category: cs.LG

TL;DR: 论文研究了弹性权重巩固（EWC）在持续学习中的作用，验证了其在减少遗忘方面的有效性，同时探讨了其在监督学习中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在持续学习中的灾难性遗忘问题，验证EWC方法的有效性。

Method: 使用PermutedMNIST和RotatedMNIST基准测试，比较EWC与L2正则化和SGD的表现。

Result: EWC显著减少了遗忘，但略微降低了新任务的学习效率。

Conclusion: EWC是神经网络终身学习的可行解决方案。

Abstract: Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

</details>


### [287] [Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing](https://arxiv.org/abs/2507.10494)
*Tanveer Khan,Mindaugas Budzys,Antonis Michalas*

Main category: cs.LG

TL;DR: SplitHappens结合FSS和U型SL，提升数据隐私保护，同时降低通信和计算成本，并抵御多种攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管SL在保护客户端数据方面有潜力，但其易受攻击，隐私保护效果存疑。SplitHappens旨在通过FSS和U型SL提升安全性。

Method: 采用FSS和U型SL结合的方法，隐藏公共输入并随机掩码，保护标签数据不被服务器获取。

Result: 实验表明，该方法在减少训练时间和通信成本的同时，保持了与之前相当的准确性。

Conclusion: SplitHappens通过U型SL和FSS的结合，提供了更高的安全性，同时优化了性能。

Abstract: Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.

</details>


### [288] [On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance](https://arxiv.org/abs/2507.10536)
*Qiaoyue Tang,Alain Zhiyanov,Mathias Lécuyer*

Main category: cs.LG

TL;DR: 论文分析了在重尾类别不平衡分布下，常见隐私学习优化算法的行为，发现DP-GD在低频类别学习中表现不佳，而DP-AdamBC通过消除DP偏差显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究在重尾类别不平衡分布下，隐私学习优化算法的表现差异，以改进低频类别的学习效果。

Method: 比较了DP-GD和DP-AdamBC在重尾类别不平衡数据上的优化行为，DP-AdamBC通过消除损失曲率的DP偏差来改善学习。

Result: DP-AdamBC在低频类别学习中表现更优，训练准确率提升了约8%（实验数据）和5%（真实数据）。

Conclusion: DP-AdamBC能有效缓解重尾类别不平衡带来的问题，提升低频类别的学习效果。

Abstract: In this work, we analyze the optimization behaviour of common private
learning optimization algorithms under heavy-tail class imbalanced
distribution. We show that, in a stylized model, optimizing with Gradient
Descent with differential privacy (DP-GD) suffers when learning low-frequency
classes, whereas optimization algorithms that estimate second-order information
do not. In particular, DP-AdamBC that removes the DP bias from estimating loss
curvature is a crucial component to avoid the ill-condition caused by
heavy-tail class imbalance, and empirically fits the data better with
$\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the
least frequent classes on both controlled experiments and real data
respectively.

</details>


### [289] [Graph World Model](https://arxiv.org/abs/2507.10539)
*Tao Feng,Yexin Wu,Guanyu Lin,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出了Graph World Model (GWM)，支持多模态和结构化数据，通过消息传递算法和动作节点实现跨领域任务。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型无法处理结构化数据（如图），且图基础模型局限于图学习任务，无法扩展到多模态数据和跨学科任务。

Method: GWM通过通用消息传递算法聚合结构化信息，支持多模态数据（文本或嵌入空间），并引入动作节点链接其他节点。

Result: 在六种跨领域任务中，GWM表现优于或匹配领域专用基线，并展示零样本/少样本能力。

Conclusion: GWM为多模态和结构化数据的跨领域任务提供了通用解决方案，具有广泛适用性。

Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation,
and planning tasks. Existing WMs primarily focus on unstructured data and
cannot leverage the ubiquitous structured data, often represented as graphs, in
the digital world. While multiple graph foundation models have been proposed,
they focus on graph learning tasks and cannot extend to diverse multi-modal
data and interdisciplinary tasks. To address these challenges, we propose the
Graph World Model (GWM), a world model that supports both unstructured and
graph-structured states with multi-modal information and represents diverse
tasks as actions. The core of a GWM is a generic message-passing algorithm to
aggregate structured information, either over a unified multi-modal token space
by converting multi-modal data into text (GWM-T) or a unified multi-modal
embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces
action nodes to support diverse tasks, where action nodes are linked to other
nodes via direct reference or similarity computation. Extensive experiments on
six tasks from diverse domains, including multi-modal generation and matching,
recommendation, graph prediction, multi-agent, retrieval-augmented generation,
and planning and optimization, show that the same GWM outperforms or matches
domain-specific baselines' performance, benefits from multi-hop structures, and
demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our
code for GWM is released at https://github.com/ulab-uiuc/GWM.

</details>


### [290] [Fusing LLM Capabilities with Routing Data](https://arxiv.org/abs/2507.10540)
*Tao Feng,Haozhen Zhang,Zijie Lei,Pengrui Han,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jiaxuan You*

Main category: cs.LG

TL;DR: 论文提出FusionBench和FusionFactory，通过多级融合框架（查询级、思维级、模型级）优化LLM路由，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有应用多依赖单一后端模型，无法充分利用不同LLM的互补优势，导致性能和成本效率低下。

Method: 提出FusionBench基准和FusionFactory框架，包括查询级、思维级和模型级融合，利用路由数据和思维模板优化模型选择。

Result: FusionFactory在14个基准测试中均优于单一LLM，展示了系统融合的价值。

Conclusion: 系统化的LLM融合能有效利用互补优势，提升整体性能。

Abstract: The rapid advancement of large language models (LLMs) has created a vibrant
ecosystem of diverse architectures, each with unique strengths due to
differences in design, training data, and objectives. However, most
applications still rely on a single backend model, limiting coverage of
capabilities and leading to inefficiencies in performance and token cost when
tackling complex tasks. We highlight an underexploited opportunity: LLM routing
data, produced when hosting platforms route diverse queries to different
models, which can reveal comparative strengths across tasks. To address this,
we propose FusionBench, a comprehensive routing benchmark covering 14 tasks
across five domains with 20 open-source LLMs (8B to 671B parameters), capturing
103M tokens and summarizing reusable thought templates from top models.
Building on this, we introduce FusionFactory, a systematic fusion framework
with three levels: (1) query-level fusion, tailoring routers for each query
using both direct responses and reasoning-augmented outputs; (2) thought-level
fusion, leveraging abstract templates derived from top-performing LLMs' answers
to similar queries; and (3) model-level fusion, transferring capabilities
between models via distillation, using top responses or highest judge scores as
training data. Experiments show FusionFactory consistently outperforms the best
individual LLM across all 14 benchmarks, with optimal fusion configurations
varying by benchmark, demonstrating the value of systematic LLM fusion in
harnessing complementary strengths and improving overall performance.

</details>
