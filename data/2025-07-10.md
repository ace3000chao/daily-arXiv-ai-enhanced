<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.HC](#cs.HC) [Total: 10]
- [cs.LG](#cs.LG) [Total: 72]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities](https://arxiv.org/abs/2507.06261)
*Gheorghe Comanici,Eric Bieber,Mike Schaekermann,Ice Pasupat,Noveen Sachdeva,Inderjit Dhillon,Marcel Blistein,Ori Ram,Dan Zhang,Evan Rosen,Luke Marris,Sam Petulla,Colin Gaffney,Asaf Aharoni,Nathan Lintz,Tiago Cardal Pais,Henrik Jacobsson,Idan Szpektor,Nan-Jiang Jiang,Krishna Haridasan,Ahmed Omran,Nikunj Saunshi,Dara Bahri,Gaurav Mishra,Eric Chu,Toby Boyd,Brad Hekman,Aaron Parisi,Chaoyi Zhang,Kornraphop Kawintiranon,Tania Bedrax-Weiss,Oliver Wang,Ya Xu,Ollie Purkiss,Uri Mendlovic,Ilaï Deutel,Nam Nguyen,Adam Langley,Flip Korn,Lucia Rossazza,Alexandre Ramé,Sagar Waghmare,Helen Miller,Vaishakh Keshava,Ying Jian,Xiaofan Zhang,Raluca Ada Popa,Kedar Dhamdhere,Blaž Bratanič,Kyuyeun Kim,Terry Koo,Ferran Alet,Yi-ting Chen,Arsha Nagrani,Hannah Muckenhirn,Zhiyuan Zhang,Corbin Quick,Filip Pavetić,Duc Dung Nguyen,Joao Carreira,Michael Elabd,Haroon Qureshi,Fabian Mentzer,Yao-Yuan Yang,Danielle Eisenbud,Anmol Gulati,Ellie Talius,Eric Ni,Sahra Ghalebikesabi,Edouard Yvinec,Alaa Saade,Thatcher Ulrich,Lorenzo Blanco,Dan A. Calian,Muhuan Huang,Aäron van den Oord,Naman Goyal,Terry Chen,Praynaa Rawlani,Christian Schallhart,Swachhand Lokhande,Xianghong Luo,Jyn Shan,Ceslee Montgomery,Victoria Krakovna,Federico Piccinini,Omer Barak,Jingyu Cui,Yiling Jia,Mikhail Dektiarev,Alexey Kolganov,Shiyu Huang,Zhe Chen,Xingyu Wang,Jessica Austin,Peter de Boursac,Evgeny Sluzhaev,Frank Ding,Huijian Li,Surya Bhupatiraju,Mohit Agarwal,Sławek Kwasiborski,Paramjit Sandhu,Patrick Siegler,Ahmet Iscen,Eyal Ben-David,Shiraz Butt,Miltos Allamanis,Seth Benjamin,Robert Busa-Fekete,Felix Hernandez-Campos,Sasha Goldshtein,Matt Dibb,Weiyang Zhang,Annie Marsden,Carey Radebaugh,Stephen Roller,Abhishek Nayyar,Jacob Austin,Tayfun Terzi,Bhargav Kanagal Shamanna,Pete Shaw,Aayush Singh,Florian Luisier,Artur Mendonça,Vaibhav Aggarwal,Larisa Markeeva,Claudio Fantacci,Sergey Brin,HyunJeong Choe,Guanyu Wang,Hartwig Adam,Avigail Dabush,Tatsuya Kiyono,Eyal Marcus,Jeremy Cole,Theophane Weber,Hongrae Lee,Ronny Huang,Alex Muzio,Leandro Kieliger,Maigo Le,Courtney Biles,Long Le,Archit Sharma,Chengrun Yang,Avery Lamp,Dave Dopson,Nate Hurley,Katrina,Xu,Zhihao Shan,Shuang Song,Jiewen Tan,Alexandre Senges,George Zhang,Chong You,Yennie Jun,David Raposo,Susanna Ricco,Xuan Yang,Weijie Chen,Prakhar Gupta,Arthur Szlam,Kevin Villela,Chun-Sung Ferng,Daniel Kasenberg,Chen Liang,Rui Zhu,Arunachalam Narayanaswamy,Florence Perot,Paul Pucciarelli,Anna Shekhawat,Alexey Stern,Rishikesh Ingale,Stefani Karp,Sanaz Bahargam,Adrian Goedeckemeyer,Jie Han,Sicheng Li,Andrea Tacchetti,Dian Yu,Abhishek Chakladar,Zhiying Zhang,Mona El Mahdy,Xu Gao,Dale Johnson,Samrat Phatale,AJ Piergiovanni,Hyeontaek Lim,Clement Farabet,Carl Lebsack,Theo Guidroz,John Blitzer,Nico Duduta,David Madras,Steve Li,Daniel von Dincklage,Xin Li,Mahdis Mahdieh,George Tucker,Ganesh Jawahar,Owen Xiao,Danny Tarlow,Robert Geirhos,Noam Velan,Daniel Vlasic,Kalesha Bullard,SK Park,Nishesh Gupta,Kellie Webster,Ayal Hitron,Jieming Mao,Julian Eisenschlos,Laurel Prince,Nina D'Souza,Kelvin Zheng,Sara Nasso,Gabriela Botea,Carl Doersch,Caglar Unlu,Chris Alberti,Alexey Svyatkovskiy,Ankita Goel,Krzysztof Choromanski,Pan-Pan Jiang,Richard Nguyen,Four Flynn,Daria Ćurko,Peter Chen,Nicholas Roth,Kieran Milan,Caleb Habtegebriel,Shashi Narayan,Michael Moffitt,Jake Marcus,Thomas Anthony,Brendan McMahan,Gowoon Cheon,Ruibo Liu,Megan Barnes,Lukasz Lew,Rebeca Santamaria-Fernandez,Mayank Upadhyay,Arjun Akula,Arnar Mar Hrafnkelsson,Alvaro Caceres,Andrew Bunner,Michal Sokolik,Subha Puttagunta,Lawrence Moore,Berivan Isik,Weilun Chen,Jay Hartford,Lawrence Chan,Pradeep Shenoy,Dan Holtmann-Rice,Jane Park,Fabio Viola,Alex Salcianu,Sujeevan Rajayogam,Ian Stewart-Binks,Zelin Wu,Richard Everett,Xi Xiong,Pierre-Antoine Manzagol,Gary Leung,Carl Saroufim,Bo Pang,Dawid Wegner,George Papamakarios,Jennimaria Palomaki,Helena Pankov,Guangda Lai,Guilherme Tubone,Shubin Zhao,Theofilos Strinopoulos,Seth Neel,Mingqiu Wang,Joe Kelley,Li Li,Pingmei Xu,Anitha Vijayakumar,Andrea D'olimpio,Omer Levy,Massimo Nicosia,Grigory Rozhdestvenskiy,Ni Lao,Sirui Xie,Yash Katariya,Jon Simon,Sanjiv Kumar,Florian Hartmann,Michael Kilgore,Jinhyuk Lee,Aroma Mahendru,Roman Ring,Tom Hennigan,Fiona Lang,Colin Cherry,David Steiner,Dawsen Hwang,Ray Smith,Pidong Wang,Jeremy Chen,Ming-Hsuan Yang,Sam Kwei,Philippe Schlattner,Donnie Kim,Ganesh Poomal Girirajan,Nikola Momchev,Ayushi Agarwal,Xingyi Zhou,Ilkin Safarli,Zachary Garrett,AJ Pierigiovanni,Sarthak Jauhari,Alif Raditya Rochman,Shikhar Vashishth,Quan Yuan,Christof Angermueller,Jon Blanton,Xinying Song,Nitesh Bharadwaj Gundavarapu,Thi Avrahami,Maxine Deines,Subhrajit Roy,Manish Gupta,Christopher Semturs,Shobha Vasudevan,Aditya Srikanth Veerubhotla,Shriya Sharma,Josh Jacob,Zhen Yang,Andreas Terzis,Dan Karliner,Auriel Wright,Tania Rojas-Esponda,Ashley Brown,Abhijit Guha Roy,Pawan Dogra,Andrei Kapishnikov,Peter Young,Wendy Kan,Vinodh Kumar Rajendran,Maria Ivanova,Salil Deshmukh,Chia-Hua Ho,Mike Kwong,Stav Ginzburg,Annie Louis,KP Sawhney,Slav Petrov,Jing Xie,Yunfei Bai,Georgi Stoyanov,Alex Fabrikant,Rajesh Jayaram,Yuqi Li,Joe Heyward,Justin Gilmer,Yaqing Wang,Radu Soricut,Luyang Liu,Qingnan Duan,Jamie Hayes,Maura O'Brien,Gaurav Singh Tomar,Sivan Eiger,Bahar Fatemi,Jeffrey Hui,Catarina Barros,Adaeze Chukwuka,Alena Butryna,Saksham Thakur,Austin Huang,Zhufeng Pan,Haotian Tang,Serkan Cabi,Tulsee Doshi,Michiel Bakker,Sumit Bagri,Ruy Ley-Wild,Adam Lelkes,Jennie Lees,Patrick Kane,David Greene,Shimu Wu,Jörg Bornschein,Gabriela Surita,Sarah Hodkinson,Fangtao Li,Chris Hidey,Sébastien Pereira,Sean Ammirati,Phillip Lippe,Adam Kraft,Pu Han,Sebastian Gerlach,Zifeng Wang,Liviu Panait,Feng Han,Brian Farris,Yingying Bi,Hannah DeBalsi,Miaosen Wang,Gladys Tyen,James Cohan,Susan Zhang,Jarred Barber,Da-Woon Chung,Jaeyoun Kim,Markus Kunesch,Steven Pecht,Nami Akazawa,Abe Friesen,James Lyon,Ali Eslami,Junru Wu,Jie Tan,Yue Song,Ravi Kumar,Chris Welty,Ilia Akolzin,Gena Gibson,Sean Augenstein,Arjun Pillai,Nancy Yuen,Du Phan,Xin Wang,Iain Barr,Heiga Zen,Nan Hua,Casper Liu,Jilei,Wang,Tanuj Bhatia,Hao Xu,Oded Elyada,Pushmeet Kohli,Mirek Olšák,Ke Chen,Azalia Mirhoseini,Noam Shazeer,Shoshana Jakobovits,Maggie Tran,Nolan Ramsden,Tarun Bharti,Fred Alcober,Yunjie Li,Shilpa Shetty,Jing Chen,Dmitry Kalashnikov,Megha Nawhal,Sercan Arik,Hanwen Chen,Michiel Blokzijl,Shubham Gupta,James Rubin,Rigel Swavely,Sophie Bridgers,Ian Gemp,Chen Su,Arun Suggala,Juliette Pluto,Mary Cassin,Alain Vaucher,Kaiyang Ji,Jiahao Cai,Andrew Audibert,Animesh Sinha,David Tian,Efrat Farkash,Amy Hua,Jilin Chen,Duc-Hieu Tran,Edward Loper,Nicole Brichtova,Lara McConnaughey,Ballie Sandhu,Robert Leland,Doug DeCarlo,Andrew Over,James Huang,Xing Wu,Connie Fan,Eric Li,Yun Lei,Deepak Sharma,Cosmin Paduraru,Luo Yu,Matko Bošnjak,Phuong Dao,Min Choi,Sneha Kudugunta,Jakub Adamek,Carlos Guía,Ali Khodaei,Jie Feng,Wenjun Zeng,David Welling,Sandeep Tata,Christina Butterfield,Andrey Vlasov,Seliem El-Sayed,Swaroop Mishra,Tara Sainath,Shentao Yang,RJ Skerry-Ryan,Jeremy Shar,Robert Berry,Arunkumar Rajendran,Arun Kandoor,Andrea Burns,Deepali Jain,Tom Stone,Wonpyo Park,Shibo Wang,Albin Cassirer,Guohui Wang,Hayato Kobayashi,Sergey Rogulenko,Vineetha Govindaraj,Mikołaj Rybiński,Nadav Olmert,Colin Evans,Po-Sen Huang,Kelvin Xu,Premal Shah,Terry Thurk,Caitlin Sikora,Mu Cai,Jin Xie,Elahe Dabir,Saloni Shah,Norbert Kalb,Carrie Zhang,Shruthi Prabhakara,Amit Sabne,Artiom Myaskovsky,Vikas Raunak,Blanca Huergo,Behnam Neyshabur,Jon Clark,Ye Zhang,Shankar Krishnan,Eden Cohen,Dinesh Tewari,James Lottes,Yumeya Yamamori,Hui,Li,Mohamed Elhawaty,Ada Maksutaj Oflazer,Adrià Recasens,Sheryl Luo,Duy Nguyen,Taylor Bos,Kalyan Andra,Ana Salazar,Ed Chi,Jeongwoo Ko,Matt Ginsberg,Anders Andreassen,Anian Ruoss,Todor Davchev,Elnaz Davoodi,Chenxi Liu,Min Kim,Santiago Ontanon,Chi Ming To,Dawei Jia,Rosemary Ke,Jing Wang,Anna Korsun,Moran Ambar,Ilya Kornakov,Irene Giannoumis,Toni Creswell,Denny Zhou,Yi Su,Ishaan Watts,Aleksandr Zaks,Evgenii Eltyshev,Ziqiang Feng,Sidharth Mudgal,Alex Kaskasoli,Juliette Love,Kingshuk Dasgupta,Sam Shleifer,Richard Green,Sungyong Seo,Chansoo Lee,Dale Webster,Prakash Shroff,Ganna Raboshchuk,Isabel Leal,James Manyika,Sofia Erell,Daniel Murphy,Zhisheng Xiao,Anton Bulyenov,Julian Walker,Mark Collier,Matej Kastelic,Nelson George,Sushant Prakash,Sailesh Sidhwani,Alexey Frolov,Steven Hansen,Petko Georgiev,Tiberiu Sosea,Chris Apps,Aishwarya Kamath,David Reid,Emma Cooney,Charlotte Magister,Oriana Riva,Alec Go,Pu-Chin Chen,Sebastian Krause,Nir Levine,Marco Fornoni,Ilya Figotin,Nick Roy,Parsa Mahmoudieh,Vladimir Magay,Mukundan Madhavan,Jin Miao,Jianmo Ni,Yasuhisa Fujii,Ian Chou,George Scrivener,Zak Tsai,Siobhan Mcloughlin,Jeremy Selier,Sandra Lefdal,Jeffrey Zhao,Abhijit Karmarkar,Kushal Chauhan,Shivanker Goel,Zhaoyi Zhang,Vihan Jain,Parisa Haghani,Mostafa Dehghani,Jacob Scott,Erin Farnese,Anastasija Ilić,Steven Baker,Julia Pawar,Li Zhong,Josh Camp,Yoel Zeldes,Shravya Shetty,Anand Iyer,Vít Listík,Jiaxian Guo,Luming Tang,Mark Geller,Simon Bucher,Yifan Ding,Hongzhi Shi,Carrie Muir,Dominik Grewe,Ramy Eskander,Octavio Ponce,Boqing Gong,Derek Gasaway,Samira Khan,Umang Gupta,Angelos Filos,Weicheng Kuo,Klemen Kloboves,Jennifer Beattie,Christian Wright,Leon Li,Alicia Jin,Sandeep Mariserla,Miteyan Patel,Jens Heitkaemper,Dilip Krishnan,Vivek Sharma,David Bieber,Christian Frank,John Lambert,Paul Caron,Martin Polacek,Mai Giménez,Himadri Choudhury,Xing Yu,Sasan Tavakkol,Arun Ahuja,Franz Och,Rodolphe Jenatton,Wojtek Skut,Bryan Richter,David Gaddy,Andy Ly,Misha Bilenko,Megh Umekar,Ethan Liang,Martin Sevenich,Mandar Joshi,Hassan Mansoor,Rebecca Lin,Sumit Sanghai,Abhimanyu Singh,Xiaowei Li,Sudheendra Vijayanarasimhan,Zaheer Abbas,Yonatan Bitton,Hansa Srinivasan,Manish Reddy Vuyyuru,Alexander Frömmgen,Yanhua Sun,Ralph Leith,Alfonso Castaño,DJ Strouse,Le Yan,Austin Kyker,Satish Kambala,Mary Jasarevic,Thibault Sellam,Chao Jia,Alexander Pritzel,Raghavender R,Huizhong Chen,Natalie Clay,Sudeep Gandhe,Sean Kirmani,Sayna Ebrahimi,Hannah Kirkwood,Jonathan Mallinson,Chao Wang,Adnan Ozturel,Kuo Lin,Shyam Upadhyay,Vincent Cohen-Addad,Sean Purser-haskell,Yichong Xu,Ebrahim Songhori,Babi Seal,Alberto Magni,Almog Gueta,Tingting Zou,Guru Guruganesh,Thais Kagohara,Hung Nguyen,Khalid Salama,Alejandro Cruzado Ruiz,Justin Frye,Zhenkai Zhu,Matthias Lochbrunner,Simon Osindero,Wentao Yuan,Lisa Lee,Aman Prasad,Lam Nguyen Thiet,Daniele Calandriello,Victor Stone,Qixuan Feng,Han Ke,Maria Voitovich,Geta Sampemane,Lewis Chiang,Ling Wu,Alexander Bykovsky,Matt Young,Luke Vilnis,Ishita Dasgupta,Aditya Chawla,Qin Cao,Bowen Liang,Daniel Toyama,Szabolcs Payrits,Anca Stefanoiu,Dimitrios Vytiniotis,Ankesh Anand,Tianxiao Shen,Blagoj Mitrevski,Michael Tschannen,Sreenivas Gollapudi,Aishwarya P S,José Leal,Zhe Shen,Han Fu,Wei Wang,Arvind Kannan,Doron Kukliansky,Sergey Yaroshenko,Svetlana Grant,Umesh Telang,David Wood,Alexandra Chronopoulou,Alexandru Ţifrea,Tao Zhou,Tony,Nguy\~ên,Muge Ersoy,Anima Singh,Meiyan Xie,Emanuel Taropa,Woohyun Han,Eirikur Agustsson,Andrei Sozanschi,Hui Peng,Alex Chen,Yoel Drori,Efren Robles,Yang Gao,Xerxes Dotiwalla,Ying Chen,Anudhyan Boral,Alexei Bendebury,John Nham,Chris Tar,Luis Castro,Jiepu Jiang,Canoee Liu,Felix Halim,Jinoo Baek,Andy Wan,Jeremiah Liu,Yuan Cao,Shengyang Dai,Trilok Acharya,Ruoxi Sun,Fuzhao Xue,Saket Joshi,Morgane Lustman,Yongqin Xian,Rishabh Joshi,Deep Karkhanis,Nora Kassner,Jamie Hall,Xiangzhuo Ding,Gan Song,Gang Li,Chen Zhu,Yana Kulizhskaya,Bin Ni,Alexey Vlaskin,Solomon Demmessie,Lucio Dery,Salah Zaiem,Yanping Huang,Cindy Fan,Felix Gimeno,Ananth Balashankar,Koji Kojima,Hagai Taitelbaum,Maya Meng,Dero Gharibian,Sahil Singla,Wei Chen,Ambrose Slone,Guanjie Chen,Sujee Rajayogam,Max Schumacher,Suyog Kotecha,Rory Blevins,Qifei Wang,Mor Hazan Taege,Alex Morris,Xin Liu,Fayaz Jamil,Richard Zhang,Pratik Joshi,Ben Ingram,Tyler Liechty,Ahmed Eleryan,Scott Baird,Alex Grills,Gagan Bansal,Shan Han,Kiran Yalasangi,Shawn Xu,Majd Al Merey,Isabel Gao,Felix Weissenberger,Igor Karpov,Robert Riachi,Ankit Anand,Gautam Prasad,Kay Lamerigts,Reid Hayes,Jamie Rogers,Mandy Guo,Ashish Shenoy,Qiong,Hu,Kyle He,Yuchen Liu,Polina Zablotskaia,Sagar Gubbi,Yifan Chang,Jay Pavagadhi,Kristian Kjems,Archita Vadali,Diego Machado,Yeqing Li,Renshen Wang,Dipankar Ghosh,Aahil Mehta,Dana Alon,George Polovets,Alessio Tonioni,Nate Kushman,Joel D'sa,Lin Zhuo,Allen Wu,Rohin Shah,John Youssef,Jiayu Ye,Justin Snyder,Karel Lenc,Senaka Buthpitiya,Matthew Tung,Jichuan Chang,Tao Chen,David Saxton,Jenny Lee,Lydia Lihui Zhang,James Qin,Prabakar Radhakrishnan,Maxwell Chen,Piotr Ambroszczyk,Metin Toksoz-Exley,Yan Zhong,Nitzan Katz,Brendan O'Donoghue,Tamara von Glehn,Adi Gerzi Rosenthal,Aga Świetlik,Xiaokai Zhao,Nick Fernando,Jinliang Wei,Jieru Mei,Sergei Vassilvitskii,Diego Cedillo,Pranjal Awasthi,Hui Zheng,Koray Kavukcuoglu,Itay Laish,Joseph Pagadora,Marc Brockschmidt,Christopher A. Choquette-Choo,Arunkumar Byravan,Yifeng Lu,Xu Chen,Mia Chen,Kenton Lee,Rama Pasumarthi,Sijal Bhatnagar,Aditya Shah,Qiyin Wu,Zhuoyuan Chen,Zack Nado,Bartek Perz,Zixuan Jiang,David Kao,Ganesh Mallya,Nino Vieillard,Lantao Mei,Sertan Girgin,Mandy Jordan,Yeongil Ko,Alekh Agarwal,Yaxin Liu,Yasemin Altun,Raoul de Liedekerke,Anastasios Kementsietsidis,Daiyi Peng,Dangyi Liu,Utku Evci,Peter Humphreys,Austin Tarango,Xiang Deng,Yoad Lewenberg,Kevin Aydin,Chengda Wu,Bhavishya Mittal,Tsendsuren Munkhdalai,Kleopatra Chatziprimou,Rodrigo Benenson,Uri First,Xiao Ma,Jinning Li,Armand Joulin,Hamish Tomlinson,Tingnan Zhang,Milad Nasr,Zhi Hong,Michaël Sander,Lisa Anne Hendricks,Anuj Sharma,Andrew Bolt,Eszter Vértes,Jiri Simsa,Tomer Levinboim,Olcan Sercinoglu,Divyansh Shukla,Austin Wu,Craig Swanson,Danny Vainstein,Fan Bu,Bo Wang,Ryan Julian,Charles Yoon,Sergei Lebedev,Antonious Girgis,Bernd Bandemer,David Du,Todd Wang,Xi Chen,Ying Xiao,Peggy Lu,Natalie Ha,Vlad Ionescu,Simon Rowe,Josip Matak,Federico Lebron,Andreas Steiner,Lalit Jain,Manaal Faruqui,Nicolas Lacasse,Georgie Evans,Neesha Subramaniam,Dean Reich,Giulia Vezzani,Aditya Pandey,Joe Stanton,Tianhao Zhou,Liam McCafferty,Henry Griffiths,Verena Rieser,Soheil Hassas Yeganeh,Eleftheria Briakou,Lu Huang,Zichuan Wei,Liangchen Luo,Erik Jue,Gabby Wang,Victor Cotruta,Myriam Khan,Jongbin Park,Qiuchen Guo,Peiran Li,Rong Rong,Diego Antognini,Anastasia Petrushkina,Chetan Tekur,Eli Collins,Parul Bhatia,Chester Kwak,Wenhu Chen,Arvind Neelakantan,Immanuel Odisho,Sheng Peng,Vincent Nallatamby,Vaibhav Tulsyan,Fabian Pedregosa,Peng Xu,Raymond Lin,Yulong Wang,Emma Wang,Sholto Douglas,Reut Tsarfaty,Elena Gribovskaya,Renga Aravamudhan,Manu Agarwal,Mara Finkelstein,Qiao Zhang,Elizabeth Cole,Phil Crone,Sarmishta Velury,Anil Das,Chris Sauer,Luyao Xu,Danfeng Qin,Chenjie Gu,Dror Marcus,CJ Zheng,Wouter Van Gansbeke,Sobhan Miryoosefi,Haitian Sun,YaGuang Li,Charlie Chen,Jae Yoo,Pavel Dubov,Alex Tomala,Adams Yu,Paweł Wesołowski,Alok Gunjan,Eddie Cao,Jiaming Luo,Nikhil Sethi,Arkadiusz Socala,Laura Graesser,Tomas Kocisky,Arturo BC,Minmin Chen,Edward Lee,Sophie Wang,Weize Kong,Qiantong Xu,Nilesh Tripuraneni,Yiming Li,Xinxin Yu,Allen Porter,Paul Voigtlaender,Biao Zhang,Arpi Vezer,Sarah York,Qing Wei,Geoffrey Cideron,Mark Kurzeja,Seungyeon Kim,Benny Li,Angéline Pouget,Hyo Lee,Kaspar Daugaard,Yang Li,Dave Uthus,Aditya Siddhant,Paul Cavallaro,Sriram Ganapathy,Maulik Shah,Rolf Jagerman,Jeff Stanway,Piermaria Mendolicchio,Li Xiao,Kayi Lee,Tara Thompson,Shubham Milind Phal,Jason Chase,Sun Jae Lee,Adrian N Reyes,Disha Shrivastava,Zhen Qin,Roykrong Sukkerd,Seth Odoom,Lior Madmoni,John Aslanides,Jonathan Herzig,Elena Pochernina,Sheng Zhang,Parker Barnes,Daisuke Ikeda,Qiujia Li,Shuo-yiin Chang,Shakir Mohamed,Jim Sproch,Richard Powell,Bidisha Samanta,Domagoj Ćevid,Anton Kovsharov,Shrestha Basu Mallick,Srinivas Tadepalli,Anne Zheng,Kareem Ayoub,Andreas Noever,Christian Reisswig,Zhuo Xu,Junhyuk Oh,Martin Matysiak,Tim Blyth,Shereen Ashraf,Julien Amelot,Boone Severson,Michele Bevilacqua,Motoki Sano,Ethan Dyer,Ofir Roval,Anu Sinha,Yin Zhong,Sagi Perel,Tea Sabolić,Johannes Mauerer,Willi Gierke,Mauro Verzetti,Rodrigo Cabrera,Alvin Abdagic,Steven Hemingray,Austin Stone,Jong Lee,Farooq Ahmad,Karthik Raman,Lior Shani,Jonathan Lai,Orhan Firat,Nathan Waters,Eric Ge,Mo Shomrat,Himanshu Gupta,Rajeev Aggarwal,Tom Hudson,Bill Jia,Simon Baumgartner,Palak Jain,Joe Kovac,Junehyuk Jung,Ante Žužul,Will Truong,Morteza Zadimoghaddam,Songyou Peng,Marco Liang,Rachel Sterneck,Balaji Lakshminarayanan,Machel Reid,Oliver Woodman,Tong Zhou,Jianling Wang,Vincent Coriou,Arjun Narayanan,Jay Hoover,Yenai Ma,Apoorv Jindal,Clayton Sanford,Doug Reid,Swaroop Ramaswamy,Alex Kurakin,Roland Zimmermann,Yana Lunts,Dragos Dena,Zalán Borsos,Vered Cohen,Shujian Zhang,Will Grathwohl,Robert Dadashi,Morgan Redshaw,Joshua Kessinger,Julian Odell,Silvano Bonacina,Zihang Dai,Grace Chen,Ayush Dubey,Pablo Sprechmann,Mantas Pajarskas,Wenxuan Zhou,Niharika Ahuja,Tara Thomas,Martin Nikoltchev,Matija Kecman,Bharath Mankalale,Andrey Ryabtsev,Jennifer She,Christian Walder,Jiaming Shen,Lu Li,Carolina Parada,Sheena Panthaplackel,Okwan Kwon,Matt Lawlor,Utsav Prabhu,Yannick Schroecker,Marc'aurelio Ranzato,Pete Blois,Iurii Kemaev,Ting Yu,Dmitry,Lepikhin,Hao Xiong,Sahand Sharifzadeh,Oleaser Johnson,Jeremiah Willcock,Rui Yao,Greg Farquhar,Sujoy Basu,Hidetoshi Shimokawa,Nina Anderson,Haiguang Li,Khiem Pham,Yizhong Liang,Sebastian Borgeaud,Alexandre Moufarek,Hideto Kazawa,Blair Kutzman,Marcin Sieniek,Sara Smoot,Ruth Wang,Natalie Axelsson,Nova Fallen,Prasha Sundaram,Yuexiang Zhai,Varun Godbole,Petros Maniatis,Alek Wang,Ilia Shumailov,Santhosh Thangaraj,Remi Crocker,Nikita Gupta,Gang Wu,Phil Chen,Gellért Weisz,Celine Smith,Mojtaba Seyedhosseini,Boya Fang,Xiyang Luo,Roey Yogev,Zeynep Cankara,Andrew Hard,Helen Ran,Rahul Sukthankar,George Necula,Gaël Liu,Honglong Cai,Praseem Banzal,Daniel Keysers,Sanjay Ghemawat,Connie Tao,Emma Dunleavy,Aditi Chaudhary,Wei Li,Maciej Mikuła,Chen-Yu Lee,Tiziana Refice,Krishna Somandepalli,Alexandre Fréchette,Dan Bahir,John Karro,Keith Rush,Sarah Perrin,Bill Rosgen,Xiaomeng Yang,Clara Huiyi Hu,Mahmoud Alnahlawi,Justin Mao-Jones,Roopal Garg,Hoang Nguyen,Bat-Orgil Batsaikhan,Iñaki Iturrate,Anselm Levskaya,Avi Singh,Ashyana Kachra,Tony Lu,Denis Petek,Zheng Xu,Mark Graham,Lukas Zilka,Yael Karov,Marija Kostelac,Fangyu Liu,Yaohui Guo,Weiyue Wang,Bernd Bohnet,Emily Pitler,Tony Bruguier,Keisuke Kinoshita,Chrysovalantis Anastasiou,Nilpa Jha,Ting Liu,Jerome Connor,Phil Wallis,Philip Pham,Eric Bailey,Shixin Li,Heng-Tze Cheng,Sally Ma,Haiqiong Li,Akanksha Maurya,Kate Olszewska,Manfred Warmuth,Christy Koh,Dominik Paulus,Siddhartha Reddy Jonnalagadda,Enrique Piqueras,Ali Elqursh,Geoff Brown,Hadar Shemtov,Loren Maggiore,Fei Xia,Ryan Foley,Beka Westberg,George van den Driessche,Livio Baldini Soares,Arjun Kar,Michael Quinn,Siqi Zuo,Jialin Wu,Kyle Kastner,Anna Bortsova,Aijun Bai,Ales Mikhalap,Luowei Zhou,Jennifer Brennan,Vinay Ramasesh,Honglei Zhuang,John Maggs,Johan Schalkwyk,Yuntao Xu,Hui Huang,Andrew Howard,Sasha Brown,Linting Xue,Gloria Shen,Brian Albert,Neha Jha,Daniel Zheng,Varvara Krayvanova,Spurthi Amba Hombaiah,Olivier Lacombe,Gautam Vasudevan,Dan Graur,Tian Xie,Meet Gandhi,Bangju Wang,Dustin Zelle,Harman Singh,Dahun Kim,Sébastien Cevey,Victor Ungureanu,Natasha Noy,Fei Liu,Annie Xie,Fangxiaoyu Feng,Katerina Tsihlas,Daniel Formoso,Neera Vats,Quentin Wellens,Yinan Wang,Niket Kumar Bhumihar,Samrat Ghosh,Matt Hoffman,Tom Lieber,Oran Lang,Kush Bhatia,Tom Paine,Aroonalok Pyne,Ronny Votel,Madeleine Clare Elish,Benoit Schillings,Alex Panagopoulos,Haichuan Yang,Adam Raveret,Zohar Yahav,Shuang Liu,Warren Chen,Dalia El Badawy,Nishant Agrawal,Mohammed Badawi,Mahdi Mirzazadeh,Carla Bromberg,Fan Ye,Chang Liu,Tatiana Sholokhova,George-Cristian Muraru,Gargi Balasubramaniam,Jonathan Malmaud,Alen Carin,Danilo Martins,Irina Jurenka,Pankil Botadra,Dave Lacey,Richa Singh,Mariano Schain,Dan Zheng,Isabelle Guyon,Victor Lavrenko,Seungji Lee,Xiang Zhou,Demis Hassabis,Jeshwanth Challagundla,Derek Cheng,Nikhil Mehta,Matthew Mauger,Michela Paganini,Pushkar Mishra,Kate Lee,Zhang Li,Lexi Baugher,Ondrej Skopek,Max Chang,Amir Zait,Gaurav Menghani,Lizzetth Bellot,Guangxing Han,Jean-Michel Sarr,Sharat Chikkerur,Himanshu Sahni,Rohan Anil,Arun Narayanan,Chandu Thekkath,Daniele Pighin,Hana Strejček,Marko Velic,Fred Bertsch,Manuel Tragut,Keran Rong,Alicia Parrish,Kai Bailey,Jiho Park,Isabela Albuquerque,Abhishek Bapna,Rajesh Venkataraman,Alec Kosik,Johannes Griesser,Zhiwei Deng,Alek Andreev,Qingyun Dou,Kevin Hui,Fanny Wei,Xiaobin Yu,Lei Shu,Avia Aharon,David Barker,Badih Ghazi,Sebastian Flennerhag,Chris Breaux,Yuchuan Liu,Matthew Bilotti,Josh Woodward,Uri Alon,Stephanie Winkler,Tzu-Kuo Huang,Kostas Andriopoulos,João Gabriel Oliveira,Penporn Koanantakool,Berkin Akin,Michael Wunder,Cicero Nogueira dos Santos,Mohammad Hossein Bateni,Lin Yang,Dan Horgan,Beer Changpinyo,Keyvan Amiri,Min Ma,Dayeong Lee,Lihao Liang,Anirudh Baddepudi,Tejasi Latkar,Raia Hadsell,Jun Xu,Hairong Mu,Michael Han,Aedan Pope,Snchit Grover,Frank Kim,Ankit Bhagatwala,Guan Sun,Yamini Bansal,Amir Globerson,Alireza Nazari,Samira Daruki,Hagen Soltau,Jane Labanowski,Laurent El Shafey,Matt Harvey,Yanif Ahmad,Elan Rosenfeld,William Kong,Etienne Pot,Yi-Xuan Tan,Aurora Wei,Victoria Langston,Marcel Prasetya,Petar Veličković,Richard Killam,Robin Strudel,Darren Ni,Zhenhai Zhu,Aaron Archer,Kavya Kopparapu,Lynn Nguyen,Emilio Parisotto,Hussain Masoom,Sravanti Addepalli,Jordan Grimstad,Hexiang Hu,Joss Moore,Avinatan Hassidim,Le Hou,Mukund Raghavachari,Jared Lichtarge,Adam R. Brown,Hilal Dib,Natalia Ponomareva,Justin Fu,Yujing Zhang,Altaf Rahman,Joana Iljazi,Edouard Leurent,Gabriel Dulac-Arnold,Cosmo Du,Chulayuth Asawaroengchai,Larry Jin,Ela Gruzewska,Ziwei Ji,Benigno Uria,Daniel De Freitas,Paul Barham,Lauren Beltrone,Víctor Campos,Jun Yan,Neel Kovelamudi,Arthur Nguyen,Elinor Davies,Zhichun Wu,Zoltan Egyed,Kristina Toutanova,Nithya Attaluri,Hongliang Fei,Peter Stys,Siddhartha Brahma,Martin Izzard,Siva Velusamy,Scott Lundberg,Vincent Zhuang,Kevin Sequeira,Adam Santoro,Ehsan Amid,Ophir Aharoni,Shuai Ye,Mukund Sundararajan,Lijun Yu,Yu-Cheng Ling,Stephen Spencer,Hugo Song,Josip Djolonga,Christo Kirov,Sonal Gupta,Alessandro Bissacco,Clemens Meyer,Mukul Bhutani,Andrew Dai,Weiyi Wang,Siqi Liu,Ashwin Sreevatsa,Qijun Tan,Maria Wang,Lucy Kim,Yicheng Wang,Alex Irpan,Yang Xiao,Stanislav Fort,Yifan He,Alex Gurney,Bryan Gale,Yue Ma,Monica Roy,Viorica Patraucean,Taylan Bilal,Golnaz Ghiasi,Anahita Hosseini,Melvin Johnson,Zhuowan Li,Yi Tay,Benjamin Beyret,Katie Millican,Josef Broder,Mayank Lunayach,Danny Swisher,Eugen Vušak,David Parkinson,MH Tessler,Adi Mayrav Gilady,Richard Song,Allan Dafoe,Yves Raimond,Masa Yamaguchi,Itay Karo,Elizabeth Nielsen,Kevin Kilgour,Mike Dusenberry,Rajiv Mathews,Jiho Choi,Siyuan Qiao,Harsh Mehta,Sahitya Potluri,Chris Knutsen,Jialu Liu,Tat Tan,Kuntal Sengupta,Keerthana Gopalakrishnan,Abodunrinwa Toki,Mencher Chiang,Mike Burrows,Grace Vesom,Zafarali Ahmed,Ilia Labzovsky,Siddharth Vashishtha,Preeti Singh,Ankur Sharma,Ada Ma,Jinyu Xie,Pranav Talluri,Hannah Forbes-Pollard,Aarush Selvan,Joel Wee,Loic Matthey,Tom Funkhouser,Parthasarathy Gopavarapu,Lev Proleev,Cheng Li,Matt Thomas,Kashyap Kolipaka,Zhipeng Jia,Ashwin Kakarla,Srinivas Sunkara,Joan Puigcerver,Suraj Satishkumar Sheth,Emily Graves,Chen Wang,Sadh MNM Khan,Kai Kang,Shyamal Buch,Fred Zhang,Omkar Savant,David Soergel,Kevin Lee,Linda Friso,Xuanyi Dong,Rahul Arya,Shreyas Chandrakaladharan,Connor Schenck,Greg Billock,Tejas Iyer,Anton Bakalov,Leslie Baker,Alex Ruiz,Angad Chandorkar,Trieu Trinh,Matt Miecnikowski,Yanqi Zhou,Yangsibo Huang,Jiazhong Nie,Ali Shah,Ashish Thapliyal,Sam Haves,Lun Wang,Uri Shaham,Patrick Morris-Suzuki,Soroush Radpour,Leonard Berrada,Thomas Strohmann,Chaochao Yan,Jingwei Shen,Sonam Goenka,Tris Warkentin,Petar Dević,Dan Belov,Albert Webson,Madhavi Yenugula,Puranjay Datta,Jerry Chang,Nimesh Ghelani,Aviral Kumar,Vincent Perot,Jessica Lo,Yang Song,Herman Schmit,Jianmin Chen,Vasilisa Bashlovkina,Xiaoyue Pan,Diana Mincu,Paul Roit,Isabel Edkins,Andy Davis,Yujia Li,Ben Horn,Xinjian Li,Pradeep Kumar S,Eric Doi,Wanzheng Zhu,Sri Gayatri Sundara Padmanabhan,Siddharth Verma,Jasmine Liu,Heng Chen,Mihajlo Velimirović,Malcolm Reynolds,Priyanka Agrawal,Nick Sukhanov,Abhinit Modi,Siddharth Goyal,John Palowitch,Nima Khajehnouri,Wing Lowe,David Klinghoffer,Sharon Silver,Vinh Tran,Candice Schumann,Francesco Piccinno,Xi Liu,Mario Lučić,Xiaochen Yang,Sandeep Kumar,Ajay Kannan,Ragha Kotikalapudi,Mudit Bansal,Fabian Fuchs,Javad Hosseini,Abdelrahman Abdelhamed,Dawn Bloxwich,Tianhe Yu,Ruoxin Sang,Gregory Thornton,Karan Gill,Yuchi Liu,Virat Shejwalkar,Jason Lin,Zhipeng Yan,Kehang Han,Thomas Buschmann,Michael Pliskin,Zhi Xing,Susheel Tatineni,Junlin Zhang,Sissie Hsiao,Gavin Buttimore,Marcus Wu,Zefei Li,Geza Kovacs,Legg Yeung,Tao Huang,Aaron Cohen,Bethanie Brownfield,Averi Nowak,Mikel Rodriguez,Tianze Shi,Hado van Hasselt,Kevin Cen,Deepanway Ghoshal,Kushal Majmundar,Weiren Yu,Warren,Chen,Danila Sinopalnikov,Hao Zhang,Vlado Galić,Di Lu,Zeyu Zheng,Maggie Song,Gary Wang,Gui Citovsky,Swapnil Gawde,Isaac Galatzer-Levy,David Silver,Ivana Balazevic,Dipanjan Das,Kingshuk Majumder,Yale Cong,Praneet Dutta,Dustin Tran,Hui Wan,Junwei Yuan,Daniel Eppens,Alanna Walton,Been Kim,Harry Ragan,James Cobon-Kerr,Lu Liu,Weijun Wang,Bryce Petrini,Jack Rae,Rakesh Shivanna,Yan Xiong,Chace Lee,Pauline Coquinot,Yiming Gu,Lisa Patel,Blake Hechtman,Aviel Boag,Orion Jankowski,Alex Wertheim,Alex Lee,Paul Covington,Hila Noga,Sam Sobell,Shanthal Vasanth,William Bono,Chirag Nagpal,Wei Fan,Xavier Garcia,Kedar Soparkar,Aybuke Turker,Nathan Howard,Sachit Menon,Yuankai Chen,Vikas Verma,Vladimir Pchelin,Harish Rajamani,Valentin Dalibard,Ana Ramalho,Yang Guo,Kartikeya Badola,Seojin Bang,Nathalie Rauschmayr,Julia Proskurnia,Sudeep Dasari,Xinyun Chen,Mikhail Sushkov,Anja Hauth,Pauline Sho,Abhinav Singh,Bilva Chandra,Allie Culp,Max Dylla,Olivier Bachem,James Besley,Heri Zhao,Timothy Lillicrap,Wei Wei,Wael Al Jishi,Ning Niu,Alban Rrustemi,Raphaël Lopez Kaufman,Ryan Poplin,Jewel Zhao,Minh Truong,Shikhar Bharadwaj,Ester Hlavnova,Eli Stickgold,Cordelia Schmid,Georgi Stephanov,Zhaoqi Leng,Frederick Liu,Léonard Hussenot,Shenil Dodhia,Juliana Vicente Franco,Lesley Katzen,Abhanshu Sharma,Sarah Cogan,Zuguang Yang,Aniket Ray,Sergi Caelles,Shen Yan,Ravin Kumar,Daniel Gillick,Renee Wong,Joshua Ainslie,Jonathan Hoech,Séb Arnold,Dan Abolafia,Anca Dragan,Ben Hora,Grace Hu,Alexey Guseynov,Yang Lu,Chas Leichner,Jinmeng Rao,Abhimanyu Goyal,Nagabhushan Baddi,Daniel Hernandez Diaz,Tim McConnell,Max Bain,Jake Abernethy,Qiqi Yan,Rylan Schaeffer,Paul Vicol,Will Thompson,Montse Gonzalez Arenas,Mathias Bellaiche,Pablo Barrio,Stefan Zinke,Riccardo Patana,Pulkit Mehta,JK Kearns,Avraham Ruderman,Scott Pollom,David D'Ambrosio,Cath Hope,Yang Yu,Andrea Gesmundo,Kuang-Huei Lee,Aviv Rosenberg,Yiqian Zhou,Yaoyiran Li,Drew Garmon,Yonghui Wu,Safeen Huda,Gil Fidel,Martin Baeuml,Jian Li,Phoebe Kirk,Rhys May,Tao Tu,Sara Mc Carthy,Toshiyuki Fukuzawa,Miranda Aperghis,Chih-Kuan Yeh,Toshihiro Yoshino,Bo Li,Austin Myers,Kaisheng Yao,Ben Limonchik,Changwan Ryu,Rohun Saxena,Alex Goldin,Ruizhe Zhao,Rocky Rhodes,Tao Zhu,Divya Tyam,Heidi Howard,Nathan Byrd,Hongxu Ma,Yan Wu,Ryan Mullins,Qingze Wang,Aida Amini,Sebastien Baur,Yiran Mao,Subhashini Venugopalan,Will Song,Wen Ding,Paul Collins,Sashank Reddi,Megan Shum,Andrei Rusu,Luisa Zintgraf,Kelvin Chan,Sheela Goenka,Mathieu Blondel,Michael Collins,Renke Pan,Marissa Giustina,Nikolai Chinaev,Christian Schuler,Ce Zheng,Jonas Valfridsson,Alyssa Loo,Alex Yakubovich,Jamie Smith,Tao Jiang,Rich Munoz,Gabriel Barcik,Rishabh Bansal,Mingyao Yang,Yilun Du,Pablo Duque,Mary Phuong,Alexandra Belias,Kunal Lad,Zeyu Liu,Tal Schuster,Karthik Duddu,Jieru Hu,Paige Kunkle,Matthew Watson,Jackson Tolins,Josh Smith,Denis Teplyashin,Garrett Bingham,Marvin Ritter,Marco Andreetto,Divya Pitta,Mohak Patel,Shashank Viswanadha,Trevor Strohman,Catalin Ionescu,Jincheng Luo,Yogesh Kalley,Jeremy Wiesner,Dan Deutsch,Derek Lockhart,Peter Choy,Rumen Dangovski,Chawin Sitawarin,Cat Graves,Tanya Lando,Joost van Amersfoort,Ndidi Elue,Zhouyuan Huo,Pooya Moradi,Jean Tarbouriech,Henryk Michalewski,Wenting Ye,Eunyoung Kim,Alex Druinsky,Florent Altché,Xinyi Chen,Artur Dwornik,Da-Cheng Juan,Rivka Moroshko,Horia Toma,Jarrod Kahn,Hai Qian,Maximilian Sieb,Irene Cai,Roman Goldenberg,Praneeth Netrapalli,Sindhu Raghuram,Yuan Gong,Lijie Fan,Evan Palmer,Yossi Matias,Valentin Gabeur,Shreya Pathak,Tom Ouyang,Don Metzler,Geoff Bacon,Srinivasan Venkatachary,Sridhar Thiagarajan,Alex Cullum,Eran Ofek,Vytenis Sakenas,Mohamed Hammad,Cesar Magalhaes,Mayank Daswani,Oscar Chang,Ashok Popat,Ruichao Li,Komal Jalan,Yanhan Hou,Josh Lipschultz,Antoine He,Wenhao Jia,Pier Giuseppe Sessa,Prateek Kolhar,William Wong,Sumeet Singh,Lukas Haas,Jay Whang,Hanna Klimczak-Plucińska,Georges Rotival,Grace Chung,Yiqing Hua,Anfal Siddiqui,Nicolas Serrano,Dongkai Chen,Billy Porter,Libin Bai,Keshav Shivam,Sho Arora,Partha Talukdar,Tom Cobley,Sangnie Bhardwaj,Evgeny Gladchenko,Simon Green,Kelvin Guu,Felix Fischer,Xiao Wu,Eric Wang,Achintya Singhal,Tatiana Matejovicova,James Martens,Hongji Li,Roma Patel,Elizabeth Kemp,Jiaqi Pan,Lily Wang,Blake JianHang Chen,Jean-Baptiste Alayrac,Navneet Potti,Erika Gemzer,Eugene Ie,Kay McKinney,Takaaki Saeki,Edward Chou,Pascal Lamblin,SQ Mah,Zach Fisher,Martin Chadwick,Jon Stritar,Obaid Sarvana,Andrew Hogue,Artem Shtefan,Hadi Hashemi,Yang Xu,Jindong Gu,Sharad Vikram,Chung-Ching Chang,Sabela Ramos,Logan Kilpatrick,Weijuan Xi,Jenny Brennan,Yinghao Sun,Abhishek Jindal,Ionel Gog,Dawn Chen,Felix Wu,Jason Lee,Sudhindra Kopalle,Srinadh Bhojanapalli,Oriol Vinyals,Natan Potikha,Burcu Karagol Ayan,Yuan Yuan,Michael Riley,Piotr Stanczyk,Sergey Kishchenko,Bing Wang,Dan Garrette,Antoine Yang,Vlad Feinberg,CJ Carey,Javad Azizi,Viral Shah,Erica Moreira,Chongyang Shi,Josh Feldman,Elizabeth Salesky,Thomas Lampe,Aneesh Pappu,Duhyeon Kim,Jonas Adler,Avi Caciularu,Brian Walker,Yunhan Xu,Yochai Blau,Dylan Scandinaro,Terry Huang,Sam El-Husseini,Abhishek Sinha,Lijie Ren,Taylor Tobin,Patrik Sundberg,Tim Sohn,Vikas Yadav,Mimi Ly,Emily Xue,Jing Xiong,Afzal Shama Soudagar,Sneha Mondal,Nikhil Khadke,Qingchun Ren,Ben Vargas,Stan Bileschi,Sarah Chakera,Cindy Wang,Boyu Wang,Yoni Halpern,Joe Jiang,Vikas Sindhwani,Petre Petrov,Pranavaraj Ponnuramu,Sanket Vaibhav Mehta,Yu Watanabe,Betty Chan,Matheus Wisniewski,Trang Pham,Jingwei Zhang,Conglong Li,Dario de Cesare,Art Khurshudov,Alex Vasiloff,Melissa Tan,Zoe Ashwood,Bobak Shahriari,Maryam Majzoubi,Garrett Tanzer,Olga Kozlova,Robin Alazard,James Lee-Thorp,Nguyet Minh Phu,Isaac Tian,Junwhan Ahn,Andy Crawford,Lauren Lax,Yuan,Shangguan,Iftekhar Naim,David Ross,Oleksandr Ferludin,Tongfei Guo,Andrea Banino,Hubert Soyer,Xiaoen Ju,Dominika Rogozińska,Ishaan Malhi,Marcella Valentine,Daniel Balle,Apoorv Kulshreshtha,Maciej Kula,Yiwen Song,Sophia Austin,John Schultz,Roy Hirsch,Arthur Douillard,Apoorv Reddy,Michael Fink,Summer Yue,Khyatti Gupta,Adam Zhang,Norman Rink,Daniel McDuff,Lei Meng,András György,Yasaman Razeghi,Ricky Liang,Kazuki Osawa,Aviel Atias,Matan Eyal,Tyrone Hill,Nikolai Grigorev,Zhengdong Wang,Nitish Kulkarni,Rachel Soh,Ivan Lobov,Zachary Charles,Sid Lall,Kazuma Hashimoto,Ido Kessler,Victor Gomes,Zelda Mariet,Danny Driess,Alessandro Agostini,Canfer Akbulut,Jingcao Hu,Marissa Ikonomidis,Emily Caveness,Kartik Audhkhasi,Saurabh Agrawal,Ioana Bica,Evan Senter,Jayaram Mudigonda,Kelly Chen,Jingchen Ye,Xuanhui Wang,James Svensson,Philipp Fränken,Josh Newlan,Li Lao,Eva Schnider,Sami Alabed,Joseph Kready,Jesse Emond,Afief Halumi,Tim Zaman,Chengxi Ye,Naina Raisinghani,Vilobh Meshram,Bo Chang,Ankit Singh Rawat,Axel Stjerngren,Sergey Levi,Rui Wang,Xiangzhu Long,Mitchelle Rasquinha,Steven Hand,Aditi Mavalankar,Lauren Agubuzu,Sudeshna Roy,Junquan Chen,Jarek Wilkiewicz,Hao Zhou,Michal Jastrzebski,Qiong Hu,Agustin Dal Lago,Ramya Sree Boppana,Wei-Jen Ko,Jennifer Prendki,Yao Su,Zhi Li,Eliza Rutherford,Girish Ramchandra Rao,Ramona Comanescu,Adrià Puigdomènech,Qihang Chen,Dessie Petrova,Christine Chan,Vedrana Milutinovic,Felipe Tiengo Ferreira,Chin-Yi Cheng,Ming Zhang,Tapomay Dey,Sherry Yang,Ramesh Sampath,Quoc Le,Howard Zhou,Chu-Cheng Lin,Hoi Lam,Christine Kaeser-Chen,Kai Hui,Dean Hirsch,Tom Eccles,Basil Mustafa,Shruti Rijhwani,Morgane Rivière,Yuanzhong Xu,Junjie Wang,Xinyang Geng,Xiance Si,Arjun Khare,Cheolmin Kim,Vahab Mirrokni,Kamyu Lee,Khuslen Baatarsukh,Nathaniel Braun,Lisa Wang,Pallavi LV,Richard Tanburn,Yuvein,Zhu,Fangda Li,Setareh Ariafar,Dan Goldberg,Ken Burke,Daniil Mirylenka,Meiqi Guo,Olaf Ronneberger,Hadas Natalie Vogel,Liqun Cheng,Nishita Shetty,Johnson Jia,Thomas Jimma,Corey Fry,Ted Xiao,Martin Sundermeyer,Ryan Burnell,Yannis Assael,Mario Pinto,JD Chen,Rohit Sathyanarayana,Donghyun Cho,Jing Lu,Rishabh Agarwal,Sugato Basu,Lucas Gonzalez,Dhruv Shah,Meng Wei,Dre Mahaarachchi,Rohan Agrawal,Tero Rissa,Yani Donchev,Ramiro Leal-Cavazos,Adrian Hutter,Markus Mircea,Alon Jacovi,Faruk Ahmed,Jiageng Zhang,Shuguang Hu,Bo-Juen Chen,Jonni Kanerva,Guillaume Desjardins,Andrew Lee,Nikos Parotsidis,Asier Mujika,Tobias Weyand,Jasper Snoek,Jo Chick,Kai Chen,Paul Chang,Ethan Mahintorabi,Zi Wang,Tolly Powell,Orgad Keller,Abhirut Gupta,Claire Sha,Kanav Garg,Nicolas Heess,Ágoston Weisz,Cassidy Hardin,Bartek Wydrowski,Ben Coleman,Karina Zainullina,Pankaj Joshi,Alessandro Epasto,Terry Spitz,Binbin Xiong,Kai Zhao,Arseniy Klimovskiy,Ivy Zheng,Johan Ferret,Itay Yona,Waleed Khawaja,Jean-Baptiste Lespiau,Maxim Krikun,Siamak Shakeri,Timothee Cour,Bonnie Li,Igor Krivokon,Dan Suh,Alex Hofer,Jad Al Abdallah,Nikita Putikhin,Oscar Akerlund,Silvio Lattanzi,Anurag Kumar,Shane Settle,Himanshu Srivastava,Folawiyo Campbell-Ajala,Edouard Rosseel,Mihai Dorin Istin,Nishanth Dikkala,Anand Rao,Nick Young,Kate Lin,Dhruva Bhaswar,Yiming Wang,Jaume Sanchez Elias,Kritika Muralidharan,James Keeling,Dayou Du,Siddharth Gopal,Gregory Dibb,Charles Blundell,Manolis Delakis,Jacky Liang,Marco Tulio Ribeiro,Georgi Karadzhov,Guillermo Garrido,Ankur Bapna,Jiawei Cao,Adam Sadovsky,Pouya Tafti,Arthur Guez,Coline Devin,Yixian Di,Jinwei Xing,Chuqiao,Xu,Hanzhao Lin,Chun-Te Chu,Sameera Ponda,Wesley Helmholz,Fan Yang,Yue Gao,Sara Javanmardi,Wael Farhan,Alex Ramirez,Ricardo Figueira,Khe Chai Sim,Yuval Bahat,Ashwin Vaswani,Liangzhe Yuan,Gufeng Zhang,Leland Rechis,Hanjun Dai,Tayo Oguntebi,Alexandra Cordell,Eugénie Rives,Kaan Tekelioglu,Naveen Kumar,Bing Zhang,Aurick Zhou,Nikolay Savinov,Andrew Leach,Alex Tudor,Sanjay Ganapathy,Yanyan Zheng,Mirko Rossini,Vera Axelrod,Arnaud Autef,Yukun Zhu,Zheng Zheng,Mingda Zhang,Baochen Sun,Jie Ren,Nenad Tomasev,Nithish Kannan,Amer Sinha,Charles Chen,Louis O'Bryan,Alex Pak,Aditya Kusupati,Weel Yang,Deepak Ramachandran,Patrick Griffin,Seokhwan Kim,Philipp Neubeck,Craig Schiff,Tammo Spalink,Mingyang Ling,Arun Nair,Ga-Young Joung,Linda Deng,Avishkar Bhoopchand,Lora Aroyo,Tom Duerig,Jordan Griffith,Gabe Barth-Maron,Jake Ades,Alex Haig,Ankur Taly,Yunting Song,Paul Michel,Dave Orr,Dean Weesner,Corentin Tallec,Carrie Grimes Bostock,Paul Niemczyk,Andy Twigg,Mudit Verma,Rohith Vallu,Henry Wang,Marco Gelmi,Kiranbir Sodhia,Aleksandr Chuklin,Omer Goldman,Jasmine George,Liang Bai,Kelvin Zhang,Petar Sirkovic,Efrat Nehoran,Golan Pundak,Jiaqi Mu,Alice Chen,Alex Greve,Paulo Zacchello,David Amos,Heming Ge,Eric Noland,Colton Bishop,Jeffrey Dudek,Youhei Namiki,Elena Buchatskaya,Jing Li,Dorsa Sadigh,Masha Samsikova,Dan Malkin,Damien Vincent,Robert David,Rob Willoughby,Phoenix Meadowlark,Shawn Gao,Yan Li,Raj Apte,Amit Jhindal,Stein Xudong Lin,Alex Polozov,Zhicheng Wang,Tomas Mery,Anirudh GP,Varun Yerram,Sage Stevens,Tianqi Liu,Noah Fiedel,Charles Sutton,Matthew Johnson,Xiaodan Song,Kate Baumli,Nir Shabat,Muqthar Mohammad,Hao Liu,Marco Selvi,Yichao Zhou,Mehdi Hafezi Manshadi,Chu-ling Ko,Anthony Chen,Michael Bendersky,Jorge Gonzalez Mendez,Nisarg Kothari,Amir Zandieh,Yiling Huang,Daniel Andor,Ellie Pavlick,Idan Brusilovsky,Jitendra Harlalka,Sally Goldman,Andrew Lampinen,Guowang Li,Asahi Ushio,Somit Gupta,Lei Zhang,Chuyuan Kelly Fu,Madhavi Sewak,Timo Denk,Jed Borovik,Brendan Jou,Avital Zipori,Prateek Jain,Junwen Bai,Thang Luong,Jonathan Tompson,Alice Li,Li Liu,George Powell,Jiajun Shen,Alex Feng,Grishma Chole,Da Yu,Yinlam Chow,Tongxin Yin,Eric Malmi,Kefan Xiao,Yash Pande,Shachi Paul,Niccolò Dal Santo,Adil Dostmohamed,Sergio Guadarrama,Aaron Phillips,Thanumalayan Sankaranarayana Pillai,Gal Yona,Amin Ghafouri,Preethi Lahoti,Benjamin Lee,Dhruv Madeka,Eren Sezener,Simon Tokumine,Adrian Collister,Nicola De Cao,Richard Shin,Uday Kalra,Parker Beak,Emily Nottage,Ryo Nakashima,Ivan Jurin,Vikash Sehwag,Meenu Gaba,Junhao Zeng,Kevin R. McKee,Fernando Pereira,Tamar Yakar,Amayika Panda,Arka Dhar,Peilin Zhong,Daniel Sohn,Mark Brand,Lars Lowe Sjoesund,Viral Carpenter,Sharon Lin,Shantanu Thakoor,Marcus Wainwright,Ashwin Chaugule,Pranesh Srinivasan,Muye Zhu,Bernett Orlando,Jack Weber,Ayzaan Wahid,Gilles Baechler,Apurv Suman,Jovana Mitrović,Gabe Taubman,Honglin Yu,Helen King,Josh Dillon,Cathy Yip,Dhriti Varma,Tomas Izo,Levent Bolelli,Borja De Balle Pigem,Julia Di Trapani,Fotis Iliopoulos,Adam Paszke,Nishant Ranka,Joe Zou,Francesco Pongetti,Jed McGiffin,Alex Siegman,Rich Galt,Ross Hemsley,Goran Žužić,Victor Carbune,Tao Li,Myle Ott,Félix de Chaumont Quitry,David Vilar Torres,Yuri Chervonyi,Tomy Tsai,Prem Eruvbetine,Samuel Yang,Matthew Denton,Jake Walker,Slavica Andačić,Idan Heimlich Shtacher,Vittal Premachandran,Harshal Tushar Lehri,Cip Baetu,Damion Yates,Lampros Lamprou,Mariko Iinuma,Ioana Mihailescu,Ben Albrecht,Shachi Dave,Susie Sargsyan,Bryan Perozzi,Lucas Manning,Chiyuan Zhang,Denis Vnukov,Igor Mordatch,Raia Hadsell Wolfgang Macherey,Ryan Kappedal,Jim Stephan,Aditya Tripathi,Klaus Macherey,Jun Qian,Abhishek Bhowmick,Shekoofeh Azizi,Rémi Leblond,Shiva Mohan Reddy Garlapati,Timothy Knight,Matthew Wiethoff,Wei-Chih Hung,Anelia Angelova,Georgios Evangelopoulos,Pawel Janus,Dimitris Paparas,Matthew Rahtz,Ken Caluwaerts,Vivek Sampathkumar,Daniel Jarrett,Shadi Noghabi,Antoine Miech,Chak Yeung,Geoff Clark,Henry Prior,Fei Zheng,Jean Pouget-Abadie,Indro Bhattacharya,Kalpesh Krishna,Will Bishop,Zhe Yuan,Yunxiao Deng,Ashutosh Sathe,Kacper Krasowiak,Ciprian Chelba,Cho-Jui Hsieh,Kiran Vodrahalli,Buhuang Liu,Thomas Köppe,Amr Khalifa,Lubo Litchev,Pichi Charoenpanit,Reed Roberts,Sachin Yadav,Yasumasa Onoe,Desi Ivanov,Megha Mohabey,Vighnesh Birodkar,Nemanja Rakićević,Pierre Sermanet,Vaibhav Mehta,Krishan Subudhi,Travis Choma,Will Ng,Luheng He,Kathie Wang,Tasos Kementsietsidis,Shane Gu,Mansi Gupta,Andrew Nystrom,Mehran Kazemi,Timothy Chung,Nacho Cano,Nikhil Dhawan,Yufei Wang,Jiawei Xia,Trevor Yacovone,Eric Jia,Mingqing Chen,Simeon Ivanov,Ashrith Sheshan,Sid Dalmia,Paweł Stradomski,Pengcheng Yin,Salem Haykal,Congchao Wang,Dennis Duan,Neslihan Bulut,Greg Kochanski,Liam MacDermed,Namrata Godbole,Shitao Weng,Jingjing Chen,Rachana Fellinger,Ramin Mehran,Daniel Suo,Hisham Husain,Tong He,Kaushal Patel,Joshua Howland,Randall Parker,Kelvin Nguyen,Sharath Maddineni,Chris Rawles,Mina Khan,Shlomi Cohen-Ganor,Amol Mandhane,Xinyi Wu,Chenkai Kuang,Iulia Comşa,Ramya Ganeshan,Hanie Sedghi,Adam Bloniarz,Nuo Wang Pierse,Anton Briukhov,Petr Mitrichev,Anita Gergely,Serena Zhan,Allan Zhou,Nikita Saxena,Eva Lu,Josef Dean,Ashish Gupta,Nicolas Perez-Nieves,Renjie Wu,Cory McLean,Wei Liang,Disha Jindal,Anton Tsitsulin,Wenhao Yu,Kaiz Alarakyia,Tom Schaul,Piyush Patil,Peter Sung,Elijah Peake,Hongkun Yu,Feryal Behbahani,JD Co-Reyes,Alan Ansell,Sean Sun,Clara Barbu,Jonathan Lee,Seb Noury,James Allingham,Bilal Piot,Mohit Sharma,Christopher Yew,Ivan Korotkov,Bibo Xu,Demetra Brady,Goran Petrovic,Shibl Mourad,Claire Cui,Aditya Gupta,Parker Schuh,Saarthak Khanna,Anna Goldie,Abhinav Arora,Vadim Zubov,Amy Stuart,Mark Epstein,Yun Zhu,Jianqiao Liu,Yury Stuken,Ziyue Wang,Karolis Misiunas,Dee Guo,Ashleah Gill,Ale Hartman,Zaid Nabulsi,Aurko Roy,Aleksandra Faust,Jason Riesa,Ben Withbroe,Mengchao Wang,Marco Tagliasacchi,Andreea Marzoca,James Noraky,Serge Toropov,Malika Mehrotra,Bahram Raad,Sanja Deur,Steve Xu,Marianne Monteiro,Zhongru Wu,Yi Luan,Sam Ritter,Nick Li,Håvard Garnes,Yanzhang He,Martin Zlocha,Jifan Zhu,Matteo Hessel,Will Wu,Spandana Raj Babbula,Chizu Kawamoto,Yuanzhen Li,Mehadi Hassen,Yan Wang,Brian Wieder,James Freedman,Yin Zhang,Xinyi Bai,Tianli Yu,David Reitter,XiangHai Sheng,Mateo Wirth,Aditya Kini,Dima Damen,Mingcen Gao,Rachel Hornung,Michael Voznesensky,Brian Roark,Adhi Kuncoro,Yuxiang Zhou,Rushin Shah,Anthony Brohan,Kuangyuan Chen,James Wendt,David Rim,Paul Kishan Rubenstein,Jonathan Halcrow,Michelle Liu,Ty Geri,Yunhsuan Sung,Jane Shapiro,Shaan Bijwadia,Chris Duvarney,Christina Sorokin,Paul Natsev,Reeve Ingle,Pramod Gupta,Young Maeng,Ndaba Ndebele,Kexin Zhu,Valentin Anklin,Katherine Lee,Yuan Liu,Yaroslav Akulov,Shaleen Gupta,Guolong Su,Flavien Prost,Tianlin Liu,Vitaly Kovalev,Pol Moreno,Martin Scholz,Sam Redmond,Zongwei Zhou,Alex Castro-Ros,André Susano Pinto,Dia Kharrat,Michal Yarom,Rachel Saputro,Jannis Bulian,Ben Caine,Ji Liu,Abbas Abdolmaleki,Shariq Iqbal,Tautvydas Misiunas,Mikhail Sirotenko,Shefali Garg,Guy Bensky,Huan Gui,Xuezhi Wang,Raphael Koster,Mike Bernico,Da Huang,Romal Thoppilan,Trevor Cohn,Ben Golan,Wenlei Zhou,Andrew Rosenberg,Markus Freitag,Tynan Gangwani,Vincent Tsang,Anand Shukla,Xiaoqi Ren,Minh Giang,Chi Zou,Andre Elisseeff,Charline Le Lan,Dheeru Dua,Shuba Lall,Pranav Shyam,Frankie Garcia,Sarah Nguyen,Michael Guzman,AJ Maschinot,Marcello Maggioni,Ming-Wei Chang,Karol Gregor,Lotte Weerts,Kumaran Venkatesan,Bogdan Damoc,Leon Liu,Jan Wassenberg,Lewis Ho,Becca Roelofs,Majid Hadian,François-Xavier Aubet,Yu Liang,Sami Lachgar,Danny Karmon,Yong Cheng,Amelio Vázquez-Reina,Angie Chen,Zhuyun Dai,Andy Brock,Shubham Agrawal,Chenxi Pang,Peter Garst,Mariella Sanchez-Vargas,Ivor Rendulic,Aditya Ayyar,Andrija Ražnatović,Olivia Ma,Roopali Vij,Neha Sharma,Ashwin Balakrishna,Bingyuan Liu,Ian Mackinnon,Sorin Baltateanu,Petra Poklukar,Gabriel Ibagon,Colin Ji,Hongyang Jiao,Isaac Noble,Wojciech Stokowiec,Zhihao Li,Jeff Dean,David Lindner,Mark Omernick,Kristen Chiafullo,Mason Dimarco,Vitor Rodrigues,Vittorio Selo,Garrett Honke,Xintian,Wu,Wei He,Adam Hillier,Anhad Mohananey,Vihari Piratla,Chang Ye,Chase Malik,Sebastian Riedel,Samuel Albanie,Zi Yang,Kenny Vassigh,Maria Bauza,Sheng Li,Yiqing Tao,Nevan Wichers,Andrii Maksai,Abe Ittycheriah,Ross Mcilroy,Bryan Seybold,Noah Goodman,Romina Datta,Steven M. Hernandez,Tian Shi,Yony Kochinski,Anna Bulanova,Ken Franko,Mikita Sazanovich,Nicholas FitzGerald,Praneeth Kacham,Shubha Srinivas Raghvendra,Vincent Hellendoorn,Alexander Grushetsky,Julian Salazar,Angeliki Lazaridou,Jason Chang,Jan-Thorsten Peter,Sushant Kafle,Yann Dauphin,Abhishek Rao,Filippo Graziano,Izhak Shafran,Yuguo Liao,Tianli Ding,Geng Yan,Grace Chu,Zhao Fu,Vincent Roulet,Gabriel Rasskin,Duncan Williams,Shahar Drath,Alex Mossin,Raphael Hoffmann,Jordi Orbay,Francesco Bertolini,Hila Sheftel,Justin Chiu,Siyang Xue,Yuheng Kuang,Ferjad Naeem,Swaroop Nath,Nana Nti,Phil Culliton,Kashyap Krishnakumar,Michael Isard,Pei Sun,Ayan Chakrabarti,Nathan Clement,Regev Cohen,Arissa Wongpanich,GS Oh,Ashwin Murthy,Hao Zheng,Jessica Hamrick,Oskar Bunyan,Suhas Ganesh,Nitish Gupta,Roy Frostig,John Wieting,Yury Malkov,Pierre Marcenac,Zhixin,Lai,Xiaodan Tang,Mohammad Saleh,Fedir Zubach,Chinmay Kulkarni,Huanjie Zhou,Vicky Zayats,Nan Ding,Anshuman Tripathi,Arijit Pramanik,Patrik Zochbauer,Harish Ganapathy,Vedant Misra,Zach Behrman,Hugo Vallet,Mingyang Zhang,Mukund Sridhar,Ye Jin,Mohammad Babaeizadeh,Siim Põder,Megha Goel,Divya Jain,Tajwar Nasir,Shubham Mittal,Tim Dozat,Diego Ardila,Aliaksei Severyn,Fabio Pardo,Sammy Jerome,Siyang Qin,Louis Rouillard,Amir Yazdanbakhsh,Zizhao Zhang,Shivani Agrawal,Kaushik Shivakumar,Caden Lu,Praveen Kallakuri,Rachita Chhaparia,Kanishka Rao,Charles Kwong,Asya Fadeeva,Shitij Nigam,Yan Virin,Yuan Zhang,Balaji Venkatraman,Beliz Gunel,Marc Wilson,Huiyu Wang,Abhinav Gupta,Xiaowei Xu,Adrien Ali Taïga,Kareem Mohamed,Doug Fritz,Daniel Rodriguez,Zoubin Ghahramani,Harry Askham,Lior Belenki,James Zhao,Rahul Gupta,Krzysztof Jastrzębski,Takahiro Kosakai,Kaan Katircioglu,Jon Schneider,Rina Panigrahy,Konstantinos Bousmalis,Peter Grabowski,Prajit Ramachandran,Chaitra Hegde,Mihaela Rosca,Angelo Scorza Scarpati,Kyriakos Axiotis,Ying Xu,Zach Gleicher,Assaf Hurwitz Michaely,Mandar Sharma,Sanil Jain,Christoph Hirnschall,Tal Marian,Xuhui Jia,Kevin Mather,Kilol Gupta,Linhai Qiu,Nigamaa Nayakanti,Lucian Ionita,Steven Zheng,Lucia Loher,Kurt Shuster,Igor Petrovski,Roshan Sharma,Rahma Chaabouni,Angel Yeh,James An,Arushi Gupta,Steven Schwarcz,Seher Ellis,Sam Conway-Rahman,Javier Snaider,Alex Zhai,James Atwood,Daniel Golovin,Liqian Peng,Te I,Vivian Xia,Salvatore Scellato,Mahan Malihi,Arthur Bražinskas,Vlad-Doru Ion,Younghoon Jun,James Swirhun,Soroosh Mariooryad,Jiao Sun,Steve Chien,Rey Coaguila,Ariel Brand,Yi Gao,Tom Kwiatkowski,Roee Aharoni,Cheng-Chun Lee,Mislav Žanić,Yichi Zhang,Dan Ethier,Vitaly Nikolaev,Pranav Nair,Yoav Ben Shalom,Hen Fitoussi,Jai Gupta,Hongbin Liu,Dee Cattle,Tolga Bolukbasi,Ben Murdoch,Fantine Huot,Yin Li,Chris Hahn*

Main category: cs.CL

TL;DR: Gemini 2.X模型家族包括Gemini 2.5 Pro和2.5 Flash，以及早期的2.0 Flash和Flash-Lite。2.5 Pro在编码和推理任务上表现最佳，支持多模态理解和长视频处理；2.5 Flash提供高效推理能力；2.0 Flash和Flash-Lite则以低延迟和低成本提供高性能。


<details>
  <summary>Details</summary>
Motivation: 开发一系列模型以满足不同需求，覆盖从高性能到低成本的完整帕累托前沿。

Method: 推出Gemini 2.X模型家族，包括不同版本的模型（如2.5 Pro、2.5 Flash等），分别优化编码、推理、多模态理解和计算效率。

Result: Gemini 2.5 Pro在多模态和长上下文任务中表现优异；2.5 Flash提供高效推理；2.0 Flash和Flash-Lite在低成本和低延迟下表现良好。

Conclusion: Gemini 2.X模型家族为用户提供了从高性能到低成本的全方位选择，支持复杂的代理工作流程。

Abstract: In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and
Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite
models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA
performance on frontier coding and reasoning benchmarks. In addition to its
incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that
excels at multimodal understanding and it is now able to process up to 3 hours
of video content. Its unique combination of long context, multimodal and
reasoning capabilities can be combined to unlock new agentic workflows. Gemini
2.5 Flash provides excellent reasoning abilities at a fraction of the compute
and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high
performance at low latency and cost. Taken together, the Gemini 2.X model
generation spans the full Pareto frontier of model capability vs cost, allowing
users to explore the boundaries of what is possible with complex agentic
problem solving.

</details>


### [2] [Humans overrely on overconfident language models, across languages](https://arxiv.org/abs/2507.06306)
*Neil Rathi,Dan Jurafsky,Kaitlyn Zhou*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）在多语言环境中存在过度自信和用户过度依赖的问题，不同语言间的表达差异显著影响模型的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在多语言环境中的校准问题，以确保其回答能准确传达不确定性，避免用户过度依赖。

Method: 分析五种语言中LLM生成的认识标记分布，并测量用户在不同语言中对模型回答的依赖行为。

Result: LLM在所有语言中均表现出过度自信，但不同语言间表达差异明显（如日语中不确定性标记最多，德语和汉语中确定性标记最多）。用户在所有语言中均高度依赖自信回答，但依赖行为因语言而异。

Conclusion: 多语言校准面临挑战，需结合文化和语言背景进行模型安全性评估。

Abstract: As large language models (LLMs) are deployed globally, it is crucial that
their responses are calibrated across languages to accurately convey
uncertainty and limitations. Previous work has shown that LLMs are
linguistically overconfident in English, leading users to overrely on confident
generations. However, the usage and interpretation of epistemic markers (e.g.,
'It's definitely,' 'I think') can differ sharply across languages. Here, we
study the risks of multilingual linguistic (mis)calibration, overconfidence,
and overreliance across five languages to evaluate the safety of LLMs in a
global context.
  We find that overreliance risks are high across all languages. We first
analyze the distribution of LLM-generated epistemic markers, and observe that
while LLMs are cross-linguistically overconfident, they are also sensitive to
documented linguistic variation. For example, models generate the most markers
of uncertainty in Japanese and the most markers of certainty in German and
Mandarin. We then measure human reliance rates across languages, finding that
while users strongly rely on confident LLM generations in all languages,
reliance behaviors differ cross-linguistically: for example, users rely
significantly more on expressions of uncertainty in Japanese than in English.
Taken together, these results indicate high risk of reliance on overconfident
model generations across languages. Our findings highlight the challenges of
multilingual linguistic calibration and stress the importance of culturally and
linguistically contextualized model safety evaluations.

</details>


### [3] [ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time](https://arxiv.org/abs/2507.06313)
*Kiarash Zahirnia,Zahra Golpayegani,Walid Ahmad,Yang Liu*

Main category: cs.CL

TL;DR: ETT方法通过高效微调短上下文Transformer模型的参数，以线性计算开销扩展上下文长度，提升模型在长序列任务中的表现。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理长序列时面临二次计算和内存开销的挑战，限制了其应用范围。

Method: ETT通过将输入上下文分块为重叠子序列并微调模型参数，实现上下文长度的扩展。

Result: 在LongBench上，ETT将GPT-Large和Phi-2的上下文长度从1k扩展到32k，准确率提升30%。

Conclusion: ETT有效扩展了Transformer模型的上下文长度，且微调FFNs的第二层比全微调更有效。

Abstract: Transformer-based Language Models' computation and memory overhead increase
quadratically as a function of sequence length. The quadratic cost poses
challenges when employing LLMs for processing long sequences. In this work, we
introduce \ourmodelacronym~(Extend at Test-Time), method for extending the
context length of short context Transformer-based LLMs, with constant memory
requirement and linear computation overhead. ETT enable the extension of the
context length at test-time by efficient fine-tuning the model's parameters on
the input context, chunked into overlapping small subsequences. We evaluate ETT
on LongBench by extending the context length of GPT-Large and Phi-2 up to 32
times, increasing from 1k to 32k tokens. This results in up to a 30 percent
improvement in the model's accuracy. We also study how context can be stored in
LLM's weights effectively and efficiently. Through a detailed ablation study,
we examine which Transformer modules are most beneficial to fine-tune at
test-time. Interestingly, we find that fine-tuning the second layer of the FFNs
is more effective than full fine-tuning, leading to a further improvement in
the models' accuracy.

</details>


### [4] [Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?](https://arxiv.org/abs/2507.06335)
*Casey Kennington,David Schlangen*

Main category: cs.CL

TL;DR: 本文提出了一种通过“词作为分类器”模型统一形式、分布和接地语义理论的方法。


<details>
  <summary>Details</summary>
Motivation: 结合形式、分布和接地语义理论的优点，解决各自单独使用的局限性。

Method: 回顾相关文献，结合认知科学的最新研究，并通过小实验验证“词作为分类器”模型。

Result: “词作为分类器”模型在交互对话环境中表现良好，并能整合到形式化和分布化语言模型中。

Conclusion: “词作为分类器”模型是统一三种语义理论的有潜力的路径。

Abstract: Formal, Distributional, and Grounded theories of computational semantics each
have their uses and their drawbacks. There has been a shift to ground models of
language by adding visual knowledge, and there has been a call to enrich models
of language with symbolic methods to gain the benefits from formal,
distributional, and grounded theories. In this paper, we attempt to make the
case that one potential path forward in unifying all three semantic fields is
paved with the words-as-classifier model, a model of word-level grounded
semantics that has been incorporated into formalisms and distributional
language models in the literature, and it has been well-tested within
interactive dialogue settings. We review that literature, motivate the
words-as-classifiers model with an appeal to recent work in cognitive science,
and describe a small experiment. Finally, we sketch a model of semantics
unified through words-as-classifiers.

</details>


### [5] [Evaluating Morphological Alignment of Tokenizers in 70 Languages](https://arxiv.org/abs/2507.06378)
*Catherine Arnett,Marisa Hudspeth,Brendan O'Connor*

Main category: cs.CL

TL;DR: 论文探讨了如何评估分词器质量，扩展了MorphScore以支持更多语言，并发现形态对齐对模型性能影响有限。


<details>
  <summary>Details</summary>
Motivation: 研究分词器质量评估的有效性，尤其是分词器是否保留语言上有意义的子词。

Method: 扩展MorphScore至70种语言，并分析形态对齐分数与下游任务性能的相关性。

Result: 形态对齐对模型性能的方差解释有限，表明其单独不足以衡量分词器质量。

Conclusion: 形态对齐不是衡量分词器质量的关键维度，需其他评估方法。

Abstract: While tokenization is a key step in language modeling, with effects on model
training and performance, it remains unclear how to effectively evaluate
tokenizer quality. One proposed dimension of tokenizer quality is the extent to
which tokenizers preserve linguistically meaningful subwords, aligning token
boundaries with morphological boundaries within a word. We expand MorphScore
(Arnett & Bergen, 2025), which previously covered 22 languages, to support a
total of 70 languages. The updated MorphScore offers more flexibility in
evaluation and addresses some of the limitations of the original version. We
then correlate our alignment scores with downstream task performance for five
pre-trained languages models on seven tasks, with at least one task in each of
the languages in our sample. We find that morphological alignment does not
explain very much variance in model performance, suggesting that morphological
alignment alone does not measure dimensions of tokenization quality relevant to
model performance.

</details>


### [6] [Hypermagmas and Colored Operads: Heads, Phases, and Theta Roles](https://arxiv.org/abs/2507.06393)
*Matilde Marcolli,Riny Huijbregts,Richard K. Larson*

Main category: cs.CL

TL;DR: 论文探讨了句法对象上的头函数如何将岩浆结构扩展到超岩浆结构，并通过着色操作系统的形式描述了句法结构的生成和过滤。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过数学结构（如超岩浆和着色操作）更形式化地描述句法生成和过滤过程，统一解释句法规则如移动、投射原则等。

Method: 使用超岩浆结构和着色操作系统的理论框架，将句法生成和过滤过程建模为着色操作系统的生成和过滤。

Result: 成功将句法规则（如移动、投射原则）统一为着色操作系统的形式，并展示了其与超岩浆结构的关联。

Conclusion: 通过着色操作和超岩浆结构，可以更统一和形式化地描述句法生成和过滤过程。

Abstract: We show that head functions on syntactic objects extend the magma structure
to a hypermagma, with the c-command relation compatible with the magma
operation and the m-command relation with the hypermagma. We then show that the
structure of head and complement and specifier, additional modifier positions,
and the structure of phases in the Extended Projection can be formulated as a
bud generating system of a colored operad, in a form similar to the structure
of theta roles. We also show that, due to the special form of the colored
operad generators, the filtering of freely generated syntactic objects by these
coloring rules can be equivalently formulated as a filtering in the course of
structure formation via a colored Merge, which can in turn be related to the
hypermagma structure. The rules on movement by Internal Merge with respect to
phases, the Extended Projection Principle, Empty Category Principle, and Phase
Impenetrability Condition are all subsumed into the form of the colored operad
generators. Movement compatibilities between the phase structure and the theta
roles assignments can then be formulated in terms of the respective colored
operads and a transduction of colored operads.

</details>


### [7] [PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning](https://arxiv.org/abs/2507.06415)
*Zeming Chen,Angelika Romanou,Gail Weiss,Antoine Bosselut*

Main category: cs.CL

TL;DR: PERK提出了一种参数高效的方法，通过梯度更新轻量级适配器在测试时编码长上下文，显著提升了长上下文推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有元学习方法在长上下文推理中内存消耗过高的问题。

Method: 采用双嵌套优化循环，内循环将上下文编码为低秩适配器，外循环学习利用适配器进行推理。

Result: 在多个任务中显著优于基线方法，性能提升最高达90%（小模型）和27%（大模型）。

Conclusion: PERK在训练时内存消耗高，但在推理时更高效，且对推理复杂性、上下文长度和信息位置更鲁棒。

Abstract: Long-context reasoning requires accurately identifying relevant information
in extensive, noisy input contexts. Previous research shows that using
test-time learning to encode context directly into model parameters can
effectively enable reasoning over noisy information. However, meta-learning
methods for enabling test-time learning are prohibitively memory-intensive,
preventing their application to long context settings. In this work, we propose
PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for
learning to encode long input contexts using gradient updates to a lightweight
model adapter at test time. Specifically, PERK employs two nested optimization
loops in a meta-training phase. The inner loop rapidly encodes contexts into a
low-rank adapter (LoRA) that serves as a parameter-efficient memory module for
the base model. Concurrently, the outer loop learns to use the updated adapter
to accurately recall and reason over relevant information from the encoded long
context. Our evaluations on several long-context reasoning tasks show that PERK
significantly outperforms the standard prompt-based long-context baseline,
achieving average absolute performance gains of up to 90% for smaller models
(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In
general, PERK is more robust to reasoning complexity, length extrapolation, and
the locations of relevant information in contexts. Finally, we show that while
PERK is memory-intensive during training, it scales more efficiently at
inference time than prompt-based long-context inference.

</details>


### [8] [Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling](https://arxiv.org/abs/2507.06419)
*Pankayaraj Pathmanathan,Furong Huang*

Main category: cs.CL

TL;DR: 提出了一种名为REFORM的自改进奖励建模框架，通过奖励引导的解码发现奖励模型的失败模式，并利用生成的对抗样本增强训练数据，显著提高了模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于人类偏好的复杂性和数据集的有限覆盖，奖励模型在分布偏移或对抗扰动下容易失效，而现有方法依赖先验知识，限制了实用性。

Method: 提出了一种不依赖偏好分布先验的方法，通过奖励引导的解码发现失败模式，并引入REFORM框架，利用奖励模型自身生成对抗样本以增强训练数据。

Result: 在Anthropic HH和PKU Beavertails数据集上，REFORM显著提高了鲁棒性，同时保持了奖励质量，并在下游策略训练中表现良好。

Conclusion: REFORM通过自改进机制有效提升了奖励模型的鲁棒性和对齐质量，且无需依赖先验知识。

Abstract: Reward modeling (RM), which captures human preferences to align large
language models (LLMs), is increasingly employed in tasks such as model
finetuning, response filtering, and ranking. However, due to the inherent
complexity of human preferences and the limited coverage of available datasets,
reward models often fail under distributional shifts or adversarial
perturbations. Existing approaches for identifying such failure modes typically
rely on prior knowledge about preference distributions or failure attributes,
limiting their practicality in real-world settings where such information is
unavailable. In this work, we propose a tractable, preference-distribution
agnostic method for discovering reward model failure modes via reward guided
controlled decoding. Building on this, we introduce REFORM, a self-improving
reward modeling framework that enhances robustness by using the reward model
itself to guide the generation of falsely scored responses. These adversarial
examples are then used to augment the training data and patch the reward
model's misaligned behavior. We evaluate REFORM on two widely used preference
datasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate
that it significantly improves robustness without sacrificing reward quality.
Notably, REFORM preserves performance both in direct evaluation and in
downstream policy training, and further improves alignment quality by removing
spurious correlations.

</details>


### [9] [Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders](https://arxiv.org/abs/2507.06427)
*Shun Wang,Tyler Loakman,Youbo Lei,Yi Liu,Bohao Yang,Yuting Zhao,Dong Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: 论文提出了一种基于字典学习和稀疏自编码器的大语言模型分解方法，提取单义特征，改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型被视为黑盒算法，缺乏透明性，限制了性能提升和信任度。

Method: 采用字典学习和稀疏自编码器分解LLM，提取单义特征并自动优化提示。

Result: 方法显著提升了下游任务（如数学推理和隐喻检测）的性能。

Conclusion: 分解方法提高了LLM的可解释性和性能，为模型优化提供了新途径。

Abstract: Large Language Models (LLMs) are traditionally viewed as black-box
algorithms, therefore reducing trustworthiness and obscuring potential
approaches to increasing performance on downstream tasks. In this work, we
apply an effective LLM decomposition method using a dictionary-learning
approach with sparse autoencoders. This helps extract monosemantic features
from polysemantic LLM neurons. Remarkably, our work identifies model-internal
misunderstanding, allowing the automatic reformulation of the prompts with
additional annotations to improve the interpretation by LLMs. Moreover, this
approach demonstrates a significant performance improvement in downstream
tasks, such as mathematical reasoning and metaphor detection.

</details>


### [10] [Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling](https://arxiv.org/abs/2507.06435)
*Rafiu Adekoya Badekale,Adewale Akinfaderin*

Main category: cs.CL

TL;DR: 论文提出了一种基于动态嵌入主题模型（DETM）的新方法，用于分析全球气候政策语言的演变，揭示了从温室气体到实施与技术合作的转变。


<details>
  <summary>Details</summary>
Motivation: 传统的手动主题编码方法耗时且难以捕捉全球政策话语的复杂性，而DETM可以高效处理高维数据。

Method: 使用DETM分析1995至2023年UNFCCC政策决策的语料库，排除2020年数据。

Result: 模型展示了政策重点从温室气体转向实施、技术合作、能力建设和全球协议。

Conclusion: DETM是分析政策语言演变的有效工具，未来可扩展到其他政策领域。

Abstract: Understanding how policy language evolves over time is critical for assessing
global responses to complex challenges such as climate change. Temporal
analysis helps stakeholders, including policymakers and researchers, to
evaluate past priorities, identify emerging themes, design governance
strategies, and develop mitigation measures. Traditional approaches, such as
manual thematic coding, are time-consuming and limited in capturing the
complex, interconnected nature of global policy discourse. With the increasing
relevance of unsupervised machine learning, these limitations can be addressed,
particularly under high-volume, complex, and high-dimensional data conditions.
In this work, we explore a novel approach that applies the dynamic embedded
topic model (DETM) to analyze the evolution of global climate policy discourse.
A probabilistic model designed to capture the temporal dynamics of topics over
time. We collected a corpus of United Nations Framework Convention on Climate
Change (UNFCCC) policy decisions from 1995 to 2023, excluding 2020 due to the
postponement of COP26 as a result of the COVID-19 pandemic. The model reveals
shifts from early emphases on greenhouse gases and international conventions to
recent focuses on implementation, technical collaboration, capacity building,
finance, and global agreements. Section 3 presents the modeling pipeline,
including preprocessing, model training, and visualization of temporal word
distributions. Our results show that DETM is a scalable and effective tool for
analyzing the evolution of global policy discourse. Section 4 discusses the
implications of these findings and we concluded with future directions and
refinements to extend this approach to other policy domains.

</details>


### [11] [Perception-Aware Policy Optimization for Multimodal Reasoning](https://arxiv.org/abs/2507.06448)
*Zhenhailong Wang,Xuehang Guo,Sofia Stoica,Haiyang Xu,Hongru Wang,Hyeonjeong Ha,Xiusi Chen,Yangyi Chen,Ming Yan,Fei Huang,Heng Ji*

Main category: cs.CL

TL;DR: PAPO是一种扩展GRPO的方法，通过引入隐式感知损失（KL散度项）提升多模态推理任务中的视觉感知能力，无需额外数据或外部模型。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR在多模态推理任务中表现不佳，主要由于视觉输入感知错误。

Method: 提出PAPO，结合隐式感知损失（KL散度项）到GRPO目标中，并引入双熵损失解决损失黑客问题。

Result: 在多模态基准测试中提升4.4%，视觉依赖任务中提升8.0%，感知错误减少30.5%。

Conclusion: PAPO为视觉基础推理的RL框架奠定了基础，实现了感知与推理的深度整合。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a
highly effective strategy for endowing Large Language Models (LLMs) with robust
multi-step reasoning abilities. However, its design and optimizations remain
tailored to purely textual domains, resulting in suboptimal performance when
applied to multimodal reasoning tasks. In particular, we observe that a major
source of error in current multimodal reasoning lies in the perception of
visual inputs. To address this bottleneck, we propose Perception-Aware Policy
Optimization (PAPO), a simple yet effective extension of GRPO that encourages
the model to learn to perceive while learning to reason, entirely from internal
supervision signals. Notably, PAPO does not rely on additional data curation,
external reward models, or proprietary models. Specifically, we introduce the
Implicit Perception Loss in the form of a KL divergence term to the GRPO
objective, which, despite its simplicity, yields significant overall
improvements (4.4%) on diverse multimodal benchmarks. The improvements are more
pronounced, approaching 8.0%, on tasks with high vision dependency. We also
observe a substantial reduction (30.5%) in perception errors, indicating
improved perceptual capabilities with PAPO. We conduct comprehensive analysis
of PAPO and identify a unique loss hacking issue, which we rigorously analyze
and mitigate through a Double Entropy Loss. Overall, our work introduces a
deeper integration of perception-aware supervision into RLVR learning
objectives and lays the groundwork for a new RL framework that encourages
visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.

</details>


### [12] [A Semantic Parsing Framework for End-to-End Time Normalization](https://arxiv.org/abs/2507.06450)
*Xin Su,Sungduk Yu,Phillip Howard,Steven Bethard*

Main category: cs.CL

TL;DR: 论文提出了一种基于SCATE框架的时间规范化方法，通过代码生成任务实现，利用LLM生成可执行代码并合成大规模标注数据，实验表明小模型在增强数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于ISO-TimeML的系统表达能力有限，难以处理复杂时间表达式，如组合式、事件相关和多跨度表达式。

Method: 将时间规范化任务重新定义为基于SCATE框架的代码生成任务，开发可执行的SCATE Python库，利用LLM生成代码并合成标注数据。

Result: 实验证明，小模型在增强数据上表现优于LLM，实现了实用、准确且可解释的时间规范化。

Conclusion: 该方法通过代码生成和数据增强，解决了传统系统的局限性，为时间规范化提供了高效解决方案。

Abstract: Time normalization is the task of converting natural language temporal
expressions into machine-readable representations. It underpins many downstream
applications in information retrieval, question answering, and clinical
decision-making. Traditional systems based on the ISO-TimeML schema limit
expressivity and struggle with complex constructs such as compositional,
event-relative, and multi-span time expressions. In this work, we introduce a
novel formulation of time normalization as a code generation task grounded in
the SCATE framework, which defines temporal semantics through symbolic and
compositional operators. We implement a fully executable SCATE Python library
and demonstrate that large language models (LLMs) can generate executable SCATE
code. Leveraging this capability, we develop an automatic data augmentation
pipeline using LLMs to synthesize large-scale annotated data with code-level
validation. Our experiments show that small, locally deployable models trained
on this augmented data can achieve strong performance, outperforming even their
LLM parents and enabling practical, accurate, and interpretable time
normalization.

</details>


### [13] [A Systematic Analysis of Hybrid Linear Attention](https://arxiv.org/abs/2507.06457)
*Dustin Wang,Rui-Jie Zhu,Steven Abreu,Yong Shan,Taylor Kergan,Yuqi Pan,Yuhong Chou,Zheng Li,Ge Zhang,Wenhao Huang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 论文系统评估了多种线性注意力模型在混合架构中的表现，发现语言建模稳定但召回性能随全注意力层增加而提升，推荐特定架构和比例以实现高效Transformer级召回。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在长序列中的二次复杂性和内存问题，同时提升线性注意力模型的召回性能。

Method: 系统评估六种线性注意力变体，训练并开源72个模型，涵盖不同参数规模和混合比例。

Result: 语言建模性能稳定，召回性能在全注意力层增加时显著提升，推荐3:1至6:1的线性-全注意力比例。

Conclusion: 选择性门控、分层循环和可控遗忘对混合模型至关重要，推荐HGRN-2或GatedDeltaNet架构。

Abstract: Transformers face quadratic complexity and memory issues with long sequences,
prompting the adoption of linear attention mechanisms using fixed-size hidden
states. However, linear models often suffer from limited recall performance,
leading to hybrid architectures that combine linear and full attention layers.
Despite extensive hybrid architecture research, the choice of linear attention
component has not been deeply explored. We systematically evaluate various
linear attention models across generations - vector recurrences to advanced
gating mechanisms - both standalone and hybridized. To enable this
comprehensive analysis, we trained and open-sourced 72 models: 36 at 340M
parameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six
linear attention variants across five hybridization ratios. Benchmarking on
standard language modeling and recall tasks reveals that superior standalone
linear models do not necessarily excel in hybrids. While language modeling
remains stable across linear-to-full attention ratios, recall significantly
improves with increased full attention layers, particularly below a 3:1 ratio.
Our study highlights selective gating, hierarchical recurrence, and controlled
forgetting as critical for effective hybrid models. We recommend architectures
such as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1
to achieve Transformer-level recall efficiently. Our models are open-sourced at
https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.

</details>


### [14] [On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks](https://arxiv.org/abs/2507.06489)
*Stephen Obadinma,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 本文首次全面研究了大型语言模型（LLMs）在对抗攻击下口头置信度的鲁棒性，揭示了当前置信度表达方法的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 确保LLMs在高风险应用中透明、可信和安全，需要其生成的口头置信度具有鲁棒性。

Method: 提出了一种通过扰动和越狱方法攻击口头置信度分数的框架，并测试了多种提示策略、模型规模和应用领域。

Result: 攻击显著危害置信度估计并导致频繁答案变化，现有防御技术大多无效或适得其反。

Conclusion: 亟需设计更鲁棒的置信度表达机制，因为即使是微小的语义保留修改也可能导致误导性置信度。

Abstract: Robust verbal confidence generated by large language models (LLMs) is crucial
for the deployment of LLMs to ensure transparency, trust, and safety in
human-AI interactions across many high-stakes applications. In this paper, we
present the first comprehensive study on the robustness of verbal confidence
under adversarial attacks. We introduce a novel framework for attacking verbal
confidence scores through both perturbation and jailbreak-based methods, and
show that these attacks can significantly jeopardize verbal confidence
estimates and lead to frequent answer changes. We examine a variety of
prompting strategies, model sizes, and application domains, revealing that
current confidence elicitation methods are vulnerable and that commonly used
defence techniques are largely ineffective or counterproductive. Our findings
underscore the urgent need to design more robust mechanisms for confidence
expression in LLMs, as even subtle semantic-preserving modifications can lead
to misleading confidence in responses.

</details>


### [15] [Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings](https://arxiv.org/abs/2507.06506)
*Russell Taylor,Benjamin Herbert,Michael Sana*

Main category: cs.CL

TL;DR: 该研究提出了一种结合大型语言模型和专门技术的创新方法，用于将英语双关语翻译成法语，旨在保留原文的语言创意和幽默。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言双关语翻译的挑战，填补翻译研究与计算语言学之间的空白。

Method: 采用三阶段方法：基线模型、引导思维链管道和多代理生成-判别框架。

Result: 在CLEF JOKER 2025竞赛中取得第一和第二名。

Conclusion: 研究推动了语言模型在语义模糊性和文化意识方面的应用，为双关语翻译提供了新思路。

Abstract: Translating wordplay across languages presents unique challenges that have
long confounded both professional human translators and machine translation
systems. This research proposes a novel approach for translating puns from
English to French by combining state-of-the-art large language models with
specialized techniques for wordplay generation.
  Our methodology employs a three-stage approach. First, we establish a
baseline using multiple frontier large language models with feedback based on a
new contrastive learning dataset. Second, we implement a guided
chain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we
implement a multi-agent generator-discriminator framework for evaluating and
regenerating puns with feedback.
  Moving beyond the limitations of literal translation, our methodology's
primary objective is to capture the linguistic creativity and humor of the
source text wordplay, rather than simply duplicating its vocabulary. Our best
runs earned first and second place in the CLEF JOKER 2025 Task 2 competition
where they were evaluated manually by expert native French speakers.
  This research addresses a gap between translation studies and computational
linguistics by implementing linguistically-informed techniques for wordplay
translation, advancing our understanding of how language models can be
leveraged to handle the complex interplay between semantic ambiguity, phonetic
similarity, and the implicit cultural and linguistic awareness needed for
successful humor.

</details>


### [16] [SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers](https://arxiv.org/abs/2507.06517)
*Zicong Tang,Shi Luohe,Zuchao Li,Baoyuan Qi,Guoming Liu,Lefei Zhang,Ping Wang*

Main category: cs.CL

TL;DR: SpindleKV是一种新型的KV缓存减少方法，通过平衡浅层和深层的处理，优化了LLMs的推理系统。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs中KV缓存内存消耗高的问题，特别是在浅层缓存减少不足的情况下。

Method: 结合注意力权重驱逐（深层）和基于代码本的替换（浅层），并解决GQA问题。

Result: 在两个常见基准测试和三种LLMs上表现优于基线方法，缓存减少效果更好且模型性能保持或提升。

Conclusion: SpindleKV有效减少了KV缓存，同时保持了模型性能，为LLMs的推理系统提供了优化方案。

Abstract: Large Language Models (LLMs) have achieved impressive accomplishments in
recent years. However, the increasing memory consumption of KV cache has
possessed a significant challenge to the inference system. Eviction methods
have revealed the inherent redundancy within the KV cache, demonstrating its
potential for reduction, particularly in deeper layers. However, KV cache
reduction for shallower layers has been found to be insufficient. Based on our
observation that, the KV cache exhibits a high degree of similarity. Based on
this observation, we proposed a novel KV cache reduction method, SpindleKV,
which balances both shallow and deep layers. For deep layers, we employ an
attention weight based eviction method, while for shallow layers, we apply a
codebook based replacement approach which is learnt by similarity and merging
policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma
faced by other attention based eviction methods. Experiments on two common
benchmarks with three different LLMs shown that SpindleKV obtained better KV
cache reduction effect compared to baseline methods, while preserving similar
or even better model performance.

</details>


### [17] [InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior](https://arxiv.org/abs/2507.06528)
*Huisheng Wang,Zhuoshi Pan,Hangjing Zhang,Mingxiao Liu,Hanqing Gao,H. Vicky Zhao*

Main category: cs.CL

TL;DR: 论文提出InvestAlign框架，通过理论解决方案生成高质量SFT数据集，以解决行为金融中LLM与投资者决策对齐的问题，避免真实数据的高成本和隐私风险。


<details>
  <summary>Details</summary>
Motivation: 行为金融中LLM与投资者决策对齐的挑战在于真实用户数据稀缺，SFT依赖大量数据导致高成本和隐私风险。

Method: 提出InvestAlign框架，利用理论解决方案生成SFT数据集，而非复杂场景的真实数据。

Result: 实验表明，使用InvestAlign生成的数据训练LLM参数收敛更快，且InvestAgent模型在简单和复杂投资问题中更接近真实用户数据。

Conclusion: InvestAlign是一种有前景的方法，可解决复杂投资问题并实现LLM与投资者决策行为的对齐。

Abstract: Aligning Large Language Models (LLMs) with investor decision-making processes
under herd behavior is a critical challenge in behavioral finance, which
grapples with a fundamental limitation: the scarcity of real-user data needed
for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM
outputs and human behavioral patterns, its reliance on massive authentic data
imposes substantial collection costs and privacy risks. We propose InvestAlign,
a novel framework that constructs high-quality SFT datasets by leveraging
theoretical solutions to similar and simple optimal investment problems rather
than complex scenarios. Our theoretical analysis demonstrates that training
LLMs with InvestAlign-generated data achieves faster parameter convergence than
using real-user data, suggesting superior learning efficiency. Furthermore, we
develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which
demonstrates significantly closer alignment to real-user data than pre-SFT
models in both simple and complex investment problems. This highlights our
proposed InvestAlign as a promising approach with the potential to address
complex optimal investment problems and align LLMs with investor
decision-making processes under herd behavior. Our code is publicly available
at https://github.com/thu-social-network-research-group/InvestAlign.

</details>


### [18] [Large Language Model for Extracting Complex Contract Information in Industrial Scenes](https://arxiv.org/abs/2507.06539)
*Yunyang Cao,Yanjun Li,Silong Dai*

Main category: cs.CL

TL;DR: 本文提出了一种工业场景中复杂合同信息抽取任务的高质量数据集构建方法，并基于此微调了大语言模型。通过聚类分析和GPT辅助标注获取高质量数据，数据增强提升模型鲁棒性，实验结果显示模型性能优异。


<details>
  <summary>Details</summary>
Motivation: 工业合同信息抽取任务复杂且需要高质量数据支持，现有方法在数据质量和模型鲁棒性上存在不足。

Method: 1. 对工业合同文本聚类分析，用GPT-4和GPT-3.5提取关键信息；2. 通过构造新文本实现数据增强；3. 基于高质量数据集微调大语言模型。

Result: 模型在保证高召回率和精度的同时，解析效率优异。LoRA、数据平衡和数据增强显著提升模型准确性和鲁棒性。

Conclusion: 该方法为工业合同信息抽取任务提供了新颖高效的解决方案。

Abstract: This paper proposes a high-quality dataset construction method for complex
contract information extraction tasks in industrial scenarios and fine-tunes a
large language model based on this dataset. Firstly, cluster analysis is
performed on industrial contract texts, and GPT-4 and GPT-3.5 are used to
extract key information from the original contract data, obtaining high-quality
data annotations. Secondly, data augmentation is achieved by constructing new
texts, and GPT-3.5 generates unstructured contract texts from randomly combined
keywords, improving model robustness. Finally, the large language model is
fine-tuned based on the high-quality dataset. Experimental results show that
the model achieves excellent overall performance while ensuring high field
recall and precision and considering parsing efficiency. LoRA, data balancing,
and data augmentation effectively enhance model accuracy and robustness. The
proposed method provides a novel and efficient solution for industrial contract
information extraction tasks.

</details>


### [19] [The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production](https://arxiv.org/abs/2507.06565)
*Juan B. Gutiérrez*

Main category: cs.CL

TL;DR: 论文提出了一种将人类与大型语言模型（LLMs）平等视为节点的网络模型，分析了四种危害（偏离真相、自我修复、新造内容、外部检测），并通过数学模型和开源算法（FOO）展示了如何通过同行评审提升网络可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究人类与LLMs交互中信息流通的新媒介，关注其可靠性问题，而非孤立地分析幻觉现象。

Method: 提出了一种基于节点平等的网络模型，定义‘无效化’概念，并通过数学模型和FOO算法验证网络动态。

Result: 网络仅依赖偏离和自我修复时，错误率较低；加入新造内容后错误率升高；引入同行评审可显著提升真相主导性。

Conclusion: 可靠性依赖于将不完美的模型整合为相互监督的网络，而非追求单一模型的完美。

Abstract: Large-language models turn writing into a live exchange between humans and
software. We capture this new medium with a discursive-network model that
treats people and LLMs as equal nodes and tracks how their statements
circulate. Broadening the focus from isolated hallucinations, we define
invalidation (any factual, logical, or structural breach) and show it follows
four hazards: drift from truth, self-repair, fresh fabrication, and external
detection. A general mathematical model of discursive networks is developed to
provide valuable insights: A network governed only by drift and self-repair
stabilizes at a modest error rate; adding fabrication reproduces the high rates
seen in current LLMs. Giving each false claim even a small chance of peer
review shifts the system to a truth-dominant state. We operationalize peer
review with the open-source \emph{Flaws-of-Others (FOO) algorithm}: a
configurable loop in which any set of agents critique one another while a
harmoniser merges their verdicts. The takeaway is practical and cultural:
reliability in this new medium comes not from perfecting single models but from
wiring imperfect ones into networks that keep each other honest.

</details>


### [20] [Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis](https://arxiv.org/abs/2507.06571)
*Srihari K B,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 提出了一种结合多模态知识图谱与生成式AI的食品领域QA框架，显著提升了性能与多样性。


<details>
  <summary>Details</summary>
Motivation: 解决食品领域QA中知识整合与多模态生成的挑战，提高答案的可靠性与多样性。

Method: 构建大规模多模态知识图谱（MMKG），结合LLaMA和Stable Diffusion进行联合微调，采用混合检索-生成策略。

Result: BERTScore提升16.2%，FID降低37.8%，CLIP对齐提升31.1%，图像重用准确率94.1%，合成充分性85%。

Conclusion: 结构化知识与多模态生成结合显著提升了食品QA的可靠性与多样性。

Abstract: We propose a unified food-domain QA framework that combines a large-scale
multimodal knowledge graph (MMKG) with generative AI. Our MMKG links 13,000
recipes, 3,000 ingredients, 140,000 relations, and 14,000 images. We generate
40,000 QA pairs using 40 templates and LLaVA/DeepSeek augmentation. Joint
fine-tuning of Meta LLaMA 3.1-8B and Stable Diffusion 3.5-Large improves
BERTScore by 16.2\%, reduces FID by 37.8\%, and boosts CLIP alignment by
31.1\%. Diagnostic analyses-CLIP-based mismatch detection (35.2\% to 7.3\%) and
LLaVA-driven hallucination checks-ensure factual and visual fidelity. A hybrid
retrieval-generation strategy achieves 94.1\% accurate image reuse and 85\%
adequacy in synthesis. Our results demonstrate that structured knowledge and
multimodal generation together enhance reliability and diversity in food QA.

</details>


### [21] [Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation](https://arxiv.org/abs/2507.06607)
*Liliang Ren,Congcong Chen,Haoran Xu,Young Jin Kim,Adam Atkinson,Zheng Zhan,Jiankai Sun,Baolin Peng,Liyuan Liu,Shuohang Wang,Hao Cheng,Jianfeng Gao,Weizhu Chen,Yelong Shen*

Main category: cs.CL

TL;DR: 论文提出了一种名为Gated Memory Unit (GMU)的机制，用于在SSM层之间高效共享内存，并基于此构建了SambaY架构，显著提升了解码效率和长上下文性能。


<details>
  <summary>Details</summary>
Motivation: 探索SSM层间表示共享的效率潜力，以提升序列建模的性能和效率。

Method: 引入GMU机制，构建SambaY架构（结合Samba和YOCO的混合解码器架构），并在跨解码器中共享内存状态。

Result: SambaY显著提升了解码效率，保持了线性预填充时间复杂度，并在长上下文任务中表现优异。最大模型在推理任务中性能优于基线，且解码吞吐量提升10倍。

Conclusion: GMU和SambaY架构在高效序列建模中表现出色，具有优越的性能扩展性和实际应用潜力。

Abstract: Recent advances in language modeling have demonstrated the effectiveness of
State Space Models (SSMs) for efficient sequence modeling. While hybrid
architectures such as Samba and the decoder-decoder architecture, YOCO, have
shown promising performance gains over Transformers, prior works have not
investigated the efficiency potential of representation sharing between SSM
layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet
effective mechanism for efficient memory sharing across layers. We apply it to
create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in
the cross-decoder to share memory readout states from a Samba-based
self-decoder. SambaY significantly enhances decoding efficiency, preserves
linear pre-filling time complexity, and boosts long-context performance, all
while eliminating the need for explicit positional encoding. Through extensive
scaling experiments, we demonstrate that our model exhibits a significantly
lower irreducible loss compared to a strong YOCO baseline, indicating superior
performance scalability under large-scale compute regimes. Our largest model
enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves
significantly better performance than Phi4-mini-Reasoning on reasoning tasks
such as Math500, AIME24/25, and GPQA Diamond without any reinforcement
learning, while delivering up to 10x higher decoding throughput on 2K-length
prompts with 32K generation length under the vLLM inference framework. We
release our training codebase on open-source data at
https://github.com/microsoft/ArchScale.

</details>


### [22] [FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation](https://arxiv.org/abs/2507.06622)
*Boshko Koloski,Senja Pollak,Roberto Navigli,Blaž Škrlj*

Main category: cs.CL

TL;DR: FuDoBa是一种基于贝叶斯优化的方法，结合LLM嵌入与领域特定知识，生成低维、任务相关的表示，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成的高维嵌入在领域特定应用中过于通用或低效的问题。

Method: 通过贝叶斯优化融合LLM嵌入与领域知识（本地和WikiData等外部资源），生成低维表示。

Result: 在六个数据集的两个领域中，FuDoBa与AutoML分类器结合，性能达到或超过专有LLM嵌入基线。

Conclusion: FuDoBa提供了一种高效、可解释的文档表示方法，适用于领域特定任务。

Abstract: Building on the success of Large Language Models (LLMs), LLM-based
representations have dominated the document representation landscape, achieving
great performance on the document embedding benchmarks. However, the
high-dimensional, computationally expensive embeddings from LLMs tend to be
either too generic or inefficient for domain-specific applications. To address
these limitations, we introduce FuDoBa a Bayesian optimisation-based method
that integrates LLM-based embeddings with domain-specific structured knowledge,
sourced both locally and from external repositories like WikiData. This fusion
produces low-dimensional, task-relevant representations while reducing training
complexity and yielding interpretable early-fusion weights for enhanced
classification performance. We demonstrate the effectiveness of our approach on
six datasets in two domains, showing that when paired with robust AutoML-based
classifiers, our proposed representation learning approach performs on par
with, or surpasses, those produced solely by the proprietary LLM-based
embedding baselines.

</details>


### [23] [Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review](https://arxiv.org/abs/2507.06623)
*James Stewart-Evans,Emma Wilson,Tessa Langley,Andrew Prayle,Angela Hands,Karen Exley,Jo Leonardi-Bee*

Main category: cs.CL

TL;DR: 论文探讨了使用Claude 3.5 Sonnet LLM和审查协议加速数据提取的效果，发现其在简单数据上表现良好，但在复杂数据上表现不佳，建议进一步评估和改进。


<details>
  <summary>Details</summary>
Motivation: 数据提取阶段耗时耗力，研究者希望通过LLM和审查协议提高效率。

Method: 使用Claude 3.5 Sonnet LLM和审查协议对10个证据源进行数据提取，并评估其性能。

Result: 简单数据提取准确率高（83.3%和100%），复杂数据准确率低（9.6%和15.8%）；总体精度高（>90%），但召回率和F1分数低（<25%和<40%）。

Conclusion: LLM在数据提取中表现有限，需进一步评估和改进；LLM反馈有助于协议优化。

Abstract: The data extraction stages of reviews are resource-intensive, and researchers
may seek to expediate data extraction using online (large language models) LLMs
and review protocols. Claude 3.5 Sonnet was used to trial two approaches that
used a review protocol to prompt data extraction from 10 evidence sources
included in a case study scoping review. A protocol-based approach was also
used to review extracted data. Limited performance evaluation was undertaken
which found high accuracy for the two extraction approaches (83.3% and 100%)
when extracting simple, well-defined citation details; accuracy was lower (9.6%
and 15.8%) when extracting more complex, subjective data items. Considering all
data items, both approaches had precision >90% but low recall (<25%) and F1
scores (<40%). The context of a complex scoping review, open response types and
methodological approach likely impacted performance due to missed and
misattributed data. LLM feedback considered the baseline extraction accurate
and suggested minor amendments: four of 15 (26.7%) to citation details and 8 of
38 (21.1%) to key findings data items were considered to potentially add value.
However, when repeating the process with a dataset featuring deliberate errors,
only 2 of 39 (5%) errors were detected. Review-protocol-based methods used for
expediency require more robust performance evaluation across a range of LLMs
and review contexts with comparison to conventional prompt engineering
approaches. We recommend researchers evaluate and report LLM performance if
using them similarly to conduct data extraction or review extracted data. LLM
feedback contributed to protocol adaptation and may assist future review
protocol drafting.

</details>


### [24] [Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models](https://arxiv.org/abs/2507.06658)
*Gennadii Iakovlev*

Main category: cs.CL

TL;DR: 该研究通过人工智能技术检测政治精英在议会演讲中的互动，提出了一种新的精英极化测量方法，并构建了一个跨党派敌意指数。


<details>
  <summary>Details</summary>
Motivation: 研究旨在量化政治精英之间的极化现象，通过分析议会演讲中的互动和情感倾向，揭示党派间的敌意程度。

Method: 利用人工智能技术识别演讲中的发言者和被提及者，并评估情感温度，构建跨党派敌意指数。

Result: 研究覆盖了英国40年、匈牙利和意大利20年的数据，结果显示指数能有效反映选举、危机等事件的影响。

Conclusion: 该方法为欧盟范围内的精英极化研究奠定了基础，指数具有较好的表面效度，能捕捉政治动态。

Abstract: This project introduces a new measure of elite polarization via actor and
subject detection using artificial intelligence. I identify when politicians
mention one another in parliamentary speeches, note who is speaking and who is
being addressed, and assess the emotional temperature behind these evaluations.
This maps how elites evaluate their various out-parties, allowing us to create
an index of mutual out-party hostility, that is, elite polarization. While I
analyzed polarization data over the past four decades for the UK, and two
decades for Hungary and Italy, my approach lays the groundwork for a
twenty-year, EU-wide time-series dataset on elite polarization. I obtain the
results that can be aggregated by party and quarter. The resulting index
demonstrates a good face validity: it reacts to events such as electoral
campaigns, country- and party-level crises, and to parties losing and assuming
power.

</details>


### [25] [CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs](https://arxiv.org/abs/2507.06715)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: CLI-RAG框架通过分层分块和双阶段检索机制，解决了临床文本生成中数据非结构化和语义密集的挑战，显著提升了生成文本的对齐分数和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决临床文本生成中数据非结构化和语义密集的挑战，确保生成内容的临床相关性和可靠性。

Method: 提出CLI-RAG框架，采用分层分块策略和双阶段检索机制（全局阶段识别相关笔记类型，局部阶段提取高价值内容）。

Result: 在MIMIC-III数据集上生成的结构化进展笔记对齐分数达87.7%，优于临床医生撰写的基线80.7%，且生成结果在LLMs间高度一致。

Conclusion: CLI-RAG框架有效提升了临床文本生成的质量和可靠性，适用于实际临床场景。

Abstract: Large language models (LLMs), including zero-shot and few-shot paradigms,
have shown promising capabilities in clinical text generation. However,
real-world applications face two key challenges: (1) patient data is highly
unstructured, heterogeneous, and scattered across multiple note types and (2)
clinical notes are often long and semantically dense, making naive prompting
infeasible due to context length constraints and the risk of omitting
clinically relevant information.
  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a
domain-specific framework for structured and clinically grounded text
generation using LLMs. It incorporates a novel hierarchical chunking strategy
that respects clinical document structure and introduces a task-specific
dual-stage retrieval mechanism. The global stage identifies relevant note types
using evidence-based queries, while the local stage extracts high-value content
within those notes creating relevance at both document and section levels.
  We apply the system to generate structured progress notes for individual
hospital visits using 15 clinical note types from the MIMIC-III dataset.
Experiments show that it preserves temporal and semantic alignment across
visits, achieving an average alignment score of 87.7%, surpassing the 80.7%
baseline from real clinician-authored notes. The generated outputs also
demonstrate high consistency across LLMs, reinforcing deterministic behavior
essential for reproducibility, reliability, and clinical trust.

</details>


### [26] [On the Effect of Uncertainty on Layer-wise Inference Dynamics](https://arxiv.org/abs/2507.06722)
*Sunwoo Kim,Haneul Yoo,Alice Oh*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型（LLMs）在隐藏状态中对不确定性的编码方式不影响其推理动态，且简单方法检测不确定性可能不可行。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何内部表示和处理不确定性，以检测不确定性和防止幻觉。

Method: 使用Tuned Lens（Logit Lens的变体）分析11个数据集和5个模型的层间概率轨迹。

Result: 不确定性和确定性预测的层间概率轨迹高度一致，表明不确定性不影响推理动态。

Conclusion: 研究挑战了简单方法检测不确定性的可行性，并展示了可解释性方法在研究不确定性影响推理中的应用。

Abstract: Understanding how large language models (LLMs) internally represent and
process their predictions is central to detecting uncertainty and preventing
hallucinations. While several studies have shown that models encode uncertainty
in their hidden states, it is underexplored how this affects the way they
process such hidden states. In this work, we demonstrate that the dynamics of
output token probabilities across layers for certain and uncertain outputs are
largely aligned, revealing that uncertainty does not seem to affect inference
dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to
analyze the layer-wise probability trajectories of final prediction tokens
across 11 datasets and 5 models. Using incorrect predictions as those with
higher epistemic uncertainty, our results show aligned trajectories for certain
and uncertain predictions that both observe abrupt increases in confidence at
similar layers. We balance this finding by showing evidence that more competent
models may learn to process uncertainty differently. Our findings challenge the
feasibility of leveraging simplistic methods for detecting uncertainty at
inference. More broadly, our work demonstrates how interpretability methods may
be used to investigate the way uncertainty affects inference.

</details>


### [27] [KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution](https://arxiv.org/abs/2507.06753)
*Ye Kyaw Thu,Thura Aung,Thazin Myint Oo,Thepchai Supnithi*

Main category: cs.CL

TL;DR: 首次将Kolmogorov-Arnold卷积（KAConvText）应用于句子分类，涵盖仇恨言论检测、新闻分类和语言识别任务，使用不同嵌入配置和分类头，结果显示KAConvText-MLP结合微调fastText嵌入表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索KAConvText在句子分类任务中的应用潜力，特别是针对不平衡数据集和多类分类问题。

Method: 比较随机和fastText嵌入（静态与微调），使用CBOW和Skip-gram模型，结合CNN和CNN-KAN基线，并测试MLP和KAN分类头。

Result: KAConvText-MLP在仇恨言论检测（91.23%准确率）、新闻分类（92.66%）和语言识别（99.82%）中表现最优。

Conclusion: KAConvText结合微调fastText嵌入和MLP分类头在多种句子分类任务中表现出色，KAN分类头增强了模型可解释性。

Abstract: This paper presents the first application of Kolmogorov-Arnold Convolution
for Text (KAConvText) in sentence classification, addressing three tasks:
imbalanced binary hate speech detection, balanced multiclass news
classification, and imbalanced multiclass ethnic language identification. We
investigate various embedding configurations, comparing random to fastText
embeddings in both static and fine-tuned settings, with embedding dimensions of
100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs
and CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we
investigated KAConvText with different classification heads - MLP and KAN,
where using KAN head supports enhanced interpretability. Results show that
KAConvText-MLP with fine-tuned fastText embeddings achieves the best
performance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,
92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%
accuracy (F1-score = 0.9982) for language identification.

</details>


### [28] [Checklist Engineering Empowers Multilingual LLM Judges](https://arxiv.org/abs/2507.06774)
*Mohammad Ghiasvand Mohammadkhani,Hamid Beigy*

Main category: cs.CL

TL;DR: 提出了一种名为CE-Judge的无训练框架，利用开源模型和清单直觉进行多语言文本评估，性能优于基线模型，与GPT-4o相当。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM评估方法在多语言场景中依赖专有模型或大量训练数据的问题，降低成本和时间。

Method: 采用无训练的Checklist Engineering框架，结合开源模型进行多语言评估。

Result: 在多种语言和三个基准数据集上表现优于基线，与GPT-4o性能相当。

Conclusion: CE-Judge是一种高效、低成本的多语言文本评估方法，具有广泛应用潜力。

Abstract: Automated text evaluation has long been a central issue in Natural Language
Processing (NLP). Recently, the field has shifted toward using Large Language
Models (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While
promising and easily adaptable across tasks, this approach has seen limited
exploration in multilingual contexts. Existing multilingual studies often rely
on proprietary models or require extensive training data for fine-tuning,
raising concerns about cost, time, and efficiency. In this paper, we propose
Checklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free
framework that uses checklist intuition for multilingual evaluation with an
open-source model. Experiments across multiple languages and three benchmark
datasets, under both pointwise and pairwise settings, show that our method
generally surpasses the baselines and performs on par with the GPT-4o model.

</details>


### [29] [Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications](https://arxiv.org/abs/2507.06795)
*Seonwu Kim,Yohan Na,Kihun Kim,Hanhee Cho,Geun Lim,Mintae Kim,Seongik Park,Ki Hyun Kim,Youngsub Han,Byoung-Ki Jeon*

Main category: cs.CL

TL;DR: 研究验证了基于DACP的方法在小型语言模型（sLLMs）上的有效性，证明其在目标领域性能提升的同时保持通用能力，为企业部署提供了经济高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型（LLMs）的普及为企业应用提供了机会，但许多组织缺乏部署和维护大规模模型的基础设施。小型LLMs（sLLMs）成为实用替代方案，但其性能有限。本研究旨在探索DACP方法在商业应用中的实用性。

Method: 采用领域自适应持续预训练（DACP）方法，应用于多种基础模型和服务领域，通过实验和实际评估验证其效果。

Result: 实验表明，应用DACP的sLLMs在目标领域性能显著提升，同时保持了通用能力。

Conclusion: DACP为小型LLMs提供了一种经济高效且可扩展的企业级部署方案。

Abstract: The emergence of open-source large language models (LLMs) has expanded
opportunities for enterprise applications; however, many organizations still
lack the infrastructure to deploy and maintain large-scale models. As a result,
small LLMs (sLLMs) have become a practical alternative, despite their inherent
performance limitations. While Domain Adaptive Continual Pretraining (DACP) has
been previously explored as a method for domain adaptation, its utility in
commercial applications remains under-examined. In this study, we validate the
effectiveness of applying a DACP-based recipe across diverse foundation models
and service domains. Through extensive experiments and real-world evaluations,
we demonstrate that DACP-applied sLLMs achieve substantial gains in target
domain performance while preserving general capabilities, offering a
cost-efficient and scalable solution for enterprise-level deployment.

</details>


### [30] [Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams](https://arxiv.org/abs/2507.06803)
*Matthew Anderson Hendricks,Alice Cicirello*

Main category: cs.CL

TL;DR: 提出一种利用领域和专家知识自动生成动态系统计算模型的策略，通过SysML图提取信息，结合NLP和LLM优化中间输出，并通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 加速工程动态系统的设计与部署，减少手动建模的复杂性。

Method: 分五步实现，使用SysML图提取组件依赖、属性和操作信息，结合NLP和LLM优化中间输出，最终生成计算模型。

Result: 通过案例研究验证了方法的适用性，生成的计算模型性能优于仅使用LLM的结果。

Conclusion: 该方法具有通用性，适用于不同系统和领域，展示了从文本到模型的端到端自动化潜力。

Abstract: This paper contributes to speeding up the design and deployment of
engineering dynamical systems by proposing a strategy for exploiting domain and
expert knowledge for the automated generation of dynamical system computational
model starting from a corpus of document relevant to the dynamical system of
interest and an input document describing the specific system. This strategy is
implemented in five steps and, crucially, it uses system modeling language
diagrams (SysML) to extract accurate information about the dependencies,
attributes, and operations of components. Natural Language Processing (NLP)
strategies and Large Language Models (LLMs) are employed in specific tasks to
improve intermediate outputs of the SySML diagrams automated generation, such
as: list of key nouns; list of extracted relationships; list of key phrases and
key relationships; block attribute values; block relationships; and BDD diagram
generation. The applicability of automated SysML diagram generation is
illustrated with different case studies. The computational models of complex
dynamical systems from SysML diagrams are then obtained via code generation and
computational model generation steps. In the code generation step, NLP
strategies are used for summarization, while LLMs are used for validation only.
The proposed approach is not limited to a specific system, domain, or
computational software. The applicability of the proposed approach is shown via
an end-to-end example from text to model of a simple pendulum, showing improved
performance compared to results yielded by LLMs only.

</details>


### [31] [Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework](https://arxiv.org/abs/2507.06829)
*Zenan Xu,Zexuan Qiu,Guanhua Huang,Kun Li,Siheng Li,Chenchen Zhang,Kejiao Li,Qi Yi,Yuhao Jiang,Bo Zhou,Fengzong Lian,Zhanhui Kang*

Main category: cs.CL

TL;DR: 论文提出了一种结合顺序推理和并行推理优势的协作推理框架，并引入了语义熵（SE）作为评估模型响应质量的指标。


<details>
  <summary>Details</summary>
Motivation: 当前顺序推理和并行推理方法存在效率低下或缺乏协调的问题，需要一种更灵活的推理框架。

Method: 设计了协作推理框架，利用语义熵（SE）动态评估和控制推理过程。

Result: 语义熵（SE）与准确性呈强负相关，能有效指示推理质量。

Conclusion: 该框架结合了两种推理范式的优势，通过SE实现了高效的动态控制。

Abstract: Recent advances in large language models (LLMs) have accelerated progress
toward artificial general intelligence, with inference-time scaling emerging as
a key technique. Contemporary approaches leverage either sequential reasoning
(iteratively extending chains of thought) or parallel reasoning (generating
multiple solutions simultaneously) to scale inference. However, both paradigms
face fundamental limitations: sequential scaling typically relies on arbitrary
token budgets for termination, leading to inefficiency or premature cutoff;
while parallel scaling often lacks coordination among parallel branches and
requires intrusive fine-tuning to perform effectively. In light of these
challenges, we aim to design a flexible test-time collaborative inference
framework that exploits the complementary strengths of both sequential and
parallel reasoning paradigms. Towards this goal, the core challenge lies in
developing an efficient and accurate intrinsic quality metric to assess model
responses during collaborative inference, enabling dynamic control and early
termination of the reasoning trace. To address this challenge, we introduce
semantic entropy (SE), which quantifies the semantic diversity of parallel
model responses and serves as a robust indicator of reasoning quality due to
its strong negative correlation with accuracy...

</details>


### [32] [Shifting from Ranking to Set Selection for Retrieval Augmented Generation](https://arxiv.org/abs/2507.06838)
*Dahyun Lee,Yongrae Jo,Haeju Park,Moontae Lee*

Main category: cs.CL

TL;DR: SETR提出了一种基于集合的段落选择方法，通过Chain-of-Thought推理明确查询的信息需求，并选择最优段落集合以满足需求，显著提升了多跳问答中的检索质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于段落个体相关性重排序，难以满足复杂查询（如多跳问答）的信息需求。

Method: 采用Chain-of-Thought推理明确查询需求，并提出集合式段落选择方法SETR。

Result: 在多项基准测试中，SETR在答案正确性和检索质量上优于现有方法。

Conclusion: SETR为RAG系统提供了一种高效且有效的替代方案，优于传统重排序方法。

Abstract: Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved
passages are not only individually relevant but also collectively form a
comprehensive set. Existing approaches primarily rerank top-k passages based on
their individual relevance, often failing to meet the information needs of
complex queries in multi-hop question answering. In this work, we propose a
set-wise passage selection approach and introduce SETR, which explicitly
identifies the information requirements of a query through Chain-of-Thought
reasoning and selects an optimal set of passages that collectively satisfy
those requirements. Experiments on multi-hop RAG benchmarks show that SETR
outperforms both proprietary LLM-based rerankers and open-source baselines in
terms of answer correctness and retrieval quality, providing an effective and
efficient alternative to traditional rerankers in RAG systems. The code is
available at https://github.com/LGAI-Research/SetR

</details>


### [33] [Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights](https://arxiv.org/abs/2507.06893)
*Alexandra Abbas,Celia Waggoner,Justin Olive*

Main category: cs.CL

TL;DR: 论文总结了八个月维护开源AI评估工具的经验，提出了解决社区贡献扩展、统计方法和质量控制等挑战的方案。


<details>
  <summary>Details</summary>
Motivation: AI评估对评估大语言模型的能力和安全性至关重要，但现有工具在实现和维护上面临挑战。

Method: 提出了结构化队列管理框架、统计方法优化重采样和跨模型比较，以及系统质量控制流程。

Result: 研究发现AI评估需要专门的基础设施、统计严谨性和社区协作。

Conclusion: AI评估需超越传统软件开发实践，结合基础设施、统计方法和社区协作。

Abstract: AI evaluations have become critical tools for assessing large language model
capabilities and safety. This paper presents practical insights from eight
months of maintaining $inspect\_evals$, an open-source repository of 70+
community-contributed AI evaluations. We identify key challenges in
implementing and maintaining AI evaluations and develop solutions including:
(1) a structured cohort management framework for scaling community
contributions, (2) statistical methodologies for optimal resampling and
cross-model comparison with uncertainty quantification, and (3) systematic
quality control processes for reproducibility. Our analysis reveals that AI
evaluation requires specialized infrastructure, statistical rigor, and
community coordination beyond traditional software development practices.

</details>


### [34] [SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN](https://arxiv.org/abs/2507.06895)
*Luca Mariotti,Veronica Guidetti,Federica Mandreoli*

Main category: cs.CL

TL;DR: SCoRE是一种模块化、低成本的句子级关系抽取系统，结合对比学习和贝叶斯kNN分类器，在低监督环境下表现优异，并引入新评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱（KG）增强中关系抽取（RE）在低监督和噪声环境下的需求，提供适应性强且与预训练语言模型（PLM）无缝集成的解决方案。

Method: SCoRE采用监督对比学习和贝叶斯kNN分类器进行多标签分类，无需微调，支持灵活切换PLM。

Result: 在五个基准测试中，SCoRE表现优于或匹配现有方法，同时显著降低能耗。

Conclusion: SCoRE凭借高效、模块化和可扩展性，成为现实世界关系抽取应用的理想选择。

Abstract: The growing demand for efficient knowledge graph (KG) enrichment leveraging
external corpora has intensified interest in relation extraction (RE),
particularly under low-supervision settings. To address the need for adaptable
and noise-resilient RE solutions that integrate seamlessly with pre-trained
large language models (PLMs), we introduce SCoRE, a modular and cost-effective
sentence-level RE system. SCoRE enables easy PLM switching, requires no
finetuning, and adapts smoothly to diverse corpora and KGs. By combining
supervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)
classifier for multi-label classification, it delivers robust performance
despite the noisy annotations of distantly supervised corpora. To improve RE
evaluation, we propose two novel metrics: Correlation Structure Distance (CSD),
measuring the alignment between learned relational patterns and KG structures,
and Precision at R (P@R), assessing utility as a recommender system. We also
release Wiki20d, a benchmark dataset replicating real-world RE conditions where
only KG-derived annotations are available. Experiments on five benchmarks show
that SCoRE matches or surpasses state-of-the-art methods while significantly
reducing energy consumption. Further analyses reveal that increasing model
complexity, as seen in prior work, degrades performance, highlighting the
advantages of SCoRE's minimal design. Combining efficiency, modularity, and
scalability, SCoRE stands as an optimal choice for real-world RE applications.

</details>


### [35] [VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation](https://arxiv.org/abs/2507.06899)
*Ziang Ye,Yang Zhang,Wentao Shi,Xiaoyu You,Fuli Feng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 论文揭示了GUI代理在视觉接地过程中存在漏洞，可能被后门攻击利用，并提出了一种名为VisualTrap的攻击方法，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: GUI代理与个人设备的紧密集成带来了安全风险，尤其是后门攻击的威胁尚未充分研究。

Method: 提出VisualTrap方法，通过在视觉接地预训练阶段注入有毒数据，误导代理定位触发位置而非目标。

Result: 实验表明，仅需5%的有毒数据即可有效攻击，且攻击具有高度隐蔽性和跨环境泛化能力。

Conclusion: 研究强调了GUI代理后门攻击风险的紧迫性，需进一步研究防护措施。

Abstract: Graphical User Interface (GUI) agents powered by Large Vision-Language Models
(LVLMs) have emerged as a revolutionary approach to automating human-machine
interactions, capable of autonomously operating personal devices (e.g., mobile
phones) or applications within the device to perform complex real-world tasks
in a human-like manner. However, their close integration with personal devices
raises significant security concerns, with many threats, including backdoor
attacks, remaining largely unexplored. This work reveals that the visual
grounding of GUI agent-mapping textual plans to GUI elements-can introduce
vulnerabilities, enabling new types of backdoor attacks. With backdoor attack
targeting visual grounding, the agent's behavior can be compromised even when
given correct task-solving plans. To validate this vulnerability, we propose
VisualTrap, a method that can hijack the grounding by misleading the agent to
locate textual plans to trigger locations instead of the intended targets.
VisualTrap uses the common method of injecting poisoned data for attacks, and
does so during the pre-training of visual grounding to ensure practical
feasibility of attacking. Empirical results show that VisualTrap can
effectively hijack visual grounding with as little as 5% poisoned data and
highly stealthy visual triggers (invisible to the human eye); and the attack
can be generalized to downstream tasks, even after clean fine-tuning. Moreover,
the injected trigger can remain effective across different GUI environments,
e.g., being trained on mobile/web and generalizing to desktop environments.
These findings underscore the urgent need for further research on backdoor
attack risks in GUI agents.

</details>


### [36] [MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection](https://arxiv.org/abs/2507.06908)
*Ziyan Liu,Chunxiao Fan,Haoran Lou,Yuexin Wu,Kaiwei Deng*

Main category: cs.CL

TL;DR: MIND是一个多智能体框架，用于零样本有害模因检测，无需依赖标注数据，通过检索相似模因、双向洞察机制和多智能体辩论实现鲁棒决策。


<details>
  <summary>Details</summary>
Motivation: 社交媒体的模因快速传播，传统数据驱动方法难以检测新模因，因缺乏最新标注数据。

Method: 1) 从未标注参考集中检索相似模因；2) 提出双向洞察机制；3) 使用多智能体辩论机制进行决策。

Result: 在三个模因数据集上表现优于现有零样本方法，且在不同模型架构和参数规模上具有强泛化能力。

Conclusion: MIND为有害模因检测提供了可扩展的解决方案。

Abstract: The rapid expansion of memes on social media has highlighted the urgent need
for effective approaches to detect harmful content. However, traditional
data-driven approaches struggle to detect new memes due to their evolving
nature and the lack of up-to-date annotated data. To address this issue, we
propose MIND, a multi-agent framework for zero-shot harmful meme detection that
does not rely on annotated data. MIND implements three key strategies: 1) We
retrieve similar memes from an unannotated reference set to provide contextual
information. 2) We propose a bi-directional insight derivation mechanism to
extract a comprehensive understanding of similar memes. 3) We then employ a
multi-agent debate mechanism to ensure robust decision-making through reasoned
arbitration. Extensive experiments on three meme datasets demonstrate that our
proposed framework not only outperforms existing zero-shot approaches but also
shows strong generalization across different model architectures and parameter
scales, providing a scalable solution for harmful meme detection. The code is
available at https://github.com/destroy-lonely/MIND.

</details>


### [37] [MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction](https://arxiv.org/abs/2507.06909)
*Xiao Wang,Jiahuan Pei,Diancheng Shui,Zhiguang Han,Xin Sun,Dawei Zhu,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 论文探讨了法律判决预测中多被告和多指控是否应分开处理的问题，并提出了MPMCP数据集和四种场景的评估结果。


<details>
  <summary>Details</summary>
Motivation: 研究法律判决预测中多被告和多指控的处理方式，填补研究空白。

Method: 引入MPMCP数据集，评估四种场景下法律大语言模型的性能。

Result: 多被告多指控场景（S4）最具挑战性，不同模型表现差异显著。

Conclusion: 多被告多指控场景对模型性能影响最大，需进一步优化。

Abstract: Legal judgment prediction offers a compelling method to aid legal
practitioners and researchers. However, the research question remains
relatively under-explored: Should multiple defendants and charges be treated
separately in LJP? To address this, we introduce a new dataset namely
multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating
the performance of several prevailing legal large language models (LLMs) on
four practical legal judgment scenarios: (S1) single defendant with a single
charge, (S2) single defendant with multiple charges, (S3) multiple defendants
with a single charge, and (S4) multiple defendants with multiple charges. We
evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty
term prediction. We have conducted extensive experiments and found that the
scenario involving multiple defendants and multiple charges (S4) poses the
greatest challenges, followed by S2, S3, and S1. The impact varies
significantly depending on the model. For example, in S4 compared to S1,
InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,
while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.
Our dataset and code are available at
https://github.com/lololo-xiao/MultiJustice-MPMCP.

</details>


### [38] [Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues](https://arxiv.org/abs/2507.06910)
*Fareya Ikram,Alexander Scarlatos,Andrew Lan*

Main category: cs.CL

TL;DR: 研究探讨了现代大语言模型（如Llama 3和GPT-4o）在预测辅导对话中导师策略和学生表现的能力，发现即使最先进的模型也难以准确预测导师策略，但导师策略对学生表现有显著影响。


<details>
  <summary>Details</summary>
Motivation: 在线学习和AI辅导的兴起使得导师策略对学生表现的影响备受关注，但目前缺乏预测导师策略的研究。

Method: 使用两个数学辅导对话数据集，测试Llama 3和GPT-4o模型预测导师策略和学生表现的能力。

Result: 最先进的大语言模型在预测导师策略方面表现不佳，但导师策略对学生表现有显著影响。

Conclusion: 需要更强大的方法来预测导师策略，以优化AI辅导效果。

Abstract: Tutoring dialogues have gained significant attention in recent years, given
the prominence of online learning and the emerging tutoring abilities of
artificial intelligence (AI) agents powered by large language models (LLMs).
Recent studies have shown that the strategies used by tutors can have
significant effects on student outcomes, necessitating methods to predict how
tutors will behave and how their actions impact students. However, few works
have studied predicting tutor strategy in dialogues. Therefore, in this work we
investigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to
predict both future tutor moves and student outcomes in dialogues, using two
math tutoring dialogue datasets. We find that even state-of-the-art LLMs
struggle to predict future tutor strategy while tutor strategy is highly
indicative of student outcomes, outlining a need for more powerful methods to
approach this task.

</details>


### [39] [Rethinking Verification for LLM Code Generation: From Generation to Testing](https://arxiv.org/abs/2507.06920)
*Zihan Ma,Taolin Zhang,Maosong Cao,Wenwei Zhang,Minnan Luo,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 论文提出了一种人机协作的方法（SAGA）和测试用例生成基准（TCGBench），以提高代码生成评估的覆盖率和质量，解决了现有基准测试用例不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评估基准的测试用例数量有限且同质化，导致性能测量不准确，影响强化学习框架中的奖励估计。

Method: 提出多维度指标量化测试套件的全面性，并开发了人机协作方法SAGA和基准TCGBench。

Result: SAGA在TCGBench上的检测率为90.62%，验证器准确率为32.58%，优于LiveCodeBench-v6。

Conclusion: 该方法为可靠的LLM代码评估奠定了基础，推动了代码生成中的RLVR发展，并为自动化对抗测试和自适应基准集成提供了可能。

Abstract: Large language models (LLMs) have recently achieved notable success in
code-generation benchmarks such as HumanEval and LiveCodeBench. However, a
detailed examination reveals that these evaluation suites often comprise only a
limited number of homogeneous test cases, resulting in subtle faults going
undetected. This not only artificially inflates measured performance but also
compromises accurate reward estimation in reinforcement learning frameworks
utilizing verifiable rewards (RLVR). To address these critical shortcomings, we
systematically investigate the test-case generation (TCG) task by proposing
multi-dimensional metrics designed to rigorously quantify test-suite
thoroughness. Furthermore, we introduce a human-LLM collaborative method
(SAGA), leveraging human programming expertise with LLM reasoning capability,
aimed at significantly enhancing both the coverage and the quality of generated
test cases. In addition, we develop a TCGBench to facilitate the study of the
TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a
verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)
of the code generation evaluation benchmark synthesized by SAGA is 10.78%
higher than that of LiveCodeBench-v6. These results demonstrate the
effectiveness of our proposed method. We hope this work contributes to building
a scalable foundation for reliable LLM code evaluation, further advancing RLVR
in code generation, and paving the way for automated adversarial test synthesis
and adaptive benchmark integration.

</details>


### [40] [Investigating the Robustness of Retrieval-Augmented Generation at the Query Level](https://arxiv.org/abs/2507.06956)
*Sezen Perçin,Xin Su,Qutub Sha Syed,Phillip Howard,Aleksei Kuvshinov,Leo Schwinn,Kay-Ulrich Scholl*

Main category: cs.CL

TL;DR: 论文研究了检索增强生成（RAG）系统对查询扰动的敏感性，发现常用检索器性能在轻微查询变化下显著下降，并提出评估框架和实用建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）更新成本高且效率低，RAG虽能动态整合外部知识，但其性能高度依赖查询质量，亟需研究其鲁棒性。

Method: 通过分析RAG流程中各组件对查询扰动的敏感性，研究其性能变化，并提出评估框架，基于1092次实验提供建议。

Result: 常用检索器在轻微查询变化下性能显著下降，端到端问答系统中模块组合效果也受影响。

Conclusion: RAG系统对查询质量敏感，需改进鲁棒性，论文提出的评估框架和建议为实践提供了指导。

Abstract: Large language models (LLMs) are very costly and inefficient to update with
new information. To address this limitation, retrieval-augmented generation
(RAG) has been proposed as a solution that dynamically incorporates external
knowledge during inference, improving factual consistency and reducing
hallucinations. Despite its promise, RAG systems face practical challenges-most
notably, a strong dependence on the quality of the input query for accurate
retrieval. In this paper, we investigate the sensitivity of different
components in the RAG pipeline to various types of query perturbations. Our
analysis reveals that the performance of commonly used retrievers can degrade
significantly even under minor query variations. We study each module in
isolation as well as their combined effect in an end-to-end question answering
setting, using both general-domain and domain-specific datasets. Additionally,
we propose an evaluation framework to systematically assess the query-level
robustness of RAG pipelines and offer actionable recommendations for
practitioners based on the results of more than 1092 experiments we performed.

</details>


### [41] [FRaN-X: FRaming and Narratives-eXplorer](https://arxiv.org/abs/2507.06974)
*Artur Muratov,Hana Fatima Shaikh,Vanshikaa Jani,Tarek Mahmoud,Zhuohan Xie,Daniil Orel,Aaryamonvikram Singh,Yuxia Wang,Aadi Joshi,Hasan Iqbal,Ming Shan Hee,Dhruv Sahnan,Nikolaos Nikolaidis,Purificação Silvano,Dimitar Dimitrov,Roman Yangarber,Ricardo Campos,Alípio Jorge,Nuno Guimarães,Elisa Sartori,Nicolas Stefanovitch,Giovanni Da San Martino,Jakub Piskorski,Preslav Nakov*

Main category: cs.CL

TL;DR: FRaN-X是一个自动检测实体提及并分类其叙事角色的工具，支持五种语言和两个领域，提供交互式界面和可视化分析。


<details>
  <summary>Details</summary>
Motivation: 解决自动检测和标记实体如何被框架化的挑战，帮助媒体分析师探索和比较不同来源的叙事。

Method: 两阶段系统，结合序列标记和细粒度角色分类，使用22种细粒度角色分类实体。

Result: 支持多语言和多领域分析，提供交互式界面、搜索功能和可视化工具。

Conclusion: FRaN-X为媒体分析师提供了强大的工具，帮助理解和比较不同叙事框架。

Abstract: We present FRaN-X, a Framing and Narratives Explorer that automatically
detects entity mentions and classifies their narrative roles directly from raw
text. FRaN-X comprises a two-stage system that combines sequence labeling with
fine-grained role classification to reveal how entities are portrayed as
protagonists, antagonists, or innocents, using a unique taxonomy of 22
fine-grained roles nested under these three main categories. The system
supports five languages (Bulgarian, English, Hindi, Russian, and Portuguese)
and two domains (the Russia-Ukraine Conflict and Climate Change). It provides
an interactive web interface for media analysts to explore and compare framing
across different sources, tackling the challenge of automatically detecting and
labeling how entities are framed. Our system allows end users to focus on a
single article as well as analyze up to four articles simultaneously. We
provide aggregate level analysis including an intuitive graph visualization
that highlights the narrative a group of articles are pushing. Our system
includes a search feature for users to look up entities of interest, along with
a timeline view that allows analysts to track an entity's role transitions
across different contexts within the article. The FRaN-X system and the trained
models are licensed under an MIT License. FRaN-X is publicly accessible at
https://fran-x.streamlit.app/ and a video demonstration is available at
https://youtu.be/VZVi-1B6yYk.

</details>


### [42] [FlexOlmo: Open Language Models for Flexible Data Use](https://arxiv.org/abs/2507.07024)
*Weijia Shi,Akshita Bhagia,Kevin Farhat,Niklas Muennighoff,Pete Walsh,Jacob Morrison,Dustin Schwenk,Shayne Longpre,Jake Poznanski,Allyson Ettinger,Daogao Liu,Margaret Li,Dirk Groeneveld,Mike Lewis,Wen-tau Yih,Luca Soldaini,Kyle Lo,Noah A. Smith,Luke Zettlemoyer,Pang Wei Koh,Hannaneh Hajishirzi,Ali Farhadi,Sewon Min*

Main category: cs.CL

TL;DR: FlexOlmo是一种新型语言模型，支持分布式训练和数据灵活推理，无需共享数据，通过混合专家架构实现独立训练和灵活组合。


<details>
  <summary>Details</summary>
Motivation: 解决在受监管行业中敏感或受保护数据的共享问题，同时尊重数据所有者的偏好。

Method: 采用混合专家（MoE）架构，每个专家在封闭数据集上独立训练，通过领域感知路由集成，无需联合训练。

Result: 在31个任务上评估，FlexOlmo平均相对性能提升41%，优于现有模型合并方法10.1%。

Conclusion: FlexOlmo为数据所有者和研究人员提供了一种解决方案，既能利用封闭数据，又能控制数据访问权限。

Abstract: We introduce FlexOlmo, a new class of language models (LMs) that supports (1)
distributed training without data sharing, where different model parameters are
independently trained on closed datasets, and (2) data-flexible inference,
where these parameters along with their associated data can be flexibly
included or excluded from model inferences with no further training. FlexOlmo
employs a mixture-of-experts (MoE) architecture where each expert is trained
independently on closed datasets and later integrated through a new
domain-informed routing without any joint training. FlexOlmo is trained on
FlexMix, a corpus we curate comprising publicly available datasets alongside
seven domain-specific sets, representing realistic approximations of closed
sets. We evaluate models with up to 37 billion parameters (20 billion active)
on 31 diverse downstream tasks. We show that a general expert trained on public
data can be effectively combined with independently trained experts from other
data owners, leading to an average 41% relative improvement while allowing
users to opt out of certain data based on data licensing or permission
requirements. Our approach also outperforms prior model merging methods by
10.1% on average and surpasses the standard MoE trained without data
restrictions using the same training FLOPs. Altogether, this research presents
a solution for both data owners and researchers in regulated industries with
sensitive or protected data. FlexOlmo enables benefiting from closed data while
respecting data owners' preferences by keeping their data local and supporting
fine-grained control of data access during inference.

</details>


### [43] [UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations](https://arxiv.org/abs/2507.07030)
*Fengran Mo,Yifan Gao,Chuan Meng,Xin Liu,Zhuofeng Wu,Kelong Mao,Zhengyang Wang,Pei Chen,Zheng Li,Xian Li,Bing Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 论文提出了一种统一密集检索和响应生成的方法，通过联合微调和机制设计提升对话搜索系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有对话搜索系统通常使用分离的模型，无法同时利用模型的内在知识，影响检索和生成的效果。

Method: 联合微调不同目标，设计两种机制以减少不一致风险并缓解数据差异。

Result: 在五个对话搜索数据集上的评估显示，统一模型能同时提升两项任务性能，优于现有基线。

Conclusion: 统一模型能有效结合检索和生成，提升对话搜索系统的整体表现。

Abstract: The rapid advancement of conversational search systems revolutionizes how
information is accessed by enabling the multi-turn interaction between the user
and the system. Existing conversational search systems are usually built with
two different models. This separation restricts the system from leveraging the
intrinsic knowledge of the models simultaneously, which cannot ensure the
effectiveness of retrieval benefiting the generation. The existing studies for
developing unified models cannot fully address the aspects of understanding
conversational context, managing retrieval independently, and generating
responses. In this paper, we explore how to unify dense retrieval and response
generation for large language models in conversation. We conduct joint
fine-tuning with different objectives and design two mechanisms to reduce the
inconsistency risks while mitigating data discrepancy. The evaluations on five
conversational search datasets demonstrate that our unified model can mutually
improve both tasks and outperform the existing baselines.

</details>


### [44] [Discrete Diffusion Models for Language Generation](https://arxiv.org/abs/2507.07050)
*Ashen Weligalle*

Main category: cs.CL

TL;DR: 论文研究了离散扩散模型（D3PM）在自然语言生成中的表现，并与传统自回归模型（AR）进行了比较。结果显示D3PM在并行生成速度上有优势，但AR在压缩性能上更优。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在连续数据（如图像、视频）生成中表现优异，但在离散数据（如自然语言）中的应用仍具挑战性。本文旨在探索离散扩散模型在自然语言生成中的可行性和性能。

Method: 采用离散去噪扩散概率模型（D3PM），与传统自回归模型（AR）对比，评估指标包括Bits Per Token（BPT）、负对数似然（NLL）、困惑度（PPL）和批处理速度。

Result: D3PM在生成速度上表现更优（3.97批次/秒），但AR在压缩性能上更佳（平均BPT为4.59）。D3PM的最佳BPT为5.72。

Conclusion: 离散扩散模型在自然语言生成中展现出潜力，尤其在并行生成方面，但在生成质量上仍需改进。研究为未来非自回归语言生成提供了参考。

Abstract: Diffusion models have emerged as a powerful class of generative models,
achieving state-of-the-art results in continuous data domains such as image and
video generation. Their core mechanism involves a forward diffusion process
that gradually transforms structured data into a Gaussian-like distribution,
followed by a learned reverse process to reconstruct the data. While successful
in continuous modalities, applying this framework to discrete data-particularly
natural language-remains challenging due to token dependency complexities and
the lack of a defined generation order.This thesis investigates the feasibility
and performance of discrete diffusion models for natural language generation.
Specifically, we evaluate the Discrete Denoising Diffusion Probabilistic Model
(D3PM) and compare it with traditional autoregressive (AR) language models. To
assess generative performance, we use Bits Per Token (BPT), Negative
Log-Likelihood (NLL), Perplexity (PPL), and Batch Processing Speed.
  Results show the best-performing D3PM model achieves a BPT of 5.72, with a
mean of 8.05. The AR model outperforms in compression with a lower mean BPT of
4.59, but D3PM achieves higher processing speed, reaching up to 3.97 batches
per sec., indicating potential for parallel generation.All evaluations were
conducted under consistent conditions-generating 100,000 tokens per model with
a fixed batch size of four-for fair comparison. This research presents a
detailed analysis of diffusion-based vs. autoregressive models, highlighting
trade-offs in generative quality and efficiency. Findings emphasize both the
promise and limitations of diffusion models for discrete data, supporting
future work in non-autoregressive language generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [45] [Digital Wargames to Enhance Military Medical Evacuation Decision-Making](https://arxiv.org/abs/2507.06373)
*Jeremy Fischer,Ram Krishnamoorthy,Vishal Kumar,Mahdi Al-Husseini*

Main category: cs.AI

TL;DR: MEWI是一个三维多人模拟工具，用于在课堂环境中模拟和评估医疗后送网络，显著提升了学员的学习效果和协作决策能力。


<details>
  <summary>Details</summary>
Motivation: 缺乏在课堂环境中模拟医疗后送网络的工具，无法有效评估离线和在线决策性能。

Method: 开发了基于Unity的三维多人模拟工具MEWI，模拟战场约束和不确定性，并引入两个实战场景。

Result: MEWI显著提高了学员对医疗后送经验的学习吸收和协作决策能力。

Conclusion: MEWI是医疗教育高保真培训工具的重要进步，为联合部队的医疗后送教育和操作提供了关键见解。

Abstract: Medical evacuation is one of the United States Army's most storied and
critical mission sets, responsible for efficiently and expediently evacuating
the battlefield ill and injured. Medical evacuation planning involves designing
a robust network of medical platforms and facilities capable of moving and
treating large numbers of casualties. Until now, there has not been a medium to
simulate these networks in a classroom setting and evaluate both offline
planning and online decision-making performance. This work describes the
Medical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer
simulation developed in Unity that replicates battlefield constraints and
uncertainties. MEWI accurately models patient interactions at casualty
collection points, ambulance exchange points, medical treatment facilities, and
evacuation platforms. Two operational scenarios are introduced: an amphibious
island assault in the Pacific and a Eurasian conflict across a sprawling road
and river network. These scenarios pit students against the clock to save as
many casualties as possible while adhering to doctrinal lessons learned during
didactic training. We visualize performance data collected from two iterations
of the MEWI Pacific scenario executed in the United States Army's Medical
Evacuation Doctrine Course. We consider post-wargame Likert survey data from
student participants and external observer notes to identify key planning
decision points, document medical evacuation lessons learned, and quantify
general utility. Results indicate that MEWI participation substantially
improves uptake of medical evacuation lessons learned and co-operative
decision-making. MEWI is a substantial step forward in the field of
high-fidelity training tools for medical education, and our study findings
offer critical insights into improving medical evacuation education and
operations across the joint force.

</details>


### [46] [Representing Prompting Patterns with PDL: Compliance Agent Case Study](https://arxiv.org/abs/2507.06396)
*Mandana Vaziri,Louis Mandel,Yuji Watanabe,Hirokuni Kitahara,Martin Hirzel,Anca Sailer*

Main category: cs.AI

TL;DR: 提出了Prompt Declaration Language (PDL)，一种新型提示表示方法，旨在解决现有框架的复杂性和不灵活性问题，通过优化提示组合提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程框架要么过于复杂，要么缺乏灵活性，限制了高级代理编程的实现。

Method: 提出PDL方法，将提示置于核心位置，支持手动和自动提示调优，并整合LLM调用、规则代码和外部工具。

Result: 通过合规代理的案例研究，PDL的提示调优实现了4倍性能提升。

Conclusion: PDL通过声明式表示和优化潜力，显著提升了程序员生产力和代理性能。

Abstract: Prompt engineering for LLMs remains complex, with existing frameworks either
hiding complexity behind restrictive APIs or providing inflexible canned
patterns that resist customization -- making sophisticated agentic programming
challenging. We present the Prompt Declaration Language (PDL), a novel approach
to prompt representation that tackles this fundamental complexity by bringing
prompts to the forefront, enabling manual and automatic prompt tuning while
capturing the composition of LLM calls together with rule-based code and
external tools. By abstracting away the plumbing for such compositions, PDL
aims at improving programmer productivity while providing a declarative
representation that is amenable to optimization. This paper demonstrates PDL's
utility through a real-world case study of a compliance agent. Tuning the
prompting pattern of this agent yielded up to 4x performance improvement
compared to using a canned agent and prompt pattern.

</details>


### [47] [Jolting Technologies: Superexponential Acceleration in AI Capabilities and Implications for AGI](https://arxiv.org/abs/2507.06398)
*David Orban*

Main category: cs.AI

TL;DR: 论文研究了Jolting Technologies假说，提出AI能力发展可能呈现超指数增长，并通过理论框架和模拟验证检测方法。


<details>
  <summary>Details</summary>
Motivation: 探索AI能力发展的加速模式及其潜在影响，为未来实证研究提供工具。

Method: 开发理论框架，通过蒙特卡洛模拟验证检测方法，分析驱动因素如想法到行动的间隔缩短和AI迭代改进。

Result: 提供了理解AI发展轨迹的数学基础，为研究和政策提供见解。

Conclusion: 研究为AI超指数增长的可能性和影响提供了理论支持，但需未来实证数据验证。

Abstract: This paper investigates the Jolting Technologies Hypothesis, which posits
superexponential growth (increasing acceleration, or a positive third
derivative) in the development of AI capabilities. We develop a theoretical
framework and validate detection methodologies through Monte Carlo simulations,
while acknowledging that empirical validation awaits suitable longitudinal
data. Our analysis focuses on creating robust tools for future empirical
studies and exploring the potential implications should the hypothesis prove
valid. The study examines how factors such as shrinking idea-to-action
intervals and compounding iterative AI improvements drive this jolting pattern.
By formalizing jolt dynamics and validating detection methods through
simulation, this work provides the mathematical foundation necessary for
understanding potential AI trajectories and their consequences for AGI
emergence, offering insights for research and policy.

</details>


### [48] [Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)](https://arxiv.org/abs/2507.06798)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 论文证明了q-辩证系统比p-辩证系统更强大，后者又比d-辩证系统更强，揭示了反例和矛盾在自动信念修正中的互补作用。


<details>
  <summary>Details</summary>
Motivation: 研究辩证系统以统一、可计算的方式建模知识库的动态更新，特别是数学家和研究社区在追求真理过程中的信念修正。

Method: 区分并比较了三种辩证系统模型：d-辩证系统（基于不一致性修正）、p-辩证系统（基于反例修正）和q-辩证系统（结合两者）。

Result: 证明了q-辩证系统严格强于p-辩证系统，后者又严格强于d-辩证系统。

Conclusion: 结果强调了反例和矛盾在自动信念修正及数学推理中的互补作用。

Abstract: Dialectical systems are a mathematical formalism for modeling an agent
updating a knowledge base seeking consistency. Introduced in the 1970s by
Roberto Magari, they were originally conceived to capture how a working
mathematician or a research community refines beliefs in the pursuit of truth.
Dialectical systems also serve as natural models for the belief change of an
automated agent, offering a unifying, computable framework for dynamic belief
management.
  The literature distinguishes three main models of dialectical systems:
(d-)dialectical systems based on revising beliefs when they are seen to be
inconsistent, p-dialectical systems based on revising beliefs based on finding
a counterexample, and q-dialectical systems which can do both. We answer an
open problem in the literature by proving that q-dialectical systems are
strictly more powerful than p-dialectical systems, which are themselves known
to be strictly stronger than (d-)dialectical systems. This result highlights
the complementary roles of counterexample and contradiction in automated belief
revision, and thus also in the reasoning processes of mathematicians and
research communities.

</details>


### [49] [SCC-recursiveness in infinite argumentation (extended version)](https://arxiv.org/abs/2507.06852)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 论文探讨了如何将SCC递归语义扩展到无限论证框架（AFs），并提出了两种方法，评估了其方向性，并在有限框架中验证了部分语义的适用性。


<details>
  <summary>Details</summary>
Motivation: SCC递归语义在有限AFs中有效，但在无限AFs中因基础性问题而失效，因此需要扩展其适用性。

Method: 提出了两种扩展SCC递归语义到无限AFs的方法，并系统评估其方向性。

Result: 发现方向性在无限AFs中普遍失效，但在有限框架中部分语义满足方向性。

Conclusion: 研究推进了无限论证理论，为处理无界或动态领域的推理系统奠定了基础。

Abstract: Argumentation frameworks (AFs) are a foundational tool in artificial
intelligence for modeling structured reasoning and conflict. SCC-recursiveness
is a well-known design principle in which the evaluation of arguments is
decomposed according to the strongly connected components (SCCs) of the attack
graph, proceeding recursively from "higher" to "lower" components. While
SCC-recursive semantics such as \cft and \stgt have proven effective for finite
AFs, Baumann and Spanring showed the failure of SCC-recursive semantics to
generalize reliably to infinite AFs due to issues with well-foundedness.
  We propose two approaches to extending SCC-recursiveness to the infinite
setting. We systematically evaluate these semantics using Baroni and Giacomin's
established criteria, showing in particular that directionality fails in
general. We then examine these semantics' behavior in finitary frameworks,
where we find some of our semantics satisfy directionality. These results
advance the theory of infinite argumentation and lay the groundwork for
reasoning systems capable of handling unbounded or evolving domains.

</details>


### [50] [Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report](https://arxiv.org/abs/2507.06968)
*Li Du,Hanyu Zhao,Yiming Ju,Tengfei Pan*

Main category: cs.AI

TL;DR: 提出了一种系统化的指令数据构建框架，通过分层标注、种子选择、数据合成和模型诊断生成高质量指令数据集，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有指令数据集虽规模大，但在复杂任务和罕见领域表现不足，需提升覆盖范围和深度。

Method: 采用分层标注系统、种子选择算法、进化数据合成和模型诊断生成，形成闭环迭代优化。

Result: 构建了包含150万指令的高质量数据集InfinityInstruct-Subject，实验证明其提升模型指令跟随能力。

Conclusion: 该框架为指令数据集从数量扩张到质量提升提供了理论和实践基础。

Abstract: Instruction tuning has become a foundation for unlocking the capabilities of
large-scale pretrained models and improving their performance on complex tasks.
Thus, the construction of high-quality instruction datasets is crucial for
enhancing model performance and generalizability. Although current instruction
datasets have reached tens of millions of samples, models finetuned on them may
still struggle with complex instruction following and tasks in rare domains.
This is primarily due to limited expansion in both ``coverage'' (coverage of
task types and knowledge areas) and ``depth'' (instruction complexity) of the
instruction set. To address this issue, we propose a systematic instruction
data construction framework, which integrates a hierarchical labeling system,
an informative seed selection algorithm, an evolutionary data synthesis
process, and a model deficiency diagnosis with targeted data generation. These
components form an iterative closed-loop to continuously enhance the coverage
and depth of instruction data. Based on this framework, we construct
InfinityInstruct-Subject, a high-quality dataset containing ~1.5 million
instructions. Experiments on multiple foundation models and benchmark tasks
demonstrate its effectiveness in improving instruction-following capabilities.
Further analyses suggest that InfinityInstruct-Subject shows enlarged coverage
and depth compared to comparable synthesized instruction datasets. Our work
lays a theoretical and practical foundation for the efficient, continuous
evolution of instruction datasets, moving from data quantity expansion to
qualitative improvement.

</details>


### [51] [The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation](https://arxiv.org/abs/2507.06993)
*Jieren Deng,Aleksandar Cvetkovic,Pak Kiu Chung,Dragomir Yankov,Chiqun Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种动态旅行规划系统，通过三个协作代理解决传统系统的不足，显著提升了查询解析、导航准确性和应对突发情况的能力。


<details>
  <summary>Details</summary>
Motivation: 传统旅行规划系统静态且碎片化，无法应对现实中的复杂变化，如环境条件变化和行程中断。

Method: 提出三个协作代理：旅行规划代理（基于网格空间定位和地图分析）、目的地助手代理（提供精细导航）和本地发现代理（利用图像嵌入和RAG应对行程中断）。

Result: 系统在查询解析、导航准确性和应对突发情况方面表现出显著改进。

Conclusion: 该系统在从城市探索到应急响应等多个领域具有应用潜力。

Abstract: Traditional travel-planning systems are often static and fragmented, leaving
them ill-equipped to handle real-world complexities such as evolving
environmental conditions and unexpected itinerary disruptions. In this paper,
we identify three gaps between existing service providers causing frustrating
user experience: intelligent trip planning, precision "last-100-meter"
navigation, and dynamic itinerary adaptation. We propose three cooperative
agents: a Travel Planning Agent that employs grid-based spatial grounding and
map analysis to help resolve complex multi-modal user queries; a Destination
Assistant Agent that provides fine-grained guidance for the final navigation
leg of each journey; and a Local Discovery Agent that leverages image
embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to
trip plan disruptions. With evaluations and experiments, our system
demonstrates substantial improvements in query interpretation, navigation
accuracy, and disruption resilience, underscoring its promise for applications
from urban exploration to emergency response.

</details>


### [52] [First Return, Entropy-Eliciting Explore](https://arxiv.org/abs/2507.07017)
*Tianyu Zheng,Tianshun Xing,Qingshui Gu,Taoran Liang,Xingwei Qu,Xin Zhou,Yizhi Li,Zhoufutu Wen,Chenghua Lin,Wenhao Huang,Qian Liu,Ge Zhang,Zejun Ma*

Main category: cs.AI

TL;DR: FR3E框架通过结构化探索提升LLM推理能力，解决RLVR的不稳定探索问题。


<details>
  <summary>Details</summary>
Motivation: RLVR在提升LLM推理能力时存在探索不稳定的问题，需要更有效的探索方法。

Method: 提出FR3E框架，识别推理轨迹中的高不确定性决策点，并通过针对性rollout生成语义基础的中期反馈。

Result: 在数学推理基准测试中，FR3E实现了更稳定的训练、更长的连贯回答，并提高了完全正确轨迹的比例。

Conclusion: FR3E通过结构化探索显著提升了LLM的推理能力。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning
abilities of Large Language Models (LLMs) but it struggles with unstable
exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a
structured exploration framework that identifies high-uncertainty decision
points in reasoning trajectories and performs targeted rollouts to construct
semantically grounded intermediate feedback. Our method provides targeted
guidance without relying on dense supervision. Empirical results on
mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable
training, produces longer and more coherent responses, and increases the
proportion of fully correct trajectories. These results highlight the
framework's effectiveness in improving LLM reasoning through more robust and
structured exploration.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [53] [Super Kawaii Vocalics: Amplifying the "Cute" Factor in Computer Voice](https://arxiv.org/abs/2507.06235)
*Yuto Mandai,Katie Seaborn,Tomoyasu Nakano,Xin Sun,Yijia Wang,Jun Kato*

Main category: cs.HC

TL;DR: 研究探索了声音中的“可爱”（kawaii）元素及其操纵方法，填补了以往仅关注视觉可爱性的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于视觉层面的可爱性，而声音方面的研究几乎空白，因此需要探索声音中的可爱元素及其操纵方法。

Method: 通过四阶段研究（N=512），使用文本转语音（TTS）和游戏角色声音，操纵基频和共振峰频率来寻找“可爱点”。

Result: 发现某些声音在特定基频和共振峰频率下存在“可爱点”，但存在天花板效应。

Conclusion: 初步验证了可爱声音模型，并提供了操纵计算机声音可爱感知的基本方法。

Abstract: "Kawaii" is the Japanese concept of cute, which carries sociocultural
connotations related to social identities and emotional responses. Yet,
virtually all work to date has focused on the visual side of kawaii, including
in studies of computer agents and social robots. In pursuit of formalizing the
new science of kawaii vocalics, we explored what elements of voice relate to
kawaii and how they might be manipulated, manually and automatically. We
conducted a four-phase study (grand N = 512) with two varieties of computer
voices: text-to-speech (TTS) and game character voices. We found kawaii "sweet
spots" through manipulation of fundamental and formant frequencies, but only
for certain voices and to a certain extent. Findings also suggest a ceiling
effect for the kawaii vocalics of certain voices. We offer empirical validation
of the preliminary kawaii vocalics model and an elementary method for
manipulating kawaii perceptions of computer voice.

</details>


### [54] [Ragged Blocks: Rendering Structured Text with Style](https://arxiv.org/abs/2507.06460)
*Sam Cohen,Ravi Chugh*

Main category: cs.HC

TL;DR: 本文提出了一种新的文本布局算法，生成“ragged blocks”（不规则块），以在不破坏文本排版的情况下实现嵌套装饰，并展示了其在代码编辑器和文档渲染中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前文本可视化局限于扁平或嵌套盒子的方式，限制了信息的展示。本文旨在解决这一问题，提供更丰富的文本装饰选项。

Method: 提出了一种布局算法，生成不规则块（ragged blocks），支持嵌套装饰且不影响文本排版。

Result: 算法生成的布局比传统矩形块更紧凑，并通过场景展示其在代码编辑器和文档渲染中的应用潜力。

Conclusion: ragged blocks 算法为文本可视化提供了新的可能性，未来可广泛应用于多种文档渲染场景。

Abstract: Whether it be source code in a programming language, prose in natural
language, or otherwise, text is highly structured. Currently, text
visualizations are confined either to _flat, line-based_ decorations, which can
convey only limited information about textual structure, or _nested boxes_,
which convey structure but often destroy the typographic layout of the
underlying text. We hypothesize that the lack of rich styling options limits
the kinds of information that are displayed alongside text, wherever it may be
displayed.
  In this paper, we show that it is possible to achieve arbitrarily nested
decorations while minimally disturbing the underlying typographic layout.
Specifically, we present a layout algorithm that generates _ragged blocks_, or
_rocks_, which are rectilinear polygons that allow nested text to be compactly
rendered even when styled with borders and padding.
  We evaluate our layout algorithm in two ways. First, on a benchmark suite
comprising representative source code files in multiple programming languages,
we show that the (ragged block) layouts produced by our algorithm are
substantially more compact than the (rectangular block) layouts produced by
conventional techniques, when uniformly styling every element in the syntax
tree with borders and padding. Second, through a small gallery of usage
scenarios, we demonstrate how future code editors, word processors, and other
document-rendering GUIs might convey rich semantic information through
domain-specific styling of ragged blocks.

</details>


### [55] [Learning Japanese with Jouzu: Interaction Outcomes with Stylized Dialogue Fictional Agents](https://arxiv.org/abs/2507.06483)
*Zackary Rackauckas,Julia Hirschberg*

Main category: cs.HC

TL;DR: 研究探讨了动漫风格语音代理如何影响用户在多模态语言学习环境中的互动，发现代理的设计（如声音、角色和语言风格）显著影响用户体验和学习动机。


<details>
  <summary>Details</summary>
Motivation: 探索风格化语音代理在多模态语言学习环境中的作用，特别是其对用户互动、情感反应和学习行为的影响。

Method: 采用混合方法评估54名参与者与基于大型语言模型和文本转语音合成的动漫风格代理的互动，分析用户参与度、可用性、情感反应和学习行为。

Result: 代理的设计（声音、角色和语言风格）显著影响用户体验、学习动机和策略，尤其在语言水平和文化背景不同的用户中表现明显。

Conclusion: 研究为理解情感化和文化风格化代理在人机交互中的作用提供了见解，并为设计更具吸引力和社交响应性的系统提供了指导。

Abstract: This study investigates how stylized, voiced agents shape user interaction in
a multimodal language learning environment. We conducted a mixed-methods
evaluation of 54 participants interacting with anime-inspired characters
powered by large language models and expressive text-to-speech synthesis. These
agents responded in Japanese character language, offering users asynchronous,
semi-structured conversation in varying speech styles and emotional tones. We
analyzed user engagement patterns, perceived usability, emotional responses,
and learning behaviors, with particular attention to how agent stylization
influenced interaction across language proficiency levels and cultural
backgrounds. Our findings reveal that agent design, especially voice, persona,
and linguistic style, substantially affected user experience, motivation, and
strategy. This work contributes to the understanding of affective, culturally
stylized agents in human-agent interaction and offers guidance for designing
more engaging, socially responsive systems.

</details>


### [56] [Towards Designing Social Interventions for Online Climate Change Denialism Discussions](https://arxiv.org/abs/2507.06561)
*Ruican zhong,Shruti Phadke,Beth Goldberg,Tanushree Mitra*

Main category: cs.HC

TL;DR: 研究提出了一种使用内部语言在Reddit上对抗气候变化否认阴谋论的新框架，结合人工和AI生成的方法，发现证据干预能促进开放讨论。


<details>
  <summary>Details</summary>
Motivation: 随着阴谋论的流行，研究有效的干预策略以促进基于证据和科学的讨论变得至关重要。

Method: 结合人工和生成AI方法，设计干预消息并通过透明标记的机器人账户在Reddit上发布。

Result: 证据干预促进气候变化否认者的开放讨论，支持者则积极参与并提供更多证据。

Conclusion: 研究为社交媒体干预提供了宝贵见解，并指导未来研究。

Abstract: As conspiracy theories gain traction, it has become crucial to research
effective intervention strategies that can foster evidence and science-based
discussions in conspiracy theory communities online. This study presents a
novel framework using insider language to contest conspiracy theory ideology in
climate change denialism on Reddit. Focusing on discussions in two Reddit
communities, our research investigates reactions to pro-social and
evidence-based intervention messages for two cohorts of users: climate change
deniers and climate change supporters. Specifically, we combine manual and
generative AI-based methods to craft intervention messages and deploy the
interventions as replies on Reddit posts and comments through transparently
labeled bot accounts. On the one hand, we find that evidence-based
interventions with neutral language foster positive engagement, encouraging
open discussions among believers of climate change denialism. On the other,
climate change supporters respond positively, actively participating and
presenting additional evidence. Our study contributes valuable insights into
the process and challenges of automatically delivering interventions in
conspiracy theory communities on social media, and helps inform future research
on social media interventions.

</details>


### [57] [Smartphone Exergames with Real-Time Markerless Motion Capture: Challenges and Trade-offs](https://arxiv.org/abs/2507.06669)
*Mathieu Phosanarack,Laura Wallard,Sophie Lepreux,Christophe Kolski,Eugénie Avril*

Main category: cs.HC

TL;DR: 智能手机无标记动作捕捉技术为健康与康复提供低成本解决方案，但需解决实时性与准确性平衡等挑战。


<details>
  <summary>Details</summary>
Motivation: 通过智能手机摄像头实现无标记动作捕捉，降低传统系统的高成本与硬件限制，使更多人受益于健康与康复游戏。

Method: 开发一款结合实时无标记动作捕捉的移动应用，利用AI姿态估计技术追踪用户动作。

Result: 研究指出需优化AI模型以实现实时性能，并改进用户交互设计。

Conclusion: 智能手机运动游戏有望成为促进健康与康复的有效工具，但需进一步研究解决技术挑战。

Abstract: Markerless Motion Capture (MoCap) using smartphone cameras is a promising
approach to making exergames more accessible and cost-effective for health and
rehabilitation. Unlike traditional systems requiring specialized hardware,
recent advancements in AI-powered pose estimation enable movement tracking
using only a mobile device. For an upcoming study, a mobile application with
real-time exergames including markerless motion capture is being developed.
However, implementing such technology introduces key challenges, including
balancing accuracy and real-time responsiveness, ensuring proper user
interaction. Future research should explore optimizing AI models for realtime
performance, integrating adaptive gamification, and refining user-centered
design principles. By overcoming these challenges, smartphone-based exergames
could become powerful tools for engaging users in physical activity and
rehabilitation, extending their benefits to a broader audience.

</details>


### [58] [Effects of task difficulty and music expertise in virtual reality: Observations of cognitive load and task accuracy in a rhythm exergame](https://arxiv.org/abs/2507.06691)
*Kyla Ellahiyoun,Emma Jane Pretty,Renan Guarese,Marcel Takac,Haytham Fayek,Fabio Zambetta*

Main category: cs.HC

TL;DR: 音乐训练在VR游戏中提高任务准确性，但不直接减少主观认知负荷。


<details>
  <summary>Details</summary>
Motivation: 探索音乐训练、认知负荷和任务准确性在VR游戏中的关系。

Method: 32名参与者在不同难度下玩Beat Saber游戏，通过问卷和生理数据测量分析。

Result: 音乐训练显著提高任务准确性，但未显著影响主观认知负荷。

Conclusion: 音乐训练对任务表现有积极影响，未来研究可进一步探讨音乐专长的分类方法。

Abstract: This study explores the relationship between musical training, cognitive load
(CL), and task accuracy within the virtual reality (VR) exergame Beat Saber
across increasing levels of difficulty. Participants (N=32) completed a series
of post-task questionnaires after playing the game under three task difficulty
levels while having their physiological data measured by an Emotibit. Using
regression analyses, we found that task difficulty and gaming experience
significantly predicted subjective CL, whereas musical training did not.
However, musical training significantly predicted higher task accuracy, along
with lower subjective CL, increased gaming experience, and greater
physiological arousal. These results suggest that musical training enhances
task-specific performance but does not directly reduce subjective CL. Future
research should consider alternative methods of grouping musical expertise and
the additional predictability of flow and self-efficacy.

</details>


### [59] [Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted Classification in an Open-Source Telegram Monitoring Tool](https://arxiv.org/abs/2507.06734)
*Milena Pustet,Elisabeth Steffen,Helena Mihaljević,Grischa Stanjek,Yannis Illies*

Main category: cs.HC

TL;DR: 探讨公民社会组织（CSOs）如何参与开发AI辅助的开源工具，用于监测Telegram上的反民主运动。


<details>
  <summary>Details</summary>
Motivation: 平台减少内容审核投入，CSOs在监测有害内容中的作用日益重要，但缺乏开源工具支持其工作。

Method: 与CSO合作开发AI辅助的开源监测工具，强调CSOs的主动参与而非被动使用。

Result: 目前为进行中的项目，旨在填补研究与实践之间的鸿沟。

Conclusion: CSOs应成为技术开发的合作伙伴，以确保工具符合实际需求与价值观。

Abstract: The role of civil society organizations (CSOs) in monitoring harmful online
content is increasingly crucial, especially as platform providers reduce their
investment in content moderation. AI tools can assist in detecting and
monitoring harmful content at scale. However, few open-source tools offer
seamless integration of AI models and social media monitoring infrastructures.
Given their thematic expertise and contextual understanding of harmful content,
CSOs should be active partners in co-developing technological tools, providing
feedback, helping to improve models, and ensuring alignment with stakeholder
needs and values, rather than as passive 'consumers'. However, collaborations
between the open source community, academia, and civil society remain rare, and
research on harmful content seldom translates into practical tools usable by
civil society actors. This work in progress explores how CSOs can be
meaningfully involved in an AI-assisted open-source monitoring tool of
anti-democratic movements on Telegram, which we are currently developing in
collaboration with CSO stakeholders.

</details>


### [60] [Combining Human-centred Explainability and Explainable AI](https://arxiv.org/abs/2507.06751)
*Janin Koch,Vitor Fortes Rey*

Main category: cs.HC

TL;DR: 探讨人类中心可解释性（HCx）与可解释AI（xAI）的差异与结合机会，提出新的代数机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 研究当前HCx与xAI的理解差异，探索两者结合的可能性。

Method: 分析两领域现有观点，提出初步的代数机器学习方法。

Result: 发现差异与结合机会，展示初步工作成果。

Conclusion: 期待与HCxAI社区进一步探讨设计机会。

Abstract: This position paper looks at differences between the current understandings
of human-centered explainability and explainability AI. We discuss current
ideas in both fields, as well as the differences and opportunities we
discovered. As an example of combining both, we will present preliminary work
on a new algebraic machine learning approach. We are excited to continue
discussing design opportunities for human-centered explainability (HCx) and xAI
with the broader HCxAI community.

</details>


### [61] [Tailoring deep learning for real-time brain-computer interfaces: From offline models to calibration-free online decoding](https://arxiv.org/abs/2507.06779)
*Martin Wimpff,Jan Zerfowski,Bin Yang*

Main category: cs.HC

TL;DR: 论文提出了一种名为RAP的无参数方法，解决了深度学习在实时脑机接口应用中的三大挑战：离线到在线解码的过渡、计算复杂度高和数据需求大。


<details>
  <summary>Details</summary>
Motivation: 深度学习在离线脑机接口中表现优异，但在实时应用中面临三大挑战：离线到在线解码的模糊性、高计算复杂性和数据稀缺性。

Method: 引入实时自适应池化（RAP），通过修改现有离线模型的池化层来满足在线需求，并利用源自由域适应减少数据需求。

Result: RAP提供了一个高效、隐私保护的实时脑机接口框架，减少了校准需求并支持协同适应系统。

Conclusion: RAP为深度学习在在线脑机接口中的广泛应用奠定了基础，推动了高性能、用户为中心的脑机接口发展。

Abstract: Despite the growing success of deep learning (DL) in offline brain-computer
interfaces (BCIs), its adoption in real-time applications remains limited due
to three primary challenges. First, most DL solutions are designed for offline
decoding, making the transition to online decoding unclear. Second, the use of
sliding windows in online decoding substantially increases computational
complexity. Third, DL models typically require large amounts of training data,
which are often scarce in BCI applications. To address these challenges and
enable real-time, cross-subject decoding without subject-specific calibration,
we introduce realtime adaptive pooling (RAP), a novel parameter-free method.
RAP seamlessly modifies the pooling layers of existing offline DL models to
meet online decoding requirements. It also reduces computational complexity
during training by jointly decoding consecutive sliding windows. To further
alleviate data requirements, our method leverages source-free domain
adaptation, enabling privacy-preserving adaptation across varying amounts of
target data. Our results demonstrate that RAP provides a robust and efficient
framework for real-time BCI applications. It preserves privacy, reduces
calibration demands, and supports co-adaptive BCI systems, paving the way for
broader adoption of DL in online BCIs. These findings lay a strong foundation
for developing user-centered, high-performance BCIs that facilitate immediate
feedback and user learning.

</details>


### [62] [Toward Neurodivergent-Aware Productivity: A Systems and AI-Based Human-in-the-Loop Framework for ADHD-Affected Professionals](https://arxiv.org/abs/2507.06864)
*Raghavendra Deshmukh*

Main category: cs.HC

TL;DR: 论文提出了一种结合系统思维、人机交互设计、AI/ML和隐私优先自适应代理的框架，以支持ADHD患者在数字工作环境中的注意力管理。


<details>
  <summary>Details</summary>
Motivation: 针对ADHD患者在IT和知识密集型行业中的注意力管理、任务切换和自我调节挑战，现有工具未能满足其需求。

Method: 采用系统思维和人机交互设计，结合AI/ML技术，通过设备端机器学习感知用户行为，提供非干扰性的提示和支持。

Result: 开发了一个可复制的模型，用于在高干扰工作环境中提供自适应、包容性的支持工具。

Conclusion: 该框架为ADHD患者提供了有效的注意力管理支持，同时兼顾隐私和适应性。

Abstract: Digital work environments in IT and knowledge-based sectors demand high
levels of attention management, task juggling, and self-regulation. For adults
with ADHD, these settings often amplify challenges such as time blindness,
digital distraction, emotional reactivity, and executive dysfunction. These
individuals prefer low-touch, easy-to-use interventions for daily tasks.
Conventional productivity tools often fail to support the cognitive variability
and overload experienced by neurodivergent professionals. This paper presents a
framework that blends Systems Thinking, Human-in-the-Loop design, AI/ML, and
privacy-first adaptive agents to support ADHD-affected users. The assistant
senses tab usage, application focus, and inactivity using on-device ML. These
cues are used to infer attention states and deliver nudges, reflective prompts,
or accountability-based presence (body doubling) that aid regulation without
disruption. Technically grounded in AI, the approach views attention as shaped
by dynamic feedback loops. The result is a replicable model for adaptive,
inclusive support tools in high-distraction work environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [63] [Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals](https://arxiv.org/abs/2507.06267)
*Hyeontae Jo,Krešimir Josić,Jae Kyoung Kim*

Main category: cs.LG

TL;DR: 提出了一种名为HADES-NN的新方法，用于估计非自治微分方程的参数，通过神经网络平滑处理不连续的外部信号，从而提高参数估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 非自治微分方程在建模受外部信号影响的系统时非常重要，但当信号突变时，参数估计变得困难。

Method: HADES-NN分为两阶段：首先用神经网络将不连续信号平滑化，然后用平滑信号估计模型参数。

Result: HADES-NN在多种应用中（如生物钟系统和酵母交配反应）提供了高精度的参数估计。

Conclusion: HADES-NN扩展了可拟合实际测量数据的模型范围。

Abstract: Non-autonomous differential equations are crucial for modeling systems
influenced by external signals, yet fitting these models to data becomes
particularly challenging when the signals change abruptly. To address this
problem, we propose a novel parameter estimation method utilizing functional
approximations with artificial neural networks. Our approach, termed Harmonic
Approximation of Discontinuous External Signals using Neural Networks
(HADES-NN), operates in two iterated stages. In the first stage, the algorithm
employs a neural network to approximate the discontinuous signal with a smooth
function. In the second stage, it uses this smooth approximate signal to
estimate model parameters. HADES-NN gives highly accurate and precise parameter
estimates across various applications, including circadian clock systems
regulated by external light inputs measured via wearable devices and the mating
response of yeast to external pheromone signals. HADES-NN greatly extends the
range of model systems that can be fit to real-world measurements.

</details>


### [64] [Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease](https://arxiv.org/abs/2507.06326)
*Harsh Ravivarapu,Gaurav Bagwe,Xiaoyong Yuan,Chunxiu Yu,Lan Zhang*

Main category: cs.LG

TL;DR: SEA-DBS是一种基于强化学习的自适应脑深部刺激（aDBS）框架，通过预测奖励模型和Gumbel Softmax探索方法，解决了样本效率低和硬件资源受限的问题。


<details>
  <summary>Details</summary>
Motivation: 传统开环DBS缺乏适应性和个性化，而现有RL方法存在样本复杂度高、探索不稳定和硬件兼容性差的问题。

Method: SEA-DBS结合预测奖励模型和Gumbel Softmax探索，优化样本效率和探索稳定性。

Result: 在帕金森病基底节模拟中，SEA-DBS表现出更快的收敛性、更强的病理β波段抑制能力，且对FP16量化具有鲁棒性。

Conclusion: SEA-DBS为实时、资源受限的神经调控提供了一种实用且有效的RL框架。

Abstract: Deep brain stimulation (DBS) is an established intervention for Parkinson's
disease (PD), but conventional open-loop systems lack adaptability, are
energy-inefficient due to continuous stimulation, and provide limited
personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a
closed-loop alternative, using biomarkers such as beta-band oscillations to
dynamically modulate stimulation. While reinforcement learning (RL) holds
promise for personalized aDBS control, existing methods suffer from high sample
complexity, unstable exploration in binary action spaces, and limited
deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses
the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a
predictive reward model to reduce reliance on real-time feedback and employs
Gumbel Softmax-based exploration for stable, differentiable policy updates in
binary action spaces. Together, these components improve sample efficiency,
exploration robustness, and compatibility with resource-constrained
neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic
simulation of Parkinsonian basal ganglia activity, demonstrating faster
convergence, stronger suppression of pathological beta-band power, and
resilience to post-training FP16 quantization. Our results show that SEA-DBS
offers a practical and effective RL-based aDBS framework for real-time,
resource-constrained neuromodulation.

</details>


### [65] [SymFlux: deep symbolic regression of Hamiltonian vector fields](https://arxiv.org/abs/2507.06342)
*M. A. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: SymFlux是一个深度学习框架，通过符号回归从标准辛平面上的向量场中识别哈密顿函数。


<details>
  <summary>Details</summary>
Motivation: 推动哈密顿力学中的自动化发现，通过深度学习模型直接从向量场中提取哈密顿函数的符号表达式。

Method: 采用混合CNN-LSTM架构，训练和验证基于新开发的哈密顿向量场数据集。

Result: 模型能准确恢复哈密顿函数的符号表达式。

Conclusion: SymFlux在哈密顿力学的自动化发现中表现出色，为相关领域提供了新工具。

Abstract: We present SymFlux, a novel deep learning framework that performs symbolic
regression to identify Hamiltonian functions from their corresponding vector
fields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM
architectures to learn and output the symbolic mathematical expression of the
underlying Hamiltonian. Training and validation are conducted on newly
developed datasets of Hamiltonian vector fields, a key contribution of this
work. Our results demonstrate the model's effectiveness in accurately
recovering these symbolic expressions, advancing automated discovery in
Hamiltonian mechanics.

</details>


### [66] [DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2507.06366)
*Yupu Zhang,Zelin Xu,Tingsong Xiao,Gustavo Seabra,Yanjun Li,Chenglong Li,Zhe Jiang*

Main category: cs.LG

TL;DR: 论文提出DecoyDB数据集和定制化GCL框架，用于蛋白质-配体复合物的自监督学习，显著提升结合亲和力预测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 药物发现中蛋白质-配体结合亲和力预测因缺乏大规模高质量标记数据而受限，现有PDBbind数据集标记样本不足20K。自监督学习（尤其是图对比学习）可通过预训练解决此问题，但缺乏合适的未标记数据集和算法设计。

Method: 提出DecoyDB数据集，包含高分辨率真实复合物和多样化的计算生成诱饵结构（负样本），并设计定制化GCL框架进行预训练和微调。

Result: 实验表明，基于DecoyDB预训练的模型在准确性、标签效率和泛化能力上表现优越。

Conclusion: DecoyDB和定制化GCL框架填补了蛋白质-配体复合物自监督学习的空白，为药物发现提供了有效工具。

Abstract: Predicting the binding affinity of protein-ligand complexes plays a vital
role in drug discovery. Unfortunately, progress has been hindered by the lack
of large-scale and high-quality binding affinity labels. The widely used
PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,
especially graph contrastive learning (GCL), provides a unique opportunity to
break the barrier by pre-training graph neural network models based on vast
unlabeled complexes and fine-tuning the models on much fewer labeled complexes.
However, the problem faces unique challenges, including a lack of a
comprehensive unlabeled dataset with well-defined positive/negative complex
pairs and the need to design GCL algorithms that incorporate the unique
characteristics of such data. To fill the gap, we propose DecoyDB, a
large-scale, structure-aware dataset specifically designed for self-supervised
GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground
truth complexes (less than 2.5 Angstrom) and diverse decoy structures with
computationally generated binding poses that range from realistic to suboptimal
(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation
(RMSD) from the native pose. We further design a customized GCL framework to
pre-train graph neural networks based on DecoyDB and fine-tune the models with
labels from PDBbind. Extensive experiments confirm that models pre-trained with
DecoyDB achieve superior accuracy, label efficiency, and generalizability.

</details>


### [67] [The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks](https://arxiv.org/abs/2507.06367)
*El Mehdi Achour,Kathlén Kohn,Holger Rauhut*

Main category: cs.LG

TL;DR: 论文研究了深度线性卷积网络梯度流的几何性质，发现参数空间的梯度流可以表示为函数空间上的黎曼梯度流，且不受初始化条件限制。


<details>
  <summary>Details</summary>
Motivation: 探讨线性卷积网络梯度流的几何性质，扩展了全连接网络的类似结果。

Method: 分析线性卷积网络的梯度流，证明其在参数空间上的梯度流可以表示为函数空间上的黎曼梯度流。

Result: 对于D≥2的卷积或D=1且步长大于1的情况，梯度流均可表示为黎曼梯度流。

Conclusion: 线性卷积网络的梯度流具有几何不变性，且黎曼度量依赖于初始化。

Abstract: We study geometric properties of the gradient flow for learning deep linear
convolutional networks. For linear fully connected networks, it has been shown
recently that the corresponding gradient flow on parameter space can be written
as a Riemannian gradient flow on function space (i.e., on the product of weight
matrices) if the initialization satisfies a so-called balancedness condition.
We establish that the gradient flow on parameter space for learning linear
convolutional networks can be written as a Riemannian gradient flow on function
space regardless of the initialization. This result holds for $D$-dimensional
convolutions with $D \geq 2$, and for $D =1$ it holds if all so-called strides
of the convolutions are greater than one. The corresponding Riemannian metric
depends on the initialization.

</details>


### [68] [Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation](https://arxiv.org/abs/2507.06380)
*Habibur Rahaman,Atri Chatterjee,Swarup Bhunia*

Main category: cs.LG

TL;DR: WINGs框架通过动态生成全连接层权重和压缩卷积层权重，显著减少内存需求，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂神经网络存储大量权重所需的高内存问题。

Method: 使用PCA降维和轻量级SVR模型预测权重，结合敏感性分析压缩低敏感性层。

Result: FC层压缩53倍，AlexNet在MNIST上压缩28倍，CIFAR-10上压缩18倍，精度损失1-2%。

Conclusion: WINGs显著降低内存需求，提升吞吐量并降低能耗，适用于资源受限的边缘应用。

Abstract: Complex neural networks require substantial memory to store a large number of
synaptic weights. This work introduces WINGs (Automatic Weight Generator for
Secure and Storage-Efficient Deep Learning Models), a novel framework that
dynamically generates layer weights in a fully connected neural network (FC)
and compresses the weights in convolutional neural networks (CNNs) during
inference, significantly reducing memory requirements without sacrificing
accuracy. WINGs framework uses principal component analysis (PCA) for
dimensionality reduction and lightweight support vector regression (SVR) models
to predict layer weights in the FC networks, removing the need for storing
full-weight matrices and achieving substantial memory savings. It also
preferentially compresses the weights in low-sensitivity layers of CNNs using
PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers
an added level of security, as any bit-flip attack with weights in compressed
layers has an amplified and readily detectable effect on accuracy. WINGs
achieves 53x compression for the FC layers and 28x for AlexNet with MNIST
dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.
This significant reduction in memory results in higher throughput and lower
energy for DNN inference, making it attractive for resource-constrained edge
applications.

</details>


### [69] [KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks](https://arxiv.org/abs/2507.06381)
*James Hazelden,Laura Driscoll,Eli Shlizerman,Eric Shea-Brown*

Main category: cs.LG

TL;DR: 论文提出了一种梯度流分解方法，通过参数算子K和线性化流传播算子P，揭示了梯度下降在非线性循环模型中塑造学习表示的机制。


<details>
  <summary>Details</summary>
Motivation: 理解梯度下降在循环动态系统（如RNNs、Neural ODEs和GRUs）中如何塑造学习表示，尤其是有限非线性模型的机制。

Method: 将梯度流分解为参数算子K和线性化流传播算子P的乘积，分析其相互作用。

Result: 揭示了低维潜在动态的成因，并展示了多任务训练中目标对齐的测量方法。

Conclusion: 该分解方法为理解非线性循环模型中的梯度下降学习提供了新工具，并通过实验和理论验证了其有效性。

Abstract: Gradient Descent (GD) and its variants are the primary tool for enabling
efficient training of recurrent dynamical systems such as Recurrent Neural
Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics
that are formed in these models exhibit features such as neural collapse and
emergence of latent representations that may support the remarkable
generalization properties of networks. In neuroscience, qualitative features of
these representations are used to compare learning in biological and artificial
systems. Despite recent progress, there remains a need for theoretical tools to
rigorously understand the mechanisms shaping learned representations,
especially in finite, non-linear models. Here, we show that the gradient flow,
which describes how the model's dynamics evolve over GD, can be decomposed into
a product that involves two operators: a Parameter Operator, K, and a
Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in
feed-forward neural networks, while P appears in Lyapunov stability and optimal
control theory. We demonstrate two applications of our decomposition. First, we
show how their interplay gives rise to low-dimensional latent dynamics under
GD, and, specifically, how the collapse is a result of the network structure,
over and above the nature of the underlying task. Second, for multi-task
training, we show that the operators can be used to measure how objectives
relevant to individual sub-tasks align. We experimentally and theoretically
validate these findings, providing an efficient Pytorch package, \emph{KPFlow},
implementing robust analysis tools for general recurrent architectures. Taken
together, our work moves towards building a next stage of understanding of GD
learning in non-linear recurrent models.

</details>


### [70] [Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning](https://arxiv.org/abs/2507.06402)
*Siddhant Deshpande,Yalemzerf Getnet,Waltenegus Dargie*

Main category: cs.LG

TL;DR: 论文分析了CNN、ResNet和混合Transformer-CNN模型在心电图（ECG）信号篡改检测中的性能，并评估了Siamese网络在ECG身份验证中的表现。实验结果显示，这些模型在多种篡改场景下均表现出高准确率。


<details>
  <summary>Details</summary>
Motivation: 随着无线ECG系统在健康监测和身份验证中的普及，保护信号完整性免受篡改变得至关重要。

Method: 使用连续小波变换（CWT）将一维ECG信号转换为二维时频表示，并训练和评估多种模型（CNN、ResNet、混合Transformer-CNN及Siamese网络）。

Result: 在高度碎片化篡改场景中，模型准确率超过99.5%；在细微篡改中，FeatCNN-TranCNN模型平均准确率为98%；身份验证中，混合CNN-Transformer Siamese模型达到100%准确率。

Conclusion: 混合Transformer-CNN模型和Siamese网络在ECG篡改检测和身份验证中表现出色，为信号完整性保护提供了有效解决方案。

Abstract: With the proliferation of wireless electrocardiogram (ECG) systems for health
monitoring and authentication, protecting signal integrity against tampering is
becoming increasingly important. This paper analyzes the performance of CNN,
ResNet, and hybrid Transformer-CNN models for tamper detection. It also
evaluates the performance of a Siamese network for ECG based identity
verification. Six tampering strategies, including structured segment
substitutions and random insertions, are emulated to mimic real world attacks.
The one-dimensional ECG signals are transformed into a two dimensional
representation in the time frequency domain using the continuous wavelet
transform (CWT). The models are trained and evaluated using ECG data from 54
subjects recorded in four sessions 2019 to 2025 outside of clinical settings
while the subjects performed seven different daily activities. Experimental
results show that in highly fragmented manipulation scenarios, CNN,
FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding
99.5 percent . Similarly, for subtle manipulations (for example, 50 percent
from A and 50 percent from B and, 75 percent from A and 25 percent from B
substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable
performance, achieving an average accuracy of 98 percent . For identity
verification, the pure Transformer-Siamese network achieved an average accuracy
of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model
delivered perfect verification performance with 100 percent accuracy.

</details>


### [71] [Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction](https://arxiv.org/abs/2507.06432)
*Mingcheng Zhu,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: KnowRare是一个基于领域适应的深度学习框架，用于预测ICU中罕见疾病的临床结果，解决了数据稀缺和异质性问题，并在多项任务中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: ICU中罕见疾病因数据稀缺和异质性而未被充分研究，KnowRare旨在填补这一空白。

Method: 通过自监督预训练学习条件无关表示，并利用条件知识图选择性适应临床相似条件的知识。

Result: 在五项临床预测任务中，KnowRare表现优于现有模型和ICU评分系统。

Conclusion: KnowRare是一个实用且稳健的解决方案，可支持ICU罕见疾病的临床决策。

Abstract: Artificial Intelligence has revolutionised critical care for common
conditions. Yet, rare conditions in the intensive care unit (ICU), including
recognised rare diseases and low-prevalence conditions in the ICU, remain
underserved due to data scarcity and intra-condition heterogeneity. To bridge
such gaps, we developed KnowRare, a domain adaptation-based deep learning
framework for predicting clinical outcomes for rare conditions in the ICU.
KnowRare mitigates data scarcity by initially learning condition-agnostic
representations from diverse electronic health records through self-supervised
pre-training. It addresses intra-condition heterogeneity by selectively
adapting knowledge from clinically similar conditions with a developed
condition knowledge graph. Evaluated on two ICU datasets across five clinical
prediction tasks (90-day mortality, 30-day readmission, ICU mortality,
remaining length of stay, and phenotyping), KnowRare consistently outperformed
existing state-of-the-art models. Additionally, KnowRare demonstrated superior
predictive performance compared to established ICU scoring systems, including
APACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in
adapting its parameters to accommodate dataset-specific and task-specific
characteristics, its generalisation to common conditions under limited data
scenarios, and its rationality in selecting source conditions. These findings
highlight KnowRare's potential as a robust and practical solution for
supporting clinical decision-making and improving care for rare conditions in
the ICU.

</details>


### [72] [eegFloss: A Python package for refining sleep EEG recordings using machine learning models](https://arxiv.org/abs/2507.06433)
*Niloy Sikder,Paul Zerr,Mahdad Jafarzadeh Esfahani,Martin Dresler,Matthias Krauledat*

Main category: cs.LG

TL;DR: 介绍了一个名为eegFloss的开源Python工具包，用于检测睡眠EEG记录中的伪影，提高睡眠研究的准确性。


<details>
  <summary>Details</summary>
Motivation: EEG信号在睡眠研究中易受伪影干扰，导致自动睡眠分期错误，影响研究结果。

Method: 开发了eegUsability机器学习模型，用于检测EEG数据中的伪影段，并提供了其他功能如自动检测在床时间和生成睡眠统计。

Result: eegUsability在分类性能上表现良好（F1-score约0.85，Cohen's kappa 0.78），识别可用EEG数据的召回率达94%。

Conclusion: eegFloss通过解决伪影问题，提升了睡眠研究的分析精度和结果可靠性。

Abstract: Electroencephalography (EEG) allows monitoring of brain activity, providing
insights into the functional dynamics of various brain regions and their roles
in cognitive processes. EEG is a cornerstone in sleep research, serving as the
primary modality of polysomnography, the gold standard in the field. However,
EEG signals are prone to artifacts caused by both internal (device-specific)
factors and external (environmental) interferences. As sleep studies are
becoming larger, most rely on automatic sleep staging, a process highly
susceptible to artifacts, leading to erroneous sleep scores. This paper
addresses this challenge by introducing eegFloss, an open-source Python package
to utilize eegUsability, a novel machine learning (ML) model designed to detect
segments with artifacts in sleep EEG recordings. eegUsability has been trained
and evaluated on manually artifact-labeled EEG data collected from 15
participants over 127 nights using the Zmax headband. It demonstrates solid
overall classification performance (F1-score is approximately 0.85, Cohens
kappa is 0.78), achieving a high recall rate of approximately 94% in
identifying channel-wise usable EEG data, and extends beyond Zmax.
Additionally, eegFloss offers features such as automatic time-in-bed detection
using another ML model named eegMobility, filtering out certain artifacts, and
generating hypnograms and sleep statistics. By addressing a fundamental
challenge faced by most sleep studies, eegFloss can enhance the precision and
rigor of their analysis as well as the accuracy and reliability of their
outcomes.

</details>


### [73] [Can Interpretation Predict Behavior on Unseen Data?](https://arxiv.org/abs/2507.06445)
*Victoria R. Li,Jenny Kaufmann,Martin Wattenberg,David Alvarez-Melis,Naomi Saphra*

Main category: cs.LG

TL;DR: 研究探讨了可解释性工具在预测模型在分布外（OOD）数据上行为的潜力，发现注意力模式与OOD泛化之间存在关联。


<details>
  <summary>Details</summary>
Motivation: 旨在验证可解释性工具是否能预测模型在未见数据上的行为，填补现有研究的空白。

Method: 通过分析数百个独立训练的Transformer模型的注意力模式，研究其与OOD泛化能力的关系。

Result: 发现当注意力呈现分层模式时，模型倾向于在OOD数据上分层泛化，即使规则实现不依赖分层模式。

Conclusion: 结果为可解释性工具预测模型行为提供了概念验证，鼓励进一步研究。

Abstract: Interpretability research often aims to predict how a model will respond to
targeted interventions on specific mechanisms. However, it rarely predicts how
a model will respond to unseen input data. This paper explores the promises and
challenges of interpretability as a tool for predicting out-of-distribution
(OOD) model behavior. Specifically, we investigate the correspondence between
attention patterns and OOD generalization in hundreds of Transformer models
independently trained on a synthetic classification task. These models exhibit
several distinct systematic generalization rules OOD, forming a diverse
population for correlational analysis. In this setting, we find that simple
observational tools from interpretability can predict OOD performance. In
particular, when in-distribution attention exhibits hierarchical patterns, the
model is likely to generalize hierarchically on OOD data -- even when the
rule's implementation does not rely on these hierarchical patterns, according
to ablation tests. Our findings offer a proof-of-concept to motivate further
interpretability work on predicting unseen model behavior.

</details>


### [74] [FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models](https://arxiv.org/abs/2507.06449)
*Qianyu Long,Qiyuan Wang,Christos Anagnostopoulos,Daning Bi*

Main category: cs.LG

TL;DR: FedPhD是一种新颖的联邦学习方法，专门用于高效训练扩散模型（DMs），通过分层联邦学习和同质性感知模型聚合解决数据异构性和高通信成本问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）在联邦学习（FL）环境中训练时面临数据异构性和高通信成本的挑战，现有研究对此关注不足。

Method: FedPhD采用分层联邦学习、同质性感知模型聚合和选择策略，并结合分布式结构化剪枝以提高计算效率和减少存储需求。

Result: 实验表明，FedPhD在FID分数上表现优异，通信成本降低88%，计算和通信资源仅需56%，FID改进至少34%。

Conclusion: FedPhD为FL环境中高效训练DMs提供了有效解决方案，显著提升了性能和资源利用率。

Abstract: Federated Learning (FL), as a distributed learning paradigm, trains models
over distributed clients' data. FL is particularly beneficial for distributed
training of Diffusion Models (DMs), which are high-quality image generators
that require diverse data. However, challenges such as high communication costs
and data heterogeneity persist in training DMs similar to training Transformers
and Convolutional Neural Networks. Limited research has addressed these issues
in FL environments. To address this gap and challenges, we introduce a novel
approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD
leverages Hierarchical FL with homogeneity-aware model aggregation and
selection policy to tackle data heterogeneity while reducing communication
costs. The distributed structured pruning of FedPhD enhances computational
efficiency and reduces model storage requirements in clients. Our experiments
across multiple datasets demonstrate that FedPhD achieves high model
performance regarding Fr\'echet Inception Distance (FID) scores while reducing
communication costs by up to $88\%$. FedPhD outperforms baseline methods
achieving at least a $34\%$ improvement in FID, while utilizing only $56\%$ of
the total computation and communication resources.

</details>


### [75] [Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models](https://arxiv.org/abs/2507.06458)
*Arjun Banerjee,David Martinez,Camille Dang,Ethan Tam*

Main category: cs.LG

TL;DR: 本文提出了一种自动化框架，用于为蛋白质语言模型（PLM）中的每个神经元标记生物学意义的自然语言描述，并开发了一种基于神经元激活的引导方法，以生成具有目标特性的蛋白质。


<details>
  <summary>Details</summary>
Motivation: 理解PLM内部神经元的生物学表征，并利用这些信息生成具有特定特性的蛋白质。

Method: 引入自动化框架标记神经元，开发神经元激活引导方法生成目标蛋白质。

Result: 揭示了神经元对多样生化特性的选择性敏感，并成功生成了具有目标特性的蛋白质。

Conclusion: 该方法为PLM的神经元理解提供了新视角，并展示了其在蛋白质设计中的应用潜力。

Abstract: Protein language models (PLMs) encode rich biological information, yet their
internal neuron representations are poorly understood. We introduce the first
automated framework for labeling every neuron in a PLM with biologically
grounded natural language descriptions. Unlike prior approaches relying on
sparse autoencoders or manual annotation, our method scales to hundreds of
thousands of neurons, revealing individual neurons are selectively sensitive to
diverse biochemical and structural properties. We then develop a novel neuron
activation-guided steering method to generate proteins with desired traits,
enabling convergence to target biochemical properties like molecular weight and
instability index as well as secondary and tertiary structural motifs,
including alpha helices and canonical Zinc Fingers. We finally show that
analysis of labeled neurons in different model sizes reveals PLM scaling laws
and a structured neuron space distribution.

</details>


### [76] [Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm](https://arxiv.org/abs/2507.06461)
*Risi Jaiswal,Supriyo Datta,Joseph G. Makin*

Main category: cs.LG

TL;DR: 该论文提出了一种基于前向-前向算法的二进制随机单元训练方法，以减少机器学习中的能耗，并通过实验验证了其性能接近实数算法，同时显著节省能源。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习因大规模神经网络的高能耗问题亟需解决方案，反向传播算法在硬件加速器上存在挑战，因此探索替代方法以减少能耗。

Method: 研究推导了二进制随机单元的前向-前向算法，通过激活二值化和随机性结合，利用p-bits硬件高效实现矩阵乘法。

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上，性能接近实数算法，但能耗估计降低了一个数量级。

Conclusion: 二进制随机单元的前向-前向算法是一种有效的低能耗替代方案，适用于硬件加速器。

Abstract: Reducing energy consumption has become a pressing need for modern machine
learning, which has achieved many of its most impressive results by scaling to
larger and more energy-consumptive neural networks. Unfortunately, the main
algorithm for training such networks, backpropagation, poses significant
challenges for custom hardware accelerators, due to both its serial
dependencies and the memory footprint needed to store forward activations for
the backward pass. Alternatives to backprop, although less effective, do exist;
here the main computational bottleneck becomes matrix multiplication. In this
study, we derive forward-forward algorithms for binary, stochastic units.
Binarization of the activations transforms matrix multiplications into indexing
operations, which can be executed efficiently in hardware. Stochasticity,
combined with tied weights across units with different biases, bypasses the
information bottleneck imposed by binary units. Furthermore, although slow and
expensive in traditional hardware, binary sampling that is very fast can be
implemented cheaply with p-bits (probabilistic bits), novel devices made up of
unstable magnets. We evaluate our proposed algorithms on the MNIST,
Fashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to
real-valued forward-forward, but with an estimated energy savings of about one
order of magnitude.

</details>


### [77] [SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam](https://arxiv.org/abs/2507.06464)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Wen Gao*

Main category: cs.LG

TL;DR: 论文提出了一种新的优化器SignSoftSGD（S3），通过改进Adam的更新机制，解决了其梯度波动大和损失峰值的问题，并在理论和实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: Adam在训练深度神经网络中表现优异，但其成功机制和局限性尚未充分研究。论文旨在增强Adam的优势并解决其不稳定性问题。

Method: S3通过引入p阶动量、统一EMA系数和NAG模块，改进了Adam的更新机制，实现了稳定训练和快速收敛。

Result: S3在多种任务中表现优于Adam，收敛更快且损失峰值更少，甚至能以10倍学习率稳定训练。

Conclusion: S3在效率和性能上均优于Adam，为深度学习的优化提供了更可靠的解决方案。

Abstract: Adam has proven remarkable successful in training deep neural networks, but
the mechanisms underlying its empirical successes and limitations remain
underexplored. In this study, we demonstrate that the effectiveness of Adam
stems largely from its similarity to SignSGD in robustly handling large
gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes
due to its uncontrolled update scaling. To enhance the advantage of Adam and
mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with
three key innovations. \emph{First}, S3 generalizes the sign-like update by
employing a flexible $p$-th order momentum ($p \geq 1$) in the denominator,
departing from the conventional second-order momentum (variance)
preconditioning. This design enables enhanced performance while achieving
stable training even with aggressive learning rates. \emph{Second}, S3
minimizes the occurrences of loss spikes through unified exponential moving
average coefficients for numerator and denominator momenta, which inherently
bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \emph{Third}, S3
incorporates an equivalent Nesterov's accelerated gradient(NAG) module,
accelerating convergence without memory overhead. Theoretically, we prove that
S3 achieves the optimal convergence rate of
$O\left(\frac{1}{T^{\sfrac{1}{4}}}\right)$ for general nonconvex stochastic
optimization under weak assumptions. Extensive experiments across a range of
vision and language tasks show that \textsf{\small S3} not only converges more
rapidly and improves performance but also rarely experiences loss spikes, even
with a \textbf{$\bm{10 \times}$} larger learning rate. In fact, S3 delivers
performance comparable to or better than AdamW with \textbf{$2 \times$} the
training steps, establishing its efficacy in both efficiency and final task
performance.

</details>


### [78] [Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models](https://arxiv.org/abs/2507.06466)
*Aaron Dharna,Cong Lu,Jeff Clune*

Main category: cs.LG

TL;DR: 论文提出了一种名为FMSP的新方法，利用基础模型的代码生成能力和广泛知识，通过三种变体（vFMSP、NSSP、QDSP）改进自博弈算法，解决传统自博弈的局限性，并在Car Tag和Gandalf实验中展示了其多样性和高质量策略发现的能力。


<details>
  <summary>Details</summary>
Motivation: 传统自博弈算法（SP）在生成多样性和避免局部最优行为方面存在不足，论文旨在通过结合基础模型的能力来克服这些问题。

Method: 提出了三种FMSP变体：vFMSP（持续优化策略）、NSSP（生成多样性策略）、QDSP（结合多样性和高质量策略），并在Car Tag和Gandalf实验中验证其效果。

Result: 在Car Tag中，FMSP超越了人类设计的策略；在Gandalf中，FMSP成功突破了LLM的防御并自动修补漏洞。

Conclusion: FMSP为自博弈算法提供了新的研究方向，能够更开放和创造性地发现策略。

Abstract: Multi-agent interactions have long fueled innovation, from natural
predator-prey dynamics to the space race. Self-play (SP) algorithms try to
harness these dynamics by pitting agents against ever-improving opponents,
thereby creating an implicit curriculum toward learning high-quality solutions.
However, SP often fails to produce diverse solutions and can get stuck in
locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a
new direction that leverages the code-generation capabilities and vast
knowledge of foundation models (FMs) to overcome these challenges by leaping
across local optima in policy space. We propose a family of approaches: (1)
\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent
policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play
(NSSP)} builds a diverse population of strategies, ignoring performance; and
(3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)},
creates a diverse set of high-quality policies by combining the diversity of
NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a
continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety
simulation in which an attacker tries to jailbreak an LLM's defenses. In Car
Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and
heuristic-based methods, to name just a few. In terms of discovered policy
quality, \ouralgo and vFMSP surpass strong human-designed strategies. In
Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through
and jailbreaking six different, progressively stronger levels of defense.
Furthermore, FMSPs can automatically proceed to patch the discovered
vulnerabilities. Overall, FMSPs represent a promising new research frontier of
improving self-play with foundation models, opening fresh paths toward more
creative and open-ended strategy discovery

</details>


### [79] [Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning](https://arxiv.org/abs/2507.06469)
*Yudan Song,Yuecen Wei,Yuhang Lu,Qingyun Sun,Minglai Shao,Li-e Wang,Chunming Hu,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: 论文提出了一种双视图图表示学习方法（MimbFD），用于解决基于GNN的欺诈检测中的消息不平衡问题，通过拓扑消息可达性和局部混淆去偏模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法在欺诈检测中因局部交互和类别不平衡导致全局拓扑信息传输不均，欺诈节点信息易被淹没。

Method: 设计了拓扑消息可达性模块和局部混淆去偏模块，分别用于穿透欺诈伪装和平衡类别影响。

Result: 在三个公开欺诈数据集上的实验表明，MimbFD在欺诈检测中表现优异。

Conclusion: MimbFD通过双视图设计有效缓解了消息不平衡问题，提升了欺诈检测性能。

Abstract: Graph representation learning has become a mainstream method for fraud
detection due to its strong expressive power, which focuses on enhancing node
representations through improved neighborhood knowledge capture. However, the
focus on local interactions leads to imbalanced transmission of global
topological information and increased risk of node-specific information being
overwhelmed during aggregation due to the imbalance between fraud and benign
nodes. In this paper, we first summarize the impact of topology and class
imbalance on downstream tasks in GNN-based fraud detection, as the problem of
imbalanced supervisory messages is caused by fraudsters' topological behavior
obfuscation and identity feature concealment. Based on statistical validation,
we propose a novel dual-view graph representation learning method to mitigate
Message imbalance in Fraud Detection(MimbFD). Specifically, we design a
topological message reachability module for high-quality node representation
learning to penetrate fraudsters' camouflage and alleviate insufficient
propagation. Then, we introduce a local confounding debiasing module to adjust
node representations, enhancing the stable association between node
representations and labels to balance the influence of different classes.
Finally, we conducted experiments on three public fraud datasets, and the
results demonstrate that MimbFD exhibits outstanding performance in fraud
detection.

</details>


### [80] [FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning](https://arxiv.org/abs/2507.06482)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Jiahua Shi,Jun Shen*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedDifRC的新方法，通过引入扩散模型来缓解联邦学习中的数据异构性问题，利用文本驱动的对比学习和噪声驱动的一致性正则化提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异构性会损害模型的收敛和性能，因此需要一种有效的方法来解决这一问题。

Method: 提出FedDifRC方法，结合扩散模型的表示能力，通过文本驱动的对比学习和噪声驱动的一致性正则化来优化联邦学习过程。

Result: 实验验证了FedDifRC的有效性，并展示了关键组件的高效性。

Conclusion: FedDifRC通过扩散模型的指导成功缓解了数据异构性问题，提升了联邦学习的性能。

Abstract: Federated learning aims at training models collaboratively across
participants while protecting privacy. However, one major challenge for this
paradigm is the data heterogeneity issue, where biased data preferences across
multiple clients, harming the model's convergence and performance. In this
paper, we first introduce powerful diffusion models into the federated learning
paradigm and show that diffusion representations are effective steers during
federated training. To explore the possibility of using diffusion
representations in handling data heterogeneity, we propose a novel
diffusion-inspired Federated paradigm with Diffusion Representation
Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion
models to mitigate data heterogeneity. The key idea is to construct text-driven
diffusion contrasting and noise-driven diffusion regularization, aiming to
provide abundant class-related semantic information and consistent convergence
signals. On the one hand, we exploit the conditional feedback from the
diffusion model for different text prompts to build a text-driven contrastive
learning strategy. On the other hand, we introduce a noise-driven consistency
regularization to align local instances with diffusion denoising
representations, constraining the optimization region in the feature space. In
addition, FedDifRC can be extended to a self-supervised scheme without relying
on any labeled data. We also provide a theoretical analysis for FedDifRC to
ensure convergence under non-convex objectives. The experiments on different
scenarios validate the effectiveness of FedDifRC and the efficiency of crucial
components.

</details>


### [81] [MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models](https://arxiv.org/abs/2507.06502)
*Yiwen Liu,Chenyu Zhang,Junjie Song,Siqi Chen,Sun Yin,Zihan Wang,Lingming Zeng,Yuji Cao,Junming Jiao*

Main category: cs.LG

TL;DR: MoFE-Time是一种创新的时间序列预测模型，通过结合时间和频域特征，在预训练-微调范式中提升复杂时间序列的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在预训练-微调范式中未能同时建模时间和频域特征，导致复杂时间序列预测性能不佳。

Method: 提出MoFE-Time模型，结合时间和频域特征，使用混合专家（MoE）网络和多维稀疏表示。

Result: 在六个公共基准测试中，MoFE-Time表现最优，MSE和MAE分别降低6.95%和6.02%。在私有数据集NEV-sales上也取得显著效果。

Conclusion: MoFE-Time在理论和实际应用中均表现出色，验证了其有效性。

Abstract: As a prominent data modality task, time series forecasting plays a pivotal
role in diverse applications. With the remarkable advancements in Large
Language Models (LLMs), the adoption of LLMs as the foundational architecture
for time series modeling has gained significant attention. Although existing
models achieve some success, they rarely both model time and frequency
characteristics in a pretraining-finetuning paradigm leading to suboptimal
performance in predictions of complex time series, which requires both modeling
periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an
innovative time series forecasting model that integrates time and frequency
domain features within a Mixture of Experts (MoE) network. Moreover, we use the
pretraining-finetuning paradigm as our training framework to effectively
transfer prior pattern knowledge across pretraining and finetuning datasets
with different periodicity distributions. Our method introduces both frequency
and time cells as experts after attention modules and leverages the MoE routing
mechanism to construct multidimensional sparse representations of input
signals. In experiments on six public benchmarks, MoFE-Time has achieved new
state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared
to the representative methods Time-MoE. Beyond the existing evaluation
benchmarks, we have developed a proprietary dataset, NEV-sales, derived from
real-world business scenarios. Our method achieves outstanding results on this
dataset, underscoring the effectiveness of the MoFE-Time model in practical
commercial applications.

</details>


### [82] [Instance-Wise Monotonic Calibration by Constrained Transformation](https://arxiv.org/abs/2507.06516)
*Yunrui Zhang,Gustavo Batista,Salil S. Kanhere*

Main category: cs.LG

TL;DR: 提出了一种新的单调后校准方法，通过线性参数化校准映射，确保表达性、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络常产生校准不良的概率估计，现有方法无法保证单调性或牺牲表达性。

Method: 使用线性参数化的约束优化问题设计校准映射，确保单调性和表达性。

Result: 在多个数据集和模型上表现优于现有校准方法，且数据与计算效率高。

Conclusion: 该方法在保持单调性的同时，提高了校准性能，兼具高效性和可解释性。

Abstract: Deep neural networks often produce miscalibrated probability estimates,
leading to overconfident predictions. A common approach for calibration is
fitting a post-hoc calibration map on unseen validation data that transforms
predicted probabilities. A key desirable property of the calibration map is
instance-wise monotonicity (i.e., preserving the ranking of probability
outputs). However, most existing post-hoc calibration methods do not guarantee
monotonicity. Previous monotonic approaches either use an under-parameterized
calibration map with limited expressive ability or rely on black-box neural
networks, which lack interpretability and robustness. In this paper, we propose
a family of novel monotonic post-hoc calibration methods, which employs a
constrained calibration map parameterized linearly with respect to the number
of classes. Our proposed approach ensures expressiveness, robustness, and
interpretability while preserving the relative ordering of the probability
output by formulating the proposed calibration map as a constrained
optimization problem. Our proposed methods achieve state-of-the-art performance
across datasets with different deep neural network models, outperforming
existing calibration methods while being data and computation-efficient. Our
code is available at
https://github.com/YunruiZhang/Calibration-by-Constrained-Transformation

</details>


### [83] [AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks](https://arxiv.org/abs/2507.06525)
*Huiqi Zhang,Fang Xie*

Main category: cs.LG

TL;DR: 提出了一种名为AdaDPIGU的差分隐私SGD框架，通过重要性梯度更新和自适应剪枝机制，在高维环境下实现高效隐私保护与性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私方法在高维设置中因噪声规模随维度增加而性能下降，需改进。

Method: 预训练阶段使用差分隐私高斯机制估计参数重要性，梯度更新阶段剪枝低重要性坐标并引入自适应剪枝机制。

Result: 在MNIST和CIFAR-10上表现优异，隐私预算下性能接近或超过非隐私模型。

Conclusion: AdaDPIGU通过自适应稀疏化同时提升隐私保护与模型效用。

Abstract: Differential privacy has been proven effective for stochastic gradient
descent; however, existing methods often suffer from performance degradation in
high-dimensional settings, as the scale of injected noise increases with
dimensionality. To tackle this challenge, we propose AdaDPIGU--a new
differentially private SGD framework with importance-based gradient updates
tailored for deep neural networks. In the pretraining stage, we apply a
differentially private Gaussian mechanism to estimate the importance of each
parameter while preserving privacy. During the gradient update phase, we prune
low-importance coordinates and introduce a coordinate-wise adaptive clipping
mechanism, enabling sparse and noise-efficient gradient updates. Theoretically,
we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy
and retains convergence guarantees. Extensive experiments on standard
benchmarks validate the effectiveness of AdaDPIGU. All results are reported
under a fixed retention ratio of 60%. On MNIST, our method achieves a test
accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching
the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at
$\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating
that adaptive sparsification can enhance both privacy and utility.

</details>


### [84] [Direct Regret Optimization in Bayesian Optimization](https://arxiv.org/abs/2507.06529)
*Fengxue Zhang,Yuxin Chen*

Main category: cs.LG

TL;DR: 提出了一种基于直接后悔优化的贝叶斯优化方法，通过联合学习最优模型和非近视获取函数，显著降低了多步后悔。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法依赖手工设计的获取函数和代理模型，且通常是近视的，无法全局优化。

Method: 使用高斯过程集合生成模拟轨迹，训练端到端决策变换器，结合密集训练-稀疏学习范式。

Result: 在合成和真实基准测试中表现优于基线，实现了更低的简单后悔和更鲁棒的探索。

Conclusion: 该方法为贝叶斯优化提供了一种更高效和鲁棒的解决方案。

Abstract: Bayesian optimization (BO) is a powerful paradigm for optimizing expensive
black-box functions. Traditional BO methods typically rely on separate
hand-crafted acquisition functions and surrogate models for the underlying
function, and often operate in a myopic manner. In this paper, we propose a
novel direct regret optimization approach that jointly learns the optimal model
and non-myopic acquisition by distilling from a set of candidate models and
acquisitions, and explicitly targets minimizing the multi-step regret. Our
framework leverages an ensemble of Gaussian Processes (GPs) with varying
hyperparameters to generate simulated BO trajectories, each guided by an
acquisition function chosen from a pool of conventional choices, until a
Bayesian early stop criterion is met. These simulated trajectories, capturing
multi-step exploration strategies, are used to train an end-to-end decision
transformer that directly learns to select next query points aimed at improving
the ultimate objective. We further adopt a dense training--sparse learning
paradigm: The decision transformer is trained offline with abundant simulated
data sampled from ensemble GPs and acquisitions, while a limited number of real
evaluations refine the GPs online. Experimental results on synthetic and
real-world benchmarks suggest that our method consistently outperforms BO
baselines, achieving lower simple regret and demonstrating more robust
exploration in high-dimensional or noisy settings.

</details>


### [85] [Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits](https://arxiv.org/abs/2507.06535)
*Shan Shen,Shenglu Hua,Jiajun Zou,Jiawei Liu,Jianwang Zhai,Chuan Shi,Wenjian Yu*

Main category: cs.LG

TL;DR: 论文提出了一种名为CircuitGCL的图对比学习框架，用于解决模拟混合信号电路图表示学习中的挑战，如数据稀缺和标签不平衡。该方法通过自监督学习和标签重平衡技术，显著提升了寄生估计任务的性能。


<details>
  <summary>Details</summary>
Motivation: 模拟混合信号电路图的表示学习面临数据稀缺、标签分布不平衡和电路实现多样性等挑战，需要一种鲁棒且可迁移的表示学习方法。

Method: CircuitGCL结合了表示散射和标签重平衡技术，采用自监督学习策略生成拓扑不变的节点嵌入，并使用平衡MSE和bsmCE损失来缓解标签分布差异。

Result: 在TSMC 28nm AMS设计上的实验中，CircuitGCL在边级回归任务中R²提升了33.64%至44.20%，在节点分类任务中F1分数提升了0.9倍至2.1倍。

Conclusion: CircuitGCL通过创新的对比学习和标签重平衡技术，显著提升了电路图表示学习的性能，为下游任务提供了更鲁棒和可迁移的表示。

Abstract: Graph representation learning on Analog-Mixed Signal (AMS) circuits is
crucial for various downstream tasks, e.g., parasitic estimation. However, the
scarcity of design data, the unbalanced distribution of labels, and the
inherent diversity of circuit implementations pose significant challenges to
learning robust and transferable circuit representations. To address these
limitations, we propose CircuitGCL, a novel graph contrastive learning
framework that integrates representation scattering and label rebalancing to
enhance transferability across heterogeneous circuit graphs. CircuitGCL employs
a self-supervised strategy to learn topology-invariant node embeddings through
hyperspherical representation scattering, eliminating dependency on large-scale
data. Simultaneously, balanced mean squared error (MSE) and softmax
cross-entropy (bsmCE) losses are introduced to mitigate label distribution
disparities between circuits, enabling robust and transferable parasitic
estimation. Evaluated on parasitic capacitance estimation (edge-level task) and
ground capacitance classification (node-level task) across TSMC 28nm AMS
designs, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the
$R^2$ improvement of $33.64\% \sim 44.20\%$ for edge regression and F1-score
gain of $0.9\times \sim 2.1\times$ for node classification. Our code is
available at
\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.

</details>


### [86] [The Primacy of Magnitude in Low-Rank Adaptation](https://arxiv.org/abs/2507.06558)
*Zicheng Zhang,Haoran Li,Yifeng Zhang,Guoqiang Gong,Jiaxing Wang,Pengzhang Liu,Qixia Jiang,Junxing Hu*

Main category: cs.LG

TL;DR: LoRAM是一种基于更新幅度的初始化方案，优化了LoRA的性能，避免了谱方法的额外开销。


<details>
  <summary>Details</summary>
Motivation: 谱初始化方法虽然提升了LoRA的性能，但其计算和存储开销影响了效率。本文旨在通过更新幅度优化LoRA的初始化策略。

Method: 提出LoRAM方案，通过确定性的正交基和预训练权重幅度模拟谱增益，优化更新幅度。

Result: LoRAM在保持LoRA高效性的同时，性能与谱初始化相当或更优。

Conclusion: 更新幅度是LoRA性能的关键驱动因素，LoRAM提供了一种高效且性能优异的初始化方案。

Abstract: Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning
large models. While recent spectral initialization methods improve convergence
and performance over the naive "Noise & Zeros" scheme, their extra
computational and storage overhead undermines efficiency. In this paper, we
establish update magnitude as the fundamental driver of LoRA performance and
propose LoRAM, a magnitude-driven "Basis & Basis" initialization scheme that
matches spectral methods without their inefficiencies. Our key contributions
are threefold: (i) Magnitude of weight updates determines convergence. We prove
low-rank structures intrinsically bound update magnitudes, unifying
hyperparameter tuning in learning rate, scaling factor, and initialization as
mechanisms to optimize magnitude regulation. (ii) Spectral initialization
succeeds via magnitude amplification. We demystify that the presumed
knowledge-driven benefit of the spectral component essentially arises from the
boost in the weight update magnitude. (iii) A novel and compact initialization
strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight
magnitudes to simulate spectral gains. Extensive experiments show that LoRAM
serves as a strong baseline, retaining the full efficiency of LoRA while
matching or outperforming spectral initialization across benchmarks.

</details>


### [87] [Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction](https://arxiv.org/abs/2507.06538)
*Shan Shen,Yibin Zhang,Hector Rodriguez Rodriguez,Wenjian Yu*

Main category: cs.LG

TL;DR: CircuitGPS是一种少样本学习方法，用于预测AMS电路中的寄生效应，通过图表示学习和混合图Transformer提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 由于集成电路设计数据的稀缺性，训练深度学习模型用于AMS设计受到限制，因此需要一种高效的少样本学习方法。

Method: 将电路网表表示为异构图，通过小跳采样技术生成子图，使用混合图Transformer学习子图嵌入，并结合低成本位置编码。

Result: CircuitGPS将耦合电容预测的准确率提高至少20%，电容估计的MAE降低至少0.067。

Conclusion: CircuitGPS具有强扩展性，可直接应用于多样化的AMS电路设计，并通过消融研究为图表示学习提供了有价值的见解。

Abstract: Graph representation learning is a powerful method to extract features from
graph-structured data, such as analog/mixed-signal (AMS) circuits. However,
training deep learning models for AMS designs is severely limited by the
scarcity of integrated circuit design data. In this work, we present
CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS
circuits. The circuit netlist is represented as a heterogeneous graph, with the
coupling capacitance modeled as a link. CircuitGPS is pre-trained on link
prediction and fine-tuned on edge regression. The proposed method starts with a
small-hop sampling technique that converts a link or a node into a subgraph.
Then, the subgraph embeddings are learned with a hybrid graph Transformer.
Additionally, CircuitGPS integrates a low-cost positional encoding that
summarizes the positional and structural information of the sampled subgraph.
CircuitGPS improves the accuracy of coupling existence by at least 20\% and
reduces the MAE of capacitance estimation by at least 0.067 compared to
existing methods. Our method demonstrates strong inherent scalability, enabling
direct application to diverse AMS circuit designs through zero-shot learning.
Furthermore, the ablation studies provide valuable insights into graph models
for representation learning.

</details>


### [88] [From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization](https://arxiv.org/abs/2507.06573)
*Xinjie Chen,Minpeng Liao,Guoxin Chen,Chengxi Li,Biao Fu,Kai Fan,Xinggao Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为LPPO的框架，通过前缀引导采样和学习进度加权优化强化学习，利用少量高质量示范数据提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效利用少量高质量示范数据，而非单纯增加数据量，以提升大型语言模型的推理能力。

Method: 提出前缀引导采样（在线数据增强方法）和学习进度加权（动态调整样本影响的策略），通过LPPO框架实现渐进优化。

Result: 在数学推理基准测试中，LPPO表现优于基线方法，收敛更快且性能上限更高。

Conclusion: LPPO框架通过样本中心视角优化强化学习，显著提升了模型性能，为数据高效利用提供了新思路。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently advanced
the reasoning capabilities of large language models (LLMs). While prior work
has emphasized algorithmic design, data curation, and reward shaping, we
investigate RLVR from a sample-centric perspective and introduce LPPO
(Learning-Progress and Prefix-guided Optimization), a framework of progressive
optimization techniques. Our work addresses a critical question: how to best
leverage a small set of trusted, high-quality demonstrations, rather than
simply scaling up data volume. First, motivated by how hints aid human
problem-solving, we propose prefix-guided sampling, an online data augmentation
method that incorporates partial solution prefixes from expert demonstrations
to guide the policy, particularly for challenging instances. Second, inspired
by how humans focus on important questions aligned with their current
capabilities, we introduce learning-progress weighting, a dynamic strategy that
adjusts each training sample's influence based on model progression. We
estimate sample-level learning progress via an exponential moving average of
per-sample pass rates, promoting samples that foster learning and
de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks
demonstrate that our methods outperform strong baselines, yielding faster
convergence and a higher performance ceiling.

</details>


### [89] [A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning](https://arxiv.org/abs/2507.06542)
*Tongtian Zhu,Tianyu Zhang,Mingze Wang,Zhanpeng Zhou,Can Wang*

Main category: cs.LG

TL;DR: 研究去中心化学习中通信调度对性能的影响，发现后期集中通信预算可显著提升泛化能力，且最终全局合并可媲美中心化训练。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化学习中因有限通信导致的性能瓶颈问题。

Method: 通过实验和理论分析，研究通信调度策略，包括同步时机和频率。

Result: 后期集中通信和最终全局合并可匹配中心化训练性能，低通信下模型仍可合并。

Conclusion: 挑战了去中心化学习在数据异构和有限通信下泛化差的观念，为模型合并和损失景观提供新见解。

Abstract: Decentralized learning provides a scalable alternative to traditional
parameter-server-based training, yet its performance is often hindered by
limited peer-to-peer communication. In this paper, we study how communication
should be scheduled over time, including determining when and how frequently
devices synchronize. Our empirical results show that concentrating
communication budgets in the later stages of decentralized training markedly
improves global generalization. Surprisingly, we uncover that fully connected
communication at the final step, implemented by a single global merging, is
sufficient to match the performance of server-based training. We further show
that low communication in decentralized learning preserves the
\textit{mergeability} of local models throughout training. Our theoretical
contributions, which explains these phenomena, are first to establish that the
globally merged model of decentralized SGD can converge faster than centralized
mini-batch SGD. Technically, we novelly reinterpret part of the discrepancy
among local models, which were previously considered as detrimental noise, as
constructive components that accelerate convergence. This work challenges the
common belief that decentralized learning generalizes poorly under data
heterogeneity and limited communication, while offering new insights into model
merging and neural network loss landscapes.

</details>


### [90] [Learning controllable dynamics through informative exploration](https://arxiv.org/abs/2507.06582)
*Peter N. Loxley,Friedrich T. Sommer*

Main category: cs.LG

TL;DR: 论文提出了一种基于“预测信息增益”的方法，用于确定环境中信息量最大的探索区域，并通过强化学习找到有效的探索策略。


<details>
  <summary>Details</summary>
Motivation: 在无法获取明确模型的环境中，通过学习探索来估计可控动态。

Method: 使用“预测信息增益”作为信息度量，结合强化学习方法寻找探索策略。

Result: 该方法优于几种短视探索方法，能可靠估计环境的可控动态。

Conclusion: 基于信息增益的探索策略能有效学习环境动态，优于传统方法。

Abstract: Environments with controllable dynamics are usually understood in terms of
explicit models. However, such models are not always available, but may
sometimes be learned by exploring an environment. In this work, we investigate
using an information measure called "predicted information gain" to determine
the most informative regions of an environment to explore next. Applying
methods from reinforcement learning allows good suboptimal exploring policies
to be found, and leads to reliable estimates of the underlying controllable
dynamics. This approach is demonstrated by comparing with several myopic
exploration approaches.

</details>


### [91] [Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs](https://arxiv.org/abs/2507.06549)
*Shan Shen,Dingcheng Yang,Yuyang Xie,Chunyan Pei,Wenjian Yu,Bei Yu*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的2阶段模型，用于在预布局阶段准确预测SRAM电路中的寄生效应，显著提升了仿真效率和准确性。


<details>
  <summary>Details</summary>
Motivation: SRAM在SoC中的定制化设计常因寄生效应导致预布局与后布局仿真结果差异大，设计迭代频繁，亟需一种能在预布局阶段准确预测寄生效应的方法。

Method: 结合图神经网络（GNN）分类器和多层感知机（MLP）回归器，采用Focal Loss处理类别不平衡，并引入子电路信息抽象层次结构。

Result: 在4个实际SRAM设计中，模型误差最大降低19倍，仿真速度提升高达598倍。

Conclusion: 该方法显著提升了寄生效应预测的准确性和仿真效率，为SRAM设计优化提供了有力工具。

Abstract: To achieve higher system energy efficiency, SRAM in SoCs is often customized.
The parasitic effects cause notable discrepancies between pre-layout and
post-layout circuit simulations, leading to difficulty in converging design
parameters and excessive design iterations. Is it possible to well predict the
parasitics based on the pre-layout circuit, so as to perform parasitic-aware
pre-layout simulation? In this work, we propose a deep-learning-based 2-stage
model to accurately predict these parasitics in pre-layout stages. The model
combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron
(MLP) regressors, effectively managing class imbalance of the net parasitics in
SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant
internal net samples and integrate subcircuit information into the graph to
abstract the hierarchical structure of schematics. Experiments on 4 real SRAM
designs show that our approach not only surpasses the state-of-the-art model in
parasitic prediction by a maximum of 19X reduction of error but also
significantly boosts the simulation process by up to 598X speedup.

</details>


### [92] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal,Yuhta Takida,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成模型框架，通过结合不同β值的VAE和非线性扩散模型，平衡解耦性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型中解耦性和重建质量之间的权衡问题。

Method: 训练单一VAE，结合新损失函数控制潜在表示的信息保留，并引入非线性扩散模型平滑过渡不同β值的表示。

Result: 实现了高质量的解耦和生成，支持无输入图像的样本生成，并在潜在空间中观察到平滑过渡。

Conclusion: 该框架在解耦性和生成质量上表现优异，支持灵活的潜在表示操作。

Abstract: Disentangled and interpretable latent representations in generative models
typically come at the cost of generation quality. The $\beta$-VAE framework
introduces a hyperparameter $\beta$ to balance disentanglement and
reconstruction quality, where setting $\beta > 1$ introduces an information
bottleneck that favors disentanglement over sharp, accurate reconstructions. To
address this trade-off, we propose a novel generative modeling framework that
leverages a range of $\beta$ values to learn multiple corresponding latent
representations. First, we obtain a slew of representations by training a
single variational autoencoder (VAE), with a new loss function that controls
the information retained in each latent representation such that the higher
$\beta$ value prioritize disentanglement over reconstruction fidelity. We then,
introduce a non-linear diffusion model that smoothly transitions latent
representations corresponding to different $\beta$ values. This model denoises
towards less disentangled and more informative representations, ultimately
leading to (almost) lossless representations, enabling sharp reconstructions.
Furthermore, our model supports sample generation without input images,
functioning as a standalone generative model. We evaluate our framework in
terms of both disentanglement and generation quality. Additionally, we observe
smooth transitions in the latent spaces with respect to changes in $\beta$,
facilitating consistent manipulation of generated outputs.

</details>


### [93] [Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance](https://arxiv.org/abs/2507.06615)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: 提出了一种名为CTPG的新框架，通过跨任务策略指导加速多任务强化学习，利用已掌握任务的策略指导未掌握任务，并引入两种门控机制提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注参数共享，但忽略了利用已掌握任务的策略直接指导未掌握任务，从而加速技能获取。

Method: 提出CTPG框架，为每个任务训练一个指导策略，从所有任务的控制策略中选择行为策略，并引入两种门控机制优化学习效率。

Result: 实验表明，CTPG与现有参数共享方法结合后，在操作和运动基准测试中性能显著提升。

Conclusion: CTPG是一种通用框架，能有效利用跨任务相似性，提升多任务强化学习性能。

Abstract: Multi-task reinforcement learning endeavors to efficiently leverage shared
information across various tasks, facilitating the simultaneous learning of
multiple tasks. Existing approaches primarily focus on parameter sharing with
carefully designed network structures or tailored optimization procedures.
However, they overlook a direct and complementary way to exploit cross-task
similarities: the control policies of tasks already proficient in some skills
can provide explicit guidance for unmastered tasks to accelerate skills
acquisition. To this end, we present a novel framework called Cross-Task Policy
Guidance (CTPG), which trains a guide policy for each task to select the
behavior policy interacting with the environment from all tasks' control
policies, generating better training trajectories. In addition, we propose two
gating mechanisms to improve the learning efficiency of CTPG: one gate filters
out control policies that are not beneficial for guidance, while the other gate
blocks tasks that do not necessitate guidance. CTPG is a general framework
adaptable to existing parameter sharing approaches. Empirical evaluations
demonstrate that incorporating CTPG with these approaches significantly
enhances performance in manipulation and locomotion benchmarks.

</details>


### [94] [SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference](https://arxiv.org/abs/2507.06567)
*Qian Chen,Xianhao Chen,Kaibin Huang*

Main category: cs.LG

TL;DR: 该论文提出了一种分布式专家缓存优化方法，以减少边缘设备上Mixture-of-Experts (MoE)模型的推理延迟。


<details>
  <summary>Details</summary>
Motivation: MoE模型通过激活少量专家网络来提升大型语言模型的可扩展性，但大量专家网络在边缘设备上存储负担重，需优化分布式推理。

Method: 基于Top-$K$专家选择策略，设计了贪心算法和动态规划方法，优化专家缓存以减少延迟。

Result: 仿真结果表明，该方法显著降低了推理延迟。

Conclusion: 提出的方法在边缘网络中有效优化了MoE模型的推理性能。

Abstract: Mixture-of-Experts (MoE) models improve the scalability of large language
models (LLMs) by activating only a small subset of relevant experts per input.
However, the sheer number of expert networks in an MoE model introduces a
significant storage burden for an edge device. To address this challenge, we
consider a scenario where experts are dispersed within an edge network for
distributed inference. Based on the popular Top-$K$ expert selection strategy,
we formulate a latency minimization problem by optimizing expert caching on
edge servers under storage constraints. When $K=1$, the problem reduces to a
monotone submodular maximization problem with knapsack constraints, for which
we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.
For the general case where $K\geq1$, expert co-activation within the same MoE
layer introduces non-submodularity, causing greedy methods to be ineffective.
To tackle this issue, we propose a successive greedy decomposition method to
decompose the original problem into a series of subproblems, with each being
solved by a dynamic programming approach. Furthermore, we design an accelerated
algorithm based on the max-convolution technique to obtain the approximate
solution with a provable guarantee in polynomial time. Simulation results on
various MoE models demonstrate that our method significantly reduces inference
latency compared to existing baselines.

</details>


### [95] [Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2507.06628)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: GO-Skill是一种离线多任务强化学习方法，通过目标导向的技能提取和离散技能库构建，提升知识共享和任务性能。


<details>
  <summary>Details</summary>
Motivation: 离线多任务强化学习面临跨任务知识共享的挑战，受人类学习启发，提出GO-Skill以提取和利用可重用技能。

Method: 通过目标导向技能提取和向量量化构建离散技能库，引入技能增强阶段优化技能，并采用分层策略学习动态协调技能。

Result: 在MetaWorld基准测试中，GO-Skill在多样化机器人操作任务中表现出高效性和通用性。

Conclusion: GO-Skill通过技能抽象和分层策略学习，有效解决了离线多任务强化学习中的知识共享问题。

Abstract: Offline multi-task reinforcement learning aims to learn a unified policy
capable of solving multiple tasks using only pre-collected task-mixed datasets,
without requiring any online interaction with the environment. However, it
faces significant challenges in effectively sharing knowledge across tasks.
Inspired by the efficient knowledge abstraction observed in human learning, we
propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed
to extract and utilize reusable skills to enhance knowledge transfer and task
performance. Our approach uncovers reusable skills through a goal-oriented
skill extraction process and leverages vector quantization to construct a
discrete skill library. To mitigate class imbalances between broadly applicable
and task-specific skills, we introduce a skill enhancement phase to refine the
extracted skills. Furthermore, we integrate these skills using hierarchical
policy learning, enabling the construction of a high-level policy that
dynamically orchestrates discrete skills to accomplish specific tasks.
Extensive experiments on diverse robotic manipulation tasks within the
MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.

</details>


### [96] [Deep Disentangled Representation Network for Treatment Effect Estimation](https://arxiv.org/abs/2507.06650)
*Hui Meng,Keping Yang,Xuyu Peng,Bo Zheng*

Main category: cs.LG

TL;DR: 提出一种新算法，通过多头注意力机制和线性正交正则化分解预处理变量，并结合重要性采样消除选择偏差，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 观测数据中个体层面治疗效果估计是因果推断的核心问题，但现有方法难以精确分解变量。

Method: 结合多头注意力机制和线性正交正则化软分解预处理变量，并通过重要性采样消除选择偏差。

Result: 在公开半合成和真实数据集上实验，算法性能优于现有方法。

Conclusion: 新算法在个体治疗效果估计中表现优异，解决了现有方法的局限性。

Abstract: Estimating individual-level treatment effect from observational data is a
fundamental problem in causal inference and has attracted increasing attention
in the fields of education, healthcare, and public policy.In this work, we
concentrate on the study of disentangled representation methods that have shown
promising outcomes by decomposing observed covariates into instrumental,
confounding, and adjustment factors. However, most of the previous work has
primarily revolved around generative models or hard decomposition methods for
covariates, which often struggle to guarantee the attainment of precisely
disentangled factors. In order to effectively model different causal
relationships, we propose a novel treatment effect estimation algorithm that
incorporates a mixture of experts with multi-head attention and a linear
orthogonal regularizer to softly decompose the pre-treatment variables, and
simultaneously eliminates selection bias via importance sampling re-weighting
techniques. We conduct extensive experiments on both public semi-synthetic and
real-world production datasets. The experimental results clearly demonstrate
that our algorithm outperforms the state-of-the-art methods focused on
individual treatment effects.

</details>


### [97] [Generalization in Reinforcement Learning for Radio Access Networks](https://arxiv.org/abs/2507.06602)
*Burak Demirel,Yu Wang,Cristian Tatino,Pablo Soldati*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的通用框架，用于解决现代无线接入网络中的资源管理问题，通过图注意力网络和域随机化提高泛化能力，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现代无线接入网络环境动态且异构，传统基于规则的资源管理算法表现不佳，而现有强化学习方法容易过拟合训练条件，难以泛化到新场景。

Method: 采用注意力机制编码网络拓扑和节点属性，结合域随机化扩展训练分布，并通过分布式数据生成和集中训练架构提升泛化能力。

Result: 在多个5G基准测试中，该框架在吞吐量和频谱效率上比基线方法提升10%-20%，并在多场景下表现优于专用强化学习模型。

Conclusion: 该框架为AI原生的6G无线接入网络提供了一种可扩展且泛化能力强的解决方案。

Abstract: Modern RAN operate in highly dynamic and heterogeneous environments, where
hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass
such heuristics in constrained settings, the diversity of deployments and
unpredictable radio conditions introduce major generalization challenges.
Data-driven policies frequently overfit to training conditions, degrading
performance in unseen scenarios. To address this, we propose a
generalization-centered RL framework for RAN control that: (i) encodes cell
topology and node attributes via attention-based graph representations; (ii)
applies domain randomization to broaden the training distribution; and (iii)
distributes data generation across multiple actors while centralizing training
in a cloud-compatible architecture aligned with O-RAN principles. Although
generalization increases computational and data-management complexity, our
distributed design mitigates this by scaling data collection and training
across diverse network conditions. Applied to downlink link adaptation in five
5G benchmarks, our policy improves average throughput and spectral efficiency
by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and
by >20% under high mobility. It matches specialized RL in full-buffer traffic
and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,
respectively. In nine-cell deployments, GAT models offer 30% higher throughput
over MLP baselines. These results, combined with our scalable architecture,
offer a path toward AI-native 6G RAN using a single, generalizable RL agent.

</details>


### [98] [Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000](https://arxiv.org/abs/2507.06619)
*Xiaobo Huang,Fang Xie*

Main category: cs.LG

TL;DR: 提出SAD-DPSGD方法，通过线性衰减机制优化噪声和裁剪阈值，解决医疗图像分类中的梯度泄漏问题，性能优于Auto-DPSGD。


<details>
  <summary>Details</summary>
Motivation: 医疗图像数据不平衡导致传统差分隐私方法失效，梯度信息丢失，模型陷入次优解。

Method: SAD-DPSGD采用线性衰减机制，初始阶段分配更多隐私预算和更高裁剪阈值。

Result: 在HAM10000数据集上，SAD-DPSGD比Auto-DPSGD准确率提高2.15%（ε=3.0，δ=10^-3）。

Conclusion: SAD-DPSGD有效解决了医疗图像分类中的数据泄漏问题，提升了模型性能。

Abstract: When applying machine learning to medical image classification, data leakage
is a critical issue. Previous methods, such as adding noise to gradients for
differential privacy, work well on large datasets like MNIST and CIFAR-100, but
fail on small, imbalanced medical datasets like HAM10000. This is because the
imbalanced distribution causes gradients from minority classes to be clipped
and lose crucial information, while majority classes dominate. This leads the
model to fall into suboptimal solutions early. To address this, we propose
SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping
thresholds. By allocating more privacy budget and using higher clipping
thresholds in the initial training phases, the model avoids suboptimal
solutions and enhances performance. Experiments show that SAD-DPSGD outperforms
Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ ,
$\delta = 10^{-3}$.

</details>


### [99] [UniOD: A Universal Model for Outlier Detection across Diverse Domains](https://arxiv.org/abs/2507.06624)
*Dazhi Fu,Jicong Fan*

Main category: cs.LG

TL;DR: UniOD是一个通用的异常检测框架，通过利用标记数据集训练单一模型，能够检测多样领域的异常数据，避免了繁琐的超参数调优和模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法需要针对每个数据集进行超参数调优和模型训练，过程繁琐且成本高。UniOD旨在解决这一问题，提高异常检测的便利性和准确性。

Method: UniOD将每个数据集转化为多个图，生成一致的节点特征，并将异常检测任务转化为节点分类问题，从而泛化到未见过的领域。

Result: 在15个基准异常检测数据集上，UniOD优于15个现有先进方法，证明了其有效性。

Conclusion: UniOD通过避免模型选择和超参数调优，降低了计算成本，并有效利用历史数据集的知识，提升了实际应用的便利性和准确性。

Abstract: Outlier detection (OD) seeks to distinguish inliers and outliers in
completely unlabeled datasets and plays a vital role in science and
engineering. Most existing OD methods require troublesome dataset-specific
hyperparameter tuning and costly model training before they can be deployed to
identify outliers. In this work, we propose UniOD, a universal OD framework
that leverages labeled datasets to train a single model capable of detecting
outliers of datasets from diverse domains. Specifically, UniOD converts each
dataset into multiple graphs, produces consistent node features, and frames
outlier detection as a node-classification task, and is able to generalize to
unseen domains. As a result, UniOD avoids effort on model selection and
hyperparameter tuning, reduces computational cost, and effectively utilizes the
knowledge from historical datasets, which improves the convenience and accuracy
in real applications. We evaluate UniOD on 15 benchmark OD datasets against 15
state-of-the-art baselines, demonstrating its effectiveness.

</details>


### [100] [Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator](https://arxiv.org/abs/2507.06631)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 提出了一种基于网格数据结构的过拟合检测与预防方法，通过拉普拉斯算子二阶导数计算熵损失，优化超参数以减少振荡。


<details>
  <summary>Details</summary>
Motivation: 解决回归任务中数据过拟合问题，特别是在网格数据结构中，通过熵损失优化模型性能。

Method: 在原始训练网格上计算导数作为真实标签，在交错网格上计算训练数据的导数以检测振荡，利用拉普拉斯算子导数损失进行超参数优化。

Result: 通过最小化熵损失，减少了模型的不必要振荡，且无需从训练数据中分离测试点。

Conclusion: 该方法有效减少了过拟合，并通过交错网格上的拉普拉斯算子作为替代测试指标，验证了模型的扩散特性。

Abstract: This document reports on a method for detecting and preventing overfitting on
data regressions, herein applied to mesh-like data structures. The mesh
structure allows for the straightforward computation of the Laplace-operator
second-order derivatives in a finite-difference fashion for noiseless data.
Derivatives of the training data are computed on the original training mesh to
serve as a true label of the entropy of the training data. Derivatives of the
trained data are computed on a staggered mesh to identify oscillations in the
interior of the original training mesh cells. The loss of the Laplace-operator
derivatives is used for hyperparameter optimisation, achieving a reduction of
unwanted oscillation through the minimisation of the entropy of the trained
model. In this setup, testing does not require the splitting of points from the
training data, and training is thus directly performed on all available
training points. The Laplace operator applied to the trained data on a
staggered mesh serves as a surrogate testing metric based on diffusion
properties.

</details>


### [101] [Intrinsic Training Signals for Federated Learning Aggregation](https://arxiv.org/abs/2507.06813)
*Cosimo Fiorini,Matteo Mosconi,Pietro Buzzega,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: LIVAR是一种无需架构修改或损失函数变化的联邦学习方法，通过利用现有训练信号实现高效模型聚合。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在聚合客户端特定分类头和适应主干参数时需要修改架构或损失函数，而LIVAR旨在利用标准优化中已有的训练信号实现高效聚合。

Method: LIVAR提出：1）基于自然涌现特征统计的方差加权分类器聚合方案；2）基于SHAP分析的LoRA合并技术。

Result: LIVAR在多个基准测试中达到最先进性能，且无需额外架构开销。

Conclusion: LIVAR证明仅通过现有训练信号即可实现高效模型聚合，为联邦学习提供了新范式。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. While existing approaches
for aggregating client-specific classification heads and adapted backbone
parameters require architectural modifications or loss function changes, our
method uniquely leverages intrinsic training signals already available during
standard optimization. We present LIVAR (Layer Importance and VARiance-based
merging), which introduces: i) a variance-weighted classifier aggregation
scheme using naturally emergent feature statistics, and ii) an
explainability-driven LoRA merging technique based on SHAP analysis of existing
update parameter patterns. Without any architectural overhead, LIVAR achieves
state-of-the-art performance on multiple benchmarks while maintaining seamless
integration with existing FL methods. This work demonstrates that effective
model merging can be achieved solely through existing training signals,
establishing a new paradigm for efficient federated model aggregation. The code
will be made publicly available upon acceptance.

</details>


### [102] [Comprehensive Evaluation of Prototype Neural Networks](https://arxiv.org/abs/2507.06819)
*Philipp Schlinge,Steffen Meinert,Martin Atzmueller*

Main category: cs.LG

TL;DR: 本文对原型模型（如ProtoPNet、ProtoPool和PIPNet）进行了深入分析，提出了一套全面的评估指标，包括新提出的指标，以补充模型可解释性分析。实验在多种数据集上进行，并开源了代码库。


<details>
  <summary>Details</summary>
Motivation: 原型模型是可解释人工智能（XAI）的重要方法，本文旨在通过全面评估指标深入分析其性能。

Method: 应用标准和新提出的指标评估原型模型，实验涵盖多种数据集（细粒度分类、非独立同分布设置和多标签分类）。

Result: 通过实验对比了原型模型在不同数据集上的性能，并开源了代码库以支持进一步研究。

Conclusion: 本文为原型模型的评估提供了全面的指标和工具，促进了可解释性研究的进一步发展。

Abstract: Prototype models are an important method for explainable artificial
intelligence (XAI) and interpretable machine learning. In this paper, we
perform an in-depth analysis of a set of prominent prototype models including
ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive
set of metrics. In addition to applying standard metrics from literature, we
propose several new metrics to further complement the analysis of model
interpretability. In our experimentation, we apply the set of prototype models
on a diverse set of datasets including fine-grained classification, Non-IID
settings and multi-label classification to further contrast the performance.
Furthermore, we also provide our code as an open-source library, which
facilitates simple application of the metrics itself, as well as extensibility
- providing the option for easily adding new metrics and models.
https://github.com/uos-sis/quanproto

</details>


### [103] [Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making](https://arxiv.org/abs/2507.06652)
*Arthur Alexander Lim,Zhen Bin It,Jovan Bowen Heng,Tee Hui Teo*

Main category: cs.LG

TL;DR: 本文探讨如何通过机器学习和联邦学习改进模糊系统，以处理不确定性，并讨论其潜在改进与局限性。


<details>
  <summary>Details</summary>
Motivation: 模糊系统能处理不确定性，但仍有改进空间，尤其是结合机器学习和联邦学习的新技术。

Method: 提出将机器学习和联邦学习的理念（如更新模糊规则）应用于模糊系统。

Result: 潜在改进包括提升模糊系统的性能和效率，但仍需进一步研究验证。

Conclusion: 这些改进方案具有潜力，但需更多研究以确定其实际效果。

Abstract: Fuzzy systems are a way to allow machines, systems and frameworks to deal
with uncertainty, which is not possible in binary systems that most computers
use. These systems have already been deployed for certain use cases, and fuzzy
systems could be further improved as proposed in this paper. Such technologies
to draw inspiration from include machine learning and federated learning.
Machine learning is one of the recent breakthroughs of technology and could be
applied to fuzzy systems to further improve the results it produces. Federated
learning is also one of the recent technologies that have huge potential, which
allows machine learning training to improve by reducing privacy risk, reducing
burden on networking infrastructure, and reducing latency of the latest model.
Aspects from federated learning could be used to improve federated learning,
such as applying the idea of updating the fuzzy rules that make up a key part
of fuzzy systems, to further improve it over time. This paper discusses how
these improvements would be implemented in fuzzy systems, and how it would
improve fuzzy systems. It also discusses certain limitations on the potential
improvements. It concludes that these proposed ideas and improvements require
further investigation to see how far the improvements are, but the potential is
there to improve fuzzy systems.

</details>


### [104] [HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning](https://arxiv.org/abs/2507.06821)
*Chuhang Zheng,Chunwei Tian,Jie Wen,Daoqiang Zhang,Qi Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种名为HeLo的多模态情感分布学习框架，旨在挖掘多模态数据的异质性和互补信息，以及混合基本情感中的标签相关性。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别在人机交互中具有重要意义，但现有方法在挖掘多模态异质性和利用情感标签相关性方面存在不足。

Method: 采用交叉注意力融合生理数据，设计基于最优传输的异质性挖掘模块，引入可学习的标签嵌入和相关性矩阵对齐，并通过标签相关性驱动的交叉注意力机制整合多模态表示。

Result: 在两个公开数据集上的实验结果表明，所提方法在情感分布学习方面表现优越。

Conclusion: HeLo框架有效解决了多模态情感分布学习中的异质性挖掘和标签相关性利用问题，具有显著性能优势。

Abstract: Multi-modal emotion recognition has garnered increasing attention as it plays
a significant role in human-computer interaction (HCI) in recent years. Since
different discrete emotions may exist at the same time, compared with
single-class emotion recognition, emotion distribution learning (EDL) that
identifies a mixture of basic emotions has gradually emerged as a trend.
However, existing EDL methods face challenges in mining the heterogeneity among
multiple modalities. Besides, rich semantic correlations across arbitrary basic
emotions are not fully exploited. In this paper, we propose a multi-modal
emotion distribution learning framework, named HeLo, aimed at fully exploring
the heterogeneity and complementary information in multi-modal emotional data
and label correlation within mixed basic emotions. Specifically, we first adopt
cross-attention to effectively fuse the physiological data. Then, an optimal
transport (OT)-based heterogeneity mining module is devised to mine the
interaction and heterogeneity between the physiological and behavioral
representations. To facilitate label correlation learning, we introduce a
learnable label embedding optimized by correlation matrix alignment. Finally,
the learnable label embeddings and label correlation matrices are integrated
with the multi-modal representations through a novel label correlation-driven
cross-attention mechanism for accurate emotion distribution learning.
Experimental results on two publicly available datasets demonstrate the
superiority of our proposed method in emotion distribution learning.

</details>


### [105] [Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study](https://arxiv.org/abs/2507.06694)
*Raffael Theiler,Olga Fink*

Main category: cs.LG

TL;DR: 论文提出了一种基于异构图注意力网络的电力系统状态预测方法，用于解决多物理域传感器数据融合问题，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统因可再生能源和分布式能源的引入而变得复杂，需要可靠的短期状态预测以确保稳定运行。传统图神经网络方法难以处理多物理域异构数据。

Method: 采用异构图注意力网络（Heterogeneous Graph Attention Networks）建模传感器数据中的同质和异质关系，特别针对水力与电力两个物理域。

Result: 实验表明，该方法在归一化均方根误差上平均优于传统基线35.5%。

Conclusion: 该方法有效解决了多物理域电力系统状态预测问题，为复杂能源系统的稳定运行提供了新工具。

Abstract: Accurate short-term state forecasting is essential for efficient and stable
operation of modern power systems, especially in the context of increasing
variability introduced by renewable and distributed energy resources. As these
systems evolve rapidly, it becomes increasingly important to reliably predict
their states in the short term to ensure operational stability, support control
decisions, and enable interpretable monitoring of sensor and machine behavior.
Modern power systems often span multiple physical domains - including
electrical, mechanical, hydraulic, and thermal - posing significant challenges
for modeling and prediction. Graph Neural Networks (GNNs) have emerged as a
promising data-driven framework for system state estimation and state
forecasting in such settings. By leveraging the topological structure of sensor
networks, GNNs can implicitly learn inter-sensor relationships and propagate
information across the network. However, most existing GNN-based methods are
designed under the assumption of homogeneous sensor relationships and are
typically constrained to a single physical domain. This limitation restricts
their ability to integrate and reason over heterogeneous sensor data commonly
encountered in real-world energy systems, such as those used in energy
conversion infrastructure. In this work, we propose the use of Heterogeneous
Graph Attention Networks to address these limitations. Our approach models both
homogeneous intra-domain and heterogeneous inter-domain relationships among
sensor data from two distinct physical domains - hydraulic and electrical -
which exhibit fundamentally different temporal dynamics. Experimental results
demonstrate that our method significantly outperforms conventional baselines on
average by 35.5% in terms of normalized root mean square error, confirming its
effectiveness in multi-domain, multi-rate power system state forecasting.

</details>


### [106] [Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning](https://arxiv.org/abs/2507.06825)
*Matej Straka,Martin Schmid*

Main category: cs.LG

TL;DR: 介绍了一个基于Generals.io的实时策略游戏环境，兼容Gymnasium和PettingZoo，支持高性能运行，并提出了一个通过监督预训练和自对弈训练的顶级智能体。


<details>
  <summary>Details</summary>
Motivation: 为多智能体强化学习研究提供一个易于访问且具有挑战性的平台。

Method: 使用监督预训练和自对弈训练智能体，结合基于潜在奖励塑造和记忆特征加速学习。

Result: 智能体在36小时内达到1v1人类排行榜前0.003%，运行性能高达每秒数千帧。

Conclusion: 该环境为多智能体强化学习研究提供了模块化基准和竞争性基线智能体。

Abstract: We introduce a real-time strategy game environment built on Generals.io, a
game that hosts thousands of active players each week across multiple game
formats. Our environment is fully compatible with Gymnasium and PettingZoo,
capable of running thousands of frames per second on commodity hardware. Our
reference agent -- trained with supervised pre-training and self-play -- hits
the top 0.003\% of the 1v1 human leaderboard after just 36 hours on a single
H100 GPU. To accelerate learning, we incorporate potential-based reward shaping
and memory features. Our contributions -- a modular RTS benchmark and a
competitive, state-of-the-art baseline agent -- provide an accessible yet
challenging platform for advancing multi-agent reinforcement learning research.

</details>


### [107] [Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement](https://arxiv.org/abs/2507.06701)
*Michael Bloesch,Markus Wulfmeier,Philemon Brakel,Todor Davchev,Martina Zambelli,Jost Tobias Springenberg,Abbas Abdolmaleki,William F Whitney,Nicolas Heess,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 该论文研究了模仿学习中的观察学习（IfO），提出了一种从动作无标注演示中学习的方法，并通过价值函数在专家与非专家数据间传递信息。


<details>
  <summary>Details</summary>
Motivation: 现有IfO研究多关注理想化的双峰数据分布，限制了结果的实用性。本文旨在探索更复杂的数据分布，并提出一种方法以实现迭代式自我改进的模仿学习。

Method: 采用基于强化学习的模仿学习方法，利用价值函数在动作无标注的演示数据中传递信息。

Result: 通过全面评估，揭示了不同数据分布与算法适用性的关系，并指出了现有方法的局限性。

Conclusion: 研究结果为开发更稳健、实用的IfO技术提供了有价值的见解，为实现可扩展的行为学习铺平了道路。

Abstract: Imitation Learning from Observation (IfO) offers a powerful way to learn
behaviors at large-scale: Unlike behavior cloning or offline reinforcement
learning, IfO can leverage action-free demonstrations and thus circumvents the
need for costly action-labeled demonstrations or reward functions. However,
current IfO research focuses on idealized scenarios with mostly bimodal-quality
data distributions, restricting the meaningfulness of the results. In contrast,
this paper investigates more nuanced distributions and introduces a method to
learn from such data, moving closer to a paradigm in which imitation learning
can be performed iteratively via self-improvement. Our method adapts RL-based
imitation learning to action-free demonstrations, using a value function to
transfer information between expert and non-expert data. Through comprehensive
evaluation, we delineate the relation between different data distributions and
the applicability of algorithms and highlight the limitations of established
methods. Our findings provide valuable insights for developing more robust and
practical IfO techniques on a path to scalable behaviour learning.

</details>


### [108] [DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models](https://arxiv.org/abs/2507.06853)
*Liang Wang,Yu Rong,Tingyang Xu,Zhenyi Zhong,Zhiyuan Liu,Pengju Wang,Deli Zhao,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.LG

TL;DR: DiffSpectra是一种基于扩散模型的生成框架，直接从多模态光谱数据推断2D和3D分子结构，解决了传统方法和现有机器学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 分子结构解析是化学中的基础问题，传统方法依赖专家解读且缺乏扩展性，现有机器学习方法受限于有限库和忽略3D几何信息。

Method: DiffSpectra采用扩散模型，结合SE(3)-等变架构的Diffusion Molecule Transformer和基于Transformer的光谱编码器SpecFormer，实现多模态光谱数据的条件生成。

Result: 实验表明，DiffSpectra在结构解析中表现出高准确性，top-1准确率为16.01%，top-20准确率为96.86%。

Conclusion: DiffSpectra首次统一了多模态光谱推理和2D/3D生成建模，为解决分子结构解析问题提供了有效方案。

Abstract: Molecular structure elucidation from spectra is a foundational problem in
chemistry, with profound implications for compound identification, synthesis,
and drug development. Traditional methods rely heavily on expert interpretation
and lack scalability. Pioneering machine learning methods have introduced
retrieval-based strategies, but their reliance on finite libraries limits
generalization to novel molecules. Generative models offer a promising
alternative, yet most adopt autoregressive SMILES-based architectures that
overlook 3D geometry and struggle to integrate diverse spectral modalities. In
this work, we present DiffSpectra, a generative framework that directly infers
both 2D and 3D molecular structures from multi-modal spectral data using
diffusion models. DiffSpectra formulates structure elucidation as a conditional
generation process. Its denoising network is parameterized by Diffusion
Molecule Transformer, an SE(3)-equivariant architecture that integrates
topological and geometric information. Conditioning is provided by SpecFormer,
a transformer-based spectral encoder that captures intra- and inter-spectral
dependencies from multi-modal spectra. Extensive experiments demonstrate that
DiffSpectra achieves high accuracy in structure elucidation, recovering exact
structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through
sampling. The model benefits significantly from 3D geometric modeling,
SpecFormer pre-training, and multi-modal conditioning. These results highlight
the effectiveness of spectrum-conditioned diffusion modeling in addressing the
challenge of molecular structure elucidation. To our knowledge, DiffSpectra is
the first framework to unify multi-modal spectral reasoning and joint 2D/3D
generative modeling for de novo molecular structure elucidation.

</details>


### [109] [PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems](https://arxiv.org/abs/2507.06712)
*Ayoub Farkane,Mohamed Boutayeb,Mustapha Oudani,Mounir Ghogho*

Main category: cs.LG

TL;DR: 本文提出了一种基于自适应物理信息神经网络的观测器（PINN-Obs），用于非线性系统的精确状态估计，无需显式系统变换或线性化。


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统的状态估计在控制和工程应用中具有挑战性，尤其是在部分和噪声测量可用的情况下。

Method: 通过将系统动态和传感器数据直接集成到物理信息学习过程中，自适应学习最优增益矩阵，确保估计状态收敛到真实系统状态。

Result: 理论分析证明了在温和可观测性条件下的统一误差最小化，数值模拟验证了其在多种非线性系统中的有效性。

Conclusion: PINN-Obs在准确性、鲁棒性和适应性方面优于现有观测器设计。

Abstract: State estimation for nonlinear dynamical systems is a critical challenge in
control and engineering applications, particularly when only partial and noisy
measurements are available. This paper introduces a novel Adaptive
Physics-Informed Neural Network-based Observer (PINN-Obs) for accurate state
estimation in nonlinear systems. Unlike traditional model-based observers,
which require explicit system transformations or linearization, the proposed
framework directly integrates system dynamics and sensor data into a
physics-informed learning process. The observer adaptively learns an optimal
gain matrix, ensuring convergence of the estimated states to the true system
states. A rigorous theoretical analysis establishes formal convergence
guarantees, demonstrating that the proposed approach achieves uniform error
minimization under mild observability conditions. The effectiveness of PINN-Obs
is validated through extensive numerical simulations on diverse nonlinear
systems, including an induction motor model, a satellite motion system, and
benchmark academic examples. Comparative experimental studies against existing
observer designs highlight its superior accuracy, robustness, and adaptability.

</details>


### [110] [Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model](https://arxiv.org/abs/2507.06892)
*Jing Liang,Hongyao Tang,Yi Ma,Jinyi Liu,Yan Zheng,Shuyue Hu,Lei Bai,Jianye Hao*

Main category: cs.LG

TL;DR: 论文提出ReMix方法，通过混合策略和策略重生技术，改进现有强化学习微调方法，显著降低训练成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习微调方法（如PPO、GRPO）主要依赖在线策略学习，导致计算和时间成本高昂，限制了经济高效的扩展。

Method: ReMix包含三个主要组件：混合策略近端策略梯度、KL凸策略约束和策略重生技术，以高效利用离线数据并平衡稳定性与灵活性。

Result: 实验显示，ReMix在多个数学推理基准上表现优异，训练成本降低30至450倍，性能达到SOTA水平。

Conclusion: ReMix为强化学习微调提供了一种高效、低成本的方法，并揭示了离线策略学习中的新发现。

Abstract: Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.

</details>


### [111] [Mathematical artificial data for operator learning](https://arxiv.org/abs/2507.06752)
*Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: MAD框架结合物理定律与数据驱动学习，通过生成物理嵌入的解析解和合成数据，解决了传统方法对标记数据或效率-精度权衡的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统方法在解决微分方程时面临标记数据成本高或效率-精度权衡的问题，MAD框架旨在克服这些限制。

Method: MAD框架利用微分方程的数学结构生成物理嵌入的解析解和合成数据，实现无需实验或模拟数据的算子学习。

Result: 在2D参数化问题中，MAD展示了其通用性和高效性/准确性，适用于多种微分方程场景。

Conclusion: MAD框架有望成为科学计算中物理信息机器智能的通用范式。

Abstract: Machine learning has emerged as a transformative tool for solving
differential equations (DEs), yet prevailing methodologies remain constrained
by dual limitations: data-driven methods demand costly labeled datasets while
model-driven techniques face efficiency-accuracy trade-offs. We present the
Mathematical Artificial Data (MAD) framework, a new paradigm that integrates
physical laws with data-driven learning to facilitate large-scale operator
discovery. By exploiting DEs' intrinsic mathematical structure to generate
physics-embedded analytical solutions and associated synthetic data, MAD
fundamentally eliminates dependence on experimental or simulated training data.
This enables computationally efficient operator learning across multi-parameter
systems while maintaining mathematical rigor. Through numerical demonstrations
spanning 2D parametric problems where both the boundary values and source term
are functions, we showcase MAD's generalizability and superior
efficiency/accuracy across various DE scenarios. This
physics-embedded-data-driven framework and its capacity to handle complex
parameter spaces gives it the potential to become a universal paradigm for
physics-informed machine intelligence in scientific computing.

</details>


### [112] [Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric](https://arxiv.org/abs/2507.06765)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 提出了一种参数化激活函数（Leaky Exponential Linear Unit）用于多维非线性数据回归，解决了平滑性和梯度问题，并引入扩散损失度量模型性能。


<details>
  <summary>Details</summary>
Motivation: 非线性激活函数对学习非线性数据集至关重要，但现有函数（如ELU、SiLU、RELU等）在平滑性和梯度方面存在局限性，影响模型性能。

Method: 提出平滑且具有非零梯度的Leaky Exponential Linear Unit激活函数，并设计扩散损失度量模型过拟合情况。

Result: 新激活函数在性能上优于传统函数，扩散损失能有效评估模型过拟合。

Conclusion: Leaky Exponential Linear Unit和扩散损失为多维非线性数据回归提供了更优的解决方案。

Abstract: This document proposes a parametric activation function (ac.f.) aimed at
improving multidimensional nonlinear data regression. It is a established
knowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.
This work shows that smoothness and gradient properties of the ac.f. further
impact the performance of large neural networks in terms of overfitting and
sensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as
ELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and
Leaky-RELU further impart discontinuity in the trained model. Improved
performance is demonstrated with a smooth "Leaky Exponential Linear Unit", with
non-zero gradient that can be trained. A novel diffusion-loss metric is also
proposed to gauge the performance of the trained models in terms of
overfitting.

</details>


### [113] [Mutual Information Free Topological Generalization Bounds via Stability](https://arxiv.org/abs/2507.06775)
*Mario Tuci,Lennart Bastian,Benjamin Dupuis,Nassir Navab,Tolga Birdal,Umut Şimşekli*

Main category: cs.LG

TL;DR: 论文提出了一种新的拓扑泛化边界框架，避免了复杂的信息论项，通过轨迹稳定性证明泛化误差的上界与拓扑数据分析和算法稳定性相关。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑泛化边界依赖复杂的信息论项，难以应用于实际算法（如ADAM），因此需要提出更全面且可解释的边界。

Method: 引入新的学习理论框架，基于算法稳定性（轨迹稳定性），将泛化误差与拓扑复杂性和算法稳定性参数关联。

Result: 实验证明拓扑数据项在边界中至关重要，尤其随着训练样本增加，解释了拓扑泛化边界的实证成功。

Conclusion: 提出的框架提供了更实用且可解释的拓扑泛化边界，为理解优化算法的泛化性能提供了新视角。

Abstract: Providing generalization guarantees for stochastic optimization algorithms is
a major challenge in modern learning theory. Recently, several studies
highlighted the impact of the geometry of training trajectories on the
generalization error, both theoretically and empirically. Among these works, a
series of topological generalization bounds have been proposed, relating the
generalization error to notions of topological complexity that stem from
topological data analysis (TDA). Despite their empirical success, these bounds
rely on intricate information-theoretic (IT) terms that can be bounded in
specific cases but remain intractable for practical algorithms (such as ADAM),
potentially reducing the relevance of the derived bounds. In this paper, we
seek to formulate comprehensive and interpretable topological generalization
bounds free of intractable mutual information terms. To this end, we introduce
a novel learning theoretic framework that departs from the existing strategies
via proof techniques rooted in algorithmic stability. By extending an existing
notion of \textit{hypothesis set stability}, to \textit{trajectory stability},
we prove that the generalization error of trajectory-stable algorithms can be
upper bounded in terms of (i) TDA quantities describing the complexity of the
trajectory of the optimizer in the parameter space, and (ii) the trajectory
stability parameter of the algorithm. Through a series of experimental
evaluations, we demonstrate that the TDA terms in the bound are of great
importance, especially as the number of training samples grows. This ultimately
forms an explanation of the empirical success of the topological generalization
bounds.

</details>


### [114] [Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm](https://arxiv.org/abs/2507.06780)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: 提出了一种模仿学习方法，学习符合专家轨迹约束的最大熵策略，通过KL散度连接性能与策略差异，结合强化学习目标与约束目标进行优化。


<details>
  <summary>Details</summary>
Motivation: 研究如何在模仿学习中学习符合约束的最大熵策略，以解决多约束、多模态行为下的策略学习问题。

Method: 利用KL散度连接专家与学习策略的性能边界，结合强化学习与约束目标，通过双梯度下降优化学习目标。

Result: 实验表明，该方法能有效学习符合约束的策略，适应多约束、多模态行为，并具备泛化能力。

Conclusion: 该方法在模仿学习中成功实现了约束遵守与熵最大化的平衡，适用于复杂任务场景。

Abstract: This article introduces an imitation learning method for learning maximum
entropy policies that comply with constraints demonstrated by expert
trajectories executing a task. The formulation of the method takes advantage of
results connecting performance to bounds for the KL-divergence between
demonstrated and learned policies, and its objective is rigorously justified
through a connection to a probabilistic inference framework for reinforcement
learning, incorporating the reinforcement learning objective and the objective
to abide by constraints in an entropy maximization setting. The proposed
algorithm optimizes the learning objective with dual gradient descent,
supporting effective and stable training. Experiments show that the proposed
method can learn effective policy models for constraints-abiding behaviour, in
settings with multiple constraints of different types, accommodating different
modalities of demonstrated behaviour, and with abilities to generalize.

</details>


### [115] [Speech Tokenizer is Key to Consistent Representation](https://arxiv.org/abs/2507.06802)
*Wonjin Jung,Sungil Kang,Dong-Yeon Cho*

Main category: cs.LG

TL;DR: 提出一种新型语音分词器，同时编码语言和声学信息，提升语音表示保真度，适用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有残差向量量化方法虽引入语义元素，但忽略关键声学特征，影响语音处理的全面性。

Method: 提出一种先进方法，同时编码语言和声学信息，保留韵律和情感内容。

Result: 实证评估显示，该方法在语音编码、语音转换、情感识别和多模态语言建模中表现优异，无需额外训练。

Conclusion: 该方法具有广泛适用性，有望成为AI驱动语音处理的关键工具。

Abstract: Speech tokenization is crucial in digital speech processing, converting
continuous speech signals into discrete units for various computational tasks.
This paper introduces a novel speech tokenizer with broad applicability across
downstream tasks. While recent advances in residual vector quantization (RVQ)
have incorporated semantic elements, they often neglect critical acoustic
features. We propose an advanced approach that simultaneously encodes both
linguistic and acoustic information, preserving prosodic and emotional content.
Our method significantly enhances speech representation fidelity across diverse
applications. Empirical evaluations demonstrate its effectiveness in speech
coding, voice conversion, emotion recognition, and multimodal language
modeling, without requiring additional training. This versatility underscores
its potential as a key tool for advancing AI-driven speech processing.

</details>


### [116] [What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models](https://arxiv.org/abs/2507.06952)
*Keyon Vafa,Peter G. Chang,Ashesh Rambachan,Sendhil Mullainathan*

Main category: cs.LG

TL;DR: 本文提出了一种评估基础模型的方法，通过生成合成数据集来测试模型是否真正理解底层世界模型，发现模型在适应新任务时未能发展出正确的归纳偏差。


<details>
  <summary>Details</summary>
Motivation: 评估基础模型是否真正捕捉到更深层次的结构，而不仅仅是完成训练任务。

Method: 开发了一种称为“归纳偏差探针”的技术，通过生成合成数据集并测试模型对新任务的适应能力。

Result: 发现基础模型在适应新任务时未能发展出与底层世界模型一致的归纳偏差，尤其是在物理任务中表现不佳。

Conclusion: 基础模型可能仅发展出任务特定的启发式方法，而未能真正理解底层结构。

Abstract: Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.

</details>


### [117] [Noisy PDE Training Requires Bigger PINNs](https://arxiv.org/abs/2507.06967)
*Sebastien Andre-Sloan,Anirbit Mukherjee,Matthew Colbrook*

Main category: cs.LG

TL;DR: 论文研究了在噪声数据下，物理信息神经网络（PINNs）逼近偏微分方程（PDE）解的条件，证明了网络规模与监督数据噪声方差的关系。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，数据通常带有噪声，但关于PINNs在噪声条件下有效性的条件尚不清楚。

Method: 通过理论分析，证明了神经网络规模与监督数据噪声方差的下界关系，并通过实验验证了PINNs在噪声条件下能达到低经验风险。

Result: 证明了网络参数数量需满足$d_N\log d_N\gtrsim N_s \eta^2$才能实现低经验风险，且增加噪声监督数据并不能无代价降低风险。

Conclusion: 研究为定量理解噪声条件下训练PINNs的参数需求奠定了基础。

Abstract: Physics-Informed Neural Networks (PINNs) are increasingly used to approximate
solutions of partial differential equations (PDEs), especially in high
dimensions. In real-world applications, data samples are noisy, so it is
important to know when a predictor can still achieve low empirical risk.
However, little is known about the conditions under which a PINN can do so
effectively. We prove a lower bound on the size of neural networks required for
the supervised PINN empirical risk to fall below the variance of noisy
supervision labels. Specifically, if a predictor achieves an empirical risk
$O(\eta)$ below $\sigma^2$ (variance of supervision data), then necessarily
$d_N\log d_N\gtrsim N_s \eta^2$, where $N_s$ is the number of samples and $d_N$
is the number of trainable parameters of the PINN. A similar constraint applies
to the fully unsupervised PINN setting when boundary labels are sampled
noisily. Consequently, increasing the number of noisy supervision labels alone
does not provide a ``free lunch'' in reducing empirical risk. We also show
empirically that PINNs can indeed achieve empirical risks below $\sigma^2$
under such conditions. As a case study, we investigate PINNs applied to the
Hamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for
quantitatively understanding the parameter requirements for training PINNs in
the presence of noise.

</details>


### [118] [Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy](https://arxiv.org/abs/2507.06969)
*Bogdan Kulynych,Juan Felipe Gomez,Georgios Kaissis,Jamie Hayes,Borja Balle,Flavio du Pin Calmon,Jean Louis Raisaro*

Main category: cs.LG

TL;DR: 论文提出了一种基于假设检验解释的差分隐私（$f$-DP）方法，统一了重识别、属性推断和数据重建风险的攻击成功界限，提供了更紧致的隐私保护评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私机制在隐私参数映射到具体风险（如重识别、属性推断和数据重建）时过于悲观且不一致，需要更统一和可调的方法。

Method: 采用$f$-DP的假设检验解释，推导出统一的攻击成功界限，适用于多种攻击场景，并可调整基线风险水平。

Result: 新方法比$\varepsilon$-DP、Rényi DP和集中DP更紧致，噪声校准可减少20%的噪声需求，在文本分类任务中提升15%以上的准确率。

Conclusion: $f$-DP为差分隐私的保护程度提供了统一的解释和校准框架，适用于具体风险水平的评估。

Abstract: Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.

</details>


### [119] [Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing](https://arxiv.org/abs/2507.06996)
*Eunbyeol Cho,Jiyoun Kim,Minjae Lee,Sungjin Park,Edward Choi*

Main category: cs.LG

TL;DR: RawMed是一个生成多表时间序列电子健康记录（EHR）数据的框架，旨在解决隐私问题并保留原始数据的复杂结构和动态。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和监管限制，真实EHR数据难以共享和使用，需要生成合成数据。传统方法仅生成部分专家选择的特征，无法完全模拟原始数据。

Method: RawMed使用基于文本的表示和压缩技术，无需过多预处理即可捕捉复杂结构和时间动态。

Result: 在开源EHR数据集上验证，RawMed在保真度和实用性上优于基线模型。

Conclusion: RawMed为合成多表时间序列EHR数据提供了高效且隐私安全的解决方案。

Abstract: Electronic Health Records (EHR) are time-series relational databases that
record patient interactions and medical events over time, serving as a critical
resource for healthcare research and applications. However, privacy concerns
and regulatory restrictions limit the sharing and utilization of such sensitive
data, necessitating the generation of synthetic EHR datasets. Unlike previous
EHR synthesis methods, which typically generate medical records consisting of
expert-chosen features (e.g. a few vital signs or structured codes only), we
introduce RawMed, the first framework to synthesize multi-table, time-series
EHR data that closely resembles raw EHRs. Using text-based representation and
compression techniques, RawMed captures complex structures and temporal
dynamics with minimal preprocessing. We also propose a new evaluation framework
for multi-table time-series synthetic EHRs, assessing distributional
similarity, inter-table relationships, temporal dynamics, and privacy.
Validated on two open-source EHR datasets, RawMed outperforms baseline models
in fidelity and utility. The code is available at
https://github.com/eunbyeol-cho/RawMed.

</details>


### [120] [Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning](https://arxiv.org/abs/2507.06839)
*Jihao Andreas Lin*

Main category: cs.LG

TL;DR: 该论文提出了一种结合迭代方法和路径条件化的方法，以提高高斯过程在大规模数据下的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在经典框架下难以适应大规模数据和现代硬件并行计算的需求，因此需要改进其可扩展性。

Method: 通过结合迭代线性系统求解器和路径条件化，将昂贵计算转化为线性方程组求解，减少内存需求。

Result: 显著降低了内存需求，适用于更大规模数据，并优化了现代硬件的矩阵乘法操作。

Conclusion: 该方法为高斯过程在现代大规模计算环境中的应用提供了有效解决方案。

Abstract: Gaussian processes are a powerful framework for uncertainty-aware function
approximation and sequential decision-making. Unfortunately, their classical
formulation does not scale gracefully to large amounts of data and modern
hardware for massively-parallel computation, prompting many researchers to
develop techniques which improve their scalability. This dissertation focuses
on the powerful combination of iterative methods and pathwise conditioning to
develop methodological contributions which facilitate the use of Gaussian
processes in modern large-scale settings. By combining these two techniques
synergistically, expensive computations are expressed as solutions to systems
of linear equations and obtained by leveraging iterative linear system solvers.
This drastically reduces memory requirements, facilitating application to
significantly larger amounts of data, and introduces matrix multiplication as
the main computational operation, which is ideal for modern hardware.

</details>


### [121] [PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments](https://arxiv.org/abs/2507.07032)
*Hanqun Cao,Xinyi Zhou,Zijun Gao,Chenyu Wang,Xin Gao,Zhi Zhang,Chunbin Gu,Ge Liu,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: PLAME是一种新型MSA设计模型，利用预训练蛋白质语言模型的进化嵌入，提升低同源性和孤儿蛋白质的结构预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有折叠模型依赖多序列比对（MSA），但在低同源性和孤儿蛋白质上效果有限。PLAME旨在解决这一问题。

Method: PLAME引入预训练表示增强进化信息，采用保守-多样性损失提升生成质量，并提出新的MSA筛选方法和序列质量评估指标。

Result: 在AlphaFold2和AlphaFold3基准测试中，PLAME在折叠增强和序列质量评估上达到最先进性能。

Conclusion: PLAME不仅提升了预测性能，还可作为适配器结合ESMFold实现高效推理。

Abstract: Protein structure prediction is essential for drug discovery and
understanding biological functions. While recent advancements like AlphaFold
have achieved remarkable accuracy, most folding models rely heavily on multiple
sequence alignments (MSAs) to boost prediction performance. This dependency
limits their effectiveness on low-homology proteins and orphan proteins, where
MSA information is sparse or unavailable. To address this limitation, we
propose PLAME, a novel MSA design model that leverages evolutionary embeddings
from pretrained protein language models. Unlike existing methods, PLAME
introduces pretrained representations to enhance evolutionary information and
employs a conservation-diversity loss to enhance generation quality.
Additionally, we propose a novel MSA selection method to effectively screen
high-quality MSAs and improve folding performance. We also propose a sequence
quality assessment metric that provides an orthogonal perspective to evaluate
MSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,
PLAME achieves state-of-the-art performance in folding enhancement and sequence
quality assessment, with consistent improvements demonstrated on AlphaFold3.
Ablation studies validate the effectiveness of the MSA selection method, while
extensive case studies on various protein types provide insights into the
relationship between AlphaFold's prediction quality and MSA characteristics.
Furthermore, we demonstrate that PLAME can serve as an adapter achieving
AlphaFold2-level accuracy with the ESMFold's inference speed.

</details>


### [122] [Episodic Contextual Bandits with Knapsacks under Conversion Models](https://arxiv.org/abs/2507.06859)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 研究在线情境下的决策者与上下文带背包的赌博机（BwK）实例的交互，提出一种算法实现次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决动态定价和首次价格拍卖等应用中资源分配的非平稳性和上下文多样性问题。

Method: 设计在线算法，利用置信边界预言机实现次线性遗憾，处理无界状态空间。

Result: 算法在特定情境下提供改进的遗憾边界，尤其在提供未标记特征数据时。

Conclusion: 该框架为上下文BwK文献提供了新的解决方案，尤其在资源动态分配方面具有潜力。

Abstract: We study an online setting, where a decision maker (DM) interacts with
contextual bandit-with-knapsack (BwK) instances in repeated episodes. These
episodes start with different resource amounts, and the contexts' probability
distributions are non-stationary in an episode. All episodes share the same
latent conversion model, which governs the random outcome contingent upon a
request's context and an allocation decision. Our model captures applications
such as dynamic pricing on perishable resources with episodic replenishment,
and first price auctions in repeated episodes with different starting budgets.
We design an online algorithm that achieves a regret sub-linear in $T$, the
number of episodes, assuming access to a \emph{confidence bound oracle} that
achieves an $o(T)$-regret. Such an oracle is readily available from existing
contextual bandit literature. We overcome the technical challenge with
arbitrarily many possible contexts, which leads to a reinforcement learning
problem with an unbounded state space. Our framework provides improved regret
bounds in certain settings when the DM is provided with unlabeled feature data,
which is novel to the contextual BwK literature.

</details>


### [123] [Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants](https://arxiv.org/abs/2507.06888)
*Wei Chen,Wanyang Gu,Linjun Peng,Ruichu Cai,Zhifeng Hao,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种联邦因果发现方法，结合水平和垂直联邦设置，利用高阶累积量构建全局因果图，解决了数据隐私和变量不完整的问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果发现方法主要针对水平联邦设置，但实际中不同客户端可能包含不同变量，导致虚假因果关系。本文旨在解决这一问题。

Method: 通过聚合客户端的高阶累积量信息构建全局估计，递归识别因果源，生成全局因果强度矩阵。

Result: 实验表明，该方法在合成和真实数据上表现优越，能重建因果图并估计因果强度系数。

Conclusion: 该方法在水平和垂直联邦设置下均有效，解决了变量不完整和隐私保护问题。

Abstract: Federated causal discovery aims to uncover the causal relationships between
entities while protecting data privacy, which has significant importance and
numerous applications in real-world scenarios. Existing federated causal
structure learning methods primarily focus on horizontal federated settings.
However, in practical situations, different clients may not necessarily contain
data on the same variables. In a single client, the incomplete set of variables
can easily lead to spurious causal relationships, thereby affecting the
information transmitted to other clients. To address this issue, we
comprehensively consider causal structure learning methods under both
horizontal and vertical federated settings. We provide the identification
theories and methods for learning causal structure in the horizontal and
vertical federal setting via higher-order cumulants. Specifically, we first
aggregate higher-order cumulant information from all participating clients to
construct global cumulant estimates. These global estimates are then used for
recursive source identification, ultimately yielding a global causal strength
matrix. Our approach not only enables the reconstruction of causal graphs but
also facilitates the estimation of causal strength coefficients. Our algorithm
demonstrates superior performance in experiments conducted on both synthetic
data and real-world data.

</details>


### [124] [Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams](https://arxiv.org/abs/2507.06901)
*Abolfazl Zarghani,Sadegh Abedi*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的动态滑动窗口优化方法（RL-Window），用于处理多维数据流，解决了固定窗口难以适应动态变化的问题。


<details>
  <summary>Details</summary>
Motivation: 多维数据流（如IoT、金融市场）的高速度、无界性和复杂依赖关系对处理技术提出了挑战，固定窗口难以适应动态变化（如概念漂移）。

Method: 将窗口大小选择建模为强化学习问题，使用Dueling DQN和优先经验回放技术，自适应调整窗口大小。

Result: 在多个基准数据集上，RL-Window在分类精度、漂移鲁棒性和计算效率上优于现有方法（如ADWIN、CNN-Adaptive）。

Conclusion: RL-Window具有适应性和稳定性，适用于实时应用，并通过扩展指标（如能效、延迟）验证了其优势。

Abstract: Multi-dimensional data streams, prevalent in applications like IoT, financial
markets, and real-time analytics, pose significant challenges due to their high
velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding
window techniques are critical for processing such streams, but fixed-size
windows struggle to adapt to dynamic changes like concept drift or bursty
patterns. This paper proposes a novel reinforcement learning (RL)-based
approach to dynamically optimize sliding window sizes for multi-dimensional
data streams. By formulating window size selection as an RL problem, we enable
an agent to learn an adaptive policy based on stream characteristics, such as
variance, correlations, and temporal trends. Our method, RL-Window, leverages a
Dueling Deep Q-Network (DQN) with prioritized experience replay to handle
non-stationarity and high-dimensionality. Evaluations on benchmark datasets
(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms
state-of-the-art methods like ADWIN and CNN-Adaptive in classification
accuracy, drift robustness, and computational efficiency. Additional
qualitative analyses, extended metrics (e.g., energy efficiency, latency), and
a comprehensive dataset characterization further highlight its adaptability and
stability, making it suitable for real-time applications.

</details>


### [125] [Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting](https://arxiv.org/abs/2507.06907)
*Linyun Gao,Qiang Wen,Fumio Machida*

Main category: cs.LG

TL;DR: 提出了一种基于N版本机器学习（NVML）的框架，通过安全感知加权软投票机制提升交通标志识别系统在对抗攻击下的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的交通标志识别系统易受对抗攻击影响，威胁驾驶安全，需提升其鲁棒性。

Method: 采用NVML框架，结合FMEA评估安全风险，动态分配安全感知权重，并通过FGSM和PGD攻击测试三版本NVML系统的鲁棒性。

Result: 实验表明，NVML方法显著提升了交通标志识别系统在对抗条件下的鲁棒性和安全性。

Conclusion: NVML框架能有效增强自动驾驶系统的安全性，对抗攻击下表现优异。

Abstract: Autonomous driving is rapidly advancing as a key application of machine
learning, yet ensuring the safety of these systems remains a critical
challenge. Traffic sign recognition, an essential component of autonomous
vehicles, is particularly vulnerable to adversarial attacks that can compromise
driving safety. In this paper, we propose an N-version machine learning (NVML)
framework that integrates a safety-aware weighted soft voting mechanism. Our
approach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential
safety risks and assign dynamic, safety-aware weights to the ensemble outputs.
We evaluate the robustness of three-version NVML systems employing various
voting mechanisms against adversarial samples generated using the Fast Gradient
Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental
results demonstrate that our NVML approach significantly enhances the
robustness and safety of traffic sign recognition systems under adversarial
conditions.

</details>


### [126] [DICE: Data Influence Cascade in Decentralized Learning](https://arxiv.org/abs/2507.06931)
*Tongtian Zhu,Wenhao Li,Can Wang,Fengxiang He*

Main category: cs.LG

TL;DR: 论文提出了一种名为DICE的方法，用于在去中心化网络中估计数据影响力的级联效应，以解决参与激励不足的问题。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习虽然能分散数据消耗和计算负载，但缺乏公平的激励机制，阻碍了参与积极性。

Method: 设计了DICE方法，通过理论推导近似计算任意邻居跳数的影响力级联，结合数据、通信拓扑和损失景观曲率。

Result: DICE为选择合适合作者和识别恶意行为提供了理论基础。

Conclusion: DICE是首个在去中心化环境中估计数据影响力级联的方法，具有实际应用价值。

Abstract: Decentralized learning offers a promising approach to crowdsource data
consumptions and computational workloads across geographically distributed
compute interconnected through peer-to-peer networks, accommodating the
exponentially increasing demands. However, proper incentives are still in
absence, considerably discouraging participation. Our vision is that a fair
incentive mechanism relies on fair attribution of contributions to
participating nodes, which faces non-trivial challenges arising from the
localized connections making influence ``cascade'' in a decentralized network.
To overcome this, we design the first method to estimate \textbf{D}ata
\textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized
environment. Theoretically, the framework derives tractable approximations of
influence cascade over arbitrary neighbor hops, suggesting the influence
cascade is determined by an interplay of data, communication topology, and the
curvature of loss landscape. DICE also lays the foundations for applications
including selecting suitable collaborators and identifying malicious behaviors.
Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.

</details>


### [127] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas,Efthymios Georgiou,Giorgos Bouritsas,Theodoros Giannakopoulos,Mihalis A. Nicolaou,Yannis Panagakis*

Main category: cs.LG

TL;DR: 本文提出两种新的损失函数（MV-InfoNCE和MV-DHEL），解决了多视图对比学习中存在的四个关键问题，显著提升了性能并支持多模态扩展。


<details>
  <summary>Details</summary>
Motivation: 当前对比学习方法在处理多视图数据时存在优化冲突、交互建模不足、对齐-均匀性耦合等问题，无法充分利用多视图的优势。

Method: 提出MV-InfoNCE和MV-DHEL两种损失函数，前者同时建模所有视图交互，后者解耦对齐与均匀性并支持多视图扩展。

Result: 在ImageNet1K等数据集上，新方法显著优于现有多视图方法，且能有效利用多视图和多模态数据。

Conclusion: 新方法通过理论支持和实验验证，解决了多视图对比学习的核心问题，并展示了其在多模态场景中的潜力。

Abstract: Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [128] [Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions](https://arxiv.org/abs/2507.07008)
*Emile Pierret,Bruno Galerne*

Main category: cs.LG

TL;DR: 本文研究了扩散模型在高斯数据分布去模糊任务中的准确性，通过计算Wasserstein距离比较了不同算法的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在贝叶斯逆问题中作为先验具有灵活性和高方差，但其性能尚不明确。本文旨在评估其在去模糊任务中的准确性。

Method: 在约束的高斯数据分布下，计算扩散模型采样器与理想解分布之间的精确Wasserstein距离。

Result: 研究发现可以比较文献中不同算法的性能。

Conclusion: 扩散模型在特定任务中的准确性可通过理论分析进行评估，为算法比较提供了依据。

Abstract: Used as priors for Bayesian inverse problems, diffusion models have recently
attracted considerable attention in the literature. Their flexibility and high
variance enable them to generate multiple solutions for a given task, such as
inpainting, super-resolution, and deblurring. However, several unresolved
questions remain about how well they perform. In this article, we investigate
the accuracy of these models when applied to a Gaussian data distribution for
deblurring. Within this constrained context, we are able to precisely analyze
the discrepancy between the theoretical resolution of inverse problems and
their resolution obtained using diffusion models by computing the exact
Wasserstein distance between the distribution of the diffusion model sampler
and the ideal distribution of solutions to the inverse problem. Our findings
allow for the comparison of different algorithms from the literature.

</details>


### [129] [On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence](https://arxiv.org/abs/2507.07016)
*Jian Huang,Yongli Zhu,Linna Xu,Zhe Zheng,Wenpeng Cui,Mingyang Sun*

Main category: cs.LG

TL;DR: 论文研究了在资源有限的智能电表上进行边缘端模型训练的可行性，提出了混合和降低精度的训练方案，并通过光伏功率预测案例验证了效果。


<details>
  <summary>Details</summary>
Motivation: 推动电网边缘智能化，探索在设备端进行模型训练的可行性。

Method: 介绍了设备端训练的技术准备步骤，研究了梯度提升树和循环神经网络模型，并设计了混合和降低精度的训练方案。

Result: 实验结果表明，通过现有先进计量基础设施可以经济地实现电网边缘智能化。

Conclusion: 在资源受限的智能电表上实现边缘端模型训练是可行的，为电网边缘智能化提供了经济有效的解决方案。

Abstract: In this paper, an edge-side model training study is conducted on a
resource-limited smart meter. The motivation of grid-edge intelligence and the
concept of on-device training are introduced. Then, the technical preparation
steps for on-device training are described. A case study on the task of
photovoltaic power forecasting is presented, where two representative machine
learning models are investigated: a gradient boosting tree model and a
recurrent neural network model. To adapt to the resource-limited situation in
the smart meter, "mixed"- and "reduced"-precision training schemes are also
devised. Experiment results demonstrate the feasibility of economically
achieving grid-edge intelligence via the existing advanced metering
infrastructures.

</details>


### [130] [Self-Supervised Learning at the Edge: The Cost of Labeling](https://arxiv.org/abs/2507.07033)
*Roberto Pereira,Fernanda Famá,Asal Rangrazi,Marco Miozzo,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 对比学习（CL）在资源受限的边缘设备上实现高效学习，通过定制自监督学习（SSL）策略，性能接近传统方法且资源消耗减少4倍。


<details>
  <summary>Details</summary>
Motivation: 探索自监督学习在边缘设备上的可行性，解决数据量大、计算资源需求高的问题，平衡性能与能源效率。

Method: 分析不同SSL技术在有限计算、数据和能源预算下的适应性，评估其在资源受限环境中学习鲁棒表示的有效性，并考虑半监督学习降低能源成本。

Result: 定制SSL策略在性能接近传统方法的同时，资源消耗减少高达4倍。

Conclusion: 自监督学习在边缘设备上具有高效学习的潜力，尤其在资源受限环境中表现优异。

Abstract: Contrastive learning (CL) has recently emerged as an alternative to
traditional supervised machine learning solutions by enabling rich
representations from unstructured and unlabeled data. However, CL and, more
broadly, self-supervised learning (SSL) methods often demand a large amount of
data and computational resources, posing challenges for deployment on
resource-constrained edge devices. In this work, we explore the feasibility and
efficiency of SSL techniques for edge-based learning, focusing on trade-offs
between model performance and energy efficiency. In particular, we analyze how
different SSL techniques adapt to limited computational, data, and energy
budgets, evaluating their effectiveness in learning robust representations
under resource-constrained settings. Moreover, we also consider the energy
costs involved in labeling data and assess how semi-supervised learning may
assist in reducing the overall energy consumed to train CL models. Through
extensive experiments, we demonstrate that tailored SSL strategies can achieve
competitive performance while reducing resource consumption by up to 4X,
underscoring their potential for energy-efficient learning at the edge.

</details>


### [131] [An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems](https://arxiv.org/abs/2507.07061)
*Shervin Ghaffari,Zohre Bahranifard,Mohammad Akbari*

Main category: cs.LG

TL;DR: 本文提出了一种基于集成嵌入的语义缓存方法，通过结合多个嵌入模型提升LLM系统中语义相似性检测的性能，显著优于单模型方法。


<details>
  <summary>Details</summary>
Motivation: 现有语义缓存框架依赖单一嵌入模型，难以捕捉真实查询分布的多样语义关系，限制了缓存效率。

Method: 采用集成嵌入方法，通过训练元编码器结合多个嵌入模型，提升语义相似性检测能力。

Result: 在QQP数据集上，集成方法实现了92%的缓存命中率和85%的非等价查询拒绝准确率。

Conclusion: 集成嵌入方法显著提升了语义缓存的性能，减少了LLM系统的计算开销。

Abstract: Semantic caching enhances the efficiency of large language model (LLM)
systems by identifying semantically similar queries, storing responses once,
and serving them for subsequent equivalent requests. However, existing semantic
caching frameworks rely on single embedding models for query representation,
which limits their ability to capture the diverse semantic relationships
present in real-world query distributions. This paper presents an ensemble
embedding approach that combines multiple embedding models through a trained
meta-encoder to improve semantic similarity detection in LLM caching systems.
We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring
cache hit ratios, cache miss ratios, token savings, and response times. Our
ensemble approach achieves a 92\% cache hit ratio for semantically equivalent
queries while maintaining an 85\% accuracy in correctly rejecting
non-equivalent queries as cache misses. These results demonstrate that ensemble
embedding methods significantly outperform single-model approaches in
distinguishing between semantically similar and dissimilar queries, leading to
more effective caching performance and reduced computational overhead in
LLM-based systems.

</details>


### [132] [Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts](https://arxiv.org/abs/2507.07100)
*Lan Li,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: DCE框架通过频率感知专家组和动态专家选择器解决DIL中的类不平衡和域间分布偏移问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: DIL在非平稳环境中面临类内不平衡和跨域分布偏移的挑战，影响模型性能。

Method: 提出DCE框架，包括频率感知专家组和动态专家选择器，分别处理类内不平衡和跨域知识迁移。

Result: 在四个基准数据集上，DCE表现出最先进的性能。

Conclusion: DCE有效解决了DIL中的关键挑战，提升了模型在动态环境中的适应能力。

Abstract: Domain-Incremental Learning (DIL) focuses on continual learning in
non-stationary environments, requiring models to adjust to evolving domains
while preserving historical knowledge. DIL faces two critical challenges in the
context of imbalanced data: intra-domain class imbalance and cross-domain class
distribution shifts. These challenges significantly hinder model performance,
as intra-domain imbalance leads to underfitting of few-shot classes, while
cross-domain shifts require maintaining well-learned many-shot classes and
transferring knowledge to improve few-shot class performance in old domains. To
overcome these challenges, we introduce the Dual-Balance Collaborative Experts
(DCE) framework. DCE employs a frequency-aware expert group, where each expert
is guided by specialized loss functions to learn features for specific
frequency groups, effectively addressing intra-domain class imbalance.
Subsequently, a dynamic expert selector is learned by synthesizing
pseudo-features through balanced Gaussian sampling from historical class
statistics. This mechanism navigates the trade-off between preserving many-shot
knowledge of previous domains and leveraging new data to improve few-shot class
performance in earlier tasks. Extensive experimental results on four benchmark
datasets demonstrate DCE's state-of-the-art performance.

</details>


### [133] [Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful](https://arxiv.org/abs/2507.07101)
*Martin Marek,Sanae Lotfi,Aditya Somasundaram,Andrew Gordon Wilson,Micah Goldblum*

Main category: cs.LG

TL;DR: 研究发现小批量训练语言模型更稳定且高效，提出调整Adam超参数的规则，并建议避免梯度累积。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为小批量训练不稳定，但本文挑战这一观点，探索小批量（甚至批量大小为1）的潜力。

Method: 提出调整Adam超参数的规则，并测试小批量训练的效果。

Result: 小批量训练更稳定、对超参数更鲁棒，性能优于大批量，且支持SGD稳定训练。

Conclusion: 建议选择小批量并调整超参数，避免梯度累积，除非在多设备训练中受带宽限制。

Abstract: Conventional wisdom dictates that small batch sizes make language model
pretraining and fine-tuning unstable, motivating gradient accumulation, which
trades off the number of optimizer steps for a proportional increase in batch
size. While it is common to decrease the learning rate for smaller batch sizes,
other hyperparameters are often held fixed. In this work, we revisit small
batch sizes all the way down to batch size one, and we propose a rule for
scaling Adam hyperparameters to small batch sizes. We find that small batch
sizes (1) train stably, (2) are consistently more robust to hyperparameter
choices, (3) achieve equal or better per-FLOP performance than larger batch
sizes, and (4) notably enable stable language model training with vanilla SGD,
even without momentum, despite storing no optimizer state. Building on these
results, we provide practical recommendations for selecting a batch size and
setting optimizer hyperparameters. We further recommend against gradient
accumulation unless training on multiple devices with multiple model replicas,
bottlenecked by inter-device bandwidth.

</details>


### [134] [Does Data Scaling Lead to Visual Compositional Generalization?](https://arxiv.org/abs/2507.07102)
*Arnas Uselis,Andrea Dittadi,Seong Joon Oh*

Main category: cs.LG

TL;DR: 研究发现数据多样性而非数据规模驱动组合泛化能力，线性分解表示结构是关键。预训练模型表现部分支持这一结论。


<details>
  <summary>Details</summary>
Motivation: 探讨当代视觉模型是否具备组合理解能力，验证数据规模和多样性对组合泛化的影响。

Method: 通过控制实验系统变化数据规模、概念多样性和组合覆盖率，分析模型表现。

Result: 组合泛化能力由数据多样性驱动，线性分解表示结构实现高效泛化。预训练模型表现部分支持这一结论。

Conclusion: 强调构建多样化数据集的重要性，并关注支持高效组合学习的表示结构。

Abstract: Compositional understanding is crucial for human intelligence, yet it remains
unclear whether contemporary vision models exhibit it. The dominant machine
learning paradigm is built on the premise that scaling data and model sizes
will improve out-of-distribution performance, including compositional
generalization. We test this premise through controlled experiments that
systematically vary data scale, concept diversity, and combination coverage. We
find that compositional generalization is driven by data diversity, not mere
data scale. Increased combinatorial coverage forces models to discover a
linearly factored representational structure, where concepts decompose into
additive components. We prove this structure is key to efficiency, enabling
perfect generalization from few observed combinations. Evaluating pretrained
models (DINO, CLIP), we find above-random yet imperfect performance, suggesting
partial presence of this structure. Our work motivates stronger emphasis on
constructing diverse datasets for compositional generalization, and considering
the importance of representational structure that enables efficient
compositional learning. Code available at
https://github.com/oshapio/visual-compositional-generalization.

</details>
