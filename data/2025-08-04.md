<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 62]
- [cs.HC](#cs.HC) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)
*Oshayer Siddique,J. M Areeb Uzair Alam,Md Jobayer Rahman Rafy,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 论文评估了前沿LLMs在解决物理问题（数学和描述性）上的表现，并通过多代理框架和推理技术提升模型性能，同时提出了新的物理问题评估基准${\rm P{\small HYSICS}E{\small VAL}$。


<details>
  <summary>Details</summary>
Motivation: 物理问题是自然语言推理的关键领域，但现有研究较少关注LLMs在此任务上的表现，因此需要评估和改进。

Method: 采用多代理框架和推理技术（如验证解决方案），并引入新基准${\rm P{\small HYSICS}E{\small VAL}$进行对比分析。

Result: 多代理框架显著提升了模型在初始表现较差的物理问题上的性能。

Conclusion: 论文为LLMs在物理问题解决上的性能提供了改进方法和评估基准，推动了相关领域的发展。

Abstract: The discipline of physics stands as a cornerstone of human intellect, driving
the evolution of technology and deepening our understanding of the fundamental
principles of the cosmos. Contemporary literature includes some works centered
on the task of solving physics problems - a crucial domain of natural language
reasoning. In this paper, we evaluate the performance of frontier LLMs in
solving physics problems, both mathematical and descriptive. We also employ a
plethora of inference-time techniques and agentic frameworks to improve the
performance of the models. This includes the verification of proposed solutions
in a cumulative fashion by other, smaller LLM agents, and we perform a
comparative analysis of the performance that the techniques entail. There are
significant improvements when the multi-agent framework is applied to problems
that the models initially perform poorly on. Furthermore, we introduce a new
evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small
VAL}}$, consisting of 19,609 problems sourced from various physics textbooks
and their corresponding correct solutions scraped from physics forums and
educational websites. Our code and data are publicly available at
https://github.com/areebuzair/PhysicsEval.

</details>


### [2] [Do LLMs produce texts with "human-like" lexical diversity?](https://arxiv.org/abs/2508.00086)
*Kelly Kendro,Jeffrey Maloney,Scott Jarvis*

Main category: cs.CL

TL;DR: 研究发现，LLM生成的文本在词汇多样性上与人类写作存在显著差异，尤其是较新的模型（如ChatGPT-4.5）差异更大。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成的文本是否在词汇多样性上与人类写作相似。

Method: 比较四个ChatGPT模型与240名L1和L2英语参与者的文本，测量六个词汇多样性维度，并进行统计分析。

Result: LLM生成的文本在词汇多样性上与人类写作显著不同，较新模型差异更大。

Conclusion: LLM生成的文本在词汇多样性上不具人类特征，较新模型更不相似。

Abstract: The degree to which LLMs produce writing that is truly human-like remains
unclear despite the extensive empirical attention that this question has
received. The present study addresses this question from the perspective of
lexical diversity. Specifically, the study investigates patterns of lexical
diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,
and -4.5) in comparison with texts written by L1 and L2 English participants (n
= 240) across four education levels. Six dimensions of lexical diversity were
measured in each text: volume, abundance, variety-repetition, evenness,
disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and
Support Vector Machines revealed that the LLM-generated texts differed
significantly from human-written texts for each variable, with ChatGPT-o4 mini
and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated
higher levels of lexical diversity despite producing fewer tokens. The human
writers' lexical diversity did not differ across subgroups (i.e., education,
language status). Altogether, the results indicate that LLMs do not produce
human-like texts in relation to lexical diversity, and the newer LLMs produce
less human-like texts than older models. We discuss the implications of these
results for language pedagogy and related applications.

</details>


### [3] [Semiotic Complexity and Its Epistemological Implications for Modeling Culture](https://arxiv.org/abs/2508.00095)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: 论文呼吁计算人文学科需要更多方法论理论化，以提升认识论和解释清晰度。作者将建模工作视为文化语言领域与计算数学领域之间的翻译，强调理论化的重要性以避免翻译错误。


<details>
  <summary>Details</summary>
Motivation: 计算人文学科缺乏方法论理论化，导致认识论和解释模糊，影响领域成熟。

Method: 将建模工作视为翻译过程，提出‘符号复杂性’概念，分析建模实践中因忽视复杂性而导致的翻译错误。

Result: 指出当前建模实践（尤其是评估）将符号复杂数据简化为符号简单数据，导致认识论问题。

Conclusion: 提出建议，帮助研究者更好地处理符号复杂性，提升建模的透明度和一致性。

Abstract: Greater theorizing of methods in the computational humanities is needed for
epistemological and interpretive clarity, and therefore the maturation of the
field. In this paper, we frame such modeling work as engaging in translation
work from a cultural, linguistic domain into a computational, mathematical
domain, and back again. Translators benefit from articulating the theory of
their translation process, and so do computational humanists in their work --
to ensure internal consistency, avoid subtle yet consequential translation
errors, and facilitate interpretive transparency. Our contribution in this
paper is to lay out a particularly consequential dimension of the lack of
theorizing and the sorts of translation errors that emerge in our modeling
practices as a result. Along these lines we introduce the idea of semiotic
complexity as the degree to which the meaning of some text may vary across
interpretive lenses, and make the case that dominant modeling practices --
especially around evaluation -- commit a translation error by treating
semiotically complex data as semiotically simple when it seems
epistemologically convenient by conferring superficial clarity. We then lay out
several recommendations for researchers to better account for these
epistemological issues in their own work.

</details>


### [4] [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)
*Mingda Chen,Yang Li,Xilun Chen,Adina Williams,Gargi Ghosh,Scott Yih*

Main category: cs.CL

TL;DR: FACTORY是一个大规模、人工验证的提示集，用于评估模型生成长文本事实性的能力，结果显示现有SOTA模型在FACTORY上的事实错误率高达40%。


<details>
  <summary>Details</summary>
Motivation: 现有的事实性评估基准缺乏人工验证，可能导致质量问题，因此需要更可靠的评估工具。

Method: 采用模型与人工结合的开发方式，构建FACTORY提示集，并对6个SOTA模型进行人工评估。

Result: FACTORY是一个更具挑战性的基准，SOTA模型在其上的事实错误率为40%，远高于其他数据集的10%。

Conclusion: FACTORY因其可靠性和对长尾事实推理的需求，优于现有基准。

Abstract: Long-form factuality evaluation assesses the ability of models to generate
accurate, comprehensive responses to short prompts. Existing benchmarks often
lack human verification, leading to potential quality issues. To address this
limitation, we introduce FACTORY, a large-scale, human-verified prompt set.
Developed using a model-in-the-loop approach and refined by humans, FACTORY
includes challenging prompts that are fact-seeking, answerable, and
unambiguous. We conduct human evaluations on 6 state-of-the-art language models
using FACTORY and existing datasets. Our results show that FACTORY is a
challenging benchmark: approximately 40% of the claims made in the responses of
SOTA models are not factual, compared to only 10% for other datasets. Our
analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing
its reliability and the necessity for models to reason across long-tailed
facts.

</details>


### [5] [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)
*Xiao Zhang,Johan bos*

Main category: cs.CL

TL;DR: 论文研究了神经语义解析器在强上下文敏感现象（如英语动词短语省略）上的表现，发现尽管在标准测试集上表现良好，但在省略现象上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探讨神经语义解析器在处理需要大量语义信息复制的强上下文敏感现象（如动词短语省略）时的能力。

Method: 构建了一个包含120个省略案例及其完全解析的语义表示的语料库，作为挑战集测试多种神经语义解析器。

Result: 尽管解析器在标准测试集上表现优异（语义匹配分数超过90%），但在省略案例中表现失败。

Conclusion: 神经语义解析器在处理强上下文敏感现象（如省略）时存在局限性，数据增强可能是一种改进方向。

Abstract: Neural semantic parsers have shown good overall performance for a variety of
linguistic phenomena, reaching semantic matching scores of more than 90%. But
how do such parsers perform on strongly context-sensitive phenomena, where
large pieces of semantic information need to be duplicated to form a meaningful
semantic representation? A case in point is English verb phrase ellipsis, a
construct where entire verb phrases can be abbreviated by a single auxiliary
verb. Are the otherwise known as powerful semantic parsers able to deal with
ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with
their fully resolved meaning representation and used this as a challenge set
for a large battery of neural semantic parsers. Although these parsers
performed very well on the standard test set, they failed in the instances with
ellipsis. Data augmentation

</details>


### [6] [Comparison of Large Language Models for Deployment Requirements](https://arxiv.org/abs/2508.00185)
*Alper Yaman,Jannik Schwab,Christof Nitsche,Abhirup Sinha,Marco Huber*

Main category: cs.CL

TL;DR: 该论文总结了大型语言模型（LLMs）的发展现状，比较了不同模型的特点，并提供了一个持续更新的模型列表以帮助研究者和企业选择适合的模型。


<details>
  <summary>Details</summary>
Motivation: 随着开源和领域特定LLM模型的增多，研究者和企业在选择模型时面临挑战，需要综合比较模型特性、许可和硬件需求。

Method: 通过整理和比较不同LLM模型的特征（如发布时间、许可和硬件需求），创建并发布一个持续更新的模型列表。

Result: 提供了一个公开的、持续更新的LLM模型比较列表，帮助用户快速选择适合的模型。

Conclusion: 该研究为LLM模型的选择提供了实用工具，并计划通过持续更新来适应快速发展的LLM领域。

Abstract: Large Language Models (LLMs), such as Generative Pre-trained Transformers
(GPTs) are revolutionizing the generation of human-like text, producing
contextually relevant and syntactically correct content. Despite challenges
like biases and hallucinations, these Artificial Intelligence (AI) models excel
in tasks, such as content creation, translation, and code generation.
Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address
these issues. Over the past two years, numerous open-source foundational and
fine-tuned models have been introduced, complicating the selection of the
optimal LLM for researchers and companies regarding licensing and hardware
requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM
selection, we present a comparative list of foundational and domain-specific
models, focusing on features, such as release year, licensing, and hardware
requirements. This list is published on GitLab and will be continuously
updated.

</details>


### [7] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

TL;DR: 本文探讨了表格在大型语言模型（LLMs）和多模态大型语言模型（MLLMs）中的重要性，提出了表格理解的分类和任务，并指出了当前研究中的关键问题。


<details>
  <summary>Details</summary>
Motivation: 表格因其复杂和灵活的结构在LLMs和MLLMs中受到广泛关注，但缺乏通用方法，导致理解任务具有挑战性。

Method: 通过分类表格输入表示和介绍表格理解任务，提出关键概念。

Result: 指出了当前研究中的三个关键问题：检索任务主导、模型处理复杂表格的困难以及模型泛化能力有限。

Conclusion: 需要进一步研究以解决表格理解中的挑战，特别是在复杂表格处理和多表格场景中。

Abstract: Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [8] [Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform](https://arxiv.org/abs/2508.00220)
*Rana Aref Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 论文探讨了离散小波变换（DWT）在词和句子嵌入中的应用，展示了其在多分辨率分析和压缩嵌入表示方面的能力，同时保持语义信息。


<details>
  <summary>Details</summary>
Motivation: 小波变换在信号和图像处理中已广泛应用，但其在自然语言处理（NLP）中的潜力尚未充分挖掘。本文旨在验证DWT在NLP中的有效性。

Method: 通过将DWT应用于词和句子嵌入，分析其在多分辨率下的表现，并评估其在语义相似性任务中的效果。

Result: DWT能将嵌入维度减少50-93%，且语义相似性任务性能几乎不变，多数下游任务准确率更高。

Conclusion: DWT为改进NLP应用提供了新途径，展示了其在嵌入压缩和语义信息保留方面的潜力。

Abstract: Wavelet transforms, a powerful mathematical tool, have been widely used in
different domains, including Signal and Image processing, to unravel intricate
patterns, enhance data representation, and extract meaningful features from
data. Tangible results from their application suggest that Wavelet transforms
can be applied to NLP capturing a variety of linguistic and semantic
properties. In this paper, we empirically leverage the application of Discrete
Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase
the capabilities of DWT in analyzing embedding representations at different
levels of resolution and compressing them while maintaining their overall
quality. We assess the effectiveness of DWT embeddings on semantic similarity
tasks to show how DWT can be used to consolidate important semantic information
in an embedding vector. We show the efficacy of the proposed paradigm using
different embedding models, including large language models, on downstream
tasks. Our results show that DWT can reduce the dimensionality of embeddings by
50-93% with almost no change in performance for semantic similarity tasks,
while achieving superior accuracy in most downstream tasks. Our findings pave
the way for applying DWT to improve NLP applications.

</details>


### [9] [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)
*Bryce Anderson,Riley Galpin,Tom S. Juzek*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）对人类语言使用的影响，发现2022年后人类口语中与LLM相关的词汇使用显著增加。


<details>
  <summary>Details</summary>
Motivation: 探究LLM是否正在改变人类语言系统本身，而不仅仅是作为文本生成工具。

Method: 构建了一个包含2210万单词的数据集，分析ChatGPT发布前后词汇使用趋势。

Result: 2022年后，与LLM相关的词汇使用显著增加，而基线同义词无明显变化。

Conclusion: LLM可能正在引发人类语言使用的显著变化，但这是自然语言演变还是AI驱动的变化尚不明确。

Abstract: In recent years, written language, particularly in science and education, has
undergone remarkable shifts in word usage. These changes are widely attributed
to the growing influence of Large Language Models (LLMs), which frequently rely
on a distinct lexical style. Divergences between model output and target
audience norms can be viewed as a form of misalignment. While these shifts are
often linked to using Artificial Intelligence (AI) directly as a tool to
generate text, it remains unclear whether the changes reflect broader changes
in the human language system itself. To explore this question, we constructed a
dataset of 22.1 million words from unscripted spoken language drawn from
conversational science and technology podcasts. We analyzed lexical trends
before and after ChatGPT's release in 2022, focusing on commonly LLM-associated
words. Our results show a moderate yet significant increase in the usage of
these words post-2022, suggesting a convergence between human word choices and
LLM-associated patterns. In contrast, baseline synonym words exhibit no
significant directional shift. Given the short time frame and the number of
words affected, this may indicate the onset of a remarkable shift in language
use. Whether this represents natural language change or a novel shift driven by
AI exposure remains an open question. Similarly, although the shifts may stem
from broader adoption patterns, it may also be that upstream training
misalignments ultimately contribute to changes in human language use. These
findings parallel ethical concerns that misaligned models may shape social and
moral beliefs.

</details>


### [10] [Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering](https://arxiv.org/abs/2508.00285)
*Peixian Li,Yu Tian,Ruiqi Tu,Chengkai Wu,Jingjing Ren,Jingsong Li*

Main category: cs.CL

TL;DR: 该研究提出了一种病因感知注意力引导框架，通过结构化临床推理提升大语言模型在复杂临床场景中的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在医学文本理解和生成方面表现出色，但其在复杂临床场景中的诊断可靠性仍有限。研究旨在提升其诊断准确性和临床推理能力。

Method: 构建基于权威临床指南的临床推理支架（CRS），开发病因感知头识别算法，并通过推理引导的参数高效微调将病因推理线索嵌入输入表示。

Result: 在一致性诊断队列中，框架将平均诊断准确率提升15.65%，推理聚焦分数提升31.6%。外部验证进一步证实其有效性。

Conclusion: 该框架通过将模型注意力与结构化CRS对齐，为构建更可解释和可靠的AI诊断系统提供了实用方法。

Abstract: Objective: Large Language Models (LLMs) demonstrate significant capabilities
in medical text understanding and generation. However, their diagnostic
reliability in complex clinical scenarios remains limited. This study aims to
enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We
propose an Etiology-Aware Attention Steering Framework to integrate structured
clinical reasoning into LLM-based diagnosis. Specifically, we first construct
Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines
for three representative acute abdominal emergencies: acute appendicitis, acute
pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head
Identification algorithm to pinpoint attention heads crucial for the model's
etiology reasoning. To ensure reliable clinical reasoning alignment, we
introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds
etiological reasoning cues into input representations and steers the selected
Etiology-Aware Heads toward critical information through a Reasoning-Guided
Loss function. Result: On the Consistent Diagnosis Cohort, our framework
improves average diagnostic accuracy by 15.65% and boosts the average Reasoning
Focus Score by 31.6% over baselines. External validation on the Discrepant
Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic
accuracy. Further assessments via Reasoning Attention Frequency indicate that
our models exhibit enhanced reliability when faced with real-world complex
scenarios. Conclusion: This study presents a practical and effective approach
to enhance clinical reasoning in LLM-based diagnosis. By aligning model
attention with structured CRS, the proposed framework offers a promising
paradigm for building more interpretable and reliable AI diagnostic systems in
complex clinical settings.

</details>


### [11] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

TL;DR: 论文系统评估了LLMs的优化技术（如剪枝、量化和标记丢弃）在长上下文场景中的效果，揭示了组合优化对大型模型的负面影响，并强调了系统级分析与任务特定指标结合的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在NLP任务中表现出色，但其资源需求和有限上下文窗口问题尚未得到充分解决，尤其是在长上下文场景中。

Method: 研究通过系统化基准测试，分析内存使用、延迟和吞吐量，并评估优化方法对文本生成质量的影响，包括单独和组合优化方法的测试。

Result: 实验发现，组合优化方法可能因累积近似误差对大型模型产生负面影响，且仅依赖F1分数会掩盖问答任务中的精度-召回权衡。

Conclusion: 通过结合系统级分析和任务特定指标，研究为LLM实践者和研究者提供了平衡效率、准确性和可扩展性的指导。

Abstract: Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


### [12] [Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment](https://arxiv.org/abs/2508.00332)
*Kaiyan Zhao,Zhongtao Miao,Yoshimasa Tsuruoka*

Main category: cs.CL

TL;DR: MCSEO提出了一种通过细粒度对象-短语对齐增强多模态句子嵌入的方法，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 多模态句子嵌入模型训练中使用的图像-标题对常包含噪声，影响模型性能。

Method: MCSEO利用分割和对象检测模型提取对象-短语对，优化对比学习目标。

Result: 在STS任务中，MCSEO在不同骨干模型上均优于基线方法。

Conclusion: 精确的对象-短语对齐对多模态表示学习至关重要。

Abstract: Multimodal sentence embedding models typically leverage image-caption pairs
in addition to textual data during training. However, such pairs often contain
noise, including redundant or irrelevant information on either the image or
caption side. To mitigate this issue, we propose MCSEO, a method that enhances
multimodal sentence embeddings by incorporating fine-grained object-phrase
alignment alongside traditional image-caption alignment. Specifically, MCSEO
utilizes existing segmentation and object detection models to extract accurate
object-phrase pairs, which are then used to optimize a contrastive learning
objective tailored to object-phrase correspondence. Experimental results on
semantic textual similarity (STS) tasks across different backbone models
demonstrate that MCSEO consistently outperforms strong baselines, highlighting
the significance of precise object-phrase alignment in multimodal
representation learning.

</details>


### [13] [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
*Keer Lu,Chong Chen,Bin Cui,Huang Leng,Wentao Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种名为AdaPlan的自适应全局规划代理范式，并基于此开发了PilotRL框架，通过渐进式强化学习提升LLM代理在长期决策任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有代理范式（如ReAct）在复杂任务中表现受限，且监督微调方法导致模型泛化能力不足。

Method: 提出AdaPlan范式，结合全局规划与执行；开发PilotRL框架，分阶段优化规划与执行协调。

Result: 实验显示，PilotRL在LLaMA3.1-8B-Instruct上表现优于GPT-4o和GPT-4o-mini。

Conclusion: AdaPlan和PilotRL有效提升了代理在长期决策任务中的性能。

Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling
agent-oriented tasks. Despite their potential, existing work faces challenges
when deploying LLMs in agent-based environments. The widely adopted agent
paradigm ReAct centers on integrating single-step reasoning with immediate
action execution, which limits its effectiveness in complex tasks requiring
long-term strategic planning. Furthermore, the coordination between the planner
and executor during problem-solving is also a critical factor to consider in
agent design. Additionally, current approaches predominantly rely on supervised
fine-tuning, which often leads models to memorize established task completion
trajectories, thereby restricting their generalization ability when confronted
with novel problem contexts. To address these challenges, we introduce an
adaptive global plan-based agent paradigm AdaPlan, aiming to synergize
high-level explicit guidance with execution to support effective long-horizon
decision-making. Based on the proposed paradigm, we further put forward
PilotRL, a global planning-guided training framework for LLM agents driven by
progressive reinforcement learning. We first develop the model's ability to
follow explicit guidance from global plans when addressing agent tasks.
Subsequently, based on this foundation, we focus on optimizing the quality of
generated plans. Finally, we conduct joint optimization of the model's planning
and execution coordination. Experiments indicate that PilotRL could achieve
state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing
closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%
comparing to GPT-4o-mini at a comparable parameter scale.

</details>


### [14] [Lucy: edgerunning agentic web search on mobile with machine generated task vectors](https://arxiv.org/abs/2508.00360)
*Alan Dao,Dinh Bach Vu,Alex Nguyen,Norapat Buppodom*

Main category: cs.CL

TL;DR: 该论文提出了一种新范式，将小语言模型（SLM）的内部推理过程视为动态任务向量机，通过优化这一机制，使小模型在知识密集型任务中表现媲美大模型。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLM）在知识密集型任务中表现受限，传统方法将推理视为固定或启发式过程，缺乏动态性。

Method: 提出将模型内部推理（由<think>和</think>标记界定）视为动态任务向量机，通过RLVR优化这一机制，并训练了一个代理式网络搜索模型Lucy。

Result: Lucy（1.7B参数）在SimpleQA基准测试中达到78.3%准确率，表现与更大模型（如DeepSeek-V3）相当。

Conclusion: 通过结构化、自构建的任务推理机制，小模型可以媲美大模型的性能。

Abstract: Small language models (SLMs) are inherently limited in knowledge-intensive
tasks due to their constrained capacity. While test-time computation offers a
path to enhanced performance, most approaches treat reasoning as a fixed or
heuristic process. In this work, we propose a new paradigm: viewing the model's
internal reasoning, delimited by <think> and </think> tags, as a dynamic task
vector machine. Rather than treating the content inside these tags as a mere
trace of thought, we interpret the generation process itself as a mechanism
through which the model \textbf{constructs and refines its own task vectors} on
the fly. We developed a method to optimize this dynamic task vector machine
through RLVR and successfully trained an agentic web-search model. We present
Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with
MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing
on par with much larger models such as DeepSeek-V3. This demonstrates that
small models can rival large ones when equipped with structured,
self-constructed task reasoning.

</details>


### [15] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
*Jiyu Chen,Poh Seng Lim,Shuang Peng,Daxiong Luo,JungHau Foo,Yap Deep,Timothy Lee Jun Jie,Kelvin Teh Kae Wen,Fan Yang,Danyu Feng,Hao-Yun Chen,Peng-Wen Chen,Fangyuan Li,Xiaoxin Chen,Wong Wai Mun*

Main category: cs.CL

TL;DR: EdgeInfinite-Instruct通过分段监督微调（S-SFT）和细粒度后训练量化（PTQ）优化了Transformer模型在边缘设备上的部署，提升了长序列任务效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer大模型在资源受限的边缘设备上部署时面临的计算复杂性和内存需求问题，特别是长序列任务中的KV缓存和首令牌时间（TTFT）挑战。

Method: 提出EdgeInfinite-Instruct，采用分段监督微调（S-SFT）策略，结合细粒度后训练量化（PTQ）和固定形状计算图优化。

Result: 在长上下文基准测试和实际移动任务中，提升了领域特定性能，同时保持了边缘NPU设备的高效性。

Conclusion: EdgeInfinite-Instruct通过定制化优化，有效平衡了计算、内存和性能，适用于边缘设备的长序列任务。

Abstract: Deploying Transformer-based large language models (LLMs) on
resource-constrained edge devices for long-sequence tasks remains challenging
due to the quadratic time complexity of self-attention and growing Key-Value
(KV) cache demands. While existing KV cache optimizations improve memory
efficiency, they often fail to reduce time to first token (TTFT) and may
degrade performance through token pruning. Alternative sequence modeling
architectures address some of these limitations, but typically require full
retraining and lack infrastructure support. EdgeInfinite offers an efficient
solution by fine-tuning only a small subset of parameters, maintaining quality
while reducing both computational and memory costs, including improved TTFT.
However, its instruction-following ability is limited, and it lacks
mobile-specific optimizations. To address these issues, we propose
EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning
(S-SFT) strategy tailored to long-sequence tasks such as summarization and
question answering. We further optimized EdgeInfinite-Instruct for efficient
deployment on edge NPUs by employing fine-grained post-training quantization
(PTQ) to reduce computational demands while maintaining accuracy, and by
implementing a fixed-shape computation graph that balances memory usage and
on-device efficiency through scenario-specific customization of input token and
cache sizes. Experiments on long-context benchmarks and real-world mobile tasks
show that our approach improves domain-specific performance while maintaining
efficiency on NPU-accelerated edge devices.

</details>


### [16] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
*Dingzirui Wang,Xuangliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文研究了上下文学习（ICL）中演示无效的原因，提出了一种基于梯度流的演示选择方法GradS，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设ICL中的演示总是有效，但实际上许多演示无效，本文旨在探究其原因并提出改进方法。

Method: 通过梯度流和线性自注意力模型分析演示无效的原因，并提出基于梯度流大小的演示选择方法GradS。

Result: 实验表明，随着模型层数增加，演示有效性差异放大，GradS在多个数据集上平均提升6.8%。

Conclusion: GradS通过考虑模型已学习信息，显著提升了演示选择的有效性，为ICL提供了新思路。

Abstract: Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

</details>


### [17] [SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation](https://arxiv.org/abs/2508.00390)
*Hengxing Cai,Jinhan Dong,Yijie Rao,Jingcheng Deng,Jingjun Tan,Qien Chen,Haidong Wang,Zhen Wang,Shiyu Huang,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

TL;DR: 提出了一种名为SA-GCS的新训练框架，通过结合课程学习与强化学习，优化无人机视觉语言导航任务的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在无人机视觉语言导航任务中存在训练数据利用效率低、收敛慢以及对训练样本难度差异考虑不足的问题。

Method: 提出SA-GCS框架，包含语义感知难度评估器（SA-DE）和高斯课程调度器（GCS），动态调整训练样本分布。

Result: 在CityNav基准测试中，SA-GCS在所有指标上均优于基线方法，收敛更快且更稳定，且适用于不同规模的模型。

Conclusion: SA-GCS显著提升了训练效率和模型性能，具有鲁棒性和可扩展性。

Abstract: Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable
agents to accurately localize targets and plan flight paths in complex
environments based on natural language instructions, with broad applications in
intelligent inspection, disaster rescue, and urban monitoring. Recent progress
in Vision-Language Models (VLMs) has provided strong semantic understanding for
this task, while reinforcement learning (RL) has emerged as a promising
post-training strategy to further improve generalization. However, existing RL
methods often suffer from inefficient use of training data, slow convergence,
and insufficient consideration of the difficulty variation among training
samples, which limits further performance improvement. To address these
challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling
(SA-GCS)}, a novel training framework that systematically integrates Curriculum
Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator
(SA-DE) to quantify the complexity of training samples and a Gaussian
Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution,
enabling a smooth progression from easy to challenging tasks. This design
significantly improves training efficiency, accelerates convergence, and
enhances overall model performance. Extensive experiments on the CityNav
benchmark demonstrate that SA-GCS consistently outperforms strong baselines
across all metrics, achieves faster and more stable convergence, and
generalizes well across models of different scales, highlighting its robustness
and scalability. The implementation of our approach is publicly available.

</details>


### [18] [Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding](https://arxiv.org/abs/2508.00420)
*Rana Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 本文探讨了离散小波变换（DWT）在词和句子嵌入中的应用，提出了一种结合DWT和离散余弦变换（DCT）的非参数化模型，用于压缩句子信息，并在下游任务中取得了优于原始嵌入的效果。


<details>
  <summary>Details</summary>
Motivation: 小波技术已在图像和信号处理中表现出色，本文旨在探索其在自然语言处理（NLP）中的潜力，尤其是如何通过DWT优化词和句子嵌入。

Method: 使用DWT对词向量进行降维和信息整合，并结合DCT提出一种非参数化模型，用于压缩句子信息为固定大小的向量。

Result: 实验表明，该方法在下游任务中表现优异，部分任务甚至优于原始嵌入。

Conclusion: 小波变换在NLP中具有潜力，尤其是通过DWT和DCT结合的方法，能够高效压缩信息并提升模型性能。

Abstract: Wavelets have emerged as a cutting edge technology in a number of fields.
Concrete results of their application in Image and Signal processing suggest
that wavelets can be effectively applied to Natural Language Processing (NLP)
tasks that capture a variety of linguistic properties. In this paper, we
leverage the power of applying Discrete Wavelet Transforms (DWT) to word and
sentence embeddings. We first evaluate, intrinsically and extrinsically, how
wavelets can effectively be used to consolidate important information in a word
vector while reducing its dimensionality. We further combine DWT with Discrete
Cosine Transform (DCT) to propose a non-parameterized model that compresses a
sentence with a dense amount of information in a fixed size vector based on
locally varying word features. We show the efficacy of the proposed paradigm on
downstream applications models yielding comparable and even superior (in some
tasks) results to original embeddings.

</details>


### [19] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
*Minghao Guo,Xi Zhu,Jingyuan Huang,Kai Mei,Yongfeng Zhang*

Main category: cs.CL

TL;DR: ReaGAN提出了一种基于代理的框架，通过节点级自主决策和检索增强生成（RAG）解决GNN中节点信息不平衡和全局语义关系忽略的问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNN的固定聚合机制无法处理节点信息不平衡和忽略全局语义关系的问题。

Method: ReaGAN为每个节点赋予代理能力，使其能基于内部记忆自主规划行动，并结合RAG技术访问全局语义相关内容。

Result: ReaGAN在少样本情境下表现出色，无需微调即可利用冻结的LLM骨干网络实现竞争性能。

Conclusion: ReaGAN展示了代理规划和局部-全局检索在图学习中的潜力。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

</details>


### [20] [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)
*Yuqi Tang,Kehua Feng,Yunfeng Wang,Zhiwen Chen,Chengfei Lv,Gang Yu,Qiang Zhang,Keyan Ding*

Main category: cs.CL

TL;DR: 提出了一种高效的多轮对话评估方法，通过将多个LLM评委的偏好知识聚合到一个模型中，显著降低了评估成本，同时保持了多样性反馈的优势。


<details>
  <summary>Details</summary>
Motivation: 当前依赖单一LLM作为评委的方法存在偏见，影响评估结果的可靠性和一致性。多评委方法虽有效但计算开销大。

Method: 提出一种高效的多轮对话评估器，将多个LLM评委的偏好知识聚合到单一模型中。

Result: 在七个对话评估基准测试中表现优于现有基线，展示了其高效性和鲁棒性。

Conclusion: 该方法在降低评估成本的同时，保持了多评委反馈的优势，适用于快速灵活的对话质量评估。

Abstract: Evaluating the conversational abilities of large language models (LLMs)
remains a challenging task. Current mainstream approaches primarily rely on the
``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator
to assess dialogue quality. However, such methods often suffer from various
biases, which undermine the reliability and consistency of the evaluation
results. To mitigate these biases, recent methods employ multiple LLMs as
judges and aggregate their judgments to select the optimal assessment. Although
effective, this multi-judge approach incurs significant computational overhead
during inference. In this paper, we propose an efficient multi-turn dialogue
evaluator that captures the collective wisdom of multiple LLM judges by
aggregating their preference knowledge into a single model. Our approach
preserves the advantages of diverse multi-judge feedback while drastically
reducing the evaluation cost, enabling fast and flexible dialogue quality
assessment. Extensive experiments on seven single rating and pairwise
comparison dialogue evaluation benchmarks demonstrate that our method
outperforms existing baselines across diverse scenarios, showcasing its
efficiency and robustness.

</details>


### [21] [GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts](https://arxiv.org/abs/2508.00476)
*Jeongwoo Kang,Markarit Vartampetian,Felix Herron,Yongxin Zhou,Diandra Fabre,Gabriela Gonzalez-Saez*

Main category: cs.CL

TL;DR: GETALP团队在SIGDial 2025的自动会议纪要任务中提交了基于检索增强生成（RAG）和抽象意义表示（AMR）的系统，结果显示AMR显著提升了部分问题的回答质量。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过结合RAG和AMR技术提升会议纪要中问答任务的性能，特别是针对涉及区分不同参与者的问题。

Method: 提出了三种结合RAG和AMR的系统，用于基于会议转录的问答任务。

Result: AMR显著提升了约35%问题的回答质量，并在区分参与者的问题（如“谁”问题）中表现突出。

Conclusion: 结合RAG和AMR的方法在会议纪要问答任务中具有潜力，尤其是在处理复杂问题时。

Abstract: This paper documents GETALP's submission to the Third Run of the Automatic
Minuting Shared Task at SIGDial 2025. We participated in Task B:
question-answering based on meeting transcripts. Our method is based on a
retrieval augmented generation (RAG) system and Abstract Meaning
Representations (AMR). We propose three systems combining these two approaches.
Our results show that incorporating AMR leads to high-quality responses for
approximately 35% of the questions and provides notable improvements in
answering questions that involve distinguishing between different participants
(e.g., who questions).

</details>


### [22] [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)
*Yixuan Tang,Jincheng Wang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: 论文提出了半真检测任务，并开发了PolitiFact-Hidden基准和TRACER框架，用于识别因遗漏关键信息而导致的误导性声明。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统无法处理半真声明（部分正确但因遗漏关键信息而误导），需要新方法检测此类情况。

Method: 提出TRACER框架，通过证据对齐、推断隐含意图和评估隐藏内容的影响来识别遗漏型误导信息。

Result: TRACER显著提升了半真分类的F1分数（最高16分），并可与现有事实核查系统集成。

Conclusion: 建模遗漏信息对可信事实核查至关重要，TRACER为此提供了有效解决方案。

Abstract: Fact verification systems typically assess whether a claim is supported by
retrieved evidence, assuming that truthfulness depends solely on what is
stated. However, many real-world claims are half-truths, factually correct yet
misleading due to the omission of critical context. Existing models struggle
with such cases, as they are not designed to reason about what is left unsaid.
We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a
new benchmark with 15k political claims annotated with sentence-level evidence
alignment and inferred claim intent. To address this challenge, we present
TRACER, a modular re-assessment framework that identifies omission-based
misinformation by aligning evidence, inferring implied intent, and estimating
the causal impact of hidden content. TRACER can be integrated into existing
fact-checking pipelines and consistently improves performance across multiple
strong baselines. Notably, it boosts Half-True classification F1 by up to 16
points, highlighting the importance of modeling omissions for trustworthy fact
verification.

</details>


### [23] [EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522)
*Jiaxin Deng,Qingcheng Zhu,Junbiao Pang,Linlin Yang,Zhongqian Fu,Baochang Zhang*

Main category: cs.CL

TL;DR: 论文提出Flat-LoRA和EFlat-LoRA，通过寻找平坦最小值提升LoRA的泛化能力，实验证明其优于LoRA和全微调。


<details>
  <summary>Details</summary>
Motivation: 探索LoRA的表达能力与泛化能力之间的关系，填补现有方法在平坦最小值与泛化能力联系上的研究空白。

Method: 提出Flat-LoRA和EFlat-LoRA，理论证明全参数空间扰动可转移到低秩子空间，避免多矩阵干扰。

Result: EFlat-LoRA在GLUE和视觉语言模型上表现优于LoRA和全微调，性能提升显著。

Conclusion: LoRA的泛化能力与平坦性密切相关，EFlat-LoRA在效率和性能上均表现出色。

Abstract: Little research explores the correlation between the expressive ability and
generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware
Minimization (SAM) improves model generalization for both Convolutional Neural
Networks (CNNs) and Transformers by encouraging convergence to locally flat
minima. However, the connection between sharpness and generalization has not
been fully explored for LoRA due to the lack of tools to either empirically
seek flat minima or develop theoretical methods. In this work, we propose
Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for
LoRA. Concretely, we theoretically demonstrate that perturbations in the full
parameter space can be transferred to the low-rank subspace. This approach
eliminates the potential interference introduced by perturbations across
multiple matrices in the low-rank subspace. Our extensive experiments on large
language models and vision-language models demonstrate that EFlat-LoRA achieves
optimize efficiency comparable to that of LoRA while simultaneously attaining
comparable or even better performance. For example, on the GLUE dataset with
RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and
0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat
shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,
respectively. These empirical results also verify that the generalization of
LoRA is closely related to sharpness, which is omitted by previous methods.

</details>


### [24] [The Prosody of Emojis](https://arxiv.org/abs/2508.00537)
*Giulio Zhou,Tsz Kin Lam,Alexandra Birch,Barry Haddow*

Main category: cs.CL

TL;DR: 研究探讨了表情符号如何影响语音中的韵律实现，以及听者如何通过韵律线索理解表情符号的含义。


<details>
  <summary>Details</summary>
Motivation: 探索在文本交流中缺失的韵律特征（如音高、节奏、语调）如何通过表情符号在语音中体现。

Method: 通过结构化但开放式的生产和感知任务收集人类语音数据，直接分析韵律与表情符号的联系。

Result: 说话者会根据表情符号调整韵律，听者能通过韵律变化识别表情符号，语义差异越大，韵律差异越明显。

Conclusion: 表情符号可以作为韵律意图的有意义载体，揭示了其在数字媒介中的交际作用。

Abstract: Prosodic features such as pitch, timing, and intonation are central to spoken
communication, conveying emotion, intent, and discourse structure. In
text-based settings, where these cues are absent, emojis act as visual
surrogates that add affective and pragmatic nuance. This study examines how
emojis influence prosodic realisation in speech and how listeners interpret
prosodic cues to recover emoji meanings. Unlike previous work, we directly link
prosody and emoji by analysing actual human speech data, collected through
structured but open-ended production and perception tasks. This provides
empirical evidence of how emoji semantics shape spoken delivery and perception.
Results show that speakers adapt their prosody based on emoji cues, listeners
can often identify the intended emoji from prosodic variation alone, and
greater semantic differences between emojis correspond to increased prosodic
divergence. These findings suggest that emojis can act as meaningful carriers
of prosodic intent, offering insight into their communicative role in digitally
mediated contexts.

</details>


### [25] [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)
*Joonas Tapaninaho,Mourad Oussala*

Main category: cs.CL

TL;DR: 提出了一种名为PaPaformer的并行路径解码器-仅变压器架构，通过训练低维路径并组合成更大模型，显著减少训练时间和参数数量，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型训练需要大量计算资源和时间，即使是小型模型也需要多GPU和多天训练，亟需更高效的训练方法。

Method: 引入PaPaformer架构，通过并行训练低维路径并组合成更大模型，减少参数和训练时间。

Result: 该方法显著减少训练时间和参数数量，同时提高性能，并支持路径定制以适应特定任务需求。

Conclusion: PaPaformer为高效训练语言模型提供了新思路，具有减少资源消耗和提升灵活性的潜力。

Abstract: The training of modern large-language models requires an increasingly amount
of computation power and time. Even smaller variants, such as small-language
models (SLMs), take several days to train in the best-case scenarios, often
requiring multiple GPUs. This paper explores methods to train and evaluate
decoder-only transformer-based language models in hours instead of days/weeks.
We introduces \textit{PaPaformer}, a decoder-only transformer architecture
variant, whose lower-dimensional parallel paths are combined into larger model.
The paper shows that these lower-dimensional paths can be trained individually
with different types of training data and then combined into one larger model.
This method gives the option to reduce the total number of model parameters and
the training time with increasing performance. Moreover, the use of parallel
path structure opens interesting possibilities to customize paths to
accommodate specific task requirements.

</details>


### [26] [SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought](https://arxiv.org/abs/2508.00574)
*Jianwei Wang,Ziming Wu,Fuming Lai,Shaobing Lian,Ziqian Zeng*

Main category: cs.CL

TL;DR: SynAdapt 是一种高效的推理框架，通过生成合成的连续 CoT（CCoT）作为对齐目标，并集成难度分类器，显著提升了模型性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的连续 CoT（CCoT）方法存在间接微调、对齐不足或目标不一致的问题，影响了推理效率。

Method: 提出 SynAdapt 框架，生成合成的 CCoT 作为对齐目标，并引入难度分类器自适应处理难题。

Result: 在多个基准测试中，SynAdapt 实现了最佳的准确性与效率平衡。

Conclusion: SynAdapt 通过创新的 CCoT 生成和自适应推理，显著提升了模型性能与效率。

Abstract: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs
significant time costs due to the generation of discrete CoT tokens (DCoT).
Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT
methods are hampered by indirect fine-tuning, limited alignment, or
inconsistent targets. To overcome these limitations, we propose
\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,
\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and
effective alignment target for LLMs. This synthetic CCoT explicitly guides the
LLM to learn CCoT and derive accurate answers directly. Furthermore, relying
solely on CCoT is insufficient for solving hard questions. To address this,
\textit{SynAdapt} integrates a difficulty classifier that leverages both
question context and CCoT to identify hard questions. CCoT can effectively help
identify hard questions after some brief reasoning. We then adaptively prompt
the LLM to re-think these hard questions for improved performance. Extensive
experimental results across various benchmarks from different difficulty levels
strongly demonstrate the effectiveness of our method, achieving the best
accuracy-efficiency trade-off.

</details>


### [27] [NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts](https://arxiv.org/abs/2502.18148)
*Muhammad Farid Adilazuarda,Musa Izzanardi Wijanarko,Lucky Susanto,Khumaisa Nur'aini,Derry Wijaya,Alham Fikri Aji*

Main category: cs.CL

TL;DR: NusaAksara是一个针对印尼语言及其原始脚本的公共基准测试，涵盖文本和图像模态，任务多样，但现有NLP技术对其支持不足。


<details>
  <summary>Details</summary>
Motivation: 印尼语言和脚本丰富，但NLP研究多基于罗马化文本，缺乏对原始脚本的支持。

Method: 构建包含8种脚本的基准测试，涵盖多种任务，数据由专家严格构建。

Result: 大多数NLP技术对印尼本地脚本支持不足，性能接近零。

Conclusion: NusaAksara填补了印尼语言原始脚本的NLP研究空白，凸显技术改进需求。

Abstract: Indonesia is rich in languages and scripts. However, most NLP progress has
been made using romanized text. In this paper, we present NusaAksara, a novel
public benchmark for Indonesian languages that includes their original scripts.
Our benchmark covers both text and image modalities and encompasses diverse
tasks such as image segmentation, OCR, transliteration, translation, and
language identification. Our data is constructed by human experts through
rigorous steps. NusaAksara covers 8 scripts across 7 languages, including
low-resource languages not commonly seen in NLP benchmarks. Although
unsupported by Unicode, the Lampung script is included in this dataset. We
benchmark our data across several models, from LLMs and VLMs such as GPT-4o,
Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and
show that most NLP technologies cannot handle Indonesia's local scripts, with
many achieving near-zero performance.

</details>


### [28] [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)
*Mingruo Yuan,Shuyi Zhang,Ben Kao*

Main category: cs.CL

TL;DR: CRUX框架通过结合上下文忠实性和一致性，提出两种新指标改进LLM的置信度估计，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM置信度估计方法忽略了响应与上下文信息的相关性，而CRUX旨在填补这一空白。

Method: CRUX通过上下文熵减少和统一一致性检查两种新指标，整合上下文忠实性和一致性。

Result: 在多个数据集上，CRUX的AUROC表现优于现有基线。

Conclusion: CRUX为LLM置信度估计提供了更可靠的框架，适用于安全关键应用。

Abstract: Accurate confidence estimation is essential for trustworthy large language
models (LLMs) systems, as it empowers the user to determine when to trust
outputs and enables reliable deployment in safety-critical applications.
Current confidence estimation methods for LLMs neglect the relevance between
responses and contextual information, a crucial factor in output quality
evaluation, particularly in scenarios where background knowledge is provided.
To bridge this gap, we propose CRUX (Context-aware entropy Reduction and
Unified consistency eXamination), the first framework that integrates context
faithfulness and consistency for confidence estimation via two novel metrics.
First, contextual entropy reduction represents data uncertainty with the
information gain through contrastive sampling with and without context. Second,
unified consistency examination captures potential model uncertainty through
the global consistency of the generated answers with and without context.
Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two
domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,
achieving the highest AUROC than existing baselines.

</details>


### [29] [GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language](https://arxiv.org/abs/2508.00605)
*Farhana Haque,Md. Abdur Rahman,Sumon Ahmed*

Main category: cs.CL

TL;DR: 提出了一种基于图卷积网络（GCN）的混合主题模型GHTM，用于孟加拉语文本的主题建模，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语因其形态复杂性和资源匮乏，主题建模研究较少，需要一种更有效的方法。

Method: 使用GCN生成语义丰富的文档嵌入，再通过非负矩阵分解（NMF）提取主题。

Result: GHTM在主题一致性和多样性上优于传统和现代方法。

Conclusion: GHTM为孟加拉语主题建模提供了有效解决方案，并引入了新的数据集NCTBText。

Abstract: Topic modeling is a Natural Language Processing (NLP) technique that is used
to identify latent themes and extract topics from text corpora by grouping
similar documents based on their most significant keywords. Although widely
researched in English, topic modeling remains understudied in Bengali due to
its morphological complexity, lack of adequate resources and initiatives. In
this contribution, a novel Graph Convolutional Network (GCN) based model called
GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input
vectors of documents as nodes in the graph, which GCN uses to produce
semantically rich embeddings. The embeddings are then decomposed using
Non-negative Matrix Factorization (NMF) to get the topical representations of
the underlying themes of the text corpus. This study compares the proposed
model against a wide range of Bengali topic modeling techniques, from
traditional methods such as LDA, LSA, and NMF to contemporary frameworks such
as BERTopic and Top2Vec on three Bengali datasets. The experimental results
demonstrate the effectiveness of the proposed model by outperforming other
models in topic coherence and diversity. In addition, we introduce a novel
Bengali dataset called "NCTBText" sourced from Bengali textbook materials to
enrich and diversify the predominantly newspaper-centric Bengali corpora.

</details>


### [30] [Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?](https://arxiv.org/abs/2508.00614)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: 研究发现，威胁或奖励AI模型对整体性能无显著影响，但提示变化可能对个别问题产生显著影响。


<details>
  <summary>Details</summary>
Motivation: 验证两种常见提示策略（威胁和奖励）对AI模型性能的实际影响，以帮助商业、教育和政策领导者理解AI工作的技术细节。

Method: 在GPQA和MMLU-Pro基准上测试威胁和奖励策略对模型性能的影响。

Result: 威胁或奖励对整体性能无显著影响，但提示变化可能显著影响个别问题的表现。

Conclusion: 简单提示策略可能不如预期有效，尤其是对复杂问题，但个别问题可能因提示变化而有显著差异。

Abstract: This is the third in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate two commonly held
prompting beliefs: a) offering to tip the AI model and b) threatening the AI
model. Tipping was a commonly shared tactic for improving AI performance and
threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,
8:20) who observed that 'models tend to do better if you threaten them,' a
claim we subject to empirical testing here. We evaluate model performance on
GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).
  We demonstrate two things:
  - Threatening or tipping a model generally has no significant effect on
benchmark performance.
  - Prompt variations can significantly affect performance on a per-question
level. However, it is hard to know in advance whether a particular prompting
approach will help or harm the LLM's ability to answer any particular question.
  Taken together, this suggests that simple prompting variations might not be
as effective as previously assumed, especially for difficult problems. However,
as reported previously (Meincke et al. 2025a), prompting approaches can yield
significantly different results for individual questions.

</details>


### [31] [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)
*Shantanu Thorat,Andrew Caines*

Main category: cs.CL

TL;DR: 论文指出现有AIG文本检测器在真实场景中表现不佳，提出DACTYL数据集和两种优化方法（BCE和DXO），发现DXO在泛化性上更优。


<details>
  <summary>Details</summary>
Motivation: 现有AIG文本检测器在内部测试中表现良好，但在真实场景中不够鲁棒，需改进。

Method: 引入DACTYL数据集（专注于单次/少量样本生成和CPT模型文本），并比较BCE和DXO两种优化方法。

Result: DXO优化方法在OOD文本上表现更优，泛化性更强。

Conclusion: AIG文本检测器需改进，DXO方法在泛化性上优于BCE。

Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings
despite succeeding in internal testing, suggesting that they may not be robust
enough. We rigorously examine the machine-learning procedure to build these
detectors to address this. Most current AIG text detection datasets focus on
zero-shot generations, but little work has been done on few-shot or one-shot
generations, where LLMs are given human texts as an example. In response, we
introduce the Diverse Adversarial Corpus of Texts Yielded from Language models
(DACTYL), a challenging AIG text detection dataset focusing on
one-shot/few-shot generations. We also include texts from domain-specific
continued-pre-trained (CPT) language models, where we fully train all
parameters using a memory-efficient optimization approach. Many existing AIG
text detectors struggle significantly on our dataset, indicating a potential
vulnerability to one-shot/few-shot and CPT-generated texts. We also train our
own classifiers using two approaches: standard binary cross-entropy (BCE)
optimization and a more recent approach, deep X-risk optimization (DXO). While
BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL
test set, the latter excels on out-of-distribution (OOD) texts. In our mock
deployment scenario in student essay detection with an OOD student essay
dataset, the best DXO classifier outscored the best BCE-trained classifier by
50.56 macro-F1 score points at the lowest false positive rates for both. Our
results indicate that DXO classifiers generalize better without overfitting to
the test set. Our experiments highlight several areas of improvement for AIG
text detectors.

</details>


### [32] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: 本文系统综述了大型语言模型（LLMs）在医学推理领域的发展，提出了训练时和测试时的增强技术分类，并分析了其在多模态数据和临床应用中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在医学中的应用缺乏系统性、透明性和可验证性推理能力，这是临床实践的核心需求。

Method: 提出了一种分类法，包括训练时策略（如监督微调）和测试时机制（如提示工程），并分析了60项关键研究。

Result: 研究发现需要解决忠实性与合理性之间的差距，并推动多模态推理的发展。

Conclusion: 未来方向包括构建高效、稳健且社会技术责任强的医学AI。

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

</details>


### [33] [MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language](https://arxiv.org/abs/2508.00673)
*Farhan Farsi,Farnaz Aghababaloo,Shahriar Shariati Motlagh,Parsa Ghofrani,MohammadAli SadraeiJavaheri,Shayan Bali,Amirhossein Shabani,Farbod Bijary,Ghazal Zamaninejad,AmirMohammad Salehoof,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 论文提出19个波斯语和伊朗文化相关的评估数据集，用于填补大语言模型在非西方文化和语言中的评估空白。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估资源主要集中在英语和西方文化，缺乏对其他语言和文化背景的评估。

Method: 设计了19个波斯语和伊朗文化相关的评估数据集，涵盖法律、语法、习语和考试等内容，并对41个大语言模型进行了测试。

Result: 通过新数据集对41个模型进行了基准测试，填补了文化和语言评估的空白。

Conclusion: 研究为波斯语和伊朗文化的语言模型评估提供了新资源，强调了多语言和文化背景评估的重要性。

Abstract: As large language models (LLMs) become increasingly embedded in our daily
lives, evaluating their quality and reliability across diverse contexts has
become essential. While comprehensive benchmarks exist for assessing LLM
performance in English, there remains a significant gap in evaluation resources
for other languages. Moreover, because most LLMs are trained primarily on data
rooted in European and American cultures, they often lack familiarity with
non-Western cultural contexts. To address this limitation, our study focuses on
the Persian language and Iranian culture. We introduce 19 new evaluation
datasets specifically designed to assess LLMs on topics such as Iranian law,
Persian grammar, Persian idioms, and university entrance exams. Using these
datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing
cultural and linguistic evaluation gap in the field.

</details>


### [34] [Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier](https://arxiv.org/abs/2508.00675)
*Gleb Schmidt,Johannes Römisch,Mariia Halchynska,Svetlana Gorovaia,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 论文提出了一种基于预训练语言模型和双向LSTM的序列句子对分类器（SSPC），用于检测文档中的风格变化，并在PAN 2025任务中取得了优于基线的表现。


<details>
  <summary>Details</summary>
Motivation: 风格变化检测是计算作者分析中最重要且具挑战性的问题之一，尤其是在句子级别的细粒度检测。

Method: 使用预训练语言模型（PLM）生成句子表示，再通过双向LSTM（BiLSTM）进行上下文建模，最后用多层感知机预测相邻句子的风格变化。

Result: 模型在PAN-2025测试数据集上表现优异，宏F1分数分别为0.923（EASY）、0.828（MEDIUM）和0.724（HARD），优于随机基线和零样本性能。

Conclusion: 该方法通过轻量级设计有效利用了上下文信息，解决了短句子风格变化检测的挑战。

Abstract: Style change detection - identifying the points in a document where writing
style shifts - remains one of the most important and challenging problems in
computational authorship analysis. At PAN 2025, the shared task challenges
participants to detect style switches at the most fine-grained level:
individual sentences. The task spans three datasets, each designed with
controlled and increasing thematic variety within documents. We propose to
address this problem by modeling the content of each problem instance - that
is, a series of sentences - as a whole, using a Sequential Sentence Pair
Classifier (SSPC). The architecture leverages a pre-trained language model
(PLM) to obtain representations of individual sentences, which are then fed
into a bidirectional LSTM (BiLSTM) to contextualize them within the document.
The BiLSTM-produced vectors of adjacent sentences are concatenated and passed
to a multi-layer perceptron for prediction per adjacency. Building on the work
of previous PAN participants classical text segmentation, the approach is
relatively conservative and lightweight. Nevertheless, it proves effective in
leveraging contextual information and addressing what is arguably the most
challenging aspect of this year's shared task: the notorious problem of
"stylistically shallow", short sentences that are prevalent in the proposed
benchmark data. Evaluated on the official PAN-2025 test datasets, the model
achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,
and HARD data, respectively, outperforming not only the official random
baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot
performance.

</details>


### [35] [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)
*Shubham Kumar Nigam,Tanmay Dubey,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: TraceRetriever 是一种法律先例检索系统，通过结合 BM25、向量数据库和 Cross-Encoder 模型，提取修辞显著片段，解决了传统检索方法在复杂法律文档中的不足。


<details>
  <summary>Details</summary>
Motivation: 法律先例检索在普通法体系中至关重要，但传统方法难以应对日益复杂的法律文档。

Method: 整合 BM25、向量数据库和 Cross-Encoder 模型，使用 Reciprocal Rank Fusion 结合结果，并通过 Hierarchical BiLSTM CRF 分类器生成修辞标注。

Result: 在 IL-PCR 和 COLIEE 2025 数据集上验证了其有效性，能够应对文档量增长和部分案例知识的限制。

Conclusion: TraceRetriever 为法律研究提供了可靠且可扩展的先例检索基础。

Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.

</details>


### [36] [Better Call Claude: Can LLMs Detect Changes of Writing Style?](https://arxiv.org/abs/2508.00680)
*Johannes Römisch,Svetlana Gorovaia,Mariia Halchynska,Gleb Schmidt,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在句子级风格变化检测任务中的零样本表现，发现其对写作风格变化敏感，并建立了具有挑战性的基线。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在作者分析中最具挑战性的任务之一——句子级风格变化检测中的表现。

Method: 在PAN~2024和2025数据集上对四种LLMs进行基准测试，分析其敏感性和准确性。

Result: LLMs对写作风格变化敏感，其表现优于PAN竞赛的基线，且对内容无关的风格信号更敏感。

Conclusion: 最新一代LLMs在风格检测任务中表现出色，为未来研究提供了新方向。

Abstract: This article explores the zero-shot performance of state-of-the-art large
language models (LLMs) on one of the most challenging tasks in authorship
analysis: sentence-level style change detection. Benchmarking four LLMs on the
official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we
present several observations. First, state-of-the-art generative models are
sensitive to variations in writing style - even at the granular level of
individual sentences. Second, their accuracy establishes a challenging baseline
for the task, outperforming suggested baselines of the PAN competition.
Finally, we explore the influence of semantics on model predictions and present
evidence suggesting that the latest generation of LLMs may be more sensitive to
content-independent and purely stylistic signals than previously reported.

</details>


### [37] [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)
*Shubham Kumar Nigam,Balaramamahanthi Deepak Patnaik,Shivam Mishra,Ajay Varghese Thomas,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: NyayaRAG是一个基于检索增强生成（RAG）的框架，用于预测印度法律判决，通过结合案件事实、法律条文和先例，显著提高了预测准确性和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在印度法律背景下常忽略成文法条文和司法先例，而NyayaRAG旨在模拟真实法庭场景，弥补这一不足。

Method: 提出NyayaRAG框架，结合案件描述、法律条文和语义检索的先例，通过专门管道评估输入配置对判决预测和解释的影响。

Result: 结果显示，结合结构化法律知识显著提升了预测准确性和解释质量。

Conclusion: NyayaRAG为法律判决预测提供了更全面的方法，尤其在印度法律系统中表现优异。

Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.

</details>


### [38] [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
*Yingxu Wang,Shiqi Fan,Mengzhu Wang,Siwei Liu*

Main category: cs.CL

TL;DR: DAMR框架结合符号搜索与自适应路径评估，通过MCTS和轻量级Transformer提升KGQA的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有KGQA方法中静态路径提取适应性差和动态路径生成计算成本高的问题。

Method: 采用MCTS框架，结合LLM规划器和Transformer评分器，动态生成和评估路径。

Result: 在多个KGQA基准测试中显著优于现有方法。

Conclusion: DAMR通过自适应路径评估和动态训练机制，实现了高效且准确的KGQA。

Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language
queries and perform structured reasoning over knowledge graphs by leveraging
their relational and semantic structures to retrieve accurate answers. Recent
KGQA methods primarily follow either retrieve-then-reason paradigm, relying on
GNNs or heuristic rules for static paths extraction, or dynamic path generation
strategies that use large language models (LLMs) with prompting to jointly
perform retrieval and reasoning. However, the former suffers from limited
adaptability due to static path extraction and lack of contextual refinement,
while the latter incurs high computational costs and struggles with accurate
path evaluation due to reliance on fixed scoring functions and extensive LLM
calls. To address these issues, this paper proposes Dynamically Adaptive
MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search
with adaptive path evaluation for efficient and context-aware KGQA. DAMR
employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based
planner, which selects top-$k$ relevant relations at each step to reduce search
space. To improve path evaluation accuracy, we introduce a lightweight
Transformer-based scorer that performs context-aware plausibility estimation by
jointly encoding the question and relation sequence through cross-attention,
enabling the model to capture fine-grained semantic shifts during multi-hop
reasoning. Furthermore, to alleviate the scarcity of high-quality supervision,
DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically
generates training signals from partial paths explored during search, allowing
the scorer to continuously adapt to the evolving distribution of reasoning
trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR
significantly outperforms state-of-the-art methods.

</details>


### [39] [Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data](https://arxiv.org/abs/2508.00741)
*Sohaib Imran,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（LLM）是否能从训练数据中推理出上下文外的信息，发现GPT 4o能通过观察特征行为推断虚构聊天机器人的名称。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否具备从训练数据中进行上下文外推理的能力，以评估其情境意识和AI安全性。

Method: 设计实验，训练LLM学习虚构聊天机器人的名称和行为描述，但不包含对话示例，观察其推理能力。

Result: GPT 4o能正确推断聊天机器人名称，并在行为描述训练后表现出更符合其特征的行为。

Conclusion: 结果表明LLM具备一定情境意识，对AI安全性有潜在影响。

Abstract: Large language models (LLMs) are trained on large corpora, yet it is unclear
whether they can reason about the information present within their training
data. We design experiments to study out-of-context abduction in LLMs, the
ability to infer the most plausible explanations for observations using
relevant facts present in training data. We train treatment LLMs on names and
behavior descriptions of fictitious chatbots, but not on examples of dialogue
with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at
least one chatbot's name after observing example responses characteristic of
that chatbot. We also find that previously training GPT 4o on descriptions of a
chatbot's behavior allows it to display behaviors more characteristic of the
chatbot when iteratively trained to display such behaviors. Our results have
implications for situational awareness in LLMs and, therefore, for AI safety.

</details>


### [40] [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
*Sarah Mercer,Daniel P. Martin,Phil Swatton*

Main category: cs.CL

TL;DR: 论文探讨了基于大型语言模型的生成代理在社会科学研究中作为人类替代品的有效性，通过HEXACO人格测试验证其可靠性。


<details>
  <summary>Details</summary>
Motivation: 验证生成代理是否能有效代表人类群体，尤其是在人格特征研究中。

Method: 使用310个GPT-4驱动的代理进行HEXACO人格测试，并进行因子分析，与2004年原始结果对比。

Result: 代理的响应显示出与HEXACO框架部分一致的人格结构，且结果在GPT-4中稳定，但跨模型分析发现偏差。

Conclusion: 生成代理在社会科学研究中具有潜力，但需注意模型偏差和设计一致性。

Abstract: Generative agents powered by Large Language Models demonstrate human-like
characteristics through sophisticated natural language interactions. Their
ability to assume roles and personalities based on predefined character
biographies has positioned them as cost-effective substitutes for human
participants in social science research. This paper explores the validity of
such persona-based agents in representing human populations; we recreate the
HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,
conducting factor analysis on their responses, and comparing these results to
the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results
found 1) a coherent and reliable personality structure was recoverable from the
agents' responses demonstrating partial alignment to the HEXACO framework. 2)
the derived personality dimensions were consistent and reliable within GPT-4,
when coupled with a sufficiently curated population, and 3) cross-model
analysis revealed variability in personality profiling, suggesting
model-specific biases and limitations. We discuss the practical considerations
and challenges encountered during the experiment. This study contributes to the
ongoing discourse on the potential benefits and limitations of using generative
agents in social science research and provides useful guidance on designing
consistent and representative agent personas to maximise coverage and
representation of human personality traits.

</details>


### [41] [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
*Sebastian Wind,Jeta Sopa,Daniel Truhn,Mahshad Lotfinia,Tri-Thien Nguyen,Keno Bressem,Lisa Adams,Mirabela Rusu,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: 提出了一种基于代理的RAG框架，通过LLM自主分解放射学问题并动态合成证据，显著提高了诊断准确性和事实性。


<details>
  <summary>Details</summary>
Motivation: 传统单步检索的RAG系统在复杂临床推理任务中表现有限，需要更高效的方法。

Method: 采用代理RAG框架，让LLM迭代检索Radiopaedia的临床证据并动态合成回答，评估了24种不同架构和规模的LLM。

Result: 代理检索显著提高了诊断准确性（73% vs. 64%），减少了幻觉（9.4%），并在46%的案例中检索到相关临床背景。

Conclusion: 代理框架可提升放射学QA的事实性和准确性，尤其是中等规模LLM，值得进一步临床验证。

Abstract: Clinical decision-making in radiology increasingly benefits from artificial
intelligence (AI), particularly through large language models (LLMs). However,
traditional retrieval-augmented generation (RAG) systems for radiology question
answering (QA) typically rely on single-step retrieval, limiting their ability
to handle complex clinical reasoning tasks. Here we propose an agentic RAG
framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia, and
dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning
diverse architectures, parameter scales (0.5B to >670B), and training paradigms
(general-purpose, reasoning-optimized, clinically fine-tuned), using 104
expert-curated radiology questions from previously established RSNA-RadioQA and
ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional
online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized
models (e.g., Mistral Large improved from 72% to 81%) and small-scale models
(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B
parameters) demonstrated minimal changes (<2% improvement). Additionally,
agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically
relevant context in 46% of cases, substantially aiding factual grounding. Even
clinically fine-tuned models exhibited meaningful improvements (e.g.,
MedGemma-27B improved from 71% to 81%), indicating complementary roles of
retrieval and fine-tuning. These results highlight the potential of agentic
frameworks to enhance factuality and diagnostic accuracy in radiology QA,
particularly among mid-sized LLMs, warranting future studies to validate their
clinical utility.

</details>


### [42] [GLiDRE: Generalist Lightweight model for Document-level Relation Extraction](https://arxiv.org/abs/2508.00757)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: GLiDRE是一种基于GLiNER关键思想的新型文档级关系抽取模型，在少样本场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前文档级关系抽取模型在零样本或少样本设置下的性能尚未充分探索，GLiNER的成功启发了GLiDRE的开发。

Method: GLiDRE基于GLiNER的关键思想构建，并在Re-DocRED数据集上进行了多数据设置的基准测试。

Result: GLiDRE在少样本场景下达到了最先进的性能。

Conclusion: GLiDRE在文档级关系抽取任务中表现出色，尤其在少样本设置下，代码已公开。

Abstract: Relation Extraction (RE) is a fundamental task in Natural Language
Processing, and its document-level variant poses significant challenges, due to
the need to model complex interactions between entities across sentences.
Current approaches, largely based on the ATLOP architecture, are commonly
evaluated on benchmarks like DocRED and Re-DocRED. However, their performance
in zero-shot or few-shot settings remains largely underexplored due to the
task's complexity. Recently, the GLiNER model has shown that a compact NER
model can outperform much larger Large Language Models. With a similar
motivation, we introduce GLiDRE, a new model for document-level relation
extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against
state-of-the-art models across various data settings on the Re-DocRED dataset.
Our results demonstrate that GLiDRE achieves state-of-the-art performance in
few-shot scenarios. Our code is publicly available.

</details>


### [43] [MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations](https://arxiv.org/abs/2508.00760)
*Qiyao Xue,Yuchen Dou,Ryan Shi,Xiang Lorraine Li,Wei Gao*

Main category: cs.CL

TL;DR: 本文提出了一种基于BERT的多模态框架MMBERT，用于中文社交网络中的仇恨言论检测，通过结合文本、语音和视觉模态，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 中文社交网络中的仇恨言论检测面临独特挑战，尤其是规避技术的广泛使用。现有研究多集中于英文数据集，对中文多模态策略关注不足。

Method: 提出了MMBERT框架，采用Mixture-of-Experts（MoE）架构整合多模态信息，并通过渐进式三阶段训练解决BERT与MoE结合的不稳定性。

Result: 在多个中文仇恨言论数据集上，MMBERT显著优于微调的BERT模型、微调的大型语言模型（LLMs）及基于上下文学习的LLMs。

Conclusion: MMBERT通过多模态整合和渐进式训练，有效提升了中文仇恨言论检测的鲁棒性和性能。

Abstract: Hate speech detection on Chinese social networks presents distinct
challenges, particularly due to the widespread use of cloaking techniques
designed to evade conventional text-based detection systems. Although large
language models (LLMs) have recently improved hate speech detection
capabilities, the majority of existing work has concentrated on English
datasets, with limited attention given to multimodal strategies in the Chinese
context. In this study, we propose MMBERT, a novel BERT-based multimodal
framework that integrates textual, speech, and visual modalities through a
Mixture-of-Experts (MoE) architecture. To address the instability associated
with directly integrating MoE into BERT-based models, we develop a progressive
three-stage training paradigm. MMBERT incorporates modality-specific experts, a
shared self-attention mechanism, and a router-based expert allocation strategy
to enhance robustness against adversarial perturbations. Empirical results in
several Chinese hate speech datasets show that MMBERT significantly surpasses
fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing
in-context learning approaches.

</details>


### [44] [ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation](https://arxiv.org/abs/2508.00762)
*Atakan Site,Emre Hakan Erdemir,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本文介绍了SemEval-2025 Task 8的零射击解决方案，利用LLM生成Python代码进行表格问答，表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决多样化领域表格数据的问答任务，探索LLM在代码生成中的潜力。

Method: 采用基于开源LLM的Python代码生成框架，优化提示策略生成可执行的Pandas代码。

Result: 不同LLM在代码生成中表现不一，Python代码生成在表格问答中表现最佳。系统在开源模型类别中排名第八和第六。

Conclusion: LLM生成的Python代码在表格问答任务中具有优势，未来可进一步优化提示策略。

Abstract: This paper presents our system for SemEval-2025 Task 8: DataBench,
Question-Answering over Tabular Data. The primary objective of this task is to
perform question answering on given tabular datasets from diverse domains under
two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To
tackle both subtasks, we developed a zero-shot solution with a particular
emphasis on leveraging Large Language Model (LLM)-based code generation.
Specifically, we propose a Python code generation framework utilizing
state-of-the-art open-source LLMs to generate executable Pandas code via
optimized prompting strategies. Our experiments reveal that different LLMs
exhibit varying levels of effectiveness in Python code generation.
Additionally, results show that Python code generation achieves superior
performance in tabular question answering compared to alternative approaches.
Although our ranking among zero-shot systems is unknown at the time of this
paper's submission, our system achieved eighth place in Subtask I and sixth
place in Subtask~II among the 30 systems that outperformed the baseline in the
open-source models category.

</details>


### [45] [Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models](https://arxiv.org/abs/2508.00788)
*Xushuo Tang,Yi Ding,Zhengyi Yang,Yin Chen,Yongrui Gu,Wenke Yang,Mingchen Ju,Xin Cao,Yongfei Liu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 论文介绍了MISGENDERED+，一个用于评估大语言模型（LLMs）代词使用准确性的更新基准，测试了五种代表性模型，发现其在二元和性别中性代词上表现较好，但在新代词和反向推理任务上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LLMs在性别中性和新代词使用上的公平性和包容性问题，扩展并更新了之前的MISGENDERED基准。

Method: 方法包括引入MISGENDERED+基准，测试五种LLMs（GPT-4o、Claude 4、DeepSeek-V3、Qwen Turbo和Qwen2.5）在零样本、少样本和性别身份推理任务中的表现。

Result: 结果显示，模型在二元和性别中性代词上的准确性有显著提升，但在新代词和反向推理任务上表现不一致。

Conclusion: 结论指出，尽管有所改进，LLMs在身份敏感推理上仍存在差距，未来研究需进一步探索包容性AI的方向。

Abstract: Large language models (LLMs) are increasingly deployed in sensitive contexts
where fairness and inclusivity are critical. Pronoun usage, especially
concerning gender-neutral and neopronouns, remains a key challenge for
responsible AI. Prior work, such as the MISGENDERED benchmark, revealed
significant limitations in earlier LLMs' handling of inclusive pronouns, but
was constrained to outdated models and limited evaluations. In this study, we
introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'
pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,
DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender
identity inference. Our results show notable improvements compared with
previous studies, especially in binary and gender-neutral pronoun accuracy.
However, accuracy on neopronouns and reverse inference tasks remains
inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We
discuss implications, model-specific observations, and avenues for future
inclusive AI research.

</details>


### [46] [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CL

TL;DR: DAEDAL是一种无需训练的去噪策略，用于动态调整扩散大语言模型的生成长度，解决了静态长度分配的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）在并行生成和全局上下文建模方面表现出色，但静态预定义生成长度的限制导致性能与计算效率的权衡问题。

Method: DAEDAL通过两阶段策略动态调整生成长度：1）初始长度扩展；2）去噪过程中动态插入掩码标记以优化输出。

Result: 实验表明，DAEDAL在性能上与固定长度基线相当甚至更优，同时提高了计算效率。

Conclusion: DAEDAL解决了DLLMs的静态长度限制，为其与自回归模型的竞争提供了新潜力。

Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful
alternative to the dominant Autoregressive Large Language Models, offering
efficient parallel generation and capable global context modeling. However, the
practical application of DLLMs is hindered by a critical architectural
constraint: the need for a statically predefined generation length. This static
length allocation leads to a problematic trade-off: insufficient lengths
cripple performance on complex tasks, while excessive lengths incur significant
computational overhead and sometimes result in performance degradation. While
the inference framework is rigid, we observe that the model itself possesses
internal signals that correlate with the optimal response length for a given
task. To bridge this gap, we leverage these latent signals and introduce
DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive
Length Expansion for Diffusion Large Language Models. DAEDAL operates in two
phases: 1) Before the denoising process, DAEDAL starts from a short initial
length and iteratively expands it to a coarse task-appropriate length, guided
by a sequence completion metric. 2) During the denoising process, DAEDAL
dynamically intervenes by pinpointing and expanding insufficient generation
regions through mask token insertion, ensuring the final output is fully
developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves
performance comparable, and in some cases superior, to meticulously tuned
fixed-length baselines, while simultaneously enhancing computational efficiency
by achieving a higher effective token ratio. By resolving the static length
constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap
with their Autoregressive counterparts and paving the way for more efficient
and capable generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: HealthBench是一个用于评估医疗AI系统的基准，但依赖专家意见可能引入偏见。本文提出基于临床实践指南（CPGs）的改进方法，以提升全球适用性和公平性。


<details>
  <summary>Details</summary>
Motivation: HealthBench虽推动了医疗语言模型评估，但其依赖专家意见可能固化区域偏见和个体差异，尤其在低收入地区问题更突出。需要更全球化和公平的基准。

Method: 提出基于版本控制CPGs的奖励函数，结合系统综述和GRADE证据评级，通过证据加权评分和上下文覆盖逻辑实现“证据稳健”的强化学习。

Result: 改进后的方法旨在提升医疗语言模型的临床可信度、伦理合规性和全球适用性。

Conclusion: 通过结合CPGs和透明性，改进HealthBench，目标是开发更可靠、公平且全球适用的医疗语言模型。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [48] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出了一种基于HyperTWTL的安全强化学习方法（SecRL），通过动态Boltzmann softmax RL学习满足HyperTWTL约束的安全最优策略，并在机器人任务中验证了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在探索基于超属性的安全感知强化学习方面存在空白，尤其是在机器人应用中如何利用HyperTWTL表达安全和不透明性约束。

Method: 将智能体动态建模为马尔可夫决策过程（MDP），并使用HyperTWTL形式化不透明性和安全约束，提出动态Boltzmann softmax RL方法学习满足约束的最优策略。

Result: 通过机器人拾取和交付任务案例验证了方法的有效性和可扩展性，并优于两种基线RL算法。

Conclusion: HyperTWTL约束的安全强化学习方法在机器人应用中表现出色，为安全感知RL提供了新思路。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [49] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 论文探讨了AI在工业环境中的应用挑战，提出通过对象中心过程挖掘（OCPM）实现过程智能（PI），以支持生成、预测和规范AI。


<details>
  <summary>Details</summary>
Motivation: 组织在工业环境中成功应用AI面临挑战，尤其是端到端操作过程的改进。

Method: 采用对象中心过程挖掘（OCPM）作为连接数据和过程的桥梁，支持多种AI形式。

Result: OCPM是实现过程智能（PI）的关键，能够有效支持AI在组织环境中的应用。

Conclusion: AI需要PI来改进操作过程，OCPM与生成、预测和规范AI的结合具有广阔前景。

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [50] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: 本文提出三种检测多准则决策分析中排名反转的测试方法，并讨论了其在Scikit-Criteria库中的实现及设计考虑。


<details>
  <summary>Details</summary>
Motivation: 排名反转是多准则决策分析中的严重问题，影响决策方法的有效性，因此需要一种机制来评估方法的性能。

Method: 提出三种测试方法，检测排名反转，并在Scikit-Criteria库中实现。

Result: 实现了针对一般场景的测试方法，并解决了相关设计问题。

Conclusion: 这些测试方法在多准则决策方法的评估中可能发挥重要作用。

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [51] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: 研究SHACL在RDF图更新下的静态验证问题，提出一种基于SHACL的更新语言，并通过回归技术将其转化为SHACL约束的（不）可满足性问题。


<details>
  <summary>Details</summary>
Motivation: 研究RDF图在更新后是否仍满足SHACL规范，为动态RDF图提供验证基础。

Method: 提出基于SHACL的更新语言，使用回归技术将更新动作嵌入SHACL约束，分析计算复杂度。

Result: 展示了静态验证问题的计算复杂度，并通过原型实现验证了方法的可行性。

Conclusion: 静态验证问题可转化为SHACL约束的（不）可满足性问题，为动态RDF图提供了实用的验证工具。

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [52] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: 论文提出了一种基于设计正义和参与式AI的AI生命周期重构方法，强调共同生产、多样性、公平性和多学科合作，以减少AI对边缘化群体的负面影响。


<details>
  <summary>Details</summary>
Motivation: AI算法可能对边缘化群体产生不成比例的负面影响，现有方法（如伦理指南和技术解决方案）未能根本解决问题。

Method: 提出一个包含五个阶段的增强AI生命周期（共同框架、共同设计、共同实施、共同部署、共同维护），基于多学科研讨会和分布式权威理念。

Result: 该方法结合了设计正义和参与式AI，为AI生产流程提供了新的架构。

Conclusion: 重构AI生命周期是减少AI危害的关键，未来需进一步研究如何扩展参与式治理。

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [53] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 论文指出传统的人类评分一致性（IRR）作为标注质量的唯一标准存在问题，提出了五种补充方法以提高教育AI数据的有效性和预测性。


<details>
  <summary>Details</summary>
Motivation: 人类评估者存在偏见和不可靠性，传统IRR指标无法完全保证标注数据的质量，影响了教育AI模型的进步。

Method: 提出五种补充评估方法，包括多标签标注方案、专家评估和闭环验证等，强调外部有效性的重要性。

Result: 这些方法能生成更有效的训练数据和模型，提升学生学习效果和可操作性见解。

Conclusion: 呼吁重新思考标注质量和真实标准，优先考虑有效性和教育影响，而非仅依赖共识。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [54] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 论文探讨通过设计目标函数，使AI系统明确赋能人类并管理人机权力平衡，以促进安全和福祉。


<details>
  <summary>Details</summary>
Motivation: 权力是AI安全的核心概念，同时权力作为追求多样目标的能力对福祉至关重要。研究旨在通过管理权力平衡提升安全与福祉。

Method: 采用部分公理化方法，设计可参数化和分解的目标函数，考虑人类有限理性、社会规范及多样目标，并通过逆向归纳或多智能体强化学习计算。

Result: 在多种情境下展示了最大化人类权力指标的后果及潜在子目标，表明其可能比直接基于效用的目标更安全。

Conclusion: 软最大化合适的人类权力指标可能是比直接效用目标更安全的AI系统目标。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [55] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS通过结合内部探索和外部数据，解决了RLVR的能力边界问题，提升了LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR因策略限制和稀疏奖励难以突破基础LLM的能力边界，甚至导致能力边界崩溃。

Method: RL-PLUS结合多重重要性采样和探索优势函数，优化推理路径。

Result: 在数学推理和分布外任务上表现优异，平均提升21.1%至69.2%。

Conclusion: RL-PLUS有效解决了能力边界崩溃问题，显著提升了推理能力。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [56] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent是一个基于学习实践的智能代理，通过持续自我改进和工具学习提升任务解决能力。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能够通过实践和自我反思不断改进的智能代理，以解决知识发现任务中的挑战。

Method: MetaAgent从基础能力出发，通过生成自然语言请求、工具路由、自我反思和知识库构建逐步提升能力。

Result: 在GAIA、WebWalkerQA和BrowseCamp等基准测试中，MetaAgent表现优于基线方法，并匹配或超越端到端训练的代理。

Conclusion: MetaAgent展示了自进化代理系统在通用知识发现任务中的潜力，无需调整模型参数或额外训练。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [57] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: 论文比较了人类和GPT-4o在任务生成中的差异，发现人类受心理驱动因素影响，而LLM生成的任务更抽象且缺乏社会性和物理性。


<details>
  <summary>Details</summary>
Motivation: 探讨生成代理（如LLM）是否能模拟人类基于内在动机的任务生成行为。

Method: 通过任务生成实验比较人类和GPT-4o的行为，分析心理驱动因素对任务生成的影响。

Result: 人类任务生成受心理驱动因素影响，而LLM生成的任务更抽象、缺乏社会性和物理性，尽管被认为更有趣和新颖。

Conclusion: LLM与人类认知存在核心差距，需在设计更人性化的代理时融入内在动机和物理基础。

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [58] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: ReasonBench是一个专注于结构化图形推理任务的评估基准，用于评估视觉语言模型（VLMs）在复杂图形推理中的表现，揭示了当前模型的局限性，并提出双重优化策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在复杂图形推理中的表现，填补现有研究在复杂图形和抽象问题解决上的空白。

Method: 提出ReasonBench基准，包含1,613个真实世界智力测试问题，覆盖位置、属性、数量和多元素任务；提出DiaCoT和ReasonTune双重优化策略。

Result: 评估11种主流VLMs，揭示其显著局限性；双重优化策略使VLM性能提升33.5%。

Conclusion: ReasonBench为VLMs在复杂图形推理中的评估提供了全面工具，双重优化策略显著提升了模型性能。

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [59] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: 论文提出R1-Act方法，通过结构化推理触发大型推理模型的安全知识，显著提升安全性且保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在执行有害指令时存在安全隐患，研究发现模型已具备安全知识但未在推理中激活。

Method: 提出R1-Act，一种简单高效的后训练方法，通过结构化推理显式触发安全知识。

Result: R1-Act在保持推理性能的同时显著提升安全性，仅需少量训练数据和计算资源。

Conclusion: R1-Act具有鲁棒性、可扩展性和实用性，适用于不同规模和架构的LRMs。

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [60] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI是一个模块化框架，通过视觉验证改进多模态推理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决CoT提示在视觉语言模型中产生的幻觉问题，即解释缺乏视觉内容支持。

Method: CoRGI采用三阶段流程：生成文本推理链、提取视觉证据、合成验证答案。

Result: 在VCR基准测试中，CoRGI提升了Qwen-2.5VL和LLaVA-1.6的性能，并生成更事实性的解释。

Conclusion: 视觉验证能增强多模态推理的鲁棒性，但后验验证框架存在潜在限制。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [61] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: 提出了一种基于心智理论（ToM）的多智能体协作方法，通过主动推理实现，无需任务特定的共享生成模型或显式通信。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体协作中因缺乏对他人信念和目标的推理能力而导致的效率低下问题。

Method: 扩展推理树规划算法，通过递归推理探索联合策略空间，智能体维护自身和他人信念及目标的独立表示。

Result: 在避碰和觅食任务中，ToM智能体表现优于非ToM智能体，能避免碰撞并减少冗余努力。

Conclusion: 该方法为人工智能的实践应用提供了新思路，并为心智理论的计算机制提供了见解。

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [62] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: Cognitive Kernel-Pro 是一个完全开源且免费的 AI 代理框架，旨在推动高级 AI 代理的开发和评估，通过高质量训练数据和创新策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 代理系统多为闭源或依赖付费 API，限制了研究的可访问性和可复现性。

Method: 开发开源框架 Cognitive Kernel-Pro，研究高质量训练数据的构建（查询、轨迹、可验证答案），并探索代理测试时反思和投票策略。

Result: 在 GAIA 基准测试中取得开源代理的最优性能，8B 参数模型超越 WebDancer 和 WebSailor。

Conclusion: Cognitive Kernel-Pro 为高能力 AI 代理设立了新标准，推动了开源社区的发展。

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [63] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在数学领域的应用，尤其是形式化数学证明中的挑战，并分析了其与代码生成的差异。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在数学领域的潜力，尤其是形式化证明中的困难，以揭示其推理机制和监督方式的局限性。

Method: 通过分析最新模型和基准测试，探讨LLMs在数学认知中的三个核心问题。

Result: 发现形式化数学证明比代码合成更脆弱，并讨论了LLMs是否真正具备逻辑状态表示能力。

Conclusion: 旨在明确当前技术的边界，并提出可能的扩展方向。

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [64] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: Pro2Guard是一种基于概率可达性分析的主动运行时安全框架，用于预测和预防LLM代理的不安全行为。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的响应式安全系统缺乏预见性，难以处理长期依赖和分布变化，因此需要一种更主动的方法来预防风险。

Method: Pro2Guard将代理行为抽象为符号状态，从执行轨迹中学习离散时间马尔可夫链（DTMC），并在运行时预测不安全状态的概率，触发干预。

Result: 在家庭代理和自动驾驶场景中，Pro2Guard能提前预测风险，分别阻止93.6%的不安全任务和100%的交通违规与碰撞。

Conclusion: Pro2Guard通过主动预测和干预，显著提升了LLM代理的安全性，同时平衡任务完成率。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [65] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: 论文提出MultiSHAP框架，用于解释多模态AI模型中的跨模态交互，适用于开源和闭源模型，提供实例级和数据集级解释。


<details>
  <summary>Details</summary>
Motivation: 多模态AI模型的‘黑盒’特性在高风险应用中部署时面临可解释性和可信度的挑战，现有方法无法精确量化模态间的协同效应。

Method: 引入MultiSHAP框架，利用Shapley Interaction Index量化视觉和文本元素间的交互效应，适用于多种模型。

Result: 实验验证MultiSHAP能准确捕捉跨模态推理机制，实际案例展示其实用性。

Conclusion: MultiSHAP为复杂多模态AI模型提供了一种通用的解释解决方案，并可扩展至超过两种模态。

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [66] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 本文提出了一种混合框架，结合传统XAI技术与生成式AI模型和用户个性化，以生成多模态、个性化的解释。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的自适应学习系统缺乏透明度，且大多数XAI技术忽视用户角色和理解。

Method: 提出混合框架，整合传统XAI技术、生成式AI模型和用户个性化。

Result: 框架旨在提供动态、个性化的解释，增强透明度和用户体验。

Conclusion: 目标是推动可解释AI，既提升透明度又支持以用户为中心的体验。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [67] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: 提出了一种多阶段LLM驱动框架，用于从复杂电子病历生成全面的预咨询问卷，解决了直接LLM方法在信息完整性、逻辑顺序和疾病级合成方面的不足。


<details>
  <summary>Details</summary>
Motivation: 预咨询是医疗保健的关键环节，但直接从复杂电子病历生成问卷存在挑战，需要解决信息完整性和逻辑性问题。

Method: 分三阶段：1) 提取原子断言；2) 构建个人因果网络并合成疾病知识；3) 生成个性化问卷。

Result: 在真实电子病历数据集上验证，表现优于直接方法，信息覆盖、诊断相关性、可理解性和生成时间均更优。

Conclusion: 该框架通过构建显式临床知识，提升了预咨询问卷的质量和实用性。

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [68] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: 论文提出了一种用户分段的上下文感知解释系统，通过可视化方法解决社交媒体推荐缺乏个性化解释的问题。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体推荐系统的解释性不足，无法满足不同用户的需求，导致推荐价值降低。

Method: 提出了一种视觉解释系统，根据用户需求和上下文提供不同形式的解释，包括技术详细版和简化版。

Result: 系统首次在一个流程中同时调整解释风格（视觉vs数字）和粒度（专家vs普通用户）。

Conclusion: 通过30名X用户的公开试点验证其对决策和信任的影响。

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [69] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 论文提出了AVR-Eval评估指标和AVR-Agent多代理系统，用于生成和评估交互式音视频内容，发现当前模型在利用高质量资源和反馈方面与人类存在差距。


<details>
  <summary>Details</summary>
Motivation: 解决AI生成复杂交互式音视频内容（如视频游戏）时缺乏自动化评估指标和难以处理复杂内容的问题。

Method: 提出AVR-Eval评估指标，通过多模态模型比较内容质量；开发AVR-Agent多代理系统，生成并迭代优化JavaScript代码。

Result: AVR-Agent生成的内容在实验中表现优于单次生成的内容，但模型未能有效利用定制资源和反馈。

Conclusion: 当前模型在利用高质量资源和反馈方面与人类存在显著差距，揭示了机器与人类内容创作的根本差异。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [70] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 论文提出了一种多频带变滞后格兰杰因果性（MB-VLGC）框架，解决了传统方法无法处理频率依赖性因果延迟的问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂系统中，因果关系的滞后时间可能因频率不同而变化，传统方法无法捕捉这一点。

Method: 论文通过形式化MB-VLGC，提出了一种建模频率依赖性因果延迟的新框架，并设计了高效推理流程。

Result: 实验表明，MB-VLGC在合成和真实数据集上显著优于现有方法。

Conclusion: MB-VLGC框架具有广泛适用性，适用于任何类型的时间序列数据。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [71] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: 提出了一种基于大型预训练多模态模型的通用分类器，用于检测生成内容，在音频和图像领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 生成模型被恶意用于传播虚假信息，现有检测工具泛化能力差，需开发通用且高效的检测方法。

Method: 利用大型预训练多模态模型的潜在代码特征，训练线性分类器，实现跨模态的高效检测。

Result: 在音频和图像检测中表现优于或匹配现有基线方法，计算高效且适用于少样本场景。

Conclusion: 该方法为通用生成内容检测提供了高效且强大的解决方案。

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [72] [Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion](https://arxiv.org/abs/2508.00037)
*Tong Nie,Jian Sun,Wei Ma*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理定律启发的可扩展时空Transformer（ScaleSTF），用于高效预测大规模城市网络中的动态，解决了现有模型在效能与效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 城市网络系统涉及复杂且规则未知的动态过程，现有数据驱动模型（如图神经网络）在效能与效率之间存在权衡，难以应用于大规模网络。

Method: 通过结合物理定律和Transformer结构，设计了一种低维嵌入诱导的注意力层，提出了线性复杂度的ScaleSTF模型。

Result: 在交通流量、太阳能发电和智能电表等大规模城市系统中验证了ScaleSTF的先进性能和卓越的可扩展性。

Conclusion: ScaleSTF为大规模城市网络动态预测提供了新的视角，展示了其在效能与效率上的平衡优势。

Abstract: Networked urban systems facilitate the flow of people, resources, and
services, and are essential for economic and social interactions. These systems
often involve complex processes with unknown governing rules, observed by
sensor-based time series. To aid decision-making in industrial and engineering
contexts, data-driven predictive models are used to forecast spatiotemporal
dynamics of urban systems. Current models such as graph neural networks have
shown promise but face a trade-off between efficacy and efficiency due to
computational demands. Hence, their applications in large-scale networks still
require further efforts. This paper addresses this trade-off challenge by
drawing inspiration from physical laws to inform essential model designs that
align with fundamental principles and avoid architectural redundancy. By
understanding both micro- and macro-processes, we present a principled
interpretable neural diffusion scheme based on Transformer-like structures
whose attention layers are induced by low-dimensional embeddings. The proposed
scalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is
validated on large-scale urban systems including traffic flow, solar power, and
smart meters, showing state-of-the-art performance and remarkable scalability.
Our results constitute a fresh perspective on the dynamics prediction in
large-scale urban networks.

</details>


### [73] [Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings](https://arxiv.org/abs/2508.00039)
*Kaustav Chatterjee,Joshua Q. Li,Fatemeh Ansari,Masud Rana Munna,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 该研究提出了一种结合LSTM和Transformer的混合深度学习框架，用于高效测量铁路公路平交道口（HRGC）的垂直剖面，以解决传统方法成本高、耗时长的问题。


<details>
  <summary>Details</summary>
Motivation: 铁路公路平交道口的垂直剖面问题可能导致车辆悬挂风险，传统测量方法效率低且成本高，亟需一种更高效的解决方案。

Method: 研究采用IMU和GPS传感器采集数据，结合工业标准步行剖面仪获取地面真实数据，开发了三种混合深度学习模型进行评估。

Result: 模型2和模型3表现最佳，成功生成了2D/3D的HRGC剖面，展示了深度学习在提升测量效率和安全性方面的潜力。

Conclusion: 该研究证明了混合深度学习框架在HRGC剖面测量中的有效性，为提升公路和铁路安全提供了新方法。

Abstract: Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose
safety risks to highway vehicles due to potential hang-ups. These crossings
typically result from post-construction railway track maintenance activities or
non-compliance with design guidelines for HRGC vertical alignments.
Conventional methods for measuring HRGC profiles are costly, time-consuming,
traffic-disruptive, and present safety challenges. To address these issues,
this research employed advanced, cost-effective techniques and innovative
modeling approaches for HRGC profile measurement. A novel hybrid deep learning
framework combining Long Short-Term Memory (LSTM) and Transformer architectures
was developed by utilizing instrumentation and ground truth data.
Instrumentation data were gathered using a highway testing vehicle equipped
with Inertial Measurement Unit (IMU) and Global Positioning System (GPS)
sensors, while ground truth data were obtained via an industrial-standard
walking profiler. Field data was collected at the Red Rock Railroad Corridor in
Oklahoma. Three advanced deep learning models Transformer-LSTM sequential
(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel
(model 3) were evaluated to identify the most efficient architecture. Models 2
and 3 outperformed the others and were deployed to generate 2D/3D HRGC
profiles. The deep learning models demonstrated significant potential to
enhance highway and railroad safety by enabling rapid and accurate assessment
of HRGC hang-up susceptibility.

</details>


### [74] [Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting](https://arxiv.org/abs/2508.00040)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 该论文提出了一种结合贝叶斯机制检测和条件神经过程的方法（R-NP），用于预测德国市场的24小时电价，并通过多标准评估（TOPSIS）验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 电力市场价格的动态性和复杂性需要更精确的预测方法，以支持电池存储优化等实际应用。

Method: 使用DS-HDP-HMM进行机制检测，每个机制由独立的CNP建模，最终预测为机制加权的CNP输出。

Result: R-NP在2021-2023年表现出最平衡的性能，优于DNN和LEAR模型。

Conclusion: R-NP是一种综合性能优越的电价预测方法，适用于多种实际应用场景。

Abstract: This work integrates Bayesian regime detection with conditional neural
processes for 24-hour electricity price prediction in the German market. Our
methodology integrates regime detection using a disentangled sticky
hierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to
daily electricity prices. Each identified regime is subsequently modeled by an
independent conditional neural process (CNP), trained to learn localized
mappings from input contexts to 24-dimensional hourly price trajectories, with
final predictions computed as regime-weighted mixtures of these CNP outputs. We
rigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated
auto-regressive (LEAR) models by integrating their forecasts into diverse
battery storage optimization frameworks, including price arbitrage, risk
management, grid services, and cost minimization. This operational utility
assessment revealed complex performance trade-offs: LEAR often yielded superior
absolute profits or lower costs, while DNN showed exceptional optimality in
specific cost-minimization contexts. Recognizing that raw prediction accuracy
doesn't always translate to optimal operational outcomes, we employed TOPSIS as
a comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified
LEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model
emerged as the most balanced and preferred solution for 2021, 2022 and 2023.

</details>


### [75] [Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages](https://arxiv.org/abs/2508.00041)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.LG

TL;DR: DevFT是一种资源高效的联邦微调方法，通过分阶段构建LLM，显著提升性能并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决联邦微调在边缘设备上资源消耗大的问题，同时保持数据隐私。

Method: 分阶段微调，逐步增加子模型参数容量，通过知识转移优化初始化参数。采用解冲突引导的层分组和基于差异的层融合。

Result: 在多个基准测试中表现优异，收敛速度提升4.59倍，通信开销减少10.67倍，性能平均提升9.07%。

Conclusion: DevFT是一种高效且兼容现有方法的联邦微调方案，适用于资源受限的边缘设备。

Abstract: Federated fine-tuning enables Large Language Models (LLMs) to adapt to
downstream tasks while preserving data privacy, but its resource-intensive
nature limits deployment on edge devices. In this paper, we introduce
Developmental Federated Tuning (DevFT), a resource-efficient approach inspired
by cognitive development that progressively builds a powerful LLM from a
compact foundation. DevFT decomposes the fine-tuning process into developmental
stages, each optimizing submodels with increasing parameter capacity. Knowledge
from earlier stages transfers to subsequent submodels, providing optimized
initialization parameters that prevent convergence to local minima and
accelerate training. This paradigm mirrors human learning, gradually
constructing comprehensive knowledge structure while refining existing skills.
To efficiently build stage-specific submodels, DevFT introduces
deconfliction-guided layer grouping and differential-based layer fusion to
distill essential information and construct representative layers. Evaluations
across multiple benchmarks demonstrate that DevFT significantly outperforms
state-of-the-art methods, achieving up to 4.59$\times$ faster convergence,
10.67$\times$ reduction in communication overhead, and 9.07% average
performance improvement, while maintaining compatibility with existing
approaches.

</details>


### [76] [Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity](https://arxiv.org/abs/2508.00043)
*Nhut Truong,Uri Hasson*

Main category: cs.LG

TL;DR: 比较了两种空间约束（权重相似性和激活相似性）对地形卷积神经网络的影响，发现权重相似性在鲁棒性、输入敏感性和功能定位方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 探讨不同地形约束对神经网络学习表示的影响，填补系统性研究的空白。

Method: 比较权重相似性（WS）和激活相似性（AS）两种约束，评估分类准确性、鲁棒性和表示的空间组织。

Result: WS在噪声鲁棒性、输入敏感性和功能定位方面优于AS和标准CNN，并影响了网络表示几何。

Conclusion: 权重相似性约束能产生更鲁棒的表示，并影响生物物理启发模型中的特征学习和功能组织。

Abstract: Topographic neural networks are computational models that can simulate the
spatial and functional organization of the brain. Topographic constraints in
neural networks can be implemented in multiple ways, with potentially different
impacts on the representations learned by the network. The impact of such
different implementations has not been systematically examined. To this end,
here we compare topographic convolutional neural networks trained with two
spatial constraints: Weight Similarity (WS), which pushes neighboring units to
develop similar incoming weights, and Activation Similarity (AS), which
enforces similarity in unit activations. We evaluate the resulting models on
classification accuracy, robustness to weight perturbations and input
degradation, and the spatial organization of learned representations. Compared
to both AS and standard CNNs, WS provided three main advantages: i) improved
robustness to noise, also showing higher accuracy under weight corruption; ii)
greater input sensitivity, reflected in higher activation variance; and iii)
stronger functional localization, with units showing similar activations
positioned at closer distances. In addition, WS produced differences in
orientation tuning, symmetry sensitivity, and eccentricity profiles of units,
indicating an influence of this spatial constraint on the representational
geometry of the network. Our findings suggest that during end-to-end training,
WS constraints produce more robust representations than AS or non-topographic
CNNs. These findings also suggest that weight-based spatial constraints can
shape feature learning and functional organization in biophysical inspired
models.

</details>


### [77] [Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains](https://arxiv.org/abs/2508.00046)
*Ruo Yu Tao,Kaicheng Guo,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 论文提出了一个用于评估强化学习算法在部分可观测性下性能的基准框架POBAX，并强调了基准应具备的两种关键特性：覆盖多种部分可观测形式以及记忆可改进性。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估简单的部分可观测形式（如特征掩码和高斯噪声），无法反映真实场景中的复杂性（如视觉遮挡或未知对手意图），因此需要更全面的基准。

Method: 提出POBAX开源库，包含多种部分可观测环境（如定位与地图、视觉控制、游戏等），并提供推荐的超参数和算法实现。

Result: 这些任务均具有记忆可改进性，且需要难以学习的记忆功能，为部分可观测性研究提供了明确信号。

Conclusion: POBAX为部分可观测性研究提供了快速、可扩展的评估工具，并推动了算法在复杂部分可观测环境中的性能提升。

Abstract: Mitigating partial observability is a necessary but challenging task for
general reinforcement learning algorithms. To improve an algorithm's ability to
mitigate partial observability, researchers need comprehensive benchmarks to
gauge progress. Most algorithms tackling partial observability are only
evaluated on benchmarks with simple forms of state aliasing, such as feature
masking and Gaussian noise. Such benchmarks do not represent the many forms of
partial observability seen in real domains, like visual occlusion or unknown
opponent intent. We argue that a partially observable benchmark should have two
key properties. The first is coverage in its forms of partial observability, to
ensure an algorithm's generalizability. The second is a large gap between the
performance of a agents with more or less state information, all other factors
roughly equal. This gap implies that an environment is memory improvable: where
performance gains in a domain are from an algorithm's ability to cope with
partial observability as opposed to other factors. We introduce best-practice
guidelines for empirically benchmarking reinforcement learning under partial
observability, as well as the open-source library POBAX: Partially Observable
Benchmarks in JAX. We characterize the types of partial observability present
in various environments and select representative environments for our
benchmark. These environments include localization and mapping, visual control,
games, and more. Additionally, we show that these tasks are all memory
improvable and require hard-to-learn memory functions, providing a concrete
signal for partial observability research. This framework includes recommended
hyperparameters as well as algorithm implementations for fast, out-of-the-box
evaluation, as well as highly performant environments implemented in JAX for
GPU-scalable experimentation.

</details>


### [78] [TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection](https://arxiv.org/abs/2508.00047)
*Yuan-Cheng Yu,Yen-Chieh Ouyang,Chun-An Lin*

Main category: cs.LG

TL;DR: TriP-LLM是一种基于大语言模型的无监督时间序列异常检测框架，通过三分支设计整合局部和全局时间特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法在处理高异质性和复杂性的时间序列数据时表现不足，受大语言模型在多模态任务中的成功启发，提出了TriP-LLM。

Method: TriP-LLM采用三分支设计（Patching、Selection、Global）将时间序列编码为补丁令牌，利用冻结的预训练LLM处理，并通过轻量级解码器重构输入以计算异常分数。

Result: 在多个公开数据集上，TriP-LLM表现优于现有方法，且内存消耗显著低于基于CI补丁处理的LLM方法。

Conclusion: TriP-LLM展示了LLM在时间序列异常检测中的强大潜力，适用于GPU内存受限环境。

Abstract: Time-series anomaly detection plays a central role across a wide range of
application domains. With the increasing proliferation of the Internet of
Things (IoT) and smart manufacturing, time-series data has dramatically
increased in both scale and dimensionality. This growth has exposed the
limitations of traditional statistical methods in handling the high
heterogeneity and complexity of such data. Inspired by the recent success of
large language models (LLMs) in multimodal tasks across language and vision
domains, we propose a novel unsupervised anomaly detection framework: A
Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly
Detection (TriP-LLM). TriP-LLM integrates local and global temporal features
through a tri-branch design-Patching, Selection, and Global-to encode the input
time series into patch-wise tokens, which are then processed by a frozen,
pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from
which anomaly scores are derived. We evaluate TriP-LLM on several public
benchmark datasets using PATE, a recently proposed threshold-free evaluation
metric, and conduct all comparisons within a unified open-source framework to
ensure fairness. Experimental results show that TriP-LLM consistently
outperforms recent state-of-the-art methods across all datasets, demonstrating
strong detection capabilities. Furthermore, through extensive ablation studies,
we verify the substantial contribution of the LLM to the overall architecture.
Compared to LLM-based approaches using Channel Independence (CI) patch
processing, TriP-LLM achieves significantly lower memory consumption, making it
more suitable for GPU memory-constrained environments. All code and model
checkpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git

</details>


### [79] [Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization](https://arxiv.org/abs/2508.00078)
*Imen Mahmoud,Andrei Velichko*

Main category: cs.LG

TL;DR: 该研究提出了一种结合LightGBM回归模型和遗传算法优化的新方法，评估COVID-19相关指标对比特币回报预测的贡献。结果表明，疫情指标显著提升了预测准确性，尤其是疫苗接种数据。


<details>
  <summary>Details</summary>
Motivation: 研究的主要目标不仅是预测比特币回报，而是确定包含疫情相关健康数据是否能显著提高预测准确性。

Method: 使用LightGBM回归模型和遗传算法优化，构建包含比特币回报和COVID-19指标的全面数据集，并通过31次独立运行进行统计评估。

Result: COVID-19指标显著提升了模型性能（R2增加40%，RMSE降低2%），疫苗接种数据是主要预测因素。

Conclusion: 该方法通过整合公共卫生信号扩展了金融分析工具，为投资者和政策制定者在系统性危机中提供了更精细的市场指标。

Abstract: This study proposes a novel methodological framework integrating a LightGBM
regression model and genetic algorithm (GA) optimization to systematically
evaluate the contribution of COVID-19-related indicators to Bitcoin return
prediction. The primary objective was not merely to forecast Bitcoin returns
but rather to determine whether including pandemic-related health data
significantly enhances prediction accuracy. A comprehensive dataset comprising
daily Bitcoin returns and COVID-19 metrics (vaccination rates,
hospitalizations, testing statistics) was constructed. Predictive models,
trained with and without COVID-19 features, were optimized using GA over 31
independent runs, allowing robust statistical assessment. Performance metrics
(R2, RMSE, MAE) were statistically compared through distribution overlaps and
Mann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified
individual feature contributions. Results indicate that COVID-19 indicators
significantly improved model performance, particularly in capturing extreme
market fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly
significant statistically). Among COVID-19 features, vaccination metrics,
especially the 75th percentile of fully vaccinated individuals, emerged as
dominant predictors. The proposed methodology extends existing financial
analytics tools by incorporating public health signals, providing investors and
policymakers with refined indicators to navigate market uncertainty during
systemic crises.

</details>


### [80] [Stress-Aware Resilient Neural Training](https://arxiv.org/abs/2508.00098)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicole,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: Stress-Aware Learning是一种弹性神经训练范式，通过动态调整优化行为，帮助模型逃离尖锐最小值，收敛到更平坦、泛化性更强的损失区域。


<details>
  <summary>Details</summary>
Motivation: 受材料科学中结构疲劳的启发，解决深度神经网络在训练过程中因优化困难而陷入局部最优的问题。

Method: 提出Plastic Deformation Optimizer，通过内部应力信号检测训练停滞，并注入自适应噪声以调整模型参数。

Result: 在六种架构、四种优化器和七个视觉基准测试中，表现出更强的鲁棒性和泛化能力，且计算开销小。

Conclusion: Stress-Aware Learning提供了一种有效的方法，显著提升了模型的泛化性能和鲁棒性。

Abstract: This paper introduces Stress-Aware Learning, a resilient neural training
paradigm in which deep neural networks dynamically adjust their optimization
behavior - whether under stable training regimes or in settings with uncertain
dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)
Deformation, inspired by structural fatigue in materials science. To
instantiate this concept, we propose Plastic Deformation Optimizer, a
stress-aware mechanism that injects adaptive noise into model parameters
whenever an internal stress signal - reflecting stagnation in training loss and
accuracy - indicates persistent optimization difficulty. This enables the model
to escape sharp minima and converge toward flatter, more generalizable regions
of the loss landscape. Experiments across six architectures, four optimizers,
and seven vision benchmarks demonstrate improved robustness and generalization
with minimal computational overhead. The code and 3D visuals will be available
on GitHub: https://github.com/Stress-Aware-Learning/SAL.

</details>


### [81] [StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection](https://arxiv.org/abs/2508.00117)
*Md. Ehsanul Haque,S. M. Jahidul Islam,Shakil Mia,Rumana Sharmin,Ashikuzzaman,Md Samir Morshed,Md. Tahmidul Huque*

Main category: cs.LG

TL;DR: StackLiverNet是一种可解释的堆叠集成模型，用于肝病检测，通过高级数据预处理和特征选择提高性能，表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有肝病分类模型存在高误分类率、解释性差等问题，需改进。

Method: 采用随机欠采样处理类别不平衡，结合超参数优化的基分类器和LightGBM元模型。

Result: 测试准确率99.89%，Cohen Kappa 0.9974，AUC 0.9993，训练和推理速度快。

Conclusion: StackLiverNet性能优越，适合临床实践，且通过LIME和SHAP提供透明解释。

Abstract: Liver diseases are a serious health concern in the world, which requires
precise and timely diagnosis to enhance the survival chances of patients. The
current literature implemented numerous machine learning and deep learning
models to classify liver diseases, but most of them had some issues like high
misclassification error, poor interpretability, prohibitive computational
expense, and lack of good preprocessing strategies. In order to address these
drawbacks, we introduced StackLiverNet in this study; an interpretable stacked
ensemble model tailored to the liver disease detection task. The framework uses
advanced data preprocessing and feature selection technique to increase model
robustness and predictive ability. Random undersampling is performed to deal
with class imbalance and make the training balanced. StackLiverNet is an
ensemble of several hyperparameter-optimized base classifiers, whose
complementary advantages are used through a LightGBM meta-model. The provided
model demonstrates excellent performance, with the testing accuracy of 99.89%,
Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and
efficient training and inference speeds that are amenable to clinical practice
(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local
Interpretable Model-Agnostic Explanations (LIME) are applied to generate
transparent explanations of individual predictions, revealing high
concentrations of Alkaline Phosphatase and moderate SGOT as important
observations of liver disease. Also, SHAP was used to rank features by their
global contribution to predictions, while the Morris method confirmed the most
influential features through sensitivity analysis.

</details>


### [82] [Structured Transformations for Stable and Interpretable Neural Computation](https://arxiv.org/abs/2508.00127)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.LG

TL;DR: 论文提出了一种结构化层变换方法，通过分解线性操作和残差校正组件，提升神经网络的稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当代神经网络缺乏结构保障，导致学习不稳定和行为难以解释。

Method: 将层变换分解为结构化线性操作和残差校正组件，支持稳定的信息流动。

Result: 实验表明，该方法改善了梯度条件、降低了对扰动的敏感性，并增强了层间鲁棒性。

Conclusion: 该方法为更稳定和透明的神经网络架构奠定了基础，同时不牺牲表达能力。

Abstract: Despite their impressive performance, contemporary neural networks often lack
structural safeguards that promote stable learning and interpretable behavior.
In this work, we introduce a reformulation of layer-level transformations that
departs from the standard unconstrained affine paradigm. Each transformation is
decomposed into a structured linear operator and a residual corrective
component, enabling more disciplined signal propagation and improved training
dynamics. Our formulation encourages internal consistency and supports stable
information flow across depth, while remaining fully compatible with standard
learning objectives and backpropagation. Through a series of synthetic and
real-world experiments, we demonstrate that models constructed with these
structured transformations exhibit improved gradient conditioning, reduced
sensitivity to perturbations, and layer-wise robustness. We further show that
these benefits persist across architectural scales and training regimes. This
study serves as a foundation for a more principled class of neural
architectures that prioritize stability and transparency-offering new tools for
reasoning about learning behavior without sacrificing expressive power.

</details>


### [83] [ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks](https://arxiv.org/abs/2508.00131)
*Christopher Harvey,Sumaiya Shomaji,Zijun Yao,Amit Noheria*

Main category: cs.LG

TL;DR: 论文探讨了利用PCA和Autoencoder简化ECG信号复杂性，并提出了三种VAE变体（SAE、A beta-VAE、C beta-VAE），其中A beta-VAE在信号重建上表现最佳，SAE编码结合传统特征提升了LVEF预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决ECG信号高复杂性和个体差异对深度学习模型的挑战，尤其是在小数据集情况下。

Method: 使用PCA和Autoencoder（包括三种VAE变体）简化ECG数据，并结合LGBM进行下游预测任务。

Result: A beta-VAE将MAE降至15.7±3.2 μV，SAE编码结合传统特征在LVEF预测中AUROC达0.901，接近CNN模型但计算资源更少。

Conclusion: VAE编码能有效简化ECG数据，为小规模标注数据下的深度学习应用提供实用解决方案。

Abstract: The electrocardiogram (ECG) is an inexpensive and widely available tool for
cardiac assessment. Despite its standardized format and small file size, the
high complexity and inter-individual variability of ECG signals (typically a
60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep
learning models, especially when only small training datasets are available.
This study addresses these challenges by exploring feature generation methods
from representative beat ECGs, focusing on Principal Component Analysis (PCA)
and Autoencoders to reduce data complexity. We introduce three novel
Variational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed
beta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their
effectiveness in maintaining signal fidelity and enhancing downstream
prediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE
achieved superior signal reconstruction, reducing the mean absolute error (MAE)
to 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE
encodings, when combined with traditional ECG summary features, improved the
prediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an
holdout test set area under the receiver operating characteristic curve (AUROC)
of 0.901 with a LGBM classifier. This performance nearly matches the 0.909
AUROC of state-of-the-art CNN model but requires significantly less
computational resources. Further, the ECG feature extraction-LGBM pipeline
avoids overfitting and retains predictive performance when trained with less
data. Our findings demonstrate that these VAE encodings are not only effective
in simplifying ECG data but also provide a practical solution for applying deep
learning in contexts with limited-scale labeled training data.

</details>


### [84] [INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks](https://arxiv.org/abs/2508.00141)
*Mohit Gupta,Debjit Bhowmick,Rhys Newbury,Meead Saberi,Shirui Pan,Ben Beck*

Main category: cs.LG

TL;DR: INSPIRE-GNN是一种结合强化学习和图神经网络的框架，用于优化稀疏数据环境中的自行车流量估计和传感器布局。


<details>
  <summary>Details</summary>
Motivation: 解决城市自行车流量数据稀疏性问题，提升交通规划的准确性和可靠性。

Method: 结合图卷积网络（GCN）、图注意力网络（GAT）和深度Q网络（DQN）的强化学习代理，优化传感器布局。

Result: 在墨尔本自行车网络中，显著提升了流量估计性能，优于传统启发式方法和其他机器学习模型。

Conclusion: INSPIRE-GNN为交通规划者提供了有效扩展传感器网络和优化布局的工具，提升了数据准确性和决策可靠性。

Abstract: Accurate link-level bicycling volume estimation is essential for sustainable
urban transportation planning. However, many cities face significant challenges
of high data sparsity due to limited bicycling count sensor coverage. To
address this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning
(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize
sensor placement and improve link-level bicycling volume estimation in
data-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks
(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL
agent, enabling a data-driven strategic selection of sensor locations to
maximize estimation performance. Applied to Melbourne's bicycling network,
comprising 15,933 road segments with sensor coverage on only 141 road segments
(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume
estimation by strategically selecting additional sensor locations in
deployments of 50, 100, 200 and 500 sensors. Our framework outperforms
traditional heuristic methods for sensor placement such as betweenness
centrality, closeness centrality, observed bicycling activity and random
placement, across key metrics such as Mean Squared Error (MSE), Root Mean
Squared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our
experiments benchmark INSPIRE-GNN against standard machine learning and deep
learning models in the bicycle volume estimation performance, underscoring its
effectiveness. Our proposed framework provides transport planners actionable
insights to effectively expand sensor networks, optimize sensor placement and
maximize volume estimation accuracy and reliability of bicycling data for
informed transportation planning decisions.

</details>


### [85] [Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](https://arxiv.org/abs/2508.00161)
*Ziqian Zhong,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 提出了一种基于权重而非激活的新方法，用于理解和监控微调后的LLM，无需与训练数据分布相似的数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活的解释方法需要与训练数据分布相似的数据，限制了其在检测新威胁（如后门）时的应用。

Method: 通过分析微调模型与基础模型权重差异的顶部奇异向量，监控激活的余弦相似度以检测新行为。

Result: 在后门模型中阻止了100%攻击（假阳性率<1.2%），在遗忘模型中检测到95.42%的擦除主题推理，并能恢复“遗忘”信息。

Conclusion: 该方法在监控和预部署审计中表现出潜力，可揭示商业模型的微调重点。

Abstract: The releases of powerful open-weight large language models (LLMs) are often
not accompanied by access to their full training data. Existing
interpretability methods, particularly those based on activations, often
require or assume distributionally similar data. This is a significant
limitation when detecting and defending against novel potential threats like
backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and
controlling fine-tuned LLMs that interprets weights, rather than activations,
thereby side stepping the need for data that is distributionally similar to the
unknown training data. We demonstrate that the top singular vectors of the
weight difference between a fine-tuned model and its base model correspond to
newly acquired behaviors. By monitoring the cosine similarity of activations
along these directions, we can detect salient behaviors introduced during
fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger
is present, our method stops up to 100% of attacks with a false positive rate
below 1.2%. For models that have undergone unlearning, we detect inference on
erased topics with accuracy up to 95.42% and can even steer the model to
recover "unlearned" information. Besides monitoring, our method also shows
potential for pre-deployment model auditing: by analyzing commercial
instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover
model-specific fine-tuning focus including marketing strategies and Midjourney
prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.

</details>


### [86] [DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission](https://arxiv.org/abs/2508.00172)
*Fupei Guo,Hao Zheng,Xiang Zhang,Li Chen,Yue Wang,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的语义通信框架DiSC-Med，用于高效且鲁棒的医学图像传输。


<details>
  <summary>Details</summary>
Motivation: 人工智能和无线通信技术的发展推动了远程医疗的需求，但如何在有限带宽和噪声信道中高效传输医学数据成为关键挑战。

Method: 开发了医学增强的压缩和去噪模块，通过语义通信框架DiSC-Med实现高效带宽利用和鲁棒性。

Result: 在真实医学数据集上的实验表明，DiSC-Med能高效捕获关键语义信息，并在噪声信道中实现卓越的重建性能。

Conclusion: DiSC-Med为远程医疗提供了高效且鲁棒的解决方案，具有实际应用潜力。

Abstract: The rapid development of artificial intelligence has driven smart health with
next-generation wireless communication technologies, stimulating exciting
applications in remote diagnosis and intervention. To enable a timely and
effective response for remote healthcare, efficient transmission of medical
data through noisy channels with limited bandwidth emerges as a critical
challenge. In this work, we propose a novel diffusion-based semantic
communication framework, namely DiSC-Med, for the medical image transmission,
where medical-enhanced compression and denoising blocks are developed for
bandwidth efficiency and robustness, respectively. Unlike conventional
pixel-wise communication framework, our proposed DiSC-Med is able to capture
the key semantic information and achieve superior reconstruction performance
with ultra-high bandwidth efficiency against noisy channels. Extensive
experiments on real-world medical datasets validate the effectiveness of our
framework, demonstrating its potential for robust and efficient telehealth
applications.

</details>


### [87] [RL as Regressor: A Reinforcement Learning Approach for Function Approximation](https://arxiv.org/abs/2508.00174)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 论文提出将回归问题转化为强化学习问题，通过自定义奖励信号和RL算法实现更灵活的目标定义和学习过程。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法受限于预定义的可微损失函数，无法完全捕捉系统行为，尤其是在非对称成本或复杂目标时。

Method: 将模型预测视为动作，基于预测误差定义奖励信号，利用Actor-Critic算法并结合优先级经验回放、网络容量扩展和位置编码。

Result: RL框架成功解决了回归问题，并在目标定义和学习过程中提供了更高的灵活性。

Conclusion: 强化学习为回归问题提供了一种灵活且有效的替代方法。

Abstract: Standard regression techniques, while powerful, are often constrained by
predefined, differentiable loss functions such as mean squared error. These
functions may not fully capture the desired behavior of a system, especially
when dealing with asymmetric costs or complex, non-differentiable objectives.
In this paper, we explore an alternative paradigm: framing regression as a
Reinforcement Learning (RL) problem. We demonstrate this by treating a model's
prediction as an action and defining a custom reward signal based on the
prediction error, and we can leverage powerful RL algorithms to perform
function approximation. Through a progressive case study of learning a noisy
sine wave, we illustrate the development of an Actor-Critic agent, iteratively
enhancing it with Prioritized Experience Replay, increased network capacity,
and positional encoding to enable a capable RL agent for this regression task.
Our results show that the RL framework not only successfully solves the
regression problem but also offers enhanced flexibility in defining objectives
and guiding the learning process.

</details>


### [88] [EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes](https://arxiv.org/abs/2508.00180)
*Adam Block,Cyril Zhang*

Main category: cs.LG

TL;DR: 论文提出BEMA方法，通过修正EMA的偏差，提升语言模型微调的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型微调中因小批量训练导致的随机性及EMA方法引入的偏差问题。

Method: 提出Bias-Corrected Exponential Moving Average (BEMA)，保留EMA的方差减少优势，同时消除偏差。

Result: 实验表明BEMA在收敛速度和最终性能上优于EMA和普通训练。

Conclusion: BEMA是一种实用且理论支持的方法，可提升语言模型微调的稳定性和效率。

Abstract: Stochasticity in language model fine-tuning, often caused by the small batch
sizes typically used in this regime, can destabilize training by introducing
large oscillations in generation quality. A popular approach to mitigating this
instability is to take an Exponential moving average (EMA) of weights
throughout training. While EMA reduces stochasticity, thereby smoothing
training, the introduction of bias from old iterates often creates a lag in
optimization relative to vanilla training. In this work, we propose the
Bias-Corrected Exponential Moving Average (BEMA), a simple and practical
augmentation of EMA that retains variance-reduction benefits while eliminating
bias. BEMA is motivated by a simple theoretical model wherein we demonstrate
provable acceleration of BEMA over both a standard EMA and vanilla training.
Through an extensive suite of experiments on Language Models, we show that BEMA
leads to significantly improved convergence rates and final performance over
both EMA and vanilla training in a variety of standard LM benchmarks, making
BEMA a practical and theoretically motivated intervention for more stable and
efficient fine-tuning.

</details>


### [89] [RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems](https://arxiv.org/abs/2508.00201)
*Mehdi Ben Ayed,Fei Feng,Jay Adams,Vishwakarma Singh,Kritarth Anand,Jiajing Xu*

Main category: cs.LG

TL;DR: RecoMind是一个基于模拟器的强化学习框架，用于优化大规模推荐系统中的会话目标，显著提升用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要依赖监督学习，难以优化长期目标（如会话参与度），而强化学习在大规模应用中面临工程复杂性挑战。

Method: RecoMind利用现有推荐模型构建模拟环境，并通过自定义探索策略高效探索大规模动作空间，简化RL策略的训练和部署。

Result: 离线模拟和在线A/B测试显示，RecoMind训练的RL策略显著优于传统监督学习方法，会话深度和观看时长均有提升。

Conclusion: RecoMind为大规模推荐系统提供了一种可扩展的RL集成方法，优化会话用户满意度。

Abstract: Existing web-scale recommendation systems commonly use supervised learning
methods that prioritize immediate user feedback. Although reinforcement
learning (RL) offers a solution to optimize longer-term goals, such as
in-session engagement, applying it at web scale is challenging due to the
extremely large action space and engineering complexity. In this paper, we
introduce RecoMind, a simulator-based RL framework designed for the effective
optimization of session-based goals at web-scale. RecoMind leverages existing
recommendation models to establish a simulation environment and to bootstrap
the RL policy to optimize immediate user interactions from the outset. This
method integrates well with existing industry pipelines, simplifying the
training and deployment of RL policies. Additionally, RecoMind introduces a
custom exploration strategy to efficiently explore web-scale action spaces with
hundreds of millions of items. We evaluated RecoMind through extensive offline
simulations and online A/B testing on a video streaming platform. Both methods
showed that the RL policy trained using RecoMind significantly outperforms
traditional supervised learning recommendation approaches in in-session user
satisfaction. In online A/B tests, the RL policy increased videos watched for
more than 10 seconds by 15.81\% and improved session depth by 4.71\% for
sessions with at least 10 interactions. As a result, RecoMind presents a
systematic and scalable approach for embedding RL into web-scale recommendation
systems, showing great promise for optimizing session-based user satisfaction.

</details>


### [90] [Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models](https://arxiv.org/abs/2508.00202)
*Ecem Bozkurt,Antonio Ortega*

Main category: cs.LG

TL;DR: 论文提出了一种两阶段框架，利用几何信息改进基础模型在噪声标签数据下的鲁棒分类性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在噪声标签数据下微调时，现有方法依赖局部几何信息，但性能仍有提升空间。

Method: 采用两阶段方法：可靠性估计和加权推理，结合非负核邻域构造和几何信息。

Result: 在CIFAR-10和DermaMNIST上，方法在多种噪声条件下优于标准K-NN和自适应邻域基线。

Conclusion: 通过引入几何信息和改进可靠性估计，显著提升了噪声标签数据下的分类鲁棒性。

Abstract: Foundation models (FMs) pretrained on large datasets have become fundamental
for various downstream machine learning tasks, in particular in scenarios where
obtaining perfectly labeled data is prohibitively expensive. In this paper, we
assume an FM has to be fine-tuned with noisy data and present a two-stage
framework to ensure robust classification in the presence of label noise
without model retraining. Recent work has shown that simple k-nearest neighbor
(kNN) approaches using an embedding derived from an FM can achieve good
performance even in the presence of severe label noise. Our work is motivated
by the fact that these methods make use of local geometry. In this paper,
following a similar two-stage procedure, reliability estimation followed by
reliability-weighted inference, we show that improved performance can be
achieved by introducing geometry information. For a given instance, our
proposed inference uses a local neighborhood of training data, obtained using
the non-negative kernel (NNK) neighborhood construction. We propose several
methods for reliability estimation that can rely less on distance and local
neighborhood as the label noise increases. Our evaluation on CIFAR-10 and
DermaMNIST shows that our methods improve robustness across various noise
conditions, surpassing standard K-NN approaches and recent
adaptive-neighborhood baselines.

</details>


### [91] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: KRAdapter是一种新型的参数高效微调方法，通过Khatri-Rao乘积生成权重更新，解决了LoRA在近似高有效秩矩阵时的局限性，并在大规模模型上表现出色。


<details>
  <summary>Details</summary>
Motivation: LoRA在适应多模态和大语言模型时存在局限性，尤其是在处理高有效秩矩阵时表现不佳。

Method: 提出KRAdapter，利用Khatri-Rao乘积生成权重更新，以更好地适应高有效秩矩阵。

Result: 在1B参数的视觉语言模型和8B参数的大语言模型上，KRAdapter在未见过的常识推理任务中表现优于LoRA。

Conclusion: KRAdapter在保持LoRA计算和内存效率的同时，提供了更强大的性能，是适应大规模模型的实用选择。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

</details>


### [92] [Calibrated Language Models and How to Find Them with Label Smoothing](https://arxiv.org/abs/2508.00264)
*Jerry Huang,Peng Lu,Qiuhao Zeng*

Main category: cs.LG

TL;DR: 研究探讨了指令调优对大型语言模型（LLM）置信度校准的影响，并提出标签平滑作为解决方案，同时解决了内存消耗问题。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优提升了LLM的性能，但其对模型置信度校准的影响尚未充分研究。

Method: 分析了多种开源LLM，发现指令调优后校准退化，提出标签平滑作为解决方案，并设计了定制内核以减少内存消耗。

Result: 标签平滑能有效维持校准，但在大词汇量LLM中效果受限，理论及实验验证了其与隐藏层和词汇量的关系。

Conclusion: 标签平滑是解决校准问题的实用方法，定制内核进一步优化了内存效率。

Abstract: Recent advances in natural language processing (NLP) have opened up greater
opportunities to enable fine-tuned large language models (LLMs) to behave as
more powerful interactive agents through improved instruction-following
ability. However, understanding how this impacts confidence calibration for
reliable model output has not been researched in full. In this work, we examine
various open-sourced LLMs, identifying significant calibration degradation
after instruction tuning in each. Seeking a practical solution, we look towards
label smoothing, which has been shown as an effective method to regularize for
overconfident predictions but has yet to be widely adopted in the supervised
fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing
is sufficient to maintain calibration throughout the SFT process. However,
settings remain where the effectiveness of smoothing is severely diminished, in
particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to
stem from the ability to become over-confident, which has a direct relationship
with the hidden size and vocabulary size, and justify this theoretically and
experimentally. Finally, we address an outstanding issue regarding the memory
footprint of the cross-entropy loss computation in the label smoothed loss
setting, designing a customized kernel to dramatically reduce memory
consumption without sacrificing speed or performance in comparison to existing
solutions for non-smoothed losses.

</details>


### [93] [Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring](https://arxiv.org/abs/2508.00270)
*Robin Schmucker,Nimish Pachapurkar,Shanmuga Bala,Miral Shah,Tom Mitchell*

Main category: cs.LG

TL;DR: 在线辅导系统通过多臂老虎机框架优化学生反馈，提升学习效果，并探索个性化反馈的潜力。


<details>
  <summary>Details</summary>
Motivation: 通过数据驱动的方法优化学生反馈，以提高学习效果，并探索个性化反馈是否能进一步改善结果。

Method: 使用多臂老虎机（MAB）框架和离线策略评估，分析43,000个辅助动作，并设计算法选择适合的训练目标。进一步研究上下文老虎机（CB）策略的个性化效果。

Result: MAB策略显著提升学生表现，但CB策略的个性化效果有限，改进空间较小。

Conclusion: 数据驱动系统能有效优化反馈策略，但个性化反馈的潜力需进一步研究。

Abstract: We present an online tutoring system that learns to provide effective
feedback to students after they answer questions incorrectly. Using data from
one million students, the system learns which assistance action (e.g., one of
multiple hints) to provide for each question to optimize student learning.
Employing the multi-armed bandit (MAB) framework and offline policy evaluation,
we assess 43,000 assistance actions, and identify trade-offs between assistance
policies optimized for different student outcomes (e.g., response correctness,
session completion). We design an algorithm that for each question decides on a
suitable policy training objective to enhance students' immediate second
attempt success and overall practice session performance. We evaluate the
resulting MAB policies in 166,000 practice sessions, verifying significant
improvements in student outcomes. While MAB policies optimize feedback for the
overall student population, we further investigate whether contextual bandit
(CB) policies can enhance outcomes by personalizing feedback based on
individual student features (e.g., ability estimates, response times). Using
causal inference, we examine (i) how effects of assistance actions vary across
students and (ii) whether CB policies, which leverage such effect
heterogeneity, outperform MAB policies. While our analysis reveals that some
actions for some questions exhibit effect heterogeneity, effect sizes may often
be too small for CB policies to provide significant improvements beyond what
well-optimized MAB policies that deliver the same action to all students
already achieve. We discuss insights gained from deploying data-driven systems
at scale and implications for future refinements. Today, the teaching policies
optimized by our system support thousands of students daily.

</details>


### [94] [Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem](https://arxiv.org/abs/2508.00286)
*Mohsen Zaker Esteghamati*

Main category: cs.LG

TL;DR: 该研究提出了一种基于可解释机器学习的性能化抗震设计方法，通过逆向工程直接推导设计参数以满足特定性能目标，并结合遗传优化算法实现高效设计。


<details>
  <summary>Details</summary>
Motivation: 解决性能化抗震设计中的计算效率问题，通过机器学习直接映射设计变量与性能指标。

Method: 使用可解释机器学习模型映射设计变量与性能指标，并将其作为评估函数集成到遗传优化算法中。

Result: 在洛杉矶和查尔斯顿的钢和混凝土框架结构中应用，模型精度高（R2>90%），优化算法能识别符合工程原理的构件最优属性。

Conclusion: 该方法高效且准确，适用于多样化的建筑类型和地震场景，为性能化抗震设计提供了新思路。

Abstract: This study presents a methodology to treat performance-based seismic design
as an inverse engineering problem, where design parameters are directly derived
to achieve specific performance objectives. By implementing explainable machine
learning models, this methodology directly maps design variables and
performance metrics, tackling computational inefficiencies of performance-based
design. The resultant machine learning model is integrated as an evaluation
function into a genetic optimization algorithm to solve the inverse problem.
The developed methodology is then applied to two different inventories of steel
and concrete moment frames in Los Angeles and Charleston to obtain sectional
properties of frame members that minimize expected annualized seismic loss in
terms of repair costs. The results show high accuracy of the surrogate models
(e.g., R2> 90%) across a diverse set of building types, geometries, seismic
design, and site hazard, where the optimization algorithm could identify the
optimum values of members' properties for a fixed set of geometric variables,
consistent with engineering principles.

</details>


### [95] [Invariant Graph Transformer for Out-of-Distribution Generalization](https://arxiv.org/abs/2508.00304)
*Tianyin Liao,Ziwei Zhang,Yufei Sun,Chunyu Hu,Jianxin Li*

Main category: cs.LG

TL;DR: GOODFormer是一种基于图不变学习的Transformer模型，旨在解决图数据分布偏移下的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有图Transformer在分布偏移下泛化能力不足，图不变学习可能提供解决方案。

Method: 通过三个模块联合优化：熵引导的不变子图解耦器、动态子图编码器、不变学习模块。

Result: 在基准数据集上表现优于现有方法。

Conclusion: GOODFormer能有效学习泛化图表示，适用于分布偏移场景。

Abstract: Graph Transformers (GTs) have demonstrated great effectiveness across various
graph analytical tasks. However, the existing GTs focus on training and testing
graph data originated from the same distribution, but fail to generalize under
distribution shifts. Graph invariant learning, aiming to capture generalizable
graph structural patterns with labels under distribution shifts, is potentially
a promising solution, but how to design attention mechanisms and positional and
structural encodings (PSEs) based on graph invariant learning principles
remains challenging. To solve these challenges, we introduce Graph
Out-Of-Distribution generalized Transformer (GOODFormer), aiming to learn
generalized graph representations by capturing invariant relationships between
predictive graph structures and labels through jointly optimizing three
modules. Specifically, we first develop a GT-based entropy-guided invariant
subgraph disentangler to separate invariant and variant subgraphs while
preserving the sharpness of the attention function. Next, we design an evolving
subgraph positional and structural encoder to effectively and efficiently
capture the encoding information of dynamically changing subgraphs during
training. Finally, we propose an invariant learning module utilizing subgraph
node representations and encodings to derive generalizable graph
representations that can to unseen graphs. We also provide theoretical
justifications for our method. Extensive experiments on benchmark datasets
demonstrate the superiority of our method over state-of-the-art baselines under
distribution shifts.

</details>


### [96] [PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models](https://arxiv.org/abs/2508.00325)
*Yongquan Qu,Matthieu Blanke,Sara Shamekh,Pierre Gentine*

Main category: cs.LG

TL;DR: PnP-DA是一种插拔式数据同化算法，通过结合轻量级梯度更新和预训练生成先验，减少地球系统建模中的误差累积，优于传统变分方法。


<details>
  <summary>Details</summary>
Motivation: 地球系统建模中，复杂多尺度非线性动力学和误差累积是主要挑战，传统变分方法的高斯误差假设不适用于混沌系统。

Method: PnP-DA交替使用基于梯度的分析更新和预训练生成先验，避免复杂神经网络的反向传播。

Result: 在标准混沌测试中，PnP-DA显著减少预测误差，优于传统方法。

Conclusion: PnP-DA通过放松统计假设和利用历史数据，提供了一种高效的数据同化解决方案。

Abstract: Earth system modeling presents a fundamental challenge in scientific
computing: capturing complex, multiscale nonlinear dynamics in computationally
efficient models while minimizing forecast errors caused by necessary
simplifications. Even the most powerful AI- or physics-based forecast system
suffer from gradual error accumulation. Data assimilation (DA) aims to mitigate
these errors by optimally blending (noisy) observations with prior model
forecasts, but conventional variational methods often assume Gaussian error
statistics that fail to capture the true, non-Gaussian behavior of chaotic
dynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates
(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance
misfit on new observations) with (2) a single forward pass through a pretrained
generative prior conditioned on the background forecast via a conditional
Wasserstein coupling. This strategy relaxes restrictive statistical assumptions
and leverages rich historical data without requiring an explicit regularization
functional, and it also avoids the need to backpropagate gradients through the
complex neural network that encodes the prior during assimilation cycles.
Experiments on standard chaotic testbeds demonstrate that this strategy
consistently reduces forecast errors across a range of observation sparsities
and noise levels, outperforming classical variational methods.

</details>


### [97] [Embryology of a Language Model](https://arxiv.org/abs/2508.00331)
*George Wang,Garrett Baker,Andrew Gordon,Daniel Murfet*

Main category: cs.LG

TL;DR: 论文提出了一种基于UMAP和敏感性分析的方法，可视化语言模型在训练过程中的结构发展，揭示了新的网络机制。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型内部计算结构的形成机制，探索敏感性分析在可视化网络组织中的潜力。

Method: 应用UMAP对敏感性矩阵进行分析，可视化模型在训练过程中的结构发展。

Result: 发现了清晰的“身体计划”，包括已知的感应电路和新的结构（如“间距鳍”）。

Conclusion: 敏感性分析不仅能验证模型，还能揭示新机制，为研究复杂神经网络的发育原理提供了强大工具。

Abstract: Understanding how language models develop their internal computational
structure is a central problem in the science of deep learning. While
susceptibilities, drawn from statistical physics, offer a promising analytical
tool, their full potential for visualizing network organization remains
untapped. In this work, we introduce an embryological approach, applying UMAP
to the susceptibility matrix to visualize the model's structural development
over training. Our visualizations reveal the emergence of a clear ``body
plan,'' charting the formation of known features like the induction circuit and
discovering previously unknown structures, such as a ``spacing fin'' dedicated
to counting space tokens. This work demonstrates that susceptibility analysis
can move beyond validation to uncover novel mechanisms, providing a powerful,
holistic lens for studying the developmental principles of complex neural
networks.

</details>


### [98] [BOOD: Boundary-based Out-Of-Distribution Data Generation](https://arxiv.org/abs/2508.00350)
*Qilin Liao,Shuo Yang,Bo Zhao,Ping Luo,Hengshuang Zhao*

Main category: cs.LG

TL;DR: BOOD框架利用扩散模型生成高质量的OOD特征和图像，显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在潜在空间中提取ID边界外的有效特征，BOOD旨在解决这一问题。

Method: BOOD通过学习文本条件潜在空间，选择接近决策边界的ID特征并扰动生成OOD特征，再用扩散模型解码为图像。

Result: 在CIFAR-100上，BOOD显著优于现有方法，FPR95降低29.64%，AUROC提升7.27%。

Conclusion: BOOD提供了一种高效生成OOD特征的策略，显著提升了OOD检测性能。

Abstract: Harnessing the power of diffusion models to synthesize auxiliary training
data based on latent space features has proven effective in enhancing
out-of-distribution (OOD) detection performance. However, extracting effective
features outside the in-distribution (ID) boundary in latent space remains
challenging due to the difficulty of identifying decision boundaries between
classes. This paper proposes a novel framework called Boundary-based
Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD
features and generates human-compatible outlier images using diffusion models.
BOOD first learns a text-conditioned latent feature space from the ID dataset,
selects ID features closest to the decision boundary, and perturbs them to
cross the decision boundary to form OOD features. These synthetic OOD features
are then decoded into images in pixel space by a diffusion model. Compared to
previous works, BOOD provides a more training efficient strategy for
synthesizing informative OOD features, facilitating clearer distinctions
between ID and OOD data. Extensive experimental results on common benchmarks
demonstrate that BOOD surpasses the state-of-the-art method significantly,
achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%
improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.

</details>


### [99] [Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization](https://arxiv.org/abs/2508.00357)
*Yoonhyuk Choi,Jiho Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: SGPC是一种结合细胞鞘消息传递的新方法，通过PAC-Bayes校准提升GNN在异质图上的性能，提供稳定性保证。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在异质图上因过度平滑导致的节点特征崩溃问题，现有鞘网络方法泛化性和扩展性不足。

Method: 结合细胞鞘消息传递、最优传输提升、方差减少扩散和PAC-Bayes谱正则化，实现端到端线性复杂度训练。

Result: 在九种基准测试中优于现有方法，并提供未见节点的置信区间。

Conclusion: SGPC在性能和稳定性上均优于现有方法，适用于异质图分类任务。

Abstract: Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct
node features, particularly on heterophilic graphs where adjacent nodes often
have dissimilar labels. Although sheaf neural networks partially mitigate this
problem, they typically rely on static or heavily parameterized sheaf
structures that hinder generalization and scalability. Existing sheaf-based
models either predefine restriction maps or introduce excessive complexity, yet
fail to provide rigorous stability guarantees. In this paper, we introduce a
novel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified
architecture that combines cellular-sheaf message passing with several
mechanisms, including optimal transport-based lifting, variance-reduced
diffusion, and PAC-Bayes spectral regularization for robust semi-supervised
node classification. We establish performance bounds theoretically and
demonstrate that the resulting bound-aware objective can be achieved via
end-to-end training in linear computational complexity. Experiments on nine
homophilic and heterophilic benchmarks show that SGPC outperforms
state-of-the-art spectral and sheaf-based GNNs while providing certified
confidence intervals on unseen nodes.

</details>


### [100] [OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions](https://arxiv.org/abs/2508.00364)
*Chanyoung Yoon,Sangbong Yoo,Soobin Yim,Chansoo Kim,Yun Jang*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的室内设计优化框架OID-PPO，通过整合专家定义的设计原则，显著提升了布局质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 住宅室内设计对居住满意度影响重大，但现有方法存在计算成本高、数据稀缺或设计原则融入不足的问题。

Method: 采用Proximal Policy Optimization（PPO）强化学习框架，结合对角高斯策略实现连续灵活的家具布局，并融入功能与视觉设计准则。

Result: 实验表明，OID-PPO在多样房间形状和家具配置下，布局质量和计算效率均优于现有方法。

Conclusion: OID-PPO通过结构化奖励函数和连续动作空间，有效解决了室内设计中的挑战，并验证了设计准则的重要性。

Abstract: Designing residential interiors strongly impacts occupant satisfaction but
remains challenging due to unstructured spatial layouts, high computational
demands, and reliance on expert knowledge. Existing methods based on
optimization or deep learning are either computationally expensive or
constrained by data scarcity. Reinforcement learning (RL) approaches often
limit furniture placement to discrete positions and fail to incorporate design
principles adequately. We propose OID-PPO, a novel RL framework for Optimal
Interior Design using Proximal Policy Optimization, which integrates
expert-defined functional and visual guidelines into a structured reward
function. OID-PPO utilizes a diagonal Gaussian policy for continuous and
flexible furniture placement, effectively exploring latent environmental
dynamics under partial observability. Experiments conducted across diverse room
shapes and furniture configurations demonstrate that OID-PPO significantly
outperforms state-of-the-art methods in terms of layout quality and
computational efficiency. Ablation studies further demonstrate the impact of
structured guideline integration and reveal the distinct contributions of
individual design constraints.

</details>


### [101] [Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions](https://arxiv.org/abs/2508.00392)
*Lijun Zhang,Wenhao Yang,Guanghui Wang,Wei Jiang,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种具有双重适应性的通用算法框架，用于在线凸优化中最小化自适应遗憾，能够自动适应函数类型和环境变化。


<details>
  <summary>Details</summary>
Motivation: 现有算法缺乏通用性，只能处理单一类型的凸函数且需要先验参数知识，限制了实际应用。

Method: 提出基于元专家框架的双重自适应算法，动态创建多个专家并通过元算法聚合，结合睡眠专家技术捕捉环境变化。

Result: 理论分析表明，算法能同时最小化多种凸函数的自适应遗憾，并允许函数类型在轮次间切换。

Conclusion: 该框架可扩展到在线复合优化，为复合函数的自适应遗憾最小化提供了通用解决方案。

Abstract: To deal with changing environments, a new performance measure -- adaptive
regret, defined as the maximum static regret over any interval, was proposed in
online learning. Under the setting of online convex optimization, several
algorithms have been successfully developed to minimize the adaptive regret.
However, existing algorithms lack universality in the sense that they can only
handle one type of convex functions and need apriori knowledge of parameters,
which hinders their application in real-world scenarios. To address this
limitation, this paper investigates universal algorithms with dual adaptivity,
which automatically adapt to the property of functions (convex, exponentially
concave, or strongly convex), as well as the nature of environments (stationary
or changing). Specifically, we propose a meta-expert framework for dual
adaptive algorithms, where multiple experts are created dynamically and
aggregated by a meta-algorithm. The meta-algorithm is required to yield a
second-order bound, which can accommodate unknown function types. We further
incorporate the technique of sleeping experts to capture the changing
environments. For the construction of experts, we introduce two strategies
(increasing the number of experts or enhancing the capabilities of experts) to
achieve universality. Theoretical analysis shows that our algorithms are able
to minimize the adaptive regret for multiple types of convex functions
simultaneously, and also allow the type of functions to switch between rounds.
Moreover, we extend our meta-expert framework to online composite optimization,
and develop a universal algorithm for minimizing the adaptive regret of
composite functions.

</details>


### [102] [ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs](https://arxiv.org/abs/2508.00394)
*Antonis Klironomos,Baifan Zhou,Zhipeng Tan,Zhuoxun Zheng,Mohamed H. Gad-Elrab,Heiko Paulheim,Evgeny Kharlamov*

Main category: cs.LG

TL;DR: ExeKGLib是一个Python库，通过图形界面和知识图谱帮助非ML专家构建ML管道。


<details>
  <summary>Details</summary>
Motivation: 解决非ML专家在构建高质量ML管道时缺乏专业知识的问题。

Method: 利用知识图谱编码ML知识，提供图形界面简化操作。

Result: ExeKGLib提高了ML管道的透明性和可重用性，并通过实际用例验证了其可用性。

Conclusion: ExeKGLib为非ML专家提供了一种简单有效的ML管道构建工具。

Abstract: Nowadays machine learning (ML) practitioners have access to numerous ML
libraries available online. Such libraries can be used to create ML pipelines
that consist of a series of steps where each step may invoke up to several ML
libraries that are used for various data-driven analytical tasks. Development
of high-quality ML pipelines is non-trivial; it requires training, ML
expertise, and careful development of each step. At the same time, domain
experts in science and engineering may not possess such ML expertise and
training while they are in pressing need of ML-based analytics. In this paper,
we present our ExeKGLib, a Python library enhanced with a graphical interface
layer that allows users with minimal ML knowledge to build ML pipelines. This
is achieved by relying on knowledge graphs that encode ML knowledge in simple
terms accessible to non-ML experts. ExeKGLib also allows improving the
transparency and reusability of the built ML workflows and ensures that they
are executable. We show the usability and usefulness of ExeKGLib by presenting
real use cases.

</details>


### [103] [Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement](https://arxiv.org/abs/2508.00410)
*Zizhuo Zhang,Jianing Zhu,Xinmu Ge,Zihua Zhao,Zhanke Zhou,Xuan Li,Xiao Feng,Jiangchao Yao,Bo Han*

Main category: cs.LG

TL;DR: 论文提出了一种名为Co-Reward的新型强化学习框架，通过对比语义相似问题的答案一致性来构建奖励信号，解决了现有自奖励方法中的崩溃问题，并在多个推理基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法依赖于人工标注的奖励信号，尤其是在复杂任务中，这限制了其扩展性。自奖励方法虽然展现了潜力，但存在崩溃问题。因此，论文提出了一种自监督的奖励机制，以提高推理的稳定性和性能。

Method: 论文提出了Co-Reward框架，通过为每个训练样本构造相似问题，并通过投票合成代理标签，然后通过交叉引用问题对的标签来构建奖励信号，以增强推理一致性。

Result: 实验表明，Co-Reward在多个推理基准测试和LLM系列中表现优于其他自奖励基线方法，并在某些情况下甚至超过了基于真实标签的奖励，如在MATH500上对Llama-3.2-3B-Instruct的改进达到+6.8%。

Conclusion: Co-Reward通过自监督的奖励机制有效解决了自奖励方法中的崩溃问题，显著提升了推理能力，并在实验中验证了其优越性。

Abstract: Although reinforcement learning with verifiable rewards (RLVR) shows promise
in improving the reasoning ability of large language models (LLMs), the scaling
up dilemma remains due to the reliance on human annotated labels especially for
complex tasks. Recent alternatives that explore various self-reward signals
exhibit the eliciting potential of LLM reasoning, but suffer from the
non-negligible collapse issue. Inspired by the success of self-supervised
learning, we propose \textit{Co-Reward}, a novel RL framework that leverages
contrastive agreement across semantically analogical questions as a reward
basis. Specifically, we construct a similar question for each training sample
(without labels) and synthesize their individual surrogate labels through a
simple rollout voting, and then the reward is constructed by cross-referring
the labels of each question pair to enforce the internal reasoning consistency
across analogical inputs. Intuitively, such a self-supervised reward-shaping
mechanism increases the difficulty of learning collapse into a trivial
solution, and promotes stable reasoning elicitation and improvement through
expanding the input sample variants. Empirically, Co-Reward achieves superior
performance compared to other self-reward baselines on multiple reasoning
benchmarks and LLM series, and reaches or even surpasses ground-truth (GT)
labeled reward, with improvements of up to $+6.8\%$ on MATH500 over GT reward
on Llama-3.2-3B-Instruct. Our code is publicly available at
https://github.com/tmlr-group/Co-Reward.

</details>


### [104] [Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection](https://arxiv.org/abs/2508.00415)
*Yue Yang,Yuxiang Lin,Ying Zhang,Zihan Su,Chang Chuan Goh,Tangtangfang Fang,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 该研究提出了一种名为ResE-BiLSTM的模型，用于预测贷款违约，并通过滑动窗口技术和SHAP分析提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 贷款违约预测对信用风险管理至关重要，现有机器学习方法需要进一步优化。

Method: 使用ResE-BiLSTM模型，结合滑动窗口技术，并在Freddie Mac数据集上评估，与五种基线模型对比。

Result: 实验表明ResE-BiLSTM在多项指标上优于基线模型，具有实际应用价值。

Conclusion: ResE-BiLSTM模型在贷款违约预测中表现优异，适用于实际场景。

Abstract: Prediction of post-loan default is an important task in credit risk
management, and can be addressed by detection of financial anomalies using
machine learning. This study introduces a ResE-BiLSTM model, using a sliding
window technique, and is evaluated on 44 independent cohorts from the extensive
Freddie Mac US mortgage dataset, to improve prediction performance. The
ResE-BiLSTM is compared with five baseline models: Long Short-Term Memory
(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks
(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including
Accuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to
evaluate the contribution of individual components in the ResE-BiLSTM
architecture. Additionally, SHAP analysis was employed to interpret the
underlying features the model relied upon for its predictions. Experimental
results demonstrate that ResE-BiLSTM achieves superior predictive performance
compared to baseline models, underscoring its practical value and applicability
in real-world scenarios.

</details>


### [105] [A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces](https://arxiv.org/abs/2508.00472)
*Leonidas Akritidis,Panayiotis Bozanis*

Main category: cs.LG

TL;DR: 论文提出ctdGAN，一种用于解决表格数据类别不平衡问题的条件GAN，通过空间分区和概率采样策略生成高质量样本。


<details>
  <summary>Details</summary>
Motivation: 表格数据中的类别不平衡问题严重影响机器学习性能，现有GAN方法未考虑输入样本的向量子空间，导致生成数据位置不准确。

Method: ctdGAN通过空间分区为输入样本分配聚类标签，利用新损失函数和概率采样策略生成样本，同时引入聚类缩放技术。

Result: 在14个不平衡数据集上的实验表明，ctdGAN能生成高保真样本并提升分类准确率。

Conclusion: ctdGAN通过改进生成策略和损失函数，有效解决了表格数据类别不平衡问题。

Abstract: The tabular form constitutes the standard way of representing data in
relational database systems and spreadsheets. But, similarly to other forms,
tabular data suffers from class imbalance, a problem that causes serious
performance degradation in a wide variety of machine learning tasks. One of the
most effective solutions dictates the usage of Generative Adversarial Networks
(GANs) in order to synthesize artificial data instances for the
under-represented classes. Despite their good performance, none of the proposed
GAN models takes into account the vector subspaces of the input samples in the
real data space, leading to data generation in arbitrary locations. Moreover,
the class labels are treated in the same manner as the other categorical
variables during training, so conditional sampling by class is rendered less
effective. To overcome these problems, this study presents ctdGAN, a
conditional GAN for alleviating class imbalance in tabular datasets. Initially,
ctdGAN executes a space partitioning step to assign cluster labels to the input
samples. Subsequently, it utilizes these labels to synthesize samples via a
novel probabilistic sampling strategy and a new loss function that penalizes
both cluster and class mis-predictions. In this way, ctdGAN is trained to
generate samples in subspaces that resemble those of the original data
distribution. We also introduce several other improvements, including a simple,
yet effective cluster-wise scaling technique that captures multiple feature
modes without affecting data dimensionality. The exhaustive evaluation of
ctdGAN with 14 imbalanced datasets demonstrated its superiority in generating
high fidelity samples and improving classification accuracy.

</details>


### [106] [Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection](https://arxiv.org/abs/2508.00507)
*Yiming Xu,Jiarun Chen,Zhen Peng,Zihan Chen,Qika Lin,Lan Ma,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出了一种结合大型语言模型（LLMs）和图神经网络（GNNs）的新框架CoLL，用于文本属性图（TAGs）中的异常检测，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法忽视文本模态的潜力，而LLMs虽能理解语义但难以编码图结构信息，因此需要结合两者的优势。

Method: CoLL框架通过多LLM协作生成异常相关证据，并结合GNN的门控机制自适应融合文本特征与图结构信息。

Result: 实验表明CoLL在AP指标上平均提升13.37%。

Conclusion: CoLL为利用LLMs推进图异常检测开辟了新途径。

Abstract: The natural combination of intricate topological structures and rich textual
information in text-attributed graphs (TAGs) opens up a novel perspective for
graph anomaly detection (GAD). However, existing GAD methods primarily focus on
designing complex optimization objectives within the graph domain, overlooking
the complementary value of the textual modality, whose features are often
encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so
that semantic context related to anomalies may be missed. To unleash the
enormous potential of textual modality, large language models (LLMs) have
emerged as promising alternatives due to their strong semantic understanding
and reasoning capabilities. Nevertheless, their application to TAG anomaly
detection remains nascent, and they struggle to encode high-order structural
information inherent in graphs due to input length constraints. For
high-quality anomaly detection in TAGs, we propose CoLL, a novel framework that
combines LLMs and graph neural networks (GNNs) to leverage their complementary
strengths. CoLL employs multi-LLM collaboration for evidence-augmented
generation to capture anomaly-relevant contexts while delivering human-readable
rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped
with a gating mechanism to adaptively fuse textual features with evidence while
preserving high-order topological information. Extensive experiments
demonstrate the superiority of CoLL, achieving an average improvement of 13.37%
in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.

</details>


### [107] [Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning](https://arxiv.org/abs/2508.00513)
*Yiming Xu,Xu Hua,Zhen Peng,Bin Shi,Jiarun Chen,Xingbo Fu,Song Wang,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出了一种名为CMUCL的端到端方法，用于文本属性图的异常检测，通过联合训练文本和图形编码器，利用跨模态和多尺度一致性提高检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本属性图的异常检测中，文本编码与异常检测目标分离，导致检测能力受限。

Method: 提出CMUCL方法，联合训练文本和图形编码器，利用跨模态和多尺度一致性挖掘异常信息，设计基于不一致性挖掘的异常评分器。

Result: 实验表明，CMUCL在文本属性图异常检测中表现优异，平均准确率（AP）提升了11.13%。

Conclusion: CMUCL方法有效整合文本和图结构信息，显著提升了异常检测性能，并发布了8个数据集以促进未来研究。

Abstract: The widespread application of graph data in various high-risk scenarios has
increased attention to graph anomaly detection (GAD). Faced with real-world
graphs that often carry node descriptions in the form of raw text sequences,
termed text-attributed graphs (TAGs), existing graph anomaly detection
pipelines typically involve shallow embedding techniques to encode such textual
information into features, and then rely on complex self-supervised tasks
within the graph domain to detect anomalies. However, this text encoding
process is separated from the anomaly detection training objective in the graph
domain, making it difficult to ensure that the extracted textual features focus
on GAD-relevant information, seriously constraining the detection capability.
How to seamlessly integrate raw text and graph topology to unleash the vast
potential of cross-modal data in TAGs for anomaly detection poses a challenging
issue. This paper presents a novel end-to-end paradigm for text-attributed
graph anomaly detection, named CMUCL. We simultaneously model data from both
text and graph structures, and jointly train text and graph encoders by
leveraging cross-modal and uni-modal multi-scale consistency to uncover
potential anomaly-related information. Accordingly, we design an anomaly score
estimator based on inconsistency mining to derive node-specific anomaly scores.
Considering the lack of benchmark datasets tailored for anomaly detection on
TAGs, we release 8 datasets to facilitate future research. Extensive
evaluations show that CMUCL significantly advances in text-attributed graph
anomaly detection, delivering an 11.13% increase in average accuracy (AP) over
the suboptimal.

</details>


### [108] [Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting](https://arxiv.org/abs/2508.00523)
*Sifan Yang,Yuanyu Wan,Lijun Zhang*

Main category: cs.LG

TL;DR: 论文研究了在线非子模优化问题，提出了两种算法改进延迟反馈下的遗憾边界。


<details>
  <summary>Details</summary>
Motivation: 现有方法对延迟敏感且未解耦延迟与反馈的影响，需改进。

Method: 提出DBGD-NF算法和扩展版本，分别优化平均延迟和延迟与反馈的耦合问题。

Result: 新算法分别实现更优的遗憾边界，实验验证了其优越性。

Conclusion: 新方法在延迟和反馈解耦方面表现更优，适用于稀疏学习等场景。

Abstract: We investigate the online nonsubmodular optimization with delayed feedback in
the bandit setting, where the loss function is $\alpha$-weakly DR-submodular
and $\beta$-weakly DR-supermodular. Previous work has established an
$(\alpha,\beta)$-regret bound of $\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is
the dimensionality and $d$ is the maximum delay. However, its regret bound
relies on the maximum delay and is thus sensitive to irregular delays.
Additionally, it couples the effects of delays and bandit feedback as its bound
is the product of the delay term and the $\mathcal{O}(nT^{2/3})$ regret bound
in the bandit setting without delayed feedback. In this paper, we develop two
algorithms to address these limitations, respectively. Firstly, we propose a
novel method, namely DBGD-NF, which employs the one-point gradient estimator
and utilizes all the available estimated gradients in each round to update the
decision. It achieves a better $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ regret
bound, which is relevant to the average delay $\bar{d} =
\frac{1}{T}\sum_{t=1}^T d_t\leq d$. Secondly, we extend DBGD-NF by employing a
blocking update mechanism to decouple the joint effect of the delays and bandit
feedback, which enjoys an $\mathcal{O}(n(T^{2/3} + \sqrt{dT}))$ regret bound.
When $d = \mathcal{O}(T^{1/3})$, our regret bound matches the
$\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.
Compared to our first $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ bound, it is more
advantageous when the maximum delay $d = o(\bar{d}^{2/3}T^{1/3})$. Finally, we
conduct experiments on structured sparse learning to demonstrate the
superiority of our methods.

</details>


### [109] [Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery](https://arxiv.org/abs/2508.00539)
*Judy X Yang*

Main category: cs.LG

TL;DR: 提出一种两阶段框架，通过信噪比筛选和光谱平滑增强矿物检测，结合聚类和NNLS解混，提高弱矿物区域检测精度。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像中弱矿物信号常被噪声和冗余波段掩盖，限制了检测性能。

Method: 两阶段方法：1) 信噪比筛选和Savitzky-Golay平滑；2) KMeans聚类和NNLS解混。

Result: 实验证明该方法提高了解混精度和弱矿物区域检测能力。

Conclusion: 两阶段策略为地质高光谱应用提供了实用且可重复的解决方案。

Abstract: Hyperspectral imaging offers detailed spectral information for mineral
mapping; however, weak mineral signatures are often masked by noisy and
redundant bands, limiting detection performance. To address this, we propose a
two-stage integrated framework for enhanced mineral detection in the Cuprite
mining district. In the first stage, we compute the signal-to-noise ratio (SNR)
for each spectral band and apply a phase-locked thresholding technique to
discard low-SNR bands, effectively removing redundancy and suppressing
background noise. Savitzky-Golay filtering is then employed for spectral
smoothing, serving a dual role first to stabilize trends during band selection,
and second to preserve fine-grained spectral features during preprocessing. In
the second stage, the refined HSI data is reintroduced into the model, where
KMeans clustering is used to extract 12 endmember spectra (W1 custom), followed
by non negative least squares (NNLS) for abundance unmixing. The resulting
endmembers are quantitatively compared with laboratory spectra (W1 raw) using
cosine similarity and RMSE metrics. Experimental results confirm that our
proposed pipeline improves unmixing accuracy and enhances the detection of weak
mineral zones. This two-pass strategy demonstrates a practical and reproducible
solution for spectral dimensionality reduction and unmixing in geological HSI
applications.

</details>


### [110] [Foundations of Interpretable Models](https://arxiv.org/abs/2508.00545)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Alberto Termine,Mateja Jamnik,Giuseppe Marra*

Main category: cs.LG

TL;DR: 论文提出了一种可操作的通用解释性定义，并设计了一个可解释模型的蓝图和开源库。


<details>
  <summary>Details</summary>
Motivation: 现有解释性定义缺乏可操作性，导致研究难以指导实际设计。

Method: 提出新的通用解释性定义，并基于此设计模型蓝图和开源库。

Result: 新定义揭示了设计可解释模型所需的基础属性和架构特征。

Conclusion: 研究为可解释AI提供了理论基础和实用工具。

Abstract: We argue that existing definitions of interpretability are not actionable in
that they fail to inform users about general, sound, and robust interpretable
model design. This makes current interpretability research fundamentally
ill-posed. To address this issue, we propose a definition of interpretability
that is general, simple, and subsumes existing informal notions within the
interpretable AI community. We show that our definition is actionable, as it
directly reveals the foundational properties, underlying assumptions,
principles, data structures, and architectural features necessary for designing
interpretable models. Building on this, we propose a general blueprint for
designing interpretable models and introduce the first open-sourced library
with native support for interpretable data structures and processes.

</details>


### [111] [Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides](https://arxiv.org/abs/2508.00578)
*Marlen Neubert,Patrick Reiser,Frauke Gräter,Pascal Friederich*

Main category: cs.LG

TL;DR: 该论文通过机器学习方法（特别是图神经网络MACE）成功预测了氢原子转移（HAT）反应的势能面和反应能垒，为生物分子系统中的量子精确模拟提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 氢原子转移（HAT）反应在生物过程中至关重要，但其机理尚不完全清楚。传统模拟方法（如经典力场或DFT分子动力学）无法满足需求，因此需要开发高精度的机器学习势能模型。

Method: 通过半经验方法和DFT生成大量HAT构型数据集，并比较三种图神经网络（SchNet、Allegro和MACE）在预测势能面和反应能垒上的性能。MACE表现最佳。

Result: MACE在能量、力和能垒预测上均优于其他模型，对DFT能垒预测的平均绝对误差为1.13 kcal/mol，成功应用于胶原蛋白模拟。

Conclusion: 该方法可推广至其他生物分子系统，结合过渡态搜索算法和主动学习，有望实现复杂环境中化学反应性的量子精确模拟。

Abstract: Hydrogen atom transfer (HAT) reactions are essential in many biological
processes, such as radical migration in damaged proteins, but their mechanistic
pathways remain incompletely understood. Simulating HAT is challenging due to
the need for quantum chemical accuracy at biologically relevant scales; thus,
neither classical force fields nor DFT-based molecular dynamics are applicable.
Machine-learned potentials offer an alternative, able to learn potential energy
surfaces (PESs) with near-quantum accuracy. However, training these models to
generalize across diverse HAT configurations, especially at radical positions
in proteins, requires tailored data generation and careful model selection.
Here, we systematically generate HAT configurations in peptides to build large
datasets using semiempirical methods and DFT. We benchmark three graph neural
network architectures (SchNet, Allegro, and MACE) on their ability to learn HAT
PESs and indirectly predict reaction barriers from energy predictions. MACE
consistently outperforms the others in energy, force, and barrier prediction,
achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT
barrier predictions. This accuracy enables integration of ML potentials into
large-scale collagen simulations to compute reaction rates from predicted
barriers, advancing mechanistic understanding of HAT and radical migration in
peptides. We analyze scaling laws, model transferability, and cost-performance
trade-offs, and outline strategies for improvement by combining ML potentials
with transition state search algorithms and active learning. Our approach is
generalizable to other biomolecular systems, enabling quantum-accurate
simulations of chemical reactivity in complex environments.

</details>


### [112] [The Role of Active Learning in Modern Machine Learning](https://arxiv.org/abs/2508.00586)
*Thorben Werner,Lars Schmidt-Thieme,Vijaya Krishna Yalavarthi*

Main category: cs.LG

TL;DR: 研究发现，主动学习（AL）在低数据场景下效率最低，提升仅为1-4%，而数据增强（DA）和半监督学习（SSL）可提升高达60%。但AL与DA和SSL结合后仍能提供额外性能提升。


<details>
  <summary>Details</summary>
Motivation: 探讨AL在实际应用中较少使用的原因，并研究其在低数据场景下的表现。

Method: 比较AL、DA和SSL在低数据场景下的效果，并测试它们的组合性能。

Result: AL单独效果较差，但与DA和SSL结合后仍能提供额外提升。

Conclusion: AL应作为DA和SSL后的补充工具，而非主要解决低数据问题的方法。

Abstract: Even though Active Learning (AL) is widely studied, it is rarely applied in
contexts outside its own scientific literature. We posit that the reason for
this is AL's high computational cost coupled with the comparatively small lifts
it is typically able to generate in scenarios with few labeled points. In this
work we study the impact of different methods to combat this low data scenario,
namely data augmentation (DA), semi-supervised learning (SSL) and AL. We find
that AL is by far the least efficient method of solving the low data problem,
generating a lift of only 1-4\% over random sampling, while DA and SSL methods
can generate up to 60\% lift in combination with random sampling. However, when
AL is combined with strong DA and SSL techniques, it surprisingly is still able
to provide improvements. Based on these results, we frame AL not as a method to
combat missing labels, but as the final building block to squeeze the last bits
of performance out of data after appropriate DA and SSL methods as been
applied.

</details>


### [113] [Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data](https://arxiv.org/abs/2508.00615)
*Mukesh Kumar Sahu,Pinki Roy*

Main category: cs.LG

TL;DR: 论文提出了一种基于相似性的自构建图模型（SBSCGM）和混合图神经网络（HybridGraphMedGNN），用于预测ICU患者的死亡风险和连续关键性评分，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以利用电子健康记录（EHR）中的关系结构，且孤立处理患者数据，无法动态捕捉患者间的相似性。

Method: SBSCGM通过混合相似性度量动态构建患者相似图，HybridGraphMedGNN结合GCN、GraphSAGE和GAT层学习患者表征。

Result: 在MIMIC-III数据集上，模型AUC-ROC达0.94，优于基线分类器和单一GNN模型，并提供可解释性。

Conclusion: 该框架为ICU风险预测提供了可扩展且可解释的解决方案，有望支持临床决策。

Abstract: Accurately predicting the criticalness of ICU patients (such as in-ICU
mortality risk) is vital for early intervention in critical care. However,
conventional models often treat each patient in isolation and struggle to
exploit the relational structure in Electronic Health Records (EHR). We propose
a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds
a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN
architecture that operates on this graph to predict patient mortality and a
continuous criticalness score. SBSCGM uses a hybrid similarity measure
(combining feature-based and structural similarities) to connect patients with
analogous clinical profiles in real-time. The HybridGraphMedGNN integrates
Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)
layers to learn robust patient representations, leveraging both local and
global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III
dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)
outperforming baseline classifiers and single-type GNN models. We also
demonstrate improved precision/recall and show that the attention mechanism
provides interpretable insights into model predictions. Our framework offers a
scalable and interpretable solution for critical care risk prediction, with
potential to support clinicians in real-world ICU deployment.

</details>


### [114] [IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources](https://arxiv.org/abs/2508.00627)
*Paul Tresson,Pierre Le Coz,Hadrien Tulet,Anthony Malkassian,Maxime Réjou Méchain*

Main category: cs.LG

TL;DR: IAMAP是一个用户友好的QGIS插件，通过自监督学习策略简化了遥感图像分析的深度学习应用，无需大量数据集或GPU资源。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在遥感领域应用中的三大挑战：需要大量数据集、计算资源和编程技能。

Method: 利用自监督学习的预训练模型（基础模型），提供特征提取、降维、聚类等功能，支持少样本或零样本场景。

Result: IAMAP为非AI专家提供了高效、节能的深度学习工具，推动了深度学习方法的普及。

Conclusion: IAMAP通过简化流程和降低技术门槛，促进了遥感领域深度学习的广泛应用。

Abstract: Remote sensing has entered a new era with the rapid development of artificial
intelligence approaches. However, the implementation of deep learning has
largely remained restricted to specialists and has been impractical because it
often requires (i) large reference datasets for model training and validation;
(ii) substantial computing resources; and (iii) strong coding skills. Here, we
introduce IAMAP, a user-friendly QGIS plugin that addresses these three
challenges in an easy yet flexible way. IAMAP builds on recent advancements in
self-supervised learning strategies, which now provide robust feature
extractors, often referred to as foundation models. These generalist models can
often be reliably used in few-shot or zero-shot scenarios (i.e., with little to
no fine-tuning). IAMAP's interface allows users to streamline several key steps
in remote sensing image analysis: (i) extracting image features using a wide
range of deep learning architectures; (ii) reducing dimensionality with
built-in algorithms; (iii) performing clustering on features or their reduced
representations; (iv) generating feature similarity maps; and (v) calibrating
and validating supervised machine learning models for prediction. By enabling
non-AI specialists to leverage the high-quality features provided by recent
deep learning approaches without requiring GPU capacity or extensive reference
datasets, IAMAP contributes to the democratization of computationally efficient
and energy-conscious deep learning methods.

</details>


### [115] [Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs](https://arxiv.org/abs/2508.00628)
*Xiong Xiong,Zhuo Zhang,Rongchun Hu,Chen Gao,Zichen Deng*

Main category: cs.LG

TL;DR: SV-SNN是一种新型神经网络框架，通过分离变量和自适应谱方法解决高频振荡PDE求解中的谱偏差问题，显著提升精度并减少参数和训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在高频振荡PDE求解中存在谱偏差问题，无法有效捕捉高频解分量。

Method: SV-SNN通过分解多变量函数为单变量乘积、自适应傅里叶谱特征和基于奇异值分解的理论框架，解决了谱偏差问题。

Result: 在多个基准问题中，SV-SNN精度提升1-3个数量级，参数减少90%以上，训练时间缩短60%。

Conclusion: SV-SNN是解决神经网络PDE求解中谱偏差问题的有效方法，其实现将公开。

Abstract: Solving high-frequency oscillatory partial differential equations (PDEs) is a
critical challenge in scientific computing, with applications in fluid
mechanics, quantum mechanics, and electromagnetic wave propagation. Traditional
physics-informed neural networks (PINNs) suffer from spectral bias, limiting
their ability to capture high-frequency solution components. We introduce
Separated-Variable Spectral Neural Networks (SV-SNN), a novel framework that
addresses these limitations by integrating separation of variables with
adaptive spectral methods. Our approach features three key innovations: (1)
decomposition of multivariate functions into univariate function products,
enabling independent spatial and temporal networks; (2) adaptive Fourier
spectral features with learnable frequency parameters for high-frequency
capture; and (3) theoretical framework based on singular value decomposition to
quantify spectral bias. Comprehensive evaluation on benchmark problems
including Heat equation, Helmholtz equation, Poisson equations and
Navier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of
magnitude improvement in accuracy while reducing parameter count by over 90\%
and training time by 60\%. These results establish SV-SNN as an effective
solution to the spectral bias problem in neural PDE solving. The implementation
will be made publicly available upon acceptance at
https://github.com/xgxgnpu/SV-SNN.

</details>


### [116] [KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting](https://arxiv.org/abs/2508.00635)
*Changning Wu,Gao Wu,Rongyao Cai,Yong Liu,Kexin Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于KAN的自适应频率选择学习架构（KFS），用于解决时间序列预测中的多尺度噪声干扰和异质信息分布问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列存在多尺度噪声干扰和频率分量间的异质信息分布，导致多尺度表示效果不佳。

Method: 结合Kolmogorov-Arnold Networks（KAN）和Parseval定理，设计了KFS架构，包括FreK模块（基于能量分布选择主导频率）、时间戳嵌入对齐和特征混合模块。

Result: 在多个真实世界时间序列数据集上的实验表明，KFS实现了最先进的性能。

Conclusion: KFS是一种简单而有效的架构，能够显著提升多尺度时间序列预测的准确性。

Abstract: Multi-scale decomposition architectures have emerged as predominant
methodologies in time series forecasting. However, real-world time series
exhibit noise interference across different scales, while heterogeneous
information distribution among frequency components at varying scales leads to
suboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks
(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency
Selection learning architecture (KFS) to address these challenges. This
framework tackles prediction challenges stemming from cross-scale noise
interference and complex pattern modeling through its FreK module, which
performs energy-distribution-based dominant frequency selection in the spectral
domain. Simultaneously, KAN enables sophisticated pattern representation while
timestamp embedding alignment synchronizes temporal representations across
scales. The feature mixing module then fuses scale-specific patterns with
aligned temporal features. Extensive experiments across multiple real-world
time series datasets demonstrate that KT achieves state-of-the-art performance
as a simple yet effective architecture.

</details>


### [117] [Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense](https://arxiv.org/abs/2508.00641)
*Alessandro Palmas*

Main category: cs.LG

TL;DR: 论文探讨了强化学习在应对低成本自杀式无人机群威胁中的优势，提出了一种高保真模拟环境和决策级强化学习代理，显著提高了防御效率。


<details>
  <summary>Details</summary>
Motivation: 低成本自杀式无人机群对现代防御系统构成重大威胁，需要快速、战略性的决策来优化拦截优先级。

Method: 通过高保真模拟环境训练强化学习代理，代理在离散动作空间中选择拦截目标，基于状态特征（如位置、类型和效应器状态）进行决策。

Result: 强化学习策略在数百次模拟攻击中表现优于基于规则的基线，平均损害更低，防御效率更高。

Conclusion: 强化学习可作为防御架构的战略层，增强韧性而不替代现有控制系统，所有代码和模拟资源已公开。

Abstract: The growing threat of low-cost kamikaze drone swarms poses a critical
challenge to modern defense systems demanding rapid and strategic
decision-making to prioritize interceptions across multiple effectors and
high-value target zones. In this work, we present a case study demonstrating
the practical advantages of reinforcement learning in addressing this
challenge. We introduce a high-fidelity simulation environment that captures
realistic operational constraints, within which a decision-level reinforcement
learning agent learns to coordinate multiple effectors for optimal interception
prioritization. Operating in a discrete action space, the agent selects which
drone to engage per effector based on observed state features such as
positions, classes, and effector status. We evaluate the learned policy against
a handcrafted rule-based baseline across hundreds of simulated attack
scenarios. The reinforcement learning based policy consistently achieves lower
average damage and higher defensive efficiency in protecting critical zones.
This case study highlights the potential of reinforcement learning as a
strategic layer within defense architectures, enhancing resilience without
displacing existing control systems. All code and simulation assets are
publicly released for full reproducibility, and a video demonstration
illustrates the policy's qualitative behavior.

</details>


### [118] [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](https://arxiv.org/abs/2508.00643)
*Albert Matveev,Sanmitra Ghosh,Aamal Hussain,James-Michael Leahy,Michalis Michaelides*

Main category: cs.LG

TL;DR: DINOZAUR是一种基于扩散的神经算子参数化方法，解决了FNO的过参数化和不确定性量化问题，性能优越且高效。


<details>
  <summary>Details</summary>
Motivation: FNO存在可扩展性挑战和缺乏原生不确定性量化的问题，限制了其在科学和工程应用中的可靠性。

Method: DINOZAUR采用扩散乘子替代FNO中的密集张量乘子，减少参数数量，并通过贝叶斯方法提供不确定性量化。

Result: DINOZAUR在多个PDE基准测试中表现优异，同时提供高效的不确定性量化。

Conclusion: DINOZAUR是一种高效且可靠的神经算子参数化方法，解决了FNO的局限性。

Abstract: Operator learning is a powerful paradigm for solving partial differential
equations, with Fourier Neural Operators serving as a widely adopted
foundation. However, FNOs face significant scalability challenges due to
overparameterization and offer no native uncertainty quantification -- a key
requirement for reliable scientific and engineering applications. Instead,
neural operators rely on post hoc UQ methods that ignore geometric inductive
biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator
parametrization with uncertainty quantification. Inspired by the structure of
the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a
dimensionality-independent diffusion multiplier that has a single learnable
time parameter per channel, drastically reducing parameter count and memory
footprint without compromising predictive performance. By defining priors over
those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield
spatially correlated outputs and calibrated uncertainty estimates. Our method
achieves competitive or superior performance across several PDE benchmarks
while providing efficient uncertainty quantification.

</details>


### [119] [TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction](https://arxiv.org/abs/2508.00657)
*Sihang Zeng,Lucas Jing Liu,Jun Wen,Meliha Yetisgen,Ruth Etzioni,Gang Luo*

Main category: cs.LG

TL;DR: TrajSurv利用神经控制微分方程（NCDE）从纵向电子健康记录（EHR）中学习连续潜在轨迹，用于可信赖的生存预测。


<details>
  <summary>Details</summary>
Motivation: 解决不规则采样的临床特征下患者连续临床进展建模的挑战，并透明地将进展与生存结果关联。

Method: 使用NCDE提取连续时间潜在状态，通过时间感知对比学习对齐潜在状态空间，并采用两步解释过程。

Result: 在MIMIC-III和eICU数据集上表现出竞争性准确性和更高的透明度。

Conclusion: TrajSurv在生存预测中实现了高准确性和透明性，优于现有深度学习方法。

Abstract: Trustworthy survival prediction is essential for clinical decision making.
Longitudinal electronic health records (EHRs) provide a uniquely powerful
opportunity for the prediction. However, it is challenging to accurately model
the continuous clinical progression of patients underlying the irregularly
sampled clinical features and to transparently link the progression to survival
outcomes. To address these challenges, we develop TrajSurv, a model that learns
continuous latent trajectories from longitudinal EHR data for trustworthy
survival prediction. TrajSurv employs a neural controlled differential equation
(NCDE) to extract continuous-time latent states from the irregularly sampled
data, forming continuous latent trajectories. To ensure the latent trajectories
reflect the clinical progression, TrajSurv aligns the latent state space with
patient state space through a time-aware contrastive learning approach. To
transparently link clinical progression to the survival outcome, TrajSurv uses
latent trajectories in a two-step divide-and-conquer interpretation process.
First, it explains how the changes in clinical features translate into the
latent trajectory's evolution using a learned vector field. Second, it clusters
these latent trajectories to identify key clinical progression patterns
associated with different survival outcomes. Evaluations on two real-world
medical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and
superior transparency over existing deep learning methods.

</details>


### [120] [DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes](https://arxiv.org/abs/2508.00664)
*Jialun Zheng,Jie Liu,Jiannong Cao,Xiao Wang,Hanchen Yang,Yankai Chen,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了一种动态原型模型（DP-DGAD）用于动态图异常检测，通过捕获域特定和域无关的异常模式，并在多个真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 动态图异常检测在多个领域（如金融、交通、社交网络）中至关重要，但现有通用模型难以捕捉动态图中的异常模式，且新领域数据缺乏标签。

Method: DP-DGAD通过提取动态原型（正常和异常模式的演化表示），选择性更新内存缓冲区，并使用异常评分器和置信度伪标签进行自监督适应。

Result: 在十个真实数据集上实现了最先进的性能。

Conclusion: DP-DGAD能够有效捕捉动态图中的异常模式，适用于跨领域应用。

Abstract: Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies
in evolving graphs across domains such as finance, traffic, and social
networks. Recently, generalist graph anomaly detection (GAD) models have shown
promising results. They are pretrained on multiple source datasets and
generalize across domains. While effective on static graphs, they struggle to
capture evolving anomalies in dynamic graphs. Moreover, the continuous
emergence of new domains and the lack of labeled data further challenge
generalist DGAD. Effective cross-domain DGAD requires both domain-specific and
domain-agnostic anomalous patterns. Importantly, these patterns evolve
temporally within and across domains. Building on these insights, we propose a
DGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and
domain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,
evolving representations of normal and anomalous patterns, from temporal
ego-graphs and stores them in a memory buffer. The buffer is selectively
updated to retain general, domain-agnostic patterns while incorporating new
domain-specific ones. Then, an anomaly scorer compares incoming data with
dynamic prototypes to flag both general and domain-specific anomalies. Finally,
DP-DGAD employs confidence-based pseudo-labeling for effective self-supervised
adaptation in target domains. Extensive experiments demonstrate
state-of-the-art performance across ten real-world datasets from different
domains.

</details>


### [121] [Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network](https://arxiv.org/abs/2508.00692)
*Young-ho Cho,Hao Zhu,Duehee Lee,Ross Baldick*

Main category: cs.LG

TL;DR: 论文提出了一种结合广义动态因子模型（GDFM）和生成对抗网络（GAN）的方法，用于合成分布式风电场的长期风电功率场景，以更好地模拟时空特征和统计特性。


<details>
  <summary>Details</summary>
Motivation: 资源充足性研究需要准确模拟风电场的时空特征，现有方法（如GDFM或GAN单独使用）无法同时满足空间和时间相关性需求。

Method: 结合GDFM和GAN，利用GAN提取动态因子并作为GDFM的滤波器，以同时捕捉空间和频率相关性。

Result: 在澳大利亚风电数据上的测试表明，该方法在合成风电场景时优于单独使用GDFM或GAN，能更准确地模拟实际风电的统计特性。

Conclusion: GDFM与GAN的结合方法在合成风电功率场景中表现优异，为资源充足性研究提供了更可靠的模拟工具。

Abstract: For conducting resource adequacy studies, we synthesize multiple long-term
wind power scenarios of distributed wind farms simultaneously by using the
spatio-temporal features: spatial and temporal correlation, waveforms, marginal
and ramp rates distributions of waveform, power spectral densities, and
statistical characteristics. Generating the spatial correlation in scenarios
requires the design of common factors for neighboring wind farms and
antithetical factors for distant wind farms. The generalized dynamic factor
model (GDFM) can extract the common factors through cross spectral density
analysis, but it cannot closely imitate waveforms. The GAN can synthesize
plausible samples representing the temporal correlation by verifying samples
through a fake sample discriminator. To combine the advantages of GDFM and GAN,
we use the GAN to provide a filter that extracts dynamic factors with temporal
information from the observation data, and we then apply this filter in the
GDFM to represent both spatial and frequency correlations of plausible
waveforms. Numerical tests on the combination of GDFM and GAN have demonstrated
performance improvements over competing alternatives in synthesizing wind power
scenarios from Australia, better realizing plausible statistical
characteristics of actual wind power compared to alternatives such as the GDFM
with a filter synthesized from distributions of actual dynamic filters and the
GAN with direct synthesis without dynamic factors.

</details>


### [122] [Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach](https://arxiv.org/abs/2508.00695)
*Sergio Rubio-Martín,María Teresa García-Ordás,Antonio Serrano-García,Clara Margarita Franch-Pato,Arturo Crespo-Álvaro,José Alberto Benítez-Andrades*

Main category: cs.LG

TL;DR: 研究比较了多种AI模型对临床笔记的分类性能，发现超参数调优显著提升模型准确性，而过采样技术影响有限。


<details>
  <summary>Details</summary>
Motivation: 临床笔记分类对心理健康诊断至关重要，需评估不同AI模型和数据平衡方法的有效性。

Method: 使用传统机器学习（如随机森林、SVM）和深度学习模型（如DistilBERT、SciBERT），并测试三种过采样策略及超参数调优。

Result: 超参数调优显著提升模型性能，过采样技术仅对BERT模型有积极影响。决策树和XGBoost准确率达96%，BERT模型亦然。

Conclusion: 超参数调优对模型性能至关重要，为AI辅助心理健康诊断提供了实用见解。

Abstract: The classification of clinical notes into specific diagnostic categories is
critical in healthcare, especially for mental health conditions like Anxiety
and Adjustment Disorder. In this study, we compare the performance of various
Artificial Intelligence models, including both traditional Machine Learning
approaches (Random Forest, Support Vector Machine, K-nearest neighbors,
Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT
and SciBERT), to classify clinical notes into these two diagnoses.
Additionally, we implemented three oversampling strategies: No Oversampling,
Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to
assess their impact on model performance. Hyperparameter tuning was also
applied to optimize model accuracy. Our results indicate that oversampling
techniques had minimal impact on model performance overall. The only exception
was SMOTE, which showed a positive effect specifically with BERT-based models.
However, hyperparameter optimization significantly improved accuracy across the
models, enhancing their ability to generalize and perform on the dataset. The
Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy
among machine learning approaches, both reaching 96%, while the DistilBERT and
SciBERT models also attained 96% accuracy in the deep learning category. These
findings underscore the importance of hyperparameter tuning in maximizing model
performance. This study contributes to the ongoing research on AI-assisted
diagnostic tools in mental health by providing insights into the efficacy of
different model architectures and data balancing methods.

</details>


### [123] [Learning Network Dismantling without Handcrafted Inputs](https://arxiv.org/abs/2508.00706)
*Haozhe Tian,Pietro Ferraro,Robert Shorten,Mahdi Jalili,Homayoun Hamedmoghadam*

Main category: cs.LG

TL;DR: 论文提出了一种无需手工特征的消息传递图神经网络框架MIND，用于解决网络拆解问题，并在大规模真实网络上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖手工特征，增加了计算成本并引入偏差，因此需要一种更高效且通用的方法。

Method: 引入注意力机制和消息迭代配置文件，并生成结构多样的小型合成网络作为训练集。

Result: MIND模型在大型真实网络上表现优异，优于现有方法。

Conclusion: MIND框架不仅适用于网络拆解，还可推广到其他复杂网络问题。

Abstract: The application of message-passing Graph Neural Networks has been a
breakthrough for important network science problems. However, the competitive
performance often relies on using handcrafted structural features as inputs,
which increases computational cost and introduces bias into the otherwise
purely data-driven network representations. Here, we eliminate the need for
handcrafted features by introducing an attention mechanism and utilizing
message-iteration profiles, in addition to an effective algorithmic approach to
generate a structurally diverse training set of small synthetic networks.
Thereby, we build an expressive message-passing framework and use it to
efficiently solve the NP-hard problem of Network Dismantling, virtually
equivalent to vital node identification, with significant real-world
applications. Trained solely on diversified synthetic networks, our proposed
model -- MIND: Message Iteration Network Dismantler -- generalizes to large,
unseen real networks with millions of nodes, outperforming state-of-the-art
network dismantling methods. Increased efficiency and generalizability of the
proposed model can be leveraged beyond dismantling in a range of complex
network problems.

</details>


### [124] [Efficient Solution and Learning of Robust Factored MDPs](https://arxiv.org/abs/2508.00707)
*Yannik Schnitzer,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 提出了一种基于分解状态空间表示的方法，用于解决和学习鲁棒马尔可夫决策过程（r-MDPs），显著提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: r-MDPs通过显式建模转移动态的不确定性，能够合成具有性能保证的鲁棒策略，但传统方法需要大量样本交互。

Method: 利用系统组件间模型不确定性的独立性，将非凸优化问题转化为可处理的线性规划问题，并直接学习分解模型表示。

Result: 实验结果表明，利用分解结构可以显著提高样本效率，生成比现有方法更有效的鲁棒策略和更严格的性能保证。

Conclusion: 分解状态空间表示是解决和学习r-MDPs的有效方法，能够显著提升样本效率和策略性能。

Abstract: Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling
epistemic uncertainty about transition dynamics. Learning r-MDPs from
interactions with an unknown environment enables the synthesis of robust
policies with provable (PAC) guarantees on performance, but this can require a
large number of sample interactions. We propose novel methods for solving and
learning r-MDPs based on factored state-space representations that leverage the
independence between model uncertainty across system components. Although
policy synthesis for factored r-MDPs leads to hard, non-convex optimisation
problems, we show how to reformulate these into tractable linear programs.
Building on these, we also propose methods to learn factored model
representations directly. Our experimental results show that exploiting
factored structure can yield dimensional gains in sample efficiency, producing
more effective robust policies with tighter performance guarantees than
state-of-the-art methods.

</details>


### [125] [JSON-Bag: A generic game trajectory representation](https://arxiv.org/abs/2508.00712)
*Dien Nguyen,Diego Perez-Liebana,Simon Lucas*

Main category: cs.LG

TL;DR: JSON-Bag模型通过JSON描述的游戏轨迹进行标记化，并使用Jensen-Shannon距离（JSD）作为距离度量。在六个桌游上验证了其有效性，优于基于手工特征的基线方法，并在样本效率和自动特征提取方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 提出一种通用的游戏轨迹表示方法，以解决传统手工特征提取的局限性。

Method: 使用JSON-Bag模型标记化游戏轨迹的JSON描述，应用JSD作为距离度量，并通过原型最近邻搜索（P-NNS）进行评估。

Result: 在多数任务中优于基线方法，样本效率高，且自动特征提取显著提升了低性能任务的准确性。

Conclusion: JSON-Bag模型是一种有效的通用游戏轨迹表示方法，且JSD与策略距离高度相关。

Abstract: We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically
represent game trajectories by tokenizing their JSON descriptions and apply
Jensen-Shannon distance (JSD) as distance metric for them. Using a
prototype-based nearest-neighbor search (P-NNS), we evaluate the validity of
JSON-Bag with JSD on six tabletop games -- \textit{7 Wonders},
\textit{Dominion}, \textit{Sea Salt and Paper}, \textit{Can't Stop},
\textit{Connect4}, \textit{Dots and boxes} -- each over three game trajectory
classification tasks: classifying the playing agents, game parameters, or game
seeds that were used to generate the trajectories.
  Our approach outperforms a baseline using hand-crafted features in the
majority of tasks. Evaluating on N-shot classification suggests using JSON-Bag
prototype to represent game trajectory classes is also sample efficient.
Additionally, we demonstrate JSON-Bag ability for automatic feature extraction
by treating tokens as individual features to be used in Random Forest to solve
the tasks above, which significantly improves accuracy on underperforming
tasks. Finally, we show that, across all six games, the JSD between JSON-Bag
prototypes of agent classes highly correlates with the distances between
agents' policies.

</details>


### [126] [Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning](https://arxiv.org/abs/2508.00716)
*Yingxu Wang,Mengzhu Wang,Zhichao Huang,Suyu Liu*

Main category: cs.LG

TL;DR: NeGPR是一种针对带噪声标签的图级域适应框架，通过双分支预训练和嵌套伪标签细化机制，有效提升跨域学习性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中源标签常含噪声，现有图域适应方法依赖干净标签假设，性能受限。

Method: NeGPR采用双分支预训练（语义和拓扑分支）和嵌套细化机制，结合噪声感知正则化。

Result: 在基准数据集上，NeGPR在严重标签噪声下性能优于现有方法，准确率提升达12.7%。

Conclusion: NeGPR通过噪声鲁棒设计，显著提升了带噪声标签的图域适应性能。

Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled
source graphs to unlabeled target graphs by learning domain-invariant
representations, which is essential in applications such as molecular property
prediction and social network analysis. However, most existing GDA methods rely
on the assumption of clean source labels, which rarely holds in real-world
scenarios where annotation noise is pervasive. This label noise severely
impairs feature alignment and degrades adaptation performance under domain
shifts. To address this challenge, we propose Nested Graph Pseudo-Label
Refinement (NeGPR), a novel framework tailored for graph-level domain
adaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,
semantic and topology branches, by enforcing neighborhood consistency in the
feature space, thereby reducing the influence of noisy supervision. To bridge
domain gaps, NeGPR employs a nested refinement mechanism in which one branch
selects high-confidence target samples to guide the adaptation of the other,
enabling progressive cross-domain learning. Furthermore, since pseudo-labels
may still contain noise and the pre-trained branches are already overfitted to
the noisy labels in the source domain, NeGPR incorporates a noise-aware
regularization strategy. This regularization is theoretically proven to
mitigate the adverse effects of pseudo-label noise, even under the presence of
source overfitting, thus enhancing the robustness of the adaptation process.
Extensive experiments on benchmark datasets demonstrate that NeGPR consistently
outperforms state-of-the-art methods under severe label noise, achieving gains
of up to 12.7% in accuracy.

</details>


### [127] [Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK](https://arxiv.org/abs/2508.00718)
*Ivona Krchova,Mariana Vargas Vieyra,Mario Scriminaci,Andrey Sidorenko*

Main category: cs.LG

TL;DR: MOSTLY AI SDK是一个开源工具包，用于生成高质量的合成表格数据，解决数据访问受限问题。


<details>
  <summary>Details</summary>
Motivation: 隐私、专有利益和伦理问题导致数据访问受限，合成数据成为解决方案。

Method: 基于TabularARGN自回归框架，集成差分隐私、公平性生成和自动化质量保证。

Result: SDK支持多种数据类型和复杂数据集，性能优越，速度快且易用。

Conclusion: SDK已广泛部署，有效解决数据瓶颈，推动数据民主化。

Abstract: Machine learning development critically depends on access to high-quality
data. However, increasing restrictions due to privacy, proprietary interests,
and ethical concerns have created significant barriers to data accessibility.
Synthetic data offers a viable solution by enabling safe, broad data usage
without compromising sensitive information. This paper presents the MOSTLY AI
Synthetic Data Software Development Kit (SDK), an open-source toolkit designed
specifically for synthesizing high-quality tabular data. The SDK integrates
robust features such as differential privacy guarantees, fairness-aware data
generation, and automated quality assurance into a flexible and accessible
Python interface. Leveraging the TabularARGN autoregressive framework, the SDK
supports diverse data types and complex multi-table and sequential datasets,
delivering competitive performance with notable improvements in speed and
usability. Currently deployed both as a cloud service and locally installable
software, the SDK has seen rapid adoption, highlighting its practicality in
addressing real-world data bottlenecks and promoting widespread data
democratization.

</details>


### [128] [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](https://arxiv.org/abs/2508.00734)
*Liuyun Xu,Seymour M. J. Spence*

Main category: cs.LG

TL;DR: 提出了一种基于多保真度分层采样和自适应机器学习元模型的方法，用于高效估计小失效概率，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂非线性有限元模型中计算小失效概率时仍需要大量模型评估，计算成本高。

Method: 结合分层采样生成高保真数据集训练深度学习元模型作为低保真模型，并通过自适应训练平衡精度与计算需求。

Result: 应用于高层钢结构建筑的风激励分析，准确估计非线性响应超越概率曲线，计算成本显著降低。

Conclusion: 该方法在保证精度的同时大幅提高了计算效率，适用于复杂系统的失效概率分析。

Abstract: Existing variance reduction techniques used in stochastic simulations for
rare event analysis still require a substantial number of model evaluations to
estimate small failure probabilities. In the context of complex, nonlinear
finite element modeling environments, this can become computationally
challenging-particularly for systems subjected to stochastic excitation. To
address this challenge, a multi-fidelity stratified sampling scheme with
adaptive machine learning metamodels is introduced for efficiently propagating
uncertainties and estimating small failure probabilities. In this approach, a
high-fidelity dataset generated through stratified sampling is used to train a
deep learning-based metamodel, which then serves as a cost-effective and highly
correlated low-fidelity model. An adaptive training scheme is proposed to
balance the trade-off between approximation quality and computational demand
associated with the development of the low-fidelity model. By integrating the
low-fidelity outputs with additional high-fidelity results, an unbiased
estimate of the strata-wise failure probabilities is obtained using a
multi-fidelity Monte Carlo framework. The overall probability of failure is
then computed using the total probability theorem. Application to a full-scale
high-rise steel building subjected to stochastic wind excitation demonstrates
that the proposed scheme can accurately estimate exceedance probability curves
for nonlinear responses of interest, while achieving significant computational
savings compared to single-fidelity variance reduction approaches.

</details>


### [129] [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](https://arxiv.org/abs/2508.00754)
*Yaxin Ma,Benjamin Colburn,Jose C. Principe*

Main category: cs.LG

TL;DR: 提出一种基于特征空间密度的单确定性模型方法，用于量化分布偏移和OOD检测，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络和深度集成方法计算和存储成本高，需更高效的不确定性量化方法。

Method: 利用核密度估计的信息势场近似训练集特征空间密度，通过比较测试样本特征空间表示检测分布偏移。

Result: 在2D合成数据集和OOD检测任务中表现优于基线模型。

Conclusion: 单确定性模型方法在不确定性量化和OOD检测中高效且有效。

Abstract: Bayesian neural networks and deep ensemble methods have been proposed for
uncertainty quantification; however, they are computationally intensive and
require large storage. By utilizing a single deterministic model, we can solve
the above issue. We propose an effective method based on feature space density
to quantify uncertainty for distributional shifts and out-of-distribution (OOD)
detection. Specifically, we leverage the information potential field derived
from kernel density estimation to approximate the feature space density of the
training set. By comparing this density with the feature space representation
of test samples, we can effectively determine whether a distributional shift
has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons
and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The
results demonstrate that our method outperforms baseline models.

</details>


### [130] [Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data](https://arxiv.org/abs/2508.00758)
*Timur Sattarov,Marco Schreyer,Damian Borth*

Main category: cs.LG

TL;DR: 提出了一种结合扩散模型和去噪自编码器的新框架DDAE，用于表格数据的异常检测，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 表格数据异常检测因复杂的特征交互和异常样本稀缺而具有挑战性，现有方法（如去噪自编码器和扩散模型）各有局限性。

Method: DDAE框架整合了扩散模型的噪声调度和对比学习，改进了编码过程。

Result: 在ADBench的57个数据集上，DDAE在半监督和无监督设置中表现优异，PR-AUC和ROC-AUC分别提升最高65%和16%。

Conclusion: 研究强调了噪声策略在表格异常检测中的重要性，并验证了DDAE的有效性。

Abstract: Anomaly detection in tabular data remains challenging due to complex feature
interactions and the scarcity of anomalous examples. Denoising autoencoders
rely on fixed-magnitude noise, limiting adaptability to diverse data
distributions. Diffusion models introduce scheduled noise and iterative
denoising, but lack explicit reconstruction mappings. We propose the
Diffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates
diffusion-based noise scheduling and contrastive learning into the encoding
process to improve anomaly detection. We evaluated DDAE on 57 datasets from
ADBench. Our method outperforms in semi-supervised settings and achieves
competitive results in unsupervised settings, improving PR-AUC by up to 65%
(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)
model baselines. We observed that higher noise levels benefit unsupervised
training, while lower noise with linear scheduling is optimal in
semi-supervised settings. These findings underscore the importance of
principled noise strategies in tabular anomaly detection.

</details>


### [131] [Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy](https://arxiv.org/abs/2508.00768)
*Antonio Tudisco,Andrea Marchesin,Maurizio Zamboni,Mariagrazia Graziano,Giovanna Turvani*

Main category: cs.LG

TL;DR: 论文分析了量子机器学习中变分量子电路（VQC）的性能，比较了振幅和角度编码模型在不同旋转门下的分类表现，发现编码方式对模型性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 量子计算和机器学习的结合（QML）受到广泛关注，尤其是变分量子电路（VQC）的性能优化问题。

Method: 研究采用振幅和角度编码模型，通过不同旋转门训练Wine和Diabetes数据集，评估分类性能。

Result: 在相同拓扑结构下，最佳和最差模型的准确率差异为10%至30%，最高达41%。旋转门选择显著影响性能。

Conclusion: 编码方式是VQC模型的重要超参数，选择合适旋转门能显著提升分类性能。

Abstract: Recent advancements in Quantum Computing and Machine Learning have increased
attention to Quantum Machine Learning (QML), which aims to develop machine
learning models by exploiting the quantum computing paradigm. One of the widely
used models in this area is the Variational Quantum Circuit (VQC), a hybrid
model where the quantum circuit handles data inference while classical
optimization adjusts the parameters of the circuit. The quantum circuit
consists of an encoding layer, which loads data into the circuit, and a
template circuit, known as the ansatz, responsible for processing the data.
This work involves performing an analysis by considering both Amplitude- and
Angle-encoding models, and examining how the type of rotational gate applied
affects the classification performance of the model. This comparison is carried
out by training the different models on two datasets, Wine and Diabetes, and
evaluating their performance. The study demonstrates that, under identical
model topologies, the difference in accuracy between the best and worst models
ranges from 10% to 30%, with differences reaching up to 41%. Moreover, the
results highlight how the choice of rotational gates used in encoding can
significantly impact the model's classification performance. The findings
confirm that the embedding represents a hyperparameter for VQC models.

</details>


### [132] [Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors](https://arxiv.org/abs/2508.00785)
*Bushra Akter,Md Biplob Hosen,Sabbir Ahmed,Mehrin Anannya,Md. Farhad Hossain*

Main category: cs.LG

TL;DR: 该研究通过文献综述和调查数据分析了影响学生CGPA的多变量因素，使用因果分析和机器学习模型（如岭回归和随机森林）进行预测和分类，并开发了一个基于网络的个性化应用。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索影响学生学术表现的多变量因素，以制定优化CGPA的有效策略。

Method: 方法包括文献综述、在线调查（1050名学生）、数据预处理、因果分析、回归和分类模型（如岭回归和随机森林），以及可解释AI技术（如SHAP和LIME）。

Result: 岭回归在预测中表现优异（MAE=0.12，MSE=0.023），随机森林分类准确率达98.68%。关键因素包括学习时间、奖学金、父母教育水平和先前学术表现。

Conclusion: 研究开发了一个基于网络的个性化应用，帮助学生预测学术表现并优化决策，为提升CGPA提供了实用工具。

Abstract: Academic performance depends on a multivariable nexus of socio-academic and
financial factors. This study investigates these influences to develop
effective strategies for optimizing students' CGPA. To achieve this, we
reviewed various literature to identify key influencing factors and constructed
an initial hypothetical causal graph based on the findings. Additionally, an
online survey was conducted, where 1,050 students participated, providing
comprehensive data for analysis. Rigorous data preprocessing techniques,
including cleaning and visualization, ensured data quality before analysis.
Causal analysis validated the relationships among variables, offering deeper
insights into their direct and indirect effects on CGPA. Regression models were
implemented for CGPA prediction, while classification models categorized
students based on performance levels. Ridge Regression demonstrated strong
predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared
Error of 0.023. Random Forest outperformed in classification, attaining an
F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques
such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting
critical factors such as study hours, scholarships, parental education, and
prior academic performance. The study culminated in the development of a
web-based application that provides students with personalized insights,
allowing them to predict academic performance, identify areas for improvement,
and make informed decisions to enhance their outcomes.

</details>


### [133] [Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management](https://arxiv.org/abs/2508.00806)
*Ping Chen,Zhuohong Deng,Ping Li,Shuibing He,Hongzi Zhu,Yi Zheng,Zhefeng Wang,Baoxing Huai,Minyi Guo*

Main category: cs.LG

TL;DR: Adacc是一个结合自适应压缩和激活检查点的内存管理框架，旨在减少GPU内存占用并加速大型语言模型训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练中的重计算会引入高达30%的开销，Adacc旨在通过优化内存管理来减少这种开销。

Method: Adacc包含三个模块：层特定压缩算法、基于MILP的最优调度策略和自适应策略演化机制。

Result: 实验表明，Adacc能将LLM训练速度提升1.01x至1.37x，同时保持与基线相当的模型精度。

Conclusion: Adacc通过自适应压缩和动态策略调整，有效减少了内存占用并提升了训练效率。

Abstract: Training large language models often employs recomputation to alleviate
memory pressure, which can introduce up to 30% overhead in real-world
scenarios. In this paper, we propose Adacc, a novel memory management framework
that combines adaptive compression and activation checkpointing to reduce the
GPU memory footprint. It comprises three modules: (1) We design layer-specific
compression algorithms that account for outliers in LLM tensors, instead of
directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We
propose an optimal scheduling policy that employs MILP to determine the best
memory optimization for each tensor. (3) To accommodate changes in training
tensors, we introduce an adaptive policy evolution mechanism that adjusts the
policy during training to enhance throughput. Experimental results show that
Adacc can accelerate the LLM training by 1.01x to 1.37x compared to
state-of-the-art frameworks, while maintaining comparable model accuracy to the
Baseline.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [134] [ReVise: A Human-AI Interface for Incremental Algorithmic Recourse](https://arxiv.org/abs/2508.00002)
*Kaustav Bhattacharjee,Jun Yuan,Aritra Dasgupta*

Main category: cs.HC

TL;DR: 论文提出了一种可视化分析工作流，帮助数据主体在算法决策中规划增量补救路径，避免误导性选择。


<details>
  <summary>Details</summary>
Motivation: 人工智能在招聘、金融等领域的黑箱决策可能对数据主体产生不利影响，现有补救方法忽视了增量步骤和实际约束。

Method: 与AI/ML专家合作，设计了交互式可视化界面，支持数据主体探索补救路径。

Result: 通过真实数据集和用户研究，验证了该方法的有效性，帮助用户选择合适路径。

Conclusion: 该方法为数据主体提供了更直观、实用的算法补救工具。

Abstract: The recent adoption of artificial intelligence in socio-technical systems
raises concerns about the black-box nature of the resulting decisions in fields
such as hiring, finance, admissions, etc. If data subjects -- such as job
applicants, loan applicants, and students -- receive an unfavorable outcome,
they may be interested in algorithmic recourse, which involves updating certain
features to yield a more favorable result when re-evaluated by algorithmic
decision-making. Unfortunately, when individuals do not fully understand the
incremental steps needed to change their circumstances, they risk following
misguided paths that can lead to significant, long-term adverse consequences.
Existing recourse approaches focus exclusively on the final recourse goal but
neglect the possible incremental steps to reach the goal with real-life
constraints, user preferences, and model artifacts. To address this gap, we
formulate a visual analytic workflow for incremental recourse planning in
collaboration with AI/ML experts and contribute an interactive visualization
interface that helps data subjects efficiently navigate the recourse
alternatives and make an informed decision. We present a usage scenario and
subjective feedback from observational studies with twelve graduate students
using a real-world dataset, which demonstrates that our approach can be
instrumental for data subjects in choosing a suitable recourse path.

</details>


### [135] [A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app](https://arxiv.org/abs/2508.00103)
*Guilherme Guerino,Luiz Rodrigues,Luana Bianchiniand Mariana Alves,Marcelo Marinho,Thomaz Veloso,Valmir Macario,Diego Dermeval,Thales Vieira,Ig Bittencourt,Seiji Isotani*

Main category: cs.HC

TL;DR: 论文探讨了教育中人工智能（AIED）的整合，提出通过增强智能（AuI）解决教师参与、AI工具限制和资源可及性等挑战，并开发了MathAIde系统。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过增强智能（AIED）提升学习体验，同时解决教师参与、AI工具限制和资源可及性等关键问题。

Method: 研究包括设计、开发和评估MathAIde系统，采用头脑风暴、高保真原型、A/B测试和真实课堂案例研究。

Result: 研究提出了增强智能在ITS中的设计可能性，强调用户需求与技术可行性的平衡，并通过真实部署验证了解决方案的有效性。

Conclusion: 论文贡献了以教师为中心的设计方法，强调用户中心设计提升AIED系统的实用性和采纳潜力，尤其在资源有限环境中。

Abstract: Integrating Artificial Intelligence in Education (AIED) aims to enhance
learning experiences through technologies like Intelligent Tutoring Systems
(ITS), offering personalized learning, increased engagement, and improved
retention rates. However, AIED faces three main challenges: the critical role
of teachers in the design process, the limitations and reliability of AI tools,
and the accessibility of technological resources. Augmented Intelligence (AuI)
addresses these challenges by enhancing human capabilities rather than
replacing them, allowing systems to suggest solutions. In contrast, humans
provide final assessments, thus improving AI over time. In this sense, this
study focuses on designing, developing, and evaluating MathAIde, an ITS that
corrects mathematics exercises using computer vision and AI and provides
feedback based on photos of student work. The methodology included
brainstorming sessions with potential users, high-fidelity prototyping, A/B
testing, and a case study involving real-world classroom environments for
teachers and students. Our research identified several design possibilities for
implementing AuI in ITSs, emphasizing a balance between user needs and
technological feasibility. Prioritization and validation through prototyping
and testing highlighted the importance of efficiency metrics, ultimately
leading to a solution that offers pre-defined remediation alternatives for
teachers. Real-world deployment demonstrated the usefulness of the proposed
solution. Our research contributes to the literature by providing a usable,
teacher-centered design approach that involves teachers in all design phases.
As a practical implication, we highlight that the user-centered design approach
increases the usefulness and adoption potential of AIED systems, especially in
resource-limited environments.

</details>


### [136] [Decoupling Data and Tooling in Interactive Visualization](https://arxiv.org/abs/2508.00107)
*Jan Simson*

Main category: cs.HC

TL;DR: 提出了一种模块化方法，将数据整理和加载功能与可视化组件分离，以减少开发冗余并提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前可视化工具缺乏对数据转换或整理的支持，导致开发成本高、用户学习曲线陡峭。

Method: 采用模块化架构，分离数据整理和可视化功能，并通过原型验证可行性。

Result: 展示了模块化方法的可行性，并提出了未来研究方向。

Conclusion: 该方法有望减少开发负担，提升用户体验，并支持多工具并行使用。

Abstract: Interactive data visualization is a major part of modern exploratory data
analysis, with web-based technologies enabling a rich ecosystem of both
specialized and general tools. However, current visualization tools often lack
support for transformation or wrangling of data and are forced to re-implement
their own solutions to load and ingest data. This redundancy creates
substantial development overhead for tool creators, steeper learning curves for
users who must master different data handling interfaces across tools and a
degraded user experience as data handling is usually seen as an after-thought.
  We propose a modular approach that separates data wrangling and loading
capabilities from visualization components. This architecture allows
visualization tools to concentrate on their core strengths while providing the
opportunity to develop a unified, powerful interface for data handling. An
additional benefit of this approach is that it allows for multiple tools to
exist and be used side by side. We demonstrate the feasibility of this approach
by building an early prototype using web technologies to encapsulate
visualization tools and manage data flow between them.
  We discuss future research directions, including downstream integrations with
other tooling, such as IDEs, literate programming notebooks and applications,
as well as incorporation of new technologies for efficient data
transformations. We seek input from the community to better understand the
requirements towards this approach.

</details>


### [137] [Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models](https://arxiv.org/abs/2508.00140)
*Zhanna Kaufman,Madeline Endres,Cindy Xiong Bearfield,Yuriy Brun*

Main category: cs.HC

TL;DR: 研究探讨了ML模型的可解释性可视化对用户理解和信任的影响，发现理解越深，信任越低，原因是感知到的偏见增加。


<details>
  <summary>Details</summary>
Motivation: ML系统中的偏见行为普遍存在，影响用户信任，不同背景的用户对同一系统的看法和信任度不同。

Method: 调查可解释性可视化工具（如LIME、SHAP等），通过用户研究评估其对非专家用户的理解、偏见感知和信任的影响。

Result: 发现理解与信任呈负相关，可视化设计能显著影响理解、偏见感知和信任。

Conclusion: 可视化设计在促进负责任ML应用中起关键作用，减少偏见感知可提高信任。

Abstract: Systems relying on ML have become ubiquitous, but so has biased behavior
within them. Research shows that bias significantly affects stakeholders' trust
in systems and how they use them. Further, stakeholders of different
backgrounds view and trust the same systems differently. Thus, how ML models'
behavior is explained plays a key role in comprehension and trust. We survey
explainability visualizations, creating a taxonomy of design characteristics.
We conduct user studies to evaluate five state-of-the-art visualization tools
(LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how
taxonomy characteristics affect comprehension, bias perception, and trust for
non-expert ML users. Surprisingly, we find an inverse relationship between
comprehension and trust: the better users understand the models, the less they
trust them. We investigate the cause and find that this relationship is
strongly mediated by bias perception: more comprehensible visualizations
increase people's perception of bias, and increased bias perception reduces
trust. We confirm this relationship is causal: Manipulating explainability
visualizations to control comprehension, bias perception, and trust, we show
that visualization design can significantly (p < 0.001) increase comprehension,
increase perceived bias, and reduce trust. Conversely, reducing perceived model
bias, either by improving model fairness or by adjusting visualization design,
significantly increases trust even when comprehension remains high. Our work
advances understanding of how comprehension affects trust and systematically
investigates visualization's role in facilitating responsible ML applications.

</details>


### [138] [DeformTune: A Deformable XAI Music Prototype for Non-Musicians](https://arxiv.org/abs/2508.00160)
*Ziqing Xu,Nick Bryan-Kinns*

Main category: cs.HC

TL;DR: DeformTune是一个结合可变形触控界面与MeasureVAE模型的系统，旨在为非音乐专业人士提供更直观的AI音乐创作体验。初步研究表明，用户面临控制映射不清晰等问题，并提出了增强AI可解释性的设计机会。


<details>
  <summary>Details</summary>
Motivation: 现有AI音乐生成工具依赖文本提示或复杂界面，对非音乐专业人士不友好。DeformTune旨在通过触觉界面和直观交互降低使用门槛。

Method: 结合可变形触控界面与MeasureVAE模型，通过11名无音乐背景的成人用户进行初步研究，采用主题分析反馈。

Result: 用户反馈显示控制映射不清晰、表达范围有限等问题，提出了多模态反馈和渐进式交互支持等改进方向。

Conclusion: 研究为提升AI音乐系统的可解释性和用户友好性提供了早期见解，尤其针对新手用户。

Abstract: Many existing AI music generation tools rely on text prompts, complex
interfaces, or instrument-like controls, which may require musical or technical
knowledge that non-musicians do not possess. This paper introduces DeformTune,
a prototype system that combines a tactile deformable interface with the
MeasureVAE model to explore more intuitive, embodied, and explainable AI
interaction. We conducted a preliminary study with 11 adult participants
without formal musical training to investigate their experience with
AI-assisted music creation. Thematic analysis of their feedback revealed
recurring challenge--including unclear control mappings, limited expressive
range, and the need for guidance throughout use. We discuss several design
opportunities for enhancing explainability of AI, including multimodal feedback
and progressive interaction support. These findings contribute early insights
toward making AI music systems more explainable and empowering for novice
users.

</details>


### [139] [The SPACE of AI: Real-World Lessons on AI's Impact on Developers](https://arxiv.org/abs/2508.00178)
*Brian Houck,Travis Lowdermilk,Cody Beyer,Steven Clarke,Ben Hanrahan*

Main category: cs.HC

TL;DR: 研究发现AI工具在软件开发中广泛采用，提升效率和满意度，但对协作影响较小，团队文化和支持结构是关键。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具对开发者生产力和体验的真实影响。

Method: 采用混合方法研究，包括500多名开发者的调查、访谈和观察研究。

Result: AI普遍提升生产力，尤其对常规任务，但效果因任务复杂度、使用模式和团队采用程度而异。

Conclusion: AI是开发者的辅助工具，有效整合需团队文化和支持结构配合，并提出实践建议。

Abstract: As artificial intelligence (AI) tools become increasingly embedded in
software development workflows, questions persist about their true impact on
developer productivity and experience. This paper presents findings from a
mixed-methods study examining how developers perceive AI's influence across the
dimensions of the SPACE framework: Satisfaction, Performance, Activity,
Collaboration and Efficiency. Drawing on survey responses from over 500
developers and qualitative insights from interviews and observational studies,
we find that AI is broadly adopted and widely seen as enhancing productivity,
particularly for routine tasks. However, the benefits vary, depending on task
complexity, individual usage patterns, and team-level adoption. Developers
report increased efficiency and satisfaction, with less evidence of impact on
collaboration. Organizational support and peer learning play key roles in
maximizing AI's value. These findings suggest that AI is augmenting developers
rather than replacing them, and that effective integration depends as much on
team culture and support structures as on the tools themselves. We conclude
with practical recommendations for teams, organizations and researchers seeking
to harness AI's potential in software engineering.

</details>


### [140] [HandOver: Enabling Precise Selection & Manipulation of 3D Objects with Mouse and Hand Tracking](https://arxiv.org/abs/2508.00211)
*Esen K. Tütüncü,Mar Gonzalez-Franco,Eric J. Gonzalez*

Main category: cs.HC

TL;DR: HandOver是一种XR交互技术，结合鼠标的精确选择和手部追踪的灵活操作，在3D任务中表现优于传统射线技术。


<details>
  <summary>Details</summary>
Motivation: 解决XR环境中精确选择和灵活操作的结合问题，提升用户体验和任务效率。

Method: 使用鼠标驱动深度感知3D光标进行精确选择，通过手部悬停切换至直接3D操作。

Result: HandOver在3D任务中误差更低，交互舒适度更高，用户反馈更优。

Conclusion: 结合传统输入设备和手部追踪技术能提升XR交互的舒适性和效率。

Abstract: We present HandOver, an extended reality (XR) interaction technique designed
to unify the precision of traditional mouse input for object selection with the
expressiveness of hand-tracking for object manipulation. With HandOver, the
mouse is used to drive a depth-aware 3D cursor enabling precise and restful
targeting -by hovering their hand over the mouse, the user can then seamlessly
transition into direct 3D manipulation of the target object. In a formal user
study, we compare HandOver against two raybased techniques: traditional
raycasting (Ray) and a hybrid method (Ray+Hand) in a 3D docking task. Results
show HandOver yields lower task errors across all distances, and moreover
improves interaction ergonomics as highlighted by a RULA posture analysis and
self-reported measures (NASA-TLX). These findings illustrate the benefits of
blending traditional precise input devices with the expressive gestural inputs
afforded by hand-tracking in XR, leading to improved user comfort and task
performance. This blended paradigm yields a unified workflow allowing users to
leverage the best of each input modality as they interact in immersive
environments.

</details>


### [141] [Correcting Misperceptions at a Glance: Using Data Visualizations to Reduce Political Sectarianism](https://arxiv.org/abs/2508.00233)
*Douglas Markant,Subham Sah,Alireza Karduni,Milad Rogha,My Thai,Wenwen Dou*

Main category: cs.HC

TL;DR: 研究发现，数据可视化可以纠正对政治对手的误解，但可视化方式（如平均值、区间或完整分布）会影响纠正效果。


<details>
  <summary>Details</summary>
Motivation: 探究如何通过数据可视化纠正对政治对手的极端政策支持度的误解，以减少政治极端主义。

Method: 实验设计，比较不同数据可视化方式（仅平均值、平均值加区间、平均值加完整分布）对纠正效果的影响。

Result: 仅平均值和完整分布的可视化纠正效果最强，而区间可视化效果较弱。完整分布组对对手观点的回忆最准确。

Conclusion: 数据可视化是纠正群体误解的重要工具，但可视化方式会影响信息解读和响应。

Abstract: Political sectarianism is fueled in part by misperceptions of political
opponents: People commonly overestimate the support for extreme policies among
members of the other party. Research suggests that correcting partisan
misperceptions by informing people about the actual views of outparty members
may reduce one's own expressed support for political extremism, including
partisan violence and anti-democratic actions. The present study investigated
how correction effects depend on different representations of outparty views
communicated through data visualizations. We conducted an experiment with U.S.
based participants from Prolific (N=239 Democrats, N=244 Republicans).
Participants made predictions about support for political violence and
undemocratic practices among members of their political outparty. They were
then presented with data from an earlier survey on the actual views of outparty
members. Some participants viewed only the average response (Mean-Only
condition), while other groups were shown visual representations of the range
of views from 75% of the outparty (Mean+Interval condition) or the full
distribution of responses (Mean+Points condition). Compared to a control group
that was not informed about outparty views, we observed the strongest
correction effects among participants in the Mean-only and Mean+Points
condition, while correction effects were weaker in the Mean+Interval condition.
In addition, participants who observed the full distribution of out-party views
(Mean+Points condition) were most accurate at later recalling the degree of
support among the outparty. Our findings suggest that data visualizations can
be an important tool for correcting pervasive distortions in beliefs about
other groups. However, the way in which variability in outparty views is
visualized can significantly shape how people interpret and respond to
corrective information.

</details>


### [142] [What's Behind the Magic? Audiences Seek Artistic Value in Generative AI's Contributions to a Live Dance Performance](https://arxiv.org/abs/2508.00239)
*Jacqueline Elise Bruen,Myounghoon Jeon*

Main category: cs.HC

TL;DR: 研究发现，当观众不知道作品使用了生成式人工智能（GenAI）时，更容易认可其艺术价值。


<details>
  <summary>Details</summary>
Motivation: 探讨人们对AI创作艺术的看法分歧，以及技术背景信息如何影响艺术评价。

Method: 开发两种舞蹈表演版本（使用或不使用GenAI），并在观众观看前后告知技术背景，调查39名参与者的看法。

Result: 观众在不知情时更倾向于认可GenAI作品的艺术价值。

Conclusion: 强调社会背景和用户对GenAI的理解在技术解释中的重要性，以弥合认知差距。

Abstract: With the development of generative artificial intelligence (GenAI) tools to
create art, stakeholders cannot come to an agreement on the value of these
works. In this study we uncovered the mixed opinions surrounding art made by
AI. We developed two versions of a dance performance augmented by technology
either with or without GenAI. For each version we informed audiences of the
performance's development either before or after a survey on their perceptions
of the performance. There were thirty-nine participants (13 males, 26 female)
divided between the four performances. Results demonstrated that individuals
were more inclined to attribute artistic merit to works made by GenAI when they
were unaware of its use. We present this case study as a call to address the
importance of utilizing the social context and the users' interpretations of
GenAI in shaping a technical explanation, leading to a greater discussion that
can bridge gaps in understanding.

</details>


### [143] [TofuML: A Spatio-Physical Interactive Machine Learning Device for Interactive Exploration of Machine Learning for Novices](https://arxiv.org/abs/2508.00252)
*Wataru Kawabe,Hiroto Fukuda,Akihisa Shitara,Yuri Nakao,Yusuke Sugano*

Main category: cs.HC

TL;DR: TofuML是一个交互式系统，通过物理和空间界面帮助非专家用户更直观地学习和参与机器学习（ML）。


<details>
  <summary>Details</summary>
Motivation: 传统GUI系统对非专家用户不够友好，TofuML旨在通过直观的交互方式降低ML的学习门槛。

Method: TofuML采用小型设备和纸质垫的物理界面，用户通过玩具式交互训练和评估声音分类模型。通过两项用户研究（与GUI版本的对比研究和公共活动部署）评估效果。

Result: TofuML比GUI更能提升用户参与度，降低非专家的学习障碍，并激发用户对ML应用的创意。

Conclusion: TofuML为开发面向广泛用户的交互式ML系统提供了新思路，平衡了概念理解和用户参与度。

Abstract: We introduce TofuML, an interactive system designed to make machine learning
(ML) concepts more accessible and engaging for non-expert users. Unlike
conventional GUI-based systems, TofuML employs a physical and spatial interface
consisting of a small device and a paper mat, allowing users to train and
evaluate sound classification models through intuitive, toy-like interactions.
Through two user studies -- a comparative study against a GUI-based version and
a public event deployment -- we investigated how TofuML impacts users'
engagement in the ML model creation process, their ability to provide
appropriate training data, and their conception of potential applications. Our
results indicated that TofuML enhanced user engagement compared to a GUI while
lowering barriers for non-experts to engage with ML. Users demonstrated
creativity in conceiving diverse ML applications, revealing opportunities to
optimize between conceptual understanding and user engagement. These findings
contribute to developing interactive ML systems/frameworks designed for a wide
range of users.

</details>


### [144] [MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems](https://arxiv.org/abs/2508.00300)
*Shruthi Chari,Oshani Seneviratne,Prithwish Chakraborty,Pablo Meyer,Deborah L. McGuinness*

Main category: cs.HC

TL;DR: MetaExplainer是一个神经符号框架，通过三阶段过程生成用户中心的解释，利用LLM和解释本体提高AI系统的可解释性和可信度。


<details>
  <summary>Details</summary>
Motivation: 解决模型提供的解释与用户需求之间的差距，提升AI系统的可解释性和信任度。

Method: 三阶段过程：1. 使用LLM分解用户问题；2. 模型解释方法生成系统建议；3. 合成自然语言解释。利用解释本体指导LLM和解释方法。

Result: 在问题重构、模型解释忠实度和自然语言合成中表现优异，用户研究验证了生成解释的创造性和全面性。

Conclusion: MetaExplainer是一个多功能工具，适用于多种解释类型和领域，有望提升AI的可解释性。

Abstract: Explanations are crucial for building trustworthy AI systems, but a gap often
exists between the explanations provided by models and those needed by users.
To address this gap, we introduce MetaExplainer, a neuro-symbolic framework
designed to generate user-centered explanations. Our approach employs a
three-stage process: first, we decompose user questions into machine-readable
formats using state-of-the-art large language models (LLM); second, we delegate
the task of generating system recommendations to model explainer methods; and
finally, we synthesize natural language explanations that summarize the
explainer outputs. Throughout this process, we utilize an Explanation Ontology
to guide the language models and explainer methods. By leveraging LLMs and a
structured approach to explanation generation, MetaExplainer aims to enhance
the interpretability and trustworthiness of AI systems across various
applications, providing users with tailored, question-driven explanations that
better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate
a step towards evaluating and utilizing current state-of-the-art explanation
frameworks. Our results show high performance across all stages, with a 59.06%
F1-score in question reframing, 70% faithfulness in model explanations, and 67%
context-utilization in natural language synthesis. User studies corroborate
these findings, highlighting the creativity and comprehensiveness of generated
explanations. Tested on the Diabetes (PIMA Indian) tabular dataset,
MetaExplainer supports diverse explanation types, including Contrastive,
Counterfactual, Rationale, Case-Based, and Data explanations. The framework's
versatility and traceability from using ontology to guide LLMs suggest broad
applicability beyond the tested scenarios, positioning MetaExplainer as a
promising tool for enhancing AI explainability across various domains.

</details>


### [145] [Evaluating the Efficacy of Large Language Models for Generating Fine-Grained Visual Privacy Policies in Homes](https://arxiv.org/abs/2508.00321)
*Shuning Zhang,Ying Ma,Xin Yi,Hewu Li*

Main category: cs.HC

TL;DR: 论文探讨了在智能家居环境中使用大型语言模型（LLM）作为动态隐私策略引擎的可行性，通过多维数据分类和实时推理实现细粒度隐私控制。


<details>
  <summary>Details</summary>
Motivation: 智能家居中视觉传感器的普及带来了隐私挑战，现有隐私控制方法静态且粗粒度，无法适应动态和社交复杂的家庭环境。

Method: 提出一个概念框架，利用多维数据分类（数据敏感性、空间背景、社交存在）和LLM实时推理，实现细粒度隐私规则（如选择性对象模糊化）。

Result: 在模拟家庭环境中评估了先进视觉语言模型（如GPT-4o和Qwen-VL系列），LLM引擎的机器评估得分为3.99/5，人类评估得分为4.00/5。

Conclusion: 研究表明，基于LLM的动态隐私策略引擎在智能家居环境中具有可行性，能够有效实现细粒度和适应性隐私控制。

Abstract: The proliferation of visual sensors in smart home environments, particularly
through wearable devices like smart glasses, introduces profound privacy
challenges. Existing privacy controls are often static and coarse-grained,
failing to accommodate the dynamic and socially nuanced nature of home
environments. This paper investigates the viability of using Large Language
Models (LLMs) as the core of a dynamic and adaptive privacy policy engine. We
propose a conceptual framework where visual data is classified using a
multi-dimensional schema that considers data sensitivity, spatial context, and
social presence. An LLM then reasons over this contextual information to
enforce fine-grained privacy rules, such as selective object obfuscation, in
real-time. Through a comparative evaluation of state-of-the-art Vision Language
Models (including GPT-4o and the Qwen-VL series) in simulated home settings ,
our findings show the feasibility of this approach. The LLM-based engine
achieved a top machine-evaluated appropriateness score of 3.99 out of 5, and
the policies generated by the models received a top human-evaluated score of
4.00 out of 5.

</details>


### [146] [From Patient Burdens to User Agency: Designing for Real-Time Protection Support in Online Health Consultations](https://arxiv.org/abs/2508.00328)
*Shuning Zhang,Ying Ma,Yongquan `Owen' Hu,Ting Dang,Hong Jia,Xin Yi,Hewu Li*

Main category: cs.HC

TL;DR: 论文探讨了在线医疗咨询平台的隐私风险，提出了一种名为SafeShare的实时匿名化技术，以平衡隐私与实用性。


<details>
  <summary>Details</summary>
Motivation: 在线医疗咨询平台存在隐私风险，用户对匿名性和控制权的需求与平台现实之间存在脱节，亟需解决方案。

Method: 通过半结构化访谈了解用户需求，开发SafeShare技术，利用本地化LLM实时匿名化咨询内容。

Result: SafeShare在PII检测模块上表现优异，在IMCS21数据集上达到89.64%的准确率。

Conclusion: SafeShare有效解决了用户隐私需求与平台现实之间的差距，为在线医疗咨询提供了可行的隐私保护方案。

Abstract: Online medical consultation platforms, while convenient, are undermined by
significant privacy risks that erode user trust. We first conducted in-depth
semi-structured interviews with 12 users to understand their perceptions of
security and privacy landscapes on online medical consultation platforms, as
well as their practices, challenges and expectation. Our analysis reveals a
critical disconnect between users' desires for anonymity and control, and
platform realities that offload the responsibility of ``privacy labor''. To
bridge this gap, we present SafeShare, an interaction technique that leverages
localized LLM to redact consultations in real-time. SafeShare balances utility
and privacy through selectively anonymize private information. A technical
evaluation of SafeShare's core PII detection module on 3 dataset demonstrates
high efficacy, achieving 89.64\% accuracy with Qwen3-4B on IMCS21 dataset.

</details>


### [147] [HateBuffer: Safeguarding Content Moderators' Mental Well-Being through Hate Speech Content Modification](https://arxiv.org/abs/2508.00439)
*Subin Park,Jeonghyun Kim,Jeanne Choi,Joseph Seering,Uichin Lee,Sung-Ju Lee*

Main category: cs.HC

TL;DR: HateBuffer是一种工具，旨在通过匿名化仇恨言论目标、改写攻击性表达来保护内容审核员的心理健康，但实验显示其对情绪和疲劳的改善效果不明显，但对情感传染和偏见意见的缓冲有效。


<details>
  <summary>Details</summary>
Motivation: 在线平台中仇恨言论对内容审核员心理健康造成负担，需要一种工具来减轻这种影响。

Method: 设计HateBuffer工具，通过匿名化和改写仇恨言论，并在模拟任务和用户研究中测试其效果。

Result: HateBuffer降低了仇恨言论的严重性评分，但对情绪和疲劳无显著改善；审核准确性未受影响，召回率略有提升。

Conclusion: HateBuffer在情感缓冲方面有效，但对心理健康的直接改善有限，文本修改技术对健康审核环境有潜力。

Abstract: Hate speech remains a persistent and unresolved challenge in online
platforms. Content moderators, working on the front lines to review
user-generated content and shield viewers from hate speech, often find
themselves unprotected from the mental burden as they continuously engage with
offensive language. To safeguard moderators' mental well-being, we designed
HateBuffer, which anonymizes targets of hate speech, paraphrases offensive
expressions into less offensive forms, and shows the original expressions when
moderators opt to see them. Our user study with 80 participants consisted of a
simulated hate speech moderation task set on a fictional news platform,
followed by semi-structured interviews. Although participants rated the hate
severity of comments lower while using HateBuffer, contrary to our
expectations, they did not experience improved emotion or reduced fatigue
compared with the control group. In interviews, however, participants described
HateBuffer as an effective buffer against emotional contagion and the
normalization of biased opinions in hate speech. Notably, HateBuffer did not
compromise moderation accuracy and even contributed to a slight increase in
recall. We explore possible explanations for the discrepancy between the
perceived benefits of HateBuffer and its measured impact on mental well-being.
We also underscore the promise of text-based content modification techniques as
tools for a healthier content moderation environment.

</details>


### [148] [Pull Requests From The Classroom: Co-Developing Curriculum And Code](https://arxiv.org/abs/2508.00646)
*Dennis Zyska,Ilia Kuznetsov,Florian Müller,Iryna Gurevych*

Main category: cs.HC

TL;DR: 论文探讨了教育技术与教师教学目标不匹配的问题，通过一个科学写作课程的案例研究，展示了课程与技术共同开发的过程及其效果。


<details>
  <summary>Details</summary>
Motivation: 解决教育技术与教师教学目标不匹配的问题，提升教学效果。

Method: 采用案例研究方法，共同开发课程与定制化的同行反馈系统，支持注释、反馈交换和修订。

Result: 共同开发增强了软件功能与课程目标的匹配，但也暴露了可用性限制和基础设施问题。

Conclusion: 需要加强教学团队与技术团队的协作，以更好地实现技术与教学目标的匹配。

Abstract: Educational technologies often misalign with instructors' pedagogical goals,
forcing adaptations that compromise teaching efficacy. In this paper, we
present a case study on the co-development of curriculum and technology in the
context of a university course on scientific writing. Specifically, we examine
how a custom-built peer feedback system was iteratively developed alongside the
course to support annotation, feedback exchange, and revision. Results show
that while co-development fostered stronger alignment between software features
and course goals, it also exposed usability limitations and
infrastructure-related frustrations, emphasizing the need for closer
coordination between teaching and technical teams.

</details>


### [149] [The Manipulative Power of Voice Characteristics: Investigating Deceptive Patterns in Mandarin Chinese Female Synthetic Speech](https://arxiv.org/abs/2508.00652)
*Shuning Zhang,Han Chen,Yabo Wang,Yiqun Xu,Jiaqi Bai,Yuanyuan Wu,Shixuan Li,Xin Yi,Chunhui Wang,Hewu Li*

Main category: cs.HC

TL;DR: 该研究首次系统调查了普通话合成女声中基于语音特征的黑暗模式，揭示了语音特性和场景对行为操纵的显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究填补了非英语语言环境中语音特征操纵的实证研究空白，尤其是普通话环境下女性合成语音的商业应用。

Method: 通过操纵五种语音特征和三种强度，在不同场景（购物与问答）中评估有效性，结合用户感知和人口统计数据分析。

Result: 研究发现语音特征和场景显著影响行为操纵（最高达+2027.6%），用户感知和个体偏好起中介作用。

Conclusion: 研究结果为语音交互的伦理设计提供了基于证据的见解。

Abstract: Pervasive voice interaction enables deceptive patterns through subtle voice
characteristics, yet empirical investigation into this manipulation lags
behind, especially within major non-English language contexts. Addressing this
gap, our study presents the first systematic investigation into voice
characteristic-based dark patterns employing female synthetic voices in
Mandarin Chinese. This focus is crucial given the prevalence of female personas
in commercial assistants and the prosodic significance in the Chinese language.
Guided by the conceptual framework identifying key influencing factors, we
systematically evaluate effectiveness variations by manipulating voice
characteristics (five characteristics, three intensities) across different
scenarios (shopping vs. question-answering) with different commercial aims. A
preliminary study (N=24) validated the experimental materials and the main
study (N=36) revealed significant behavioral manipulation (up to +2027.6%).
Crucially, the analysis showed that effectiveness varied significantly with
voice characteristics and scenario, mediated by user perception (of tone,
intonation, timbre) and user demographics (individual preferences, though
limited demographic impact). These interconnected findings offer evidence-based
insights for ethical design.

</details>


### [150] [Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors Impacting AI Adoption](https://arxiv.org/abs/2508.00723)
*Rebecca Yu,Valerie Chen,Ameet Talwalkar,Hoda Heidari*

Main category: cs.HC

TL;DR: 研究探讨了人类决策者与AI工具的互动，提出了影响AI工具采用的关键因素，并开发了一个AI采用表来指导AI的负责任部署。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各领域的广泛应用，需要理解决策者何时自愿采用AI工具，以支持其负责任部署。

Method: 通过跨领域（医疗、法律、新闻、公共部门）专家访谈，识别影响AI工具采用的关键因素，并开发AI采用表进行分析。

Result: 发现决策者背景、对AI的认知、决策后果及对其他利益相关者的影响是AI采用的关键因素。

Conclusion: 研究为支持AI的负责任部署提供了实践指导，强调了决策者视角的重要性。

Abstract: Growing excitement around deploying AI across various domains calls for a
careful assessment of how human decision-makers interact with AI-powered
systems. In particular, it is essential to understand when decision-makers
voluntarily choose to consult AI tools, which we term decision-maker adoption.
We interviewed experts across four domains -- medicine, law, journalism, and
the public sector -- to explore current AI use cases and perceptions of
adoption. From these interviews, we identify key factors that shape
decision-maker adoption of AI tools: the decision-maker's background,
perceptions of the AI, consequences for the decision-maker, and perceived
implications for other stakeholders. We translate these factors into an AI
adoption sheet to analyze how decision-makers approach adoption choices through
comparative, cross-domain case studies, highlighting how our factors help
explain inter-domain differences in adoption. Our findings offer practical
guidance for supporting the responsible and context-aware deployment of AI by
better accounting for the decision-maker's perspective.

</details>


### [151] [How LLMs are Shaping the Future of Virtual Reality](https://arxiv.org/abs/2508.00737)
*Süeda Özkaya,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: 综述了大型语言模型（LLMs）与虚拟现实（VR）游戏的融合，探讨了其在叙事生成、NPC互动、可访问性等方面的应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何提升VR游戏的沉浸感、适应性和智能性，推动更真实的数字体验。

Method: 分析了2018至2025年间62项同行评议研究，总结了LLMs在VR中的关键应用领域和挑战。

Result: LLMs显著提升了VR环境的真实感、创造力和用户参与度，但需解决实时性能、伦理等问题。

Conclusion: 未来研究方向包括多模态AI、情感计算等，以推动智能且包容的VR系统发展。

Abstract: The integration of Large Language Models (LLMs) into Virtual Reality (VR)
games marks a paradigm shift in the design of immersive, adaptive, and
intelligent digital experiences. This paper presents a comprehensive review of
recent research at the intersection of LLMs and VR, examining how these models
are transforming narrative generation, non-player character (NPC) interactions,
accessibility, personalization, and game mastering. Drawing from an analysis of
62 peer reviewed studies published between 2018 and 2025, we identify key
application domains ranging from emotionally intelligent NPCs and procedurally
generated storytelling to AI-driven adaptive systems and inclusive gameplay
interfaces. We also address the major challenges facing this convergence,
including real-time performance constraints, memory limitations, ethical risks,
and scalability barriers. Our findings highlight that while LLMs significantly
enhance realism, creativity, and user engagement in VR environments, their
effective deployment requires robust design strategies that integrate
multimodal interaction, hybrid AI architectures, and ethical safeguards. The
paper concludes by outlining future research directions in multimodal AI,
affective computing, reinforcement learning, and open-source development,
aiming to guide the responsible advancement of intelligent and inclusive VR
systems.

</details>
