<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.LG](#cs.LG) [Total: 67]
- [cs.HC](#cs.HC) [Total: 16]
- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Unifying Scheme for Extractive Content Selection Tasks](https://arxiv.org/abs/2507.16922)
*Shmuel Amar,Ori Shapira,Aviv Slobodkin,Ido Dagan*

Main category: cs.CL

TL;DR: 本文提出了指令引导内容选择(IGCS)框架，统一处理多种NLP内容选择任务，并创建了首个统一基准测试IGCSBench和大规模合成数据集，显著提升了跨任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统的内容选择任务（如从源文本中选择相关文本片段）一直被孤立研究，各自使用不同的建模方法、数据集和评估指标，缺乏统一的框架来处理这类共享相同目标的任务。

Method: 提出指令引导内容选择(IGCS)框架，将任务定义和实例特定请求封装为对语言模型的指令；创建IGCSBench统一基准测试；构建大规模通用合成数据集；采用迁移学习方法；解决基于LLM的内容选择建模中的通用推理时间问题。

Result: 迁移学习使用合成数据集能够提升性能，无论是否有针对目标任务的专门训练；创建了涵盖多样化内容选择任务的首个统一基准；提出了通用评估指标并解决了推理时间问题。

Conclusion: IGCS框架为内容选择任务提供了有效的统一建模方法，所创建的资源和方法对未来的内容选择模型具有实用价值，相关模型和数据集已公开可用。

Abstract: A broad range of NLP tasks involve selecting relevant text spans from given
source texts. Despite this shared objective, such \textit{content selection}
tasks have traditionally been studied in isolation, each with its own modeling
approaches, datasets, and evaluation metrics. In this work, we propose
\textit{instruction-guided content selection (IGCS)} as a beneficial unified
framework for such settings, where the task definition and any
instance-specific request are encapsulated as instructions to a language model.
To promote this framework, we introduce \igcsbench{}, the first unified
benchmark covering diverse content selection tasks. Further, we create a large
generic synthetic dataset that can be leveraged for diverse content selection
tasks, and show that transfer learning with these datasets often boosts
performance, whether dedicated training for the targeted task is available or
not. Finally, we address generic inference time issues that arise in LLM-based
modeling of content selection, assess a generic evaluation metric, and overall
propose the utility of our resources and methods for future content selection
models. Models and datasets available at https://github.com/shmuelamar/igcs.

</details>


### [2] [AI-based Clinical Decision Support for Primary Care: A Real-World Study](https://arxiv.org/abs/2507.16947)
*Robert Korom,Sarah Kiptinness,Najib Adan,Kassim Said,Catherine Ithuli,Oliver Rotich,Boniface Kimani,Irene King'ori,Stellah Kamau,Elizabeth Atemba,Muna Aden,Preston Bowman,Michael Sharman,Rebecca Soskin Hicks,Rebecca Distler,Johannes Heidecke,Rahul K. Arora,Karan Singhal*

Main category: cs.CL

TL;DR: 研究评估了基于大语言模型的临床决策支持系统AI Consult在肯尼亚内罗毕初级医疗诊所的实际应用效果，发现该系统能显著降低诊断和治疗错误率


<details>
  <summary>Details</summary>
Motivation: 在真实医疗环境中验证大语言模型临床决策支持工具的实际效果，探索AI辅助系统如何在保持临床医生自主性的同时减少医疗错误

Method: 与肯尼亚内罗毕Penda Health初级医疗网络合作，开展质量改进研究，比较15家诊所中39,849次患者就诊的结果，分析有无AI Consult辅助下临床医生的表现差异，并由独立医生评估临床错误

Result: 使用AI Consult的临床医生错误率显著降低：诊断错误减少16%，治疗错误减少13%。仅在Penda网络，AI Consult每年可避免22,000次诊断错误和29,000次治疗错误。所有使用该系统的临床医生都认为它提高了医疗质量，75%认为效果"显著"

Conclusion: 基于大语言模型的临床决策支持工具在真实医疗环境中具有减少错误的巨大潜力，但需要与临床工作流程相匹配的实施方案和积极的部署策略来促进临床医生采用，为负责任地推广此类AI工具提供了实用框架

Abstract: We evaluate the impact of large language model-based clinical decision
support in live care. In partnership with Penda Health, a network of primary
care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a
safety net for clinicians by identifying potential documentation and clinical
decision-making errors. AI Consult integrates into clinician workflows,
activating only when needed and preserving clinician autonomy. We conducted a
quality improvement study, comparing outcomes for 39,849 patient visits
performed by clinicians with or without access to AI Consult across 15 clinics.
Visits were rated by independent physicians to identify clinical errors.
Clinicians with access to AI Consult made relatively fewer errors: 16% fewer
diagnostic errors and 13% fewer treatment errors. In absolute terms, the
introduction of AI Consult would avert diagnostic errors in 22,000 visits and
treatment errors in 29,000 visits annually at Penda alone. In a survey of
clinicians with AI Consult, all clinicians said that AI Consult improved the
quality of care they delivered, with 75% saying the effect was "substantial".
These results required a clinical workflow-aligned AI Consult implementation
and active deployment to encourage clinician uptake. We hope this study
demonstrates the potential for LLM-based clinical decision support tools to
reduce errors in real-world settings and provides a practical framework for
advancing responsible adoption.

</details>


### [3] [Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs](https://arxiv.org/abs/2507.16951)
*Shuyuan Lin,Lei Duan,Philip Hughes,Yuxuan Sheng*

Main category: cs.CL

TL;DR: 提出了SALU方法，一种能够在对话信息检索系统中自主识别无法回答问题的大语言模型，通过多任务学习和置信度引导的强化学习显著减少了幻觉现象


<details>
  <summary>Details</summary>
Motivation: 对话信息检索系统面临可靠处理无法回答问题的重大挑战，传统方法依赖外部分类器容易与核心生成式大语言模型产生不一致性，需要一种能够直接在LLM生成过程中集成无法回答检测的方法

Method: 提出SALU（Self-Aware LLM for Unanswerability）方法，采用多任务学习框架同时进行标准问答和显式拒绝生成，并结合置信度分数引导的人类反馈强化学习（RLHF）阶段，明确惩罚幻觉回应并奖励适当的拒绝回答

Result: 在C-IR_Answerability数据集上的广泛实验表明，SALU在正确回答或拒绝回答问题的整体准确性方面始终优于强基线方法（包括混合LLM-分类器系统）。人类评估进一步确认了SALU在事实性、适当拒绝方面的优越可靠性，最重要的是显著减少了幻觉现象

Conclusion: SALU方法成功实现了大语言模型的内在自我意识，能够稳健地"知道何时说'我不知道'"，为对话信息检索系统提供了更可靠的解决方案，有效防止了误导性或幻觉内容的生成

Abstract: Conversational Information Retrieval (CIR) systems, while offering intuitive
access to information, face a significant challenge: reliably handling
unanswerable questions to prevent the generation of misleading or hallucinated
content. Traditional approaches often rely on external classifiers, which can
introduce inconsistencies with the core generative Large Language Models
(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a
novel approach that deeply integrates unanswerability detection directly within
the LLM's generative process. SALU is trained using a multi-task learning
framework for both standard Question Answering (QA) and explicit abstention
generation for unanswerable queries. Crucially, it incorporates a
confidence-score-guided reinforcement learning with human feedback (RLHF)
phase, which explicitly penalizes hallucinated responses and rewards
appropriate abstentions, fostering intrinsic self-awareness of knowledge
boundaries. Through extensive experiments on our custom-built
C-IR_Answerability dataset, SALU consistently outperforms strong baselines,
including hybrid LLM-classifier systems, in overall accuracy for correctly
answering or abstaining from questions. Human evaluation further confirms
SALU's superior reliability, achieving high scores in factuality, appropriate
abstention, and, most importantly, a dramatic reduction in hallucination,
demonstrating its ability to robustly "know when to say 'I don't know'."

</details>


### [4] [Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning](https://arxiv.org/abs/2507.16971)
*Aleksandr Perevalov,Andreas Both*

Main category: cs.CL

TL;DR: 本文提出了mKGQAgent框架，通过模块化的LLM代理工作流将多语言自然语言问题转换为SPARQL查询，在Text2SPARQL挑战赛2025中获得第一名


<details>
  <summary>Details</summary>
Motivation: 现有的多语言知识图谱问答方法主要依赖组合式组件来解决下游任务，缺乏模块化和可解释性。需要开发一种人类启发的框架来处理将自然语言问题转换为SPARQL查询的复杂任务

Method: 提出mKGQAgent框架，将自然语言到SPARQL查询的转换任务分解为可解释的子任务，通过协调的LLM代理工作流进行规划、实体链接和查询精炼，并利用经验池进行上下文学习指导

Result: 在Text2SPARQL挑战赛2025的DBpedia和企业级KGQA基准测试中，该方法在所有参与者中获得第一名，有效处理了多语言知识图谱问答任务

Conclusion: mKGQAgent框架成功实现了多语言知识图谱问答的高效处理，为开发多语言语义解析中的类人推理系统开辟了新的途径

Abstract: Accessing knowledge via multilingual natural-language interfaces is one of
the emerging challenges in the field of information retrieval and related ones.
Structured knowledge stored in knowledge graphs can be queried via a specific
query language (e.g., SPARQL). Therefore, one needs to transform
natural-language input into a query to fulfill an information need. Prior
approaches mostly focused on combining components (e.g., rule-based or
neural-based) that solve downstream tasks and come up with an answer at the
end. We introduce mKGQAgent, a human-inspired framework that breaks down the
task of converting natural language questions into SPARQL queries into modular,
interpretable subtasks. By leveraging a coordinated LLM agent workflow for
planning, entity linking, and query refinement - guided by an experience pool
for in-context learning - mKGQAgent efficiently handles multilingual KGQA.
Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the
Text2SPARQL challenge 2025, our approach took first place among the other
participants. This work opens new avenues for developing human-like reasoning
systems in multilingual semantic parsing.

</details>


### [5] [Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain](https://arxiv.org/abs/2507.16974)
*Rishemjit Kaur,Arshdeep Singh Bhankhar,Surangika Ranathunga,Jashanpreet Singh Salh,Sudhir Rajput,Vidhi,Kashish Mahendra,Bhavika Berwal,Ritesh Kumar*

Main category: cs.CL

TL;DR: 该研究通过生成多语言合成农业数据集并微调特定语言的大语言模型，显著提升了农业问答系统在多语言环境下的准确性和本地化程度


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在农业领域缺乏精确性和本地化能力，特别是在多语言和低资源环境下，由于缺乏特定领域训练和高质量区域数据集，无法为农民提供准确的本土化农业建议

Method: 从农业特定文档生成多语言合成农业数据集（英语、印地语、旁遮普语），然后对特定语言的大语言模型进行微调，构建农业问答系统

Result: 在精心策划的多语言数据集上的评估显示，与基线模型相比，微调后的模型在事实准确性、相关性和农业共识方面都有显著改善

Conclusion: 基于合成数据驱动的特定语言微调是提升大语言模型在农业领域性能的有效策略，特别适用于多语言和低资源环境，能够为不同语言社区提供更准确的本土化农业咨询服务，为AI驱动的农业解决方案缩小知识差距迈出了重要一步

Abstract: Enabling farmers to access accurate agriculture-related information in their
native languages in a timely manner is crucial for the success of the
agriculture field. Although large language models (LLMs) can be used to
implement Question Answering (QA) systems, simply using publicly available
general-purpose LLMs in agriculture typically offer generic advisories, lacking
precision in local and multilingual contexts due to insufficient
domain-specific training and scarcity of high-quality, region-specific
datasets. Our study addresses these limitations by generating multilingual
synthetic agricultural datasets (English, Hindi, Punjabi) from
agriculture-specific documents and fine-tuning language-specific LLMs. Our
evaluation on curated multilingual datasets demonstrates significant
improvements in factual accuracy, relevance, and agricultural consensus for the
fine-tuned models compared to their baseline counterparts. These results
highlight the efficacy of synthetic data-driven, language-specific fine-tuning
as an effective strategy to improve the performance of LLMs in agriculture,
especially in multilingual and low-resource settings. By enabling more accurate
and localized agricultural advisory services, this study provides a meaningful
step toward bridging the knowledge gap in AI-driven agricultural solutions for
diverse linguistic communities.

</details>


### [6] [Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks](https://arxiv.org/abs/2507.16989)
*Giulio Pelosio,Devesh Batra,Noémie Bovey,Robert Hankache,Cristovao Iglesias,Greig Cowan,Raad Khraishi*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在使用文化指示性姓名替代明确国籍标签时仍表现出潜在偏见，小模型比大模型表现出更多偏见和更低准确性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型即使在没有明确人口统计标记的情况下也可能对特定国籍表现出潜在偏见，需要研究在更接近真实世界应用场景中用文化指示性姓名替代明确国籍标签对模型偏见的影响

Method: 基于BBQ数据集引入新颖的基于姓名的基准测试方法，用文化指示性姓名替代明确的国籍标签，在OpenAI、Google和Anthropic等主要厂商的不同规模LLM上测试偏见程度和准确性变化

Result: 小模型相比大模型准确性更低且表现出更多偏见。在模糊语境下，Claude Haiku的刻板印象偏见得分为9%，而其大型版本Claude Sonnet仅为3.5%，后者准确性还超出117.7%。小模型在模糊语境中保留更多现有错误，如GPT-4o保留68%错误率而GPT-4o-mini保留76%

Conclusion: 研究强调了LLM中偏见的顽固性和韧性，突出了其对AI系统在多元化全球环境中开发和部署的深远影响

Abstract: Large Language Models (LLMs) can exhibit latent biases towards specific
nationalities even when explicit demographic markers are not present. In this
work, we introduce a novel name-based benchmarking approach derived from the
Bias Benchmark for QA (BBQ) dataset to investigate the impact of substituting
explicit nationality labels with culturally indicative names, a scenario more
reflective of real-world LLM applications. Our novel approach examines how this
substitution affects both bias magnitude and accuracy across a spectrum of LLMs
from industry leaders such as OpenAI, Google, and Anthropic. Our experiments
show that small models are less accurate and exhibit more bias compared to
their larger counterparts. For instance, on our name-based dataset and in the
ambiguous context (where the correct choice is not revealed), Claude Haiku
exhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for
its larger counterpart, Claude Sonnet, where the latter also outperformed it by
117.7% in accuracy. Additionally, we find that small models retain a larger
portion of existing errors in these ambiguous contexts. For example, after
substituting names for explicit nationality references, GPT-4o retains 68% of
the error rate versus 76% for GPT-4o-mini, with similar findings for other
model providers, in the ambiguous context. Our research highlights the stubborn
resilience of biases in LLMs, underscoring their profound implications for the
development and deployment of AI systems in diverse, global contexts.

</details>


### [7] [Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors](https://arxiv.org/abs/2507.17009)
*Ming Huang,Zehan Li,Yan Hu,Wanjing Wang,Andrew Wen,Scott Lane,Salih Selek,Lokesh Shahani,Rodrigo Machado-Vieira,Jair Soares,Hua Xu,Hongfang Liu*

Main category: cs.CL

TL;DR: 研究使用GPT-3.5和GPT-4.5等生成式大语言模型对精神科电子健康记录进行自杀相关因素的多标签分类，开发了端到端生成式分类管道，并引入了先进的评估方法。微调的GPT-3.5达到了0.94的部分匹配准确率和0.91的F1分数，证明了生成式AI在复杂临床分类任务中的可行性。


<details>
  <summary>Details</summary>
Motivation: 自杀是全球性健康危机，每年超过72万人死亡，数百万人受自杀意念和自杀企图影响。早期识别自杀相关因素对及时干预至关重要。虽然已有研究将AI应用于临床笔记中的自杀相关因素检测，但大多数将自杀视为二元分类任务，忽略了共现风险因素的复杂性。

Method: 使用生成式大语言模型GPT-3.5和GPT-4.5对精神科电子健康记录进行自杀相关因素的多标签分类。提出了新颖的端到端生成式多标签分类管道，并引入了先进的评估方法，包括标签集级别指标和多标签混淆矩阵用于错误分析。

Result: 微调的GPT-3.5取得了最佳性能，部分匹配准确率为0.94，F1分数为0.91。使用引导提示的GPT-4.5在标签集上表现更优，包括罕见或少数标签集，显示出更平衡和稳健的性能。研究发现了系统性错误模式，如自杀意念和自杀企图的混淆，并突出了模型倾向于谨慎过度标记的特点。

Conclusion: 这项工作不仅证明了使用生成式AI进行复杂临床分类任务的可行性，还为结构化非结构化电子健康记录数据提供了蓝图，以支持大规模临床研究和循证医学。研究为使用大语言模型处理复杂医疗分类任务提供了重要参考。

Abstract: Suicide remains a pressing global health crisis, with over 720,000 deaths
annually and millions more affected by suicide ideation (SI) and suicide
attempts (SA). Early identification of suicidality-related factors (SrFs),
including SI, SA, exposure to suicide (ES), and non-suicidal self-injury
(NSSI), is critical for timely intervention. While prior studies have applied
AI to detect SrFs in clinical notes, most treat suicidality as a binary
classification task, overlooking the complexity of cooccurring risk factors.
This study explores the use of generative large language models (LLMs),
specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs
from psychiatric electronic health records (EHRs). We present a novel end to
end generative MLC pipeline and introduce advanced evaluation methods,
including label set level metrics and a multilabel confusion matrix for error
analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match
accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior
performance across label sets, including rare or minority label sets,
indicating a more balanced and robust performance. Our findings reveal
systematic error patterns, such as the conflation of SI and SA, and highlight
the models tendency toward cautious over labeling. This work not only
demonstrates the feasibility of using generative AI for complex clinical
classification tasks but also provides a blueprint for structuring unstructured
EHR data to support large scale clinical research and evidence based medicine.

</details>


### [8] [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](https://arxiv.org/abs/2507.17015)
*Arduin Findeis,Floris Weers,Guoli Yin,Ke Ye,Ruoming Pang,Tom Gunter*

Main category: cs.CL

TL;DR: 本文提出了一个使用外部工具（网络搜索和代码执行）的智能体系统，用于改进大语言模型在长篇事实性、数学和代码任务中的成对偏好评估质量，通过外部验证来减少内部知识和偏见的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的成对偏好评估方法在某些领域（如包含大量事实陈述的响应）中存在问题，注释者可能过度关注写作质量而忽视底层事实准确性。对于长篇事实性、数学和代码等具有挑战性的响应域，获得高质量的成对比较评估变得困难。

Method: 提出了一个使用工具的智能体系统，通过集成网络搜索和代码执行工具来提供更高质量的反馈。该系统基于外部验证进行判断，独立于大语言模型的内部知识和偏见，专门针对长篇事实性、数学和代码任务等三个具有挑战性的响应域进行优化。

Result: 在RewardBench（包括AlpacaEval和LLMBar）、RewardMath以及三个新数据集上进行了广泛的实验评估。结果表明外部工具在许多情况下确实能够改善性能，但并非在所有情况下都有效。实验还揭示了性能对简单参数（如提示）的敏感性。

Conclusion: 外部工具可以在多数情况下改进大语言模型的评估性能，但效果并非普遍适用。研究强调了改进非饱和注释者基准测试的必要性，并指出了评估系统对参数设置的敏感性问题。

Abstract: Pairwise preferences over model responses are widely collected to evaluate
and provide feedback to large language models (LLMs). Given two alternative
model responses to the same input, a human or AI annotator selects the "better"
response. This approach can provide feedback for domains where other hard-coded
metrics are difficult to obtain (e.g., chat response quality), thereby helping
model evaluation or training. However, for some domains high-quality pairwise
comparisons can be tricky to obtain - from AI and humans. For example, for
responses with many factual statements, annotators may disproportionately weigh
writing quality rather than underlying facts. In this work, we explore
augmenting standard AI annotator systems with additional tools to improve
performance on three challenging response domains: long-form factual, math and
code tasks. We propose a tool-using agentic system to provide higher quality
feedback on these domains. Our system uses web-search and code execution to
ground itself based on external validation, independent of the LLM's internal
knowledge and biases. We provide extensive experimental results evaluating our
method across the three targeted response domains as well as general annotation
tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as
three new datasets for domains with saturated pre-existing datasets. Our
results indicate that external tools can indeed improve performance in many,
but not all, cases. More generally, our experiments highlight the sensitivity
of performance to simple parameters (e.g., prompt) and the need for improved
(non-saturated) annotator benchmarks. We share our code at
https://github.com/apple/ml-agent-evaluator.

</details>


### [9] [Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings](https://arxiv.org/abs/2507.17025)
*Soumen Sinha,Shahryar Rahnamayan,Azam Asilian Bidgoli*

Main category: cs.CL

TL;DR: 本文提出了一种基于坐标搜索的优化框架，为文本嵌入的每个特征找到最优阈值，将连续嵌入转换为二进制表示（条形码），相比传统的固定阈值方法在准确性上有显著提升，同时保持了存储和计算效率。


<details>
  <summary>Details</summary>
Motivation: 大规模自然语言处理应用中，存储和计算效率是关键问题。传统的实值特征嵌入在效率方面存在挑战，而现有的二进制转换方法通常使用固定阈值，无法充分发挥每个特征的潜力，因此需要一种更精确的二进制编码方法。

Method: 提出了基于坐标搜索的优化框架，为每个特征识别最优阈值，而不是使用传统的固定阈值方法。该方法可以为BERT等机器学习模型导出的NLP嵌入生成特征特定的阈值，从而创建更准确的二进制表示（条形码）。

Result: 在不同NLP任务和数据集上进行的广泛实验和统计测试表明，使用最优阈值生成的二进制嵌入在准确性方面优于传统的二值化方法。最优条形码表示在各种NLP应用中显示出良好的效果，证明了其转换文本表示的潜力。

Conclusion: 该技术生成的二进制表示既准确又高效，不仅适用于NLP嵌入，还可以应用于任何特征，使其在机器学习应用的广泛领域中都具有实用价值。这种方法为大规模NLP应用提供了一种有效的解决方案，平衡了性能和效率需求。

Abstract: Efficient text embedding is crucial for large-scale natural language
processing (NLP) applications, where storage and computational efficiency are
key concerns. In this paper, we explore how using binary representations
(barcodes) instead of real-valued features can be used for NLP embeddings
derived from machine learning models such as BERT. Thresholding is a common
method for converting continuous embeddings into binary representations, often
using a fixed threshold across all features. We propose a Coordinate
Search-based optimization framework that instead identifies the optimal
threshold for each feature, demonstrating that feature-specific thresholds lead
to improved performance in binary encoding. This ensures that the binary
representations are both accurate and efficient, enhancing performance across
various features. Our optimal barcode representations have shown promising
results in various NLP applications, demonstrating their potential to transform
text representation. We conducted extensive experiments and statistical tests
on different NLP tasks and datasets to evaluate our approach and compare it to
other thresholding methods. Binary embeddings generated using using optimal
thresholds found by our method outperform traditional binarization methods in
accuracy. This technique for generating binary representations is versatile and
can be applied to any features, not just limited to NLP embeddings, making it
useful for a wide range of domains in machine learning applications.

</details>


### [10] [CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards](https://arxiv.org/abs/2507.17147)
*Cheng Liu,Yifei Lu,Fanghua Ye,Jian Li,Xingyu Chen,Feiliang Ren,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: 本文提出了CogDual，一种基于认知心理学的角色扮演语言智能体，采用"认知-响应"推理范式，通过建模外部情境感知和内部自我感知来提升角色一致性和上下文对齐能力


<details>
  <summary>Details</summary>
Motivation: 现有的角色扮演语言智能体(RPLAs)主要依赖提示工程或监督微调来模仿角色行为，但忽略了驱动这些行为的底层认知机制，缺乏对角色心理状态的深入理解

Method: 提出CogDual方法，采用"认知-响应"推理范式，联合建模外部情境感知和内部自我感知；使用强化学习和两种通用奖励机制来优化开放域文本生成性能

Result: 在CoSER基准测试以及Cross-MR和LifeChoice数据集上的广泛实验表明，CogDual在各种角色扮演任务中持续优于现有基线方法，并展现出良好的泛化能力

Conclusion: CogDual通过引入认知心理学启发的双重感知机制，有效提升了角色扮演语言智能体的角色一致性和上下文对齐能力，为构建更智能的角色扮演系统提供了新的解决方案

Abstract: Role-Playing Language Agents (RPLAs) have emerged as a significant
application direction for Large Language Models (LLMs). Existing approaches
typically rely on prompt engineering or supervised fine-tuning to enable models
to imitate character behaviors in specific scenarios, but often neglect the
underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by
cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a
\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external
situational awareness and internal self-awareness, CogDual generates responses
with improved character consistency and contextual alignment. To further
optimize the performance, we employ reinforcement learning with two
general-purpose reward schemes designed for open-domain text generation.
Extensive experiments on the CoSER benchmark, as well as Cross-MR and
LifeChoice, demonstrate that CogDual consistently outperforms existing
baselines and generalizes effectively across diverse role-playing tasks.

</details>


### [11] [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)
*Zhiqiang Liu,Enpei Niu,Yin Hua,Mengshu Sun,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 本文提出了SKA-Bench，一个用于评估大语言模型结构化知识理解能力的综合基准测试，涵盖知识图谱、表格等四种结构化知识形式，并从四个维度进行细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化知识理解评估方法不够严格（缺乏对特定能力的评估）且只关注单一类型的结构化知识，因此需要提出更全面、更严格的结构化知识理解基准来诊断大语言模型的不足。

Method: 构建了包含四种结构化知识形式（知识图谱、表格、知识图谱+文本、表格+文本）的SKA-Bench基准。采用三阶段流水线构建测试实例，包括问题、答案、正向知识单元和噪声知识单元。将实例扩展为四个基本能力测试台：噪声鲁棒性、顺序不敏感性、信息整合和负面拒绝。

Result: 对8个代表性大语言模型（包括先进的DeepSeek-R1）的实证评估显示，现有大语言模型在理解结构化知识方面仍面临重大挑战，其性能受到噪声数量、知识单元顺序和幻觉现象等因素的影响。

Conclusion: 通过SKA-Bench基准测试发现，当前的大语言模型在结构化知识理解方面存在显著不足，需要进一步改进以提高其处理结构化知识的能力。该基准为评估和改进大语言模型的结构化知识理解能力提供了有价值的工具。

Abstract: Although large language models (LLMs) have made significant progress in
understanding Structured Knowledge (SK) like KG and Table, existing evaluations
for SK understanding are non-rigorous (i.e., lacking evaluations of specific
capabilities) and focus on a single type of SK. Therefore, we aim to propose a
more comprehensive and rigorous structured knowledge understanding benchmark to
diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a
Structured Knowledge Augmented QA Benchmark that encompasses four widely used
structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a
three-stage pipeline to construct SKA-Bench instances, which includes a
question, an answer, positive knowledge units, and noisy knowledge units. To
evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we
expand the instances into four fundamental ability testbeds: Noise Robustness,
Order Insensitivity, Information Integration, and Negative Rejection. Empirical
evaluations on 8 representative LLMs, including the advanced DeepSeek-R1,
indicate that existing LLMs still face significant challenges in understanding
structured knowledge, and their performance is influenced by factors such as
the amount of noise, the order of knowledge units, and hallucination
phenomenon. Our dataset and code are available at
https://github.com/Lza12a/SKA-Bench.

</details>


### [12] [FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance](https://arxiv.org/abs/2507.17186)
*Lingfeng Zeng,Fangqi Lou,Zixuan Wang,Jiajie Xu,Jinyi Niu,Mengping Li,Yifan Dong,Qi Qi,Wei Zhang,Ziwei Yang,Jun Han,Ruilun Feng,Ruiqi Hu,Lejie Zhang,Zhengbo Feng,Yicheng Ren,Xin Guo,Zhaowei Liu,Dongpo Cheng,Weige Cai,Liwen Zhang*

Main category: cs.CL

TL;DR: 本文提出了FinGAIA，这是第一个专门评估AI智能体在金融领域实际能力的端到端基准测试，包含407个精心设计的任务，覆盖七个金融子领域和三个层次的场景深度。


<details>
  <summary>Details</summary>
Motivation: AI智能体的蓬勃发展为各领域复杂任务自动化带来了前所未有的机遇，但其在金融领域的多步骤、多工具协作能力仍未得到充分探索，缺乏专门的评估基准。

Method: 构建了包含407个任务的FinGAIA基准测试，涵盖证券、基金、银行、保险、期货、信托和资产管理七个金融子领域，按基础业务分析、资产决策支持和战略风险管理三个层次组织，并在零样本设置下评估了10个主流AI智能体。

Result: 表现最佳的ChatGPT智能体总体准确率为48.9%，虽然优于非专业人士，但仍比金融专家低35个百分点以上。错误分析揭示了五种反复出现的失败模式：跨模态对齐缺陷、金融术语偏见、操作流程认知障碍等。

Conclusion: 本研究提供了首个与金融领域密切相关的智能体基准测试，旨在客观评估和促进智能体在这一关键领域的发展，为未来研究指明了重要方向。

Abstract: The booming development of AI agents presents unprecedented opportunities for
automating complex tasks across various domains. However, their multi-step,
multi-tool collaboration capabilities in the financial sector remain
underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed
to evaluate the practical abilities of AI agents in the financial domain.
FinGAIA comprises 407 meticulously crafted tasks, spanning seven major
financial sub-domains: securities, funds, banking, insurance, futures, trusts,
and asset management. These tasks are organized into three hierarchical levels
of scenario depth: basic business analysis, asset decision support, and
strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot
setting. The best-performing agent, ChatGPT, achieved an overall accuracy of
48.9\%, which, while superior to non-professionals, still lags financial
experts by over 35 percentage points. Error analysis has revealed five
recurring failure patterns: Cross-modal Alignment Deficiency, Financial
Terminological Bias, Operational Process Awareness Barrier, among others. These
patterns point to crucial directions for future research. Our work provides the
first agent benchmark closely related to the financial domain, aiming to
objectively assess and promote the development of agents in this crucial field.
Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.

</details>


### [13] [The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models](https://arxiv.org/abs/2507.17216)
*Giuseppe Russo,Debora Nozza,Paul Röttger,Dirk Hovy*

Main category: cs.CL

TL;DR: 本研究发现大语言模型在道德判断上与人类存在显著差异，特别是在争议性道德问题上表现更差，并提出了动态道德画像方法来改善模型与人类道德判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越依赖大语言模型获取道德建议，了解模型道德判断与人类道德判断的一致性变得至关重要，但目前对此了解甚少。

Method: 构建了包含1,618个真实道德困境的基准数据集，采用多元分布对齐任务比较模型与人类判断分布；建立60个价值观分类体系分析道德价值观差异；提出基于狄利克雷分布的动态道德画像采样方法。

Result: 发现模型仅在高度共识情况下能复现人类判断，当人类分歧增加时一致性急剧下降；模型依赖的道德价值观范围比人类更窄；动态道德画像方法将一致性提升了64.3%并增强了价值观多样性。

Conclusion: 研究揭示了大语言模型与人类在道德判断分布和价值观多样性上的"多元道德鸿沟"，提出的动态道德画像方法为实现更加多元化和人类对齐的道德指导提供了解决方案。

Abstract: People increasingly rely on Large Language Models (LLMs) for moral advice,
which may influence humans' decisions. Yet, little is known about how closely
LLMs align with human moral judgments. To address this, we introduce the Moral
Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a
distribution of human moral judgments consisting of a binary evaluation and a
free-text rationale. We treat this problem as a pluralistic distributional
alignment task, comparing the distributions of LLM and human judgments across
dilemmas. We find that models reproduce human judgments only under high
consensus; alignment deteriorates sharply when human disagreement increases. In
parallel, using a 60-value taxonomy built from 3,783 value expressions
extracted from rationales, we show that LLMs rely on a narrower set of moral
values than humans. These findings reveal a pluralistic moral gap: a mismatch
in both the distribution and diversity of values expressed. To close this gap,
we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method
that conditions model outputs on human-derived value profiles. DMP improves
alignment by 64.3% and enhances value diversity, offering a step toward more
pluralistic and human-aligned moral guidance from LLMs.

</details>


### [14] [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)
*Danny D. Leybzon,Shreyas Tirumala,Nishant Jain,Summer Gillen,Michael Jackson,Cameron McPhee,Jennifer Schmidt*

Main category: cs.CL

TL;DR: 研究者开发了基于大语言模型的AI电话调研系统，用于进行定量调查，该系统比传统IVR技术更自然和适应性强，能够处理语音中断和修正等人类语言特征。


<details>
  <summary>Details</summary>
Motivation: 随着语音AI技术的兴起，定量调研人员需要一种新的数据收集模式来扩大研究规模，同时平衡类人交互性和方法论严谨性的双重目标。传统的IVR技术在处理人类语音的复杂性方面存在局限性。

Method: 构建了基于大语言模型(LLM)、自动语音识别(ASR)和语音合成技术的AI调研系统。系统专门为定量研究设计，严格遵循研究最佳实践，包括问题顺序随机化、答案顺序随机化和准确措辞。通过SSRS意见小组进行两项试点调查，并进行人工管理的后续调查来评估受访者体验。

Result: 测量了三个关键指标：调查完成率、中断率和受访者满意度得分。结果表明，较短的调查工具和更具响应性的AI访问员可能有助于改善所研究的三个指标。

Conclusion: AI电话调研系统在定量研究中具有可行性，通过优化调查长度和提高AI响应性，可以改善调查完成率、降低中断率并提高受访者满意度，为量化调研提供了新的有效数据收集方式。

Abstract: With the rise of voice-enabled artificial intelligence (AI) systems,
quantitative survey researchers have access to a new data-collection mode: AI
telephone surveying. By using AI to conduct phone interviews, researchers can
scale quantitative studies while balancing the dual goals of human-like
interactivity and methodological rigor. Unlike earlier efforts that used
interactive voice response (IVR) technology to automate these surveys, voice AI
enables a more natural and adaptive respondent experience as it is more robust
to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on
large language models (LLM), automatic speech recognition (ASR), and speech
synthesis technologies. The system was specifically designed for quantitative
research, and strictly adhered to research best practices like question order
randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot
surveys with the SSRS Opinion Panel and followed-up with a separate
human-administered survey to assess respondent experiences. We measured three
key metrics: the survey completion rates, break-off rates, and respondent
satisfaction scores. Our results suggest that shorter instruments and more
responsive AI interviewers may contribute to improvements across all three
metrics studied.

</details>


### [15] [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)
*Kyeongkyu Lee,Seonghwan Yoon,Hongki Lim*

Main category: cs.CL

TL;DR: 提出了CLARIFID框架，通过模拟专家的两步工作流程（从发现到印象）来自动生成放射学报告，使用多视图胸部X光图像和强化学习优化诊断正确性


<details>
  <summary>Details</summary>
Motivation: 现有的放射学报告自动生成方法难以提供临床可靠的结论，主要问题包括：缺乏对报告事实正确性的有效保证，多数方法依赖单视图图像限制了诊断的全面性，以及未能有效确保临床推理的连贯性

Method: CLARIFID框架包含四个核心组件：(1)通过分段感知预训练学习从发现到印象的逻辑流程；(2)使用近端策略优化进行微调，以印象部分的CheXbert F1分数作为奖励；(3)强制推理感知解码，先完成"发现"再合成"印象"；(4)通过视觉变换器的多视图编码器融合多个胸部X光视图

Result: 在MIMIC-CXR数据集上的实验结果表明，该方法在标准NLG指标和临床感知评分上都优于现有基线方法，实现了卓越的临床效果

Conclusion: CLARIFID通过模拟专家工作流程和多视图融合，成功提高了自动放射学报告生成的诊断正确性和临床可靠性，为减轻放射科医生工作负担提供了有效解决方案

Abstract: Automatic generation of radiology reports has the potential to alleviate
radiologists' significant workload, yet current methods struggle to deliver
clinically reliable conclusions. In particular, most prior approaches focus on
producing fluent text without effectively ensuring the factual correctness of
the reports and often rely on single-view images, limiting diagnostic
comprehensiveness. We propose CLARIFID, a novel framework that directly
optimizes diagnostic correctness by mirroring the two-step workflow of experts.
Specifically, CLARIFID (1) learns the logical flow from Findings to Impression
through section-aware pretraining, (2) is fine-tuned with Proximal Policy
Optimization in which the CheXbert F1 score of the Impression section serves as
the reward, (3) enforces reasoning-aware decoding that completes "Findings"
before synthesizing the "Impression", and (4) fuses multiple chest X-ray views
via a vision-transformer-based multi-view encoder. During inference, we apply a
reasoning-aware next-token forcing strategy followed by report-level
re-ranking, ensuring that the model first produces a comprehensive Findings
section before synthesizing the Impression and thereby preserving coherent
clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate
that our method achieves superior clinical efficacy and outperforms existing
baselines on both standard NLG metrics and clinically aware scores.

</details>


### [16] [Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.17288)
*Miaomiao Gao,Xiaoxiao Xiang,Yiwen Guo*

Main category: cs.CL

TL;DR: 本文提出了Triple X语音识别系统，采用编码器-适配器-大语言模型架构，通过多阶段训练策略优化多语言对话语音识别，在MLC-SLM挑战赛中获得第二名


<details>
  <summary>Details</summary>
Motivation: 针对多语言对话场景中的语音识别准确性优化问题，需要充分利用文本大语言模型的推理能力并结合特定领域的适应性调整

Method: 提出创新的编码器-适配器-大语言模型架构，结合精心设计的多阶段训练策略，利用大规模多语言音频数据集进行训练

Result: 在开发集和测试集上都获得了具有竞争力的词错误率(WER)性能表现，在挑战赛排名中获得第二名

Conclusion: 所提出的方法成功实现了多语言对话语音识别的性能优化，证明了编码器-适配器-大语言模型架构和多阶段训练策略的有效性

Abstract: This paper describes our Triple X speech recognition system submitted to Task
1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)
Challenge. Our work focuses on optimizing speech recognition accuracy in
multilingual conversational scenarios through an innovative encoder-adapter-LLM
architecture. This framework harnesses the powerful reasoning capabilities of
text-based large language models while incorporating domain-specific
adaptations. To further enhance multilingual recognition performance, we
adopted a meticulously designed multi-stage training strategy leveraging
extensive multilingual audio datasets. Experimental results demonstrate that
our approach achieves competitive Word Error Rate (WER) performance on both dev
and test sets, obtaining second place in the challenge ranking.

</details>


### [17] [Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents](https://arxiv.org/abs/2507.17399)
*Zhili Shen,Chenxin Diao,Pascual Merita,Pavlos Vougiouklis,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 本文将最先进的基于图的检索增强生成方法GeAR适配到SIGIR 2025 LiveRAG挑战赛，探索其在更广泛数据集上的性能表现和局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的检索增强生成方法通常针对特定任务设计（如多跳问答和查询导向摘要），在更广泛数据集上的通用适用性缺乏充分证据，需要验证这些方法的通用性能。

Method: 采用最先进的基于图的RAG解决方案GeAR，将其适配到SIGIR 2025 LiveRAG挑战赛的数据集和任务要求上，利用从文档中提取的实体及其关系等结构化或半结构化信息来增强检索过程。

Result: 通过在SIGIR 2025 LiveRAG挑战赛上的实验，评估了GeAR方法在更广泛数据集上的性能表现，并识别了该方法在实际应用中的局限性。

Conclusion: 基于图的RAG方法虽然在特定任务上表现良好，但在更广泛的数据集和任务上仍存在一定的局限性，需要进一步改进以提高其通用适用性。

Abstract: Recent studies have explored graph-based approaches to retrieval-augmented
generation, leveraging structured or semi-structured information -- such as
entities and their relations extracted from documents -- to enhance retrieval.
However, these methods are typically designed to address specific tasks, such
as multi-hop question answering and query-focused summarisation, and therefore,
there is limited evidence of their general applicability across broader
datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG
solution: $\text{GeAR}$ and explore its performance and limitations on the
SIGIR 2025 LiveRAG Challenge.

</details>


### [18] [Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging](https://arxiv.org/abs/2507.17409)
*Carlotta Quensel,Neele Falk,Gabriella Lapesa*

Main category: cs.CL

TL;DR: 本研究分析了主观因素（情感、叙事、模糊表达）对论证强度的影响，发现叙事和模糊表达在客观和主观论证质量上产生相反效果，而情感的影响取决于修辞运用而非领域。


<details>
  <summary>Details</summary>
Motivation: 随着NLP领域将主观性视为资产而非问题的趋势发展，论证质量的新维度正在被研究。虽然存在关于个人故事等个别主观特征的研究，但缺乏对这些特征与论证强度关系的大规模分析。

Method: 对两个标注了客观论证质量和主观说服力的标准数据集进行回归分析，量化主观因素（情感、叙事、模糊表达）的影响。比较和评估每个主观特征的自动标注方法。

Result: 叙事和模糊表达对客观和主观论证质量产生相反效果；情感的影响取决于其修辞运用而非所在领域。研究揭示了主观特征对数据集中编码的两个论证强度方面的不同影响模式。

Conclusion: 主观因素对论证强度具有复杂且差异化的影响，其效果因论证质量的客观性/主观性维度而异。情感的影响主要由修辞运用方式决定，而非应用领域。

Abstract: In assessing argument strength, the notions of what makes a good argument are
manifold. With the broader trend towards treating subjectivity as an asset and
not a problem in NLP, new dimensions of argument quality are studied. Although
studies on individual subjective features like personal stories exist, there is
a lack of large-scale analyses of the relation between these features and
argument strength. To address this gap, we conduct regression analysis to
quantify the impact of subjective factors $-$ emotions, storytelling, and
hedging $-$ on two standard datasets annotated for objective argument quality
and subjective persuasion. As such, our contribution is twofold: at the level
of contributed resources, as there are no datasets annotated with all studied
dimensions, this work compares and evaluates automated annotation methods for
each subjective feature. At the level of novel insights, our regression
analysis uncovers different patterns of impact of subjective features on the
two facets of argument strength encoded in the datasets. Our results show that
storytelling and hedging have contrasting effects on objective and subjective
argument quality, while the influence of emotions depends on their rhetoric
utilization rather than the domain.

</details>


### [19] [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)
*Shiting Chen,Zijian Zhao,Jinsong Chen*

Main category: cs.CL

TL;DR: 本文提出了两种增强RAG的方法来结合多个嵌入模型的优势：混合嵌入RAG和置信度RAG，其中置信度RAG通过选择最高置信度的响应显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 不同的嵌入模型在RAG中表现出不同的优势，导致相似性计算结果和LLM响应质量存在差异，需要一种方法来结合多个嵌入模型的优势以提升RAG性能。

Method: 提出两种方法：1）混合嵌入RAG：基于标准化相似性对多个嵌入模型的检索结果进行排序和选择；2）置信度RAG：使用不同嵌入模型多次生成响应，然后选择置信度最高的响应。

Result: 置信度RAG相比原始LLM和RAG分别平均提升约10%和5%的性能，而混合嵌入RAG没有超越原始RAG。在不同LLM和嵌入模型上的一致结果表明置信度RAG是一种高效的即插即用方法。

Conclusion: 置信度RAG是一种有效的增强RAG性能的方法，通过结合多个嵌入模型并选择最高置信度响应，能够在各种领域中提供稳定的性能提升，具有良好的通用性和实用性。

Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various
fields, the methods for incorporating up-to-date information into LLMs or
adding external knowledge to construct domain-specific models have garnered
wide attention. Retrieval-Augmented Generation (RAG), serving as an
inference-time scaling method, is notable for its low cost and minimal effort
for parameter tuning. However, due to heterogeneous training data and model
architecture, the variant embedding models used in RAG exhibit different
benefits across various areas, often leading to different similarity
calculation results and, consequently, varying response quality from LLMs. To
address this problem, we propose and examine two approaches to enhance RAG by
combining the benefits of multiple embedding models, named Mixture-Embedding
RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects
retrievals from multiple embedding models based on standardized similarity;
however, it does not outperform vanilla RAG. In contrast, Confident RAG
generates responses multiple times using different embedding models and then
selects the responses with the highest confidence level, demonstrating average
improvements of approximately 10% and 5% over vanilla LLMs and RAG,
respectively. The consistent results across different LLMs and embedding models
indicate that Confident RAG is an efficient plug-and-play approach for various
domains. We will release our code upon publication.

</details>


### [20] [MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs](https://arxiv.org/abs/2507.17476)
*Alexander R. Fabbri,Diego Mares,Jorge Flores,Meher Mankikar,Ernesto Hernandez,Dean Lee,Bing Liu,Chen Xing*

Main category: cs.CL

TL;DR: 本文介绍了MultiNRC基准测试，这是一个评估大语言模型在法语、西班牙语和中文等多种语言文化背景下推理能力的新基准。研究发现当前主流LLM在多语言本土推理任务上表现不佳，没有模型在MultiNRC上得分超过50%。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言推理基准通常是通过翻译英语推理基准构建的，这导致这些基准偏向于英语语言和文化背景的推理问题。因此需要一个真正由本土说话者创建的、具有语言和文化根基的多语言推理基准来更准确评估LLM的多语言推理能力。

Method: 构建了MultiNRC基准，包含超过1000个由法语、西班牙语和中文本土说话者编写的推理问题。基准涵盖四个核心推理类别：特定语言的语言推理、文字游戏和谜语、文化/传统推理，以及具有文化相关性的数学推理。对于文化推理和文化数学推理部分，还提供了英语等价翻译版本以便直接比较。对14个主流LLM进行了系统评估。

Result: 评估结果显示：(1)当前LLM在本土多语言推理方面表现不佳，没有模型在MultiNRC上得分超过50%；(2)LLM在处理语言、文化和逻辑推理任务时表现出不同的优势和劣势；(3)大多数模型在英语数学推理方面的表现明显优于原始语言(+10%)，表明在文化根基知识方面仍存在持续挑战。

Conclusion: 研究表明当前的大语言模型在多语言本土推理能力方面仍有很大提升空间，特别是在处理具有深厚文化背景的推理任务时。模型在英语和其他语言之间存在显著的性能差距，这突出了开发真正多语言、多文化智能系统的重要性和挑战性。

Abstract: Although recent Large Language Models (LLMs) have shown rapid improvement on
reasoning benchmarks in English, the evaluation of such LLMs' multilingual
reasoning capability across diverse languages and cultural contexts remains
limited. Existing multilingual reasoning benchmarks are typically constructed
by translating existing English reasoning benchmarks, biasing these benchmarks
towards reasoning problems with context in English language/cultures. In this
work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a
benchmark designed to assess LLMs on more than 1,000 native, linguistic and
culturally grounded reasoning questions written by native speakers in French,
Spanish, and Chinese. MultiNRC covers four core reasoning categories:
language-specific linguistic reasoning, wordplay & riddles, cultural/tradition
reasoning, and math reasoning with cultural relevance. For cultural/tradition
reasoning and math reasoning with cultural relevance, we also provide English
equivalent translations of the multilingual questions by manual translation
from native speakers fluent in English. This set of English equivalents can
provide a direct comparison of LLM reasoning capacity in other languages vs.
English on the same reasoning questions. We systematically evaluate current 14
leading LLMs covering most LLM families on MultiNRC and its English equivalent
set. The results show that (1) current LLMs are still not good at native
multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs
exhibit distinct strengths and weaknesses in handling linguistic, cultural, and
logical reasoning tasks; (3) Most models perform substantially better in math
reasoning in English compared to in original languages (+10%), indicating
persistent challenges with culturally grounded knowledge.

</details>


### [21] [Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice](https://arxiv.org/abs/2507.17527)
*Shanbo Cheng,Yu Bao,Zhichao Huang,Yu Lu,Ningxin Peng,Lu Xu,Runsheng Yu,Rong Cao,Ting Han,Zeyang Li,Sitong Liu,Shengtao Ma,Shiguang Pan,Jiongchen Xiao,Nuo Xu,Meng Yang,Rong Ye,Yiming Yu,Ruofei Zhang,Wanyi Zhang,Wenhao Zhu,Liehao Zou,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: 本文介绍了Seed-LiveInterpret 2.0，一个端到端的同声传译模型，通过双工语音理解生成框架实现高保真、超低延迟的语音到语音生成，并具备语音克隆能力，在翻译质量和延迟方面显著优于商业解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的同声传译自动系统面临多重挑战：转录和翻译质量不佳、缺乏实时语音生成、多说话人混淆、翻译语音膨胀等问题，特别是在长篇话语中表现更为突出，亟需一个产品级的端到端解决方案。

Method: 提出了一个新颖的双工语音到语音理解生成框架，结合大规模预训练和强化学习技术，构建端到端的同声传译模型，实现语音到语音的直接转换并支持语音克隆功能。

Result: 模型在翻译准确性和延迟之间实现了显著更好的平衡，经人工译员验证在复杂场景下准确率超过70%；相比商业同声传译解决方案在翻译质量上有显著提升，同时将克隆语音的平均延迟从近10秒大幅缩短至接近实时的3秒，减少约70%。

Conclusion: Seed-LiveInterpret 2.0成功解决了同声传译系统的关键技术挑战，实现了高质量、低延迟的语音到语音翻译，显著提升了实用性，为同声传译技术的产业化应用提供了有效解决方案。

Abstract: Simultaneous Interpretation (SI) represents one of the most daunting
frontiers in the translation industry, with product-level automatic systems
long plagued by intractable challenges: subpar transcription and translation
quality, lack of real-time speech generation, multi-speaker confusion, and
translated speech inflation, especially in long-form discourses. In this study,
we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers
high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning
capabilities. As a fully operational product-level solution, Seed-LiveInterpret
2.0 tackles these challenges head-on through our novel duplex speech-to-speech
understanding-generating framework. Experimental results demonstrate that
through large-scale pretraining and reinforcement learning, the model achieves
a significantly better balance between translation accuracy and latency,
validated by human interpreters to exceed 70% correctness in complex scenarios.
Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by
significant margins in translation quality, while slashing the average latency
of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is
around a near 70% reduction that drastically enhances practical usability.

</details>


### [22] [Synthetic Voice Data for Automatic Speech Recognition in African Languages](https://arxiv.org/abs/2507.17578)
*Brian DeRenzi,Anna Dixon,Mohamed Aymane Farhi,Christian Resch*

Main category: cs.CL

TL;DR: 该研究首次系统性评估了大规模合成语音语料库在非洲自动语音识别(ASR)中的应用，通过LLM生成文本、TTS合成语音和ASR微调的三步流程，为非洲语言创建了超过2500小时的合成语音数据，成本仅为真实数据的1%，并证明了合成数据在改善ASR性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 非洲超过2300种语言中的大多数仍无法使用语音技术，缺乏足够的语音数据来训练有效的ASR系统。现有的真实语音数据收集成本高昂且耗时，特别是对于资源极度稀缺的非洲语言而言。

Method: 采用三步流程：1)使用大语言模型(LLM)驱动的文本创建；2)通过文本转语音(TTS)技术进行语音合成；3)使用合成数据对ASR模型进行微调。研究涵盖10种语言，重点评估了3种语言(豪萨语、多卢奥语、奇切瓦语)的ASR改进效果。

Result: 10种语言中有8种的合成文本可读性评分超过5分(满分7分)。创建了超过2500小时的合成语音数据，成本低于真实数据的1%。在豪萨语上，250小时真实数据+250小时合成数据训练的Wav2Vec-BERT-2.0模型性能与500小时纯真实数据基线相匹配。奇切瓦语在1:2真实与合成数据比例下WER相对改善约6.5%。

Conclusion: 合成语音数据可以有效补充真实数据来改善非洲语言的ASR性能，特别是在资源极度稀缺的情况下。然而，对于极低资源语言，效果存在差异，需要更稳健的评估协议和更准确的评估数据。研究公开发布了所有数据和模型以促进后续研究。

Abstract: Speech technology remains out of reach for most of the over 2300 languages in
Africa. We present the first systematic assessment of large-scale synthetic
voice corpora for African ASR. We apply a three-step process: LLM-driven text
creation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages
for which we create synthetic text achieved readability scores above 5 out of
7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created
more than 2,500 hours of synthetic voice data at below 1% of the cost of real
data. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h
synthetic Hausa matched a 500h real-data-only baseline, while 579h real and
450h to 993h synthetic data created the best performance. We also present
gender-disaggregated ASR performance evaluation. For very low-resource
languages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2
real-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on
some evaluation data, but not on others. Investigating intercoder reliability,
ASR errors and evaluation datasets revealed the need for more robust reviewer
protocols and more accurate evaluation data. All data and models are publicly
released to invite further work to improve synthetic data for African
languages.

</details>


### [23] [A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)](https://arxiv.org/abs/2507.17618)
*Bowen Zheng,Ming Ma,Zhongqiao Lin,Tianming Yang*

Main category: cs.CL

TL;DR: 研究者提出了SPADE方法来解决大语言模型早期退出算法中表示不对齐的问题，通过空间对齐解码显著降低推理成本同时保持准确性


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算成本昂贵，现有的早期退出算法虽能减少推理成本，但因中间层与输出层表示不对齐导致解码不准确，性能较差

Method: 提出SPADE（空间对齐解码）方法，通过传播仅包含起始标记和答案标记的最小化序列来对齐中间层与输出层表示；训练SPADE的线性近似来计算基于熵的置信度指标；结合两者创建混合早期退出算法

Result: 该方法在不损害准确性的前提下显著降低了推理成本，为大语言模型的实际应用部署提供了可扩展且高效的解决方案

Conclusion: SPADE方法通过解决表示不对齐问题，成功实现了高效的早期退出机制，为大语言模型的实际部署提供了实用的优化方案

Abstract: Large language models are computationally expensive due to their deep
structures. Prior research has shown that intermediate layers contain
sufficient information to generate accurate answers, leading to the development
of early-exit algorithms that reduce inference costs by terminating computation
at earlier layers. However, these methods often suffer from poor performance
due to misalignment between intermediate and output layer representations that
lead to decoding inaccuracy. To address these challenges, we propose SPADE
(SPace Alignment DEcoding), a novel decoding method that aligns intermediate
layer representations with the output layer by propagating a minimally reduced
sequence consisting of only the start token and the answer token. We further
optimize the early-exit decision-making process by training a linear
approximation of SPADE that computes entropy-based confidence metrics. Putting
them together, we create a hybrid early-exit algorithm that monitors confidence
levels and stops inference at intermediate layers while using SPADE to generate
high-quality outputs. This approach significantly reduces inference costs
without compromising accuracy, offering a scalable and efficient solution for
deploying large language models in real-world applications.

</details>


### [24] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 本文提出基于增长边界矩阵(GBM)的正则化技术来提高NLP模型对对抗性攻击的鲁棒性，特别针对LSTM、S4状态空间模型和CNN架构，在多个基准数据集上实现了高达8.8%的鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP领域取得了进展，但模型仍然容易受到同义词替换等对抗性攻击的影响。虽然之前的工作专注于改善前馈和卷积架构的鲁棒性，但对于循环网络和现代状态空间模型(如S4)的鲁棒性研究不足。这些架构由于其序列处理和复杂的参数动态特性带来了独特的挑战。

Method: 引入基于增长边界矩阵(GBM)的新型正则化技术，通过减少输入扰动对模型输出的影响来提高NLP模型的鲁棒性。重点计算三种架构的GBM：长短期记忆网络(LSTM)、状态空间模型(S4)和卷积神经网络(CNN)。

Result: 在多个架构和基准数据集上进行的广泛实验表明，该方法比现有基线方法提高了高达8.8%的对抗鲁棒性，在对抗防御方面优于多种最先进的方法。

Conclusion: 提出的GBM正则化技术有效提高了NLP模型的对抗鲁棒性，特别是针对词汇替换攻击，同时改善了在干净文本上的泛化能力，并首次对SSM(S4)鲁棒性进行了系统分析。该方法的有效性通过实验得到验证，超越了现有的对抗防御方法。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [25] [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)
*Changxin Tian,Jiapeng Wang,Qian Zhao,Kunlong Chen,Jia Liu,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 提出了Warmup-Stable and Merge (WSM)框架，通过模型合并技术替代传统学习率衰减，在多个基准测试中显著优于传统Warmup-Stable-Decay方法


<details>
  <summary>Details</summary>
Motivation: 传统学习率衰减方法存在局限性，而无衰减的学习率调度方法和模型合并技术显示出潜力，需要建立学习率衰减与模型合并之间的正式连接

Method: 提出WSM框架，建立学习率衰减与模型合并的理论联系，将各种衰减策略（余弦衰减、线性衰减、逆平方根衰减）转化为原则性的模型平均方案，兼容多种优化方法

Result: 在多个基准测试中显著优于WSD方法：MATH提升3.5%，HumanEval提升2.9%，MMLU-Pro提升5.5%。发现合并持续时间是影响模型性能的最关键因素

Conclusion: WSM框架成功建立了学习率衰减与模型合并的统一理论基础，在多种场景下表现优异，为长期模型优化提供了有效解决方案

Abstract: Recent advances in learning rate (LR) scheduling have demonstrated the
effectiveness of decay-free approaches that eliminate the traditional decay
phase while maintaining competitive performance. Model merging techniques have
emerged as particularly promising solutions in this domain. We present
Warmup-Stable and Merge (WSM), a general framework that establishes a formal
connection between learning rate decay and model merging. WSM provides a
unified theoretical foundation for emulating various decay strategies-including
cosine decay, linear decay and inverse square root decay-as principled model
averaging schemes, while remaining fully compatible with diverse optimization
methods. Through extensive experiments, we identify merge duration-the training
window for checkpoint aggregation-as the most critical factor influencing model
performance, surpassing the importance of both checkpoint interval and merge
quantity. Our framework consistently outperforms the widely-adopted
Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving
significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on
MMLU-Pro. The performance advantages extend to supervised fine-tuning
scenarios, highlighting WSM's potential for long-term model refinement.

</details>


### [26] [Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries](https://arxiv.org/abs/2507.17636)
*Victor Hartman,Petter Törnberg*

Main category: cs.CL

TL;DR: 本研究使用零样本大语言模型对19个欧洲国家议员的1800万条推文进行负面竞选分析，发现执政党较少使用负面信息，而极端主义和民粹主义政党（特别是极右翼）的负面程度显著更高


<details>
  <summary>Details</summary>
Motivation: 现有的负面竞选分类方法成本高且可扩展性有限，限制了相关实证研究的发展。需要一种新的方法来进行大规模、跨语言的负面竞选研究

Method: 引入零样本大语言模型（LLMs）作为跨语言负面竞选分类的新方法，使用十种语言的基准数据集进行验证，然后应用该方法分析2017-2022年间19个欧洲国家议员发布的1800万条推文

Result: LLMs在负面竞选分类任务上的表现与母语人类编码员相当，优于传统的监督机器学习方法。研究发现执政党使用负面信息的可能性较低，而意识形态极端和民粹主义政党（特别是激进右翼）的负面程度显著更高

Conclusion: 党派层面的特征会影响多党制系统中的战略传播模式。大语言模型具有在跨语言和跨文化背景下实现可扩展、透明和可复制的政治传播研究的潜力

Abstract: Negative campaigning is a central feature of political competition, yet
empirical research has been limited by the high cost and limited scalability of
existing classification methods. This study makes two key contributions. First,
it introduces zero-shot Large Language Models (LLMs) as a novel approach for
cross-lingual classification of negative campaigning. Using benchmark datasets
in ten languages, we demonstrate that LLMs achieve performance on par with
native-speaking human coders and outperform conventional supervised machine
learning approaches. Second, we leverage this novel method to conduct the
largest cross-national study of negative campaigning to date, analyzing 18
million tweets posted by parliamentarians in 19 European countries between 2017
and 2022. The results reveal consistent cross-national patterns: governing
parties are less likely to use negative messaging, while ideologically extreme
and populist parties -- particularly those on the radical right -- engage in
significantly higher levels of negativity. These findings advance our
understanding of how party-level characteristics shape strategic communication
in multiparty systems. More broadly, the study demonstrates the potential of
LLMs to enable scalable, transparent, and replicable research in political
communication across linguistic and cultural contexts.

</details>


### [27] [Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models](https://arxiv.org/abs/2507.17702)
*Changxin Tian,Kunlong Chen,Jia Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 本文提出了效率杠杆(EL)指标来量化MoE模型相对于密集模型的计算优势，通过训练300多个模型发现了MoE架构配置与EL之间的可预测规律，并开发了统一的缩放定律来指导高效MoE模型的设计。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能够在不增加计算成本的情况下扩展大语言模型，但如何预测给定MoE配置的模型容量仍然是一个未解决的关键问题，缺乏系统性的理论指导来优化MoE架构设计。

Method: 引入效率杠杆(EL)指标量化MoE模型的计算优势，进行大规模实证研究训练超过300个模型(最大28B参数)，系统研究MoE架构配置与EL的关系，发现专家激活比率和总计算预算遵循幂律规律，专家粒度作为非线性调节器，将这些发现整合为统一的缩放定律。

Result: 发现EL主要由专家激活比率和总计算预算驱动，两者都遵循可预测的幂律；专家粒度作为非线性调节器存在明确的最优范围。开发的Ling-mini-beta模型仅用0.85B活跃参数就达到了6.1B密集模型的性能，同时计算资源消耗减少7倍以上，验证了缩放定律的准确性。

Conclusion: 本研究为高效MoE模型的缩放提供了有原则性和实证基础的理论框架，通过统一的缩放定律能够准确预测MoE架构的效率杠杆，为未来大语言模型的高效扩展提供了重要指导。

Abstract: Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large
Language Models (LLMs) efficiently by decoupling total parameters from
computational cost. However, this decoupling creates a critical challenge:
predicting the model capacity of a given MoE configurations (e.g., expert
activation ratio and granularity) remains an unresolved problem. To address
this gap, we introduce Efficiency Leverage (EL), a metric quantifying the
computational advantage of an MoE model over a dense equivalent. We conduct a
large-scale empirical study, training over 300 models up to 28B parameters, to
systematically investigate the relationship between MoE architectural
configurations and EL. Our findings reveal that EL is primarily driven by the
expert activation ratio and the total compute budget, both following
predictable power laws, while expert granularity acts as a non-linear modulator
with a clear optimal range. We integrate these discoveries into a unified
scaling law that accurately predicts the EL of an MoE architecture based on its
configuration. To validate our derived scaling laws, we designed and trained
Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active
parameters, alongside a 6.1B dense model for comparison. When trained on an
identical 1T high-quality token dataset, Ling-mini-beta matched the performance
of the 6.1B dense model while consuming over 7x fewer computational resources,
thereby confirming the accuracy of our scaling laws. This work provides a
principled and empirically-grounded foundation for the scaling of efficient MoE
models.

</details>


### [28] [TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa](https://arxiv.org/abs/2507.17709)
*Parker Riley,Siamak Shakeri,Waleed Ammar,Jonathan H. Clark*

Main category: cs.CL

TL;DR: 本文介绍了TyDi QA-WANA数据集，这是一个涵盖西亚和北非10种语言变体的问答数据集，包含28K个样本，旨在评估模型在大文本语境下的问答能力


<details>
  <summary>Details</summary>
Motivation: 现有问答数据集缺乏对西亚和北非语言的覆盖，且多数通过翻译生成，存在文化相关性问题。需要构建直接用各语言变体收集的、包含大文本语境的问答数据集来评估模型的跨语言理解能力

Method: 设计数据收集流程来引发信息寻求型问题，确保提问者真正想知道答案；每个问题配对一篇完整文章（可能包含或不包含答案）；直接用各语言变体收集数据而非翻译，涵盖西亚和北非的10种语言变体，构建包含28K样本的数据集

Result: 成功构建了TyDi QA-WANA数据集，包含28K个问答样本，覆盖10种西亚和北非语言变体。文章篇幅较大，适合评估模型利用大文本语境的能力。提供了两个基线模型的性能表现

Conclusion: TyDi QA-WANA数据集为西亚和北非语言的问答研究提供了重要资源，避免了翻译带来的文化相关性问题，能够有效评估模型在大文本语境下的问答能力。研究团队公开了代码和数据，促进社区进一步改进

Abstract: We present TyDi QA-WANA, a question-answering dataset consisting of 28K
examples divided among 10 language varieties of western Asia and northern
Africa. The data collection process was designed to elicit information-seeking
questions, where the asker is genuinely curious to know the answer. Each
question in paired with an entire article that may or may not contain the
answer; the relatively large size of the articles results in a task suitable
for evaluating models' abilities to utilize large text contexts in answering
questions. Furthermore, the data was collected directly in each language
variety, without the use of translation, in order to avoid issues of cultural
relevance. We present performance of two baseline models, and release our code
and data to facilitate further improvement by the research community.

</details>


### [29] [From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes](https://arxiv.org/abs/2507.17717)
*Karen Zhou,John Giorgi,Pranav Mani,Peng Xu,Davis Liang,Chenhao Tan*

Main category: cs.CL

TL;DR: 本文提出了一种系统化的管道，将真实用户反馈转化为结构化清单，用于评估AI生成的临床笔记质量，该方法在覆盖率、多样性和人类评分预测能力方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: AI生成的临床笔记在医疗保健中使用日益增加，但由于专家评审的主观性强和可扩展性有限，评估其质量仍然是一个挑战。现有的自动化指标往往无法与现实世界中医生的偏好保持一致。

Method: 提出了一个系统化管道，将真实用户反馈蒸馏为结构化的笔记评估清单。这些清单设计为可解释的、基于人类反馈的，并可由基于LLM的评估器执行。使用来自已部署AI医疗记录系统的超过21,000次临床遭遇的去标识化数据进行研究。

Result: 反馈衍生的清单在离线评估中的覆盖率、多样性和人类评分预测能力方面优于基线方法。广泛的实验证实了清单对质量降级扰动的鲁棒性，与临床医生偏好的显著一致性，以及作为评估方法的实用价值。在离线研究环境中，该清单可以帮助识别可能低于选定质量阈值的笔记。

Conclusion: 提出的基于反馈的结构化清单方法为AI生成临床笔记的质量评估提供了一个有效、可解释且与临床医生偏好高度一致的解决方案，具有良好的鲁棒性和实用价值。

Abstract: AI-generated clinical notes are increasingly used in healthcare, but
evaluating their quality remains a challenge due to high subjectivity and
limited scalability of expert review. Existing automated metrics often fail to
align with real-world physician preferences. To address this, we propose a
pipeline that systematically distills real user feedback into structured
checklists for note evaluation. These checklists are designed to be
interpretable, grounded in human feedback, and enforceable by LLM-based
evaluators. Using deidentified data from over 21,000 clinical encounters,
prepared in accordance with the HIPAA safe harbor standard, from a deployed AI
medical scribe system, we show that our feedback-derived checklist outperforms
baseline approaches in our offline evaluations in coverage, diversity, and
predictive power for human ratings. Extensive experiments confirm the
checklist's robustness to quality-degrading perturbations, significant
alignment with clinician preferences, and practical value as an evaluation
methodology. In offline research settings, the checklist can help identify
notes likely to fall below our chosen quality thresholds.

</details>


### [30] [Megrez2 Technical Report](https://arxiv.org/abs/2507.17728)
*Boxun Li,Yadong Li,Zhiyuan Li,Congyi Liu,Weilin Liu,Guowei Niu,Zheyue Tan,Haiyang Xu,Zhuyu Yao,Tao Yuan,Dong Zhou,Yueqing Zhuang,Bo Zhao,Guohao Dai,Yu Wang*

Main category: cs.CL

TL;DR: Megrez2是一个轻量级高性能语言模型架构，通过跨层专家共享和预门控路由机制，仅用3B激活参数和7.5B存储参数就达到了与更大模型相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限的设备上部署高性能语言模型，需要开发一种能够在保持模型能力的同时显著减少参数数量的轻量级架构，以实现准确性、效率和可部署性之间的平衡。

Method: 提出了Megrez2架构，引入了两个关键技术：1）跨层专家共享机制，通过在相邻transformer层之间重用专家模块来减少总参数数量；2）预门控路由技术，实现内存高效的专家加载和更快的推理速度。模型在5万亿token语料上预训练，并通过监督微调和可验证奖励强化学习进一步增强。

Result: Megrez2-Preview模型仅使用3B激活参数和7.5B存储参数，在语言理解、指令遵循、数学推理和代码生成等多项任务上展现出与更大模型相当或更优的性能表现。

Conclusion: Megrez2架构成功实现了准确性、效率和可部署性之间的平衡，证明了通过创新的架构设计可以用更少的参数达到更好的性能，为在真实世界资源受限环境中的应用提供了强有力的候选方案。

Abstract: We present Megrez2, a novel lightweight and high-performance language model
architecture optimized for device native deployment. Megrez2 introduces a novel
cross-layer expert sharing mechanism, which significantly reduces total
parameter count by reusing expert modules across adjacent transformer layers
while maintaining most of the model's capacity. It also incorporates pre-gated
routing, enabling memory-efficient expert loading and faster inference. As the
first instantiation of the Megrez2 architecture, we introduce the
Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and
further enhanced through supervised fine-tuning and reinforcement learning with
verifiable rewards. With only 3B activated and 7.5B stored parameters,
Megrez2-Preview demonstrates competitive or superior performance compared to
larger models on a wide range of tasks, including language understanding,
instruction following, mathematical reasoning, and code generation. These
results highlight the effectiveness of the Megrez2 architecture to achieve a
balance between accuracy, efficiency, and deployability, making it a strong
candidate for real-world, resource-constrained applications.

</details>


### [31] [Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks](https://arxiv.org/abs/2507.17747)
*Linbo Cao,Jinman Zhao*

Main category: cs.CL

TL;DR: 提出了一种基于辩论的评估范式，将现有QA数据集转换为结构化对抗辩论，通过多轮论证增加难度并减少记忆化问题，为评估先进语言模型的真实推理能力提供可持续路径。


<details>
  <summary>Details</summary>
Motivation: 随着前沿语言模型在标准QA基准测试中逐渐饱和，数据污染、记忆化和数据集创建成本不断上升的问题日益突出，需要一种新的评估方法来真实测量模型的推理能力而非记忆能力。

Method: 设计了一个辩论驱动的评估范式：将任何现有QA数据集转换为结构化对抗辩论，其中一个模型为官方答案辩护，另一个模型构建并为替代答案辩护，由对正确答案盲目的裁判模型进行评判。通过强制多轮论证来增加难度并惩罚浅层记忆化。

Result: 在MMLU-Pro问题子集上的实验验证了方法的有效性：针对测试问题微调的Llama 3.1模型虽然准确率从50%提升到82%，但在辩论中表现更差，证明了该方法对数据污染的抗性。即使较弱的裁判也能可靠地区分更强的辩论者。

Conclusion: 该框架表明"在测试集上预训练不再是万能的"，为测量先进语言模型的真实推理能力提供了可持续的路径，能够以创建新基准测试的一小部分成本扩展到未来更强大的系统。

Abstract: As frontier language models increasingly saturate standard QA benchmarks,
concerns about data contamination, memorization, and escalating dataset
creation costs persist. We propose a debate-driven evaluation paradigm that
transforms any existing QA dataset into structured adversarial debates--where
one model is given the official answer to defend, and another constructs and
defends an alternative answer--adjudicated by a judge model blind to the
correct solution. By forcing multi-round argumentation, this approach
substantially increases difficulty while penalizing shallow memorization, yet
reuses QA items to reduce curation overhead. We make two main contributions:
(1) an evaluation pipeline to systematically convert QA tasks into debate-based
assessments, and (2) a public benchmark that demonstrates our paradigm's
effectiveness on a subset of MMLU-Pro questions, complete with standardized
protocols and reference models. Empirical results validate the robustness of
the method and its effectiveness against data contamination--a Llama 3.1 model
fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)
but performed worse in debates. Results also show that even weaker judges can
reliably differentiate stronger debaters, highlighting how debate-based
evaluation can scale to future, more capable systems while maintaining a
fraction of the cost of creating new benchmarks. Overall, our framework
underscores that "pretraining on the test set is no longer all you need,"
offering a sustainable path for measuring the genuine reasoning ability of
advanced language models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Evaluating Artificial Intelligence Algorithms for the Standardization of Transtibial Prosthetic Socket Shape Design](https://arxiv.org/abs/2507.16818)
*C. H. E. Jordaan,M. van der Stelt,T. J. J. Maal,V. M. A. Stirler,R. Leijendekkers,T. Kachman,G. A. de Jong*

Main category: cs.LG

TL;DR: 该研究开发了多种人工智能方法来标准化经胫假肢接受腔设计，通过比较三种算法（3D神经网络、前馈神经网络和随机森林）发现，预测假肢师适应性调整比直接预测最终接受腔形状效果更好，其中随机森林模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 经胫假肢接受腔的质量依赖于假肢师的技能和专业知识，因为配制过程是手动完成的。为了帮助标准化经胫假肢接受腔设计并减少对个人技能的依赖，研究者希望通过人工智能方法来改进这一过程。

Method: 收集了118名患者的数据，包括残肢的3D扫描和对应的假肢师设计的3D接受腔模型。采用多种数据预处理步骤进行对齐、标准化，并可选择使用形态模型和主成分分析进行压缩。开发了三种不同算法（3D神经网络、前馈神经网络和随机森林）来预测最终接受腔形状或假肢师的适应性调整。

Result: 所有算法中，估计所需适应性调整的表现都优于直接预测最终接受腔形状。随机森林模型在适应性预测方面产生最低误差，表面到表面距离的中位数为1.24毫米，第一四分位数为1.03毫米，第三四分位数为1.54毫米。

Conclusion: 研究表明人工智能方法可以有效辅助经胫假肢接受腔设计的标准化。通过预测假肢师的适应性调整而非直接预测最终形状能够获得更好的效果，随机森林模型在这方面表现最优，为假肢制作领域的AI应用提供了有前景的解决方案。

Abstract: The quality of a transtibial prosthetic socket depends on the prosthetist's
skills and expertise, as the fitting is performed manually. This study
investigates multiple artificial intelligence (AI) approaches to help
standardize transtibial prosthetic socket design. Data from 118 patients were
collected by prosthetists working in the Dutch healthcare system. This data
consists of a three-dimensional (3D) scan of the residual limb and a
corresponding 3D model of the prosthetist-designed socket. Multiple data
pre-processing steps are performed for alignment, standardization and
optionally compression using Morphable Models and Principal Component Analysis.
Afterward, three different algorithms - a 3D neural network, Feedforward neural
network, and random forest - are developed to either predict 1) the final
socket shape or 2) the adaptations performed by a prosthetist to predict the
socket shape based on the 3D scan of the residual limb. Each algorithm's
performance was evaluated by comparing the prosthetist-designed socket with the
AI-generated socket, using two metrics in combination with the error location.
First, we measure the surface-to-surface distance to assess the overall surface
error between the AI-generated socket and the prosthetist-designed socket.
Second, distance maps between the AI-generated and prosthetist sockets are
utilized to analyze the error's location. For all algorithms, estimating the
required adaptations outperformed direct prediction of the final socket shape.
The random forest model applied to adaptation prediction yields the lowest
error with a median surface-to-surface distance of 1.24 millimeters, a first
quartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.

</details>


### [33] [Exploring the Frontiers of kNN Noisy Feature Detection and Recovery for Self-Driving Labs](https://arxiv.org/abs/2507.16833)
*Qiuyu Shi,Kangming Li,Yao Fehlis,Daniel Persaud,Robert Black,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: 这项研究开发了一个自动化工作流程，用于检测和纠正自驱动实验室中的噪声特征，以提高材料发现的数据质量和实验精度。


<details>
  <summary>Details</summary>
Motivation: 自驱动实验室在材料发现中展现出加速潜力，但输入参数捕获错误可能破坏用于建模系统性能的特征，从而影响当前和未来的实验活动。需要一个系统性的方法来检测和纠正噪声特征。

Method: 开发了一个自动化工作流程，系统性地检测噪声特征，确定可以纠正的样本-特征配对，并最终恢复正确的特征值。进行系统性研究，考察数据集大小、噪声强度和特征值分布如何影响噪声特征的可检测性和可恢复性。

Result: 高强度噪声和大型训练数据集有利于噪声特征的检测和纠正。低强度噪声会降低检测和恢复能力，但可以通过更大的清洁训练数据集来补偿。连续和分散特征分布相比离散或窄分布特征显示出更好的可恢复性。该研究还为材料数据集中的kNN插补提供了切实的基准。

Conclusion: 该研究展示了一个与模型无关的框架，用于在存在噪声、有限数据和不同特征分布的情况下进行合理的数据恢复，最终旨在提高自动化材料发现中的数据质量和实验精度。

Abstract: Self-driving laboratories (SDLs) have shown promise to accelerate materials
discovery by integrating machine learning with automated experimental
platforms. However, errors in the capture of input parameters may corrupt the
features used to model system performance, compromising current and future
campaigns. This study develops an automated workflow to systematically detect
noisy features, determine sample-feature pairings that can be corrected, and
finally recover the correct feature values. A systematic study is then
performed to examine how dataset size, noise intensity, and feature value
distribution affect both the detectability and recoverability of noisy
features. In general, high-intensity noise and large training datasets are
conducive to the detection and correction of noisy features. Low-intensity
noise reduces detection and recovery but can be compensated for by larger clean
training data sets. Detection and correction results vary between features with
continuous and dispersed feature distributions showing greater recoverability
compared to features with discrete or narrow distributions. This systematic
study not only demonstrates a model agnostic framework for rational data
recovery in the presence of noise, limited data, and differing feature
distributions but also provides a tangible benchmark of kNN imputation in
materials data sets. Ultimately, it aims to enhance data quality and
experimental precision in automated materials discovery.

</details>


### [34] [TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning](https://arxiv.org/abs/2507.16844)
*Jie He,Vincent Theo Willem Kenbeek,Zhantao Yang,Meixun Qu,Ezio Bartocci,Dejan Ničković,Radu Grosu*

Main category: cs.LG

TL;DR: 本文介绍了TD-Interpreter，一个帮助工程师理解复杂时序图的机器学习工具，通过微调LLaVA多模态大语言模型实现视觉问答功能，并在基准测试中大幅超越了未调优的GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 工程师在设计和验证过程中需要理解来自第三方的复杂时序图，但缺乏有效的工具来辅助分析和查询这些时序图，因此需要开发一个专门的ML工具来解决这个问题。

Method: 通过微调LLaVA（一个轻量级7B参数的多模态大语言模型）实现多模态学习，构建视觉问答环境；为解决训练数据不足问题，开发了合成数据生成工作流，将视觉信息与文本解释对齐。

Result: 实验评估表明TD-Interpreter在评估基准上大幅超越了未调优的GPT-4o，证明了该工具的有效性和实用性。

Conclusion: TD-Interpreter成功地为工程师提供了一个有效的时序图理解工具，通过多模态学习和合成数据生成技术，实现了优于现有大模型的性能表现。

Abstract: We introduce TD-Interpreter, a specialized ML tool that assists engineers in
understanding complex timing diagrams (TDs), originating from a third party,
during their design and verification process. TD-Interpreter is a visual
question-answer environment which allows engineers to input a set of TDs and
ask design and verification queries regarding these TDs. We implemented
TD-Interpreter with multimodal learning by fine-tuning LLaVA, a lightweight 7B
Multimodal Large Language Model (MLLM). To address limited training data
availability, we developed a synthetic data generation workflow that aligns
visual information with its textual interpretation. Our experimental evaluation
demonstrates the usefulness of TD-Interpreter which outperformed untuned GPT-4o
by a large margin on the evaluated benchmarks.

</details>


### [35] [Reinforcement Learning in hyperbolic space for multi-step reasoning](https://arxiv.org/abs/2507.16864)
*Tao Xu,Dung-Yang Lee,Momiao Xiong*

Main category: cs.LG

TL;DR: 本文提出了一种将双曲变换器集成到强化学习中的新框架，用于解决多步推理问题。该方法利用双曲嵌入有效建模层次结构，在FrontierMath和非线性最优控制基准上相比传统变换器RL显著提升了准确性（32%-45%）并减少了计算时间（16%-32%）。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在处理复杂推理任务时面临信用分配、高维状态表示和稳定性等挑战。多步推理是人工智能的基础挑战，广泛应用于数学问题求解和动态环境决策。需要新的解决方案来克服这些局限性。

Method: 提出了一个将双曲变换器集成到强化学习中的新框架。该方法利用双曲嵌入来有效建模层次结构，结合了变换器架构和双曲几何的最新进展。文章提供了理论见解、算法细节和实验结果。

Result: 在FrontierMath基准上，相比使用普通变换器的强化学习，准确率提升32%-44%，计算时间减少16%-32%。在非线性最优控制基准上，准确率提升43%-45%，计算时间减少16%-17%。

Conclusion: 双曲变换器在强化学习中具有巨大潜力，特别适用于涉及层次结构的多步推理任务。该方法成功解决了传统RL在复杂推理任务中的局限性，为人工智能中的多步推理问题提供了有效的解决方案。

Abstract: Multi-step reasoning is a fundamental challenge in artificial intelligence,
with applications ranging from mathematical problem-solving to decision-making
in dynamic environments. Reinforcement Learning (RL) has shown promise in
enabling agents to perform multi-step reasoning by optimizing long-term
rewards. However, conventional RL methods struggle with complex reasoning tasks
due to issues such as credit assignment, high-dimensional state
representations, and stability concerns. Recent advancements in Transformer
architectures and hyperbolic geometry have provided novel solutions to these
challenges. This paper introduces a new framework that integrates hyperbolic
Transformers into RL for multi-step reasoning. The proposed approach leverages
hyperbolic embeddings to model hierarchical structures effectively. We present
theoretical insights, algorithmic details, and experimental results that
include Frontier Math and nonlinear optimal control problems. Compared to RL
with vanilla transformer, the hyperbolic RL largely improves accuracy by
(32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control
benchmark, while achieving impressive reduction in computational time by
(16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control
benchmark. Our work demonstrates the potential of hyperbolic Transformers in
reinforcement learning, particularly for multi-step reasoning tasks that
involve hierarchical structures.

</details>


### [36] [Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization](https://arxiv.org/abs/2507.16867)
*Yunyi Zhao,Wei Zhang,Cheng Xiang,Hongyang Du,Dusit Niyato,Shuhua Gao*

Main category: cs.LG

TL;DR: 本文提出了DiffCarl算法，将扩散模型与深度强化学习结合，用于多微电网系统的智能运行，在不确定性环境下实现碳感知和风险感知的能源调度优化。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源集成度不断提高和系统复杂性增加，微电网社区在不确定性条件下面临实时能源调度和优化的重大挑战，需要一种能够同时考虑碳排放和运营风险的智能调度方法。

Method: 将扩散模型集成到深度强化学习框架中，通过去噪生成过程学习动作分布，增强DRL策略的表达能力，在动态不确定的微电网环境中实现碳感知和风险感知的自适应能源调度。

Result: 大量实验研究表明，DiffCarl算法相比经典算法和最先进的DRL解决方案，运营成本降低2.3-30.1%，碳排放比非碳感知变体减少28.7%，并显著降低了性能变异性。

Conclusion: DiffCarl是一个实用且前瞻性的解决方案，其灵活的设计允许高效适应不同的系统配置和目标，支持在不断发展的能源系统中的实际部署。

Abstract: This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware
reinforcement learning algorithm for intelligent operation of multi-microgrid
systems. With the growing integration of renewables and increasing system
complexity, microgrid communities face significant challenges in real-time
energy scheduling and optimization under uncertainty. DiffCarl integrates a
diffusion model into a deep reinforcement learning (DRL) framework to enable
adaptive energy scheduling under uncertainty and explicitly account for carbon
emissions and operational risk. By learning action distributions through a
denoising generation process, DiffCarl enhances DRL policy expressiveness and
enables carbon- and risk-aware scheduling in dynamic and uncertain microgrid
environments. Extensive experimental studies demonstrate that it outperforms
classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower
operational cost. It also achieves 28.7% lower carbon emissions than those of
its carbon-unaware variant and reduces performance variability. These results
highlight DiffCarl as a practical and forward-looking solution. Its flexible
design allows efficient adaptation to different system configurations and
objectives to support real-world deployment in evolving energy systems.

</details>


### [37] [Navigation through Non-Compact Symmetric Spaces: a mathematical perspective on Cartan Neural Networks](https://arxiv.org/abs/2507.16871)
*Pietro Giuseppe Fré,Federico Milanesio,Guido Sanguinetti,Matteo Santoro*

Main category: cs.LG

TL;DR: 本文提出了基于非紧对称空间U/H的Cartan神经网络理论，旨在建立几何一致性的神经网络数学框架，为几何可解释的神经网络理论奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络缺乏几何一致性和可解释性，需要利用齐次流形和群论结构来构建更具几何意义的神经网络理论框架。

Method: 利用非紧对称空间U/H作为齐次流形，构建Cartan神经网络，详细分析网络层的几何性质以及层间映射如何与这些结构相互作用，使网络具备协变性和几何可解释性。

Result: 成功展示了Cartan神经网络的数学结构基础，证明了这种几何概念在机器学习中的可行性和性能表现。

Conclusion: 该工作与姊妹论文共同构成了利用群论结构发展完全几何可解释神经网络理论的重要第一步，为神经网络的几何化发展提供了新的理论方向。

Abstract: Recent work has identified non-compact symmetric spaces U/H as a promising
class of homogeneous manifolds to develop a geometrically consistent theory of
neural networks. An initial implementation of these concepts has been presented
in a twin paper under the moniker of Cartan Neural Networks, showing both the
feasibility and the performance of these geometric concepts in a machine
learning context. The current paper expands on the mathematical structures
underpinning Cartan Neural Networks, detailing the geometric properties of the
layers and how the maps between layers interact with such structures to make
Cartan Neural Networks covariant and geometrically interpretable. Together,
these twin papers constitute a first step towards a fully geometrically
interpretable theory of neural networks exploiting group-theoretic structures

</details>


### [38] [Confidence Optimization for Probabilistic Encoding](https://arxiv.org/abs/2507.16881)
*Pengjiu Xia,Yidian Huang,Wenchao Wei,Yuwen Tan*

Main category: cs.LG

TL;DR: 提出了一种置信度优化概率编码(CPE)方法，通过置信度感知机制和L2正则化改进概率编码中的距离度量问题，在BERT和RoBERTa模型上显著提升了自然语言分类任务的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 概率编码虽然能够实现从确定性到不确定性状态的平滑过渡并增强泛化能力，但高斯噪声的随机性会扭曲分类任务中基于点的距离度量，影响分类效果。现有的KL散度方差正则化依赖于不可靠的先验假设。

Method: 提出置信度优化概率编码(CPE)方法，包含两个关键策略：1）引入置信度感知机制来调整距离计算，确保概率编码分类任务中的一致性和可靠性；2）用更简单的L2正则化项替换传统的基于KL散度的方差正则化，直接约束方差。该方法是模型无关的。

Result: 在自然语言分类任务上进行了广泛实验，结果表明该方法在BERT和RoBERTa模型上都显著提升了性能和泛化能力。

Conclusion: CPE方法成功解决了概率编码中距离度量的可靠性问题，通过置信度感知机制和L2正则化有效改进了表示学习，在多个模型上都取得了显著的性能提升，证明了方法的有效性和通用性。

Abstract: Probabilistic encoding introduces Gaussian noise into neural networks,
enabling a smooth transition from deterministic to uncertain states and
enhancing generalization ability. However, the randomness of Gaussian noise
distorts point-based distance measurements in classification tasks. To mitigate
this issue, we propose a confidence optimization probabilistic encoding (CPE)
method that improves distance reliability and enhances representation learning.
Specifically, we refine probabilistic encoding with two key strategies: First,
we introduce a confidence-aware mechanism to adjust distance calculations,
ensuring consistency and reliability in probabilistic encoding classification
tasks. Second, we replace the conventional KL divergence-based variance
regularization, which relies on unreliable prior assumptions, with a simpler L2
regularization term to directly constrain variance. The method we proposed is
model-agnostic, and extensive experiments on natural language classification
tasks demonstrate that our method significantly improves performance and
generalization on both the BERT and the RoBERTa model.

</details>


### [39] [SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling](https://arxiv.org/abs/2507.16884)
*Yi Guo,Wei Wang,Zhihang Yuan,Rong Cao,Kuan Chen,Zhengyang Chen,Yuanyuan Huo,Yang Zhang,Yuping Wang,Shouda Liu,Yuxuan Wang*

Main category: cs.LG

TL;DR: 本文提出SplitMeanFlow方法，通过纯代数的区间分割一致性原理来学习平均速度场，相比基于微分算子的MeanFlow方法更加高效稳定，在大规模语音合成中实现20倍加速。


<details>
  <summary>Details</summary>
Motivation: Flow Matching等生成模型虽然性能优秀，但迭代采样过程计算开销昂贵。现有的少步或单步生成方法如MeanFlow依赖微分恒等式来学习平均速度场，但这种微分公式存在局限性，需要计算雅可比向量积(JVP)，导致实现复杂、训练不稳定且硬件兼容性差。

Method: 基于定积分的可加性原理，推导出新的纯代数恒等式——区间分割一致性(Interval Splitting Consistency)。该恒等式为不同时间区间的平均速度场建立自引用关系，无需微分算子。基于此原理构建SplitMeanFlow训练框架，直接将代数一致性作为学习目标进行强化。

Result: 理论上证明了当区间分割趋于无穷小时，SplitMeanFlow的代数一致性可以恢复MeanFlow的微分恒等式，建立了更一般的基础。实践中，代数方法消除了JVP计算需求，实现更简单、训练更稳定、硬件兼容性更好。单步和两步SplitMeanFlow模型已成功部署在大规模语音合成产品(如豆包)中。

Conclusion: SplitMeanFlow为学习平均速度场提供了更直接和通用的基础，在保持理论严谨性的同时显著提升了实用性，实现了20倍的加速效果，为生成模型的高效部署提供了新的解决方案。

Abstract: Generative models like Flow Matching have achieved state-of-the-art
performance but are often hindered by a computationally expensive iterative
sampling process. To address this, recent work has focused on few-step or
one-step generation by learning the average velocity field, which directly maps
noise to data. MeanFlow, a leading method in this area, learns this field by
enforcing a differential identity that connects the average and instantaneous
velocities. In this work, we argue that this differential formulation is a
limiting special case of a more fundamental principle. We return to the first
principles of average velocity and leverage the additivity property of definite
integrals. This leads us to derive a novel, purely algebraic identity we term
Interval Splitting Consistency. This identity establishes a self-referential
relationship for the average velocity field across different time intervals
without resorting to any differential operators. Based on this principle, we
introduce SplitMeanFlow, a new training framework that enforces this algebraic
consistency directly as a learning objective. We formally prove that the
differential identity at the core of MeanFlow is recovered by taking the limit
of our algebraic consistency as the interval split becomes infinitesimal. This
establishes SplitMeanFlow as a direct and more general foundation for learning
average velocity fields. From a practical standpoint, our algebraic approach is
significantly more efficient, as it eliminates the need for JVP computations,
resulting in simpler implementation, more stable training, and broader hardware
compatibility. One-step and two-step SplitMeanFlow models have been
successfully deployed in large-scale speech synthesis products (such as
Doubao), achieving speedups of 20x.

</details>


### [40] [SiLQ: Simple Large Language Model Quantization-Aware Training](https://arxiv.org/abs/2507.16933)
*Steven K. Esser,Jeffrey L. McKinstry,Deepika Bablani,Rathinakumar Appuswamy,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 提出了一种简单的端到端量化感知训练方法，仅增加不到0.1%的训练预算就能在多个现代基准测试中大幅超越现有量化方法，适用于不同模型架构且与推理加速器兼容。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要量化来减少推理延迟、模型大小和能耗，从而以更低成本提供更好的用户体验。现有挑战是在合理时间内以最小精度损失交付量化模型，特别是不需要与专用推理加速器不兼容的机制。

Method: 采用简单的端到端量化感知训练方法，该方法易于跨不同模型架构泛化，可应用于激活、缓存和权重，除了量化本身外不需要向模型引入任何额外操作。

Result: 在多个现代基准测试中大幅超越领先的已发表量化方法，适用于基础模型和指令模型变体，总模型训练预算增加不到0.1%。

Conclusion: 证明了一种高效的量化感知训练方法，能够在极小的训练成本增加下显著提升量化模型性能，具有良好的泛化性和实用性，与推理加速器兼容。

Abstract: Large language models can be quantized to reduce inference time latency,
model size, and energy consumption, thereby delivering a better user experience
at lower cost. A challenge exists to deliver quantized models with minimal loss
of accuracy in reasonable time, and in particular to do so without requiring
mechanisms incompatible with specialized inference accelerators. Here, we
demonstrate a simple, end-to-end quantization-aware training approach that,
with an increase in total model training budget of less than 0.1%, outperforms
the leading published quantization methods by large margins on several modern
benchmarks, with both base and instruct model variants. The approach easily
generalizes across different model architectures, can be applied to
activations, cache, and weights, and requires the introduction of no additional
operations to the model other than the quantization itself.

</details>


### [41] [Hierarchical Reinforcement Learning Framework for Adaptive Walking Control Using General Value Functions of Lower-Limb Sensor Signals](https://arxiv.org/abs/2507.16983)
*Sonny T. Jones,Grange M. Simpson,Patrick M. Pilarski,Ashley N. Dalrymple*

Main category: cs.LG

TL;DR: 研究使用分层强化学习开发下肢外骨骼的自适应控制策略，通过生物感知运动处理模型启发，结合通用价值函数的预测信息来提高外骨骼在不同地形上的决策能力和行走性能。


<details>
  <summary>Details</summary>
Motivation: 康复技术为研究人机协作学习和决策提供了自然环境。针对运动障碍个体，需要开发能够增强移动性和自主性的下肢外骨骼自适应控制策略，以帮助他们在不同地形环境中安全行走。

Method: 采用分层强化学习方法，受生物感知运动处理模型启发，将复杂的外骨骼控制适应任务分解为：1）高层地形策略适应框架；2）低层预测信息提供框架，通过持续学习通用价值函数(GVFs)实现。GVFs从多种可穿戴下肢传感器（肌电图、压力鞋垫、角度计）生成未来信号值的时间抽象，并研究两种方法将实际和预测传感器信号整合到策略网络中。

Result: 添加GVFs预测信息显著提高了网络整体准确性。在平地、不平地面、上下坡道和转弯等地形上都观察到性能提升，这些地形通常在没有预测信息时容易被误分类。预测信息在不确定性情况下（如高误分类概率的地形）能够辅助决策制定。

Conclusion: 这项工作为分层强化学习的细节研究提供了新见解，并为未来外骨骼开发贡献了重要成果，有助于实现在不同行走环境中的安全过渡和穿越，推动了康复技术中人机协作学习的发展。

Abstract: Rehabilitation technology is a natural setting to study the shared learning
and decision-making of human and machine agents. In this work, we explore the
use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control
strategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy
for individuals with motor impairments. Inspired by prominent models of
biological sensorimotor processing, our investigated HRL approach breaks down
the complex task of exoskeleton control adaptation into a higher-level
framework for terrain strategy adaptation and a lower-level framework for
providing predictive information; this latter element is implemented via the
continual learning of general value functions (GVFs). GVFs generated temporal
abstractions of future signal values from multiple wearable lower-limb sensors,
including electromyography, pressure insoles, and goniometers. We investigated
two methods for incorporating actual and predicted sensor signals into a policy
network with the intent to improve the decision-making capacity of the control
system of a lower-limb exoskeleton during ambulation across varied terrains. As
a key result, we found that the addition of predictions made from GVFs
increased overall network accuracy. Terrain-specific performance increases were
seen while walking on even ground, uneven ground, up and down ramps, and turns,
terrains that are often misclassified without predictive information. This
suggests that predictive information can aid decision-making during
uncertainty, e.g., on terrains that have a high chance of being misclassified.
This work, therefore, contributes new insights into the nuances of HRL and the
future development of exoskeletons to facilitate safe transitioning and
traversing across different walking environments.

</details>


### [42] [PyG 2.0: Scalable Learning on Real World Graphs](https://arxiv.org/abs/2507.16991)
*Matthias Fey,Jinu Sunil,Akihiro Nitta,Rishi Puri,Manan Shah,Blaž Stojanovič,Ramona Bendias,Alexandria Barghi,Vid Kocijan,Zecheng Zhang,Xinwei He,Jan Eric Lenssen,Jure Leskovec*

Main category: cs.LG

TL;DR: PyG 2.0是PyTorch Geometric框架的重大更新，显著提升了图神经网络的可扩展性和实际应用能力，支持异构图、时序图等复杂场景，并在关系深度学习和大语言模型等重要领域得到广泛应用。


<details>
  <summary>Details</summary>
Motivation: 随着图神经网络应用的不断扩展，需要一个更强大、更可扩展的框架来处理大规模图学习问题，特别是在异构图、时序图以及实际应用场景中的复杂需求。

Method: 发布PyG 2.0版本，引入增强的架构设计，包括：支持异构图和时序图、可扩展的特征/图存储系统、各种性能优化技术，以及针对关系深度学习和大语言模型的专门支持。

Result: PyG 2.0在可扩展性和实际应用能力方面实现了显著改进，能够高效处理大规模图学习问题，并在多个应用领域得到成功应用，特别是在关系深度学习和大语言模型领域表现突出。

Conclusion: PyG 2.0成功建立了其作为图神经网络领先框架的地位，通过架构增强和功能扩展，为研究者和实践者提供了处理复杂图学习任务的强大工具，推动了图学习在各个应用领域的发展。

Abstract: PyG (PyTorch Geometric) has evolved significantly since its initial release,
establishing itself as a leading framework for Graph Neural Networks. In this
paper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive
update that introduces substantial improvements in scalability and real-world
application capabilities. We detail the framework's enhanced architecture,
including support for heterogeneous and temporal graphs, scalable feature/graph
stores, and various optimizations, enabling researchers and practitioners to
tackle large-scale graph learning problems efficiently. Over the recent years,
PyG has been supporting graph learning in a large variety of application areas,
which we will summarize, while providing a deep dive into the important areas
of relational deep learning and large language modeling.

</details>


### [43] [Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation](https://arxiv.org/abs/2507.17001)
*Yan Li,Guangyi Chen,Yunlong Deng,Zijian Li,Zeyu Tang,Anpeng Wu,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一个新颖的框架，通过战略性地利用偏差来补充不变表示学习，在领域泛化任务中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的分布外（OOD）领域适应方法主要依赖不变表示学习来消除偏差特征的影响，但偏差是否总是应该被消除？何时应该保留偏差，以及如何有效利用偏差？这些问题亟待解决。

Method: 提出了一个战略性利用偏差的新框架，包含两个关键组件：(1)使用不变性作为指导从偏差中提取预测性成分；(2)利用识别出的偏差来估计环境条件，然后使用偏差感知预测器来缓解环境差异。

Result: 在合成数据集和标准领域泛化基准测试上的实验结果一致表明，该方法优于现有方法，展现了其鲁棒性和适应性。

Conclusion: 通过理论分析确定了偏差特征可以被识别和有效利用的条件，并基于此提出的框架能够在推理过程中战略性地利用偏差来补充不变表示，从而在领域泛化任务中取得更好的性能。

Abstract: Most existing methods for adapting models to out-of-distribution (OOD)
domains rely on invariant representation learning to eliminate the influence of
biased features. However, should bias always be eliminated -- and if not, when
should it be retained, and how can it be leveraged? To address these questions,
we first present a theoretical analysis that explores the conditions under
which biased features can be identified and effectively utilized. Building on
this theoretical foundation, we introduce a novel framework that strategically
leverages bias to complement invariant representations during inference. The
framework comprises two key components that leverage bias in both direct and
indirect ways: (1) using invariance as guidance to extract predictive
ingredients from bias, and (2) exploiting identified bias to estimate the
environmental condition and then use it to explore appropriate bias-aware
predictors to alleviate environment gaps. We validate our approach through
experiments on both synthetic datasets and standard domain generalization
benchmarks. Results consistently demonstrate that our method outperforms
existing approaches, underscoring its robustness and adaptability.

</details>


### [44] [laplax -- Laplace Approximations with JAX](https://arxiv.org/abs/2507.17013)
*Tobias Weber,Bálint Mucsányi,Lenard Rommel,Thomas Christie,Lars Kasüschke,Marvin Pförtner,Philipp Hennig*

Main category: cs.LG

TL;DR: 本文介绍了laplax，一个基于JAX的开源Python包，用于在深度神经网络中执行拉普拉斯近似，以实现权重空间不确定性量化和贝叶斯推理。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络需要高效可扩展的方法来量化权重空间的不确定性，以便应用贝叶斯工具进行预测不确定性估计和模型选择，但缺乏灵活易用的实现框架。

Method: 开发了laplax这一开源Python包，采用模块化和纯函数式架构设计，基于JAX框架实现拉普拉斯近似，具有最小外部依赖和研究友好的接口。

Result: 成功构建了一个灵活的拉普拉斯近似框架，支持快速原型开发和实验，为贝叶斯神经网络研究提供了便利的工具。

Conclusion: laplax为深度学习中的不确定性量化、贝叶斯神经网络研究以及拉普拉斯近似技术的改进提供了有效的工具支持，促进了相关领域的研究发展。

Abstract: The Laplace approximation provides a scalable and efficient means of
quantifying weight-space uncertainty in deep neural networks, enabling the
application of Bayesian tools such as predictive uncertainty and model
selection via Occam's razor. In this work, we introduce laplax, a new
open-source Python package for performing Laplace approximations with jax.
Designed with a modular and purely functional architecture and minimal external
dependencies, laplax offers a flexible and researcher-friendly framework for
rapid prototyping and experimentation. Its goal is to facilitate research on
Bayesian neural networks, uncertainty quantification for deep learning, and the
development of improved Laplace approximation techniques.

</details>


### [45] [Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting](https://arxiv.org/abs/2507.17016)
*Omid Orang,Patricia O. Lucas,Gabriel I. F. Paiva,Petronio C. L. Silva,Felipe Augusto Rocha da Silva,Adriano Alonso Veloso,Frederico Gadelha Guimaraes*

Main category: cs.LG

TL;DR: 该研究提出了CGF-LLM框架，首次将GPT-2与模糊时间序列和因果图结合用于多变量时间序列预测，通过模糊化和因果分析将数值时间序列转换为可解释的文本表示，在四个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在时间序列预测中的应用缺乏可解释性，难以处理复杂的多变量时间序列数据的语义理解和结构洞察问题。

Method: 提出CGF-LLM框架，结合GPT-2、模糊时间序列(FTS)和因果图，通过并行的模糊化和因果分析将数值时间序列转换为可解释的文本表示，然后输入预训练的GPT-2模型进行预测。

Result: 在四个不同的多变量时间序列数据集上验证了所提出模型的有效性，证明了基于LLM的时间序列预测方法的可行性。

Conclusion: CGF-LLM成功实现了数值时间序列的可解释性转换，为基于大语言模型的时间序列预测领域开辟了新的研究方向，特别是在结合模糊时间序列方法方面具有重要意义。

Abstract: In recent years, the application of Large Language Models (LLMs) to time
series forecasting (TSF) has garnered significant attention among researchers.
This study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with
fuzzy time series (FTS) and causal graph to predict multivariate time series,
marking the first such architecture in the literature. The key objective is to
convert numerical time series into interpretable forms through the parallel
application of fuzzification and causal analysis, enabling both semantic
understanding and structural insight as input for the pretrained GPT-2 model.
The resulting textual representation offers a more interpretable view of the
complex dynamics underlying the original time series. The reported results
confirm the effectiveness of our proposed LLM-based time series forecasting
model, as demonstrated across four different multivariate time series datasets.
This initiative paves promising future directions in the domain of TSF using
LLMs based on FTS.

</details>


### [46] [BiLO: Bilevel Local Operator Learning for PDE Inverse Problems. Part II: Efficient Uncertainty Quantification with Low-Rank Adaptation](https://arxiv.org/abs/2507.17019)
*Ray Zirui Zhang,Christopher E. Miles,Xiaohui Xie,John S. Lowengrub*

Main category: cs.LG

TL;DR: 本文提出了一种基于双层局部算子学习(BiLO)的贝叶斯推理框架，用于偏微分方程约束的不确定性量化问题。该方法通过梯度MCMC采样和低秩适应技术实现高效推理，避免了在神经网络权重高维空间中采样的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于贝叶斯神经网络的PDE不确定性量化方法存在高维权重空间采样困难、需要指定先验分布等问题。需要开发一种能够有效处理PDE约束优化问题中参数推理和不确定性量化的新方法。

Method: 采用双层优化框架：下层训练神经网络逼近局部解算子，通过最小化局部算子损失；上层从后验分布中采样PDE参数，使用基于梯度的马尔可夫链蒙特卡罗(MCMC)方法和低秩适应(LoRA)技术实现高效采样。通过强制执行PDE约束来传播不确定性。

Result: 数值实验表明该方法在多种PDE模型上都能提供准确的推理和不确定性量化，同时保持高计算效率。分析了MCMC采样器中的动态梯度误差和下层问题非精确最小化导致的静态后验分布误差，建立了下层问题求解容差与不确定性量化精度之间的直接联系。

Conclusion: 提出的BiLO贝叶斯推理框架成功解决了PDE约束的不确定性量化问题，避免了传统贝叶斯神经网络方法的局限性，通过PDE约束自然传播不确定性，在保证计算效率的同时提高了参数推理和不确定性量化的精度。

Abstract: Uncertainty quantification and inverse problems governed by partial
differential equations (PDEs) are central to a wide range of scientific and
engineering applications. In this second part of a two part series, we extend
Bilevel Local Operator Learning (BiLO) for PDE-constrained optimization
problems developed in Part 1 to the Bayesian inference framework. At the lower
level, we train a network to approximate the local solution operator by
minimizing the local operator loss with respect to the weights of the neural
network. At the upper level, we sample the PDE parameters from the posterior
distribution. We achieve efficient sampling through gradient-based Markov Chain
Monte Carlo (MCMC) methods and low-rank adaptation (LoRA). Compared with
existing methods based on Bayesian neural networks, our approach bypasses the
challenge of sampling in the high-dimensional space of neural network weights
and does not require specifying a prior distribution on the neural network
solution. Instead, uncertainty propagates naturally from the data through the
PDE constraints. By enforcing strong PDE constraints, the proposed method
improves the accuracy of both parameter inference and uncertainty
quantification. We analyze the dynamic error of the gradient in the MCMC
sampler and the static error in the posterior distribution due to inexact
minimization of the lower level problem and demonstrate a direct link between
the tolerance for solving the lower level problem and the accuracy of the
resulting uncertainty quantification. Through numerical experiments across a
variety of PDE models, we demonstrate that our method delivers accurate
inference and quantification of uncertainties while maintaining high
computational efficiency.

</details>


### [47] [Pragmatic Policy Development via Interpretable Behavior Cloning](https://arxiv.org/abs/2507.17056)
*Anton Matsson,Yaochen Rao,Heather J. Litman,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 本文提出了一种基于树模型的离线强化学习方法，通过分析最频繁的治疗行为来制定可解释的医疗政策，解决了传统离线RL在安全关键领域中的可解释性和评估难题。


<details>
  <summary>Details</summary>
Motivation: 传统离线强化学习在安全关键医疗领域面临两大挑战：1) 无约束RL策略的黑盒特性导致可解释性差；2) 离策略评估对行为策略的大幅偏离敏感，特别是使用重要性采样方法时评估不可靠。

Method: 提出基于树模型的实用替代方案：通过可解释的行为策略模型估计每个患者状态下最频繁选择的行动来制定治疗策略。使用专门设计的树模型挖掘数据模式，获得治疗方面的自然状态分组，通过控制考虑的行动数量来调节与行为策略的重叠程度。

Result: 在类风湿关节炎和脓毒症护理的真实世界案例中，该框架下制定的策略能够超越当前实践表现，提供了相比传统离线RL更可解释的替代方案，成功标准化了频繁的治疗模式并捕获了数据中的集体临床判断。

Conclusion: 这种务实的策略开发方法通过设计确保了可解释性，同时实现了可靠的离策略评估，为医疗决策提供了一个实用且可信的离线强化学习解决方案，有效平衡了性能和可解释性需求。

Abstract: Offline reinforcement learning (RL) holds great promise for deriving optimal
policies from observational data, but challenges related to interpretability
and evaluation limit its practical use in safety-critical domains.
Interpretability is hindered by the black-box nature of unconstrained RL
policies, while evaluation -- typically performed off-policy -- is sensitive to
large deviations from the data-collecting behavior policy, especially when
using methods based on importance sampling. To address these challenges, we
propose a simple yet practical alternative: deriving treatment policies from
the most frequently chosen actions in each patient state, as estimated by an
interpretable model of the behavior policy. By using a tree-based model, which
is specifically designed to exploit patterns in the data, we obtain a natural
grouping of states with respect to treatment. The tree structure ensures
interpretability by design, while varying the number of actions considered
controls the degree of overlap with the behavior policy, enabling reliable
off-policy evaluation. This pragmatic approach to policy development
standardizes frequent treatment patterns, capturing the collective clinical
judgment embedded in the data. Using real-world examples in rheumatoid
arthritis and sepsis care, we demonstrate that policies derived under this
framework can outperform current practice, offering interpretable alternatives
to those obtained via offline RL.

</details>


### [48] [Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation](https://arxiv.org/abs/2507.17066)
*Jessup Byun,Xiaofeng Lin,Joshua Ward,Guang Cheng*

Main category: cs.LG

TL;DR: 本文首次对基础模型在表格数据合成中的隐私风险进行了基准测试，发现大型预训练模型在低数据环境下存在严重的成员推理攻击风险，并提出了三种零成本的提示优化方法来改善隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在低数据环境下容易过拟合和泄露敏感记录，而基础模型通过上下文学习进行表格合成虽然避免了重训练，但会逐字重复种子行，在表格合成中引入新的隐私风险。然而，这种风险在表格数据中的严重程度尚不清楚，需要系统性评估。

Method: 构建了首个针对表格合成的隐私基准测试，对比了三个基础模型（GPT-4o-mini、LLaMA 3.3 70B、TabPFN v2）与四个基线模型在35个来自健康、金融和政策领域真实表格上的表现。评估指标包括统计保真度、下游效用和成员推理泄露。同时进行因子研究，测试了小批量大小、低温度和使用汇总统计等提示优化策略。

Result: 基础模型始终具有最高的隐私风险，LLaMA 3.3 70B在1%假阳性率下的真阳性率比最安全基线高54个百分点。CTGAN和GPT-4o-mini提供了更好的隐私-效用权衡。三种零成本提示优化方法可将最坏情况AUC降低14个点，稀有类别泄露降低39个点，同时保持90%以上的保真度。

Conclusion: 基础模型在表格合成中存在显著隐私风险，但通过适当的提示优化策略可以在保持高保真度的同时显著降低隐私泄露风险。本研究为在低数据环境下使用基础模型进行更安全的表格合成提供了实用指导。

Abstract: Synthetic tabular data is essential for machine learning workflows,
especially for expanding small or imbalanced datasets and enabling
privacy-preserving data sharing. However, state-of-the-art generative models
(GANs, VAEs, diffusion models) rely on large datasets with thousands of
examples. In low-data settings, often the primary motivation for synthetic
data, these models can overfit, leak sensitive records, and require frequent
retraining. Recent work uses large pre-trained transformers to generate rows
via in-context learning (ICL), which needs only a few seed examples and no
parameter updates, avoiding retraining. But ICL repeats seed rows verbatim,
introducing a new privacy risk that has only been studied in text. The severity
of this risk in tabular synthesis-where a single row may identify a
person-remains unclear. We address this gap with the first benchmark of three
foundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four
baselines on 35 real-world tables from health, finance, and policy. We evaluate
statistical fidelity, downstream utility, and membership inference leakage.
Results show foundation models consistently have the highest privacy risk.
LLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at
1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly
vulnerable. We plot the privacy-utility frontier and show that CTGAN and
GPT-4o-mini offer better tradeoffs. A factorial study finds that three
zero-cost prompt tweaks-small batch size, low temperature, and using summary
statistics-can reduce worst-case AUC by 14 points and rare-class leakage by up
to 39 points while maintaining over 90% fidelity. Our benchmark offers a
practical guide for safer low-data synthesis with foundation models.

</details>


### [49] [Advancing Robustness in Deep Reinforcement Learning with an Ensemble Defense Approach](https://arxiv.org/abs/2507.17070)
*Adithya Mohan,Dominik Rößle,Daniel Cremers,Torsten Schön*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的集成防御架构，用于缓解自动驾驶中深度强化学习模型面临的对抗攻击，在高速公路和合并场景中显著提升了模型的鲁棒性表现。


<details>
  <summary>Details</summary>
Motivation: 现有的深度强化学习防御机制（如对抗训练和蒸馏）虽然能增强模型韧性，但在自动驾驶场景中缺乏多重防御机制的集成研究，存在重要的研究空白需要填补。

Method: 提出了一种新颖的基于集成的防御架构，整合多种防御策略来抵御自动驾驶场景中的对抗攻击。

Result: 在FGSM攻击下，相比基线方法，集成方法在高速公路和合并场景中将平均奖励从5.87提升到18.38（增长213%），平均碰撞率从0.50降低到0.09（降低82%），超越了所有单独的防御策略。

Conclusion: 所提出的集成防御架构显著增强了深度强化学习模型在自动驾驶场景中的鲁棒性，为对抗攻击防御提供了有效的解决方案。

Abstract: Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated
its applicability across various domains, including robotics, healthcare,
energy optimization, and autonomous driving. However, a critical question
remains: How robust are DRL models when exposed to adversarial attacks? While
existing defense mechanisms such as adversarial training and distillation
enhance the resilience of DRL models, there remains a significant research gap
regarding the integration of multiple defenses in autonomous driving scenarios
specifically. This paper addresses this gap by proposing a novel ensemble-based
defense architecture to mitigate adversarial attacks in autonomous driving. Our
evaluation demonstrates that the proposed architecture significantly enhances
the robustness of DRL models. Compared to the baseline under FGSM attacks, our
ensemble method improves the mean reward from 5.87 to 18.38 (over 213%
increase) and reduces the mean collision rate from 0.50 to 0.09 (an 82%
decrease) in the highway scenario and merge scenario, outperforming all
standalone defense strategies.

</details>


### [50] [Sensor Drift Compensation in Electronic-Nose-Based Gas Recognition Using Knowledge Distillation](https://arxiv.org/abs/2507.17071)
*Juntao Lin,Xianghao Zhan*

Main category: cs.LG

TL;DR: 本文提出了一种基于知识蒸馏(KD)的新方法来解决电子鼻系统中的传感器漂移问题，在UCI气体传感器阵列漂移数据集上的实验表明，该方法在准确率和F1分数上分别提升了18%和15%，显著优于现有的DRCA方法。


<details>
  <summary>Details</summary>
Motivation: 电子鼻系统在实际部署中面临传感器漂移挑战，这会影响气体分类性能。现有研究缺乏严格的统计实验验证，且可能过度补偿传感器漂移而丢失类别相关的方差信息。

Method: 提出了基于知识蒸馏(KD)的新方法用于传感器漂移补偿，设计了两个域适应任务：使用第一批数据预测其余批次，以及使用所有先前批次预测下一批次。系统性地测试了KD方法、基准DRCA方法和混合KD-DRCA方法。

Result: 在UCI数据集的30个随机测试集分割上，KD方法consistently优于DRCA和KD-DRCA方法，在准确率上提升高达18%，在F1分数上提升15%，证明了KD在漂移补偿方面的卓越有效性。

Conclusion: 这是首次将知识蒸馏应用于电子鼻漂移缓解的研究，显著超越了之前最先进的DRCA方法，提高了真实环境中传感器漂移补偿的可靠性。

Abstract: Due to environmental changes and sensor aging, sensor drift challenges the
performance of electronic nose systems in gas classification during real-world
deployment. Previous studies using the UCI Gas Sensor Array Drift Dataset
reported promising drift compensation results but lacked robust statistical
experimental validation and may overcompensate for sensor drift, losing
class-related variance.To address these limitations and improve sensor drift
compensation with statistical rigor, we first designed two domain adaptation
tasks based on the same electronic nose dataset: using the first batch to
predict the remaining batches, simulating a controlled laboratory setting; and
predicting the next batch using all prior batches, simulating continuous
training data updates for online training. We then systematically tested three
methods: our proposed novel Knowledge Distillation (KD) method, the benchmark
method Domain Regularized Component Analysis (DRCA), and a hybrid method
KD-DRCA, across 30 random test set partitions on the UCI dataset. We showed
that KD consistently outperformed both DRCA and KD-DRCA, achieving up to an 18%
improvement in accuracy and 15% in F1-score, demonstrating KD's superior
effectiveness in drift compensation. This is the first application of KD for
electronic nose drift mitigation, significantly outperforming the previous
state-of-the-art DRCA method and enhancing the reliability of sensor drift
compensation in real-world environments.

</details>


### [51] [ZORMS-LfD: Learning from Demonstrations with Zeroth-Order Random Matrix Search](https://arxiv.org/abs/2507.17096)
*Olivia Dry,Timothy L. Molloy,Wanxin Jin,Iman Shames*

Main category: cs.LG

TL;DR: 本文提出ZORMS-LfD方法，这是一种零阶随机矩阵搜索算法，用于从专家演示中学习约束最优控制问题，无需梯度计算且适用于连续和离散时间系统。


<details>
  <summary>Details</summary>
Motivation: 现有的一阶方法需要计算成本、约束、动力学和学习损失相对于状态、控制和参数的梯度，且要求学习损失景观的平滑性。大多数现有方法仅适用于离散时间，对连续时间约束问题关注不足。

Method: 提出零阶随机矩阵搜索学习演示方法(ZORMS-LfD)，该方法能够在不需要梯度存在和计算的情况下，从专家演示中学习约束最优控制问题的成本、约束和动力学，同时支持连续和离散时间系统。

Result: 在多个基准问题上，ZORMS-LfD在学习损失和计算时间方面匹配或超越了现有最先进方法的性能。在无约束连续时间基准问题上，相比一阶方法实现了相似的损失性能但计算时间减少超过80%。在约束连续时间基准问题上，优于常用的无梯度Nelder-Mead优化方法。

Conclusion: ZORMS-LfD方法成功解决了现有方法对梯度计算的依赖问题，在保持或提升学习性能的同时显著减少了计算时间，特别适用于连续时间约束最优控制问题的学习。

Abstract: We propose Zeroth-Order Random Matrix Search for Learning from Demonstrations
(ZORMS-LfD). ZORMS-LfD enables the costs, constraints, and dynamics of
constrained optimal control problems, in both continuous and discrete time, to
be learned from expert demonstrations without requiring smoothness of the
learning-loss landscape. In contrast, existing state-of-the-art first-order
methods require the existence and computation of gradients of the costs,
constraints, dynamics, and learning loss with respect to states, controls
and/or parameters. Most existing methods are also tailored to discrete time,
with constrained problems in continuous time receiving only cursory attention.
We demonstrate that ZORMS-LfD matches or surpasses the performance of
state-of-the-art methods in terms of both learning loss and compute time across
a variety of benchmark problems. On unconstrained continuous-time benchmark
problems, ZORMS-LfD achieves similar loss performance to state-of-the-art
first-order methods with an over $80$\% reduction in compute time. On
constrained continuous-time benchmark problems where there is no specialized
state-of-the-art method, ZORMS-LfD is shown to outperform the commonly used
gradient-free Nelder-Mead optimization method.

</details>


### [52] [Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models](https://arxiv.org/abs/2507.17107)
*Andrii Balashov*

Main category: cs.LG

TL;DR: 研究发现强化学习微调大语言模型时只会修改5-30%的参数，形成稀疏的子网络更新模式，这一现象在多种RL算法和模型家族中普遍存在，且仅训练这个稀疏子网络就能恢复完整模型性能。


<details>
  <summary>Details</summary>
Motivation: 挑战传统假设——强化学习微调需要更新模型大部分参数。研究者想要探索RL微调过程中参数更新的真实模式，以及是否存在更高效的微调方法。

Method: 通过分析多种强化学习算法（PPO、DPO、SimPO、PRIME）在不同模型家族（OpenAI、Meta等）上的参数更新模式，识别RL微调过程中实际发生变化的参数子集，并验证仅训练这些稀疏子网络的效果。

Result: 发现RL微调只修改5-30%的权重参数，形成RL诱导的参数更新稀疏性；不同种子、数据集和算法更新的子网络显示出显著重叠；仅微调稀疏子网络就能恢复完整模型性能；KL惩罚、梯度裁剪等因素对稀疏模式影响有限。

Conclusion: RL通过专注训练小而一致的子网络来适应模型，而非改变所有权重。这一发现为更高效的RL方法提供了新思路，并从彩票假设的角度重新解释了稀疏性现象，揭示了RL在接近模型原始分布附近操作的特性。

Abstract: Reinforcement learning (RL) is a key post-pretraining step for aligning large
language models (LLMs) with complex tasks and human preferences. While it is
often assumed that RL fine-tuning requires updating most of a model's
parameters, we challenge this assumption with a surprising finding: RL
fine-tuning consistently modifies only a small subnetwork (typically 5-30% of
weights), leaving most parameters unchanged. We call this phenomenon RL-induced
parameter update sparsity. It arises naturally, without any sparsity
constraints or parameter-efficient tuning, and appears across multiple RL
algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI,
Meta, and open-source LLMs). Moreover, the subnetworks updated by RL show
substantial overlap across different seeds, datasets, and algorithms-far
exceeding chance-suggesting a partially transferable structure in the
pretrained model. We show that fine-tuning only this sparse subnetwork recovers
full model performance and yields parameters nearly identical to the fully
fine-tuned model. Our analysis suggests this sparsity emerges because RL
operates near the model's original distribution, requiring only targeted
changes. KL penalties, gradient clipping, and on-policy dynamics have limited
effect on the sparsity pattern. These findings shed new light on how RL adapts
models: not by shifting all weights, but by focusing training on a small,
consistently updated subnetwork. This insight enables more efficient RL methods
and reframes sparsity through the lens of the lottery ticket hypothesis.

</details>


### [53] [Probabilistic Graphical Models: A Concise Tutorial](https://arxiv.org/abs/2507.17116)
*Jacqueline Maasch,Willie Neiswanger,Stefano Ermon,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: 这是一篇关于概率图模型的教程论文，介绍了概率图模型的理论基础、学习算法和推理方法，旨在为读者提供该建模框架的简明入门指导。


<details>
  <summary>Details</summary>
Motivation: 概率图模型作为机器学习的重要分支，结合了概率论和图论两个数学传统，能够在不确定性环境下进行世界建模、预测和决策支持。需要一个简明的教程来介绍这一优雅而强大的建模框架的形式化方法、技术和应用。

Method: 论文采用教程式的方法，首先回顾基础的概率论和图论知识，然后围绕三个主要主题展开：(1)用直观的图形语言表示多变量分布；(2)从数据中学习模型参数和图结构的算法；(3)精确和近似推理算法。

Result: 提供了概率图模型建模框架的完整介绍，涵盖了理论基础、算法方法和实际应用，为读者构建了从基础概念到实际应用的知识体系。

Conclusion: 概率图模型是一个强大的建模框架，它通过图的直观表示方式提供了联合概率分布的紧凑而富有表现力的表示，为概率推理提供了强大的生成模型。该教程为理解和应用这一框架提供了全面的入门指导。

Abstract: Probabilistic graphical modeling is a branch of machine learning that uses
probability distributions to describe the world, make predictions, and support
decision-making under uncertainty. Underlying this modeling framework is an
elegant body of theory that bridges two mathematical traditions: probability
and graph theory. This framework provides compact yet expressive
representations of joint probability distributions, yielding powerful
generative models for probabilistic reasoning.
  This tutorial provides a concise introduction to the formalisms, methods, and
applications of this modeling framework. After a review of basic probability
and graph theory, we explore three dominant themes: (1) the representation of
multivariate distributions in the intuitive visual language of graphs, (2)
algorithms for learning model parameters and graphical structures from data,
and (3) algorithms for inference, both exact and approximate.

</details>


### [54] [Computer Vision for Real-Time Monkeypox Diagnosis on Embedded Systems](https://arxiv.org/abs/2507.17123)
*Jacob M. Delgado-López,Ricardo A. Morell-Rodriguez,Sebastián O. Espinosa-Del Rosario,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: 本研究开发了一个基于AI的猴痘诊断工具，部署在NVIDIA Jetson Orin Nano上，使用预训练的MobileNetV2架构实现二分类，F1分数达93.07%。通过TensorRT优化实现模型压缩和加速，功耗降低约一半，并配备Wi-Fi热点和Web界面，适用于资源受限环境的实时诊断。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中，传染性疾病如猴痘的快速诊断对于有效控制和治疗至关重要。现有诊断方法在欠发达地区面临设备昂贵、功耗高、连接困难等挑战，迫切需要一种高效、可扩展且低能耗的诊断解决方案。

Method: 使用预训练的MobileNetV2架构构建二分类模型，在开源猴痘皮肤病变数据集上训练。采用TensorRT框架对模型进行优化，包括FP32推理加速和FP16、INT8格式的训练后量化。系统部署在NVIDIA Jetson Orin Nano上，配备Wi-Fi接入点热点和基于Web的用户界面。

Result: 模型达到93.07%的F1分数，在精确率和召回率之间表现平衡良好。TensorRT优化使模型尺寸减小、推理速度提高、功耗降低约一半，同时保持原始准确性。功耗分析确认优化模型在推理过程中显著降低能耗。用户可通过手机等设备直接上传和分析图像。

Conclusion: 该诊断工具作为一个高效、可扩展且节能的解决方案，成功解决了服务不足地区的诊断挑战，为低资源医疗环境中的广泛应用铺平了道路。系统的简单访问性和无缝连接性使其在现实应用中具有实用性。

Abstract: The rapid diagnosis of infectious diseases, such as monkeypox, is crucial for
effective containment and treatment, particularly in resource-constrained
environments. This study presents an AI-driven diagnostic tool developed for
deployment on the NVIDIA Jetson Orin Nano, leveraging the pre-trained
MobileNetV2 architecture for binary classification. The model was trained on
the open-source Monkeypox Skin Lesion Dataset, achieving a 93.07% F1-Score,
which reflects a well-balanced performance in precision and recall. To optimize
the model, the TensorRT framework was used to accelerate inference for FP32 and
to perform post-training quantization for FP16 and INT8 formats. TensorRT's
mixed-precision capabilities enabled these optimizations, which reduced the
model size, increased inference speed, and lowered power consumption by
approximately a factor of two, all while maintaining the original accuracy.
Power consumption analysis confirmed that the optimized models used
significantly less energy during inference, reinforcing their suitability for
deployment in resource-constrained environments. The system was deployed with a
Wi-Fi Access Point (AP) hotspot and a web-based interface, enabling users to
upload and analyze images directly through connected devices such as mobile
phones. This setup ensures simple access and seamless connectivity, making the
tool practical for real-world applications. These advancements position the
diagnostic tool as an efficient, scalable, and energy-conscious solution to
address diagnosis challenges in underserved regions, paving the way for broader
adoption in low-resource healthcare settings.

</details>


### [55] [Model Compression Engine for Wearable Devices Skin Cancer Diagnosis](https://arxiv.org/abs/2507.17125)
*Jacob M. Delgado-López,Andrea P. Seda-Hernandez,Juan D. Guadalupe-Rosado,Luis E. Fernandez Ramirez,Miguel Giboyeaux-Camilo,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: 本研究开发了一个基于MobileNetV2和TensorRT优化的AI皮肤癌诊断工具，可在NVIDIA Jetson Orin Nano等资源受限的嵌入式设备上高效运行，在保持87.18% F1分数的同时显著降低模型大小和能耗。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是最常见且可预防的癌症之一，但其早期检测仍然是一个挑战，特别是在缺乏专业医疗资源的资源受限地区。因此需要开发适合嵌入式系统的AI驱动诊断工具来解决这一差距。

Method: 采用迁移学习结合MobileNetV2架构，将模型适配为皮肤病变的二分类任务（"皮肤癌"和"其他"）。使用TensorRT框架对模型进行压缩和优化，以便在NVIDIA Jetson Orin Nano上部署，在性能和能效之间取得平衡。

Result: 优化后的模型保持了良好性能，F1分数达到87.18%，精确率93.18%，召回率81.91%。压缩后模型大小减少了0.41倍，推理速度和吞吐量得到改善，INT8精度下能耗降低了0.93倍。

Conclusion: 研究验证了在资源受限的边缘设备上部署高性能、节能诊断工具的可行性。该方法不仅适用于皮肤癌检测，还可扩展到其他医疗诊断和需要高效AI解决方案的领域，有望通过优化的AI系统革命性地改变医疗诊断，缩小先进技术与服务不足地区之间的差距。

Abstract: Skin cancer is one of the most prevalent and preventable types of cancer, yet
its early detection remains a challenge, particularly in resource-limited
settings where access to specialized healthcare is scarce. This study proposes
an AI-driven diagnostic tool optimized for embedded systems to address this
gap. Using transfer learning with the MobileNetV2 architecture, the model was
adapted for binary classification of skin lesions into "Skin Cancer" and
"Other." The TensorRT framework was employed to compress and optimize the model
for deployment on the NVIDIA Jetson Orin Nano, balancing performance with
energy efficiency. Comprehensive evaluations were conducted across multiple
benchmarks, including model size, inference speed, throughput, and power
consumption. The optimized models maintained their performance, achieving an
F1-Score of 87.18% with a precision of 93.18% and recall of 81.91%.
Post-compression results showed reductions in model size of up to 0.41, along
with improvements in inference speed and throughput, and a decrease in energy
consumption of up to 0.93 in INT8 precision. These findings validate the
feasibility of deploying high-performing, energy-efficient diagnostic tools on
resource-constrained edge devices. Beyond skin cancer detection, the
methodologies applied in this research have broader applications in other
medical diagnostics and domains requiring accessible, efficient AI solutions.
This study underscores the potential of optimized AI systems to revolutionize
healthcare diagnostics, thereby bridging the divide between advanced technology
and underserved regions.

</details>


### [56] [Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance](https://arxiv.org/abs/2507.17131)
*Yufei He,Ruoyu Li,Alex Chen,Yue Liu,Yulin Chen,Yuan Sui,Cheng Chen,Yi Zhu,Luca Luo,Frank Yang,Bryan Hooi*

Main category: cs.LG

TL;DR: 本文提出了ARIA框架，一个能够在测试时持续学习更新领域知识的大语言模型智能体，通过结构化自对话评估不确定性，主动识别知识缺口并请求人类专家指导，系统性更新内部知识库。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型智能体在规则和领域知识频繁变化的环境中表现不佳，如合规监管和用户风险筛查场景。离线微调和标准提示等传统方法无法在实际运行期间有效适应新知识。

Method: 提出自适应反思交互智能体(ARIA)框架，通过结构化自对话评估自身不确定性，主动识别知识缺口并向人类专家请求有针对性的解释或纠正，然后系统性地更新带时间戳的内部知识库，通过比较和澄清查询检测并解决冲突或过时的知识。

Result: 在TikTok Pay的客户尽职调查姓名筛查任务以及公开可用的动态知识任务上进行评估，结果显示相比使用标准离线微调和现有自改进智能体的基线方法，ARIA在适应性和准确性方面有显著提升。

Conclusion: ARIA已部署在TikTok Pay中为超过1.5亿月活跃用户提供服务，证实了其在快速变化环境中用于运营的实用性和有效性。该框架成功解决了大语言模型智能体在动态环境中的适应性问题。

Abstract: Large language model (LLM) agents often struggle in environments where rules
and required domain knowledge frequently change, such as regulatory compliance
and user risk screening. Current approaches, like offline fine-tuning and
standard prompting, are insufficient because they cannot effectively adapt to
new knowledge during actual operation. To address this limitation, we propose
the Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework
designed specifically to continuously learn updated domain knowledge at test
time. ARIA assesses its own uncertainty through structured self-dialogue,
proactively identifying knowledge gaps and requesting targeted explanations or
corrections from human experts. It then systematically updates an internal,
timestamped knowledge repository with provided human guidance, detecting and
resolving conflicting or outdated knowledge through comparisons and
clarification queries. We evaluate ARIA on the realistic customer due diligence
name screening task on TikTok Pay, alongside publicly available dynamic
knowledge tasks. Results demonstrate significant improvements in adaptability
and accuracy compared to baselines using standard offline fine-tuning and
existing self-improving agents. ARIA is deployed within TikTok Pay serving over
150 million monthly active users, confirming its practicality and effectiveness
for operational use in rapidly evolving environments.

</details>


### [57] [SADA: Stability-guided Adaptive Diffusion Acceleration](https://arxiv.org/abs/2507.17135)
*Ting Jiang,Yixiao Wang,Hancheng Ye,Zishan Shao,Jingwei Sun,Jingyang Zhang,Zekai Chen,Jianyi Zhang,Yiran Chen,Hai Li*

Main category: cs.LG

TL;DR: 本文提出了SADA（稳定性引导的自适应扩散加速）方法，通过统一的稳定性准则来实现步骤和令牌级别的稀疏性决策，在保持高保真度的同时实现了1.8倍以上的扩散模型采样加速。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型训练无关加速策略虽然能减少采样时间，但相比原始基线存在较低的保真度问题。作者假设这种保真度差距源于：(a)不同提示对应不同的去噪轨迹，(b)这些方法没有考虑底层的ODE公式及其数值解。

Method: 提出SADA方法，通过单一稳定性准则统一步骤级和令牌级稀疏性决策来加速基于ODE的生成模型。针对问题(a)，SADA根据采样轨迹自适应分配稀疏性；针对问题(b)，SADA引入原理性近似方案，利用数值ODE求解器的精确梯度信息。

Result: 在SD-2、SDXL和Flux上使用EDM和DPM++求解器的综合评估显示，相比未修改的基线，SADA实现了至少1.8倍的一致加速，同时保持最小的保真度下降（LPIPS ≤ 0.10，FID ≤ 4.5），显著优于现有方法。此外，SADA还能无缝适应其他管道和模态，无需修改即可加速ControlNet，并以约0.01的频谱图LPIPS实现MusicLDM 1.8倍加速。

Conclusion: SADA成功解决了现有扩散模型加速方法中保真度与速度的权衡问题，通过稳定性引导的自适应策略实现了高效且高保真的采样加速，并展现出良好的跨模态和跨管道适应性。

Abstract: Diffusion models have achieved remarkable success in generative tasks but
suffer from high computational costs due to their iterative sampling process
and quadratic attention costs. Existing training-free acceleration strategies
that reduce per-step computation cost, while effectively reducing sampling
time, demonstrate low faithfulness compared to the original baseline. We
hypothesize that this fidelity gap arises because (a) different prompts
correspond to varying denoising trajectory, and (b) such methods do not
consider the underlying ODE formulation and its numerical solution. In this
paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a
novel paradigm that unifies step-wise and token-wise sparsity decisions via a
single stability criterion to accelerate sampling of ODE-based generative
models (Diffusion and Flow-matching). For (a), SADA adaptively allocates
sparsity based on the sampling trajectory. For (b), SADA introduces principled
approximation schemes that leverage the precise gradient information from the
numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using
both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with
minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to
unmodified baselines, significantly outperforming prior methods. Moreover, SADA
adapts seamlessly to other pipelines and modalities: It accelerates ControlNet
without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim
0.01$ spectrogram LPIPS.

</details>


### [58] [PICore: Physics-Informed Unsupervised Coreset Selection for Data Efficient Neural Operator Training](https://arxiv.org/abs/2507.17151)
*Anirudh Satheesh,Anant Khandelwal,Mucong Ding,Radu Balan*

Main category: cs.LG

TL;DR: PICore是一个无监督核心集选择框架，通过物理信息损失函数识别最有信息量的训练样本，无需真实PDE解标签，显著提高神经算子训练效率并降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 神经算子在求解偏微分方程时面临两个主要瓶颈：需要大量训练数据学习函数空间映射，且数据需要通过昂贵的数值求解器仿真获得标签。现有方法在数据选择和标注成本方面存在效率问题。

Method: 提出PICore无监督核心集选择框架，利用物理信息损失函数评估未标记输入样本对算子学习的潜在贡献，选择最有信息量的紧凑子集进行数值仿真标注，然后在缩减的标记数据集上训练神经算子。

Result: 在四个不同PDE基准测试和多种核心集选择策略上，PICore相比有监督核心集选择方法平均提高78%的训练效率，同时准确性变化极小。显著减少了训练时间和标注成本。

Conclusion: PICore框架成功解决了神经算子训练中的数据效率和标注成本问题，通过无监督的物理信息指导的样本选择，在保持模型性能的同时大幅提升训练效率，为PDE求解提供了更实用的解决方案。

Abstract: Neural operators offer a powerful paradigm for solving partial differential
equations (PDEs) that cannot be solved analytically by learning mappings
between function spaces. However, there are two main bottlenecks in training
neural operators: they require a significant amount of training data to learn
these mappings, and this data needs to be labeled, which can only be accessed
via expensive simulations with numerical solvers. To alleviate both of these
issues simultaneously, we propose PICore, an unsupervised coreset selection
framework that identifies the most informative training samples without
requiring access to ground-truth PDE solutions. PICore leverages a
physics-informed loss to select unlabeled inputs by their potential
contribution to operator learning. After selecting a compact subset of inputs,
only those samples are simulated using numerical solvers to generate labels,
reducing annotation costs. We then train the neural operator on the reduced
labeled dataset, significantly decreasing training time as well. Across four
diverse PDE benchmarks and multiple coreset selection strategies, PICore
achieves up to 78% average increase in training efficiency relative to
supervised coreset selection methods with minimal changes in accuracy. We
provide code at https://github.com/Asatheesh6561/PICore.

</details>


### [59] [Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection](https://arxiv.org/abs/2507.17161)
*Vinura Galwaduge,Jagath Samarabandu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于扩散的反事实解释框架，用于网络入侵检测系统中的可操作解释，能够生成最小化、多样化的反事实解释并创建全局规则用于攻击防御。


<details>
  <summary>Details</summary>
Motivation: 现代网络入侵检测系统使用复杂的深度学习模型，但其"黑盒"特性阻碍了对检测决策的理解和信任，现有的可解释AI方法难以转化为可操作的对策措施。

Method: 提出了一种新颖的基于扩散的反事实解释框架，通过生成反事实解释来提供可操作的网络入侵攻击解释，并将这些解释总结为全局规则。

Result: 在3个现代网络入侵数据集上的评估显示，该方法相比其他反事实解释算法能更高效地提供最小化、多样化的反事实解释，减少了生成解释的时间，并能有效过滤入侵攻击查询。

Conclusion: 该研究首次在网络入侵检测系统背景下对反事实解释算法进行了比较分析，证明了反事实解释能够提供实例级别和全局级别的可操作解释，对高效的入侵检测和防御机制至关重要。

Abstract: Modern network intrusion detection systems (NIDS) frequently utilize the
predictive power of complex deep learning models. However, the "black-box"
nature of such deep learning methods adds a layer of opaqueness that hinders
the proper understanding of detection decisions, trust in the decisions and
prevent timely countermeasures against such attacks. Explainable AI (XAI)
methods provide a solution to this problem by providing insights into the
causes of the predictions. The majority of the existing XAI methods provide
explanations which are not convenient to convert into actionable
countermeasures. In this work, we propose a novel diffusion-based
counterfactual explanation framework that can provide actionable explanations
for network intrusion attacks. We evaluated our proposed algorithm against
several other publicly available counterfactual explanation algorithms on 3
modern network intrusion datasets. To the best of our knowledge, this work also
presents the first comparative analysis of existing counterfactual explanation
algorithms within the context of network intrusion detection systems. Our
proposed method provide minimal, diverse counterfactual explanations out of the
tested counterfactual explanation algorithms in a more efficient manner by
reducing the time to generate explanations. We also demonstrate how
counterfactual explanations can provide actionable explanations by summarizing
them to create a set of global rules. These rules are actionable not only at
instance level but also at the global level for intrusion attacks. These global
counterfactual rules show the ability to effectively filter out incoming attack
queries which is crucial for efficient intrusion detection and defense
mechanisms.

</details>


### [60] [Met$^2$Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems](https://arxiv.org/abs/2507.17189)
*Shaohan Li,Hao Yang,Min Chen,Xiaolin Qin*

Main category: cs.LG

TL;DR: 提出了一种隐式两阶段训练方法Met2Net用于天气预测，通过为每个变量配置独立的编码器和解码器，以及在潜在空间中引入自注意力机制来解决多变量天气预测中的表示不一致和变量依赖性捕获问题


<details>
  <summary>Details</summary>
Motivation: 现有端到端天气预测方法在多变量集成时存在表示不一致问题，难以有效捕获复杂天气系统中变量间的依赖关系；传统两阶段训练方法由于两个阶段训练任务不一致导致结果次优

Method: 提出隐式两阶段训练方法：第一阶段冻结翻译器，让编码器和解码器学习共享潜在空间；第二阶段冻结编码器和解码器，翻译器捕获变量间交互进行预测；同时在潜在空间引入自注意力机制进行多变量融合

Result: 在近地面气温和相对湿度预测任务上分别将MSE降低了28.82%和23.39%，达到了最先进的性能水平

Conclusion: 所提出的Met2Net方法通过隐式两阶段训练和自注意力机制有效解决了天气预测中的多变量表示问题，在多个预测任务上取得了显著的性能提升

Abstract: The increasing frequency of extreme weather events due to global climate
change urges accurate weather prediction. Recently, great advances have been
made by the \textbf{end-to-end methods}, thanks to deep learning techniques,
but they face limitations of \textit{representation inconsistency} in
multivariable integration and struggle to effectively capture the dependency
between variables, which is required in complex weather systems. Treating
different variables as distinct modalities and applying a \textbf{two-stage
training approach} from multimodal models can partially alleviate this issue,
but due to the inconformity in training tasks between the two stages, the
results are often suboptimal. To address these challenges, we propose an
implicit two-stage training method, configuring separate encoders and decoders
for each variable. In detailed, in the first stage, the Translator is frozen
while the Encoders and Decoders learn a shared latent space, in the second
stage, the Encoders and Decoders are frozen, and the Translator captures
inter-variable interactions for prediction. Besides, by introducing a
self-attention mechanism for multivariable fusion in the latent space, the
performance achieves further improvements. Empirically, extensive experiments
show the state-of-the-art performance of our method. Specifically, it reduces
the MSE for near-surface air temperature and relative humidity predictions by
28.82\% and 23.39\%, respectively. The source code is available at
https://github.com/ShremG/Met2Net.

</details>


### [61] [Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation](https://arxiv.org/abs/2507.17204)
*Zixuan Wang,Jinghao Shi,Hanzhong Liang,Xiang Shen,Vera Wen,Zhiqian Chen,Yifan Wu,Zhixin Zhang,Hongyu Xiong*

Main category: cs.LG

TL;DR: 本文提出了一种基于多模态大语言模型(MLLM)的视频内容审核系统，通过路由器-排序级联架构实现了高效的工业级部署，在提升审核效果的同时大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统视频分类模型在处理隐式有害内容和上下文歧义等复杂场景时表现不佳，而多模态大语言模型虽然具有优秀的跨模态推理和上下文理解能力，但面临计算成本高和生成模型适配判别分类任务困难的挑战，阻碍了其在工业环境中的应用。

Method: 首先提出了一种高效方法，使用最少的判别训练数据将生成式MLLM转换为多模态分类器；然后设计了路由器-排序级联系统，将MLLM与轻量级路由器模型集成，实现工业级规模的部署。

Result: 离线实验显示，基于MLLM的方法相比传统分类器F1分数提升66.50%，且仅需要2%的微调数据；在线评估表明系统将自动内容审核量提升了41%，而级联部署将计算成本降低至直接全规模部署的1.5%。

Conclusion: 通过创新的级联架构设计，成功解决了MLLM在视频内容审核中的部署难题，实现了效果和效率的双重提升，为视频平台的内容审核提供了实用的解决方案。

Abstract: Effective content moderation is essential for video platforms to safeguard
user experience and uphold community standards. While traditional video
classification models effectively handle well-defined moderation tasks, they
struggle with complicated scenarios such as implicit harmful content and
contextual ambiguity. Multimodal large language models (MLLMs) offer a
promising solution to these limitations with their superior cross-modal
reasoning and contextual understanding. However, two key challenges hinder
their industrial adoption. First, the high computational cost of MLLMs makes
full-scale deployment impractical. Second, adapting generative models for
discriminative classification remains an open research problem. In this paper,
we first introduce an efficient method to transform a generative MLLM into a
multimodal classifier using minimal discriminative training data. To enable
industry-scale deployment, we then propose a router-ranking cascade system that
integrates MLLMs with a lightweight router model. Offline experiments
demonstrate that our MLLM-based approach improves F1 score by 66.50% over
traditional classifiers while requiring only 2% of the fine-tuning data. Online
evaluations show that our system increases automatic content moderation volume
by 41%, while the cascading deployment reduces computational cost to only 1.5%
of direct full-scale deployment.

</details>


### [62] [Dataset Distillation as Data Compression: A Rate-Utility Perspective](https://arxiv.org/abs/2507.17221)
*Youneng Bao,Yiping Liu,Zhuo Chen,Yongsheng Liang,Mu Li,Kede Ma*

Main category: cs.LG

TL;DR: 本文提出了一种联合速率-效用优化的数据集蒸馏方法，通过将合成样本参数化为可优化的潜在编码并使用轻量级网络解码，在保持准确性的同时实现了比标准蒸馏方法高达170倍的压缩率。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习受"规模即一切"范式驱动，需要越来越大的数据集和模型，导致计算和存储需求过高。现有的数据集蒸馏方法要么在固定存储预算下最大化性能，要么追求合适的合成数据表示来去除冗余，但没有联合优化这两个目标。

Method: 提出联合速率-效用优化方法：将合成样本参数化为通过极轻量级网络解码的可优化潜在编码；估计量化潜在变量的香农熵作为速率度量；使用任何现有蒸馏损失作为效用度量；通过拉格朗日乘数进行权衡；引入每类比特数(bpc)作为精确的存储度量。

Result: 在CIFAR-10、CIFAR-100和ImageNet-128上，该方法在相当准确性下实现了比标准蒸馏高达170倍的压缩率。在不同的bpc预算、蒸馏损失和骨干架构下，该方法始终建立了更好的速率-效用权衡。

Conclusion: 提出的联合速率-效用优化方法能够在保持数据集蒸馏效用的同时显著提高压缩效率，为跨方法比较提供了公平的评估指标，并在多种设置下consistently优于现有方法。

Abstract: Driven by the ``scale-is-everything'' paradigm, modern machine learning
increasingly demands ever-larger datasets and models, yielding prohibitive
computational and storage requirements. Dataset distillation mitigates this by
compressing an original dataset into a small set of synthetic samples, while
preserving its full utility. Yet, existing methods either maximize performance
under fixed storage budgets or pursue suitable synthetic data representations
for redundancy removal, without jointly optimizing both objectives. In this
work, we propose a joint rate-utility optimization method for dataset
distillation. We parameterize synthetic samples as optimizable latent codes
decoded by extremely lightweight networks. We estimate the Shannon entropy of
quantized latents as the rate measure and plug any existing distillation loss
as the utility measure, trading them off via a Lagrange multiplier. To enable
fair, cross-method comparisons, we introduce bits per class (bpc), a precise
storage metric that accounts for sample, label, and decoder parameter costs. On
CIFAR-10, CIFAR-100, and ImageNet-128, our method achieves up to $170\times$
greater compression than standard distillation at comparable accuracy. Across
diverse bpc budgets, distillation losses, and backbone architectures, our
approach consistently establishes better rate-utility trade-offs.

</details>


### [63] [DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD](https://arxiv.org/abs/2507.17501)
*Xianbiao Qi,Marco Chen,Wenjie Xiao,Jiaquan Ye,Yelin He,Chun-Guang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出了深度归一化Transformer（DNT），通过在关键位置策略性地集成归一化技术，使得Transformer能够使用vanilla mSGDW优化器进行训练，而不需要AdamW等自适应学习率优化器，同时保持相当的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型通常需要AdamW等自适应学习率优化器才能有效训练，而无法使用更简单的动量SGD（mSGDW）优化器，这主要是由于梯度的重尾分布特性造成的。研究者希望设计一种能够使用vanilla mSGDW训练的Transformer架构。

Method: 设计了深度归一化Transformer（DNT），在Transformer的适当位置策略性地集成归一化技术。这种设计能够有效调节每层的雅可比矩阵，平衡权重、激活及其交互的影响，从而使梯度分布更加集中，克服重尾分布问题。

Result: 在两种流行的Transformer架构（ViT和GPT）上进行了广泛的实验评估，结果表明：1）DNT的性能优于对应的基线模型；2）DNT可以使用vanilla mSGDW有效训练，达到与使用AdamW训练的Transformer相当的性能。

Conclusion: 通过精心设计的归一化策略，DNT成功解决了Transformer训练中对自适应优化器的依赖问题，实现了使用简单的动量SGD进行有效训练，为Transformer的优化提供了新的解决方案。该工作既有理论支撑又有实验验证，证明了方法的有效性。

Abstract: Transformers have become the de facto backbone of modern deep learning, yet
their training typically demands an advanced optimizer with adaptive learning
rate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that
it is mainly due to a heavy-tailed distribution of the gradients. In this
paper, we introduce a Deeply Normalized Transformer (DNT), which is
meticulously engineered to overcome this limitation enabling seamless training
with vanilla mSGDW while yielding comparable performance to the Transformers
trained via AdamW. To be specific, in DNT, we strategically integrate
normalization techniques at proper positions in the Transformers to effectively
modulate the Jacobian matrices of each layer, balance the influence of weights,
activations, and their interactions, and thus enable the distributions of
gradients concentrated. We provide both theoretical justifications of the
normalization technique used in our DNT and extensive empirical evaluation on
two popular Transformer architectures to validate that: a) DNT outperforms its
counterparts (\ie, ViT and GPT), and b) DNT can be effectively trained with
vanilla mSGDW.

</details>


### [64] [P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices](https://arxiv.org/abs/2507.17228)
*Wei Fan,JinYi Yoon,Xiaochang Li,Huajie Shao,Bo Ji*

Main category: cs.LG

TL;DR: 本文提出P3SL框架，一个面向异构资源受限边缘设备的个性化隐私保护分割学习方案，通过双层优化技术让客户端自主确定最优分割点，在保护隐私的同时平衡能耗和模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有的分割学习方法在异构环境中面临挑战：忽视了个性化隐私需求和本地模型定制化，无法适应设备间计算资源、通信能力、环境条件和隐私要求的差异。

Method: 设计了P3SL个性化隐私保护分割学习框架，包含两个核心组件：1）个性化序列分割学习管道，允许每个客户端实现定制化隐私保护和个性化本地模型；2）双层优化技术，使客户端能够在不向服务器共享敏感信息的情况下确定最优个性化分割点。

Result: 在包含7个设备（4个Jetson Nano P3450、2个树莓派、1个笔记本电脑）的测试平台上进行实验验证，使用多种模型架构和数据集，在不同环境条件下测试了框架的有效性。

Conclusion: P3SL框架成功解决了异构边缘设备环境中分割学习的个性化隐私保护问题，通过双层优化实现了能耗、隐私泄露风险和模型精度之间的有效平衡，为资源受限的边缘设备提供了实用的隐私保护机器学习解决方案。

Abstract: Split Learning (SL) is an emerging privacy-preserving machine learning
technique that enables resource constrained edge devices to participate in
model training by partitioning a model into client-side and server-side
sub-models. While SL reduces computational overhead on edge devices, it
encounters significant challenges in heterogeneous environments where devices
vary in computing resources, communication capabilities, environmental
conditions, and privacy requirements. Although recent studies have explored
heterogeneous SL frameworks that optimize split points for devices with varying
resource constraints, they often neglect personalized privacy requirements and
local model customization under varying environmental conditions. To address
these limitations, we propose P3SL, a Personalized Privacy-Preserving Split
Learning framework designed for heterogeneous, resource-constrained edge device
systems. The key contributions of this work are twofold. First, we design a
personalized sequential split learning pipeline that allows each client to
achieve customized privacy protection and maintain personalized local models
tailored to their computational resources, environmental conditions, and
privacy needs. Second, we adopt a bi-level optimization technique that empowers
clients to determine their own optimal personalized split points without
sharing private sensitive information (i.e., computational resources,
environmental conditions, privacy requirements) with the server. This approach
balances energy consumption and privacy leakage risks while maintaining high
model accuracy. We implement and evaluate P3SL on a testbed consisting of 7
devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,
using diverse model architectures and datasets under varying environmental
conditions.

</details>


### [65] [Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains](https://arxiv.org/abs/2507.17746)
*Anisha Gunjal,Anthony Wang,Elaine Lau,Vaskar Nath,Bing Liu,Sean Hendryx*

Main category: cs.LG

TL;DR: 本文提出了"Rubrics as Rewards"(RaR)框架，使用结构化的清单式评分标准作为可解释的奖励信号来训练语言模型，在HealthBench-1k上相比传统方法取得了28%的相对性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务往往缺乏单一明确的真实标准，难以为训练后的语言模型定义可靠的奖励信号。传统的基于偏好的方法依赖于不透明且难以解释的奖励函数，容易产生虚假关联。因此需要一种更可解释、更可靠的奖励信号设计方法。

Method: 提出"Rubrics as Rewards"(RaR)框架，使用结构化的清单式评分标准作为可解释的奖励信号，结合GRPO进行在线策略训练。该方法将评分标准视为结构化的奖励信号，使较小规模的判断模型能够更好地与人类偏好对齐。

Result: 在HealthBench-1k数据集上，最佳RaR方法相比简单的李克特量表方法取得了高达28%的相对性能提升，同时匹配或超越了基于专家撰写参考答案的奖励信号性能。该方法在不同模型规模上都能保持稳健的性能表现。

Conclusion: RaR框架通过使用结构化评分标准作为奖励信号，成功解决了现实世界任务中奖励信号定义困难的问题，提供了一种更可解释、更有效的语言模型训练方法，并证明了较小规模判断模型在该框架下能够实现更好的人类偏好对齐。

Abstract: Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world
tasks often requires balancing objective and subjective evaluation criteria.
However, many such tasks lack a single, unambiguous ground truth-making it
difficult to define reliable reward signals for post-training language models.
While traditional preference-based methods offer a workaround, they rely on
opaque reward functions that are difficult to interpret and prone to spurious
correlations. We introduce $\textbf{Rubrics as Rewards}$ (RaR), a framework
that uses structured, checklist-style rubrics as interpretable reward signals
for on-policy training with GRPO. Our best RaR method yields up to a $28\%$
relative improvement on HealthBench-1k compared to simple Likert-based
approaches, while matching or surpassing the performance of reward signals
derived from expert-written references. By treating rubrics as structured
reward signals, we show that RaR enables smaller-scale judge models to better
align with human preferences and sustain robust performance across model
scales.

</details>


### [66] [Eco-Friendly AI: Unleashing Data Power for Green Federated Learning](https://arxiv.org/abs/2507.17241)
*Mattia Sabella,Monica Vitali*

Main category: cs.LG

TL;DR: 本文提出了一种以数据为中心的绿色联邦学习方法，通过减少训练数据量来降低AI模型训练的环境影响，开发了一个交互式推荐系统来优化联邦学习配置并最小化碳排放。


<details>
  <summary>Details</summary>
Motivation: AI和机器学习的广泛应用带来了显著的环境影响，特别是在能耗和碳排放方面。虽然联邦学习可以减少数据传输成本并增强隐私保护，但数据源的异构性、计算节点能力差异和环境影响仍是挑战。需要创新解决方案来减轻AI的生态足迹。

Method: 采用以数据为中心的绿色联邦学习方法，包括：1）分析联邦数据集特征；2）基于质量指标选择最优数据子集；3）选择环境影响最低的联邦节点；4）开发综合方法论检验数据质量和数量对FL训练性能和碳排放的影响；5）构建交互式推荐系统优化FL配置。

Result: 将该方法应用于时间序列分类任务，在减少联邦学习任务的环境影响方面展现了有希望的结果。通过数据减少策略成功降低了训练过程中的环境影响。

Conclusion: 提出的以数据为中心的绿色联邦学习方法能够有效减少AI模型训练的环境足迹。通过优化数据选择和节点配置，可以在保持模型性能的同时显著降低碳排放，为推进绿色AI发展提供了实用的解决方案。

Abstract: The widespread adoption of Artificial Intelligence (AI) and Machine Learning
(ML) comes with a significant environmental impact, particularly in terms of
energy consumption and carbon emissions. This pressing issue highlights the
need for innovative solutions to mitigate AI's ecological footprint. One of the
key factors influencing the energy consumption of ML model training is the size
of the training dataset. ML models are often trained on vast amounts of data
continuously generated by sensors and devices distributed across multiple
locations. To reduce data transmission costs and enhance privacy, Federated
Learning (FL) enables model training without the need to move or share raw
data. While FL offers these advantages, it also introduces challenges due to
the heterogeneity of data sources (related to volume and quality),
computational node capabilities, and environmental impact.
  This paper contributes to the advancement of Green AI by proposing a
data-centric approach to Green Federated Learning. Specifically, we focus on
reducing FL's environmental impact by minimizing the volume of training data.
Our methodology involves the analysis of the characteristics of federated
datasets, the selecting of an optimal subset of data based on quality metrics,
and the choice of the federated nodes with the lowest environmental impact. We
develop a comprehensive methodology that examines the influence of data-centric
factors, such as data quality and volume, on FL training performance and carbon
emissions. Building on these insights, we introduce an interactive
recommendation system that optimizes FL configurations through data reduction,
minimizing environmental impact during training. Applying this methodology to
time series classification has demonstrated promising results in reducing the
environmental impact of FL tasks.

</details>


### [67] [DistrAttention: An Efficient and Flexible Self-Attention Mechanism on Modern GPUs](https://arxiv.org/abs/2507.17245)
*Haolin Jin,Mengbai Xiao,Yuan Yuan,Xiao Zhang,Dongxiao Yu,Guanghui Zhang,Haoliang Wang*

Main category: cs.LG

TL;DR: 该论文提出了DistrAttention，一种高效且灵活的自注意力机制，通过在嵌入维度上分组数据来保持完整上下文信息，同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中的自注意力机制具有相对于输入序列长度的二次时间复杂度，这阻碍了Transformer的可扩展性。现有的自注意力优化方法要么丢弃完整的上下文信息，要么缺乏灵活性。

Method: 设计了DistrAttention机制，通过在嵌入维度d上对数据进行分组来实现高效且灵活的自注意力。使用轻量级的采样和融合方法，利用局部敏感哈希来分组相似数据。进一步设计了分块分组框架来限制局部敏感哈希引入的错误。通过优化分块大小的选择，使DistrAttention能够轻松与FlashAttention-2集成。

Result: DistrAttention在计算自注意力时比FlashAttention-2快37%。在ViT推理中，DistrAttention在近似自注意力机制中速度最快且精度最高。在Llama3-1B中，DistrAttention仍能实现最低的推理时间，仅有1%的精度损失。

Conclusion: DistrAttention成功解决了传统自注意力机制的计算效率问题，在保持完整上下文信息的同时显著提升了计算速度，为Transformer架构的可扩展性提供了有效解决方案。

Abstract: The Transformer architecture has revolutionized deep learning, delivering the
state-of-the-art performance in areas such as natural language processing,
computer vision, and time series prediction. However, its core component,
self-attention, has the quadratic time complexity relative to input sequence
length, which hinders the scalability of Transformers. The exsiting approaches
on optimizing self-attention either discard full-contextual information or lack
of flexibility. In this work, we design DistrAttention, an effcient and
flexible self-attention mechanism with the full context. DistrAttention
achieves this by grouping data on the embedding dimensionality, usually
referred to as $d$. We realize DistrAttention with a lightweight sampling and
fusion method that exploits locality-sensitive hashing to group similar data. A
block-wise grouping framework is further designed to limit the errors
introduced by locality sensitive hashing. By optimizing the selection of block
sizes, DistrAttention could be easily integrated with FlashAttention-2, gaining
high-performance on modern GPUs. We evaluate DistrAttention with extensive
experiments. The results show that our method is 37% faster than
FlashAttention-2 on calculating self-attention. In ViT inference,
DistrAttention is the fastest and the most accurate among approximate
self-attention mechanisms. In Llama3-1B, DistrAttention still achieves the
lowest inference time with only 1% accuray loss.

</details>


### [68] [Rethinking VAE: From Continuous to Discrete Representations Without Probabilistic Assumptions](https://arxiv.org/abs/2507.17255)
*Songxuan Shi*

Main category: cs.LG

TL;DR: 本文探索了自编码器的生成能力，提出了一种新的VAE训练方法来解决编码空间未定义区域的问题，并揭示了VAE和VQ-VAE之间的内在联系


<details>
  <summary>Details</summary>
Motivation: 传统自编码器虽然具有通过潜在空间插值和扰动进行生成的潜力，但受到编码空间中未定义区域的限制，需要找到一种方法来确保潜在空间的良好定义并增强数据紧密性

Method: 提出了一种类似VAE的新训练方法，引入聚类中心来增强数据紧密性并确保潜在空间的良好定义，无需依赖传统的KL散度或重参数化技术。将该方法扩展到多个可学习向量，观察其向连续空间中VQ-VAE模型的自然演进

Result: 在MNIST、CelebA和FashionMNIST数据集上的实验结果显示平滑的插值过渡，但仍存在模糊问题。当编码器输出多个向量时，模型退化为离散自编码器(VQ-AE)，只能组合图像片段而无法学习语义表示

Conclusion: 研究强调了编码空间紧密性和分散性在生成建模中的关键作用，揭示了VAE和VQ-VAE之间的内在联系，为其设计和局限性提供了新的视角和见解

Abstract: This paper explores the generative capabilities of Autoencoders (AEs) and
establishes connections between Variational Autoencoders (VAEs) and Vector
Quantized-Variational Autoencoders (VQ-VAEs) through a reformulated training
framework. We demonstrate that AEs exhibit generative potential via latent
space interpolation and perturbation, albeit limited by undefined regions in
the encoding space. To address this, we propose a new VAE-like training method
that introduces clustering centers to enhance data compactness and ensure
well-defined latent spaces without relying on traditional KL divergence or
reparameterization techniques. Experimental results on MNIST, CelebA, and
FashionMNIST datasets show smooth interpolative transitions, though blurriness
persists. Extending this approach to multiple learnable vectors, we observe a
natural progression toward a VQ-VAE-like model in continuous space. However,
when the encoder outputs multiple vectors, the model degenerates into a
discrete Autoencoder (VQ-AE), which combines image fragments without learning
semantic representations. Our findings highlight the critical role of encoding
space compactness and dispersion in generative modeling and provide insights
into the intrinsic connections between VAEs and VQ-VAEs, offering a new
perspective on their design and limitations.

</details>


### [69] [Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational Bottlenecks for Warehouse Planning Assistance](https://arxiv.org/abs/2507.17273)
*Rishi Parekh,Saisubramaniam Gopalakrishnan,Zishan Ahmad,Anirudh Deodhar*

Main category: cs.LG

TL;DR: 该研究提出了一个结合知识图谱(KG)和大语言模型(LLM)的框架，用于分析仓库离散事件仿真(DES)的复杂输出数据，以识别瓶颈和低效问题。通过将原始DES数据转换为语义丰富的知识图谱，并利用LLM代理进行迭代推理和自我纠错，实现了对仓库运营问题的自动化诊断。


<details>
  <summary>Details</summary>
Motivation: 分析大型复杂的仓库离散事件仿真输出数据集以识别瓶颈和低效问题是一项关键但具有挑战性的任务，通常需要大量的人工努力或专门的分析工具。现有方法在处理复杂仿真数据时缺乏自动化和智能化的解决方案。

Method: 构建了一个集成知识图谱和大语言模型代理的框架：1) 将原始DES数据转换为语义丰富的知识图谱，捕获仿真事件和实体之间的关系；2) 基于LLM的代理使用迭代推理，生成相互依赖的子问题；3) 为每个子问题创建Cypher查询与知识图谱交互，提取信息并进行自我反思以纠正错误；4) 通过这种自适应、迭代和自我纠错的过程来识别运营问题。

Result: 在设备故障和流程异常的测试中，该方法在仓库瓶颈识别方面优于基线方法。对于运营问题，在精确定位低效问题方面达到了近乎完美的通过率。对于复杂的调查性问题，展示了其在发现微妙、相互关联问题方面的卓越诊断能力。

Conclusion: 该工作连接了仿真建模和人工智能(知识图谱+大语言模型)，提供了一种更直观的方法来获得可操作的洞察，减少了洞察时间，并实现了自动化的仓库低效评估和诊断。这为仓库运营分析提供了新的智能化解决方案。

Abstract: Analyzing large, complex output datasets from Discrete Event Simulations
(DES) of warehouse operations to identify bottlenecks and inefficiencies is a
critical yet challenging task, often demanding significant manual effort or
specialized analytical tools. Our framework integrates Knowledge Graphs (KGs)
and Large Language Model (LLM)-based agents to analyze complex Discrete Event
Simulation (DES) output data from warehouse operations. It transforms raw DES
data into a semantically rich KG, capturing relationships between simulation
events and entities. An LLM-based agent uses iterative reasoning, generating
interdependent sub-questions. For each sub-question, it creates Cypher queries
for KG interaction, extracts information, and self-reflects to correct errors.
This adaptive, iterative, and self-correcting process identifies operational
issues mimicking human analysis. Our DES approach for warehouse bottleneck
identification, tested with equipment breakdowns and process irregularities,
outperforms baseline methods. For operational questions, it achieves
near-perfect pass rates in pinpointing inefficiencies. For complex
investigative questions, we demonstrate its superior diagnostic ability to
uncover subtle, interconnected issues. This work bridges simulation modeling
and AI (KG+LLM), offering a more intuitive method for actionable insights,
reducing time-to-insight, and enabling automated warehouse inefficiency
evaluation and diagnosis.

</details>


### [70] [Decentralized Federated Learning of Probabilistic Generative Classifiers](https://arxiv.org/abs/2507.17285)
*Aritz Pérez,Carlos Echegoyen,Guzmán Santafé*

Main category: cs.LG

TL;DR: 本文提出了一种去中心化联邦学习方法，通过节点间共享局部统计信息来协作学习概率生成分类器，无需中央服务器，在各种网络拓扑和非独立同分布数据下都能收敛到全局竞争模型。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习主要依赖中央服务器，而去中心化架构中用户可以直接协作更新全局模型，无需共享私有数据，但缺乏有效的概率生成分类器学习方法。

Method: 提出基于通信网络的框架，每个节点拥有本地数据和局部更新规则，通过与邻居节点共享局部统计信息，各节点聚合邻居信息并迭代学习自己的局部分类器，逐步收敛到全局模型。

Result: 广泛实验表明该算法在多种网络拓扑、网络规模、本地数据集大小以及极端非独立同分布数据分布下都能一致地收敛到全局竞争模型。

Conclusion: 所提出的去中心化联邦学习方法能够有效地在异构用户网络中协作学习概率生成分类器，在保护数据隐私的同时实现了良好的全局模型性能。

Abstract: Federated learning is a paradigm of increasing relevance in real world
applications, aimed at building a global model across a network of
heterogeneous users without requiring the sharing of private data. We focus on
model learning over decentralized architectures, where users collaborate
directly to update the global model without relying on a central server. In
this context, the current paper proposes a novel approach to collaboratively
learn probabilistic generative classifiers with a parametric form. The
framework is composed by a communication network over a set of local nodes,
each of one having its own local data, and a local updating rule. The proposal
involves sharing local statistics with neighboring nodes, where each node
aggregates the neighbors' information and iteratively learns its own local
classifier, which progressively converges to a global model. Extensive
experiments demonstrate that the algorithm consistently converges to a globally
competitive model across a wide range of network topologies, network sizes,
local dataset sizes, and extreme non-i.i.d. data distributions.

</details>


### [71] [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)
*Zhuokun Chen,Zeren Chen,Jiahao He,Mingkui Tan,Jianfei Cai,Bohan Zhuang*

Main category: cs.LG

TL;DR: R-Stitch是一个基于置信度的混合解码框架，通过在小语言模型和大语言模型之间智能切换来加速思维链推理，在保持准确性的同时实现高达85%的推理延迟减少。


<details>
  <summary>Details</summary>
Motivation: 思维链(CoT)推理虽然能提升大语言模型的问题解决能力，但由于需要自回归解码长token序列而带来巨大的计算开销。现有的加速策略存在局限性：早停或压缩奖励设计会减少序列长度，而投机解码在大小模型一致性较低时加速效果有限，且无法充分利用小模型生成简洁中间推理的潜在优势。

Method: 提出R-Stitch框架，这是一个token级别、基于置信度的混合解码方法。系统默认使用小语言模型(SLM)生成token，只有当SLM的置信度低于阈值时才切换到大语言模型(LLM)。这种设计避免了全序列回滚，选择性地在不确定步骤调用LLM，是模型无关、无需训练且与标准解码管道兼容的方法。

Result: 在数学推理基准测试中，R-Stitch实现了高达85%的推理延迟减少，同时准确性下降可忽略不计，展现了在加速CoT推理方面的实用效果。

Conclusion: R-Stitch成功解决了思维链推理中的计算效率问题，通过智能的模型切换策略在保持答案质量的同时显著提升了推理速度，为大语言模型的实际应用提供了有效的加速解决方案。

Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of
large language models by encouraging step-by-step intermediate reasoning during
inference. While effective, CoT introduces substantial computational overhead
due to its reliance on autoregressive decoding over long token sequences.
Existing acceleration strategies either reduce sequence length through early
stopping or compressive reward designs, or improve decoding speed via
speculative decoding with smaller models. However, speculative decoding suffers
from limited speedup when the agreement between small and large models is low,
and fails to exploit the potential advantages of small models in producing
concise intermediate reasoning. In this paper, we present R-Stitch, a
token-level, confidence-based hybrid decoding framework that accelerates CoT
inference by switching between a small language model (SLM) and a large
language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to
generate tokens by default and delegates to the LLM only when the SLM's
confidence falls below a threshold. This design avoids full-sequence rollback
and selectively invokes the LLM on uncertain steps, preserving both efficiency
and answer quality. R-Stitch is model-agnostic, training-free, and compatible
with standard decoding pipelines. Experiments on math reasoning benchmarks
demonstrate that R-Stitch achieves up to 85\% reduction in inference latency
with negligible accuracy drop, highlighting its practical effectiveness in
accelerating CoT reasoning.

</details>


### [72] [Confounded Causal Imitation Learning with Instrumental Variables](https://arxiv.org/abs/2507.17309)
*Yan Zeng,Shenglan Nie,Feng Xie,Libo Huang,Peng Wu,Zhi Geng*

Main category: cs.LG

TL;DR: 本文提出了一个混淆因果模仿学习(C2L)模型，通过工具变量方法解决模仿学习中未测量混淆变量导致的偏差问题，包含两阶段框架：有效工具变量识别和策略优化。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习受到未测量混淆变量的影响，这些变量会对状态和动作产生混淆效应，如果忽略这些因素会导致策略估计的偏差。现有方法通常只考虑即时的时间依赖关系，无法处理跨多个时间步影响动作的混淆因子。

Method: 提出混淆因果模仿学习(C2L)模型，利用工具变量(IV)的强大能力。开发了两阶段模仿学习框架：第一阶段基于定义的伪变量构建测试准则来识别有效的工具变量，该准则包含工具变量有效性的充分必要条件；第二阶段利用识别出的工具变量提出两种候选策略学习方法，一种基于模拟器，另一种是离线方法。

Result: 大量实验验证了有效工具变量识别的有效性以及策略学习的效果。实验结果表明该方法能够成功处理跨多个时间步的混淆因子影响，并实现有效的策略学习。

Conclusion: C2L模型通过工具变量方法成功解决了模仿学习中未测量混淆变量的问题，特别是能够处理跨时间步的混淆效应。两阶段框架提供了理论保证的工具变量识别方法和有效的策略优化方案，为因果模仿学习提供了新的解决思路。

Abstract: Imitation learning from demonstrations usually suffers from the confounding
effects of unmeasured variables (i.e., unmeasured confounders) on the states
and actions. If ignoring them, a biased estimation of the policy would be
entailed. To break up this confounding gap, in this paper, we take the best of
the strong power of instrumental variables (IV) and propose a Confounded Causal
Imitation Learning (C2L) model. This model accommodates confounders that
influence actions across multiple timesteps, rather than being restricted to
immediate temporal dependencies. We develop a two-stage imitation learning
framework for valid IV identification and policy optimization. In particular,
in the first stage, we construct a testing criterion based on the defined
pseudo-variable, with which we achieve identifying a valid IV for the C2L
models. Such a criterion entails the sufficient and necessary identifiability
conditions for IV validity. In the second stage, with the identified IV, we
propose two candidate policy learning approaches: one is based on a simulator,
while the other is offline. Extensive experiments verified the effectiveness of
identifying the valid IV as well as learning the policy.

</details>


### [73] [EarthLink: Interpreting Climate Signals with Self-Evolving AI Agents](https://arxiv.org/abs/2507.17311)
*Zijie Guo,Jiong Wang,Xiaoyu Yue,Wangxu Wei,Zhe Jiang,Wanghan Xu,Ben Fei,Wenlong Zhang,Xinyu Gu,Lijing Cheng,Jing-Jia Luo,Chao Li,Yaqiang Wang,Tao Chen,Wanli Ouyang,Fenghua Ling,Lei Bai*

Main category: cs.LG

TL;DR: EarthLink是首个专为地球科学家设计的AI代理，能够自动化端到端的研究工作流程，从规划到代码生成再到多场景分析，通过动态反馈循环持续学习和改进能力。


<details>
  <summary>Details</summary>
Motivation: 现代地球科学面临拐点，地球系统数据庞大、分散且复杂，加上日益复杂的分析需求，为快速科学发现创造了重大瓶颈。需要一个能够自动化研究工作流程的AI工具来解决这一问题。

Method: 开发了EarthLink，一个交互式AI代理，具备以下特性：1)自动化端到端研究工作流程；2)从规划到代码生成到多场景分析的全流程覆盖；3)通过动态反馈循环从用户交互中学习；4)透明、可审计的工作流程；5)自然语言界面。

Result: 在多项气候变化核心科学任务上验证了性能，包括模型-观测比较和复杂现象诊断。多专家评估显示EarthLink能产生科学上合理的分析，其分析能力被评估为可与人类初级研究员工作流程的特定方面相媲美。

Conclusion: EarthLink标志着地球系统研究向高效、可信和协作范式迈出的关键一步，使科学家能够从繁重的手动执行转向战略监督和假设生成，在全球变化加速的时代具有重要意义。

Abstract: Modern Earth science is at an inflection point. The vast, fragmented, and
complex nature of Earth system data, coupled with increasingly sophisticated
analytical demands, creates a significant bottleneck for rapid scientific
discovery. Here we introduce EarthLink, the first AI agent designed as an
interactive copilot for Earth scientists. It automates the end-to-end research
workflow, from planning and code generation to multi-scenario analysis. Unlike
static diagnostic tools, EarthLink can learn from user interaction,
continuously refining its capabilities through a dynamic feedback loop. We
validated its performance on a number of core scientific tasks of climate
change, ranging from model-observation comparisons to the diagnosis of complex
phenomena. In a multi-expert evaluation, EarthLink produced scientifically
sound analyses and demonstrated an analytical competency that was rated as
comparable to specific aspects of a human junior researcher's workflow.
Additionally, its transparent, auditable workflows and natural language
interface empower scientists to shift from laborious manual execution to
strategic oversight and hypothesis generation. EarthLink marks a pivotal step
towards an efficient, trustworthy, and collaborative paradigm for Earth system
research in an era of accelerating global change.

</details>


### [74] [A Learning-based Domain Decomposition Method](https://arxiv.org/abs/2507.17328)
*Rui Wu,Nikola Kovachki,Burigede Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于学习的域分解方法(L-DDM)，通过在域分解框架中使用预训练的神经算子作为代理模型，有效解决了复杂几何形状中大规模偏微分方程的求解问题


<details>
  <summary>Details</summary>
Motivation: 传统数值方法如有限元法在处理大规模复杂几何问题时存在计算成本高和可扩展性差的问题，而现有神经网络方法主要局限于简单域，难以应用于涉及复杂几何的实际偏微分方程问题

Method: 提出学习型域分解方法(L-DDM)，使用在简单域上预训练的单个神经算子作为代理模型，结合域分解策略来处理大规模复杂域问题；提供了域分解求解抽象偏微分方程中神经算子近似存在性的一般理论结果

Result: 在具有不连续微结构的复杂几何椭圆偏微分方程上验证了方法的有效性，使用物理预训练神经算子(PPNO)取得了优于现有最先进方法的性能，同时展现出分辨率不变性和对训练中未见微结构模式的强泛化能力

Conclusion: 所提出的L-DDM方法成功地将预训练神经算子与域分解技术结合，为复杂几何中的大规模偏微分方程求解提供了高效且具有强泛化能力的解决方案，克服了传统方法的计算限制和现有神经网络方法的域限制

Abstract: Recent developments in mechanical, aerospace, and structural engineering have
driven a growing need for efficient ways to model and analyse structures at
much larger and more complex scales than before. While established numerical
methods like the Finite Element Method remain reliable, they often struggle
with computational cost and scalability when dealing with large and
geometrically intricate problems. In recent years, neural network-based methods
have shown promise because of their ability to efficiently approximate
nonlinear mappings. However, most existing neural approaches are still largely
limited to simple domains, which makes it difficult to apply to real-world PDEs
involving complex geometries. In this paper, we propose a learning-based domain
decomposition method (L-DDM) that addresses this gap. Our approach uses a
single, pre-trained neural operator-originally trained on simple domains-as a
surrogate model within a domain decomposition scheme, allowing us to tackle
large and complicated domains efficiently. We provide a general theoretical
result on the existence of neural operator approximations in the context of
domain decomposition solution of abstract PDEs. We then demonstrate our method
by accurately approximating solutions to elliptic PDEs with discontinuous
microstructures in complex geometries, using a physics-pretrained neural
operator (PPNO). Our results show that this approach not only outperforms
current state-of-the-art methods on these challenging problems, but also offers
resolution-invariance and strong generalization to microstructural patterns
unseen during training.

</details>


### [75] [DeCo-SGD: Joint Optimization of Delay Staleness and Gradient Compression Ratio for Distributed SGD](https://arxiv.org/abs/2507.17346)
*Rongwei Lu,Jingyan Jiang,Chunyang Li,Haotian Dong,Xingguang Wei,Delin Cai,Zhi Wang*

Main category: cs.LG

TL;DR: 该论文提出了DeCo-SGD算法，通过动态调整梯度压缩比和延迟同步步数来优化分布式机器学习在高延迟、低带宽网络环境下的训练效率，相比传统方法实现了最高5.07倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有分布式SGD在高延迟、低带宽网络环境下性能严重下降。虽然梯度压缩和延迟聚合可以分别缓解低带宽和高延迟问题，但同时使用这两种策略会产生压缩比、延迟步数和模型收敛率之间复杂的三方权衡。现有方法缺乏理论指导，只能使用静态启发式策略，无法在变化的网络条件下实现最优平衡。

Method: 论文引入了新的理论分析工具，将联合优化问题分解为传统收敛率分析和多个可分析的噪声项。基于理论分析揭示延迟会指数级放大梯度压缩对训练性能的负面影响，并结合网络感知的时间最小化条件，提出DeCo-SGD算法，能够根据实时网络状况和训练任务动态调整压缩比和延迟步数。

Result: DeCo-SGD在高延迟网络环境下相比分布式SGD实现了最高5.07倍的加速，在低带宽变化网络环境下相比静态策略实现了1.37倍的加速。实验验证了该方法在不同网络条件下的有效性。

Conclusion: 论文首次从理论上揭示了延迟对梯度压缩负面影响的指数级放大效应，填补了理解压缩延迟梯度如何影响训练的关键空白。提出的DeCo-SGD算法通过动态自适应策略有效解决了分布式机器学习在恶劣网络环境下的性能问题，为实际部署提供了理论指导和实用解决方案。

Abstract: Distributed machine learning in high end-to-end latency and low, varying
bandwidth network environments undergoes severe throughput degradation. Due to
its low communication requirements, distributed SGD (D-SGD) remains the
mainstream optimizer in such challenging networks, but it still suffers from
significant throughput reduction. To mitigate these limitations, existing
approaches typically employ gradient compression and delayed aggregation to
alleviate low bandwidth and high latency, respectively. To address both
challenges simultaneously, these strategies are often combined, introducing a
complex three-way trade-off among compression ratio, staleness (delayed
synchronization steps), and model convergence rate. To achieve the balance
under varying bandwidth conditions, an adaptive policy is required to
dynamically adjust these parameters. Unfortunately, existing works rely on
static heuristic strategies due to the lack of theoretical guidance, which
prevents them from achieving this goal. This study fills in this theoretical
gap by introducing a new theoretical tool, decomposing the joint optimization
problem into a traditional convergence rate analysis with multiple analyzable
noise terms. We are the first to reveal that staleness exponentially amplifies
the negative impact of gradient compression on training performance, filling a
critical gap in understanding how compressed and delayed gradients affect
training. Furthermore, by integrating the convergence rate with a network-aware
time minimization condition, we propose DeCo-SGD, which dynamically adjusts the
compression ratio and staleness based on the real-time network condition and
training task. DeCo-SGD achieves up to 5.07 and 1.37 speed-ups over D-SGD and
static strategy in high-latency and low, varying bandwidth networks,
respectively.

</details>


### [76] [TOC-UCO: a comprehensive repository of tabular ordinal classification datasets](https://arxiv.org/abs/2507.17348)
*Rafael Ayllón-Gavilán,David Guijo-Rubio,Antonio Manuel Gómez-Orellana,David Guijo-Rubio,Francisco Bérchez-Moreno,Víctor Manuel Vargas-Yun,Pedro A. Gutiérrez*

Main category: cs.LG

TL;DR: 科尔多瓦大学(UCO)构建了一个包含46个表格有序分类数据集的公开仓库TOC-UCO，用于有序分类方法的基准测试和验证


<details>
  <summary>Details</summary>
Motivation: 有序分类(OC)领域缺乏全面的数据集集合来对新方法进行基准测试，这阻碍了该领域的发展。现有的有序分类方法需要在标准化的数据集上进行鲁棒验证

Method: 构建TOC-UCO仓库：收集46个表格有序数据集，在统一框架下进行预处理，确保合理的样本数量和适当的类别分布。提供每个数据集的来源和预处理步骤，以及30个不同随机化训练-测试分割的索引

Result: 成功构建了包含46个预处理表格有序分类数据集的公开仓库TOC-UCO，每个数据集都具有合理的样本数量和类别分布，并提供了标准化的基准测试框架

Conclusion: TOC-UCO仓库为有序分类领域提供了标准化的基准测试平台，通过提供预处理数据集、详细文档和标准化分割索引，促进了新方法的可重现验证和比较

Abstract: An ordinal classification (OC) problem corresponds to a special type of
classification characterised by the presence of a natural order relationship
among the classes. This type of problem can be found in a number of real-world
applications, motivating the design and development of many ordinal
methodologies over the last years. However, it is important to highlight that
the development of the OC field suffers from one main disadvantage: the lack of
a comprehensive set of datasets on which novel approaches to the literature
have to be benchmarked. In order to approach this objective, this manuscript
from the University of C\'ordoba (UCO), which have previous experience on the
OC field, provides the literature with a publicly available repository of
tabular data for a robust validation of novel OC approaches, namely TOC-UCO
(Tabular Ordinal Classification repository of the UCO). Specifically, this
repository includes a set of $46$ tabular ordinal datasets, preprocessed under
a common framework and ensured to have a reasonable number of patterns and an
appropriate class distribution. We also provide the sources and preprocessing
steps of each dataset, along with details on how to benchmark a novel approach
using the TOC-UCO repository. For this, indices for $30$ different randomised
train-test partitions are provided to facilitate the reproducibility of the
experiments.

</details>


### [77] [DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2507.17365)
*Chuzhan Hao,Wenfeng Feng,Yuewei Zhang,Hao Wang*

Main category: cs.LG

TL;DR: 本文提出DynaSearcher，一个结合动态知识图谱和多奖励强化学习的搜索代理，解决现有多步骤检索系统中事实不一致查询和低效搜索路径的问题，在多跳问答任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多步骤代理检索系统在复杂信息搜索任务中表现出色，但仍面临生成事实不一致的中间查询和低效搜索轨迹的挑战，这会导致推理偏差或冗余计算。

Method: 提出DynaSearcher系统，利用知识图谱作为外部结构化知识指导搜索过程，通过显式建模实体关系确保中间查询的事实一致性；采用多奖励强化学习框架对检索准确性、效率和响应质量等训练目标进行细粒度控制。

Result: 在六个多跳问答数据集上达到最先进的答案准确性，仅使用小规模模型和有限计算资源就能匹配前沿大语言模型的性能；在不同检索环境和大规模模型上展现出强泛化性和鲁棒性。

Conclusion: DynaSearcher通过结合动态知识图谱和多奖励强化学习，有效解决了多步骤检索系统的关键挑战，在保证高质量答案生成的同时提高了搜索效率，展现出广泛的适用性。

Abstract: Multi-step agentic retrieval systems based on large language models (LLMs)
have demonstrated remarkable performance in complex information search tasks.
However, these systems still face significant challenges in practical
applications, particularly in generating factually inconsistent intermediate
queries and inefficient search trajectories, which can lead to reasoning
deviations or redundant computations. To address these issues, we propose
DynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs
and multi-reward reinforcement learning (RL). Specifically, our system
leverages knowledge graphs as external structured knowledge to guide the search
process by explicitly modeling entity relationships, thereby ensuring factual
consistency in intermediate queries and mitigating biases from irrelevant
information. Furthermore, we employ a multi-reward RL framework for
fine-grained control over training objectives such as retrieval accuracy,
efficiency, and response quality. This framework promotes the generation of
high-quality intermediate queries and comprehensive final answers, while
discouraging unnecessary exploration and minimizing information omissions or
redundancy. Experimental results demonstrate that our approach achieves
state-of-the-art answer accuracy on six multi-hop question answering datasets,
matching frontier LLMs while using only small-scale models and limited
computational resources. Furthermore, our approach demonstrates strong
generalization and robustness across diverse retrieval environments and
larger-scale models, highlighting its broad applicability.

</details>


### [78] [ViRN: Variational Inference and Distribution Trilateration for Long-Tailed Continual Representation Learning](https://arxiv.org/abs/2507.17368)
*Hao Dai,Chong Tang,Jagmohan Chauhan*

Main category: cs.LG

TL;DR: 提出了ViRN框架，结合变分推理和分布三角测量技术解决长尾数据分布下的持续学习问题，在六个基准测试中平均准确率提升10.24%


<details>
  <summary>Details</summary>
Motivation: 现实世界AI系统面临长尾数据分布的持续学习挑战，模型需要在严重类别不平衡的情况下顺序适应新类别同时保持对旧类别的知识，现有方法难以平衡稳定性和可塑性，在极端样本稀缺下经常崩溃

Method: 提出ViRN框架，集成变分推理和分布三角测量：1）通过变分自编码器建模类条件分布以减轻对头部类别的偏倚；2）通过基于Wasserstein距离的邻域检索和几何融合重构尾部类别分布，实现尾部类别表示的样本高效对齐

Result: 在六个长尾分类基准测试（包括语音任务如稀有声学事件、口音识别和图像任务）上评估，ViRN相比最先进方法平均准确率提升10.24%

Conclusion: ViRN框架通过结合变分推理和分布三角测量技术，有效解决了长尾数据分布下的持续学习问题，显著提升了模型在类别不平衡场景下的性能表现

Abstract: Continual learning (CL) with long-tailed data distributions remains a
critical challenge for real-world AI systems, where models must sequentially
adapt to new classes while retaining knowledge of old ones, despite severe
class imbalance. Existing methods struggle to balance stability and plasticity,
often collapsing under extreme sample scarcity. To address this, we propose
ViRN, a novel CL framework that integrates variational inference (VI) with
distributional trilateration for robust long-tailed learning. First, we model
class-conditional distributions via a Variational Autoencoder to mitigate bias
toward head classes. Second, we reconstruct tail-class distributions via
Wasserstein distance-based neighborhood retrieval and geometric fusion,
enabling sample-efficient alignment of tail-class representations. Evaluated on
six long-tailed classification benchmarks, including speech (e.g., rare
acoustic events, accents) and image tasks, ViRN achieves a 10.24% average
accuracy gain over state-of-the-art methods.

</details>


### [79] [Continual Generalized Category Discovery: Learning and Forgetting from a Bayesian Perspective](https://arxiv.org/abs/2507.17382)
*Hao Dai,Jagmohan Chauhan*

Main category: cs.LG

TL;DR: 本文提出VB-CGCD框架，通过变分贝叶斯推理和协方差感知的最近类均值分类来解决持续广义类别发现中的灾难性遗忘问题，在标准基准上实现了+15.21%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 持续广义类别发现(C-GCD)面临关键挑战：需要从无标签数据流中增量学习新类别，同时保持对旧类别的知识。现有方法在灾难性遗忘方面存在困难，特别是当无标签数据混合了已知和新颖类别时。

Method: 通过贝叶斯视角分析C-GCD的遗忘动态，发现旧类和新类之间的协方差不对齐是性能下降的驱动因素。基于此洞察，提出变分贝叶斯C-GCD(VB-CGCD)框架，集成变分推理与协方差感知的最近类均值分类，通过随机变分更新自适应对齐类分布并抑制伪标签噪声。

Result: 实验显示VB-CGCD在标准基准的最终会话中整体准确率超越现有技术+15.21%。在新的挑战性基准(仅10%标签数据和扩展在线阶段)上，VB-CGCD达到67.86%的最终准确率，显著高于最先进方法(38.55%)。

Conclusion: VB-CGCD通过变分贝叶斯推理有效解决了持续广义类别发现中的协方差不对齐问题，在多种场景下展现了强大的适用性，为持续学习领域提供了新的解决方案。

Abstract: Continual Generalized Category Discovery (C-GCD) faces a critical challenge:
incrementally learning new classes from unlabeled data streams while preserving
knowledge of old classes. Existing methods struggle with catastrophic
forgetting, especially when unlabeled data mixes known and novel categories. We
address this by analyzing C-GCD's forgetting dynamics through a Bayesian lens,
revealing that covariance misalignment between old and new classes drives
performance degradation. Building on this insight, we propose Variational Bayes
C-GCD (VB-CGCD), a novel framework that integrates variational inference with
covariance-aware nearest-class-mean classification. VB-CGCD adaptively aligns
class distributions while suppressing pseudo-label noise via stochastic
variational updates. Experiments show VB-CGCD surpasses prior art by +15.21%
with the overall accuracy in the final session on standard benchmarks. We also
introduce a new challenging benchmark with only 10% labeled data and extended
online phases, VB-CGCD achieves a 67.86% final accuracy, significantly higher
than state-of-the-art (38.55%), demonstrating its robust applicability across
diverse scenarios. Code is available at: https://github.com/daihao42/VB-CGCD

</details>


### [80] [A Comprehensive Evaluation on Quantization Techniques for Large Language Models](https://arxiv.org/abs/2507.17417)
*Yutong Liu,Cairong Zhao,Guosheng Hu*

Main category: cs.LG

TL;DR: 本文对大语言模型的训练后量化方法进行了全面评估和理论分析，将量化方法解耦为预量化变换和量化误差缓解两个步骤，并评估了最新的MXFP4数据格式性能。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法研究缺乏公平比较的统一评估基准，且各方法之间的理论联系分析不足，需要进行系统性的综合评估和理论分析来填补这一空白。

Method: 将现有量化方法解耦为两个关键步骤：(1)预量化变换-量化前的预处理步骤，用于减少异常值影响；(2)量化误差缓解-抵消量化过程中引入的误差。在统一基准上对各组件进行综合评估和分析。

Result: 实验结果表明：(1)优化的旋转和缩放在预量化变换中表现最佳；(2)低秩补偿结合GPTQ在量化误差缓解方面有时优于单独使用GPTQ；(3)INT4的最优预量化变换策略不能很好地泛化到MXFP4格式。

Conclusion: 通过系统性的理论分析和公平评估，揭示了不同量化组件的影响，为量化方法的发展提供了重要见解，特别是发现INT4和MXFP4量化策略存在差异，需要进一步研究。

Abstract: For large language models (LLMs), post-training quantization (PTQ) can
significantly reduce memory footprint and computational overhead. Model
quantization is a rapidly evolving research field. Though many papers have
reported breakthrough performance, they may not conduct experiments on the same
ground since one quantization method usually contains multiple components. In
addition, analyzing the theoretical connections among existing methods is
crucial for in-depth understanding. To bridge these gaps, we conduct an
extensive review of state-of-the-art methods and perform comprehensive
evaluations on the same ground to ensure fair comparisons. To our knowledge,
this fair and extensive investigation remains critically important yet
underexplored. To better understand the theoretical connections, we decouple
the published quantization methods into two steps: pre-quantization
transformation and quantization error mitigation. We define the former as a
preprocessing step applied before quantization to reduce the impact of
outliers, making the data distribution flatter and more suitable for
quantization. Quantization error mitigation involves techniques that offset the
errors introduced during quantization, thereby enhancing model performance. We
evaluate and analyze the impact of different components of quantization
methods. Additionally, we analyze and evaluate the latest MXFP4 data format and
its performance. Our experimental results demonstrate that optimized rotation
and scaling yield the best performance for pre-quantization transformation, and
combining low-rank compensation with GPTQ occasionally outperforms using GPTQ
alone for quantization error mitigation. Furthermore, we explore the potential
of the latest MXFP4 quantization and reveal that the optimal pre-quantization
transformation strategy for INT4 does not generalize well to MXFP4, inspiring
further investigation.

</details>


### [81] [Persistent Patterns in Eye Movements: A Topological Approach to Emotion Recognition](https://arxiv.org/abs/2507.17450)
*Arsha Niksa,Hooman Zare,Ali Shahrabi,Hanieh Hatami,Mohammadreza Razvan*

Main category: cs.LG

TL;DR: 该研究提出了一种基于拓扑数据分析的眼动追踪情感识别方法，通过持续同调分析眼动轨迹的延迟嵌入，从持续图中提取形状特征，使用随机森林分类器在四类情感分类任务中达到75.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的眼动数据情感识别方法可能无法充分捕获眼动轨迹的复杂几何结构和动态特征，需要一种新的方法来有效编码眼动数据中的判别性信息用于情感计算。

Method: 采用拓扑数据分析流水线：首先对眼动轨迹进行延迟嵌入，然后使用持续同调分析嵌入后的数据，从生成的持续图中提取基于形状的特征（如平均持续性、最大持续性和熵），最后使用随机森林分类器进行情感分类。

Result: 在基于情感环形模型四个象限的四类情感分类任务中，该方法达到了75.6%的分类准确率，证明了持续图几何结构能够有效编码具有判别性的眼动动态特征。

Conclusion: 持续图几何结构能够有效编码眼动轨迹中的判别性动态特征，为情感计算和人类行为分析提供了一种有前景的拓扑方法，展示了拓扑数据分析在情感识别领域的应用潜力。

Abstract: We present a topological pipeline for automated multiclass emotion
recognition from eye-tracking data. Delay embeddings of gaze trajectories are
analyzed using persistent homology. From the resulting persistence diagrams, we
extract shape-based features such as mean persistence, maximum persistence, and
entropy. A random forest classifier trained on these features achieves up to
$75.6\%$ accuracy on four emotion classes, which are the quadrants the
Circumplex Model of Affect. The results demonstrate that persistence diagram
geometry effectively encodes discriminative gaze dynamics, suggesting a
promising topological approach for affective computing and human behavior
analysis.

</details>


### [82] [Efficient Neural Network Verification via Order Leading Exploration of Branch-and-Bound Trees](https://arxiv.org/abs/2507.17453)
*Guanqin Zhang,Kota Fukuda,Zhenya Zhang,H. M. N. Dilum Bandara,Shiping Chen,Jianjun Zhao,Yulei Sui*

Main category: cs.LG

TL;DR: 本文提出了Oliva框架，通过优先探索更可能包含反例的子问题来提高神经网络形式化验证的效率，在MNIST和CIFAR10数据集上分别实现了最高25倍和80倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的分支定界(BaB)神经网络验证方法采用"先到先服务"的朴素策略探索子问题空间，导致验证效率低下，难以快速得出验证结论。

Method: 提出Oliva验证框架，基于子问题包含反例的可能性对其进行排序，优先探索更可能找到反例的子问题。包含两个变体：贪心策略Oliva^GR和受模拟退火启发的平衡策略Oliva^SA。

Result: 在690个验证问题上的实验表明，与最先进方法相比，Oliva在MNIST数据集上实现最高25倍加速，在CIFAR10数据集上实现最高80倍加速。

Conclusion: Oliva通过智能的子问题探索顺序显著提高了神经网络形式化验证的效率，即使无法找到反例也不会导致性能下降，为神经网络验证提供了更高效的解决方案。

Abstract: The vulnerability of neural networks to adversarial perturbations has
necessitated formal verification techniques that can rigorously certify the
quality of neural networks. As the state-of-the-art, branch and bound (BaB) is
a "divide-and-conquer" strategy that applies off-the-shelf verifiers to
sub-problems for which they perform better. While BaB can identify the
sub-problems that are necessary to be split, it explores the space of these
sub-problems in a naive "first-come-first-serve" manner, thereby suffering from
an issue of inefficiency to reach a verification conclusion. To bridge this
gap, we introduce an order over different sub-problems produced by BaB,
concerning with their different likelihoods of containing counterexamples.
Based on this order, we propose a novel verification framework Oliva that
explores the sub-problem space by prioritizing those sub-problems that are more
likely to find counterexamples, in order to efficiently reach the conclusion of
the verification. Even if no counterexample can be found in any sub-problem, it
only changes the order of visiting different sub-problem and so will not lead
to a performance degradation. Specifically, Oliva has two variants, including
$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that
are more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy
inspired by simulated annealing that gradually shifts from exploration to
exploitation to locate the globally optimal sub-problems. We experimentally
evaluate the performance of Oliva on 690 verification problems spanning over 5
models with datasets MNIST and CIFAR10. Compared to the state-of-the-art
approaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up
to 80X in CIFAR10.

</details>


### [83] [C3RL: Rethinking the Combination of Channel-independence and Channel-mixing from Representation Learning](https://arxiv.org/abs/2507.17454)
*Shusen Ma,Yun-Bo Zhao,Yu Kang*

Main category: cs.LG

TL;DR: 提出了C3RL框架，通过对比学习联合建模通道混合(CM)和通道独立(CI)策略，在多变量时间序列预测中显著提升了性能，CI策略模型最佳性能率达81.4%，CM策略模型达76.3%。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测方法存在局限性：CM策略能捕获变量间依赖但无法识别变量特定的时间模式；CI策略改善了这一点但无法充分利用跨变量依赖；基于特征融合的混合策略泛化能力和可解释性有限。

Method: 提出C3RL表示学习框架，受计算机视觉对比学习启发，将CM和CI策略的输入视为转置视图，构建孪生网络架构：一个策略作为主干，另一个作为补充。通过自适应加权联合优化对比损失和预测损失来平衡表示和预测性能。

Result: 在七个模型上的广泛实验显示，C3RL将基于CI策略的模型最佳性能率提升至81.4%，将基于CM策略的模型最佳性能率提升至76.3%，展现了强大的泛化能力和有效性。

Conclusion: C3RL框架成功解决了现有方法的局限性，通过联合建模CM和CI策略实现了显著的性能提升，为多变量时间序列预测提供了一个有效且通用的解决方案。

Abstract: Multivariate time series forecasting has drawn increasing attention due to
its practical importance. Existing approaches typically adopt either
channel-mixing (CM) or channel-independence (CI) strategies. CM strategy can
capture inter-variable dependencies but fails to discern variable-specific
temporal patterns. CI strategy improves this aspect but fails to fully exploit
cross-variable dependencies like CM. Hybrid strategies based on feature fusion
offer limited generalization and interpretability. To address these issues, we
propose C3RL, a novel representation learning framework that jointly models
both CM and CI strategies. Motivated by contrastive learning in computer
vision, C3RL treats the inputs of the two strategies as transposed views and
builds a siamese network architecture: one strategy serves as the backbone,
while the other complements it. By jointly optimizing contrastive and
prediction losses with adaptive weighting, C3RL balances representation and
forecasting performance. Extensive experiments on seven models show that C3RL
boosts the best-case performance rate to 81.4\% for models based on CI strategy
and to 76.3\% for models based on CM strategy, demonstrating strong
generalization and effectiveness. The code will be available once the paper is
accepted.

</details>


### [84] [BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles](https://arxiv.org/abs/2507.17472)
*Junhua Liu,Roy Ka-Wei Lee,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 本文提出BGM-HAN模型，通过分层学习和多头注意力机制来增强大学招生等高风险决策领域的决策质量，有效处理半结构化申请者数据并提升预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 人类在高风险决策领域往往依赖专业知识和启发式方法，但容易受到难以察觉的认知偏见影响，威胁公平性和长期结果。需要一种新方法来增强复杂决策工作流程，减少认知偏见的影响。

Method: 提出BGM-HAN（增强型字节对编码门控多头分层注意力网络），结合分层学习和多种增强技术。该模型专门设计用于有效建模半结构化申请者数据，能够捕获对细致评估至关重要的多层次表示。

Result: 在真实招生数据上的实验结果表明，所提出的模型显著优于从传统机器学习到大型语言模型的最先进基线方法，在预测性能和可解释性方面都有显著提升。

Conclusion: BGM-HAN为在结构、上下文和公平性都很重要的领域中增强决策制定提供了一个有前景的框架，特别适用于大学招生等高风险决策场景，能够有效减少认知偏见并提高决策质量。

Abstract: Human decision-making in high-stakes domains often relies on expertise and
heuristics, but is vulnerable to hard-to-detect cognitive biases that threaten
fairness and long-term outcomes. This work presents a novel approach to
enhancing complex decision-making workflows through the integration of
hierarchical learning alongside various enhancements. Focusing on university
admissions as a representative high-stakes domain, we propose BGM-HAN, an
enhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,
designed to effectively model semi-structured applicant data. BGM-HAN captures
multi-level representations that are crucial for nuanced assessment, improving
both interpretability and predictive performance. Experimental results on real
admissions data demonstrate that our proposed model significantly outperforms
both state-of-the-art baselines from traditional machine learning to large
language models, offering a promising framework for augmenting decision-making
in domains where structure, context, and fairness matter. Source code is
available at: https://github.com/junhua/bgm-han.

</details>


### [85] [HOTA: Hamiltonian framework for Optimal Transport Advection](https://arxiv.org/abs/2507.17513)
*Nazar Buzun,Daniil Shlenskii,Maxim Bobrin,Dmitry V. Dylov*

Main category: cs.LG

TL;DR: 提出了哈密顿最优传输平流(HOTA)方法，通过Hamilton-Jacobi-Bellman方程和Kantorovich势函数解决双动力学最优传输问题，避免了显式密度建模，在标准基准和非可微成本数据集上都优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型大多假设简单几何结构(如欧几里得)并依赖强密度估计假设，导致轨迹不遵循底层流形的真正最优性原理。需要一种能够处理复杂几何和避免密度估计的最优传输方法。

Method: 提出哈密顿最优传输平流(HOTA)方法，基于Hamilton-Jacobi-Bellman方程，通过Kantorovich势函数显式处理双动力学最优传输问题，实现高效可扩展的轨迹优化，无需显式密度建模。

Result: HOTA在标准基准测试和具有非可微成本的自定义数据集上都优于所有基线方法，在可行性和最优性方面表现更佳，即使在成本函数非光滑的情况下也能有效工作。

Conclusion: HOTA方法成功解决了现有最优传输方法的局限性，通过避免密度建模和处理复杂几何结构，为概率流引导提供了更优的框架，在多种场景下都表现出色。

Abstract: Optimal transport (OT) has become a natural framework for guiding the
probability flows. Yet, the majority of recent generative models assume trivial
geometry (e.g., Euclidean) and rely on strong density-estimation assumptions,
yielding trajectories that do not respect the true principles of optimality in
the underlying manifold. We present Hamiltonian Optimal Transport Advection
(HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical
OT problem explicitly through Kantorovich potentials, enabling efficient and
scalable trajectory optimization. Our approach effectively evades the need for
explicit density modeling, performing even when the cost functionals are
non-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks,
as well as in custom datasets with non-differentiable costs, both in terms of
feasibility and optimality.

</details>


### [86] [Generalized Low-Rank Matrix Contextual Bandits with Graph Information](https://arxiv.org/abs/2507.17528)
*Yao Wang,Jiannan Li,Yue Kang,Shanxing Gao,Zhenxin Xiao*

Main category: cs.LG

TL;DR: 本文提出了一种新的矩阵上下文赌博机算法框架，能够同时利用低秩结构和图信息来改进序列决策，在理论分析和实验中都表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的矩阵上下文赌博机方法只利用了低秩结构，但在在线广告和推荐系统等实际应用中，用户/物品之间还存在图结构关系信息，现有方法无法有效利用这些图信息，导致决策策略效果不佳。

Method: 提出基于置信上界(UCB)框架的新型矩阵上下文赌博机算法，通过联合核范数和矩阵拉普拉斯正则化问题，然后实施基于图的广义线性UCB算法，统一整合低秩结构和图信息。

Result: 理论分析表明该方法在累积遗憾界方面优于几种流行的替代方法，合成数据和真实数据实验进一步验证了该方法的优势。

Conclusion: 通过有效利用图信息，所提出的矩阵上下文赌博机框架能够显著改善序列决策性能，为涉及低秩结构和图信息的应用场景提供了有效的解决方案。

Abstract: The matrix contextual bandit (CB), as an extension of the well-known
multi-armed bandit, is a powerful framework that has been widely applied in
sequential decision-making scenarios involving low-rank structure. In many
real-world scenarios, such as online advertising and recommender systems,
additional graph information often exists beyond the low-rank structure, that
is, the similar relationships among users/items can be naturally captured
through the connectivity among nodes in the corresponding graphs. However,
existing matrix CB methods fail to explore such graph information, and thereby
making them difficult to generate effective decision-making policies. To fill
in this void, we propose in this paper a novel matrix CB algorithmic framework
that builds upon the classical upper confidence bound (UCB) framework. This new
framework can effectively integrate both the low-rank structure and graph
information in a unified manner. Specifically, it involves first solving a
joint nuclear norm and matrix Laplacian regularization problem, followed by the
implementation of a graph-based generalized linear version of the UCB
algorithm. Rigorous theoretical analysis demonstrates that our procedure
outperforms several popular alternatives in terms of cumulative regret bound,
owing to the effective utilization of graph information. A series of synthetic
and real-world data experiments are conducted to further illustrate the merits
of our procedure.

</details>


### [87] [Generalized Advantage Estimation for Distributional Policy Gradients](https://arxiv.org/abs/2507.17530)
*Shahil Shaik,Jonathon M. Smereka,Yue Wang*

Main category: cs.LG

TL;DR: 本文提出分布式广义优势估计(DGAE)，利用最优传输理论和类Wasserstein方向度量来处理分布式强化学习中的价值分布，相比传统GAE能更好地处理系统随机性和噪声


<details>
  <summary>Details</summary>
Motivation: 传统GAE虽然有效降低了强化学习中策略梯度估计的方差和计算复杂度，但无法处理分布式强化学习中的价值分布，而价值分布能够捕获系统固有的随机性，对系统噪声更加鲁棒

Method: 提出基于最优传输理论的类Wasserstein方向度量，该度量能同时衡量概率分布间的距离和方向差异。结合指数加权估计，利用此度量推导出分布式GAE(DGAE)，为策略梯度算法提供低方差且可控偏差的优势估计

Result: 将DGAE集成到三种不同的策略梯度方法中，在多个OpenAI Gym环境下进行评估，与使用传统GAE的基线方法进行性能对比

Conclusion: DGAE成功扩展了GAE到分布式强化学习场景，通过处理价值分布提供了更鲁棒的优势估计，同时保持了低方差和可控偏差的特性，适用于依赖优势估计进行策略更新的算法

Abstract: Generalized Advantage Estimation (GAE) has been used to mitigate the
computational complexity of reinforcement learning (RL) by employing an
exponentially weighted estimation of the advantage function to reduce the
variance in policy gradient estimates. Despite its effectiveness, GAE is not
designed to handle value distributions integral to distributional RL, which can
capture the inherent stochasticity in systems and is hence more robust to
system noises. To address this gap, we propose a novel approach that utilizes
the optimal transport theory to introduce a Wasserstein-like directional
metric, which measures both the distance and the directional discrepancies
between probability distributions. Using the exponentially weighted estimation,
we leverage this Wasserstein-like directional metric to derive distributional
GAE (DGAE). Similar to traditional GAE, our proposed DGAE provides a
low-variance advantage estimate with controlled bias, making it well-suited for
policy gradient algorithms that rely on advantage estimation for policy
updates. We integrated DGAE into three different policy gradient methods.
Algorithms were evaluated across various OpenAI Gym environments and compared
with the baselines with traditional GAE to assess the performance.

</details>


### [88] [Federated Majorize-Minimization: Beyond Parameter Aggregation](https://arxiv.org/abs/2507.17534)
*Aymeric Dieuleveut,Gersende Fort,Mahmoud Hegazy,Hoi-To Wai*

Main category: cs.LG

TL;DR: 本文提出了一个统一的随机优化算法设计方法，可以稳健地扩展到联邦学习设置中，通过Majorize-Minimization框架设计了SSMM算法及其联邦版本QSMM。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习算法缺乏统一的理论框架来处理数据异构性、部分参与和通信约束等常见瓶颈问题，需要一个能够稳健扩展到联邦学习环境的统一随机优化算法设计方法。

Method: 研究了一类具有线性参数化代理函数族的Majorize-Minimization (MM)问题，提出了统一算法SSMM（随机近似随机代理MM），然后将其扩展到联邦设置中得到QSMM算法，该算法通过学习和聚合代理优化函数的信息而非原始参数来工作。

Result: SSMM框架涵盖了梯度算法、期望最大化算法等多种现有方法作为特例；QSMM算法能够有效处理联邦学习中的数据异构性、部分参与和通信约束问题；还展示了该方法在联邦最优传输映射计算中的应用。

Conclusion: 提出的统一MM框架为联邦学习中的随机优化算法设计提供了新的理论基础，QSMM算法通过聚合代理函数信息而非参数的创新方式，为解决联邦学习中的关键挑战提供了有效方案。

Abstract: This paper proposes a unified approach for designing stochastic optimization
algorithms that robustly scale to the federated learning setting. Our work
studies a class of Majorize-Minimization (MM) problems, which possesses a
linearly parameterized family of majorizing surrogate functions. This framework
encompasses (proximal) gradient-based algorithms for (regularized) smooth
objectives, the Expectation Maximization algorithm, and many problems seen as
variational surrogate MM. We show that our framework motivates a unifying
algorithm called Stochastic Approximation Stochastic Surrogate MM (\SSMM),
which includes previous stochastic MM procedures as special instances. We then
extend \SSMM\ to the federated setting, while taking into consideration common
bottlenecks such as data heterogeneity, partial participation, and
communication constraints; this yields \QSMM. The originality of \QSMM\ is to
learn locally and then aggregate information characterizing the
\textit{surrogate majorizing function}, contrary to classical algorithms which
learn and aggregate the \textit{original parameter}. Finally, to showcase the
flexibility of this methodology beyond our theoretical setting, we use it to
design an algorithm for computing optimal transport maps in the federated
setting.

</details>


### [89] [Enhancing Quantum Federated Learning with Fisher Information-Based Optimization](https://arxiv.org/abs/2507.17580)
*Amandeep Singh Bhatia,Sabre Kais*

Main category: cs.LG

TL;DR: 本文提出了一种基于Fisher信息的量子联邦学习(QFL)算法，通过识别和保护对量子模型性能影响关键的参数，解决联邦学习中的通信成本、数据异构性和隐私威胁等挑战


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习面临高通信成本、客户端数据异构性、处理时间长和隐私威胁增加等挑战。量子联邦学习的兴起为医疗保健和金融等领域提供了新机遇，但仍需解决上述问题。Fisher信息能够量化量子状态在参数变化下携带的信息量，可以用来解决这些挑战

Method: 提出了一种量子联邦学习算法，利用在本地客户端模型上计算的Fisher信息，处理分布在异构分区上的数据。该方法识别对量子模型性能有重大影响的关键参数，并确保这些参数在聚合过程中得到保护

Result: 在ADNI和MNIST数据集上的实验结果表明，相比量子联邦平均方法，该方法在性能和鲁棒性方面都取得了更好的效果。验证了在量子联邦学习设置中引入Fisher信息的有效性和可行性

Conclusion: 基于Fisher信息的量子联邦学习算法能够有效解决传统联邦学习面临的挑战，通过保护关键参数在聚合过程中的完整性，实现了更好的模型性能和鲁棒性，为量子联邦学习的实际应用提供了有前景的解决方案

Abstract: Federated Learning (FL) has become increasingly popular across different
sectors, offering a way for clients to work together to train a global model
without sharing sensitive data. It involves multiple rounds of communication
between the global model and participating clients, which introduces several
challenges like high communication costs, heterogeneous client data, prolonged
processing times, and increased vulnerability to privacy threats. In recent
years, the convergence of federated learning and parameterized quantum circuits
has sparked significant research interest, with promising implications for
fields such as healthcare and finance. By enabling decentralized training of
quantum models, it allows clients or institutions to collaboratively enhance
model performance and outcomes while preserving data privacy. Recognizing that
Fisher information can quantify the amount of information that a quantum state
carries under parameter changes, thereby providing insight into its geometric
and statistical properties. We intend to leverage this property to address the
aforementioned challenges. In this work, we propose a Quantum Federated
Learning (QFL) algorithm that makes use of the Fisher information computed on
local client models, with data distributed across heterogeneous partitions.
This approach identifies the critical parameters that significantly influence
the quantum model's performance, ensuring they are preserved during the
aggregation process. Our research assessed the effectiveness and feasibility of
QFL by comparing its performance against other variants, and exploring the
benefits of incorporating Fisher information in QFL settings. Experimental
results on ADNI and MNIST datasets demonstrate the effectiveness of our
approach in achieving better performance and robustness against the quantum
federated averaging method.

</details>


### [90] [XStacking: Explanation-Guided Stacked Ensemble Learning](https://arxiv.org/abs/2507.17650)
*Moncef Garouani,Ayah Barhrhouj,Olivier Teste*

Main category: cs.LG

TL;DR: 本文提出了XStacking框架，通过集成动态特征变换和Shapley可加性解释，解决了集成学习（特别是堆叠方法）缺乏可解释性的问题，在保持预测准确性的同时提供了内在的可解释性。


<details>
  <summary>Details</summary>
Motivation: 集成机器学习技术（特别是堆叠方法）虽然能通过组合多个基础模型来提高预测性能，但经常因缺乏可解释性而受到批评。现有的堆叠模型在获得高预测准确性的同时，难以解释其决策过程，这限制了它们在需要可解释性的应用场景中的使用。

Method: 提出XStacking框架，这是一个有效且内在可解释的框架。该方法通过集成动态特征变换与模型无关的Shapley可加性解释来解决可解释性问题。框架使堆叠模型能够在保持预测准确性的同时变得内在可解释。

Result: 在29个数据集上验证了框架的有效性，在学习空间的预测有效性和结果模型的可解释性两方面都取得了改进。实验结果表明XStacking能够同时提升预测性能和模型可解释性。

Conclusion: XStacking为负责任的机器学习提供了一个实用且可扩展的解决方案。该框架成功解决了集成学习中可解释性与预测性能之间的权衡问题，为需要高性能和可解释性的应用场景提供了新的选择。

Abstract: Ensemble Machine Learning (EML) techniques, especially stacking, have been
shown to improve predictive performance by combining multiple base models.
However, they are often criticized for their lack of interpretability. In this
paper, we introduce XStacking, an effective and inherently explainable
framework that addresses this limitation by integrating dynamic feature
transformation with model-agnostic Shapley additive explanations. This enables
stacked models to retain their predictive accuracy while becoming inherently
explainable. We demonstrate the effectiveness of the framework on 29 datasets,
achieving improvements in both the predictive effectiveness of the learning
space and the interpretability of the resulting models. XStacking offers a
practical and scalable solution for responsible ML.

</details>


### [91] [How Should We Meta-Learn Reinforcement Learning Algorithms?](https://arxiv.org/abs/2507.17668)
*Alexander David Goldie,Zilin Wang,Jakob Nicolaus Foerster,Shimon Whiteson*

Main category: cs.LG

TL;DR: 本文对不同元学习算法在强化学习中的应用进行了首次全面的实证比较，包括进化算法和大语言模型等方法，并提出了元学习新RL算法的指导原则。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法通常从监督或无监督学习中改编而来，存在次优性问题。虽然元学习在RL中显示出前景，但不同元学习算法之间缺乏系统性的比较研究，这阻碍了该领域的发展。

Method: 对多种元学习算法进行实证比较，包括进化算法优化黑箱函数和大语言模型生成代码等方法。这些算法被应用于针对RL流程不同部分的元学习算法上，并从多个维度进行评估。

Result: 除了元训练和元测试性能外，研究还评估了各元学习算法的可解释性、样本成本和训练时间等因素，揭示了不同方法在这些维度上的表现差异。

Conclusion: 基于实证比较的发现，提出了几个用于元学习新RL算法的指导原则，这些原则将有助于确保未来学习到的算法能够达到尽可能高的性能。

Abstract: The process of meta-learning algorithms from data, instead of relying on
manual design, is growing in popularity as a paradigm for improving the
performance of machine learning systems. Meta-learning shows particular promise
for reinforcement learning (RL), where algorithms are often adapted from
supervised or unsupervised learning despite their suboptimality for RL.
However, until now there has been a severe lack of comparison between different
meta-learning algorithms, such as using evolution to optimise over black-box
functions or LLMs to propose code. In this paper, we carry out this empirical
comparison of the different approaches when applied to a range of meta-learned
algorithms which target different parts of the RL pipeline. In addition to
meta-train and meta-test performance, we also investigate factors including the
interpretability, sample cost and train time for each meta-learning algorithm.
Based on these findings, we propose several guidelines for meta-learning new RL
algorithms which will help ensure that future learned algorithms are as
performant as possible.

</details>


### [92] [Generalized Dual Discriminator GANs](https://arxiv.org/abs/2507.17684)
*Penukonda Naga Chandana,Tejas Srivastava,Gowtham R. Kurri,V. Lalitha*

Main category: cs.LG

TL;DR: 本文提出了双判别器α-GANs (D2α-GANs)和广义双判别器GANs，通过结合双判别器架构与可调损失函数来缓解模式坍塌问题，并在理论上证明了这些模型的优化目标可简化为f-散度和反向f-散度的线性组合。


<details>
  <summary>Details</summary>
Motivation: 现有的双判别器GANs (D2GANs)虽然能缓解模式坍塌问题，但缺乏损失函数的灵活性。作者希望通过引入可调的α-损失函数和更广义的函数形式，来提高GANs的性能和灵活性。

Method: 提出了双判别器α-GANs (D2α-GANs)，结合双判别器架构与可调的α-损失函数；进一步将方法推广到定义在正实数上的任意函数，形成广义双判别器GANs；提供了理论分析，证明min-max优化可简化为f-散度和反向f-散度的线性组合。

Result: 理论上证明了所提出模型的优化目标简化形式，推广了D2-GANs中KL散度和反向KL散度线性组合的已知结果；在2D合成数据上进行了实验验证，使用多个性能指标评估了所提GANs的各种优势。

Conclusion: 成功提出了具有更强灵活性的广义双判别器GANs框架，理论分析表明该框架能够有效地将复杂的min-max优化问题简化为散度的线性组合，实验结果验证了方法的有效性。

Abstract: Dual discriminator generative adversarial networks (D2 GANs) were introduced
to mitigate the problem of mode collapse in generative adversarial networks. In
D2 GANs, two discriminators are employed alongside a generator: one
discriminator rewards high scores for samples from the true data distribution,
while the other favors samples from the generator. In this work, we first
introduce dual discriminator $\alpha$-GANs (D2 $\alpha$-GANs), which combines
the strengths of dual discriminators with the flexibility of a tunable loss
function, $\alpha$-loss. We further generalize this approach to arbitrary
functions defined on positive reals, leading to a broader class of models we
refer to as generalized dual discriminator generative adversarial networks. For
each of these proposed models, we provide theoretical analysis and show that
the associated min-max optimization reduces to the minimization of a linear
combination of an $f$-divergence and a reverse $f$-divergence. This generalizes
the known simplification for D2-GANs, where the objective reduces to a linear
combination of the KL-divergence and the reverse KL-divergence. Finally, we
perform experiments on 2D synthetic data and use multiple performance metrics
to capture various advantages of our GANs.

</details>


### [93] [Towards Effective Open-set Graph Class-incremental Learning](https://arxiv.org/abs/2507.17687)
*Jiazhen Chen,Zheng Ma,Sichao Fu,Mingbin Feng,Tony S. Wirjanto,Weihua Ou*

Main category: cs.LG

TL;DR: 本文提出了OGCIL框架，解决开放集图类增量学习问题，通过原型条件变分自编码器生成伪样本嵌入来缓解灾难性遗忘，并使用混合策略和原型超球分类损失来检测未知类别


<details>
  <summary>Details</summary>
Motivation: 现有图类增量学习方法主要关注封闭集假设，假设所有测试样本都属于已知类别，这限制了在真实场景中的应用，因为推理时会自然出现训练时未见过的未知类别。需要解决开放集图类增量学习中的两个挑战：旧类的灾难性遗忘和不充分的开放集识别

Method: 提出OGCIL框架，包含三个关键组件：1）原型条件变分自编码器，用于为旧类合成节点嵌入，实现知识重放而无需存储原始图数据；2）基于混合的策略，从伪分布内和当前节点嵌入生成分布外样本来处理未知类别；3）原型超球分类损失，将分布内嵌入锚定到各自的类原型，同时排斥分布外嵌入

Result: 在五个基准数据集上的大量实验表明，OGCIL相比现有的图类增量学习和开放集图神经网络方法表现更优，能够有效缓解灾难性遗忘并实现鲁棒的未知类别检测

Conclusion: OGCIL框架成功解决了开放集图类增量学习的挑战，通过伪样本嵌入生成有效缓解灾难性遗忘，通过原型感知的拒绝区域建模未知样本为异常值，确保了鲁棒的开放集识别能力

Abstract: Graph class-incremental learning (GCIL) allows graph neural networks (GNNs)
to adapt to evolving graph analytical tasks by incrementally learning new class
knowledge while retaining knowledge of old classes. Existing GCIL methods
primarily focus on a closed-set assumption, where all test samples are presumed
to belong to previously known classes. Such an assumption restricts their
applicability in real-world scenarios, where unknown classes naturally emerge
during inference, and are absent during training. In this paper, we explore a
more challenging open-set graph class-incremental learning scenario with two
intertwined challenges: catastrophic forgetting of old classes, which impairs
the detection of unknown classes, and inadequate open-set recognition, which
destabilizes the retention of learned knowledge. To address the above problems,
a novel OGCIL framework is proposed, which utilizes pseudo-sample embedding
generation to effectively mitigate catastrophic forgetting and enable robust
detection of unknown classes. To be specific, a prototypical conditional
variational autoencoder is designed to synthesize node embeddings for old
classes, enabling knowledge replay without storing raw graph data. To handle
unknown classes, we employ a mixing-based strategy to generate
out-of-distribution (OOD) samples from pseudo in-distribution and current node
embeddings. A novel prototypical hypersphere classification loss is further
proposed, which anchors in-distribution embeddings to their respective class
prototypes, while repelling OOD embeddings away. Instead of assigning all
unknown samples into one cluster, our proposed objective function explicitly
models them as outliers through prototype-aware rejection regions, ensuring a
robust open-set recognition. Extensive experiments on five benchmarks
demonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN
methods.

</details>


### [94] [On the Interaction of Compressibility and Adversarial Robustness](https://arxiv.org/abs/2507.17725)
*Melih Barsbey,Antônio H. Ribeiro,Umut Şimşekli,Tolga Birdal*

Main category: cs.LG

TL;DR: 该论文研究了神经网络压缩性与对抗鲁棒性之间的关系，发现压缩会在表示空间中产生高敏感方向，使对手能够构造有效扰动，揭示了结构化压缩与鲁棒性之间的根本张力。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络需要同时满足多个理想属性：准确拟合训练数据、泛化到未见输入、参数和计算效率以及对抗扰动的鲁棒性。虽然压缩性和鲁棒性都被广泛研究，但对它们相互作用的统一理解仍然缺乏。

Method: 开发了一个原则性框架来分析不同形式的压缩性（如神经元级稀疏性和谱压缩性）如何影响对抗鲁棒性。通过理论分析揭示压缩如何在表示空间中诱导少数高敏感方向，并推导出简单而有指导意义的鲁棒性界限。

Result: 理论分析表明压缩会诱导表示空间中的高敏感方向，使对手能够利用这些方向构造有效扰动。实验证实了理论预测，并显示这些脆弱性在对抗训练和迁移学习下仍然存在，且有助于通用对抗扰动的出现。

Conclusion: 研究揭示了结构化压缩性与鲁棒性之间存在根本张力，无论压缩是通过正则化、架构偏置还是隐式学习动态实现的，这种脆弱性都会出现。这些发现为设计既高效又安全的模型提供了新的途径。

Abstract: Modern neural networks are expected to simultaneously satisfy a host of
desirable properties: accurate fitting to training data, generalization to
unseen inputs, parameter and computational efficiency, and robustness to
adversarial perturbations. While compressibility and robustness have each been
studied extensively, a unified understanding of their interaction still remains
elusive. In this work, we develop a principled framework to analyze how
different forms of compressibility - such as neuron-level sparsity and spectral
compressibility - affect adversarial robustness. We show that these forms of
compression can induce a small number of highly sensitive directions in the
representation space, which adversaries can exploit to construct effective
perturbations. Our analysis yields a simple yet instructive robustness bound,
revealing how neuron and spectral compressibility impact $L_\infty$ and $L_2$
robustness via their effects on the learned representations. Crucially, the
vulnerabilities we identify arise irrespective of how compression is achieved -
whether via regularization, architectural bias, or implicit learning dynamics.
Through empirical evaluations across synthetic and realistic tasks, we confirm
our theoretical predictions, and further demonstrate that these vulnerabilities
persist under adversarial training and transfer learning, and contribute to the
emergence of universal adversarial perturbations. Our findings show a
fundamental tension between structured compressibility and robustness, and
suggest new pathways for designing models that are both efficient and secure.

</details>


### [95] [Joint Asymmetric Loss for Learning with Noisy Labels](https://arxiv.org/abs/2507.17692)
*Jialiang Wang,Xianming Liu,Xiong Zhou,Gangfeng Hu,Deming Zhai,Junjun Jiang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文提出了联合非对称损失(JAL)框架，通过将新设计的非对称均方误差(AMSE)作为被动损失，与主动损失联合优化，有效解决了噪声标签学习中的欠拟合问题，在多个实验中表现出优异的抗噪声性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对称损失函数在处理噪声标签时存在欠拟合问题，而新兴的非对称损失函数虽然理论上具有优越性，但无法与先进的优化框架(如APL)兼容，限制了其潜力和适用性。因此需要扩展非对称损失以适配复杂的被动损失场景。

Method: 提出了非对称均方误差(AMSE)作为新的非对称损失函数，严格建立了AMSE满足非对称条件的充要条件。通过将传统的对称被动损失替换为AMSE，构建了联合非对称损失(JAL)框架，实现主动损失和被动损失的联合优化。

Result: 广泛的实验表明，JAL框架在缓解标签噪声方面表现出显著的有效性，相比现有方法在噪声标签学习任务上取得了更好的性能表现。

Conclusion: 通过扩展非对称损失到被动损失场景并提出AMSE，成功构建了JAL框架，有效解决了对称损失的欠拟合问题，为噪声标签学习提供了新的解决方案，并在实验中验证了方法的有效性。

Abstract: Learning with noisy labels is a crucial task for training accurate deep
neural networks. To mitigate label noise, prior studies have proposed various
robust loss functions, particularly symmetric losses. Nevertheless, symmetric
losses usually suffer from the underfitting issue due to the overly strict
constraint. To address this problem, the Active Passive Loss (APL) jointly
optimizes an active and a passive loss to mutually enhance the overall fitting
ability. Within APL, symmetric losses have been successfully extended, yielding
advanced robust loss functions. Despite these advancements, emerging
theoretical analyses indicate that asymmetric losses, a new class of robust
loss functions, possess superior properties compared to symmetric losses.
However, existing asymmetric losses are not compatible with advanced
optimization frameworks such as APL, limiting their potential and
applicability. Motivated by this theoretical gap and the prospect of asymmetric
losses, we extend the asymmetric loss to the more complex passive loss scenario
and propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We
rigorously establish the necessary and sufficient condition under which AMSE
satisfies the asymmetric condition. By substituting the traditional symmetric
passive loss in APL with our proposed AMSE, we introduce a novel robust loss
framework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate
the effectiveness of our method in mitigating label noise. Code available at:
https://github.com/cswjl/joint-asymmetric-loss

</details>


### [96] [Flow Matching Meets Biology and Life Science: A Survey](https://arxiv.org/abs/2507.17731)
*Zihao Li,Zhichen Zeng,Xiao Lin,Feihao Fang,Yanru Qu,Zhe Xu,Zhining Liu,Xuying Ning,Tianxin Wei,Ge Liu,Hanghang Tong,Jingrui He*

Main category: cs.LG

TL;DR: 这是第一篇关于流匹配(flow matching)在生物学领域应用的综合性调研论文，系统性地回顾了流匹配的基础理论、变体以及在生物序列建模、分子生成设计、肽和蛋白质生成三个主要生物学领域的应用进展。


<details>
  <summary>Details</summary>
Motivation: 生成模型在生物学研究中发挥着重要作用，而流匹配作为扩散模型的强有力替代方案近年来备受关注。然而，缺乏对流匹配在生物学领域应用的系统性综述，因此需要对该领域的发展现状进行全面梳理和总结。

Method: 采用系统性文献综述的方法，首先回顾流匹配的理论基础和各种变体，然后将其生物学应用分为三个主要类别：生物序列建模、分子生成与设计、肽和蛋白质生成，对每个领域的最新进展进行深入分析和总结。

Result: 提供了流匹配在生物学领域应用的首个综合性调研，系统性地整理了该领域的理论发展和实际应用，总结了常用的数据集和软件工具，并建立了相应的资源库供研究者使用。

Conclusion: 流匹配作为一种高效的生成建模方法，在生物学各个领域都展现出了巨大潜力。论文通过系统性综述为该交叉领域的研究者提供了宝贵的参考资源，并对未来发展方向进行了展望，有助于推动流匹配在生物学研究中的进一步应用和发展。

Abstract: Over the past decade, advances in generative modeling, such as generative
adversarial networks, masked autoencoders, and diffusion models, have
significantly transformed biological research and discovery, enabling
breakthroughs in molecule design, protein generation, drug discovery, and
beyond. At the same time, biological applications have served as valuable
testbeds for evaluating the capabilities of generative models. Recently, flow
matching has emerged as a powerful and efficient alternative to diffusion-based
generative modeling, with growing interest in its application to problems in
biology and life sciences. This paper presents the first comprehensive survey
of recent developments in flow matching and its applications in biological
domains. We begin by systematically reviewing the foundations and variants of
flow matching, and then categorize its applications into three major areas:
biological sequence modeling, molecule generation and design, and peptide and
protein generation. For each, we provide an in-depth review of recent progress.
We also summarize commonly used datasets and software tools, and conclude with
a discussion of potential future directions. The corresponding curated
resources are available at
https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.

</details>


### [97] [HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter Merging](https://arxiv.org/abs/2507.17706)
*Taha Ceritli,Ondrej Bohdal,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.LG

TL;DR: HydraOpt是一种新的模型合并技术，通过利用低秩适配器矩阵间的相似性，在大幅减少存储需求(48%减少)的同时保持竞争性能(仅0.2-1.8%性能下降)，为资源受限环境下的大语言模型部署提供了有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常使用适配器来适应下游任务，但为每个任务存储单独的适配器会显著增加内存需求，这在移动设备等资源受限环境中构成挑战。现有的模型合并技术虽然能减少存储成本，但通常会导致性能大幅下降，且在存储大小和性能之间的权衡是固定的。

Method: HydraOpt是一种新的模型合并技术，核心思想是利用低秩适配器矩阵之间的内在相似性。与现有方法不同，HydraOpt允许在效率和性能的光谱中进行导航，而不是产生固定的权衡。该方法能够灵活地在存储大小和性能之间找到平衡点。

Result: 实验结果表明，与存储所有适配器相比，HydraOpt显著减少了存储大小(减少48%)，同时实现了竞争性的性能(仅下降0.2-1.8%)。此外，在相同或略差的存储效率下，HydraOpt在性能方面优于现有的合并技术。

Conclusion: HydraOpt成功解决了大语言模型适配器存储的关键问题，通过创新的模型合并技术实现了存储效率和性能的良好平衡。该方法为在资源受限环境中部署多任务大语言模型提供了实用的解决方案，相比现有技术在性能保持方面具有明显优势。

Abstract: Large language models (LLMs) often leverage adapters, such as low-rank-based
adapters, to achieve strong performance on downstream tasks. However, storing a
separate adapter for each task significantly increases memory requirements,
posing a challenge for resource-constrained environments such as mobile
devices. Although model merging techniques can reduce storage costs, they
typically result in substantial performance degradation. In this work, we
introduce HydraOpt, a new model merging technique that capitalizes on the
inherent similarities between the matrices of low-rank adapters. Unlike
existing methods that produce a fixed trade-off between storage size and
performance, HydraOpt allows us to navigate this spectrum of efficiency and
performance. Our experiments show that HydraOpt significantly reduces storage
size (48% reduction) compared to storing all adapters, while achieving
competitive performance (0.2-1.8% drop). Furthermore, it outperforms existing
merging techniques in terms of performance at the same or slightly worse
storage efficiency.

</details>


### [98] [Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility](https://arxiv.org/abs/2507.17748)
*Melih Barsbey,Lucas Prieto,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.LG

TL;DR: 本文发现大学习率能够同时实现模型对虚假相关性的鲁棒性和网络可压缩性，并揭示了大学习率在标准分类任务中成功的原因可能是其解决了训练数据中隐藏的虚假相关性问题。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型需要同时具备鲁棒性和资源效率，但同时实现这两个目标仍然是一个挑战。研究者希望找到一种能够同时提升模型鲁棒性（对虚假相关性的抵抗能力）和网络压缩性的方法。

Method: 将大学习率作为同时实现鲁棒性和网络可压缩性的促进因子，通过在多样化的虚假相关数据集、模型和优化器上进行实验验证大学习率的效果，并分析其产生的表示属性如不变特征利用、类别分离和激活稀疏性。

Result: 大学习率能够产生理想的表示属性，包括不变特征利用、类别分离和激活稀疏性；在多种虚假相关数据集、模型和优化器上，大学习率都表现出色；相比其他超参数和正则化方法，大学习率在同时满足鲁棒性和压缩性方面更为一致有效。

Conclusion: 大学习率是一个有效的工具，能够同时提升模型的鲁棒性和资源效率。此外，大学习率在标准分类任务中的成功很可能是因为它能够解决训练数据中隐藏或罕见的虚假相关性问题，这为理解大学习率的机制提供了新的视角。

Abstract: Robustness and resource-efficiency are two highly desirable properties for
modern machine learning models. However, achieving them jointly remains a
challenge. In this paper, we position high learning rates as a facilitator for
simultaneously achieving robustness to spurious correlations and network
compressibility. We demonstrate that large learning rates also produce
desirable representation properties such as invariant feature utilization,
class separation, and activation sparsity. Importantly, our findings indicate
that large learning rates compare favorably to other hyperparameters and
regularization methods, in consistently satisfying these properties in tandem.
In addition to demonstrating the positive effect of large learning rates across
diverse spurious correlation datasets, models, and optimizers, we also present
strong evidence that the previously documented success of large learning rates
in standard classification tasks is likely due to its effect on addressing
hidden/rare spurious correlations in the training dataset.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [99] [Assessing Medical Training Skills via Eye and Head Movements](https://arxiv.org/abs/2507.16819)
*Kayhan Latifzadeh,Luis A. Leiva,Klen Čopič Pucihar,Matjaž Kljun,Iztok Devetak,Lili Steblovnik*

Main category: cs.HC

TL;DR: 研究使用眼动和头动追踪技术来评估临床技能发展，通过分析24名从业者在模拟婴儿分娩训练中的眼动数据，发现该技术能有效区分有经验和无经验的从业者，为临床技能评估提供了新的计算模型方法。


<details>
  <summary>Details</summary>
Motivation: 传统的临床技能评估主要依赖主观评分等方法，缺乏客观的量化指标。研究者希望通过眼动和头动追踪技术开发出能够隐式评估临床技能的计算模型，为临床培训提供更科学的评估工具。

Method: 研究招募了24名从业者参与模拟婴儿分娩训练，使用商用眼动追踪眼镜收集数据，计算关键指标包括瞳孔反应率、注视持续时间和角速度等，然后分析这些指标在区分有经验和无经验从业者方面的效果。

Result: 头部相关特征在区分有经验和无经验从业者方面表现最佳，F1分数达到0.85，AUC为0.86；瞳孔相关特征的F1分数为0.77，AUC为0.85。眼动和头动追踪技术能够有效区分不同技能水平的从业者，特别是在分娩任务中表现突出。

Conclusion: 眼动和头动追踪技术可以作为传统主观评估方法的有效补充，为临床环境中的隐式技能评估和培训提供计算模型基础。该研究为使用商用眼动追踪设备进行客观技能评估奠定了基础。

Abstract: We examined eye and head movements to gain insights into skill development in
clinical settings. A total of 24 practitioners participated in simulated baby
delivery training sessions. We calculated key metrics, including pupillary
response rate, fixation duration, or angular velocity. Our findings indicate
that eye and head tracking can effectively differentiate between trained and
untrained practitioners, particularly during labor tasks. For example,
head-related features achieved an F1 score of 0.85 and AUC of 0.86, whereas
pupil-related features achieved F1 score of 0.77 and AUC of 0.85. The results
lay the groundwork for computational models that support implicit skill
assessment and training in clinical settings by using commodity eye-tracking
glasses as a complementary device to more traditional evaluation methods such
as subjective scores.

</details>


### [100] [Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances](https://arxiv.org/abs/2507.17024)
*Chase Stokes,Kylie Lin,Cindy Xiong Bearfield*

Main category: cs.HC

TL;DR: 该研究探索了评估图表可供性的可扩展研究方法，测试了四种引导方法并与GPT-4o进行了比较，发现排序和评分方法的组合可以作为自由回答的有效替代。


<details>
  <summary>Details</summary>
Motivation: 传统的图表可供性研究需要劳动密集型的众包研究来分析大量自由回答文本，成本高且效率低，因此需要探索更可扩展的替代研究方法来评估图表设计选择对读者理解的影响。

Method: 测试了四种人类被试研究的引导方法：自由回答、可视化排序、结论排序和显著性评分，并在折线图、散点图和热力图上比较其有效性。同时进行了GPT-4o案例研究，探索大语言模型作为人类参与者代理的可能性。

Result: 发现虽然没有方法能完全复制自由回答中观察到的可供性，但排序和评分方法的组合可以在大规模上作为有效代理。两种排序方法受到参与者对特定图表类型偏见的影响。显著性评分无法捕捉其他方法观察到的图表类型间的具体变化。GPT-4o在显著性评分方法上表现最佳，但在其他领域存在严重限制。

Conclusion: 不同引导方法（包括GPT-4o）在可供性方面的差异突出了有意选择和组合方法以及评估权衡的重要性。排序和评分方法的组合可以作为传统自由回答研究的可扩展替代方案。

Abstract: A growing body of work on visualization affordances highlights how specific
design choices shape reader takeaways from information visualizations. However,
mapping the relationship between design choices and reader conclusions often
requires labor-intensive crowdsourced studies, generating large corpora of
free-response text for analysis. To address this challenge, we explored
alternative scalable research methodologies to assess chart affordances. We
test four elicitation methods from human-subject studies: free response,
visualization ranking, conclusion ranking, and salience rating, and compare
their effectiveness in eliciting reader interpretations of line charts, dot
plots, and heatmaps. Overall, we find that while no method fully replicates
affordances observed in free-response conclusions, combinations of ranking and
rating methods can serve as an effective proxy at a broad scale. The two
ranking methodologies were influenced by participant bias towards certain chart
types and the comparison of suggested conclusions. Rating conclusion salience
could not capture the specific variations between chart types observed in the
other methods. To supplement this work, we present a case study with GPT-4o,
exploring the use of large language models (LLMs) to elicit human-like chart
interpretations. This aligns with recent academic interest in leveraging LLMs
as proxies for human participants to improve data collection and analysis
efficiency. GPT-4o performed best as a human proxy for the salience rating
methodology but suffered from severe constraints in other areas. Overall, the
discrepancies in affordances we found between various elicitation
methodologies, including GPT-4o, highlight the importance of intentionally
selecting and combining methods and evaluating trade-offs.

</details>


### [101] [Evaluation of the effects of frame time variation on VR task performance](https://arxiv.org/abs/2507.17139)
*Benjamin Watson,Victoria Spaulding,Neff Walker,William Ribarsky*

Main category: cs.HC

TL;DR: 本研究首次探讨了虚拟环境中帧时间变化对任务表现的影响，发现在可接受的帧时间范围内，适度的变化不会显著影响性能，但在VR最低要求的帧时间下，变化会对闭环任务产生显著影响


<details>
  <summary>Details</summary>
Motivation: 虚拟环境和沉浸式应用设计者经常需要控制帧时间变化，因为VE中图形等复杂性会产生大幅波动，但缺乏关于帧时间变化如何影响任务表现的系统性研究

Method: 选择典型的开环和闭环任务，研究帧时间变化的两个维度：围绕平均帧时间的偏差幅度和波动周期，测试这些变化对虚拟环境中任务表现的影响

Result: 在许多应用认为可接受的帧时间范围内，相当大的幅度偏差和相当宽的周期范围不会显著影响任务表现；但在通常被认为是沉浸式VR最低要求的帧时间下，帧时间变化确实会对闭环任务表现产生显著影响

Conclusion: 研究结果为虚拟环境和沉浸式应用的设计者提供了有用的指导，帮助他们更好地控制由于VE复杂性波动导致的帧时间变化

Abstract: We present a first study of the effects of frame time variations, in both
deviation around mean frame times and period of fluctuation, on task
performance in a virtual environment (VE). Chosen are open and closed loop
tasks that are typical for current applications or likely to be prominent in
future ones. The results show that at frame times in the range deemed
acceptable for many applications, fairly large deviations in amplitude over a
fairly wide range of periods do not significantly affect task performance.
However, at a frame time often considered a minimum for immersive VR, frame
time variations do produce significant effects on closed loop task performance.
The results will be of use to designers of VEs and immersive applications, who
often must control frame time variations due to large fluctuations of
complexity (graphical and otherwise) in the VE.

</details>


### [102] [HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery](https://arxiv.org/abs/2507.17209)
*Haoran Jiang,Shaohan Shi,Yunjie Yao,Chang Jiang,Quan Li*

Main category: cs.HC

TL;DR: HypoChainer是一个协作可视化框架，通过整合人类专业知识、大语言模型推理和知识图谱来增强假设生成和验证，解决了传统假设驱动研究的认知局限性和深度学习模型输出难以筛选的问题。


<details>
  <summary>Details</summary>
Motivation: 现代科学发现在整合大规模异构知识方面面临挑战，传统假设驱动研究受到人类认知局限、生物系统复杂性和高昂试验成本的约束。虽然图神经网络加速了预测生成，但大量输出使得人工筛选验证变得不可扩展。大语言模型虽有潜力，但存在幻觉问题且缺乏结构化知识基础，限制了可靠性。

Method: HypoChainer采用三阶段协作框架：1）探索和语境化阶段 - 专家使用检索增强的大语言模型和降维技术导航大规模GNN预测，通过交互式解释辅助；2）假设链形成阶段 - 专家迭代检查预测周围的知识图谱关系和语义链接实体，利用LLM和KG建议精化假设；3）验证优先级排序阶段 - 基于KG支持的证据过滤精化假设，识别高优先级实验候选，通过可视化分析强化推理中的薄弱环节。

Result: 通过两个领域的案例研究和专家访谈验证了HypoChainer的有效性，展现了其在支持可解释、可扩展和知识基础的科学发现方面的潜力。框架成功整合了人类专业知识、AI推理和结构化知识，为生物医学和药物开发领域的科学发现提供了有效工具。

Conclusion: HypoChainer框架有效解决了现代科学发现中知识整合的挑战，通过人机协作的方式结合了大语言模型、知识图谱和人类专业知识的优势，为可解释、可扩展的科学假设生成和验证提供了新的解决方案，在生物医学和药物开发领域具有重要应用价值。

Abstract: Modern scientific discovery faces growing challenges in integrating vast and
heterogeneous knowledge critical to breakthroughs in biomedicine and drug
development. Traditional hypothesis-driven research, though effective, is
constrained by human cognitive limits, the complexity of biological systems,
and the high cost of trial-and-error experimentation. Deep learning models,
especially graph neural networks (GNNs), have accelerated prediction
generation, but the sheer volume of outputs makes manual selection for
validation unscalable. Large language models (LLMs) offer promise in filtering
and hypothesis generation, yet suffer from hallucinations and lack grounding in
structured knowledge, limiting their reliability. To address these issues, we
propose HypoChainer, a collaborative visualization framework that integrates
human expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance
hypothesis generation and validation. HypoChainer operates in three stages:
First, exploration and contextualization -- experts use retrieval-augmented
LLMs (RAGs) and dimensionality reduction to navigate large-scale GNN
predictions, assisted by interactive explanations. Second, hypothesis chain
formation -- experts iteratively examine KG relationships around predictions
and semantically linked entities, refining hypotheses with LLM and KG
suggestions. Third, validation prioritization -- refined hypotheses are
filtered based on KG-supported evidence to identify high-priority candidates
for experimentation, with visual analytics further strengthening weak links in
reasoning. We demonstrate HypoChainer's effectiveness through case studies in
two domains and expert interviews, highlighting its potential to support
interpretable, scalable, and knowledge-grounded scientific discovery.

</details>


### [103] [OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena](https://arxiv.org/abs/2507.17218)
*Yang Ouyang,Yuchen Wu,Xiyuan Wang,Laixin Xie,Weicong Cheng,Jianping Gan,Quan Li,Xiaojuan Ma*

Main category: cs.HC

TL;DR: OceanVive是一个沉浸式海洋数据可视化系统，通过交互式空间叙事来传达复杂的海洋现象（如缺氧和酸化），旨在改善海洋科学传播效果。


<details>
  <summary>Details</summary>
Motivation: 传统的静态可视化和文本报告难以有效传达海洋变化的动态复杂性，尽管传感技术和计算模型已有进步，但在传达海洋缺氧、酸化等复杂现象方面仍存在挑战，需要更好的科学传播方式。

Method: 开发了OceanVive沉浸式交互可视化系统，包含桌面平板电脑的探索面板用于管理大屏幕沉浸式内容，集成了自适应视觉编码、情境化叙事和直观的导航路径，将复杂海洋数据集转换为可导航的空间叙事。

Result: 通过专家访谈验证了系统的有效性，证明该系统在增强科学传播和促进公众深度理解方面具有潜力。

Conclusion: OceanVive系统成功解决了海洋科学传播中的关键挑战，通过沉浸式交互可视化有效提升了复杂海洋现象的传达效果，为科学传播领域提供了新的解决方案。

Abstract: Communicating the complexity of oceanic phenomena-such as hypoxia and
acidification-poses a persistent challenge for marine science. Despite advances
in sensing technologies and computational models, conventional formats like
static visualizations and text-based reports often fall short in conveying the
dynamics of ocean changes. To address this gap, we present OceanVive, an
immersive and interactive visualization system that transforms complex ocean
datasets into navigable spatial narratives. OceanVive incorporates an
exploratory panel on a table-sized tablet for managing immersive content on a
large screen and integrates adaptive visual encodings, contextual storytelling,
and intuitive navigation pathways to support effective communication. We
validate the system through expert interviews, demonstrating its potential to
enhance science communication and promote deeper public understanding.

</details>


### [104] [A "watch your replay videos" reflection assignment on comparing programming without versus with generative AI: learning about programming, critical AI use and limitations, and reflection](https://arxiv.org/abs/2507.17226)
*Sarah "Magz" Fernandez,Greg L Nelson*

Main category: cs.HC

TL;DR: 本研究设计了一个对比视频反思作业，让学生录制使用和不使用生成式AI编程的过程，然后通过结构化反思问题分析自己的编程行为，帮助学生发展元认知技能和更好地理解AI如何改变编程过程。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI正在颠覆计算机教育，但大多数干预措施专注于教授如何使用GenAI，而不是帮助学生理解AI如何改变他们的编程过程。需要一种新的方法来帮助学生发展在有AI和无AI情况下都适用的元认知编程技能。

Method: 采用DEAL（描述、检查、然后阐述学习）框架，设计了一个新颖的对比视频反思作业。在入门软件工程课程中，学生在团队项目期间录制两次编程过程：第一次不使用生成式AI，第二次使用生成式AI。然后学生使用一套结构化的反思问题分析自己的视频，包括编程过程和寻求人类、互联网和AI帮助的行为。对反思内容进行定性主题分析。

Result: 学生在规划、调试和寻求帮助的行为方面发展了超越AI使用的洞察力。学生学会了在编写或生成代码之前放慢速度并理解问题，识别了解决问题方法中的模式，并明确表达了具体的过程改进。学生还了解并反思了AI的局限性和缺点，以及更批判性地使用AI的策略，包括更好的提示技巧，以及为了学习而非仅仅完成任务而使用AI。意外的是，对比反思还促进了对不涉及AI使用的编程的反思，甚至使学生自发地设定未来采用视频和其他定期反思的目标。

Conclusion: 结构化的编程会话视频反思可以发展在使用和不使用生成式AI进行编程时都必需的元认知技能，以及在不断发展的领域中终身学习的能力。这种方法不仅帮助学生更好地使用AI工具，还促进了整体编程技能和自我反思能力的发展。

Abstract: Generative AI is disrupting computing education. Most interventions focus on
teaching GenAI use rather than helping students understand how AI changes their
programming process. We designed and deployed a novel comparative video
reflection assignment adapting the Describe, Examine, then Articulate Learning
(DEAL) framework. In an introductory software engineering course, students
recorded themselves programming during their team project two times: first
without, then with using generative AI. Students then analyzed their own videos
using a scaffolded set of reflection questions, including on their programming
process and human, internet, and AI help-seeking. We conducted a qualitative
thematic analysis of the reflections, finding students developed insights about
planning, debugging, and help-seeking behaviors that transcended AI use.
Students reported learning to slow down and understand before writing or
generating code, recognized patterns in their problem-solving approaches, and
articulated specific process improvements. Students also learned and reflected
on AI limits and downsides, and strategies to use AI more critically, including
better prompting but also to benefit their learning instead of just completing
tasks. Unexpectedly, the comparative reflection also scaffolded reflection on
programming not involving AI use, and even led to students spontaneously
setting future goals to adopt video and other regular reflection. This work
demonstrates structured reflection on programming session videos can develop
metacognitive skills essential for programming with and without generative AI
and also lifelong learning in our evolving field.

</details>


### [105] [Designing for Learning with Generative AI is a Wicked Problem: An Illustrative Longitudinal Qualitative Case Series](https://arxiv.org/abs/2507.17230)
*Clara Scalzer,Saurav Pokhrel,Sara Hunt,Greg L Nelson*

Main category: cs.HC

TL;DR: 这项研究探讨了在生成式AI环境下计算机教育面临的复杂挑战，发现学生在学习GenAI技能、道德伦理、学习动机和职业发展之间存在相互冲突的"邪恶问题"，需要多维度的教育设计方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的快速发展，计算机教育工作者需要为学生准备适应AI时代的职业技能，但同时要平衡学习意义、道德伦理、技能发展和职业动机等多个目标，这构成了一个复杂的教育挑战。

Method: 采用纵向定性研究方法，对参与GenAI整合创意媒体课程的学生进行跟踪观察和分析。课程包含提示工程、伦理偏见教育和行业专家讲座等内容。重点分析了两名学生Pat和Jay的学习经历和发展轨迹。

Result: 研究发现学生在GenAI学习中出现了多个目标之间的冲突：1）Pat从最初避免使用GenAI发展到过度依赖，自称"臭名昭著的作弊者"，技能提升但伦理意识下降；2）Jay因环境担忧和对创意职业前景的恐惧而自我限制GenAI使用，伦理意识提高但技能发展受阻；3）GenAI技能的提升并未改善学生的职业信心。

Conclusion: 在GenAI时代支持学生发展是一个"邪恶问题"，需要多维度的评估和设计方法，而不是单独优化学习效果、GenAI技能、伦理道德或职业动机中的任何一个方面。教育者需要开发更加综合和平衡的教学策略。

Abstract: Students continue their education when they feel their learning is meaningful
and relevant for their future careers. Computing educators now face the
challenge of preparing students for careers increasingly shaped by generative
AI (GenAI) with the goals of supporting their learning, motivation, ethics, and
career development. Our longitudinal qualitative study of students in a
GenAI-integrated creative media course shows how this is a "wicked" problem:
progress on one goal can then impede progress on other goals. Students
developed concerning patterns despite extensive instruction in critical and
ethical GenAI use including prompt engineering, ethics and bias, and industry
panels on GenAI's career impact. We present an analysis of two students'
experiences to showcase this complexity. Increasing GenAI use skills can lower
ethics; for example, Pat started from purposefully avoiding GenAI use, to
dependency. He described himself as a "notorious cheater" who now uses GenAi to
"get all the right answers" while acknowledging he's learning less. Increasing
ethical awareness can lower the learning of GenAI use skills; for example,
Jay's newfound environmental concerns led to self-imposed usage limits that
impeded skill development, and new serious fears that GenAI would eliminate
creative careers they had been passionate about. Increased GenAI proficiency, a
potential career skill, did not improve their career confidence. These findings
suggest that supporting student development in the GenAI era is a "wicked"
problem requiring multi-dimensional evaluation and design, rather than
optimizing learning, GenAI skills, ethics, or career motivation individually.

</details>


### [106] [High-Density EEG Enables the Fastest Visual Brain-Computer Interfaces](https://arxiv.org/abs/2507.17242)
*Gege Ming,Weihua Pei,Sen Tian,Xiaogang Chen,Xiaorong Gao,Yijun Wang*

Main category: cs.HC

TL;DR: 本研究提出了一种频率-相位-空间融合编码方法，结合256通道高密度脑电图记录，开发了高速脑机接口系统，在200目标BCI范式中实现了472.7 bpm的平均实际信息传输率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉脑机接口系统的信息传输率不足以满足实际应用需求，空间信息作为视觉感知的关键组成部分在现有系统中未得到充分利用，这是由于记录方法的空间分辨率有限，无法捕获大脑信号丰富的时空动态特性。

Method: 提出频率-相位-空间融合编码方法，结合256通道高密度脑电图记录技术，开发高速脑机接口系统。测试了不同电极配置（256-66、128-32、64-21）在经典40目标和新提出的200目标BCI范式中的性能。

Result: 在经典40目标BCI范式中，256-66、128-32和64-21电极配置相比传统64-9设置分别带来83.66%、79.99%和55.50%的理论信息传输率提升。在200目标BCI范式中，这些提升分别达到195.56%、153.08%和103.07%。在线BCI系统实现了平均472.7 bpm的实际信息传输率。

Conclusion: 研究证明了高密度脑电图在解码视觉刺激时空信息方面的重要作用和巨大潜力，为开发实用的高速脑机接口系统提供了新的技术路径。

Abstract: Brain-computer interface (BCI) technology establishes a direct communication
pathway between the brain and external devices. Current visual BCI systems
suffer from insufficient information transfer rates (ITRs) for practical use.
Spatial information, a critical component of visual perception, remains
underexploited in existing systems because the limited spatial resolution of
recording methods hinders the capture of the rich spatiotemporal dynamics of
brain signals. This study proposed a frequency-phase-space fusion encoding
method, integrated with 256-channel high-density electroencephalogram (EEG)
recordings, to develop high-speed BCI systems. In the classical frequency-phase
encoding 40-target BCI paradigm, the 256-66, 128-32, and 64-21 electrode
configurations brought theoretical ITR increases of 83.66%, 79.99%, and 55.50%
over the traditional 64-9 setup. In the proposed frequency-phase-space encoding
200-target BCI paradigm, these increases climbed to 195.56%, 153.08%, and
103.07%. The online BCI system achieved an average actual ITR of 472.7 bpm.
This study demonstrates the essential role and immense potential of
high-density EEG in decoding the spatiotemporal information of visual stimuli.

</details>


### [107] [Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations](https://arxiv.org/abs/2507.17248)
*Xiaoan Liu,Difan Jia,Xianhao Carton Liu,Mar Gonzalez-Franco,Chen Zhu-Tian*

Main category: cs.HC

TL;DR: Reality Proxy是一个混合现实系统，通过引入代理对象（抽象表示）来解耦物理对象的交互限制，使用AI丰富代理的语义属性，实现了在拥挤、远距离或遮挡环境下的高效交互。


<details>
  <summary>Details</summary>
Motivation: 在混合现实中与真实世界物体交互时，当物体拥挤、距离远或部分被遮挡时往往很困难，这些困难源于直接在物理对象上进行交互，输入与物理约束紧密耦合。

Method: 提出Reality Proxy系统，通过引入代理（proxies）——真实世界物体的抽象表示，将交互从物理约束中解耦出来。系统在选择过程中无缝地将交互目标从物理对象转移到其代理，并使用AI为代理丰富语义属性和层次空间关系。

Result: Reality Proxy能够实现多种新颖交互，包括快速浏览、基于属性的过滤、导航嵌套组和复杂的多对象选择，无需新手势或菜单系统。在办公信息检索、大规模空间导航和多无人机控制等多样化场景中展现了系统的多功能性。

Conclusion: 专家评估表明该系统具有实用性和可用性，基于代理的抽象为未来混合现实系统提供了一种强大且可推广的交互范式。

Abstract: Interacting with real-world objects in Mixed Reality (MR) often proves
difficult when they are crowded, distant, or partially occluded, hindering
straightforward selection and manipulation. We observe that these difficulties
stem from performing interaction directly on physical objects, where input is
tightly coupled to their physical constraints. Our key insight is to decouple
interaction from these constraints by introducing proxies-abstract
representations of real-world objects. We embody this concept in Reality Proxy,
a system that seamlessly shifts interaction targets from physical objects to
their proxies during selection. Beyond facilitating basic selection, Reality
Proxy uses AI to enrich proxies with semantic attributes and hierarchical
spatial relationships of their corresponding physical objects, enabling novel
and previously cumbersome interactions in MR - such as skimming,
attribute-based filtering, navigating nested groups, and complex multi object
selections - all without requiring new gestures or menu systems. We demonstrate
Reality Proxy's versatility across diverse scenarios, including office
information retrieval, large-scale spatial navigation, and multi-drone control.
An expert evaluation suggests the system's utility and usability, suggesting
that proxy-based abstractions offer a powerful and generalizable interaction
paradigm for future MR systems.

</details>


### [108] [EventLines: Time Compression for Discrete Event Timelines](https://arxiv.org/abs/2507.17320)
*Yuet Ling Wong,Niklas Elmqvist*

Main category: cs.HC

TL;DR: 本文提出了EventLines技术，通过动态调整时间尺度来优化离散事件序列的可视化表示，解决了传统线性时间轴在处理突发性事件数据时的空间利用不均问题。


<details>
  <summary>Details</summary>
Motivation: 离散事件序列（如论文发表、项目里程碑、患者用药等）通常表现出突发性行为，事件在某些时期密集出现，而在其他时期则相对稀少。传统的线性时间轴图表无法有效表示这类数据，导致在事件密集期出现视觉混乱，而其他区域则利用不充分，需要一种更好的可视化方法。

Method: 开发了EventLines技术，该技术能够动态调整时间尺度以匹配底层事件分布，从而更有效地利用屏幕空间。为了解决非线性时间缩放带来的挑战，EventLines利用时间轴本身的视觉表示来传达不同的尺度变化。此外，还进行了众包图形感知研究来评估不同时间尺度表示对时间感知的影响。

Result: 通过众包图形感知研究，获得了关于不同时间尺度表示如何影响用户时间感知的研究发现，验证了EventLines技术在处理突发性事件序列可视化方面的有效性。

Conclusion: EventLines提供了一种创新的可视化解决方案，能够有效处理具有突发性特征的离散事件序列数据，通过动态时间尺度调整和巧妙的视觉设计，显著改善了此类数据的可视化效果和空间利用效率。

Abstract: Discrete event sequences serve as models for numerous real-world datasets,
including publications over time, project milestones, and medication dosing
during patient treatments. These event sequences typically exhibit bursty
behavior, where events cluster together in rapid succession, interspersed with
periods of inactivity. Standard timeline charts with linear time axes fail to
adequately represent such data, resulting in cluttered regions during event
bursts while leaving other areas unutilized. We introduce EventLines, a novel
technique that dynamically adjusts the time scale to match the underlying event
distribution, enabling more efficient use of screen space. To address the
challenges of non-linear time scaling, EventLines employs the time axis's
visual representation itself to communicate the varying scale. We present
findings from a crowdsourced graphical perception study that examines how
different time scale representations influence temporal perception.

</details>


### [109] [Layered Interactions: Exploring Non-Intrusive Digital Craftsmanship Design Through Lacquer Art Interfaces](https://arxiv.org/abs/2507.17430)
*Yan Dong,Hanjie Yu,Yanran Chen,Zipeng Zhang,Qiong Wu*

Main category: cs.HC

TL;DR: 本文提出了分层交互设计方法，将人机交互技术与传统漆器工艺相结合，通过在漆器多层结构中嵌入交互电路，创造了既保持传统工艺特色又具备现代交互功能的界面，促进了工匠与技术人员的合作。


<details>
  <summary>Details</summary>
Motivation: 随着数字化发展，如何将现代技术与传统工艺的独特特征相结合成为数字工艺领域的关键问题，需要找到一种既能保持传统工艺特色又能融入现代交互技术的设计方法。

Method: 提出分层交互（Layered Interactions）设计方法，利用漆器的多层结构和材料特性，在漆器层间嵌入交互电路和可编程硬件，创建支持多样化交互的有形界面；开发漆器工具包并进行用户实验和半结构化访谈验证方法效果。

Result: 实验表明该方法不仅使传统工匠更容易接触技术，还增强了交互界面的材料性和情感品质，促进了工匠与技术人员之间的相互学习与合作，提高了传统工艺在现代数字环境中的适应性和实用性。

Conclusion: 分层交互方法成功实现了传统漆器工艺与现代HCI技术的无缝融合，为HCI社区引入了跨学科视角，拓展了交互界面的材料和设计可能性，为数字工艺领域提供了新的发展方向。

Abstract: Integrating technology with the distinctive characteristics of craftsmanship
has become a key issue in the field of digital craftsmanship. This paper
introduces Layered Interactions, a design approach that seamlessly merges
Human-Computer Interaction (HCI) technologies with traditional lacquerware
craftsmanship. By leveraging the multi-layer structure and material properties
of lacquerware, we embed interactive circuits and integrate programmable
hardware within the layers, creating tangible interfaces that support diverse
interactions. This method enhances the adaptability and practicality of
traditional crafts in modern digital contexts. Through the development of a
lacquerware toolkit, along with user experiments and semi-structured
interviews, we demonstrate that this approach not only makes technology more
accessible to traditional artisans but also enhances the materiality and
emotional qualities of interactive interfaces. Additionally, it fosters mutual
learning and collaboration between artisans and technologists. Our research
introduces a cross-disciplinary perspective to the HCI community, broadening
the material and design possibilities for interactive interfaces.

</details>


### [110] [SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition](https://arxiv.org/abs/2507.17524)
*Jiahao Tang,Youjun Li,Xiangting Fan,Yangxuan Zheng,Siyuan Lu,Xueping Li,Peng Fang,Chenxi Li,Zi-Gang Huang*

Main category: cs.HC

TL;DR: 本文提出了一种无监督的语义-动态一致性域适应网络(SDC-Net)，用于解决跨被试脑电情感识别中的标签缺失和个体差异问题，在三个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 基于脑电图(EEG)的情感识别在情感脑机接口中具有巨大潜力，但由于被试间存在显著个体差异以及目标域缺乏标注数据，实际部署仍面临挑战。现有方法难以在完全无标签的跨被试场景下实现有效的情感识别。

Method: 提出SDC-Net网络，包含三个核心模块：1)同被试同试验混合策略，通过试验内插值生成增强样本，保持个体身份信息；2)在再生核希尔伯特空间中构建动态分布对齐模块，通过多目标核均值嵌入联合对齐边缘和条件分布，并使用置信度感知的伪标签策略；3)双域相似性一致性学习机制，基于潜在成对相似性强制跨域结构约束，实现语义边界学习。

Result: 在SEED、SEED-IV和Faced三个广泛使用的脑电基准数据集上进行了大量实验。与现有无监督域适应方法相比，SDC-Net在跨被试和跨会话条件下的情感识别任务中都达到了最先进的性能，显著提高了情感解码的准确性和泛化能力。

Conclusion: SDC-Net成功解决了跨被试脑电情感识别中的关键挑战，在完全无标签场景下实现了最优性能，为个性化情感脑机接口的实际应用奠定了坚实基础。该方法的有效性和鲁棒性为情感计算领域的发展提供了重要支撑。

Abstract: Electroencephalography(EEG) based emotion recognition holds great promise for
affective brain-computer interfaces (aBCIs), yet practical deployment remains
challenging due to substantial inter-subject variability and the lack of
labeled data in target domains. To overcome these limitations, we present a
novel unsupervised Semantic-Dynamic Consistency domain adaptation network for
fully label-free cross-subject EEG emotion recognition. First, we introduce a
Same-Subject Same-Trial Mixup strategy that generates augmented samples via
intra-trial interpolation, enhancing data diversity while explicitly preserving
individual identity to mitigate label ambiguity. Second, we construct a dynamic
distribution alignment module in reproducing kernel Hilbert space (RKHS),
jointly aligning marginal and conditional distributions through multi-objective
kernel mean embedding, and leveraging a confidence-aware pseudo-labeling
strategy to ensure stable adaptation. Third, we propose a dual-domain
similarity consistency learning mechanism that enforces cross-domain structural
constraints based on latent pairwise similarities, enabling semantic boundary
learning without relying on temporal synchronization or label priors. To
validate the effectiveness and robustness of the proposed SDC-Net, extensive
experiments are conducted on three widely used EEG benchmark datasets: SEED,
SEED-IV, and Faced. Comparative results against existing unsupervised domain
adaptation methods demonstrate that SDC-Net achieves state-of-the-art
performance in emotion recognition under both cross-subject and cross-session
conditions. This advancement significantly improves the accuracy and
generalization capability of emotion decoding, and lays a solid foundation for
real-world applications of personalized affective brain-computer interfaces
(aBCIs). The source code will be released at
https://github.com/XuanSuTrum/SDC-Net.

</details>


### [111] [Anticipate, Simulate, Reason (ASR): A Comprehensive Generative AI Framework for Combating Messaging Scams](https://arxiv.org/abs/2507.17543)
*Xue Wen Tan,Kenneth See,Stanley Kok*

Main category: cs.HC

TL;DR: 本文提出ASR框架，一个基于生成式AI的方法，通过预测诈骗者回应、创建逼真诈骗对话来帮助用户主动识别即时通讯平台中的诈骗信息，并开发了专门的ScamGPT-J模型用于诈骗检测。


<details>
  <summary>Details</summary>
Motivation: 随着即时通讯诈骗的快速增长，用户安全和财务安全面临日益严峻的挑战，需要开发能够主动识别和理解诈骗的智能系统来保护用户。

Method: 提出Anticipate, Simulate, Reason (ASR)框架，使用大语言模型预测诈骗者回应、创建逼真的诈骗对话，并提供实时可解释的支持；开发ScamGPT-J这一专门的语言模型，在涵盖多种诈骗类型的高质量诈骗对话数据集上进行微调。

Result: 实验评估表明ASR框架显著提升了诈骗检测能力，特别是在工作诈骗等具有挑战性的场景中表现出色；研究发现了用户脆弱性和对AI生成辅助感知的重要人口统计学模式，揭示了风险最高的人群往往最不愿意接受AI支持的矛盾现象。

Conclusion: 这项工作推进了可解释的、以人为中心的AI系统在对抗不断演变的数字威胁方面的实用和理论基础，强调了在AI驱动的欺诈预防中以用户为中心设计的重要性。

Abstract: The rapid growth of messaging scams creates an escalating challenge for user
security and financial safety. In this paper, we present the Anticipate,
Simulate, Reason (ASR) framework, a generative AI method that enables users to
proactively identify and comprehend scams within instant messaging platforms.
Using large language models, ASR predicts scammer responses, creates realistic
scam conversations, and delivers real-time, interpretable support to end-users.
We develop ScamGPT-J, a domain-specific language model fine-tuned on a new,
high-quality dataset of scam conversations covering multiple scam types.
Thorough experimental evaluation shows that the ASR framework substantially
enhances scam detection, particularly in challenging contexts such as job
scams, and uncovers important demographic patterns in user vulnerability and
perceptions of AI-generated assistance. Our findings reveal a contradiction
where those most at risk are often least receptive to AI support, emphasizing
the importance of user-centered design in AI-driven fraud prevention. This work
advances both the practical and theoretical foundations for interpretable,
human-centered AI systems in combating evolving digital threats.

</details>


### [112] [Explainable AI for Collaborative Assessment of 2D/3D Registration Quality](https://arxiv.org/abs/2507.17597)
*Sue Min Cho,Alexander Do,Russell H. Taylor,Mathias Unberath*

Main category: cs.HC

TL;DR: 本研究提出了首个专门用于2D/3D配准质量验证的人工智能框架，结合可解释性特征来增强手术中的质量保证，通过对比AI、人类、人机协作和可解释AI等不同条件，发现可解释性特征能适度提升用户信任但在整体性能上未超越独立AI。


<details>
  <summary>Details</summary>
Motivation: 随着手术数字化转型的推进，2D/3D配准作为图像引导手术导航的关键技术，其算法偶尔会产生不准确结果，而即使微小的配准错误也可能导致修复手术或不可逆的手术错误。当前基于可视化的策略无法让人类可靠地检测出2D/3D配准的错位问题，因此迫切需要robust的质量保证方法。

Method: 提出了首个专门针对2D/3D配准质量验证训练的人工智能框架，增强了可解释性特征来阐明模型的决策过程。通过算法中心和人本中心的评估方法，系统性地比较了四种条件：仅AI、仅人类、人机协作和人类-可解释AI协作。

Result: 研究发现可解释性特征能够适度提升用户信任度和用户推翻AI错误的意愿，但在整体性能上并未超越独立的AI系统。不同协作模式在配准质量验证任务中表现出不同的特点和局限性。

Conclusion: 虽然当前的可解释AI方法在整体性能上未能超越独立AI，但未来在算法设计和人机协作要素方面的扩展工作有望为2D/3D配准提供更robust的质量保证解决方案。该研究为手术中的AI辅助决策和质量控制提供了重要的实证基础。

Abstract: As surgery embraces digital transformation--integrating sophisticated
imaging, advanced algorithms, and robotics to support and automate complex
sub-tasks--human judgment of system correctness remains a vital safeguard for
patient safety. This shift introduces new "operator-type" roles tasked with
verifying complex algorithmic outputs, particularly at critical junctures of
the procedure, such as the intermediary check before drilling or implant
placement. A prime example is 2D/3D registration, a key enabler of image-based
surgical navigation that aligns intraoperative 2D images with preoperative 3D
data. Although registration algorithms have advanced significantly, they
occasionally yield inaccurate results. Because even small misalignments can
lead to revision surgery or irreversible surgical errors, there is a critical
need for robust quality assurance. Current visualization-based strategies alone
have been found insufficient to enable humans to reliably detect 2D/3D
registration misalignments. In response, we propose the first artificial
intelligence (AI) framework trained specifically for 2D/3D registration quality
verification, augmented by explainability features that clarify the model's
decision-making. Our explainable AI (XAI) approach aims to enhance informed
decision-making for human operators by providing a second opinion together with
a rationale behind it. Through algorithm-centric and human-centered
evaluations, we systematically compare four conditions: AI-only, human-only,
human-AI, and human-XAI. Our findings reveal that while explainability features
modestly improve user trust and willingness to override AI errors, they do not
exceed the standalone AI in aggregate performance. Nevertheless, future work
extending both the algorithmic design and the human-XAI collaboration elements
holds promise for more robust quality assurance of 2D/3D registration.

</details>


### [113] [Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills](https://arxiv.org/abs/2507.17688)
*Mohammad Nur Hossain Khan,David creswell,Jordan Albert,Patrick O'Connell,Shawn Fallon,Mathew Polowitz,Xuhai "orson" Xu,Bashima islam*

Main category: cs.HC

TL;DR: 该研究开发了基于智能手机加速度计的呼吸追踪算法和正念技能评估框架，通过生物信号反馈提升数字正念训练的用户参与度和技能发展效果


<details>
  <summary>Details</summary>
Motivation: 数字正念应用虽然让冥想训练更易获得，但长期用户参与度仍是挑战。现有方法难以准确捕捉正念冥想中的缓慢呼吸模式，且缺乏量化的正念技能评估框架

Method: 开发基于智能手机加速度计的呼吸追踪算法，无需额外可穿戴设备；构建首个量化正念技能评估框架，基于呼吸数据评估专注力、感官清晰度和平静心三个维度；在261个正念训练session中进行算法测试和用户研究

Result: 呼吸追踪模型达到1.6次/分钟的平均绝对误差，与真实数据高度一致；正念技能评估的F1分数达到80-84%；用户研究显示呼吸反馈组相比标准应用组在系统可用性方面有显著提升

Conclusion: 智能手机传感器能够有效增强数字正念训练效果。通过将呼吸追踪和正念技能评估集成到商业应用中，证明了该技术在提升用户参与度和技能发展方面的潜力

Abstract: Mindfulness training is widely recognized for its benefits in reducing
depression, anxiety, and loneliness. With the rise of smartphone-based
mindfulness apps, digital meditation has become more accessible, but sustaining
long-term user engagement remains a challenge. This paper explores whether
respiration biosignal feedback and mindfulness skill estimation enhance system
usability and skill development. We develop a smartphone's accelerometer-based
respiration tracking algorithm, eliminating the need for additional wearables.
Unlike existing methods, our approach accurately captures slow breathing
patterns typical of mindfulness meditation. Additionally, we introduce the
first quantitative framework to estimate mindfulness skills-concentration,
sensory clarity, and equanimity-based on accelerometer-derived respiration
data. We develop and test our algorithms on 261 mindfulness sessions in both
controlled and real-world settings. A user study comparing an experimental
group receiving biosignal feedback with a control group using a standard app
shows that respiration feedback enhances system usability. Our respiration
tracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute,
closely aligning with ground truth data, while our mindfulness skill estimation
attains F1 scores of 80-84% in tracking skill progression. By integrating
respiration tracking and mindfulness estimation into a commercial app, we
demonstrate the potential of smartphone sensors to enhance digital mindfulness
training.

</details>


### [114] [DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models](https://arxiv.org/abs/2507.17734)
*Liwenhan Xie,Yanna Lin,Can Liu,Huamin Qu,Xinhuan Shu*

Main category: cs.HC

TL;DR: DataWink是一个通过适配高质量示例来创建自定义数据可视化的系统，使用大型多模态模型提取数据编码，帮助无设计专业知识的用户轻松创建美观的可视化图表。


<details>
  <summary>Details</summary>
Motivation: 缺乏设计专业知识或可视化工具使用经验的用户在创建美观的数据可视化时面临挑战，需要一个更加民主化和易用的可视化创建方法。

Method: 结合大型多模态模型(LMMs)从现有SVG可视化示例中提取数据编码，设计了连接原始SVG和可视化程序的中间表示，提供对话代理和按需生成的控件界面，让用户能够修改数据映射和视觉设计元素。

Result: 通过12名用户的用户研究，包括复制任务和自由探索任务，DataWink在可学习性和个性化创作任务的有效性方面获得认可，证明了其实用性。

Conclusion: 基于示例驱动的方法在数据可视化创建民主化方面具有巨大潜力，DataWink成功展示了如何通过适配高质量示例来降低可视化创建的门槛。

Abstract: Creating aesthetically pleasing data visualizations remains challenging for
users without design expertise or familiarity with visualization tools. To
address this gap, we present DataWink, a system that enables users to create
custom visualizations by adapting high-quality examples. Our approach combines
large multimodal models (LMMs) to extract data encoding from existing SVG-based
visualization examples, featuring an intermediate representation of
visualizations that bridges primitive SVG and visualization programs. Users may
express adaptation goals to a conversational agent and control the visual
appearance through widgets generated on demand. With an interactive interface,
users can modify both data mappings and visual design elements while
maintaining the original visualization's aesthetic quality. To evaluate
DataWink, we conduct a user study (N=12) with replication and free-form
exploration tasks. As a result, DataWink is recognized for its learnability and
effectiveness in personalized authoring tasks. Our results demonstrate the
potential of example-driven approaches for democratizing visualization
creation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [115] [Towards Autonomous Sustainability Assessment via Multimodal AI Agents](https://arxiv.org/abs/2507.17012)
*Zhihan Zhang,Alexander Metzger,Yuxuan Mei,Felix Hähnlein,Zachary Englhardt,Tingyu Cheng,Gregory D. Abowd,Shwetak Patel,Adriana Schulz,Vikram Iyer*

Main category: cs.AI

TL;DR: 研究团队开发了基于多模态AI代理的生命周期评估(LCA)方法，能够在1分钟内计算电子设备的碳排放，准确率达到专家LCA的19%误差范围内，显著提高了可持续性评估的效率和可及性。


<details>
  <summary>Details</summary>
Motivation: 传统生命周期评估需要大量专业数据和专家时间（数周至数月），且经常面临数据缺失问题，阻碍了可持续性信息的广泛应用。随着对可持续性信息需求的激增，亟需开发更高效、更易获取的LCA方法。

Method: 开发多模态AI代理系统，模拟LCA专家与产品经理、工程师之间的交互；使用自定义数据抽象和软件工具从在线文本、维修社区图像和政府认证中提取信息；迭代生成详细的生命周期清单；开发基于产品描述聚类的直接环境影响估算方法；创建数据驱动的排放因子生成方法，将未知材料表示为相似材料排放因子的加权和。

Result: AI方法将专家级LCA时间从数周缩短至1分钟以下；碳足迹估算精度达到专家LCA的19%误差范围内，且无需专有数据；直接估算方法在笔记本电脑上3毫秒内完成，电子产品MAPE为12.28%；数据驱动排放因子方法比人类专家选择最接近数据库条目的MAPE改善了120.26%。

Conclusion: 多模态AI代理能够显著提高LCA的效率和可及性，为未来LCA工作流程提供了新的解决方案。该方法成功解决了数据可用性gaps，并在保持高精度的同时大幅缩短了评估时间，为可持续性评估的普及化奠定了基础。

Abstract: Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.

</details>


### [116] [New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding](https://arxiv.org/abs/2507.17054)
*Shao-Hung Chan,Thomy Phan,Jiaoyang Li,Sven Koenig*

Main category: cs.AI

TL;DR: 本文提出了改进多智能体路径规划算法EECBS的新方法，通过基于冲突数量和延迟估计的柔性分配策略，提高了算法效率并保持有界次优性保证。


<details>
  <summary>Details</summary>
Motivation: 现有的EECBS算法虽然能找到有界次优解，但使用贪心柔性分配时会增加阈值，可能导致解的成本超出预期范围，迫使算法在不同路径集合间切换而非专注解决特定路径集合的冲突，从而降低效率。

Method: 提出三种新的柔性分配机制：1) 基于冲突的柔性分配（按冲突数量比例分配柔性）；2) 基于延迟的柔性分配（估计满足约束所需延迟）；3) 混合策略柔性分配（在层次框架中结合前两种方法）。

Result: 实验结果表明，提出的新柔性分配方法在性能上优于原始的贪心柔性分配策略，同时保持了算法的完整性和有界次优性。

Conclusion: 新提出的柔性分配机制能够有效提升EECBS算法的效率，通过更智能的柔性分配策略避免了频繁的路径集合切换，在保证解质量的同时显著改善了算法性能。

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor $w$ away from optimal. EECBS maintains sets of paths and
a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of $w$ times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.

</details>


### [117] [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)
*Yihao Xue,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: 本文提出使用LoRA进行安全对齐微调可以在保持推理能力的同时提升大语言模型的安全性，有效解决了"安全税"问题。


<details>
  <summary>Details</summary>
Motivation: 传统的安全对齐微调会显著降低大语言模型的推理能力，产生"安全税"现象。需要找到一种既能保证模型安全性又不损害推理能力的方法。

Method: 使用LoRA（低秩适应）技术对拒绝数据集进行监督微调，将安全权重更新限制在低秩空间内，以最小化对推理权重的干扰。同时探索通过正则化或权重合并进一步减少权重重叠的方法。

Result: 在数学、科学和编程四个基准测试中，LoRA方法产生了高度安全的大语言模型，安全水平与全模型微调相当，但不会损害推理能力。LoRA相比全模型微调产生的权重更新与初始权重的重叠更小。

Conclusion: LoRA技术能够有效解决推理能力与安全性之间的权衡问题，为设计更一致改进推理-安全权衡的方法提供了新思路。

Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.

</details>


### [118] [HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study](https://arxiv.org/abs/2507.17118)
*Mandar Pitale,Jelena Frtunikj,Abhinaw Priyadershi,Vasu Singh,Maria Spence*

Main category: cs.AI

TL;DR: 本文提出了HySAFE-AI框架，这是一个混合安全架构分析框架，用于评估AI系统的安全性，特别是针对端到端单体架构（如大语言模型和视觉语言模型）改进传统的安全分析技术。


<details>
  <summary>Details</summary>
Motivation: 随着AI在自动驾驶系统和机器人等安全关键领域的广泛应用，以及近期自主系统向端到端单体架构（如LLMs和VLMs）发展的趋势，需要改进传统的安全分析方法来适应这些复杂的基础模型，特别是它们形成和利用潜在表示的复杂本质。

Method: 研究者回顾了不同的架构解决方案，评估了常见安全分析技术（如失效模式与影响分析FMEA和故障树分析FTA）的有效性，并提出了HySAFE-AI（AI系统混合安全架构分析框架），这是一个适应传统方法来评估AI系统安全性的混合框架。

Result: 展示了如何针对基础模型的复杂特性，特别是其潜在表示的形成和利用方式，改进传统的安全分析技术。成功开发了HySAFE-AI框架，能够更好地评估AI系统的安全性。

Conclusion: 传统的安全分析方法需要针对现代AI系统进行改进和适应。HySAFE-AI框架为评估AI系统安全性提供了有效的解决方案，并为未来AI安全标准的发展提供了指导建议。

Abstract: AI has become integral to safety-critical areas like autonomous driving
systems (ADS) and robotics. The architecture of recent autonomous systems are
trending toward end-to-end (E2E) monolithic architectures such as large
language models (LLMs) and vision language models (VLMs). In this paper, we
review different architectural solutions and then evaluate the efficacy of
common safety analyses such as failure modes and effect analysis (FMEA) and
fault tree analysis (FTA). We show how these techniques can be improved for the
intricate nature of the foundational models, particularly in how they form and
utilize latent representations. We introduce HySAFE-AI, Hybrid Safety
Architectural Analysis Framework for AI Systems, a hybrid framework that adapts
traditional methods to evaluate the safety of AI systems. Lastly, we offer
hints of future work and suggestions to guide the evolution of future AI safety
standards.

</details>


### [119] [Improving LLMs' Generalized Reasoning Abilities by Graph Problems](https://arxiv.org/abs/2507.17168)
*Qifan Zhang,Nuo Chen,Zehua Li,Miao Peng,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: 本文提出GraphPile数据集和GraphMind模型，通过图问题推理（GPR）的持续预训练来增强大语言模型的通用推理能力，在数学和非数学推理任务上都取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在新颖复杂问题上推理能力不足，领域特定的持续预训练方法（如数学推理）缺乏向更广泛推理任务的迁移能力，需要一种能够提升通用推理能力的方法。

Method: 首次使用图问题推理（GPR）进行持续预训练，构建了GraphPile数据集（109亿tokens，涵盖23个图任务），包含思维链、程序思维、执行轨迹和真实世界图数据，并在Llama 3/3.1和Gemma 2基础模型上训练GraphMind。

Result: 在数学推理任务上准确率提升高达4.9%，在逻辑推理和常识推理等非数学推理任务上提升高达21.2%，证明了GPR方法在增强LLM通用推理能力方面的有效性。

Conclusion: 本工作首次利用图问题推理来增强推理模式，引入了首个此类数据集，成功架起了领域特定预训练与通用推理能力之间的桥梁，提升了大语言模型的适应性和鲁棒性。

Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks,
yet their performance often falters on novel and complex problems.
Domain-specific continued pretraining (CPT) methods, such as those tailored for
mathematical reasoning, have shown promise but lack transferability to broader
reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning
(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,
spanning pathfinding, network analysis, numerical computation, and topological
reasoning, require sophisticated logical and relational reasoning, making them
ideal for teaching diverse reasoning patterns. To achieve this, we introduce
GraphPile, the first large-scale corpus specifically designed for CPT using GPR
data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes
chain-of-thought, program-of-thought, trace of execution, and real-world graph
data. Using GraphPile, we train GraphMind on popular base models Llama 3 and
3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in
mathematical reasoning and up to 21.2 percent improvement in non-mathematical
reasoning tasks such as logical and commonsense reasoning. By being the first
to harness GPR for enhancing reasoning patterns and introducing the first
dataset of its kind, our work bridges the gap between domain-specific
pretraining and universal reasoning capabilities, advancing the adaptability
and robustness of LLMs.

</details>


### [120] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: 本文提出将AI引入车辆作为感知平台，通过AI副驾驶实现预测性维护，促进机器与驾驶员之间的智能交互


<details>
  <summary>Details</summary>
Motivation: 传统车辆维护模式是被动响应式的，需要转变为主动预测式维护。车辆作为感知平台具有巨大潜力，但缺乏能够同时理解机器语言和驾驶员语言的AI系统来实现这一转变

Method: 集成AI副驾驶系统，使其能够"说"两种语言：机器语言和驾驶员语言。通过跨学科对话的方式，从概念和技术角度探讨智能车辆系统的实现路径

Result: 提供了智能车辆系统、预测性维护和AI驱动用户交互的概念框架和技术视角，为相关领域的研究和开发提供指导方向

Conclusion: 现在是整合双语AI副驾驶的关键时机，这将推动智能车辆系统、预测性维护和AI用户交互领域的未来研究与发展

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [121] [Agent Identity Evals: Measuring Agentic Identity](https://arxiv.org/abs/2507.17257)
*Elija Perrier,Michael Timothy Bennett*

Main category: cs.AI

TL;DR: 该论文提出了智能体身份评估框架(AIE)，用于测量大语言模型智能体(LMA)在时间维度上维持稳定身份的能力，以解决LLM固有的无状态性、随机性等问题对智能体可信度的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型智能体(LMA)从大语言模型继承了无状态性、随机性、对提示敏感等病理特征，这些特征会破坏智能体的可识别性、连续性、持久性和一致性，从而削弱其可靠性、可信度和实用性，影响推理、规划和行动等核心智能体能力。

Method: 引入智能体身份评估(AIE)框架，这是一个严格的、统计驱动的经验框架，用于测量LMA系统展现和维持其智能体身份的程度。AIE包含一系列新颖的度量指标，可以与其他性能、能力和智能体鲁棒性度量相结合，协助设计最优的LMA基础设施和支撑结构（如记忆和工具）。

Result: 论文提出了可应用于LMA生命周期各个阶段的正式定义和方法，并提供了如何应用这些方法的实际案例。AIE框架能够评估智能体的能力、属性以及从状态扰动中恢复的能力。

Conclusion: AIE框架为评估和改善大语言模型智能体的身份稳定性提供了系统性解决方案，有助于提高LMA的可信度和智能体能力，为构建更可靠的语言模型智能体系统奠定了基础。

Abstract: Central to agentic capability and trustworthiness of language model agents
(LMAs) is the extent they maintain stable, reliable, identity over time.
However, LMAs inherit pathologies from large language models (LLMs)
(statelessness, stochasticity, sensitivity to prompts and
linguistically-intermediation) which can undermine their identifiability,
continuity, persistence and consistency. This attrition of identity can erode
their reliability, trustworthiness and utility by interfering with their
agentic capabilities such as reasoning, planning and action. To address these
challenges, we introduce \textit{agent identity evals} (AIE), a rigorous,
statistically-driven, empirical framework for measuring the degree to which an
LMA system exhibit and maintain their agentic identity over time, including
their capabilities, properties and ability to recover from state perturbations.
AIE comprises a set of novel metrics which can integrate with other measures of
performance, capability and agentic robustness to assist in the design of
optimal LMA infrastructure and scaffolding such as memory and tools. We set out
formal definitions and methods that can be applied at each stage of the LMA
life-cycle, and worked examples of how to apply them.

</details>


### [122] [Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?](https://arxiv.org/abs/2507.17258)
*Andreas Scholl,Natalie Kiesler*

Main category: cs.AI

TL;DR: 研究者开发了基于ChatGPT-4o-mini的编程教育聊天机器人SCRIPT，通过136名德国大学生的实验验证，发现学生的反馈请求遵循特定序列，机器人响应与学生需求高度匹配（75%），为设计生成式AI学习支持系统提供了见解。


<details>
  <summary>Details</summary>
Motivation: 基于生成式AI在编程教育中的应用潜力，研究者希望为编程初学者开发一个既能提供开放式交互又能通过预定义提示进行结构化指导的支持工具，以更好地理解学生的反馈偏好和交互模式。

Method: 开发了基于ChatGPT-4o-mini的聊天机器人SCRIPT，支持开放式交互和结构化指导；在德国一所大学的编程入门课程中对136名学生进行实验；分析学生在解决编程任务时与SCRIPT的交互方式，重点关注其反馈偏好模式。

Result: 发现学生的反馈请求遵循特定的序列模式；聊天机器人的响应与学生请求的反馈类型高度匹配（达到75%）；系统能够很好地遵循预设的系统提示约束条件。

Conclusion: 研究结果为设计基于生成式AI的学习支持系统提供了重要见解，特别是在平衡AI辅助工具的指导性和灵活性方面面临的挑战，有助于优化编程教育中AI工具的设计和应用。

Abstract: Building on prior research on Generative AI (GenAI) and related tools for
programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,
to support novice learners. SCRIPT allows for open-ended interactions and
structured guidance through predefined prompts. We evaluated the tool via an
experiment with 136 students from an introductory programming course at a large
German university and analyzed how students interacted with SCRIPT while
solving programming tasks with a focus on their feedback preferences. The
results reveal that students' feedback requests seem to follow a specific
sequence. Moreover, the chatbot responses aligned well with students' requested
feedback types (in 75%), and it adhered to the system prompt constraints. These
insights inform the design of GenAI-based learning support systems and
highlight challenges in balancing guidance and flexibility in AI-assisted
tools.

</details>


### [123] [Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments](https://arxiv.org/abs/2507.17289)
*Shitong Zhu,Chenhao Fang,Derek Larson,Neel Reddy Pochareddy,Rajeev Rao,Sophie Zeng,Yanqing Peng,Wendy Summer,Alex Goncalves,Arya Pudota,Herve Robert*

Main category: cs.AI

TL;DR: 本文提出了合规大脑助手(CBA)，一个对话式AI助手，通过智能路由机制在快速模式和全代理模式间切换，显著提升企业合规任务效率，在关键词匹配率和LLM评判通过率等指标上大幅超越基础LLM模型。


<details>
  <summary>Details</summary>
Motivation: 企业合规人员在日常工作中面临大量繁琐的合规任务，需要一个既能保证响应质量又能控制延迟的AI助手来提升工作效率。现有的通用LLM在处理复杂合规查询时表现不佳，缺乏针对性的工具调用和上下文发现能力。

Method: 设计了一个用户查询路由器，能够智能选择两种处理模式：(1)FastTrack模式：处理只需要从知识库检索相关上下文的简单请求；(2)FullAgentic模式：处理需要复合操作和工具调用的复杂请求，能够主动发现各种合规文档中的上下文，并调用其他API/模型来满足请求需求。

Result: 实验评估显示CBA在各种真实世界的隐私/合规相关查询上显著优于开箱即用的LLM：平均关键词匹配率从41.7%提升至83.7%，LLM评判通过率从20.0%提升至82.0%。基于路由的完整设计相比单一模式在保持相似运行时间的同时获得了更好的匹配率和通过率。

Conclusion: 路由机制成功实现了响应质量和延迟之间的良好平衡，验证了设计假设。CBA通过智能路由在不同复杂度的合规查询间切换处理模式，有效提升了企业合规工作的效率和准确性，为企业级AI助手的设计提供了有价值的参考方案。

Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.

</details>


### [124] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: 提出了Ctx2TrajGen框架，这是一个基于GAIL的上下文感知轨迹生成模型，能够合成真实的城市驾驶行为，通过结合PPO和WGAN-GP技术解决微观交通建模中的非线性依赖和训练不稳定问题


<details>
  <summary>Details</summary>
Motivation: 精确的微观车辆轨迹建模对交通行为分析和自动驾驶系统至关重要，现有方法在处理复杂城市环境中的车辆交互和上下文信息方面存在不足，同时面临数据稀缺和领域偏移等挑战

Method: 提出Ctx2TrajGen上下文感知轨迹生成框架，使用GAIL（生成对抗模仿学习）作为核心，结合PPO（近端策略优化）和WGAN-GP（带梯度惩罚的Wasserstein生成对抗网络）技术，通过显式地对周围车辆和道路几何进行条件建模来生成交互感知的轨迹

Result: 在无人机捕获的DRIFT数据集上的实验表明，该方法在真实性、行为多样性和上下文保真度方面均优于现有方法，有效解决了数据稀缺和领域偏移问题，无需依赖仿真环境

Conclusion: Ctx2TrajGen框架成功实现了真实城市驾驶行为的合成，通过上下文感知的轨迹生成为交通行为分析和自动驾驶系统提供了鲁棒的解决方案，在处理复杂交通场景中的车辆交互方面表现出色

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [125] [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](https://arxiv.org/abs/2507.17477)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.AI

TL;DR: 提出了一个不确定性驱动的自适应自对齐(UDASA)框架，通过量化输出的语义、事实性和价值对齐三个维度的不确定性，自动化地改善大语言模型的人类意图对齐，无需人工标注


<details>
  <summary>Details</summary>
Motivation: 大语言模型在指令遵循和通用推理方面取得显著进展，但在没有人工标注的情况下实现与人类意图和安全规范的高质量对齐仍然是一个根本性挑战

Method: UDASA框架首先为每个输入生成多个响应，然后从语义、事实性和价值对齐三个维度量化输出不确定性。基于不确定性得分构建偏好对，并根据不确定性差异将训练样本分为保守、适中和探索性三个阶段，然后在这些阶段中逐步优化模型

Result: 实验结果表明，UDASA在多个任务上优于现有对齐方法，包括无害性、有用性、真实性和受控情感生成，显著提升了模型性能

Conclusion: UDASA框架通过不确定性驱动的自适应训练策略，成功实现了大语言模型的全自动化对齐改进，为解决模型对齐中的人工标注依赖问题提供了有效解决方案

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.

</details>


### [126] [LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning](https://arxiv.org/abs/2507.17482)
*Luca Salvatore Lorello,Nikolaos Manginas,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: 研究者提出了LTLZinc基准框架，用于生成时序推理和持续学习任务，以评估神经符号AI方法在时间维度上的表现，实验显示现有方法在时序学习推理方面存在局限性


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号AI方法主要应用于静态场景，缺乏在时间维度进行推理的挑战性设置的探索，需要一个能够评估神经符号和持续学习方法在时序和约束驱动维度上表现的基准框架

Method: 提出LTLZinc基准框架，该框架通过线性时态逻辑规范和MiniZinc约束，结合任意图像分类数据集，生成表达性强的时序推理和持续学习任务，并提供细粒度标注以支持多种神经和神经符号训练设置

Result: 在LTLZinc生成的六个神经符号序列分类任务和四个类持续学习任务上进行实验，证明了时序学习和推理的挑战性，并突出了当前最先进方法的局限性

Conclusion: 发布了LTLZinc生成器和十个即用型任务供神经符号和持续学习社区使用，旨在推动统一时序学习和推理框架的研究发展

Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.

</details>


### [127] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: 本文研究基于认知依赖(EDs)的本体受控查询评估(CQE)，结合最优GA审查器概念，针对布尔析取查询提出了安全的查询重写算法


<details>
  <summary>Details</summary>
Motivation: 在本体查询中需要保护敏感信息不被泄露，现有的CQE框架需要与认知依赖规则结合，以确保在回答查询时既能提供有用信息又能维护安全性

Method: 将认知依赖(EDs)与最优GA审查器概念结合，采用所有最优GA审查器交集的方法来回答布尔析取查询(BUCQs)，并针对DL-Lite_R本体设计了一阶查询重写算法

Result: 证明了交集方法对于完全EDs类别的安全性；对于EDs子类和DL-Lite_R本体，证明了BUCQ查询在数据复杂度上属于AC^0；实验验证了重写算法的实用可行性

Conclusion: 基于认知依赖和最优GA审查器交集的方法能够在保证强安全性的同时实现高效的查询处理，特别是在DL-Lite_R本体上具有良好的计算复杂度特性

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [128] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: 本文提出了一种自动化混合接地算法，通过数据结构启发式方法解决答案集编程中何时使用身体解耦接地和标准自底向上接地的问题，在难以接地的场景中表现出改进效果。


<details>
  <summary>Details</summary>
Motivation: 答案集编程（ASP）在工业应用中面临接地瓶颈问题，阻碍了其广泛采用。虽然混合接地技术结合了标准自底向上接地和身体解耦接地的优势，但何时使用哪种接地方法仍不明确，需要开发自动化的决策机制。

Method: 开发了基于数据结构启发式的分割算法，该算法能够自动检测何时使用身体解耦接地和何时使用标准接地。启发式方法基于规则结构和包含实例数据的估计程序，实现了自动化混合接地决策。

Result: 在原型实现上进行的实验显示了有希望的结果：在难以接地的场景中表现出明显改进，而在难以求解的实例上接近了最先进的性能水平。

Conclusion: 自动化混合接地算法成功解决了混合接地技术中的决策问题，通过智能选择接地策略，在保持求解性能的同时有效缓解了ASP的接地瓶颈，为ASP在工业中的更广泛应用提供了技术支持。

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [129] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: 本文系统研究了可验证奖励强化学习(RLVR)框架下的多领域推理能力，探索数学推理、代码生成和逻辑谜题求解三个核心领域间的相互作用，为优化大语言模型的综合推理能力提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR研究主要专注于单一推理领域，而现实世界的推理场景需要多种认知技能的综合应用。目前对强化学习框架下多个推理技能间相互作用的理解仍然不足，缺乏系统性的跨领域推理研究。

Method: 采用GRPO算法和Qwen-2.5-7B模型族，进行四个关键组件的综合研究：(1)评估单领域训练的域内改进和跨域泛化能力；(2)分析跨域联合训练中的相互增强和冲突；(3)比较基础模型和指令模型在相同RL配置下的性能差异；(4)探索课程学习策略、奖励设计变化和语言特定因素的影响。

Result: 通过大量实验揭示了领域交互的动态机制，识别出影响专业化和可泛化推理性能的关键因素。研究结果为理解多领域推理中的相互作用提供了重要见解，包括跨域增强效应和潜在冲突。

Conclusion: 研究为优化强化学习方法提供了宝贵指导，有助于培养大语言模型的综合多领域推理能力。通过系统分析域间交互，为未来RLVR框架的发展和多技能推理模型的构建奠定了理论基础。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


### [130] [TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment](https://arxiv.org/abs/2507.17514)
*Athanasios Davvetas,Xenia Ziouvelou,Ypatia Dami,Alexis Kaponis,Konstantina Giouvanopoulou,Michael Papademas*

Main category: cs.AI

TL;DR: 本文介绍了TAI扫描工具，这是一个基于RAG的AI自评估工具，专门用于支持AI法案合规性评估，通过预筛选和评估两个阶段来预测AI系统的风险等级并提供相关法规条款。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏有效的AI系统自评估工具来帮助企业和开发者评估其AI系统是否符合AI法案的要求，特别是在风险等级判定和合规义务方面需要专业的指导工具。

Method: 采用基于检索增强生成(RAG)的两阶段方法：首先进行预筛选阶段识别潜在风险，然后进行详细评估阶段。系统通过最少的输入信息来评估AI系统，并检索相关的AI法案条款来支持合规判断。

Result: 定性评估显示该工具在三个不同语义组的用例场景中能够正确预测风险等级并检索到相关条款。工具的推理主要依赖于与高风险系统设置的比较，这归因于高风险系统部署需要谨慎考虑，因此在AI法案中频繁出现。

Conclusion: TAI扫描工具成功实现了AI系统的自动化合规性评估，能够有效预测风险等级并提供相关法规指导，为AI法案的合规性提供了实用的技术解决方案。

Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.

</details>


### [131] [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](https://arxiv.org/abs/2507.17539)
*Xinyao Liu,Diping Song*

Main category: cs.AI

TL;DR: 本文提出了FundusExpert，一个专门用于眼科诊断的多模态大语言模型，通过FundusGen数据集和认知链对齐实现了定位-诊断推理能力的集成，在眼科问答和报告生成任务中显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在医疗诊断领域具有巨大潜力，但在眼科等专业领域面临注释粒度碎片化和临床推理逻辑不一致的关键挑战，阻碍了精确的跨模态理解。

Method: 开发了FundusExpert眼科专用多模态大语言模型和FundusGen数据集，通过智能Fundus-Engine系统实现自动定位和基于MLLM的语义扩展，集成全局疾病分类、局部目标检测和细粒度特征分析，并构建临床对齐的认知链指导模型生成可解释的推理路径。

Result: FundusExpert在眼科问答任务中表现最佳，平均准确率比40B MedRegA高26.6%；在零样本报告生成任务中实现77.0%的临床一致性，显著超过GPT-4o的47.6%；发现数据质量与模型能力之间的缩放定律（L ∝ N^0.068）。

Conclusion: 通过整合区域级定位与诊断推理链，开发了可扩展、临床对齐的多模态大语言模型，为特定领域MLLM中视觉-语言鸿沟的弥合探索了新路径，证明了认知对齐注释能够提高数据利用效率。

Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.

</details>


### [132] [Simulating multiple human perspectives in socio-ecological systems using large language models](https://arxiv.org/abs/2507.17680)
*Yongchao Zeng,Calum Brown,Ioannis Kyriakou,Ronja Hotz,Mark Rounsevell*

Main category: cs.AI

TL;DR: 研究开发了HoPeS框架，利用大语言模型驱动的智能体来模拟不同利益相关者的视角，用户可以扮演不同角色体验视角差异，在土地利用变化的情境中展示了该系统的潜力。


<details>
  <summary>Details</summary>
Motivation: 理解社会生态系统需要从多元利益相关者视角获得洞察，但这些视角往往难以获取。现有方法缺乏有效的替代性仿真探索手段来理解不同利益相关者的观点差异。

Method: 开发HoPeS（以人为本的视角转换）建模框架，使用大语言模型驱动的智能体代表各种利益相关者，用户可以扮演智能体角色体验视角差异。建立仿真协议作为"脚手架"来简化多视角模拟过程，支持用户反思、转换和整合不同视角。

Result: 在制度动力学和土地利用变化背景下开发了原型系统，支持叙事驱动和数值实验。实验中用户先后扮演系统观察者和研究者角色，尽管努力推荐技术上合理的政策，但由于利益相关者的竞争性倡导，政策建议与实施之间仍存在差距，反映了现实中研究者与政策制定者视角的不一致。

Conclusion: 用户体验到了作为研究者的主观挫败感，特别是在保持政治中立与获得政治影响力之间的挑战。尽管如此，用户展现出尝试替代性叙事框架策略的高度动机，表明该系统在探索不同视角方面具有潜力。进一步的系统和协议改进有望实现社会生态仿真中的新型跨学科合作。

Abstract: Understanding socio-ecological systems requires insights from diverse
stakeholder perspectives, which are often hard to access. To enable
alternative, simulation-based exploration of different stakeholder
perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)
modelling framework. HoPeS employs agents powered by large language models
(LLMs) to represent various stakeholders; users can step into the agent roles
to experience perspectival differences. A simulation protocol serves as a
"scaffold" to streamline multiple perspective-taking simulations, supporting
users in reflecting on, transitioning between, and integrating across
perspectives. A prototype system is developed to demonstrate HoPeS in the
context of institutional dynamics and land use change, enabling both
narrative-driven and numerical experiments. In an illustrative experiment, a
user successively adopts the perspectives of a system observer and a researcher
- a role that analyses data from the embedded land use model to inform
evidence-based decision-making for other LLM agents representing various
institutions. Despite the user's effort to recommend technically sound
policies, discrepancies persist between the policy recommendation and
implementation due to stakeholders' competing advocacies, mirroring real-world
misalignment between researcher and policymaker perspectives. The user's
reflection highlights the subjective feelings of frustration and disappointment
as a researcher, especially due to the challenge of maintaining political
neutrality while attempting to gain political influence. Despite this, the user
exhibits high motivation to experiment with alternative narrative framing
strategies, suggesting the system's potential in exploring different
perspectives. Further system and protocol refinement are likely to enable new
forms of interdisciplinary collaboration in socio-ecological simulations.

</details>


### [133] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型(LLM)与实时优化算法的共生智能体范式，用于构建可信赖的6G网络AGI系统，通过输入层和输出层优化器实现精确的网络管理和服务供应。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络的发展，需要从专用AI算法处理孤立任务转向具有更广泛推理能力的AGI驱动网络，实现实时决策和网络功能管理，但现有LLM智能体在数值精度和实时控制方面存在局限性。

Method: 设计了共生智能体范式，结合LLM与实时优化算法：(1)输入层优化器为数值精确任务提供有界不确定性引导；(2)输出层优化器在LLM监督下实现自适应实时控制；实现了无线接入网优化器和SLA多智能体协商器两种新型智能体。

Result: 在5G测试平台上的实验显示，共生智能体比独立LLM智能体的决策错误减少了5倍；小型语言模型在GPU资源开销减少99.9%的情况下达到相似精度，实时循环时间为82ms；多智能体协作RAN演示将RAN过度利用率降低约44%。

Conclusion: 共生范式为下一代AGI驱动的网络系统奠定了基础，该系统在LLM不断发展的情况下仍能保持适应性、高效性和可信赖性，为6G网络的智能化管理提供了新的解决方案。

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [134] [Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations](https://arxiv.org/abs/2507.17699)
*Zhao Song,Song Yue,Jiahao Zhang*

Main category: cs.AI

TL;DR: 研究发现在引入工具增强（Python解释器和草稿本）后，大型推理模型（LRMs）在各种复杂度任务上都能始终优于非推理模型，挑战了"推理是幻觉"的观点


<details>
  <summary>Details</summary>
Motivation: 近期研究表明大型推理模型的逐步思考过程可能并不能真正增强推理能力，在低复杂度和高复杂度任务上甚至不如没有显式推理的模型。作者希望重新审视这些发现，特别是在引入工具增强时LRMs的局限性是否仍然存在

Method: 在三个代表性大语言模型及其LRM对应版本上，引入两种类型的工具增强：Python解释器和草稿本，并在Apple的基准推理谜题上进行评估

Result: 结果显示，通过适当的工具使用，LRMs在所有任务复杂度级别上都始终优于其非推理对应模型

Conclusion: 这些发现挑战了最近"推理是幻觉"的观点，并突出了工具增强LRMs在解决复杂问题方面的潜力

Abstract: Large Reasoning Models (LRMs) have become a central focus in today's large
language model (LLM) research, where models are designed to output a
step-by-step thinking process before arriving at a final answer to handle
complex reasoning tasks. Despite their promise, recent empirical studies (e.g.,
[Shojaee et al., 2025] from Apple) suggest that this thinking process may not
actually enhance reasoning ability, where LLMs without explicit reasoning
actually outperform LRMs on tasks with low or high complexity. In this work, we
revisit these findings and investigate whether the limitations of LRMs persist
when tool augmentations are introduced. We incorporate two types of tools,
Python interpreters and scratchpads, and evaluate three representative LLMs and
their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show
that, with proper tool use, LRMs consistently outperform their non-reasoning
counterparts across all levels of task complexity. These findings challenge the
recent narrative that reasoning is an illusion and highlight the potential of
tool-augmented LRMs for solving complex problems.

</details>


### [135] [Online Submission and Evaluation System Design for Competition Operations](https://arxiv.org/abs/2507.17730)
*Zhe Chen,Daniel Harabor,Ryan Hechnenberger,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: 本文提出了一个自动化在线竞赛系统，用于解决研究竞赛中提交和评估过程的操作负担问题，该系统已成功应用于多个竞赛中。


<details>
  <summary>Details</summary>
Motivation: 研究社区通过基准数据集比较算法性能，但跟踪进展困难，因为论文发表在不同场所且都声称是最先进的。虽然定期竞赛可以评估算法并跟踪进展，但组织者面临管理大量提交的操作负担，参与者在不同环境下开发解决方案也导致评估时的兼容性问题。

Method: 开发了一个在线竞赛系统，自动化提交和评估过程。系统允许组织者高效管理大量提交，利用隔离环境来评估提交内容，解决兼容性问题。

Result: 该系统已成功用于多个竞赛，包括基于网格的路径规划竞赛（Grid-Based Pathfinding Competition）和机器人跑者联盟竞赛（League of Robot Runners competition）。

Conclusion: 提出的在线竞赛系统有效解决了研究竞赛中的操作负担和兼容性问题，通过自动化流程和隔离环境评估，为组织者提供了高效的竞赛管理解决方案。

Abstract: Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.

</details>
